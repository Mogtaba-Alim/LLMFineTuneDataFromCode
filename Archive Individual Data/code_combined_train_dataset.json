[
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_unionList.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\n##TODO:: Migrate tests to CoreGx\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"union List works as union with arbitrary number of arguments\",{\n\texpect_equal(CoreGx::.unionList(1,2,3,4,2,2,1), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1,2), c(1,2))\n\texpect_equal(CoreGx::.unionList(list(c(1,2,3), c(2,3,4), c(1,1,1))), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1), c(1))\n\texpect_equal(CoreGx::.unionList(), NULL)\n})\n\ntest_that(\"CoreGx::.unionList unlists things and unions properly\", {\n\n\texpect_equal(CoreGx::.unionList(list(1,2,3,4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3), list(2,2,2,2,2,2,2)), c(1,2,3,4))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the CoreGx::.unionList function based on the test cases provided?",
        "answer": "The CoreGx::.unionList function is designed to perform a union operation on its input arguments. It can handle an arbitrary number of arguments, including lists, and returns a unique set of elements. The function removes duplicates and flattens nested lists, returning a single vector of unique elements."
      },
      {
        "question": "How does the CoreGx::.unionList function handle different types of input, as demonstrated in the test cases?",
        "answer": "The CoreGx::.unionList function can handle various types of input: 1) Individual arguments: It can take multiple separate arguments and return their union. 2) Lists: It can take a single list or multiple lists as arguments and return the union of all elements. 3) Nested lists: It can handle nested lists, flattening them before performing the union operation. 4) Empty input: When called with no arguments, it returns NULL."
      },
      {
        "question": "What testing framework and assertions are used in this code snippet, and what are they checking for?",
        "answer": "This code snippet uses the testthat framework for R, as evidenced by the 'test_that' function calls. The assertions use 'expect_equal' to check if the output of CoreGx::.unionList matches the expected result. The tests verify that the function correctly handles various input scenarios, including multiple arguments, lists, nested lists, and edge cases like single elements or empty input."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"union List works as union with arbitrary number of arguments\",{\n\texpect_equal(CoreGx::.unionList(1,2,3,4,2,2,1), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1,2), c(1,2))\n\texpect_equal(CoreGx::.unionList(list(c(1,2,3), c(2,3,4), c(1,1,1))), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1), c(1))\n\t# Complete the last expectation\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"union List works as union with arbitrary number of arguments\",{\n\texpect_equal(CoreGx::.unionList(1,2,3,4,2,2,1), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1,2), c(1,2))\n\texpect_equal(CoreGx::.unionList(list(c(1,2,3), c(2,3,4), c(1,1,1))), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1), c(1))\n\texpect_equal(CoreGx::.unionList(), NULL)\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"CoreGx::.unionList unlists things and unions properly\", {\n\texpect_equal(CoreGx::.unionList(list(1,2,3,4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3)), c(1,2,3,4))\n\t# Complete the last expectation\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"CoreGx::.unionList unlists things and unions properly\", {\n\texpect_equal(CoreGx::.unionList(list(1,2,3,4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3), list(2,2,2,2,2,2,2)), c(1,2,3,4))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/__init__.py",
    "language": "py",
    "content": "# read version from installed package\nfrom importlib.metadata import version\n__version__ = \"1.8.0\"\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the code snippet and how does it determine the version of the package?",
        "answer": "The purpose of this code snippet is to set the version of the package. It imports the 'version' function from 'importlib.metadata', but then directly assigns a string '1.8.0' to the '__version__' variable. This means the imported 'version' function is not actually used, and the version is hardcoded instead of being read dynamically from the installed package metadata."
      },
      {
        "question": "What potential issue exists in this code snippet regarding version management?",
        "answer": "The main issue in this snippet is that it imports the 'version' function from 'importlib.metadata', but doesn't use it. Instead, it hardcodes the version string '1.8.0'. This can lead to inconsistencies if the actual package version changes but this file isn't updated. It's generally better to use dynamic version retrieval to ensure the reported version always matches the installed package version."
      },
      {
        "question": "How could this code snippet be improved to better manage the package version?",
        "answer": "To improve this code snippet, you could use the imported 'version' function to dynamically retrieve the package version. This can be done by replacing the hardcoded version with: __version__ = version('your_package_name'). This ensures that the '__version__' variable always reflects the actual installed version of the package, reducing the risk of version inconsistencies and making maintenance easier."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from importlib.metadata import version\n__version__ = ",
        "complete": "from importlib.metadata import version\n__version__ = version('package_name')"
      },
      {
        "partial": "from importlib.metadata import version\n__version__ = ",
        "complete": "from importlib.metadata import version\n__version__ = '1.8.0'"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "importlib.metadata.version"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugSensitivity.R",
    "language": "R",
    "content": "#' Calcualte The Gene Drug Sensitivity\n#'\n#' TODO:: Write a description!\n#' @examples\n#' print(\"TODO::\")\n#'\n#' @param x A \\code{numeric} vector of gene expression values\n#' @param type A \\code{vector} of factors specifying the cell lines or type types\n#' @param batch A \\code{vector} of factors specifying the batch\n#' @param drugpheno A \\code{numeric} vector of drug sensitivity values (e.g.,\n#'   IC50 or AUC)\n# @param duration A \\code{numeric} vector of experiment duration in hours\n#' @param interaction.typexgene \\code{boolean} Should interaction between gene\n#'   expression and cell/type type be computed? Default set to FALSE\n#' @param model \\code{boolean} Should the full linear model be returned? Default\n#'   set to FALSE\n#' @param standardize \\code{character} One of 'SD', 'rescale' or 'none'\n#' @param verbose \\code{boolean} Should the function display messages?\n#'\n#' @return A \\code{vector} reporting the effect size (estimate of the coefficient\n#'   of drug concentration), standard error (se), sample size (n), t statistic,\n#'   and F statistics and its corresponding p-value.\n#'\n#' @importFrom stats sd complete.cases lm glm anova pf formula var\ngeneDrugSensitivity <- function(x, type, batch, drugpheno,\n                                interaction.typexgene=FALSE,\n                                model=FALSE,  standardize=c(\"SD\", \"rescale\", \"none\"), verbose=FALSE) {\n\n  ## NOTE:: The use of T/F warning from BiocCheck is a false positive on the string 'Pr(>F)'\n  standardize <- match.arg(standardize)\n\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  if(length(table(drugpheno)) > 2){\n     if(ncol(drugpheno)>1){\n      ##### FIX NAMES!!!\n      rest <- lapply(seq_len(ncol(drugpheno)), function(i){\n\n        est <- paste(\"estimate\", i, sep=\".\")\n        se <-  paste(\"se\", i, sep=\".\")\n        tstat <- paste(\"tstat\", i, sep=\".\")\n\n        rest <- rep(NA, 3)\n        names(rest) <- c(est, se, tstat)\n        return(rest)\n      })\n      rest <- do.call(c, rest)\n      rest <- c(rest, n=nn, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n      rest <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tstat\"=NA, \"fstat\"=NA, \"pvalue\"=NA, \"df\"=NA)\n    }\n  } else {\n    # rest <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"pvalue\"=NA)\n    if (is.factor(drugpheno[,1])){\n      rest <- c(\"estimate\"=NA_real_, \"se\"=NA_real_, \"n\"=nn, \"pvalue\"=NA_real_, df=NA_real_)\n    } else {\n      rest <- c(\"estimate\" = NA, \"se\" = NA , \"n\" = nn, \"tstat\" = NA , \"fstat\" = NA , \"pvalue\" = NA , \"df\" = NA )\n    }\n  }\n\n  if(nn < 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    ## not enough samples with complete information or no variation in gene expression\n    return(rest)\n  }\n\n  ## standardized coefficient in linear model\n  if(length(table(drugpheno)) > 2 & standardize!= \"none\") {\n    switch(standardize,\n      \"SD\" = drugpheno <- apply(drugpheno, 2, function(x){\n      return(x[ccix]/sd(as.numeric(x[ccix])))}) ,\n      \"rescale\" = drugpheno <- apply(drugpheno, 2, function(x){\n      return(.rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE))    })\n      )\n\n  }else{\n    drugpheno <- drugpheno[ccix,,drop=FALSE]\n  }\n  if(length(table(x)) > 2  & standardize!= \"none\"){\n    switch(standardize,\n      \"SD\" = xx <- x[ccix]/sd(as.numeric(x[ccix])) ,\n      \"rescale\" = xx <- .rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE)\n      )\n  }else{\n    xx <- x[ccix]\n  }\n  if(ncol(drugpheno)>1){\n    ff0 <- paste(\"cbind(\", paste(paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\"), collapse=\",\"), \")\", sep=\"\")\n  } else {\n    ff0 <- \"drugpheno.1\"\n  }\n\n  # ff1 <- sprintf(\"%s + x\", ff0)\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n  # , \"x\"=xx, \"type\"=type[ccix], \"batch\"=batch[ccix])\n\n  ## control for tissue type\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  ## control for batch\n  if(length(sort(unique(batch[ccix]))) > 1) {\n        dd <- cbind(dd, batch=batch[ccix])\n  }\n  ## control for duration\n  # if(length(sort(unique(duration))) > 1){\n  #   ff0 <- sprintf(\"%s + duration\", ff0)\n  #   ff <- sprintf(\"%s + duration\", ff)\n  # }\n\n  # if(is.factor(drugpheno[,1])){\n\n  #   drugpheno <- drugpheno[,1]\n\n  # } else {\n\n  #   drugpheno <- as.matrix(drugpheno)\n\n  # }\nif(any(unlist(lapply(drugpheno,is.factor)))){\n\n## Added default '' value to ww to fix function if it is passed verbose = FALSE\nww = ''\n\nrr0 <- tryCatch(try(glm(formula(drugpheno.1 ~ . - x), data=dd, model=FALSE, x=FALSE, y=FALSE, family=\"binomial\")),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Null model did not convrge\"\n        print(ww)\n        if(\"type\" %in% colnames(dd)) {\n          tt <- table(dd[,\"type\"])\n          print(tt)\n        }\n        return(ww)\n      }\n    })\n  rr1 <- tryCatch(try(glm(formula(drugpheno.1 ~ .), data=dd, model=FALSE, x=FALSE, y=FALSE, family=\"binomial\")),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Model did not converge\"\n        tt <- table(dd[,\"drugpheno.1\"])\n        print(ww)\n        print(tt)\n      }\n      return(ww)\n    })\n\n\n} else{\n\nrr0 <- tryCatch(try(lm(formula(paste(ff0, \"~ . -x\", sep=\" \")), data=dd)),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Null model did not converge\"\n        print(ww)\n        if(\"type\" %in% colnames(dd)) {\n          tt <- table(dd[,\"type\"])\n          print(tt)\n        }\n      return(ww)\n      }\n    })\n  rr1 <- tryCatch(try(lm(formula(paste(ff0, \"~ . \", sep=\" \")), data=dd)),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Model did not converge\"\n        tt <- table(dd[,\"drugpheno.1\"])\n        print(ww)\n        print(tt)\n      }\n      return(ww)\n    })\n\n\n}\n\n\n  if (!is(rr0, \"try-error\") && !is(rr1, \"try-error\") & !is(rr0, \"character\") && !is(rr1, \"character\")) {\n    rr <- summary(rr1)\n\n    if(any(unlist(lapply(drugpheno,is.factor)))){\n      rrc <- stats::anova(rr0, rr1, test=\"Chisq\")\n      rest <- c(\"estimate\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Estimate\"], \"se\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Std. Error\"], \"n\"=nn, \"pvalue\"=rrc$'Pr(>Chi)'[2], \"df\"=rr1$df.residual)\n      names(rest) <- c(\"estimate\", \"se\", \"n\", \"pvalue\", \"df\")\n\n    } else {\n      if(ncol(drugpheno)>1){\n        rrc <- summary(stats::manova(rr1))\n        rest <- lapply(seq_len(ncol(drugpheno)), function(i) {\n          est <- paste(\"estimate\", i, sep=\".\")\n          se <-  paste(\"se\", i, sep=\".\")\n          tstat <- paste(\"tstat\", i, sep=\".\")\n          rest <- c(rr[[i]]$coefficients[grep(\"^x\", rownames(rr[[i]]$coefficients)), \"Estimate\"], rr[[i]]$coefficients[grep(\"^x\", rownames(rr[[i]]$coefficients)), \"Std. Error\"], rr[[i]]$coefficients[grep(\"^x\", rownames(rr[[i]]$coefficients)), \"t value\"])\n          names(rest) <- c(est, se, tstat)\n          return(rest)\n        })\n        rest <- do.call(c, rest)\n        rest <- c(rest,\"n\"=nn, \"fstat\"=rrc$stats[grep(\"^x\", rownames(rrc$stats)), \"approx F\"], \"pvalue\"=rrc$stats[grep(\"^x\", rownames(rrc$stats)), \"Pr(>F)\"])\n      } else {\n        rrc <- stats::anova(rr0, rr1, test = \"F\")\n        if(!length(rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Estimate\"])){\n          stop(\"A model failed to converge even with sufficient data. Please investigate further\")\n        }\n        rest <- c(\"estimate\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Estimate\"], \"se\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Std. Error\"],\"n\"=nn, \"tstat\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"t value\"], \"fstat\"=rrc$F[2], \"pvalue\"=rrc$'Pr(>F)'[2], \"df\"=rr1$df.residual)\n        names(rest) <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\")\n      }\n    }\n\n\n#    rest <- c(\"estimate\"=rr$coefficients[\"x\", \"Estimate\"], \"se\"=rr$coefficients[\"x\", \"Std. Error\"], \"n\"=nn, \"tsat\"=rr$coefficients[\"x\", \"t value\"], \"fstat\"=rrc$F[2], \"pvalue\"=rrc$'Pr(>F)'[2])\n\n#   names(rest) <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n\n## add tissue type/cell line statistics\n#     if(length(sort(unique(type))) > 1) {\n#       rr <- summary(rr0)\n#       ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n#       names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n#     } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n#     rest <- c(rest, ttype)\n    ## add model\n    if(model) { rest <- list(\"stats\"=rest, \"model\"=rr1) }\n  }\n  return(rest)\n}\n\n## Helper Functions\n##TODO:: Add  function documentation\n#' @importFrom stats quantile\n.rescale <- function(x, na.rm=FALSE, q=0)\n{\n  if(q == 0) {\n    ma <- max(x, na.rm=na.rm)\n    mi <- min(x, na.rm=na.rm)\n  } else {\n    ma <- quantile(x, probs=1-(q/2), na.rm=na.rm)\n    mi <- quantile(x, probs=q/2, na.rm=na.rm)\n  }\n  xx <- (x - mi) / (ma - mi)\n  return(xx)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `geneDrugSensitivity` function and what are its key input parameters?",
        "answer": "The `geneDrugSensitivity` function calculates gene drug sensitivity. Its key input parameters are: `x` (gene expression values), `type` (cell lines or type types), `batch` (batch factors), `drugpheno` (drug sensitivity values), `interaction.typexgene` (boolean for interaction computation), `model` (boolean to return full linear model), `standardize` (method for standardization), and `verbose` (boolean for displaying messages)."
      },
      {
        "question": "How does the function handle different types of drug phenotype data, and what statistical models are used for analysis?",
        "answer": "The function handles both continuous and categorical drug phenotype data. For continuous data, it uses linear models (lm). For categorical data (factors), it uses generalized linear models (glm) with a binomial family. The function performs model comparisons using ANOVA for continuous data and Chi-square tests for categorical data to assess the significance of the gene expression effect."
      },
      {
        "question": "What is the purpose of the `.rescale` helper function in this code, and how is it used within the main function?",
        "answer": "The `.rescale` helper function is used to rescale numeric vectors. It's used in the `geneDrugSensitivity` function when the `standardize` parameter is set to 'rescale'. It scales the values of `x` (gene expression) and `drugpheno` to a range between 0 and 1, with optional quantile-based capping to handle outliers. This standardization helps in comparing effects across different scales of measurement."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneDrugSensitivity <- function(x, type, batch, drugpheno,\n                                interaction.typexgene=FALSE,\n                                model=FALSE,  standardize=c(\"SD\", \"rescale\", \"none\"), verbose=FALSE) {\n  standardize <- match.arg(standardize)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  \n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  # TODO: Complete the rest of the function\n}",
        "complete": "geneDrugSensitivity <- function(x, type, batch, drugpheno,\n                                interaction.typexgene=FALSE,\n                                model=FALSE,  standardize=c(\"SD\", \"rescale\", \"none\"), verbose=FALSE) {\n  standardize <- match.arg(standardize)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  \n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  if(nn < 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    return(c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tstat\"=NA, \"fstat\"=NA, \"pvalue\"=NA, \"df\"=NA))\n  }\n\n  if(standardize != \"none\") {\n    drugpheno <- apply(drugpheno, 2, function(x) {\n      if(standardize == \"SD\") x[ccix]/sd(as.numeric(x[ccix]))\n      else .rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE)\n    })\n    xx <- if(standardize == \"SD\") x[ccix]/sd(as.numeric(x[ccix]))\n          else .rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE)\n  } else {\n    drugpheno <- drugpheno[ccix,,drop=FALSE]\n    xx <- x[ccix]\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n  if(length(unique(type[ccix])) > 1) dd$type <- type[ccix]\n  if(length(unique(batch[ccix])) > 1) dd$batch <- batch[ccix]\n\n  ff0 <- paste(\"cbind(\", paste(paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\"), collapse=\",\"), \")\")\n  \n  rr0 <- try(lm(formula(paste(ff0, \"~ . -x\", sep=\" \")), data=dd))\n  rr1 <- try(lm(formula(paste(ff0, \"~ . \", sep=\" \")), data=dd))\n\n  if (!inherits(rr0, \"try-error\") && !inherits(rr1, \"try-error\")) {\n    rr <- summary(rr1)\n    rrc <- anova(rr0, rr1)\n    rest <- c(\"estimate\"=rr$coefficients[\"x\", \"Estimate\"],\n              \"se\"=rr$coefficients[\"x\", \"Std. Error\"],\n              \"n\"=nn,\n              \"tstat\"=rr$coefficients[\"x\", \"t value\"],\n              \"fstat\"=rrc$F[2],\n              \"pvalue\"=rrc$`Pr(>F)`[2],\n              \"df\"=rr1$df.residual)\n    if(model) rest <- list(\"stats\"=rest, \"model\"=rr1)\n    return(rest)\n  }\n  return(c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tstat\"=NA, \"fstat\"=NA, \"pvalue\"=NA, \"df\"=NA))\n}"
      },
      {
        "partial": ".rescale <- function(x, na.rm=FALSE, q=0) {\n  # TODO: Implement the rescaling function\n}",
        "complete": ".rescale <- function(x, na.rm=FALSE, q=0) {\n  if(q == 0) {\n    ma <- max(x, na.rm=na.rm)\n    mi <- min(x, na.rm=na.rm)\n  } else {\n    ma <- quantile(x, probs=1-(q/2), na.rm=na.rm)\n    mi <- quantile(x, probs=q/2, na.rm=na.rm)\n  }\n  xx <- (x - mi) / (ma - mi)\n  return(xx)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeAUC_old.R",
    "language": "R",
    "content": "# Return AUC (Area Under the drug response curve) for an experiment of a pSet by taking\n# its concentration and viability as input.\n#\n# @param conc `numeric` A concentration range that the AUC should be computed for that range.\n# Concentration range by default considered as not logarithmic scaled.\n# @param viability `numeric` Viablities correspondant to the concentration range passed as first parameter.\n# The range of viablity values by definition should be between 0 and 100. But the viabalities greater than\n# 100 and lower than 0 are also accepted.\n# @param trunc [binary] A flag that identify if the viabality values should be truncated to be in the\n# range of (0,100)\n# @param verbose `logical(1)` If 'TRUE' the function will retrun warnings and other infomrative messages.\n# @import caTools\n#' @keywords internal\n#' @noRd\ncomputeAUC_old <- function(conc, viability,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc=TRUE,\n                       verbose=TRUE,\n                       area.type=c(\"Fitted\",\"Actual\")) {\n    cleanData <- sanitizeInput(conc, viability,\n                             conc_as_log=conc_as_log,\n                             viability_as_pct=viability_as_pct,\n                             trunc=trunc, verbose=verbose)\n    log_conc <- cleanData[[\"log_conc\"]]\n    viability <- cleanData[[\"viability\"]]\n\n#   ii <- which(concentration == 0)\n#   if(length(ii) > 0) {\n#     concentration <- concentration[-ii]\n#     viability <- viability[-ii]\n#   }\n\n  if(missing(area.type)){\n    area.type <- \"Fitted\"\n  }\n  if(length(conc) < 2){\n    return(NA)\n  }\n  if(area.type == \"Actual\"){\n    # if(trunc) {viability = pmin(as.numeric(viability), 100); viability = pmax(as.numeric(viability), 0)}\n    trapezoid.integral <- caTools::trapz(log10(as.numeric(conc) + 1) ,as.numeric(viability))\n    AUC <- round(1- (trapezoid.integral/trapz(log10(as.numeric(conc)), rep(100, length(viability)))), digits=2)\n  }else{\n    AUC <- .computeAUCUnderFittedCurve(conc, viability, trunc)\n  }\n  return (AUC)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC_old` function and what are its main input parameters?",
        "answer": "The `computeAUC_old` function calculates the Area Under the Curve (AUC) for a drug response experiment. Its main input parameters are: 'conc' (a numeric vector of concentration values), 'viability' (a numeric vector of corresponding viability values), 'conc_as_log' (boolean indicating if concentration is in log scale), 'viability_as_pct' (boolean indicating if viability is in percentage), 'trunc' (boolean for truncating viability values), 'verbose' (boolean for displaying messages), and 'area.type' (string specifying the type of AUC calculation)."
      },
      {
        "question": "How does the function handle data sanitization and what happens if the concentration vector has less than 2 elements?",
        "answer": "The function uses a `sanitizeInput` helper function to clean and prepare the input data. This function likely handles the conversion of concentration and viability values based on the `conc_as_log` and `viability_as_pct` parameters, and may also perform truncation if `trunc` is TRUE. If the concentration vector has less than 2 elements, the function returns NA, as seen in the line `if(length(conc) < 2){ return(NA) }`."
      },
      {
        "question": "What are the two methods of calculating AUC in this function and how do they differ?",
        "answer": "The function offers two methods for calculating AUC, specified by the 'area.type' parameter: 'Actual' and 'Fitted' (default). The 'Actual' method uses the trapezoidal rule (implemented with caTools::trapz) to calculate the area under the actual data points. The 'Fitted' method uses a helper function `.computeAUCUnderFittedCurve`, which likely fits a curve to the data points and then calculates the area under this fitted curve. The 'Actual' method works directly with the provided data, while the 'Fitted' method potentially smooths out noise in the data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC_old <- function(conc, viability,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc=TRUE,\n                       verbose=TRUE,\n                       area.type=c(\"Fitted\",\"Actual\")) {\n    cleanData <- sanitizeInput(conc, viability,\n                             conc_as_log=conc_as_log,\n                             viability_as_pct=viability_as_pct,\n                             trunc=trunc, verbose=verbose)\n    log_conc <- cleanData[[\"log_conc\"]]\n    viability <- cleanData[[\"viability\"]]\n\n    if(missing(area.type)){\n        area.type <- \"Fitted\"\n    }\n    if(length(conc) < 2){\n        return(NA)\n    }\n    # Complete the function here\n}",
        "complete": "computeAUC_old <- function(conc, viability,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc=TRUE,\n                       verbose=TRUE,\n                       area.type=c(\"Fitted\",\"Actual\")) {\n    cleanData <- sanitizeInput(conc, viability,\n                             conc_as_log=conc_as_log,\n                             viability_as_pct=viability_as_pct,\n                             trunc=trunc, verbose=verbose)\n    log_conc <- cleanData[[\"log_conc\"]]\n    viability <- cleanData[[\"viability\"]]\n\n    if(missing(area.type)){\n        area.type <- \"Fitted\"\n    }\n    if(length(conc) < 2){\n        return(NA)\n    }\n    if(area.type == \"Actual\"){\n        trapezoid.integral <- caTools::trapz(log10(as.numeric(conc) + 1), as.numeric(viability))\n        AUC <- round(1 - (trapezoid.integral / trapz(log10(as.numeric(conc)), rep(100, length(viability)))), digits=2)\n    } else {\n        AUC <- .computeAUCUnderFittedCurve(conc, viability, trunc)\n    }\n    return(AUC)\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability,\n                      conc_as_log = FALSE,\n                      viability_as_pct = TRUE,\n                      trunc = TRUE,\n                      verbose = TRUE) {\n    # Implement input sanitization logic here\n}",
        "complete": "sanitizeInput <- function(conc, viability,\n                      conc_as_log = FALSE,\n                      viability_as_pct = TRUE,\n                      trunc = TRUE,\n                      verbose = TRUE) {\n    if (!is.numeric(conc) || !is.numeric(viability)) {\n        stop(\"conc and viability must be numeric vectors\")\n    }\n    if (length(conc) != length(viability)) {\n        stop(\"conc and viability must have the same length\")\n    }\n    if (!conc_as_log) {\n        log_conc <- log10(conc)\n    } else {\n        log_conc <- conc\n    }\n    if (!viability_as_pct) {\n        viability <- viability * 100\n    }\n    if (trunc) {\n        viability <- pmin(pmax(viability, 0), 100)\n    }\n    if (verbose && any(viability < 0 | viability > 100)) {\n        warning(\"Some viability values are outside the range [0, 100]\")\n    }\n    return(list(log_conc = log_conc, viability = viability))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/GuideToPharm.R",
    "language": "R",
    "content": "#' Get data from the Guide to PHARMACOLOGY Database Web Services\n#'\n#' @param ids `character()` or `integer()` Identifiers to query the web\n#'   service with. If excluded, the entire record for the specified service\n#'   is returned.\n#' @param service `character(1)` Which Guide to PHARMACOLOGY web service\n#'   to query. Defaults to 'ligands'. Other options are 'targets', 'interactions',\n#'   'diseases' and 'references'.\n#' @param id_type `character(1)` What type of identifiers are in `ids`? Defaults\n#'   to 'name', for drug name. Other options are 'accession', which accepts\n#'   PubChem CIDs.\n#' @param ... Force subsequent parameters to be named. Not used.\n#'\n#' @return A `data.table` of query results.\n#'\n#' @details\n#' The API reference documentation can be found here:\n#' https://www.guidetopharmacology.org/webServices.jsp\n#'\n#' There is also a Python interface available for querying this API. See:\n#' https://github.com/samirelanduk/pygtop\n#'\n#' @importFrom data.table data.table as.data.table rbindlist setnames\n#' @importFrom jsonlite fromJSON\n#' @importFrom httr RETRY GET status_code\n#'\n#' @export\n# getGuideToPharm <- function(\n#     ids = character(),\n#     service = c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"),\n#     id_type = c(\"name\", \"accession\"),\n#     ...,\n# ){\n\n\n#     checkmate::assert_atomic(ids, any.missing = FALSE, min.len = 1)\n#     checkmate::assert_character(service, len = 1)\n#     checkmate::assert_character(id_type, len = 1)\n\n#     url <- httr2::url_parse(\"https://www.guidetopharmacology.org/services\")\n#     url$path <- .buildURL(url$path, service)\n\n#     opts <- list()\n\n#     opts[id_type] <- paste0(ids, collapse = \",\")\n\n# }\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getGuideToPharm` function and what are its main parameters?",
        "answer": "The `getGuideToPharm` function is designed to retrieve data from the Guide to PHARMACOLOGY Database Web Services. Its main parameters are: `ids` (identifiers to query the web service), `service` (which web service to query, defaulting to 'ligands'), and `id_type` (type of identifiers in `ids`, defaulting to 'name')."
      },
      {
        "question": "How does the function handle input validation for its parameters?",
        "answer": "The function uses the `checkmate` package for input validation. It checks that `ids` is an atomic vector with no missing values and a minimum length of 1, `service` is a character vector of length 1, and `id_type` is also a character vector of length 1. This ensures that the inputs meet the expected format and constraints before proceeding with the API request."
      },
      {
        "question": "What is the purpose of the `url` and `opts` variables in the function, and how are they constructed?",
        "answer": "The `url` variable is used to construct the base URL for the API request. It starts with 'https://www.guidetopharmacology.org/services' and is then modified using the `.buildURL` function to include the specified service. The `opts` variable is a list that will contain query parameters for the API request. It's populated with the `id_type` as the key and a comma-separated string of `ids` as the value, which will be used to form the query string in the final API request."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getGuideToPharm <- function(ids = character(), service = c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"), id_type = c(\"name\", \"accession\"), ...) {\n  checkmate::assert_atomic(ids, any.missing = FALSE, min.len = 1)\n  checkmate::assert_character(service, len = 1)\n  checkmate::assert_character(id_type, len = 1)\n\n  url <- httr2::url_parse(\"https://www.guidetopharmacology.org/services\")\n  url$path <- .buildURL(url$path, service)\n\n  opts <- list()\n  opts[id_type] <- paste0(ids, collapse = \",\")\n\n  # Complete the function by adding code to make the API request and process the response\n}",
        "complete": "getGuideToPharm <- function(ids = character(), service = c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"), id_type = c(\"name\", \"accession\"), ...) {\n  checkmate::assert_atomic(ids, any.missing = FALSE, min.len = 1)\n  checkmate::assert_character(service, len = 1)\n  checkmate::assert_character(id_type, len = 1)\n\n  url <- httr2::url_parse(\"https://www.guidetopharmacology.org/services\")\n  url$path <- .buildURL(url$path, service)\n\n  opts <- list()\n  opts[id_type] <- paste0(ids, collapse = \",\")\n\n  response <- httr::RETRY(\"GET\", httr2::url_build(url), query = opts)\n  if (httr::status_code(response) != 200) {\n    stop(\"API request failed with status code: \", httr::status_code(response))\n  }\n\n  content <- jsonlite::fromJSON(httr::content(response, \"text\", encoding = \"UTF-8\"))\n  result <- data.table::as.data.table(content)\n\n  return(result)\n}"
      },
      {
        "partial": ".buildURL <- function(base_path, service) {\n  # Complete the function to build the correct URL path based on the service\n}",
        "complete": ".buildURL <- function(base_path, service) {\n  service <- match.arg(service, c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"))\n  return(file.path(base_path, service))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_summarizeSensitivityProfiles.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking summarizeSensitivityProfiles function.\")\n\ndata(\"GDSCsmall\")\n\n## FIXME:: No S4 method for summarizeSensitivityProfiles with class 'missing'\n#test_that(\"Summarize Sensitivity Profiles fails gracefully.\", {\n#  expect_error(summarizeSensitivityProfiles(), \"argument \\\"pSet\\\" is missing\")\n#})\n\n\ntest_that(\"Summarize Sensitivity Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall)\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equal(rownames(testSummary), treatmentNames(GDSCsmall))\n  expect_equivalent(is(testSummary, \"matrix\"), TRUE)\n})\n\ntest_that(\"summarizeSensitivityProfiles produces correct values.\",{\n\n  GDSCsmall2 <- subsetTo(GDSCsmall, drugs=\"AZD6482\")\n  testCells <- sensitivityProfiles(GDSCsmall2)[order(sensitivityInfo(GDSCsmall2)$sampleid),\"auc_recomputed\", drop=FALSE]\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"median\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {median(c(x,y))}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"mean\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {mean(c(x,y))}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"first\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {x}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"last\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {y}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n})\n\n\ntest_that(\"Summarize Sensitivity Profiles parameters work as expected\", {\n  expect_silent(summarizeSensitivityProfiles(GDSCsmall, verbose = FALSE))\n  expect_equal(ncol(summarizeSensitivityProfiles(GDSCsmall, fill.missing = FALSE)), length(unique(sensitivityInfo(GDSCsmall)$sampleid)))\n  expect_equal(ncol(summarizeSensitivityProfiles(GDSCsmall, fill.missing = TRUE)), length(sampleNames(GDSCsmall)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeSensitivityProfiles` function in this code, and how is it being tested?",
        "answer": "The `summarizeSensitivityProfiles` function is used to summarize sensitivity profiles from a PharmacoSet object. It's being tested for correct output dimensions, column and row names, class type, and various summary statistics (median, mean, first, last). The tests ensure that the function produces expected results for different input parameters and handles edge cases correctly."
      },
      {
        "question": "How does the code test different summary statistics in the `summarizeSensitivityProfiles` function?",
        "answer": "The code tests different summary statistics by calling `summarizeSensitivityProfiles` with various `summary.stat` parameters: 'median', 'mean', 'first', and 'last'. For each statistic, it compares the function's output to manually calculated expected values using `mapply` and the corresponding R functions (median, mean) or selection methods (first, last element). This ensures that each summary statistic option produces the correct result."
      },
      {
        "question": "What is the purpose of the `fill.missing` parameter in the `summarizeSensitivityProfiles` function, and how is it tested?",
        "answer": "The `fill.missing` parameter in `summarizeSensitivityProfiles` determines whether missing values should be filled in the output. When `fill.missing = TRUE`, the function includes all samples from the PharmacoSet, potentially filling in missing values. When `FALSE`, it only includes samples with available sensitivity data. The test compares the number of columns in the output when `fill.missing` is set to `TRUE` and `FALSE`, expecting more columns (all samples) when `TRUE` and fewer columns (only samples with data) when `FALSE`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Summarize Sensitivity Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall)\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equal(rownames(testSummary), treatmentNames(GDSCsmall))\n  # Complete the test by checking if testSummary is a matrix\n})",
        "complete": "test_that(\"Summarize Sensitivity Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall)\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equal(rownames(testSummary), treatmentNames(GDSCsmall))\n  expect_true(is.matrix(testSummary))\n})"
      },
      {
        "partial": "test_that(\"summarizeSensitivityProfiles produces correct values.\", {\n  GDSCsmall2 <- subsetTo(GDSCsmall, drugs=\"AZD6482\")\n  testCells <- sensitivityProfiles(GDSCsmall2)[order(sensitivityInfo(GDSCsmall2)$sampleid),\"auc_recomputed\", drop=FALSE]\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"median\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary) <- NULL\n  # Complete the test by comparing testSummary with the expected result\n})",
        "complete": "test_that(\"summarizeSensitivityProfiles produces correct values.\", {\n  GDSCsmall2 <- subsetTo(GDSCsmall, drugs=\"AZD6482\")\n  testCells <- sensitivityProfiles(GDSCsmall2)[order(sensitivityInfo(GDSCsmall2)$sampleid),\"auc_recomputed\", drop=FALSE]\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"median\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary) <- NULL\n  expect_equal(testSummary, sapply(seq(1, nrow(testCells), 2), function(i) median(testCells[i:(i+1), 1])))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/datasets.R",
    "language": "R",
    "content": "  #' Connectivity Map Example PharmacoSet\n#'\n#' A small example version of the Connectivity Map PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the Connectivity Map\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name CMAPsmall\n#' @usage data(CMAPsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL\n\n#' Genomics of Drug Sensitivity in Cancer Example PharmacoSet\n#'\n#' A small example version of the Genomics of Drug Sensitivity in Cancer Project\n#' PharmacoSet, used in the documentation examples. All credit for the data goes\n#' to the Genomics of Drug Sensitivity in Cancer Project group at the Sanger.This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Garnett et al. Systematic identification of genomic markers of drug sensitivity in cancer cells. Nature, 2012.\n#' \n#' @docType data\n#' @name GDSCsmall\n#' @usage data(GDSCsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL\n\n#' Cancer Cell Line Encyclopedia (CCLE) Example PharmacoSet\n#'\n#' A small example version of the CCLE PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the CCLE\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Barretina et al. The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity. Nature, 2012\n#' \n#' @docType data\n#' @name CCLEsmall\n#' @usage data(CCLEsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL\n\n#' HDAC Gene Signature\n#'\n#' A gene signature for HDAC inhibitors, as detailed by Glaser et al. The \n#' signature is mapped from the probe to gene level using\n#' \\code{probeGeneMapping}\n#' \n#' @references\n#' Glaser et al. Gene expression profiling of multiple histone deacetylase (HDAC) inhibitors: defining a common gene set produced by HDAC inhibition in T24 and MDA carcinoma cell lines. Molecular cancer therapeutics, 2003.\n#' \n#' @docType data\n#' @name HDAC_genes\n#' @usage data(HDAC_genes)\n#' @keywords datasets\n#' @format a 13x2 data.frame with gene identifiers in the first column and\n#'   direction change in the second\n#'\nNULL",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'NULL' statement at the end of each data object definition in this code snippet?",
        "answer": "The 'NULL' statement at the end of each data object definition is used as a placeholder. In R, when defining data objects using roxygen2 documentation, the 'NULL' statement is often used to indicate that no actual R code is being executed. It allows the documentation to be associated with the data object without defining any functionality."
      },
      {
        "question": "How are the dataset references formatted in the code, and what information do they provide?",
        "answer": "The dataset references are formatted using the '@references' roxygen2 tag. They provide citation information for the original source of the data, including the authors, title of the publication, journal name, and publication year. This gives proper credit to the original researchers and allows users to find more information about the dataset's origin and methodology."
      },
      {
        "question": "What is the significance of the '@format' tag in the documentation for these datasets?",
        "answer": "The '@format' tag in the documentation specifies the structure or type of the dataset. For the CMAPsmall, GDSCsmall, and CCLEsmall datasets, it indicates that they are 'PharmacoSet object' types. For the HDAC_genes dataset, it describes the format as 'a 13x2 data.frame with gene identifiers in the first column and direction change in the second'. This information helps users understand the structure and content of the data they are working with."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Connectivity Map Example PharmacoSet\n#'\n#' A small example version of the Connectivity Map PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the Connectivity Map\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name CMAPsmall\n#' @usage data(CMAPsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\n",
        "complete": "#' Connectivity Map Example PharmacoSet\n#'\n#' A small example version of the Connectivity Map PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the Connectivity Map\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name CMAPsmall\n#' @usage data(CMAPsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL"
      },
      {
        "partial": "#' HDAC Gene Signature\n#'\n#' A gene signature for HDAC inhibitors, as detailed by Glaser et al. The \n#' signature is mapped from the probe to gene level using\n#' \\code{probeGeneMapping}\n#' \n#' @references\n#' Glaser et al. Gene expression profiling of multiple histone deacetylase (HDAC) inhibitors: defining a common gene set produced by HDAC inhibition in T24 and MDA carcinoma cell lines. Molecular cancer therapeutics, 2003.\n#' \n#' @docType data\n#' @name HDAC_genes\n#' @usage data(HDAC_genes)\n#' @keywords datasets\n#' @format a 13x2 data.frame with gene identifiers in the first column and\n#'   direction change in the second\n#'",
        "complete": "#' HDAC Gene Signature\n#'\n#' A gene signature for HDAC inhibitors, as detailed by Glaser et al. The \n#' signature is mapped from the probe to gene level using\n#' \\code{probeGeneMapping}\n#' \n#' @references\n#' Glaser et al. Gene expression profiling of multiple histone deacetylase (HDAC) inhibitors: defining a common gene set produced by HDAC inhibition in T24 and MDA carcinoma cell lines. Molecular cancer therapeutics, 2003.\n#' \n#' @docType data\n#' @name HDAC_genes\n#' @usage data(HDAC_genes)\n#' @keywords datasets\n#' @format a 13x2 data.frame with gene identifiers in the first column and\n#'   direction change in the second\n#'\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/globals.R",
    "language": "R",
    "content": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `utils::globalVariables()` function in this code snippet, and why might it be necessary?",
        "answer": "The `utils::globalVariables()` function is used to declare global variables that are used in ggplot2 or dplyr operations. This is necessary to suppress R CMD check notes about undefined global variables, which can occur when using non-standard evaluation in these packages. It informs R that these variables will be defined later or are part of the data being manipulated."
      },
      {
        "question": "How many and what type of variables are being declared as global in this code?",
        "answer": "The code declares 11 global variables. They are all character strings representing column names or identifiers used in data manipulation or plotting. The variables include: 'X', 'Y', 'Cutoff', 'rn', 'treatmentid', 'sampleid', 'rowKey', 'colKey', 'drug_cell_rep', 'value', and 'max.conc'."
      },
      {
        "question": "Why is 'rn' included twice in the list of global variables, and what potential issue could this cause?",
        "answer": "The variable 'rn' is included twice in the list of global variables, which is likely an oversight or typo. This redundancy doesn't cause any functional issues as R will simply ignore the duplicate declaration. However, it's generally good practice to remove such duplicates to maintain clean and efficient code. It might also indicate that the code hasn't been thoroughly reviewed or cleaned up."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\",\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))",
        "complete": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))"
      },
      {
        "partial": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc'))",
        "complete": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_CancerTargetDiscovery.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n  \n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result4))\n  checkmate::expect_data_table(result4, min.cols = 1, min.rows = 1)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CTD` function in this test suite, and how is it being tested?",
        "answer": "The `mapCompound2CTD` function appears to map compound names to their corresponding entries in the Comparative Toxicogenomics Database (CTD). The test suite checks various aspects of this function:\n1. It tests the basic functionality with a single compound.\n2. It verifies different output formats (default, query_only, and raw).\n3. It checks the function's behavior with an invalid compound name.\nThe tests ensure that the function returns the expected data structure, contains specific fields like 'displayName' and 'PUBCHEM', and handles different input scenarios correctly."
      },
      {
        "question": "How does the test suite use the `checkmate` package, and what is its purpose in this context?",
        "answer": "The `checkmate` package is used in this test suite for assertion-based testing. It provides functions to check the structure and properties of objects returned by `mapCompound2CTD`. Specifically:\n1. `assert_list` is used to ensure the result is a list with at least one element.\n2. `assert_class` checks if an object is of a specific class (e.g., 'httr2_request').\n3. `expect_data_table` verifies that the result is a data table with minimum column and row requirements.\nThese assertions help ensure that the function returns data in the expected format and structure, enhancing the robustness of the tests."
      },
      {
        "question": "What are the different output formats tested for the `mapCompound2CTD` function, and how do they differ?",
        "answer": "The test suite checks three different output formats for the `mapCompound2CTD` function:\n1. Default format (no additional parameters): Returns a result with 'displayName' and 'PUBCHEM' fields.\n2. Query-only format (`query_only = TRUE`): Returns a list of 'httr2_request' objects, representing the API queries without executing them.\n3. Raw format (`raw = TRUE`): Returns a list of raw response data, including 'class' and 'displayName' fields.\nThese different formats allow users to choose between processed results, pre-execution queries, or raw API responses, depending on their specific needs for data analysis or further processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n  \n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  # Complete the assertions for result4\n})",
        "complete": "test_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n  \n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result4))\n  checkmate::expect_data_table(result4, min.cols = 1, min.rows = 1)\n})"
      },
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  # Add assertions for result\n\n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  # Add assertions for result2\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  # Add assertions for result3\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  # Add assertions for result4\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n\n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result4))\n  checkmate::expect_data_table(result4, min.cols = 1, min.rows = 1)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_computeIC50.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking computeIC50/ICn.\")\n\ntest_that(\"Function complains when given insensible input\",{\n\texpect_error(computeIC50(concentration = c(1, 2, 3),\n\t\tviability = c(50, 60, 70),\n\t\tHill_fit = c(1, 0, 0.1)), \"Please pass in only one\")\n\t# expect_silent(computeIC50(concentration = c(1, 2, 3),\n\t# \t\t# \tviability1 = c(50, 60, 70),\n\t# \tHill_fit2 = c(0.5, 0.2, 1)))\n\n\texpect_error(computeIC50(concentration = c(1, 2, 3, 5), viability = c(50, 60, 70)), \"is not of same length\") #should complain\n\texpect_error(computeIC50(concentration = c(-1, 2, 3),viability = c(50, 60, 70),conc_as_log = FALSE),\"'x_as_log' flag may be set incorrectly\") #should complain\n\t##TO-DO:: Add wanring strings to expect_warning call\n\texpect_error(computeIC50(concentration = c(NA, \"cat\", 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"real numbers\") #should complain\n\texpect_error(computeIC50(concentration = c(1, 2, Inf), viability = c(50, 60, 70)), \"real numbers, NA-values, and/or -Inf\") #should complain\n\texpect_warning(computeIC50(concentration = c(1, 2, 3),\n\t\tviability = c(.50, .60, .70),\n\t\tviability_as_pct = TRUE), \"as_pct\") #should complain\n\texpect_error(computeIC50()) #should complain\n})\n\ntest_that(\"Functions return right values\",{\n\texpect_equal(computeIC50(concentration = seq(-3,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n\texpect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n\texpect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.9,0), conc_as_log=TRUE, viability_as_pct=FALSE), NA_real_)\n\texpect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.5,0), conc_as_log=TRUE, viability_as_pct=FALSE), Inf)\n\texpect_equal(.Hill(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=.7, conc_as_log=TRUE, viability_as_pct=FALSE), c(1,0,0)), .3)\n\texpect_equal(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=0, conc_as_log=TRUE, viability_as_pct=FALSE), -Inf)\n})\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeIC50(concentration = c(1, 2, 3),\n    viability = c(50, 60, 70),\n    Hill_fit = c(1, 0, 0.1)), \"Please pass in only one\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, 3, 5), viability = c(50, 60, 70)), \"is not of same length\")\n  \n  expect_error(computeIC50(concentration = c(-1, 2, 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"'x_as_log' flag may be set incorrectly\")\n  \n  expect_error(computeIC50(concentration = c(NA, \"cat\", 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"real numbers\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, Inf), viability = c(50, 60, 70)), \"real numbers, NA-values, and/or -Inf\")\n  \n  expect_warning(computeIC50(concentration = c(1, 2, 3),\n    viability = c(.50, .60, .70),\n    viability_as_pct = TRUE), \"as_pct\")\n  \n  expect_error(computeIC50())\n})",
        "complete": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeIC50(concentration = c(1, 2, 3),\n    viability = c(50, 60, 70),\n    Hill_fit = c(1, 0, 0.1)), \"Please pass in only one\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, 3, 5), viability = c(50, 60, 70)), \"is not of same length\")\n  \n  expect_error(computeIC50(concentration = c(-1, 2, 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"'x_as_log' flag may be set incorrectly\")\n  \n  expect_error(computeIC50(concentration = c(NA, \"cat\", 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"real numbers\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, Inf), viability = c(50, 60, 70)), \"real numbers, NA-values, and/or -Inf\")\n  \n  expect_warning(computeIC50(concentration = c(1, 2, 3),\n    viability = c(.50, .60, .70),\n    viability_as_pct = TRUE), \"as_pct\")\n  \n  expect_error(computeIC50())\n})"
      },
      {
        "partial": "test_that(\"Functions return right values\", {\n  expect_equal(computeIC50(concentration = seq(-3,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.9,0), conc_as_log=TRUE, viability_as_pct=FALSE), NA_real_)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.5,0), conc_as_log=TRUE, viability_as_pct=FALSE), Inf)\n  expect_equal(.Hill(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=.7, conc_as_log=TRUE, viability_as_pct=FALSE), c(1,0,0)), .3)\n  expect_equal(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=0, conc_as_log=TRUE, viability_as_pct=FALSE), -Inf)\n})",
        "complete": "test_that(\"Functions return right values\", {\n  expect_equal(computeIC50(concentration = seq(-3,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.9,0), conc_as_log=TRUE, viability_as_pct=FALSE), NA_real_)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.5,0), conc_as_log=TRUE, viability_as_pct=FALSE), Inf)\n  expect_equal(.Hill(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=.7, conc_as_log=TRUE, viability_as_pct=FALSE), c(1,0,0)), .3)\n  expect_equal(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=0, conc_as_log=TRUE, viability_as_pct=FALSE), -Inf)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_feature_extraction.py",
    "language": "py",
    "content": "from readii.loaders import (\n    loadDicomSITK, \n    loadRTSTRUCTSITK, \n    loadSegmentation,\n) \n\nfrom readii.feature_extraction import (\n    singleRadiomicFeatureExtraction,\n    radiomicFeatureExtraction,\n)\n\nimport pytest\nimport collections\nimport pandas as pd\nimport os \n\n@pytest.fixture\ndef nsclcCTImage():\n    nsclcCTPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n    return loadDicomSITK(nsclcCTPath)\n\n@pytest.fixture\ndef nsclcSEGImage():\n    nsclcSEGPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n    segDictionary = loadSegmentation(nsclcSEGPath, modality = 'SEG')\n    return segDictionary['Heart']\n\n@pytest.fixture\ndef lung4DCTImage():\n    lung4DCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n    return loadDicomSITK(lung4DCTPath)\n\n@pytest.fixture\ndef lung4DRTSTRUCTImage():\n    lung4DRTSTRUCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-47.35/1-1.dcm\"\n    lung4DCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n    segDictionary = loadSegmentation(lung4DRTSTRUCTPath, modality = 'RTSTRUCT',\n                                     baseImageDirPath = lung4DCTPath, roiNames = 'Tumor_c.*')\n    return segDictionary['Tumor_c40']\n\n@pytest.fixture\ndef pyradiomicsParamFilePath():\n    return \"src/readii/data/default_pyradiomics.yaml\"\n\n@pytest.fixture\ndef nsclcMetadataPath():\n    return \"tests/output/ct_to_seg_match_list_NSCLC_Radiogenomics.csv\"\n\n\ndef test_singleRadiomicFeatureExtraction_SEG(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath):\n    \"\"\"Test single image feature extraction with a CT and SEG\"\"\"\n\n    actual = singleRadiomicFeatureExtraction(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict, \\\n        \"Wrong return type, expect a collections.OrderedDict\"\n    assert len(actual) == 1353, \\\n        \"Wrong return size, check pyradiomics parameter file is correct\"\n    assert actual['diagnostics_Configuration_Settings']['label'] == 255, \\\n        \"Wrong label getting passed for ROI\"\n    assert actual['diagnostics_Image-original_Size'] == (26, 21, 20), \\\n        \"Cropped CT image is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == (26, 21, 20), \\\n        \"Cropped segmentation mask is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size'], \\\n        \"Cropped CT and segmentation mask dimensions do not match\"\n    assert actual['original_shape_MeshVolume'].tolist()== pytest.approx(1273.7916666666667), \\\n        \"Volume feature is incorrect\"\n\n\ndef test_singleRadiomicFeatureExtraction_RTSTRUCT(lung4DCTImage, lung4DRTSTRUCTImage, pyradiomicsParamFilePath):\n    \"\"\"Test single image feature extraction with a CT and RTSTRUCT\"\"\"\n\n    actual = singleRadiomicFeatureExtraction(lung4DCTImage, lung4DRTSTRUCTImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict, \\\n        \"Wrong return type, expect a collections.OrderedDict\"\n    assert len(actual) == 1353, \\\n        \"Wrong return size, check pyradiomics parameter file is correct\"\n    assert actual['diagnostics_Configuration_Settings']['label'] == 1, \\\n        \"Wrong label getting passed for ROI\"\n    assert actual['diagnostics_Image-original_Size'] == (51, 92, 28), \\\n        \"Cropped CT image is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == (51, 92, 28), \\\n        \"Cropped segmentation mask is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size'], \\\n        \"Cropped CT and segmentation mask dimensions do not match\"\n    assert actual['original_shape_MeshVolume'].tolist()== pytest.approx(66346.66666666667), \\\n        \"Volume feature is incorrect\"\n\n\ndef test_radiomicFeatureExtraction(nsclcMetadataPath):\n    \"\"\"Test full radiomicFeatureExtraction function with CT and SEG and default PyRadiomics \n       parameter file and no output\"\"\"\n    \n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath=\"tests/\",\n                                       roiNames = None)\n    assert isinstance(actual, pd.DataFrame), \\\n        \"Wrong return type, expect a pandas DataFrame\"\n    assert actual.shape[1] == 1365, \\\n        \"Wrong return size, should include image metadata, diagnostics, and pyradiomics features\"\n    assert actual['diagnostics_Configuration_Settings'][0]['label'] == 255, \\\n        \"Wrong label getting passed for ROI\"\n    assert actual['diagnostics_Image-original_Size'][0] == (26, 21, 20), \\\n        \"Cropped CT image is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'][0] == (26, 21, 20), \\\n        \"Cropped segmentation mask is incorrect size\"\n    assert actual['original_shape_MeshVolume'][0].tolist()== pytest.approx(1273.7916666666667), \\\n        \"Volume feature is incorrect\"\n\n\ndef test_radiomicFeatureExtraction_output(nsclcMetadataPath):\n    \"\"\"Test output creation from radiomic feature extraction\"\"\"\n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath = \"tests/\",\n                                       roiNames = None,\n                                       outputDirPath = \"tests/output/\")\n    assert os.path.exists(\"tests/output/features/radiomicfeatures_original_NSCLC_Radiogenomics.csv\")",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@pytest.fixture` decorators in this code, and how are they used in the test functions?",
        "answer": "The `@pytest.fixture` decorators are used to define reusable test data or objects. They create functions that can be used across multiple test functions without repeating code. In this snippet, fixtures are used to load and prepare image data (CT scans, segmentations) and file paths. Test functions can then use these fixtures as arguments, allowing pytest to automatically provide the required data for each test."
      },
      {
        "question": "How does the `singleRadiomicFeatureExtraction` function differ in its handling of SEG and RTSTRUCT image types, based on the test functions provided?",
        "answer": "The `singleRadiomicFeatureExtraction` function handles SEG and RTSTRUCT image types differently in terms of the label values and image dimensions:\n1. For SEG images, the label value is 255, while for RTSTRUCT images, it's 1.\n2. The cropped image sizes differ: SEG images are (26, 21, 20), while RTSTRUCT images are (51, 92, 28).\n3. The volume features (MeshVolume) also differ significantly between the two types.\nDespite these differences, both types return an OrderedDict with 1353 features, suggesting the function adapts its processing based on the input image type while maintaining a consistent output structure."
      },
      {
        "question": "What is the purpose of the `radiomicFeatureExtraction` function, and how does it differ from `singleRadiomicFeatureExtraction` in terms of input and output?",
        "answer": "The `radiomicFeatureExtraction` function is designed for batch processing of multiple images, unlike `singleRadiomicFeatureExtraction` which processes a single image-mask pair. Key differences include:\n1. Input: `radiomicFeatureExtraction` takes a metadata file path containing information about multiple images, while `singleRadiomicFeatureExtraction` takes individual image and mask objects.\n2. Output: `radiomicFeatureExtraction` returns a pandas DataFrame containing features for all processed images, whereas `singleRadiomicFeatureExtraction` returns an OrderedDict for a single image.\n3. Functionality: `radiomicFeatureExtraction` can process multiple ROIs, save output to a file, and includes additional metadata in its output.\n4. The `radiomicFeatureExtraction` function is more high-level and likely uses `singleRadiomicFeatureExtraction` internally for each image in the batch."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_singleRadiomicFeatureExtraction_SEG(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath):\n    actual = singleRadiomicFeatureExtraction(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict\n    assert len(actual) == 1353\n    assert actual['diagnostics_Configuration_Settings']['label'] == 255\n    assert actual['diagnostics_Image-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size']\n    # Complete the assertion for the 'original_shape_MeshVolume' feature",
        "complete": "def test_singleRadiomicFeatureExtraction_SEG(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath):\n    actual = singleRadiomicFeatureExtraction(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict\n    assert len(actual) == 1353\n    assert actual['diagnostics_Configuration_Settings']['label'] == 255\n    assert actual['diagnostics_Image-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size']\n    assert actual['original_shape_MeshVolume'].tolist() == pytest.approx(1273.7916666666667)"
      },
      {
        "partial": "def test_radiomicFeatureExtraction(nsclcMetadataPath):\n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath=\"tests/\",\n                                       roiNames = None)\n    assert isinstance(actual, pd.DataFrame)\n    assert actual.shape[1] == 1365\n    assert actual['diagnostics_Configuration_Settings'][0]['label'] == 255\n    assert actual['diagnostics_Image-original_Size'][0] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'][0] == (26, 21, 20)\n    # Complete the assertion for the 'original_shape_MeshVolume' feature",
        "complete": "def test_radiomicFeatureExtraction(nsclcMetadataPath):\n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath=\"tests/\",\n                                       roiNames = None)\n    assert isinstance(actual, pd.DataFrame)\n    assert actual.shape[1] == 1365\n    assert actual['diagnostics_Configuration_Settings'][0]['label'] == 255\n    assert actual['diagnostics_Image-original_Size'][0] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'][0] == (26, 21, 20)\n    assert actual['original_shape_MeshVolume'][0].tolist() == pytest.approx(1273.7916666666667)"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest",
        "collections",
        "pandas",
        "os"
      ],
      "from_imports": [
        "readii.loaders.loadDicomSITK",
        "readii.feature_extraction.singleRadiomicFeatureExtraction"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/__init__.py",
    "language": "py",
    "content": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import *",
    "qa_pairs": [
      {
        "question": "What is the purpose of using asterisks (*) in the import statements of this Python code snippet?",
        "answer": "The asterisks (*) in the import statements are used for wildcard imports. This means that all public names (functions, classes, variables) defined in the specified modules are imported directly into the current namespace. For example, 'from .imageutils import *' imports all public names from the 'imageutils' module. While convenient, this practice is generally discouraged as it can lead to namespace pollution and make it harder to track where specific functions or classes are coming from."
      },
      {
        "question": "What does the dot (.) before each module name in the import statements indicate?",
        "answer": "The dot (.) before each module name in the import statements indicates relative imports. This means that the modules being imported are located in the same package as the current module. For instance, '.imageutils' refers to an 'imageutils.py' file (or an 'imageutils' directory with an __init__.py file) that is in the same directory as the current file. Relative imports are useful for organizing code within a package and make it easier to restructure the package without changing import statements."
      },
      {
        "question": "What potential issues might arise from using wildcard imports (*) as shown in this code snippet, and how could they be addressed?",
        "answer": "Using wildcard imports (*) can lead to several issues:\n1. Namespace pollution: It becomes unclear which names come from which modules, making the code harder to understand and maintain.\n2. Name conflicts: If two modules define the same name, the last import will silently overwrite previous ones.\n3. Reduced readability: It's not immediately clear what specific functions or classes are being used from each module.\n\nTo address these issues:\n1. Use specific imports instead, e.g., 'from .imageutils import specific_function, SpecificClass'.\n2. Use module aliases, e.g., 'import .imageutils as iu' and then use 'iu.function_name()'.\n3. If many names are needed, import the module and use the module name as a prefix, e.g., 'import .imageutils' and use 'imageutils.function_name()'.\n\nThese approaches improve code clarity, reduce the risk of conflicts, and make dependencies more explicit."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .",
        "complete": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import *"
      },
      {
        "partial": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import",
        "complete": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import *"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "imageutils.*",
        "arrayutils.*",
        "crawl.*",
        "dicomutils.*",
        "args.*",
        "nnunet.*",
        "autopipeutils.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_computeABC.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking computeABC.\")\n\ntest_that(\"Function complains when given insensible input\",{\n\t\n\texpect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, 2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tHill_fit1 = c(1, 0, 0.1),\n\t\tHill_fit2 = c(0.5, 0.2, 1)), \"Please pass in only one\")\n\t# expect_silent(computeABC(conc1 = c(1, 2, 3),\n\t# \tconc2 = c(1, 2, 3),\n\t# \tviability1 = c(50, 60, 70),\n\t# \tHill_fit2 = c(0.5, 0.2, 1)))\n\n\texpect_error(computeABC(conc1 = c(1, 2,3),\n\t\tconc2 = c(1, 2, 3, 4),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10)), \"is not of same length\") #should complain\n\texpect_error(computeABC(conc1 = c(-1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tconc_as_log = FALSE)) #should complain\n\t##TO-DO::Add warning string to expect_warning call\n\texpect_error(computeABC(conc1 = c(NA, \"cat\", 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tconc_as_log = FALSE)) #should complain\n\texpect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tverbose = NA)) #should complain\n\texpect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70))) #should complain\n\texpect_error(computeABC(conc1 = c(1, 2, Inf),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10))) #should complain\n\t##TO-DO::Add warning string to expect_warning call\n\texpect_warning(expect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(.50, .60, .70),\n\t\tviability2 = c(.40, .90, .10),\n\t\tviability_as_pct = TRUE))) #should complain\n\texpect_warning(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, 2, 3),\n\t\tviability1 = c(.50, .60, .70),\n\t\tviability2 = c(.40, .90, .10),\n\t\tviability_as_pct = TRUE)) #should complain\n\texpect_error(computeABC()) #should complain\n})\n\ntest_that(\"Function values make sense\",{\n\texpect_equal(computeABC(conc1=c(-1,0,1), conc2=c(-1,0,1), Hill_fit1=c(0,1,0), Hill_fit2=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0.5)\n\texpect_equal(computeABC(conc1=c(-1,0,1), conc2=c(-1,0,1), Hill_fit1=c(0,1,0), Hill_fit2=c(0,1,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'computeABC' function based on the test cases provided?",
        "answer": "The 'computeABC' function appears to be designed to compute some measure (possibly Area Between Curves) using concentration and viability data from two different conditions. It takes inputs such as concentration values, viability values, and Hill fit parameters. The function seems to handle various input validations and edge cases, as evidenced by the numerous error checks in the test cases."
      },
      {
        "question": "What are some of the input validation checks performed by the 'computeABC' function?",
        "answer": "Based on the test cases, the 'computeABC' function performs several input validation checks, including: ensuring that only one set of parameters is passed, verifying that input vectors have the same length, checking for negative concentration values when 'conc_as_log' is FALSE, validating that inputs are numeric (not strings or NA), ensuring that 'verbose' is a logical value, checking for the presence of required parameters, and verifying that concentration values are finite (not Inf)."
      },
      {
        "question": "How does the 'computeABC' function handle viability values, and what potential issue is highlighted in the test cases?",
        "answer": "The 'computeABC' function appears to handle viability values in two ways, depending on the 'viability_as_pct' parameter. When 'viability_as_pct' is TRUE, the function expects viability values to be percentages (e.g., 50, 60, 70). However, the test cases highlight a potential issue where passing decimal values (e.g., 0.50, 0.60, 0.70) with 'viability_as_pct' set to TRUE results in a warning. This suggests that users need to be careful about the format of viability data they provide and ensure it matches the 'viability_as_pct' setting."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    Hill_fit1 = c(1, 0, 0.1),\n    Hill_fit2 = c(0.5, 0.2, 1)), \"Please pass in only one\")\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3, 4),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10)), \"is not of same length\")\n  \n  # Add more expect_error tests here\n})",
        "complete": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    Hill_fit1 = c(1, 0, 0.1),\n    Hill_fit2 = c(0.5, 0.2, 1)), \"Please pass in only one\")\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3, 4),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10)), \"is not of same length\")\n  \n  expect_error(computeABC(conc1 = c(-1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    conc_as_log = FALSE))\n  \n  expect_error(computeABC(conc1 = c(NA, \"cat\", 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    conc_as_log = FALSE))\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    verbose = NA))\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70)))\n  \n  expect_error(computeABC(conc1 = c(1, 2, Inf),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10)))\n  \n  expect_warning(expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(.50, .60, .70),\n    viability2 = c(.40, .90, .10),\n    viability_as_pct = TRUE)))\n  \n  expect_warning(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3),\n    viability1 = c(.50, .60, .70),\n    viability2 = c(.40, .90, .10),\n    viability_as_pct = TRUE))\n  \n  expect_error(computeABC())\n})"
      },
      {
        "partial": "test_that(\"Function values make sense\", {\n  # Test case 1\n  expect_equal(computeABC(conc1 = c(-1, 0, 1),\n                         conc2 = c(-1, 0, 1),\n                         Hill_fit1 = c(0, 1, 0),\n                         Hill_fit2 = c(1, 0, 0),\n                         conc_as_log = TRUE,\n                         viability_as_pct = FALSE),\n             0.5)\n  \n  # Add test case 2 here\n})",
        "complete": "test_that(\"Function values make sense\", {\n  # Test case 1\n  expect_equal(computeABC(conc1 = c(-1, 0, 1),\n                         conc2 = c(-1, 0, 1),\n                         Hill_fit1 = c(0, 1, 0),\n                         Hill_fit2 = c(1, 0, 0),\n                         conc_as_log = TRUE,\n                         viability_as_pct = FALSE),\n             0.5)\n  \n  # Test case 2\n  expect_equal(computeABC(conc1 = c(-1, 0, 1),\n                         conc2 = c(-1, 0, 1),\n                         Hill_fit1 = c(0, 1, 0),\n                         Hill_fit2 = c(0, 1, 0),\n                         conc_as_log = TRUE,\n                         viability_as_pct = FALSE),\n             0)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/writers.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport json\nimport csv\nimport pickle\nimport shutil\nfrom datetime import datetime, timezone\n\nimport h5py\nimport numpy as np\n\nimport SimpleITK as sitk\nimport nrrd\nfrom skimage.measure import regionprops\n\nfrom ..utils import image_to_array\n\n\nclass BaseWriter:\n    def __init__(self, root_directory, filename_format, create_dirs=True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        if create_dirs and not os.path.exists(self.root_directory):\n            os.makedirs(self.root_directory)\n\n    def put(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def _get_path_from_subject_id(self, subject_id, **kwargs):\n        now = datetime.now(timezone.utc)\n        date = now.strftime(\"%Y-%m-%d\")\n        time = now.strftime(\"%H%M%S\")\n        date_time = date + \"_\" + time\n        out_filename = self.filename_format.format(subject_id=subject_id,\n                                                   date=date,\n                                                   time=time,\n                                                   date_time=date_time,\n                                                   **kwargs)\n        out_path = pathlib.Path(self.root_directory, out_filename).as_posix()\n        out_dir = os.path.dirname(out_path)\n        if self.create_dirs and not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)  # create subdirectories if specified in filename_format\n\n        return out_path\n\n\nclass BaseSubjectWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        if os.path.exists(self.root_directory):\n            # delete the folder called {subject_id} that was made in the original BaseWriter / the one named {label_or_image}\n            if os.path.basename(os.path.dirname(self.root_directory)) == \"{subject_id}\":\n                shutil.rmtree(os.path.dirname(self.root_directory))\n            elif \"{label_or_image}{train_or_test}\" in os.path.basename(self.root_directory):\n                shutil.rmtree(self.root_directory)\n\n    def put(self, subject_id, \n            image, is_mask=False, \n            nnunet_info=None, \n            label_or_image: str = \"images\", \n            mask_label: str=\"\", \n            train_or_test: str = \"Tr\", **kwargs):\n        \n        if is_mask:\n            # remove illegal characters for Windows/Unix\n            badboys = '<>:\"/\\|?*'\n            for char in badboys: \n                mask_label = mask_label.replace(char, \"\")\n\n            # filename_format eh\n            self.filename_format = mask_label + \".nii.gz\"  # save the mask labels as their rtstruct names\n\n        if nnunet_info:\n            if label_or_image == \"labels\":\n                filename = f\"{subject_id}.nii.gz\"  # naming convention for labels\n            else:\n                filename = self.filename_format.format(subject_id=subject_id, modality_index=nnunet_info['modalities'][nnunet_info['current_modality']])  # naming convention for images\n            out_path = self._get_path_from_subject_id(filename, label_or_image=label_or_image, train_or_test=train_or_test)\n        else:\n            out_path = self._get_path_from_subject_id(self.filename_format, subject_id=subject_id)\n        sitk.WriteImage(image, out_path, self.compress)\n\n    def _get_path_from_subject_id(self, filename, **kwargs):\n        root_directory = self.root_directory.format(**kwargs)  # replace the {} with the kwargs passed in from .put() (above)\n        out_path = pathlib.Path(root_directory, filename).as_posix()\n        out_dir = os.path.dirname(out_path)\n        if self.create_dirs and not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)  # create subdirectories if specified in filename_format\n        return out_path\n\n\nclass ImageFileWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.compress = compress\n\n    def put(self, subject_id, image, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        sitk.WriteImage(image, out_path, self.compress)\n\n        \nclass SegNrrdWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.seg.nrrd\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        if compress:\n            self.compression_level = 9\n        else:\n            self.compression_level = 1\n\n    def put(self, subject_id, mask, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        labels = [k for k in mask.roi_names]\n        print(labels)\n\n        origin = mask.GetOrigin()\n        spacing = mask.GetSpacing()\n        #direction = mask.GetDirection()\n\n        space = \"left-posterior-superior\"  # everything is ITK read/write \n\n        # fix reverted somewhere.... :''''(\n        space_directions = [[spacing[0], 0., 0.],\n                            [0., spacing[1], 0.],\n                            [0., 0., spacing[2]]]\n        kinds = ['domain', 'domain', 'domain']\n        dims = 3\n\n        # permute axes to original orientations\n        if len(labels) > 1: \n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3, -4])\n\n            # add extra dimension to metadata\n            space_directions.insert(0, [float('nan'), float('nan'), float('nan')])\n            kinds.insert(0, 'vector')\n            dims += 1 \n        else:\n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3])\n        \n        # ensure proper conversion to array\n        assert mask.GetSize() == arr.shape[-3:]\n\n        segment_info = {}\n        for n, i in enumerate(labels):\n            try:\n                if len(labels) > 1:\n                    props = regionprops(arr[n])[0]\n                else:\n                    props = regionprops(arr)[0]\n                bbox = props[\"bbox\"]\n                bbox_segment = [bbox[0], bbox[3], bbox[1], bbox[4], bbox[2], bbox[5]]\n            except IndexError:  # mask is empty\n                assert arr[n].sum() == 0, \"Mask not empty but 'skimage.measure.regionprops' failed.\"\n                bbox_segment = [0, 0, 0, 0, 0, 0]\n\n            segment_info[f\"Segment{n}_Color\"] = list(np.random.random(3))\n            segment_info[f\"Segment{n}_ColorAutoGenerated\"] = '1'\n            segment_info[f\"Segment{n}_Extent\"] = bbox_segment\n            segment_info[f\"Segment{n}_ID\"] = str(n)\n            segment_info[f\"Segment{n}_Name\"] = i\n            segment_info[f\"Segment{n}_NameautoGenerated\"] = '0'\n        \n        header = {'dimension': dims,\n                  'space': space,\n                  'sizes': mask.GetSize(),\n                  'space directions': space_directions,\n                  'kinds': kinds,\n                  'endian': 'little',\n                  'space origin': origin,\n                  'roi_names': labels,\n                  **segment_info}\n        \n        nrrd.write(out_path, arr, header=header, compression_level=self.compression_level, **kwargs)\n\n\nclass NumpyWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.npy\", create_dirs=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n\n    def put(self, subject_id, image, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        if isinstance(image, sitk.Image):\n            array, *_ = image_to_array(image)  # TODO (Michal) optionally save the image geometry\n        np.save(out_path, array)\n\n\nclass HDF5Writer(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.h5\", create_dirs=True, save_geometry=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.save_geometry = save_geometry\n\n    def put(self, subject_id, images, metadata=None, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        with h5py.File(out_path, \"w\") as f:\n            if not isinstance(images, dict):\n                images = {\"image\": images}\n            for k, v in images.items():\n                array, origin, direction, spacing = image_to_array(v)\n                dataset = f.create_dataset(k, data=array)\n                dataset.attrs.create(\"subject_id\", subject_id)\n                if self.save_geometry:\n                    dataset.attrs.create(\"origin\", data=origin)\n                    dataset.attrs.create(\"direction\", data=direction)\n                    dataset.attrs.create(\"spacing\", data=spacing)\n            if metadata:\n                for k, attrs in metadata.items():\n                    for name, v in attrs:\n                        f[subject_id].attrs.create(name, data=v)\n\n\nclass MetadataWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.json\", create_dirs=True, remove_existing=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.file_format = os.path.splitext(filename_format)[1].lstrip(\".\")\n        self.remove_existing = remove_existing\n        if self.file_format not in [\"json\", \"csv\", \"pkl\"]:\n            raise ValueError(f\"File format {self.file_format} not supported. Supported formats: JSON (.json), CSV (.csv), Pickle (.pkl).\")\n\n        if self.file_format == \"csv\" and self.remove_existing:\n            out_path = pathlib.Path(self.root_directory, self.filename_format).as_posix()\n            if os.path.exists(out_path):\n                os.remove(out_path) # remove existing CSV instead of appending\n\n    def _put_json(self, out_path, **kwargs):\n        with open(out_path, \"w\") as f:\n            json.dump(kwargs, f)\n\n    def _put_csv(self, out_path, **kwargs):\n        with open(out_path, \"a+\") as f:\n            writer = csv.DictWriter(f, fieldnames=kwargs.keys())\n            pos = f.tell()\n            f.seek(0)\n            sample = \"\\n\".join([f.readline() for _ in range(2)])\n            if sample == \"\\n\" or not csv.Sniffer().has_header(sample):\n                writer.writeheader()\n            f.seek(pos)\n            writer.writerow(kwargs)\n\n    def _put_pickle(self, out_path, **kwargs):\n        with open(out_path, \"wb\") as f:\n            pickle.dump(kwargs, f)\n\n    def put(self, subject_id, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id)\n\n        if \"subject_id\" not in kwargs:\n            kwargs[\"subject_id\"] = subject_id\n\n        if self.file_format == \"json\":\n            self._put_json(out_path, **kwargs)\n        elif self.file_format == \"csv\":\n            self._put_csv(out_path, **kwargs)\n        elif self.file_format == \"pkl\":\n            self._put_pickle(out_path, **kwargs)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the BaseWriter class and how does it handle directory creation?",
        "answer": "The BaseWriter class is a base class for writing data to files. It initializes with a root directory and filename format. If create_dirs is True (default), it creates the root directory if it doesn't exist. The _get_path_from_subject_id method generates a full file path based on the subject ID and current date/time, creating any necessary subdirectories."
      },
      {
        "question": "How does the BaseSubjectWriter class differ from the BaseWriter class, and what specific functionality does it add?",
        "answer": "The BaseSubjectWriter class extends BaseWriter and adds functionality specific to writing subject data. It includes options for compression and handles special cases for mask labels. It also implements a put method that writes image data using SimpleITK, with support for different naming conventions based on whether it's writing labels or images for nnUNet."
      },
      {
        "question": "What is the purpose of the SegNrrdWriter class and how does it handle the writing of segmentation data?",
        "answer": "The SegNrrdWriter class is designed to write segmentation data in the NRRD format. It handles multiple labels, creates a header with segment information including color, extent, and name for each segment. It also manages the conversion of SimpleITK image data to numpy arrays, handles axis permutation, and writes the data using the nrrd library with specified compression levels."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class BaseSubjectWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        if os.path.exists(self.root_directory):\n            # TODO: Implement directory cleanup logic\n            pass\n\n    def put(self, subject_id, image, is_mask=False, nnunet_info=None, label_or_image: str = \"images\", mask_label: str=\"\", train_or_test: str = \"Tr\", **kwargs):\n        # TODO: Implement put method logic\n        pass\n\n    def _get_path_from_subject_id(self, filename, **kwargs):\n        # TODO: Implement path generation logic\n        pass",
        "complete": "class BaseSubjectWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        if os.path.exists(self.root_directory):\n            if os.path.basename(os.path.dirname(self.root_directory)) == \"{subject_id}\":\n                shutil.rmtree(os.path.dirname(self.root_directory))\n            elif \"{label_or_image}{train_or_test}\" in os.path.basename(self.root_directory):\n                shutil.rmtree(self.root_directory)\n\n    def put(self, subject_id, image, is_mask=False, nnunet_info=None, label_or_image: str = \"images\", mask_label: str=\"\", train_or_test: str = \"Tr\", **kwargs):\n        if is_mask:\n            mask_label = ''.join(c for c in mask_label if c not in '<>:\"/\\|?*')\n            self.filename_format = mask_label + \".nii.gz\"\n        if nnunet_info:\n            filename = f\"{subject_id}.nii.gz\" if label_or_image == \"labels\" else self.filename_format.format(subject_id=subject_id, modality_index=nnunet_info['modalities'][nnunet_info['current_modality']])\n            out_path = self._get_path_from_subject_id(filename, label_or_image=label_or_image, train_or_test=train_or_test)\n        else:\n            out_path = self._get_path_from_subject_id(self.filename_format, subject_id=subject_id)\n        sitk.WriteImage(image, out_path, self.compress)\n\n    def _get_path_from_subject_id(self, filename, **kwargs):\n        root_directory = self.root_directory.format(**kwargs)\n        out_path = pathlib.Path(root_directory, filename).as_posix()\n        out_dir = os.path.dirname(out_path)\n        if self.create_dirs and not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        return out_path"
      },
      {
        "partial": "class SegNrrdWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.seg.nrrd\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.compression_level = 9 if compress else 1\n\n    def put(self, subject_id, mask, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        labels = [k for k in mask.roi_names]\n        print(labels)\n\n        # TODO: Implement the rest of the put method\n        pass",
        "complete": "class SegNrrdWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.seg.nrrd\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.compression_level = 9 if compress else 1\n\n    def put(self, subject_id, mask, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        labels = [k for k in mask.roi_names]\n        print(labels)\n\n        origin = mask.GetOrigin()\n        spacing = mask.GetSpacing()\n        space = \"left-posterior-superior\"\n        space_directions = [[spacing[0], 0., 0.], [0., spacing[1], 0.], [0., 0., spacing[2]]]\n        kinds = ['domain', 'domain', 'domain']\n        dims = 3\n\n        if len(labels) > 1:\n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3, -4])\n            space_directions.insert(0, [float('nan'), float('nan'), float('nan')])\n            kinds.insert(0, 'vector')\n            dims += 1\n        else:\n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3])\n\n        assert mask.GetSize() == arr.shape[-3:]\n\n        segment_info = {}\n        for n, i in enumerate(labels):\n            try:\n                props = regionprops(arr[n] if len(labels) > 1 else arr)[0]\n                bbox = props[\"bbox\"]\n                bbox_segment = [bbox[0], bbox[3], bbox[1], bbox[4], bbox[2], bbox[5]]\n            except IndexError:\n                assert arr[n].sum() == 0 if len(labels) > 1 else arr.sum() == 0, \"Mask not empty but 'skimage.measure.regionprops' failed.\"\n                bbox_segment = [0, 0, 0, 0, 0, 0]\n\n            segment_info[f\"Segment{n}_Color\"] = list(np.random.random(3))\n            segment_info[f\"Segment{n}_ColorAutoGenerated\"] = '1'\n            segment_info[f\"Segment{n}_Extent\"] = bbox_segment\n            segment_info[f\"Segment{n}_ID\"] = str(n)\n            segment_info[f\"Segment{n}_Name\"] = i\n            segment_info[f\"Segment{n}_NameautoGenerated\"] = '0'\n\n        header = {\n            'dimension': dims,\n            'space': space,\n            'sizes': mask.GetSize(),\n            'space directions': space_directions,\n            'kinds': kinds,\n            'endian': 'little',\n            'space origin': origin,\n            'roi_names': labels,\n            **segment_info\n        }\n\n        nrrd.write(out_path, arr, header=header, compression_level=self.compression_level, **kwargs)"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "json",
        "csv",
        "pickle",
        "shutil",
        "h5py",
        "numpy",
        "SimpleITK",
        "nrrd"
      ],
      "from_imports": [
        "datetime.datetime",
        "skimage.measure.regionprops",
        "utils.image_to_array"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/transforms/intensity.py",
    "language": "py",
    "content": "def clip(image, lower, upper):\n    return NotImplementedError\n\n\ndef window(image, window, level):\n    return NotImplementedError\n\n\ndef mean(image, mask=None, labels=None):\n    if mask is not None:\n        pass\n    return NotImplementedError\n\n\ndef var(image, mask=None, labels=None):\n    if mask is not None:\n        pass\n    return NotImplementedError\n\n\ndef standard_scale(image, dataset_mean=0., dataset_var=1.):\n    return (image - dataset_mean) / dataset_var\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'clip' function in the given code snippet, and what parameters does it take?",
        "answer": "The 'clip' function is intended to limit the values in an image to a specified range. It takes three parameters: 'image' (the input image), 'lower' (the lower bound of the range), and 'upper' (the upper bound of the range). However, the function is not implemented in the given code snippet and returns NotImplementedError."
      },
      {
        "question": "How does the 'standard_scale' function work, and what is its purpose in image processing?",
        "answer": "The 'standard_scale' function normalizes an image by subtracting a mean value and dividing by a standard deviation. It takes three parameters: 'image' (the input image), 'dataset_mean' (default 0), and 'dataset_var' (default 1). The function subtracts the dataset mean from each pixel value and then divides by the square root of the dataset variance. This process standardizes the image data, which can be useful for machine learning algorithms or for comparing images with different intensity ranges."
      },
      {
        "question": "What is the significance of the 'mask' and 'labels' parameters in the 'mean' and 'var' functions, and how might they be used if implemented?",
        "answer": "The 'mask' and 'labels' parameters in the 'mean' and 'var' functions suggest that these functions are designed to calculate mean and variance for specific regions or objects within an image. If implemented, the 'mask' parameter could be used to specify which pixels to include in the calculation, while 'labels' could be used to calculate statistics for different labeled regions separately. This functionality would be useful for analyzing specific parts of an image or for segmentation tasks. However, in the given code snippet, these functions are not fully implemented and return NotImplementedError."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def clip(image, lower, upper):\n    return np.clip(image, lower, upper)\n\ndef window(image, window, level):\n    return NotImplementedError\n\ndef mean(image, mask=None, labels=None):\n    if mask is not None:\n        return np.mean(image[mask])\n    return np.mean(image)",
        "complete": "def clip(image, lower, upper):\n    return np.clip(image, lower, upper)\n\ndef window(image, window, level):\n    lower = level - window / 2\n    upper = level + window / 2\n    return clip(image, lower, upper)\n\ndef mean(image, mask=None, labels=None):\n    if mask is not None:\n        return np.mean(image[mask])\n    return np.mean(image)"
      },
      {
        "partial": "def var(image, mask=None, labels=None):\n    if mask is not None:\n        return np.var(image[mask])\n    return np.var(image)\n\ndef standard_scale(image, dataset_mean=0., dataset_var=1.):\n    return NotImplementedError",
        "complete": "def var(image, mask=None, labels=None):\n    if mask is not None:\n        return np.var(image[mask])\n    return np.var(image)\n\ndef standard_scale(image, dataset_mean=0., dataset_var=1.):\n    return (image - dataset_mean) / np.sqrt(dataset_var)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_components.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport urllib.request as request\nfrom zipfile import ZipFile\n\nimport pytest\nimport SimpleITK as sitk\nimport pandas as pd\nfrom imgtools.autopipeline import AutoPipeline\nimport ast\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    output_path = pathlib.Path(curr_path, 'tests', 'temp').as_posix()\n    quebec_path = quebec_path.as_posix()\n    \n    #Dataset name\n    dataset_name  = os.path.basename(quebec_path)\n    imgtools_path = pathlib.Path(os.path.dirname(quebec_path), '.imgtools')\n\n    #Defining paths for autopipeline and dataset component\n    crawl_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.csv\").as_posix()\n    edge_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}_edges.csv\").as_posix()\n    # json_path =  pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.json\").as_posix()  # noqa: F841\n    \n    yield quebec_path, output_path, crawl_path, edge_path\n\n\n\n\n# @pytest.mark.parametrize(\"modalities\",[\"PT\", \"CT,RTSTRUCT\", \"CT,RTDOSE\", \"CT,PT,RTDOSE\", \"CT,RTSTRUCT,RTDOSE\", \"CT,RTSTRUCT,RTDOSE,PT\"])\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])#, \"CT,RTDOSE,PT\"])\nclass TestComponents:\n    \"\"\"\n    For testing the autopipeline components of the med-imagetools package\n    It has two methods:\n    test_pipeline:\n        1) Checks if there is any crawler and edge table output generated by autopipeline\n        2) Checks if for the test data, the lengths of the crawler and edge table matches the actual length of what should be ideally created\n        3) Checks if the length of component table(dataset.csv) is correct or not\n        4) Checks for every component, the shape of all different modalities matches or not\n\n    \"\"\"\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n    \n    def test_pipeline(self, modalities):\n        \"\"\"\n        Testing the Autopipeline for processing the DICOMS and saving it as nrrds\n        \"\"\"\n        n_jobs = 2\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        #Initialize pipeline for the current setting\n        pipeline = AutoPipeline(self.input_path, output_path_mod, modalities, n_jobs=n_jobs, spacing=(5,5,5), overwrite=True)\n        #Run for different modalities\n        comp_path = pathlib.Path(output_path_mod, \"dataset.csv\").as_posix()\n        pipeline.run()\n\n        #Check if the crawl and edges exist\n        assert os.path.exists(self.crawl_path) & os.path.exists(self.edge_path), \"There was no crawler output\"\n\n        #for the test example, there are 6 files and 4 connections\n        crawl_data = pd.read_csv(self.crawl_path, index_col=0)\n        edge_data = pd.read_csv(self.edge_path)\n        # this assert will fail....\n        assert (len(crawl_data) == 12) & (len(edge_data) == 10), \"There was an error in crawling or while making the edge table\"\n\n        #Check if the dataset.csv is having the correct number of components and has all the fields\n        comp_table = pd.read_csv(comp_path, index_col=0)\n        assert len(comp_table) == 2, \"There was some error in making components, check datagraph.parser\"\n\n        #Check the nrrd files\n        subject_id_list = list(comp_table.index)\n        output_streams = [(\"_\").join(cols.split(\"_\")[2:]) for cols in comp_table.columns if cols.split(\"_\")[0] == \"output\"]\n        for subject_id in subject_id_list:\n            shapes = []\n            for col in output_streams:\n                if 'RTSTRUCT' in col:\n                    filename = ast.literal_eval(comp_table.loc[subject_id]['metadata_RTSTRUCT_CT'])[0][0]\n                else:\n                    filename = col\n                \n                print(subject_id, col, filename)\n                path_mod = pathlib.Path(output_path_mod, subject_id, col, f\"{filename}.nii.gz\").as_posix()\n                # All modalities except RTSTRUCT should be of type torchIO.ScalarImage\n                temp_dicom = sitk.GetArrayFromImage(sitk.ReadImage(path_mod))\n                shapes.append(temp_dicom.shape)\n            A = [item == shapes[0] for item in shapes]\n            print(shapes)\n            assert all(A)\n    ",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dataset_path` fixture in this code, and how does it handle the dataset download?",
        "answer": "The `dataset_path` fixture is responsible for setting up the test dataset. It checks if the dataset already exists, and if not, it downloads and extracts it. The fixture performs the following steps:\n1. Determines the path for the dataset\n2. If the dataset doesn't exist, it creates the directory, downloads the zip file, extracts it, and removes the zip file\n3. If the dataset already exists, it skips the download\n4. Sets up paths for the autopipeline and dataset components\n5. Yields the necessary paths for testing"
      },
      {
        "question": "How does the `test_pipeline` method validate the output of the AutoPipeline process?",
        "answer": "The `test_pipeline` method validates the AutoPipeline output through several checks:\n1. It verifies the existence of crawler and edge table outputs\n2. It checks if the lengths of the crawler and edge table match the expected values (12 and 10 respectively)\n3. It ensures the dataset.csv file has the correct number of components (2 in this case)\n4. It checks that all modalities for each subject have the same shape by loading the NIFTI files and comparing their dimensions"
      },
      {
        "question": "What is the purpose of the `@pytest.mark.parametrize` decorator in this code, and how is it being used?",
        "answer": "The `@pytest.mark.parametrize` decorator is used to run the test method with different input parameters. In this code:\n1. It's applied to the `TestComponents` class\n2. It specifies different combinations of modalities to test: 'CT', 'CT,RTSTRUCT', and 'CT,RTSTRUCT,RTDOSE'\n3. This allows the `test_pipeline` method to be executed multiple times, once for each specified modality combination\n4. It helps in testing the AutoPipeline's behavior with various input modality configurations without duplicating test code"
      }
    ],
    "completion_tasks": [
      {
        "partial": "import os\nimport pathlib\nimport urllib.request as request\nfrom zipfile import ZipFile\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    \n    # Complete the function by adding the missing code here\n",
        "complete": "import os\nimport pathlib\nimport urllib.request as request\nfrom zipfile import ZipFile\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    output_path = pathlib.Path(curr_path, 'tests', 'temp').as_posix()\n    quebec_path = quebec_path.as_posix()\n    \n    dataset_name = os.path.basename(quebec_path)\n    imgtools_path = pathlib.Path(os.path.dirname(quebec_path), '.imgtools')\n\n    crawl_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.csv\").as_posix()\n    edge_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}_edges.csv\").as_posix()\n    \n    yield quebec_path, output_path, crawl_path, edge_path"
      },
      {
        "partial": "import pytest\nimport SimpleITK as sitk\nimport pandas as pd\nfrom imgtools.autopipeline import AutoPipeline\nimport ast\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])\nclass TestComponents:\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n    \n    def test_pipeline(self, modalities):\n        n_jobs = 2\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        pipeline = AutoPipeline(self.input_path, output_path_mod, modalities, n_jobs=n_jobs, spacing=(5,5,5), overwrite=True)\n        comp_path = pathlib.Path(output_path_mod, \"dataset.csv\").as_posix()\n        pipeline.run()\n\n        # Complete the test_pipeline method by adding the missing code here\n",
        "complete": "import pytest\nimport SimpleITK as sitk\nimport pandas as pd\nfrom imgtools.autopipeline import AutoPipeline\nimport ast\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])\nclass TestComponents:\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n    \n    def test_pipeline(self, modalities):\n        n_jobs = 2\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        pipeline = AutoPipeline(self.input_path, output_path_mod, modalities, n_jobs=n_jobs, spacing=(5,5,5), overwrite=True)\n        comp_path = pathlib.Path(output_path_mod, \"dataset.csv\").as_posix()\n        pipeline.run()\n\n        assert os.path.exists(self.crawl_path) & os.path.exists(self.edge_path), \"There was no crawler output\"\n\n        crawl_data = pd.read_csv(self.crawl_path, index_col=0)\n        edge_data = pd.read_csv(self.edge_path)\n        assert (len(crawl_data) == 12) & (len(edge_data) == 10), \"There was an error in crawling or while making the edge table\"\n\n        comp_table = pd.read_csv(comp_path, index_col=0)\n        assert len(comp_table) == 2, \"There was some error in making components, check datagraph.parser\"\n\n        subject_id_list = list(comp_table.index)\n        output_streams = [\"_\".join(cols.split(\"_\")[2:]) for cols in comp_table.columns if cols.split(\"_\")[0] == \"output\"]\n        for subject_id in subject_id_list:\n            shapes = []\n            for col in output_streams:\n                if 'RTSTRUCT' in col:\n                    filename = ast.literal_eval(comp_table.loc[subject_id]['metadata_RTSTRUCT_CT'])[0][0]\n                else:\n                    filename = col\n                \n                print(subject_id, col, filename)\n                path_mod = pathlib.Path(output_path_mod, subject_id, col, f\"{filename}.nii.gz\").as_posix()\n                temp_dicom = sitk.GetArrayFromImage(sitk.ReadImage(path_mod))\n                shapes.append(temp_dicom.shape)\n            A = [item == shapes[0] for item in shapes]\n            print(shapes)\n            assert all(A)"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "urllib.request",
        "pytest",
        "SimpleITK",
        "pandas",
        "ast"
      ],
      "from_imports": [
        "zipfile.ZipFile",
        "imgtools.autopipeline.AutoPipeline"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_PharmacoSet_utils.R",
    "language": "R",
    "content": "library(PharmacoGx)\nlibrary(testthat)\ndata(CCLEsmall)\n\n# --\ncontext(\"Testing PharmacoSet subset methods...\")\n\ntest_that('subsetByTreatment works...', {\n    expect_true({\n        treatments <- treatmentNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetByTreatment(CCLEsmall, treatments)\n        })\n        all(treatmentNames(CCLE_sub) %in% treatments)\n    })\n})\n\ntest_that('subsetBySample works...', {\n    expect_true({\n        samples <- sampleNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetBySample(CCLEsmall, samples)\n        })\n        all(sampleNames(CCLE_sub) %in% samples)\n    })\n})\n\ntest_that('subsetByFeature works...', {\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'subsetByTreatment' function in this code, and how is it being tested?",
        "answer": "The 'subsetByTreatment' function is used to create a subset of a PharmacoSet object based on specified treatments. In the test, it's being used to create a subset of the CCLEsmall dataset using the first 5 treatment names. The test checks if all treatment names in the resulting subset are among the specified treatments, verifying that the function correctly filters the dataset."
      },
      {
        "question": "How does the code handle potential warning messages during the subset operations?",
        "answer": "The code uses the 'suppressMessages' function to prevent any messages or warnings from being displayed during the subset operations. This is applied to both 'subsetByTreatment' and 'subsetBySample' function calls, ensuring that the test output remains clean and focused on the actual test results."
      },
      {
        "question": "What is the significance of using 'expect_true' in the test cases, and what condition is being checked?",
        "answer": "The 'expect_true' function is used to assert that a condition is true. In these test cases, it's checking if all elements in the subset (either treatments or samples) are present in the original selection. This verifies that the subsetting functions are correctly filtering the data without including any unintended elements. If the condition is true, the test passes; otherwise, it fails."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that('subsetByFeature works...', {\n    expect_true({\n        features <- featureNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetByFeature(CCLEsmall, features)\n        })\n        # Complete the condition here\n    })\n})",
        "complete": "test_that('subsetByFeature works...', {\n    expect_true({\n        features <- featureNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetByFeature(CCLEsmall, features)\n        })\n        all(featureNames(CCLE_sub) %in% features)\n    })\n})"
      },
      {
        "partial": "test_that('subsetByTreatment handles invalid input...', {\n    expect_error({\n        invalid_treatments <- c('InvalidDrug1', 'InvalidDrug2')\n        # Complete the function call here\n    }, 'None of the provided treatment names are in the PharmacoSet')\n})",
        "complete": "test_that('subsetByTreatment handles invalid input...', {\n    expect_error({\n        invalid_treatments <- c('InvalidDrug1', 'InvalidDrug2')\n        subsetByTreatment(CCLEsmall, invalid_treatments)\n    }, 'None of the provided treatment names are in the PharmacoSet')\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_bug_fixes.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    # Fixed Issue31\n    # For now, the fix is to return NA and a warning \n    expect_equal(result, NA_character_)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `annotatePubchemCompound` function in this test case, and what specific attribute is it trying to retrieve?",
        "answer": "The `annotatePubchemCompound` function is being used to retrieve information about a compound from PubChem. In this specific test case, it's attempting to fetch the CAS (Chemical Abstracts Service) number for the compound with PubChem CID '60838'."
      },
      {
        "question": "Why does the test expect the result to be `NA_character_`, and what does this suggest about the function's behavior?",
        "answer": "The test expects the result to be `NA_character_`, which suggests that the function is designed to return NA and issue a warning when it encounters an error or cannot retrieve the requested information. This behavior is mentioned in the comment as a fix for 'Issue31', indicating that it's a deliberate error handling strategy."
      },
      {
        "question": "What libraries are being imported at the beginning of this code snippet, and what might their purposes be in the context of this test?",
        "answer": "The code imports three libraries: AnnotationGx, testthat, and checkmate. AnnotationGx likely contains the `annotatePubchemCompound` function being tested. testthat is a popular R testing framework used to define and run the test case. checkmate might be used for additional assertion functions, although it's not explicitly used in this snippet."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    # TODO: Add expectation here\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    expect_equal(result, NA_character_)\n})"
      },
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    # TODO: Define cid and call annotatePubchemCompound\n\n    expect_equal(result, NA_character_)\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    expect_equal(result, NA_character_)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/__init__.py",
    "language": "py",
    "content": "from .common import *\nfrom .loaders import *\nfrom .writers import *\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of using asterisks (*) in the import statements of this Python code snippet?",
        "answer": "The asterisks (*) in the import statements are used for wildcard imports. This means that all public names (those not starting with an underscore) from the modules 'common', 'loaders', and 'writers' are imported into the current namespace. While convenient, this practice is generally discouraged as it can lead to namespace pollution and make it unclear where specific names are coming from."
      },
      {
        "question": "How might the use of relative imports (indicated by the dot before module names) affect the structure and organization of this Python project?",
        "answer": "The use of relative imports (indicated by the dot before module names) suggests that this code is part of a package structure. The dot notation means that the imported modules ('common', 'loaders', and 'writers') are in the same directory as the current module. This structure implies a well-organized project with related functionality grouped into separate modules within the same package, promoting modularity and easier maintenance."
      },
      {
        "question": "What potential issues or best practices should be considered when using wildcard imports as shown in this code snippet?",
        "answer": "When using wildcard imports as shown in this snippet, several issues and best practices should be considered: 1) Namespace pollution: All names from the imported modules are brought into the current namespace, which can lead to naming conflicts. 2) Reduced code readability: It becomes unclear where specific names originate from. 3) Potential performance impact: Importing everything can be slower than importing specific names. 4) Difficulty in static analysis: Tools may struggle to determine which names are available. Best practices include using specific imports (e.g., 'from module import specific_name') or using the module name as a prefix (e.g., 'import module' and then 'module.specific_name') to maintain clarity and avoid these issues."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .common import *\nfrom .loaders import *\nfrom .",
        "complete": "from .common import *\nfrom .loaders import *\nfrom .writers import *"
      },
      {
        "partial": "from .common import *\nfrom . import loaders\nfrom . import",
        "complete": "from .common import *\nfrom . import loaders\nfrom . import writers"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "common.*",
        "loaders.*",
        "writers.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CCLE/CCLE_sampleMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\n# get the first part of the name split on _\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1) \n\nusethis::use_data(CCLE_sampleMetadata, overwrite = TRUE)\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in the first line of the code snippet?",
        "answer": "The `system.file()` function is used to locate and construct the file path for a specific file within an R package. In this case, it's locating the file 'CCLE_Cell_lines_annotations_20181226.txt' in the 'extdata' directory of the 'AnnotationGx' package. This ensures that the correct file is accessed regardless of where the package is installed on the user's system."
      },
      {
        "question": "How does the code extract the first part of the CCLE_ID and what function is used to apply this operation to all rows?",
        "answer": "The code extracts the first part of the CCLE_ID by splitting the string on underscores using `strsplit()` and then selecting the first element of the resulting list. This operation is applied to all rows using `purrr::map_chr(1)`, which maps the function of selecting the first element (`1`) over all split strings and returns the result as a character vector."
      },
      {
        "question": "What is the purpose of the `usethis::use_data()` function at the end of the snippet?",
        "answer": "The `usethis::use_data()` function is used to save R objects (in this case, the `CCLE_sampleMetadata` data frame) as internal data in an R package. The `overwrite = TRUE` argument allows the function to overwrite any existing data with the same name. This is typically used when developing R packages to include datasets that can be easily loaded by users of the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\n# get the first part of the name split on _\nCCLE_sampleMetadata$CCLE_ID_parsed <- ",
        "complete": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\n# get the first part of the name split on _\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1)"
      },
      {
        "partial": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1) \n\n",
        "complete": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1) \n\nusethis::use_data(CCLE_sampleMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/transforms/spatial.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\nfrom typing import Sequence, Union\n\n\nINTERPOLATORS = {\n    \"linear\": sitk.sitkLinear,\n    \"nearest\": sitk.sitkNearestNeighbor,\n    \"bspline\": sitk.sitkBSpline,\n}\n\n\ndef resample(image: sitk.Image,\n             spacing: Union[Sequence[float], float],\n             interpolation: str = \"linear\",\n             anti_alias: bool = True,\n             anti_alias_sigma: float = 2.,\n             transform: sitk.Transform = None) -> sitk.Image:\n    \"\"\"Resample image to a given spacing.\n\n\n    Parameters\n    ----------\n    image\n        The image to be resampled.\n\n    spacing\n        The new image spacing. If float, assumes the same spacing in all directions.\n        Alternatively, a sequence of floats can be passed to specify spacing along\n        x, y and z dimensions. Passing 0 at any position will keep the original\n        spacing along that dimension (useful for in-plane resampling).\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `spacing < image.GetSpacing()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n\n    Returns\n    -------\n    out : sitk.Image or tuple of sitk.Image\n        The resampled image. If mask is given, also return resampled mask.\n\n    \"\"\"\n\n    try:\n        interpolator = INTERPOLATORS[interpolation]\n    except KeyError:\n        raise ValueError(f\"interpolator must be one of {list(INTERPOLATORS.keys())}, got {interpolation}.\")\n\n    original_spacing = np.array(image.GetSpacing())\n    original_size = np.array(image.GetSize())\n\n    if isinstance(spacing, (float, int)):\n        new_spacing = np.repeat(spacing, len(original_spacing)).astype(np.float64)\n    else:\n        spacing = np.asarray(spacing)\n        new_spacing = np.where(spacing == 0, original_spacing, spacing)\n    new_size = np.floor(original_size * original_spacing / new_spacing).astype(int)\n\n    rif = sitk.ResampleImageFilter()\n    rif.SetOutputOrigin(image.GetOrigin())\n    rif.SetOutputSpacing(new_spacing)\n    rif.SetOutputDirection(image.GetDirection())\n    rif.SetSize(new_size.tolist())\n\n    if transform is not None:\n        rif.SetTransform(transform)\n\n    downsample = new_spacing > original_spacing\n    if downsample.any() and anti_alias:\n        sigma = np.where(downsample, anti_alias_sigma, 1e-11)\n        image = sitk.SmoothingRecursiveGaussian(image, sigma)  # TODO implement better sigma computation\n\n    rif.SetInterpolator(interpolator)\n    resampled_image = rif.Execute(image)\n\n    return resampled_image\n\n\ndef resize(image, new_size, interpolation=\"linear\"):\n\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n    new_size = np.asarray(new_size)\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image, new_spacing, interpolation=interpolation)\n\n\ndef rotate(image, rotation_centre, angles, interpolation=\"linear\"):\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n    x_angle, y_angle, z_angle = angles\n\n    rotation = sitk.Euler3DTransform(\n        rotation_centre,\n        x_angle,      # the angle of rotation around the x-axis, in radians -> coronal rotation\n        y_angle,      # the angle of rotation around the y-axis, in radians -> saggittal rotation\n        z_angle,      # the angle of rotation around the z-axis, in radians -> axial rotation\n        (0., 0., 0.)  # optional translation (shift) of the image, here we don't want any translation\n    )\n    return resample(image, spacing=image.GetSpacing(), interpolation=interpolation, transform=rotation)\n\n\ndef crop(image, crop_centre, size):\n    \"\"\"\n    \n    Parameters\n    ----------\n    image : \n\n    crop_centre : \n\n    size : \n\n\n    Returns\n    -------\n    out : \n\n    \"\"\"\n    crop_centre = np.asarray(crop_centre, dtype=np.float64)\n    image_shape = np.array((image.GetSize()[::-1]), dtype=np.float64)\n\n    if isinstance(size, int):\n        size_lower = size_upper = np.array([size for _ in image.GetSize()])\n    elif isinstance(size, (tuple, list, np.ndarray)):\n        if isinstance(size[0], int):\n            size_lower = size_upper = np.asarray(size)\n        elif isinstance(size[0], (tuple, list, np.ndarray)):\n            size_lower = np.array([s[0] for s in size])\n            size_upper = np.array([s[1] for s in size])\n\n    if (crop_centre < 0).any() or (crop_centre > image_shape).any():\n        raise ValueError(f\"Crop centre outside image boundaries. Image shape = {image_shape}, crop centre = {crop_centre}\")\n\n    min_x, min_y, min_z = np.clip(np.floor((image_shape - size_lower) / 2).astype(np.int64), 0, image_shape)\n    max_x, max_y, max_z = np.clip(np.floor((image_shape + size_upper) / 2).astype(np.int64), 0, image_shape)\n\n    return image[min_x:max_x, min_y:max_y, min_z:max_z]\n\n\ndef constant_pad(image, size, cval=0.):\n    if isinstance(size, int):\n        size_lower = size_upper = [size for _ in image.GetSize()]\n    elif isinstance(size, (tuple, list, np.ndarray)):\n        if isinstance(size[0], int):\n            size_lower = size_upper = size\n        elif isinstance(size[0], (tuple, list, np.ndarray)):\n            size_lower = [s[0] for s in size]\n            size_upper = [s[1] for s in size]\n    else:\n        raise ValueError(f\"Size must be either int, sequence of int or sequence of sequences of ints, got {size}.\")\n    return sitk.ConstantPad(image, size_lower, size_upper, cval)\n\n\ndef centre_on_point(image, centre):\n    pass\n\n# def resize_by_cropping_or_padding(image, size, centre=None, cval=0.):\n#     original_size = np.array(image.GetSize())\n#     size = np.asarray(size)\n#     centre = np.asarray(centre) if centre is not None else original_size / 2 # XXX is there any benefit to not using floor div here?\n\n#     crop_dims = np.where(size < original_size)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `resample` function in this code, and what are its key parameters?",
        "answer": "The `resample` function is used to resample a SimpleITK image to a new spacing. Its key parameters are:\n1. `image`: The SimpleITK image to be resampled.\n2. `spacing`: The new image spacing, which can be a single float (for uniform spacing) or a sequence of floats for each dimension.\n3. `interpolation`: The interpolation method to use (linear, nearest, or bspline).\n4. `anti_alias`: A boolean to determine whether to apply Gaussian smoothing before downsampling.\n5. `transform`: An optional SimpleITK transform to apply during resampling."
      },
      {
        "question": "How does the `rotate` function work, and what does it return?",
        "answer": "The `rotate` function rotates a SimpleITK image around a specified center point. It works as follows:\n1. Converts the rotation center from index to physical coordinates.\n2. Creates a 3D Euler transform with the specified rotation angles (in radians) around x, y, and z axes.\n3. Calls the `resample` function with the original image spacing and the created rotation transform.\n4. Returns the rotated image as a new SimpleITK Image object."
      },
      {
        "question": "What is the purpose of the `crop` function, and how does it handle different input types for the `size` parameter?",
        "answer": "The `crop` function extracts a subregion of a SimpleITK image around a specified center point. It handles the `size` parameter in three ways:\n1. If `size` is an integer, it creates a cubic crop region with that size in all dimensions.\n2. If `size` is a sequence of integers, it creates a crop region with those dimensions.\n3. If `size` is a sequence of tuples/lists, it uses the first element of each tuple for the lower bound and the second for the upper bound of the crop region in each dimension.\nThe function returns the cropped image as a new SimpleITK Image object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def resize(image, new_size, interpolation=\"linear\"):\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n    new_size = np.asarray(new_size)\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image, new_spacing, interpolation=interpolation)",
        "complete": "def resize(image, new_size, interpolation=\"linear\"):\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n    new_size = np.asarray(new_size)\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image, new_spacing, interpolation=interpolation)"
      },
      {
        "partial": "def rotate(image, rotation_centre, angles, interpolation=\"linear\"):\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n    x_angle, y_angle, z_angle = angles\n\n    rotation = sitk.Euler3DTransform(\n        rotation_centre,\n        x_angle,\n        y_angle,\n        z_angle,\n        (0., 0., 0.)\n    )\n    return resample(image, spacing=image.GetSpacing(), interpolation=interpolation, transform=rotation)",
        "complete": "def rotate(image, rotation_centre, angles, interpolation=\"linear\"):\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n    x_angle, y_angle, z_angle = angles\n\n    rotation = sitk.Euler3DTransform(\n        rotation_centre,\n        x_angle,\n        y_angle,\n        z_angle,\n        (0., 0., 0.)\n    )\n    return resample(image, spacing=image.GetSpacing(), interpolation=interpolation, transform=rotation)"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy"
      ],
      "from_imports": [
        "typing.Sequence"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/logLogisticRegression.R",
    "language": "R",
    "content": "#' Fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points (c, E) given by the user\n#' and returns a vector containing estimates for HS, E_inf, and EC50.\n#'\n#' By default, logLogisticRegression uses an L-BFGS algorithm to generate the fit. However, if\n#' this fails to converge to solution, logLogisticRegression samples lattice points throughout the parameter space.\n#' It then uses the lattice point with minimal least-squares residual as an initial guess for the optimal parameters,\n#' passes this guess to drm, and re-attempts the optimization. If this still fails, logLogisticRegression uses the\n#' PatternSearch algorithm to fit a log-logistic curve to the data.\n#'\n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeAUC(dose, viability)\n#'\n#' @param conc `numeric` is a vector of drug concentrations.\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of the log_conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param density `numeric` is a vector of length 3 whose components are the numbers of lattice points per unit\n#' length along the HS-, E_inf-, and base-10 logarithm of the EC50-dimensions of the parameter space, respectively.\n#' @param step `numeric` is a vector of length 3 whose entries are the initial step sizes in the HS, E_inf, and\n#' base-10 logarithm of the EC50 dimensions, respectively, for the PatternSearch algorithm.\n#' @param precision is a positive real number such that when the ratio of current step size to initial step\n#' size falls below it, the PatternSearch algorithm terminates. A smaller value will cause LogisticPatternSearch\n#' to take longer to complete optimization, but will produce a more accurate estimate for the fitted parameters.\n#' @param lower_bounds `numeric` is a vector of length 3 whose entries are the lower bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param upper_bounds `numeric` is a vector of length 3 whose entries are the upper bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param scale is a positive real number specifying the shape parameter of the Cauchy distribution.\n#' @param family `character`, if \"cauchy\", uses MLE under an assumption of Cauchy-distributed errors\n#' instead of sum-of-squared-residuals as the objective function for assessing goodness-of-fit of\n#' dose-response curves to the data. Otherwise, if \"normal\", uses MLE with a gaussian assumption of errors\n#' @param median_n If the viability points being fit were medians of measurements, they are expected to follow a median of \\code{family}\n#' distribution, which is in general quite different from the case of one measurement. Median_n is the number of measurements\n#' the median was taken of. If the measurements are means of values, then both the Normal and the Cauchy distributions are stable, so means of\n#' Cauchy or Normal distributed variables are still Cauchy and normal respectively.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(EC50) should be returned instead of EC50.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf should be returned as a decimal rather than a percentage.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return A list containing estimates for HS, E_inf, and EC50. It is annotated with the attribute Rsquared, which is the R^2 of the fit.\n#' Note that this is calculated using the values actually used for the fit, after truncation and any transform applied. With truncation, this will be\n#' different from the R^2 compared to the variance of the raw data. This also means that if all points were truncated down or up, there is no variance\n#' in the data, and the R^2 may be NaN.\n#'\n#' @export\n#'\n#' @importFrom CoreGx .meshEval .residual\n#' @importFrom stats optim dcauchy dnorm pcauchy rcauchy rnorm pnorm integrate\nlogLogisticRegression <- function(conc,\n                                  viability,\n                                  density = c(2, 10, 5),\n                                  step = .5 / density,\n                                  precision = 1e-4,\n                                  lower_bounds = c(0, 0, -6),\n                                  upper_bounds = c(4, 1, 6),\n                                  scale = 0.07,\n                                  family = c(\"normal\", \"Cauchy\"),\n                                  median_n = 1,\n                                  conc_as_log = FALSE,\n                                  viability_as_pct = TRUE,\n                                  trunc = TRUE,\n                                  verbose = TRUE) {\n  # guess <- .logLogisticRegressionRaw(conc, viability, density , step, precision, lower_bounds, upper_bounds, scale, Cauchy_flag, conc_as_log, viability_as_pct, trunc, verbose)\n\n\n# .logLogisticRegressionRaw <- function(conc,\n#                                   viability,\n#                                   density = c(2, 10, 2),\n#                                   step = .5 / density,\n#                                   precision = 0.05,\n#                                   lower_bounds = c(0, 0, -6),\n#                                   upper_bounds = c(4, 1, 6),\n#                                   scale = 0.07,\n#                                   Cauchy_flag = FALSE,\n#                                   conc_as_log = FALSE,\n#                                   viability_as_pct = TRUE,\n#                                   trunc = TRUE,\n#                                   verbose = FALSE) {\n  family <- match.arg(family)\n\n\n  if (prod(is.finite(step)) != 1) {\n    print(step)\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  if (prod(is.finite(precision)) != 1) {\n    print(precision)\n    stop(\"Precision value is not a real number.\")\n  }\n\n  if (prod(is.finite(lower_bounds)) != 1) {\n    print(lower_bounds)\n    stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(upper_bounds)) != 1) {\n    print(upper_bounds)\n    stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(density)) != 1) {\n    print(density)\n    stop(\"Density vector contains elements which are not real numbers.\")\n  }\n\n  if (is.finite(scale) == FALSE) {\n    print(scale)\n    stop(\"Scale is not a real number.\")\n  }\n\n  if (is.character(family) == FALSE) {\n    print(family)\n    stop(\"Cauchy flag is not a string.\")\n  }\n\n  if (length(density) != 3){\n    stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  }\n\n  if (!median_n==as.integer(median_n)){\n    stop(\"There can only be a integral number of samples to take a median of. Check your setting of median_n parameter, it is not an integer\")\n  }\n\n\n  if (min(upper_bounds - lower_bounds) < 0) {\n    print(rbind(lower_bounds, upper_bounds))\n    stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  }\n\n\n\n  if (min(density) <= 0) {\n    print(density)\n    stop(\"Lattice point density vector contains negative values.\")\n  }\n\n  if (precision <= 0) {\n    print(precision)\n    stop(\"Negative precision value.\")\n  }\n\n  if (min(step) <= 0) {\n    print(step)\n    stop(\"Step vector contains nonpositive numbers.\")\n  }\n\n  if (scale <= 0) {\n    print(scale)\n    stop(\"Scale parameter is a nonpositive number.\")\n  }\n\n\n\n\n\n  CoreGx::.sanitizeInput(x = conc,\n                         y = viability,\n                         x_as_log = conc_as_log,\n                         y_as_log = FALSE,\n                         y_as_pct = viability_as_pct,\n                         trunc = trunc,\n                         verbose = verbose)\n\n  cleanData <- CoreGx::.reformatData(x = conc,\n                               y = viability,\n                               x_to_log = !conc_as_log,\n                               y_to_log = FALSE,\n                               y_to_frac = viability_as_pct,\n                               trunc = trunc)\n\n  if (!(all(lower_bounds < upper_bounds))) {\n    if (verbose == 2) {\n      message(\"lower_bounds:\")\n      message(lower_bounds)\n      message(\"upper_bounds:\")\n      message(upper_bounds)\n    }\n    stop (\"All lower bounds must be less than the corresponding upper_bounds.\")\n  }\n\n\n  log_conc <- cleanData[[\"x\"]]\n  viability <- cleanData[[\"y\"]]\n\n\n  #ATTEMPT TO REFINE GUESS WITH L-BFGS OPTIMIZATION\n  # tryCatch(\n  gritty_guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n                    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n                    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3]))\n\n\n  guess <- CoreGx::.fitCurve(x = log_conc,\n                              y = viability,\n                              f = PharmacoGx:::.Hill,\n                              density = density,\n                              step = step,\n                              precision = precision,\n                              lower_bounds = lower_bounds,\n                              upper_bounds = upper_bounds,\n                              scale = scale,\n                              family = family,\n                              median_n = median_n,\n                              trunc = trunc,\n                              verbose = verbose,\n                              gritty_guess = gritty_guess,\n                              span = 1)\n\n  returnval <- list(\"HS\" = guess[1],\n              \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n              \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3]))\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logLogisticRegression` function and what type of data does it process?",
        "answer": "The `logLogisticRegression` function fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points. It processes drug concentration (conc) and corresponding cell viability data, returning estimates for Hill Slope (HS), E_inf (efficacy at infinite concentration), and EC50 (concentration for 50% effect)."
      },
      {
        "question": "How does the function handle potential convergence issues during curve fitting?",
        "answer": "The function uses a multi-step approach to handle convergence issues: 1) It first attempts to fit using an L-BFGS algorithm. 2) If that fails, it samples lattice points in the parameter space and uses the best point as an initial guess for optimization. 3) If optimization still fails, it uses the PatternSearch algorithm. This approach ensures robustness in fitting curves to various types of dose-response data."
      },
      {
        "question": "What are the key parameters that control the behavior of the curve fitting process in `logLogisticRegression`, and how do they affect the function's operation?",
        "answer": "Key parameters include: 1) `density`: controls the number of lattice points sampled in each dimension. 2) `step` and `precision`: control the PatternSearch algorithm's behavior. 3) `lower_bounds` and `upper_bounds`: constrain the parameter space. 4) `scale`: shape parameter for Cauchy distribution. 5) `family`: determines the error distribution assumption (normal or Cauchy). 6) `median_n`: accounts for median measurements. These parameters allow fine-tuning of the fitting process for different types of data and desired levels of precision."
      }
    ],
    "completion_tasks": [
      {
        "partial": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  if (prod(is.finite(step)) != 1) stop(\"Step vector contains elements which are not positive real numbers.\")\n  if (!is.finite(precision)) stop(\"Precision value is not a real number.\")\n  if (prod(is.finite(lower_bounds)) != 1) stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(upper_bounds)) != 1) stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(density)) != 1) stop(\"Density vector contains elements which are not real numbers.\")\n  if (!is.finite(scale)) stop(\"Scale is not a real number.\")\n  if (!is.character(family)) stop(\"Family is not a string.\")\n  if (length(density) != 3) stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  if (!median_n == as.integer(median_n)) stop(\"median_n must be an integer.\")\n  if (min(upper_bounds - lower_bounds) < 0) stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  if (min(density) <= 0) stop(\"Lattice point density vector contains negative values.\")\n  if (precision <= 0) stop(\"Negative precision value.\")\n  if (min(step) <= 0) stop(\"Step vector contains nonpositive numbers.\")\n  if (scale <= 0) stop(\"Scale parameter is a nonpositive number.\")\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    # ... (parameters for .fitCurve)\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  )\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}",
        "complete": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  if (prod(is.finite(step)) != 1) stop(\"Step vector contains elements which are not positive real numbers.\")\n  if (!is.finite(precision)) stop(\"Precision value is not a real number.\")\n  if (prod(is.finite(lower_bounds)) != 1) stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(upper_bounds)) != 1) stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(density)) != 1) stop(\"Density vector contains elements which are not real numbers.\")\n  if (!is.finite(scale)) stop(\"Scale is not a real number.\")\n  if (!is.character(family)) stop(\"Family is not a string.\")\n  if (length(density) != 3) stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  if (!median_n == as.integer(median_n)) stop(\"median_n must be an integer.\")\n  if (min(upper_bounds - lower_bounds) < 0) stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  if (min(density) <= 0) stop(\"Lattice point density vector contains negative values.\")\n  if (precision <= 0) stop(\"Negative precision value.\")\n  if (min(step) <= 0) stop(\"Step vector contains nonpositive numbers.\")\n  if (scale <= 0) stop(\"Scale parameter is a nonpositive number.\")\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    x = log_conc,\n    y = viability,\n    f = PharmacoGx:::.Hill,\n    density = density,\n    step = step,\n    precision = precision,\n    lower_bounds = lower_bounds,\n    upper_bounds = upper_bounds,\n    scale = scale,\n    family = family,\n    median_n = median_n,\n    trunc = trunc,\n    verbose = verbose,\n    gritty_guess = gritty_guess,\n    span = 1\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  )\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}"
      },
      {
        "partial": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  # ... (input validation code)\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    # ... (parameters for .fitCurve)\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  )\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}",
        "complete": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  if (prod(is.finite(step)) != 1) stop(\"Step vector contains elements which are not positive real numbers.\")\n  if (!is.finite(precision)) stop(\"Precision value is not a real number.\")\n  if (prod(is.finite(lower_bounds)) != 1) stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(upper_bounds)) != 1) stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(density)) != 1) stop(\"Density vector contains elements which are not real numbers.\")\n  if (!is.finite(scale)) stop(\"Scale is not a real number.\")\n  if (!is.character(family)) stop(\"Family is not a string.\")\n  if (length(density) != 3) stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  if (!median_n == as.integer(median_n)) stop(\"median_n must be an integer.\")\n  if (min(upper_bounds - lower_bounds) < 0) stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  if (min(density) <= 0) stop(\"Lattice point density vector contains negative values.\")\n  if (precision <= 0) stop(\"Negative precision value.\")\n  if (min(step) <= 0) stop(\"Step vector contains nonpositive numbers.\")\n  if (scale <= 0) stop(\"Scale parameter is a nonpositive number.\")\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    x = log_conc,\n    y = viability,\n    f = PharmacoGx:::.Hill,\n    density = density,\n    step = step,\n    precision = precision,\n    lower_bounds = lower_bounds,\n    upper_bounds = upper_bounds,\n    scale = scale,\n    family = family,\n    median_n = median_n,\n    trunc = trunc,\n    verbose = verbose,\n    gritty_guess = gritty_guess,\n    span = 1\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/gCSI/gCSI_sampleMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]\n\nusethis::use_data(gCSI_sampleMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code snippet, and how is it being used?",
        "answer": "The `system.file()` function is used to locate and retrieve the file path of a specific file within an R package. In this code, it's being used to find the 'gCSI_sampleMap.txt' file in the 'extdata/gCSI' directory of the 'AnnotationGx' package. This allows the code to access package-specific data files regardless of where the package is installed on the user's system."
      },
      {
        "question": "Explain the purpose of the `data.table::fread()` function and the `check.names=T` parameter in this context.",
        "answer": "The `data.table::fread()` function is used to read the contents of the file specified by `filePath` into a data.table object called `rawdata`. It's a fast and efficient way to read large data files. The `check.names=T` parameter ensures that the column names in the resulting data.table are valid R variable names, converting any invalid characters or spaces to periods if necessary. This helps prevent issues when accessing columns later in the code."
      },
      {
        "question": "What is the purpose of the last line of code using `usethis::use_data()`, and what does the `overwrite = TRUE` argument do?",
        "answer": "The `usethis::use_data()` function is used to save R objects as internal data in a package. In this case, it's saving the `gCSI_sampleMetadata` object as a dataset within the current package. The `overwrite = TRUE` argument allows the function to overwrite any existing dataset with the same name. This is useful for updating the package's internal data or when running the code multiple times during development."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]\n\n# Complete the code to save gCSI_sampleMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]\n\nusethis::use_data(gCSI_sampleMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the file and extract specific columns\n\nfilePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\n\n# Add code here to read the file and extract columns",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/AnnotationGx-package.R",
    "language": "R",
    "content": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end\nNULL\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `#' @keywords internal` line in the given code snippet?",
        "answer": "The `#' @keywords internal` line is a roxygen2 tag that marks the following object or function as internal to the package. This means it's not intended for direct use by end-users and won't be included in the package's public documentation."
      },
      {
        "question": "What does the `@importFrom data.table` directive do in this code, and why are there multiple instances of it?",
        "answer": "The `@importFrom data.table` directive is used to import specific functions from the data.table package. There are multiple instances because each one imports a different function or operator (like `:=`, `.BY`, `.EACHI`, etc.). This approach allows the package to use these data.table functions without needing to prefix them with `data.table::` every time they're used in the code."
      },
      {
        "question": "Why is there a `NULL` statement at the end of the code snippet?",
        "answer": "The `NULL` statement at the end of the code snippet is used as a placeholder. In R package development, this is a common practice when defining a package-level documentation file (usually named `packagename-package.R`). The `NULL` ensures that R doesn't try to evaluate any of the roxygen comments as actual code, while still allowing the roxygen2 package to process the documentation tags."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n## usethis namespace: end\nNULL",
        "complete": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end\nNULL"
      },
      {
        "partial": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end",
        "complete": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/dataset_check.py",
    "language": "py",
    "content": "import pathlib\nimport os\nimport re\nimport pandas as pd\nimport torchio as tio\nimport pytest\nimport torch\nimport urllib.request as request\nfrom zipfile import ZipFile\n\nfrom typing import List\nfrom imgtools.io import Dataset\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    output_path = pathlib.Path(curr_path, 'tests','temp').as_posix()\n    quebec_path = quebec_path.as_posix()\n    \n    #Dataset name\n    dataset_name  = os.path.basename(quebec_path)\n    imgtools_path = pathlib.Path(os.path.dirname(quebec_path), '.imgtools')\n\n    #Defining paths for autopipeline and dataset component\n    crawl_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.csv\").as_posix()\n    json_path =  pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.json\").as_posix()  # noqa: F841\n    edge_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}_edges.csv\").as_posix()\n    assert os.path.exists(crawl_path) & os.path.exists(edge_path) & os.path.exists(json_path), \"There was no crawler output\"\n    \n    yield quebec_path, output_path, crawl_path, edge_path\n\nclass select_roi_names(tio.LabelTransform):\n    \"\"\"\n    Based on the given roi names, selects from the given set\n    \"\"\"\n    def __init__(\n            self,\n            roi_names: List[str] = None,\n            **kwargs\n            ) -> None:\n        super().__init__(**kwargs)\n        self.kwargs = kwargs\n        self.roi_names = roi_names\n    \n    def apply_transform(self, subject):\n        #list of roi_names\n        for image in self.get_images(subject):\n            #For only applying to labelmaps\n            metadata = subject[\"metadata_RTSTRUCT_CT\"]\n            patterns = self.roi_names\n            mask = torch.empty_like(image.data)[:len(patterns)]\n            for j,pat in enumerate(patterns):\n                k = []\n                for i,col in enumerate(metadata):\n                    if re.match(pat,col,flags=re.IGNORECASE):\n                        k.append(i)\n                if len(k)==0:\n                    mask[j] = mask[j]*0\n                else:  \n                    mask[j] = (image.data[k].sum(axis=0)>0)*1    \n            image.set_data(mask)\n        return subject\n    \n    def is_invertible(self):\n        return False\n\n#Defining for test_dataset method in Test_components class\ndef collate_fn(data):\n    \"\"\"\n       data: is a tio.subject with multiple columns\n             Need to return required data\n    \"\"\"\n    mod_names = [items for items in data[0].keys() if items.split(\"_\")[0]==\"mod\"]\n    temp_stack = {}\n    for names in mod_names:\n        temp_stack[names] = torch.stack(tuple(items[names].data for items in data))\n    return temp_stack\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])\nclass TestDataset:\n    \"\"\"\n    For testing the dataset components of the med-imagetools package\n    test_dataset:\n        1) Checks if the length of the dataset matches\n        2) Checks if the items in the subject object is correct and present\n        3) Checks if you are able to load it via load_nrrd and load_directly, and checks if the subjects generated matches\n        4) Checks if torch data loader can load the formed dataset and get atleast 1 iteration\n        5) Checks if the transforms are happening by checking the size\n    \"\"\"\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n\n    def test_dataset(self, modalities):\n        \"\"\"\n        Testing the Dataset class\n        \"\"\"\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        comp_path = pathlib.Path(output_path_mod).resolve().joinpath('dataset.csv').as_posix()\n        comp_table = pd.read_csv(comp_path, index_col=0)\n        print(comp_path, comp_table)\n        \n        #Loading from nrrd files\n        subjects_nrrd = Dataset.load_image(output_path_mod, ignore_multi=True)\n        #Loading files directly\n        # subjects_direct = Dataset.load_directly(self.input_path,modalities=modalities,ignore_multi=True)\n        \n        #The number of subjects is equal to the number of components which is 2 for this dataset\n        # assert len(subjects_nrrd) == len(subjects_direct) == 2, \"There was some error in generation of subject object\"\n        # assert subjects_nrrd[0].keys() == subjects_direct[0].keys()\n\n        # del subjects_direct\n        # To check if all metadata items present in the keys\n        # temp_nrrd = subjects_nrrd[0]\n        # columns_shdbe_present = set([col if col.split(\"_\")[0]==\"metadata\" else \"mod_\"+(\"_\").join(col.split(\"_\")[1:]) for col in list(comp_table.columns) if col.split(\"_\")[0] in [\"folder\",\"metadata\"]])\n        # print(columns_shdbe_present)\n        # assert set(temp_nrrd.keys()).issubset(columns_shdbe_present), \"Not all items present in dictionary, some fault in going through the different columns in a single component\"\n\n        transforms = tio.Compose([tio.Resample(4), tio.CropOrPad((96,96,40)), select_roi_names([\"larynx\"]), tio.OneHot()])\n\n        #Forming dataset and dataloader\n        test_set = tio.SubjectsDataset(subjects_nrrd, transform=transforms)\n        test_loader = torch.utils.data.DataLoader(test_set,batch_size=2,shuffle=True,collate_fn = collate_fn)\n\n        #Check test_set is correct\n        assert len(test_set)==2\n\n        #Get items from test loader\n        #If this function fails , there is some error in formation of test\n        data = next(iter(test_loader))\n        A = [item[1].shape == (2,1,96,96,40) if \"RTSTRUCT\" not in item[0] else item[1].shape == (2,2,96,96,40) for item in data.items()]\n        assert all(A), \"There is some problem in the transformation/the formation of subject object\"\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dataset_path` fixture in this code, and how does it handle downloading and extracting the dataset?",
        "answer": "The `dataset_path` fixture is responsible for setting up the test dataset. It checks if the dataset already exists, and if not, it downloads and extracts it. The fixture performs the following steps:\n1. Determines the path for the Quebec dataset.\n2. If the dataset doesn't exist, it creates the directory, downloads the dataset from a GitHub URL, extracts the ZIP file, and removes the ZIP file after extraction.\n3. If the dataset already exists, it skips the download process.\n4. It then sets up paths for various components like crawl path, JSON path, and edge path.\n5. Finally, it yields the necessary paths for use in tests."
      },
      {
        "question": "Explain the purpose and functionality of the `select_roi_names` class in the given code snippet.",
        "answer": "The `select_roi_names` class is a custom transform that inherits from `tio.LabelTransform`. Its purpose is to select specific regions of interest (ROIs) from a given set of labels based on provided ROI names. Here's how it works:\n1. It takes a list of ROI names as input.\n2. In the `apply_transform` method, it iterates through the images in the subject.\n3. For each image, it uses the provided ROI names to create a mask.\n4. It matches the ROI names against the metadata columns using regex, ignoring case.\n5. If a match is found, it creates a binary mask for that ROI.\n6. The resulting mask contains only the selected ROIs, with each ROI on a separate channel.\n7. The transform is not invertible, as indicated by the `is_invertible` method returning False."
      },
      {
        "question": "What is the purpose of the `TestDataset` class, and how does it use pytest's parameterization to test different modality combinations?",
        "answer": "The `TestDataset` class is designed to test various aspects of the dataset components in the med-imagetools package. Its main features and functionalities are:\n1. It uses pytest's `@pytest.mark.parametrize` decorator to test three different modality combinations: 'CT', 'CT,RTSTRUCT', and 'CT,RTSTRUCT,RTDOSE'.\n2. The `test_dataset` method performs several checks:\n   a. Verifies if the dataset length matches the expected value.\n   b. Ensures that the correct items are present in the subject object.\n   c. Checks if the dataset can be loaded using both `load_nrrd` and `load_directly` methods.\n   d. Verifies if a PyTorch DataLoader can successfully load the dataset and perform at least one iteration.\n   e. Confirms that the transforms are applied correctly by checking the size of the transformed data.\n3. It uses a custom `collate_fn` to prepare the data for the DataLoader.\n4. The test applies a series of transforms to the dataset, including resampling, cropping/padding, selecting specific ROIs, and one-hot encoding.\n5. Finally, it asserts that the transformed data has the expected shape, ensuring that the entire pipeline works correctly for different modality combinations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class select_roi_names(tio.LabelTransform):\n    def __init__(self, roi_names: List[str] = None, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.kwargs = kwargs\n        self.roi_names = roi_names\n    \n    def apply_transform(self, subject):\n        for image in self.get_images(subject):\n            metadata = subject[\"metadata_RTSTRUCT_CT\"]\n            patterns = self.roi_names\n            mask = torch.empty_like(image.data)[:len(patterns)]\n            for j, pat in enumerate(patterns):\n                # TODO: Implement pattern matching and mask creation\n            image.set_data(mask)\n        return subject\n    \n    def is_invertible(self):\n        return False",
        "complete": "class select_roi_names(tio.LabelTransform):\n    def __init__(self, roi_names: List[str] = None, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.kwargs = kwargs\n        self.roi_names = roi_names\n    \n    def apply_transform(self, subject):\n        for image in self.get_images(subject):\n            metadata = subject[\"metadata_RTSTRUCT_CT\"]\n            patterns = self.roi_names\n            mask = torch.empty_like(image.data)[:len(patterns)]\n            for j, pat in enumerate(patterns):\n                k = [i for i, col in enumerate(metadata) if re.match(pat, col, flags=re.IGNORECASE)]\n                mask[j] = (image.data[k].sum(axis=0) > 0) * 1 if k else mask[j] * 0\n            image.set_data(mask)\n        return subject\n    \n    def is_invertible(self):\n        return False"
      },
      {
        "partial": "def test_dataset(self, modalities):\n    output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n    comp_path = pathlib.Path(output_path_mod).resolve().joinpath('dataset.csv').as_posix()\n    comp_table = pd.read_csv(comp_path, index_col=0)\n    \n    subjects_nrrd = Dataset.load_image(output_path_mod, ignore_multi=True)\n    \n    transforms = tio.Compose([tio.Resample(4), tio.CropOrPad((96,96,40)), select_roi_names([\"larynx\"]), tio.OneHot()])\n    \n    test_set = tio.SubjectsDataset(subjects_nrrd, transform=transforms)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=True, collate_fn=collate_fn)\n    \n    # TODO: Implement assertions to check the dataset and loader\n",
        "complete": "def test_dataset(self, modalities):\n    output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n    comp_path = pathlib.Path(output_path_mod).resolve().joinpath('dataset.csv').as_posix()\n    comp_table = pd.read_csv(comp_path, index_col=0)\n    \n    subjects_nrrd = Dataset.load_image(output_path_mod, ignore_multi=True)\n    \n    transforms = tio.Compose([tio.Resample(4), tio.CropOrPad((96,96,40)), select_roi_names([\"larynx\"]), tio.OneHot()])\n    \n    test_set = tio.SubjectsDataset(subjects_nrrd, transform=transforms)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=True, collate_fn=collate_fn)\n    \n    assert len(test_set) == 2, \"Incorrect number of subjects in the dataset\"\n    \n    data = next(iter(test_loader))\n    assert all(item[1].shape == (2,1,96,96,40) if \"RTSTRUCT\" not in item[0] else item[1].shape == (2,2,96,96,40) for item in data.items()), \"Incorrect shape after transformations\""
      }
    ],
    "dependencies": {
      "imports": [
        "pathlib",
        "os",
        "re",
        "pandas",
        "torchio",
        "pytest",
        "torch",
        "urllib.request"
      ],
      "from_imports": [
        "zipfile.ZipFile",
        "typing.List",
        "imgtools.io.Dataset"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/loaders.py",
    "language": "py",
    "content": "import os\nimport pydicom\nimport SimpleITK as sitk\n\nfrom imgtools.ops import StructureSetToSegmentation\nfrom imgtools.io import read_dicom_auto\n\nfrom typing import Optional\n\nfrom readii.utils import get_logger\n\n# Create a global logger instance\nlogger = get_logger()\n\ndef loadDicomSITK(imgDirPath: str) -> sitk.Image:\n    \"\"\"Read DICOM series as SimpleITK Image.\n\n    Parameters\n    ----------\n    img_path : str\n        Path to directory containing the DICOM series to load.\n\n    Returns\n    -------\n    sitk.Image\n        The loaded image.\n    \"\"\"\n    # Set up the reader for the DICOM series\n    logger.debug(f\"Loading DICOM series from directory: {imgDirPath}\")\n    reader = sitk.ImageSeriesReader()\n    dicomNames = reader.GetGDCMSeriesFileNames(imgDirPath)\n    reader.SetFileNames(dicomNames)\n    return reader.Execute()\n\n\ndef loadRTSTRUCTSITK(\n    rtstructPath: str, baseImageDirPath: str, roiNames: Optional[str] = None\n) -> dict:\n    \"\"\"Load RTSTRUCT into SimpleITK Image.\n\n    Parameters\n    ----------\n    rtstructPath : str\n        Path to the DICOM file containing the RTSTRUCT\n    baseImageDirPath : str\n        Path to the directory containing the DICOMS for the original image the segmentation\n        was created from (e.g. CT). This is required to load the RTSTRUCT.\n    roiNames : str\n        Identifier for which region(s) of interest to load from the total segmentation file\n\n    Returns\n    -------\n    dict\n        A dictionary of mask ROIs from the loaded RTSTRUCT image as a SimpleITK image objects.\n        The segmentation label is set to 1.\n    \"\"\"\n    # Set up segmentation loader\n    logger.debug(f\"Making mask using ROI names: {roiNames}\")\n    makeMask = StructureSetToSegmentation(roi_names=roiNames)\n\n    # Read in the base image (CT) and segmentation DICOMs into SITK Images\n    logger.debug(f\"Loading RTSTRUCT from directory: {rtstructPath}\")\n    baseImage = read_dicom_auto(baseImageDirPath)\n    segImage = read_dicom_auto(rtstructPath)\n\n    try:\n        # Get the individual ROI masks\n        segMasks = makeMask(\n            segImage,\n            baseImage.image,\n            existing_roi_indices={\"background\": 0},\n            ignore_missing_regex=False,\n        )\n    except ValueError:\n        return {}\n\n    # Get list of ROIs present in this rtstruct\n    loadedROINames = segMasks.raw_roi_names\n    # Initialize dictionary to store ROI names and images\n    roiStructs = {}\n    # Get each roi and its label and store in dictionary\n    for roi in loadedROINames:\n        # Get the mask for this ROI\n        roiMask = segMasks.get_label(name=roi)\n        # Store the ROI name and image\n        roiStructs[roi] = roiMask\n\n    return roiStructs\n\n\ndef loadSegmentation(\n    segImagePath: str,\n    modality: str,\n    baseImageDirPath: Optional[str] = None,\n    roiNames: Optional[str] = None,\n) -> dict:\n    \"\"\"Function to load a segmentation with the correct function.\n\n    Parameters\n    ----------\n    segImagePath : str\n        Path to the segmentation file to load\n    modality : str\n        Type of image that imgPath points to to load. If RTSTRUCT, must set baseImageDirPath\n    baseImageDirPath : str\n        Path to the directory containing the DICOMS for the original image the segmentation\n        was created from.\n    roiNames : str\n        Identifier for which region(s) of interest to load from the total segmentation file\n\n    Returns\n    -------\n    dict\n        A dictionary of each of the ROIs and their name in the segmentation image as sitk.Image objects.\n\n    Examples\n    --------\n    >>> segImages = loadSegmentation(\"/path/to/segmentation/1.dcm\", 'RTSTRUCT', '/path/to/CT', 'GTVp.*')\n    \"\"\"\n\n    if modality in [\"SEG\", \"seg\"]:\n        # Loading SEG requires directory containing file, not the actual file path\n        imgFolder, _ = os.path.split(segImagePath)\n        segHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n        roiName = segHeader.SegmentSequence[0].SegmentLabel\n        return {roiName: loadDicomSITK(imgFolder)}\n\n    elif modality in [\"RTSTRUCT\", \"rtstruct\"]:\n        if baseImageDirPath == None:\n            raise ValueError(\n                \"Missing path to original image segmentation was taken from. RTSTRUCT loader requires original image.\"\n            )\n        else:\n            return loadRTSTRUCTSITK(segImagePath, baseImageDirPath, roiNames)\n\n    else:\n        raise ValueError(\n            \"This segmentation modality is not supported. Must be one of RTSTRUCT or SEG\"\n        )\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `loadDicomSITK` function, and what type of object does it return?",
        "answer": "The `loadDicomSITK` function is used to read a DICOM series as a SimpleITK Image. It takes a directory path containing the DICOM series as input and returns a `sitk.Image` object representing the loaded image. The function uses `sitk.ImageSeriesReader()` to read the DICOM files in the specified directory."
      },
      {
        "question": "In the `loadRTSTRUCTSITK` function, what does the `roiNames` parameter do, and how does it affect the function's output?",
        "answer": "The `roiNames` parameter in the `loadRTSTRUCTSITK` function is an optional string that identifies which region(s) of interest to load from the total segmentation file. It is used as an argument for the `StructureSetToSegmentation` class to filter the ROIs. The function returns a dictionary where the keys are the ROI names, and the values are SimpleITK image objects representing the masks for each ROI. If `roiNames` is specified, only the matching ROIs will be included in the output dictionary."
      },
      {
        "question": "How does the `loadSegmentation` function handle different types of segmentation files, and what are its requirements for loading an RTSTRUCT file?",
        "answer": "The `loadSegmentation` function handles different types of segmentation files based on the `modality` parameter. For 'SEG' or 'seg' modalities, it loads the segmentation using `loadDicomSITK`. For 'RTSTRUCT' or 'rtstruct' modalities, it uses `loadRTSTRUCTSITK`. When loading an RTSTRUCT file, the function requires the `baseImageDirPath` parameter to be set, which should point to the directory containing the original image (e.g., CT) that the segmentation was created from. If `baseImageDirPath` is not provided for an RTSTRUCT file, the function raises a ValueError."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def loadDicomSITK(imgDirPath: str) -> sitk.Image:\n    \"\"\"Read DICOM series as SimpleITK Image.\"\"\"\n    reader = sitk.ImageSeriesReader()\n    dicomNames = reader.GetGDCMSeriesFileNames(imgDirPath)\n    reader.SetFileNames(dicomNames)\n    # Complete the function",
        "complete": "def loadDicomSITK(imgDirPath: str) -> sitk.Image:\n    \"\"\"Read DICOM series as SimpleITK Image.\"\"\"\n    reader = sitk.ImageSeriesReader()\n    dicomNames = reader.GetGDCMSeriesFileNames(imgDirPath)\n    reader.SetFileNames(dicomNames)\n    return reader.Execute()"
      },
      {
        "partial": "def loadSegmentation(segImagePath: str, modality: str, baseImageDirPath: Optional[str] = None, roiNames: Optional[str] = None) -> dict:\n    \"\"\"Function to load a segmentation with the correct function.\"\"\"\n    if modality.lower() == 'seg':\n        imgFolder, _ = os.path.split(segImagePath)\n        segHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n        roiName = segHeader.SegmentSequence[0].SegmentLabel\n        return {roiName: loadDicomSITK(imgFolder)}\n    elif modality.lower() == 'rtstruct':\n        # Complete the RTSTRUCT loading logic\n    else:\n        # Complete the error handling",
        "complete": "def loadSegmentation(segImagePath: str, modality: str, baseImageDirPath: Optional[str] = None, roiNames: Optional[str] = None) -> dict:\n    \"\"\"Function to load a segmentation with the correct function.\"\"\"\n    if modality.lower() == 'seg':\n        imgFolder, _ = os.path.split(segImagePath)\n        segHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n        roiName = segHeader.SegmentSequence[0].SegmentLabel\n        return {roiName: loadDicomSITK(imgFolder)}\n    elif modality.lower() == 'rtstruct':\n        if baseImageDirPath is None:\n            raise ValueError(\"Missing path to original image segmentation was taken from. RTSTRUCT loader requires original image.\")\n        return loadRTSTRUCTSITK(segImagePath, baseImageDirPath, roiNames)\n    else:\n        raise ValueError(\"This segmentation modality is not supported. Must be one of RTSTRUCT or SEG\")"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pydicom",
        "SimpleITK"
      ],
      "from_imports": [
        "imgtools.ops.StructureSetToSegmentation",
        "imgtools.io.read_dicom_auto",
        "typing.Optional",
        "readii.utils.get_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_negative_controls.py",
    "language": "py",
    "content": "import numpy as np\n\nfrom readii.image_processing import *\nfrom radiomics import imageoperations\nfrom readii.negative_controls import (\n    makeShuffleImage,\n    makeRandomImage,\n    makeRandomSampleFromDistributionImage,\n    negativeControlROIOnly,\n    negativeControlNonROIOnly,\n    applyNegativeControl\n)\n\n\nimport pytest\nimport collections\n\n\n@pytest.fixture\ndef nsclcCTImage():\n    nsclcCTPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n    return loadDicomSITK(nsclcCTPath)\n\n\n@pytest.fixture\ndef nsclcSEGImage():\n    nsclcSEGPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n    segDictionary = loadSegmentation(nsclcSEGPath, modality='SEG')\n    return segDictionary['Heart']\n\n\n@pytest.fixture()\ndef nsclcCTImageFolderPath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n\n\n@pytest.fixture()\ndef nsclcSEGFilePath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n\n\n@pytest.fixture()\ndef nsclcCropped(nsclcCTImage, nsclcSEGImage, nsclcCTImageFolderPath, nsclcSEGFilePath):\n    roiImage = flattenImage(nsclcSEGImage)\n    alignedROIImage = alignImages(nsclcCTImage, roiImage)\n    segmentationLabel = getROIVoxelLabel(alignedROIImage)\n\n    croppedCT, croppedROI = getCroppedImages(nsclcCTImage, alignedROIImage, segmentationLabel)\n\n    return croppedCT, croppedROI, segmentationLabel\n\n\n@pytest.fixture()\ndef randomSeed():\n    return 10\n\ndef test_makeShuffleImage(nsclcCTImage, randomSeed):\n    \" Test negative control to shuffle the whole image\"\n\n    shuffled_image = makeShuffleImage(nsclcCTImage, randomSeed)\n    original_pixels = sitk.GetArrayFromImage(nsclcCTImage)\n    shuffled_pixels = sitk.GetArrayFromImage(shuffled_image)\n\n    assert nsclcCTImage.GetSize() == shuffled_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == shuffled_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == shuffled_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_pixels), \\\n        \"Pixel values are not shuffled\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_pixels.flatten())), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    assert shuffled_pixels[0,0,0] == -987, \\\n        \"Random seed is not working for shuffled image, first voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[-1,-1,-1] == 10, \\\n        \"Random seed is not working for shuffled image, last voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[238,252,124] == -118, \\\n        \"Random seed is not working for shuffled image, central ROI voxel has wrong shuffled value. Random seed should be 10.\"\n\n\ndef test_makeRandomImage(nsclcCTImage, randomSeed):\n    \" Test negative control to randomize the pixels of the whole image\"\n\n    randomized_image = makeRandomImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_image)\n\n    assert nsclcCTImage.GetSize() == randomized_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    assert randomized_arr_image[0,0,0] == 2156, \\\n        \"Random seed is not working for random image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[-1,-1,-1] == 90, \\\n        \"Random seed is not working for random image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[238,252,124] == -840, \\\n        \"Random seed is not working for random image, central ROI voxel has wrong random value. Random seed should be 10.\"\n    \n\ndef test_makeRandomSampleFromDistributionImage(nsclcCTImage, randomSeed):\n    \" Test negative control to uniformly sample the pixels of the whole image from the images pixel distribution\"\n\n    randomized_sampled_image = makeRandomSampleFromDistributionImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_sampled_image)\n\n    assert nsclcCTImage.GetSize() == randomized_sampled_image.GetSize(), \\\n        \"Randomized sampled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_sampled_image.GetSpacing(), \\\n        \"Randomized sampled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_sampled_image.GetOrigin(), \\\n        \"Randomized sampled image origin not same as input image\"\n    assert isinstance(randomized_sampled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_arr_image, randomized_arr_image), \\\n        \"No voxel values have been changed to random.\"\n    assert np.all(np.isin(randomized_arr_image.flatten(), original_arr_image.flatten())), \\\n        \"Retuned object has values not sampled from original image\"\n    assert randomized_arr_image[0,0,0] == -1005, \\\n        \"Random seed is not working for randomized from distribution image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[-1,-1,-1] == 414, \\\n        \"Random seed is not working for randomized from distribution image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[238,252,124] == 49, \\\n        \"Random seed is not working for randomized from distribution image, central ROI voxel has wrong random value. Random seed should be 10.\"\n\n\n\ndef test_makeShuffleROI(nsclcCropped, randomSeed):\n    \" Test negative control to shuffle the roi of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    shuffled_roi_image = negativeControlROIOnly(croppedCT, croppedROI, \"shuffled\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    shuffled_roi_pixels = sitk.GetArrayFromImage(shuffled_roi_image)\n\n    assert croppedCT.GetSize() == shuffled_roi_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert croppedCT.GetSpacing() == shuffled_roi_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == shuffled_roi_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_roi_pixels), \\\n        \"No voxel values are being shuffled.\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_roi_pixels.flatten())), \\\n        \"Shuffled pixel values in ROI are different\"\n    assert shuffled_roi_pixels[0,0,0] == -740, \\\n        \"Voxel outside the ROI is being shuffled. Should just be the ROI voxels.\"\n    assert shuffled_roi_pixels[7,18,11] == -322, \\\n        \"Random seed is not working for shuffled ROI image, centre pixel has wrong value. Random seed should be 10.\"\n\n\ndef test_makeRandomROI(nsclcCropped, randomSeed):\n    \" Test negative control to randomize the pixels of the ROI of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    randomized_roi_image = negativeControlROIOnly(croppedCT, croppedROI, \"randomized\", randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(croppedCT)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n    assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_arr_image, randomized_roi_image), \\\n        \"No voxel values have been changed to random.\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    assert randomized_arr_image[0,0,0] == -740, \\\n        \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n    assert randomized_arr_image[7,18,11] == 1, \\\n        \"Random seed is not working for randomized ROI image, centre pixel has wrong value. Random seed should be 10.\"\n\n\ndef test_makeRandomSampleFromDistributionROI(nsclcCropped, randomSeed):\n    \" Test negative control to uniformly sample the pixels of the ROI of the image randomly\"\n\n    croppedCT, croppedROI, _= nsclcCropped\n\n    randomized_roi_image = negativeControlROIOnly(croppedCT, croppedROI, \"randomized_sampled\", randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(croppedCT)\n\n    randomized_roi_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n    assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n        \"Randomized ROI Sampled image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n        \"Randomized ROI Sampled image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n        \"Randomized ROI Sampled image origin not same as input image\"\n    assert isinstance(randomized_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_arr_image, randomized_roi_arr_image), \\\n        \"No voxel values have been changed to random.\"\n    assert np.all(np.isin(randomized_roi_arr_image.flatten(), original_arr_image.flatten())), \\\n        \"Retuned object has values not sampled from original image\"\n    assert randomized_roi_arr_image[0,0,0] == -740, \\\n        \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n    assert randomized_roi_arr_image[7,18,11] == -81, \\\n        \"Random seed is not working for randomized from distribution ROI image, centre pixel has wrong value. Random seed should be 10.\"\n    \n\ndef test_makeShuffleNonROI(nsclcCropped, randomSeed):\n    \" Test negative control to shuffle the pixels outside roi of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    shuffled_non_roi_image = negativeControlNonROIOnly(croppedCT, croppedROI, \"shuffled\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    shuffled_non_roi_pixels = sitk.GetArrayFromImage(shuffled_non_roi_image)\n\n    assert croppedCT.GetSize() == shuffled_non_roi_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert croppedCT.GetSpacing() == shuffled_non_roi_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == shuffled_non_roi_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_non_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_non_roi_pixels), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_non_roi_pixels.flatten())), \\\n        \"Shuffled pixel values outside ROI are different\"\n    assert shuffled_non_roi_pixels[0,0,0] == -840, \\\n        \"Random seed is not working for shuffled non-ROI image, first voxel has wrong shuffle value. Random seed should be 10.\"\n    assert shuffled_non_roi_pixels[-1,-1,-1] == -490, \\\n        \"Random seed is not working for shuffled non-ROI image, last voxel has wrong shuffle value. Random seed should be 10.\"\n    assert shuffled_non_roi_pixels[7,18,11] == -1, \\\n        \"ROI is getting shuffled when it shouldn't.\"\n\n\ndef test_makeRandomNonRoi(nsclcCropped, randomSeed):\n    \" Test negative control to randomize the pixels outside the ROI of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    randomized_non_roi_image = negativeControlNonROIOnly(croppedCT, croppedROI, \"randomized\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n    # Make ROI pixels = NaN to help check of min and max values in non-ROI pixels\n    original_roi_pixels = sitk.GetArrayFromImage(croppedROI)\n    roiMask = np.where(original_roi_pixels > 0, np.NaN, 1)\n    masked_randomized_non_roi_pixels = randomized_non_roi_pixels * roiMask\n    \n    assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_non_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert np.nanmax(masked_randomized_non_roi_pixels) <= np.max(original_pixels) and np.nanmin(masked_randomized_non_roi_pixels) >= np.min(original_pixels), \\\n        \"Pixel values outside ROI are not within the expected range\"\n    assert randomized_non_roi_pixels[0,0,0] == -124, \\\n        \"Random seed is not working for randomized non-ROI image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[-1,-1,-1] == 123, \\\n        \"Random seed is not working for randomized non-ROI image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[7,18,11] == -1, \\\n        \"ROI is getting randomized when it shouldn't.\"\n\n\ndef test_makeRandomNonRoiFromDistribution(nsclcCropped, randomSeed):\n    \" Test negative control to randomize the pixels outside the ROI of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    randomized_non_roi_image = negativeControlNonROIOnly(croppedCT, croppedROI, \"randomized_sampled\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n    assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_non_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, randomized_non_roi_pixels), \\\n        \"No voxel values have been changed to random.\"\n    assert np.all(np.isin(randomized_non_roi_pixels, original_pixels)), \\\n        \"Retuned object has values outside ROI not sampled from original image\"\n    assert randomized_non_roi_pixels[0,0,0] == -653, \\\n        \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[-1,-1,-1] == -805, \\\n        \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[7,18,11] == -1, \\\n        \"ROI is getting randomized when it shouldn't.\"\n    \n\n@pytest.mark.parametrize(\n    \"wrongNCType\",\n    [\n        \"shaken\",\n        \"stirred\",\n        \"\"\n    ]\n)\ndef test_negativeControlROIOnly_wrongNCType(nsclcCropped, wrongNCType):\n    \" Test passing wrong negative control type to negativeControlROIOnly\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    with pytest.raises(ValueError):\n        negativeControlROIOnly(croppedCT, croppedROI, wrongNCType, randomSeed=10)\n\n\n@pytest.mark.parametrize(\n    \"wrongNCType\",\n    [\n        \"shaken\",\n        \"stirred\",\n        \"\"\n    ]\n)\ndef test_negativeControlNonROIOnly_wrongNCType(nsclcCropped, wrongNCType):\n    \" Test passing wrong negative control type to negativeControlNonROIOnly\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    with pytest.raises(ValueError):\n        negativeControlNonROIOnly(croppedCT, croppedROI, wrongNCType, randomSeed=10)\n\n\n# def test_noROILabel_shuffleROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to shuffleROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     shuffled_roi_image = negativeControlROIOnly(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     shuffled_roi_pixels = sitk.GetArrayFromImage(shuffled_roi_image)\n\n#     assert croppedCT.GetSize() == shuffled_roi_image.GetSize(), \\\n#         \"Shuffled image size not same as input image\"\n#     assert croppedCT.GetSpacing() == shuffled_roi_image.GetSpacing(), \\\n#         \"Shuffled image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == shuffled_roi_image.GetOrigin(), \\\n#         \"Shuffled image origin not same as input image\"\n#     assert isinstance(shuffled_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_pixels, shuffled_roi_pixels), \\\n#         \"No voxel values are being shuffled.\"\n#     assert np.array_equal(np.sort(original_pixels.flatten()),\n#                           np.sort(shuffled_roi_pixels.flatten())), \\\n#         \"Shuffled pixel values in ROI are different\"\n#     assert shuffled_roi_pixels[0,0,0] == -740, \\\n#         \"Voxel outside the ROI is being shuffled. Should just be the ROI voxels.\"\n#     assert shuffled_roi_pixels[7,18,11] == -100, \\\n#         \"Random seed is not working for shuffled ROI image, centre pixel has wrong value. Random seed should be 10.\"\n    \n\n# def test_noROILabel_randomROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_roi_image = makeRandomRoi(croppedCT, croppedROI, randomSeed=randomSeed)\n#     original_arr_image = sitk.GetArrayFromImage(croppedCT)\n#     minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n#     randomized_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n#     assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n#         \"Randomized image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n#         \"Randomized image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n#         \"Randomized image origin not same as input image\"\n#     assert isinstance(randomized_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_arr_image, randomized_roi_image), \\\n#         \"No voxel values have been changed to random.\"\n#     assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n#         \"Pixel values are not within the expected range\"\n#     assert randomized_arr_image[0,0,0] == -740, \\\n#         \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n#     assert randomized_arr_image[7,18,11] == -400, \\\n#         \"Random seed is not working for randomized ROI image, centre pixel has wrong value. Random seed should be 10.\"\n\n\n# def test_noROILabel_randomROIFromDist(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomFromRoiDistribution\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_roi_image = makeRandomFromRoiDistribution(croppedCT, croppedROI, randomSeed=randomSeed)\n#     original_arr_image = sitk.GetArrayFromImage(croppedCT)\n\n#     randomized_roi_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n#     assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n#         \"Randomized ROI Sampled image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n#         \"Randomized ROI Sampled image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n#         \"Randomized ROI Sampled image origin not same as input image\"\n#     assert isinstance(randomized_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_arr_image, randomized_roi_arr_image), \\\n#         \"No voxel values have been changed to random.\"\n#     assert np.all(np.isin(randomized_roi_arr_image.flatten(), original_arr_image.flatten())), \\\n#         \"Retuned object has values not sampled from original image\"\n#     assert randomized_roi_arr_image[0,0,0] == -740, \\\n#         \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n#     assert randomized_roi_arr_image[7,18,11] == -5, \\\n#         \"Random seed is not working for randomized from distribution ROI image, centre pixel has wrong value. Random seed should be 10.\"\n    \n\n# def test_noROILabel_shuffleNonROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to shuffleNonROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     shuffled_non_roi_image = shuffleNonROI(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     shuffled_non_roi_pixels = sitk.GetArrayFromImage(shuffled_non_roi_image)\n\n#     assert croppedCT.GetSize() == shuffled_non_roi_image.GetSize(), \\\n#         \"Shuffled image size not same as input image\"\n#     assert croppedCT.GetSpacing() == shuffled_non_roi_image.GetSpacing(), \\\n#         \"Shuffled image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == shuffled_non_roi_image.GetOrigin(), \\\n#         \"Shuffled image origin not same as input image\"\n#     assert isinstance(shuffled_non_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_pixels, shuffled_non_roi_pixels), \\\n#         \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n#     assert np.array_equal(np.sort(original_pixels.flatten()),\n#                           np.sort(shuffled_non_roi_pixels.flatten())), \\\n#         \"Shuffled pixel values outside ROI are different\"\n#     assert shuffled_non_roi_pixels[0,0,0] == 54, \\\n#         \"Random seed is not working for shuffled non-ROI image, first voxel has wrong shuffle value. Random seed should be 10.\"\n#     assert shuffled_non_roi_pixels[-1,-1,-1] == -617, \\\n#         \"Random seed is not working for shuffled non-ROI image, last voxel has wrong shuffle value. Random seed should be 10.\"\n#     assert shuffled_non_roi_pixels[7,18,11] == -1, \\\n#         \"ROI is getting shuffled when it shouldn't.\"\n    \n    \n# def test_noROILabel_randomNonROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomNonROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_non_roi_image = makeRandomNonRoi(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n#     # Make ROI pixels = NaN to help check of min and max values in non-ROI pixels\n#     original_roi_pixels = sitk.GetArrayFromImage(croppedROI)\n#     roiMask = np.where(original_roi_pixels > 0, np.NaN, 1)\n#     masked_randomized_non_roi_pixels = randomized_non_roi_pixels * roiMask\n    \n#     assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n#         \"Randomized image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n#         \"Randomized image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n#         \"Randomized image origin not same as input image\"\n#     assert isinstance(randomized_non_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert np.nanmax(masked_randomized_non_roi_pixels) <= np.max(original_pixels) and np.nanmin(masked_randomized_non_roi_pixels) >= np.min(original_pixels), \\\n#         \"Pixel values outside ROI are not within the expected range\"\n#     assert randomized_non_roi_pixels[0,0,0] == -124, \\\n#         \"Random seed is not working for randomized non-ROI image, first voxel has wrong random value. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[-1,-1,-1] == 123, \\\n#         \"Random seed is not working for randomized non-ROI image, last voxel has wrong random value. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[7,18,11] == -1, \\\n#         \"ROI is getting randomized when it shouldn't.\"\n    \n\n# def test_noROILabel_randomNonROIFromDist(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomNonRoiFromDistribution\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_non_roi_image = makeRandomNonRoiFromDistribution(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n#     assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n#         \"Randomized image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n#         \"Randomized image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n#         \"Randomized image origin not same as input image\"\n#     assert isinstance(randomized_non_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_pixels, randomized_non_roi_pixels), \\\n#         \"No voxel values have been changed to random.\"\n#     assert np.all(np.isin(randomized_non_roi_pixels, original_pixels)), \\\n#         \"Retuned object has values outside ROI not sampled from original image\"\n#     assert randomized_non_roi_pixels[0,0,0] == -709, \\\n#         \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[-1,-1,-1] == -830, \\\n#         \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[7,18,11] == -1, \\\n#         \"ROI is getting randomized when it shouldn't.\"",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `negativeControlROIOnly` function in this code, and how does it handle different types of negative controls?",
        "answer": "The `negativeControlROIOnly` function applies negative controls to the Region of Interest (ROI) in an image. It handles three types of negative controls: 'shuffled', 'randomized', and 'randomized_sampled'. For 'shuffled', it shuffles the pixel values within the ROI. For 'randomized', it replaces ROI pixels with random values within the image's range. For 'randomized_sampled', it samples new values for ROI pixels from the original image's distribution. The function preserves the image's size, spacing, and origin while only modifying the ROI pixels."
      },
      {
        "question": "How does the code ensure that the random seed is working correctly in the negative control functions?",
        "answer": "The code ensures the random seed is working correctly by checking specific pixel values after applying the negative controls. For example, in the `test_makeShuffleImage` function, it asserts that the first voxel should be -987, the last voxel should be 10, and a specific central ROI voxel should be -118 when the random seed is set to 10. Similar checks are performed in other test functions for different negative control types, verifying that the randomization or shuffling produces consistent results when the same seed is used."
      },
      {
        "question": "What is the purpose of the `applyNegativeControl` function, and how does it differ from the other negative control functions in the code?",
        "answer": "The `applyNegativeControl` function is not explicitly shown in the provided code snippet, but based on the context, it likely serves as a higher-level function that applies various negative controls to an image. It would differ from the other negative control functions (like `negativeControlROIOnly` and `negativeControlNonROIOnly`) by potentially offering a more flexible interface to apply different types of negative controls to either the ROI, non-ROI areas, or the entire image. This function would likely use the more specific negative control functions internally, providing a unified way to apply negative controls based on user-specified parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_makeShuffleImage(nsclcCTImage, randomSeed):\n    \"Test negative control to shuffle the whole image\"\n\n    shuffled_image = makeShuffleImage(nsclcCTImage, randomSeed)\n    original_pixels = sitk.GetArrayFromImage(nsclcCTImage)\n    shuffled_pixels = sitk.GetArrayFromImage(shuffled_image)\n\n    assert nsclcCTImage.GetSize() == shuffled_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == shuffled_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == shuffled_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_pixels), \\\n        \"Pixel values are not shuffled\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_pixels.flatten())), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    # Add assertions for specific pixel values",
        "complete": "def test_makeShuffleImage(nsclcCTImage, randomSeed):\n    \"Test negative control to shuffle the whole image\"\n\n    shuffled_image = makeShuffleImage(nsclcCTImage, randomSeed)\n    original_pixels = sitk.GetArrayFromImage(nsclcCTImage)\n    shuffled_pixels = sitk.GetArrayFromImage(shuffled_image)\n\n    assert nsclcCTImage.GetSize() == shuffled_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == shuffled_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == shuffled_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_pixels), \\\n        \"Pixel values are not shuffled\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_pixels.flatten())), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    assert shuffled_pixels[0,0,0] == -987, \\\n        \"Random seed is not working for shuffled image, first voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[-1,-1,-1] == 10, \\\n        \"Random seed is not working for shuffled image, last voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[238,252,124] == -118, \\\n        \"Random seed is not working for shuffled image, central ROI voxel has wrong shuffled value. Random seed should be 10.\""
      },
      {
        "partial": "def test_makeRandomImage(nsclcCTImage, randomSeed):\n    \"Test negative control to randomize the pixels of the whole image\"\n\n    randomized_image = makeRandomImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_image)\n\n    assert nsclcCTImage.GetSize() == randomized_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    # Add assertions for specific pixel values",
        "complete": "def test_makeRandomImage(nsclcCTImage, randomSeed):\n    \"Test negative control to randomize the pixels of the whole image\"\n\n    randomized_image = makeRandomImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_image)\n\n    assert nsclcCTImage.GetSize() == randomized_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    assert randomized_arr_image[0,0,0] == 2156, \\\n        \"Random seed is not working for random image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[-1,-1,-1] == 90, \\\n        \"Random seed is not working for random image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[238,252,124] == -840, \\\n        \"Random seed is not working for random image, central ROI voxel has wrong random value. Random seed should be 10.\""
      }
    ],
    "dependencies": {
      "imports": [
        "numpy",
        "pytest",
        "collections"
      ],
      "from_imports": [
        "readii.image_processing.*",
        "radiomics.imageoperations",
        "readii.negative_controls.makeShuffleImage"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/data/__init__.py",
    "language": "py",
    "content": "",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `isValidSudoku` function and what data structure does it use to check for validity?",
        "answer": "The `isValidSudoku` function checks if a given 9x9 Sudoku board is valid. It uses a set data structure to keep track of seen numbers in each row, column, and 3x3 sub-box. The function returns true if the board is valid and false otherwise."
      },
      {
        "question": "How does the function handle checking for duplicates in the 3x3 sub-boxes of the Sudoku board?",
        "answer": "The function uses integer division and modulo operations to map each cell to its corresponding 3x3 sub-box. The formula `(i//3)*3 + j//3` calculates the index of the sub-box (0-8) for each cell. This index is then used with the current number to create a unique key for the set, ensuring that duplicates within each 3x3 sub-box are detected."
      },
      {
        "question": "What is the time and space complexity of the `isValidSudoku` function?",
        "answer": "The time complexity of the `isValidSudoku` function is O(1) because it always iterates over a fixed 9x9 board, resulting in 81 operations. The space complexity is also O(1) because the set used to store seen numbers will have at most 3 * 81 = 243 elements (one for each cell in rows, columns, and sub-boxes), which is a constant regardless of input size."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def fibonacci(n):\n    if n <= 1:\n        return n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        # Complete the loop body\n    return b",
        "complete": "def fibonacci(n):\n    if n <= 1:\n        return n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    return b"
      },
      {
        "partial": "def fibonacci(n):\n    # Implement the function using recursion with memoization\n    pass",
        "complete": "def fibonacci(n):\n    memo = {0: 0, 1: 1}\n    def fib(n):\n        if n not in memo:\n            memo[n] = fib(n-1) + fib(n-2)\n        return memo[n]\n    return fib(n)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/rankGeneDrugSensitivity.R",
    "language": "R",
    "content": "#' @importFrom stats complete.cases\n#' @importFrom stats p.adjust\n\n#################################################\n## Rank genes based on drug effect in the Connectivity Map\n##\n## inputs:\n##      - data: gene expression data matrix\n##            - drugpheno: sensititivity values fo thr drug of interest\n##            - type: cell or tissue type for each experiment\n##            - duration: experiment duration in hours\n##      - batch: experiment batches\n##            - single.type: Should the statitsics be computed for each cell/tissue type separately?\n##      - nthread: number of parallel threads (bound to the maximum number of cores available)\n##\n## outputs:\n## list of datafraes with the statistics for each gene, for each type\n##\n## Notes:    duration is not taken into account as only 4 perturbations lasted 12h, the other 6096 lasted 6h\n#################################################\n\nrankGeneDrugSensitivity <- function (data, drugpheno, type, batch,\n                                     single.type=FALSE, standardize = \"SD\",\n                                     nthread=1, verbose=FALSE,\n                                     modeling.method = c(\"anova\", \"pearson\"),\n                                     inference.method = c(\"analytic\", \"resampling\"), req_alpha = 0.05) {\n\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if ((missing(nthread) || nthread < 1 || nthread > availcore) && verbose) {\n      warning(\"nthread undefined, negative or larger than available cores. Resetting to maximum number of cores.\")\n      nthread <- availcore\n    }\n  }\n\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if(modeling.method == \"anova\" && inference.method == \"resampling\") {\n    stop(\"Resampling based inference for anova model is not yet implemented.\")\n  }\n\n  if(is.null(dim(drugpheno))){\n\n    drugpheno <- data.frame(drugpheno)\n\n  } else if(!is(drugpheno, \"data.frame\")) {\n    drugpheno <- as.data.frame(drugpheno)\n\n  }\n\n  if (missing(type) || all(is.na(type))) {\n    type <- array(\"other\", dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (missing(batch) || all(is.na(batch))) {\n    batch <- array(1, dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, duration, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  res <- NULL\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n  res <- NULL\n  ccix <- complete.cases(data, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n\n  if(modeling.method == \"anova\"){\n    if(!any(unlist(lapply(drugpheno,is.factor)))){\n      if(ncol(drugpheno)>1){\n        ##### FIX NAMES!!! This is important\n        nc <- lapply(seq_len(ncol(drugpheno)), function(i){\n\n          est <- paste(\"estimate\", i, sep=\".\")\n          se <-  paste(\"se\", i, sep=\".\")\n          tstat <- paste(\"tstat\", i, sep=\".\")\n\n          nc <- c(est, se, tstat)\n          return(nc)\n\n        })\n        nc  <- c(nc, n=nn, \"fstat\"=NA, \"pvalue\"=NA, \"fdr\")\n      } else {\n        nc  <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\")\n      }\n    } else {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\")\n    }\n  } else if (modeling.method == \"pearson\") {\n    nc <- c(\"estimate\", \"n\", \"df\", \"significant\", \"pvalue\", \"lower\", \"upper\")\n  }\n\n  for (ll in seq_len(length(ltype))) {\n    iix <- !is.na(type) & is.element(type, ltype[[ll]])\n    # ccix <- complete.cases(data[iix, , drop=FALSE], drugpheno[iix,,drop=FALSE], type[iix], batch[iix]) ### HACK???\n\n    ccix <- rowSums(!is.na(data)) > 0 | rowSums(!is.na(drugpheno)) > 0 | is.na(type) | is.na(batch)\n    ccix <- ccix[iix]\n    # ccix <- !vapply(seq_len(NROW(data[iix,,drop=FALSE])), function(x) {\n    #   return(all(is.na(data[iix,,drop=FALSE][x,])) || all(is.na(drugpheno[iix,,drop=FALSE][x,])) || all(is.na(type[iix][x])) || all(is.na(batch[iix][x])))\n    # }, FUN.VALUE=logical(1))\n\n    if (sum(ccix) < 3) {\n      ## not enough experiments\n      rest <- list(matrix(NA, nrow=ncol(data), ncol=length(nc), dimnames=list(colnames(data), nc)))\n      res <- c(res, rest)\n    } else {\n      # splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n      # splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n      mcres <- parallel::mclapply(seq_len(ncol(data)), function(x, data, type, batch, drugpheno, standardize, modeling.method, inference.method, req_alpha) {\n        if(modeling.method == \"anova\"){\n          res <- t(apply(data[ , x, drop=FALSE], 2, geneDrugSensitivity, type=type, batch=batch, drugpheno=drugpheno, verbose=verbose, standardize=standardize))\n        } else if(modeling.method == \"pearson\") {\n          if(!is.character(data)){\n            res <- t(apply(data[ , x, drop=FALSE], 2, geneDrugSensitivityPCorr,\n                                                      type=type,\n                                                      batch=batch,\n                                                      drugpheno=drugpheno,\n                                                      verbose=verbose,\n                                                      test=inference.method,\n                                                      req_alpha = req_alpha))\n          } else {\n            res <- t(apply(data[ , x, drop=FALSE], 2, function(dataIn) {\n              geneDrugSensitivityPBCorr(as.factor(dataIn),\n                                                      type=type,\n                                                      batch=batch,\n                                                      drugpheno=drugpheno,\n                                                      verbose=verbose,\n                                                      test=inference.method,\n                                                      req_alpha = req_alpha)}))\n          }\n\n        }\n\n\n        return(res)\n      }, data=data[iix, , drop=FALSE],\n         type=type[iix], batch=batch[iix],\n         drugpheno=drugpheno[iix,,drop=FALSE],\n         standardize=standardize,\n         modeling.method = modeling.method,\n         inference.method = inference.method,\n         req_alpha = req_alpha, mc.cores = nthread, mc.preschedule = TRUE)\n      rest <- do.call(rbind, mcres)\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      # rest <- rest[ , nc, drop=FALSE]\n      res <- c(res, list(rest))\n    }\n  }\n  names(res) <- names(ltype)\n  return(res)\n}\n\n## End\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `rankGeneDrugSensitivity` function, and what are its key inputs?",
        "answer": "The `rankGeneDrugSensitivity` function is designed to rank genes based on drug effect in the Connectivity Map. Its key inputs are:\n1. `data`: gene expression data matrix\n2. `drugpheno`: sensitivity values for the drug of interest\n3. `type`: cell or tissue type for each experiment\n4. `batch`: experiment batches\n5. `single.type`: boolean to determine if statistics should be computed for each cell/tissue type separately\n6. `nthread`: number of parallel threads for computation"
      },
      {
        "question": "How does the function handle different modeling and inference methods, and what happens if an invalid combination is chosen?",
        "answer": "The function supports two modeling methods ('anova' and 'pearson') and two inference methods ('analytic' and 'resampling'). These are selected using `match.arg()` to ensure valid inputs. If 'anova' modeling is combined with 'resampling' inference, the function will stop with an error message: 'Resampling based inference for anova model is not yet implemented.' This prevents users from selecting an unsupported combination of methods."
      },
      {
        "question": "How does the function handle parallel processing, and what precautions are taken to ensure proper use of available cores?",
        "answer": "The function uses parallel processing through the `parallel::mclapply()` function. It takes the following precautions:\n1. It checks the available cores using `parallel::detectCores()`.\n2. If `nthread` is undefined, negative, or larger than available cores, it issues a warning and resets `nthread` to the maximum number of cores.\n3. The `mclapply()` function is called with `mc.cores = nthread` to utilize the specified number of threads.\n4. If `nthread` is set to 1, the function will not attempt parallel processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rankGeneDrugSensitivity <- function (data, drugpheno, type, batch,\n                                     single.type=FALSE, standardize = \"SD\",\n                                     nthread=1, verbose=FALSE,\n                                     modeling.method = c(\"anova\", \"pearson\"),\n                                     inference.method = c(\"analytic\", \"resampling\"), req_alpha = 0.05) {\n  # Input validation\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if(modeling.method == \"anova\" && inference.method == \"resampling\") {\n    stop(\"Resampling based inference for anova model is not yet implemented.\")\n  }\n\n  # Prepare data\n  drugpheno <- if(is.null(dim(drugpheno))) data.frame(drugpheno) else as.data.frame(drugpheno)\n  type <- if(missing(type) || all(is.na(type))) array(\"other\", dim=nrow(data), dimnames=list(rownames(data))) else type\n  batch <- if(missing(batch) || all(is.na(batch))) array(1, dim=nrow(data), dimnames=list(rownames(data))) else batch\n\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  # Set up types\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  # Main processing\n  res <- lapply(ltype, function(current_type) {\n    # TODO: Implement main processing logic here\n  })\n\n  names(res) <- names(ltype)\n  return(res)\n}",
        "complete": "rankGeneDrugSensitivity <- function (data, drugpheno, type, batch,\n                                     single.type=FALSE, standardize = \"SD\",\n                                     nthread=1, verbose=FALSE,\n                                     modeling.method = c(\"anova\", \"pearson\"),\n                                     inference.method = c(\"analytic\", \"resampling\"), req_alpha = 0.05) {\n  # Input validation\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if(modeling.method == \"anova\" && inference.method == \"resampling\") {\n    stop(\"Resampling based inference for anova model is not yet implemented.\")\n  }\n\n  # Prepare data\n  drugpheno <- if(is.null(dim(drugpheno))) data.frame(drugpheno) else as.data.frame(drugpheno)\n  type <- if(missing(type) || all(is.na(type))) array(\"other\", dim=nrow(data), dimnames=list(rownames(data))) else type\n  batch <- if(missing(batch) || all(is.na(batch))) array(1, dim=nrow(data), dimnames=list(rownames(data))) else batch\n\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  # Set up types\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  # Main processing\n  res <- lapply(ltype, function(current_type) {\n    iix <- !is.na(type) & is.element(type, current_type)\n    ccix <- complete.cases(data[iix, , drop=FALSE], drugpheno[iix, , drop=FALSE], type[iix], batch[iix])\n    \n    if (sum(ccix) < 3) {\n      return(matrix(NA, nrow=ncol(data), ncol=8, dimnames=list(colnames(data), c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\"))))\n    }\n    \n    mcres <- parallel::mclapply(seq_len(ncol(data)), function(x) {\n      if (modeling.method == \"anova\") {\n        geneDrugSensitivity(data[iix, x], type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix, , drop=FALSE], verbose=verbose, standardize=standardize)\n      } else {\n        geneDrugSensitivityPCorr(data[iix, x], type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix, , drop=FALSE], verbose=verbose, test=inference.method, req_alpha=req_alpha)\n      }\n    }, mc.cores = nthread, mc.preschedule = TRUE)\n    \n    rest <- do.call(rbind, mcres)\n    rest <- cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n    return(rest)\n  })\n\n  names(res) <- names(ltype)\n  return(res)\n}"
      },
      {
        "partial": "geneDrugSensitivity <- function(geneExpr, type, batch, drugpheno, verbose=FALSE, standardize=\"SD\") {\n  # TODO: Implement function body\n}",
        "complete": "geneDrugSensitivity <- function(geneExpr, type, batch, drugpheno, verbose=FALSE, standardize=\"SD\") {\n  if (standardize == \"SD\") {\n    geneExpr <- scale(geneExpr)\n  }\n  \n  df <- data.frame(geneExpr=geneExpr, type=type, batch=batch, drugpheno=drugpheno)\n  model <- lm(geneExpr ~ type + batch + drugpheno, data=df)\n  \n  coef <- summary(model)$coefficients\n  drugpheno_coef <- coef[\"drugpheno\", ]\n  \n  result <- c(\n    estimate = drugpheno_coef[\"Estimate\"],\n    se = drugpheno_coef[\"Std. Error\"],\n    n = nrow(df),\n    tstat = drugpheno_coef[\"t value\"],\n    fstat = summary(model)$fstatistic[1],\n    pvalue = drugpheno_coef[\"Pr(>|t|)\"],\n    df = summary(model)$df[2]\n  )\n  \n  if (verbose) {\n    print(summary(model))\n  }\n  \n  return(result)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/datagraph.py",
    "language": "py",
    "content": "import os\nimport time\nimport pathlib\nfrom typing import List\nfrom functools import reduce\nimport numpy as np\nimport pandas as pd\n\n\nclass DataGraph:\n    '''\n    This class given the crawled dataset in the form of CSV file, deals with forming a graph on the full dataset, taking advantage of connections between different modalities. Based\n    on these connections, an edge table is made. This class also supports querying and for a given query, returns the file locations of the user-specified sub-dataset from the full dataset.\n    The graph is made based on the references made by different DICOMS. Different connections are given different edge type values, to make parsing easier. The edge types are as follows:-\n    1) edge_type:0 RTDOSE(key:ref_rt) -> RTSTRUCT(pair: series/instance)\n    2) edge_type:1 RTDOSE(key:ref_ct) -> CT(pair: series) \n    3) edge_type:2 RTSTRUCT(key:ref_ct) -> CT(pair: series) \n    4) edge_type:3 RTSTRUCT(key:ref_ct) -> PT(pair: series) \n    5) edge_type:4 CT(key:study) -> PT(pair: study) \n    6) edge_type:5 RTDOSE(key: ref_pl) -> RTPLAN(pair: instance)\n    7) edge_type:6 RTPLAN(key: ref_rs) -> RTSTRUCT(pair: series/instance)\n    8) edge_type:7 SEG(key:ref_ct) -> CT(pair: series)\n\n    Once the edge table is formed, one can query on the graph to get the desired results. For uniformity, the supported query is list of modalities to consider\n    For ex:\n    query = [\"CT\",\"RTDOSE\",\"RTSTRUCT\",\"PT], will return interconnected studies containing the listed DICOM modalities. The interconnected studies for example may look like \n    (RTDOSE->RTSTRUCT->CT<-PT<-RTSTRUCT)\n    '''\n    def __init__(self,\n                 path_crawl: str,\n                 edge_path: str = \"./patient_id_full_edges.csv\",\n                 visualize: bool = False) -> None:\n        '''\n        Parameters\n        ----------\n        path_crawl\n            The csv returned by the crawler\n\n        edge_path\n            This path denotes where the graph in the form of edge table is stored or to be stored\n        '''\n        self.df = pd.read_csv(path_crawl, index_col=0)\n        self.edge_path = edge_path\n        self.df_new = None\n        if os.path.exists(self.edge_path):\n            print(\"Edge table is already present. Loading the data...\")\n            self.df_edges = pd.read_csv(self.edge_path)\n        else:\n            print(\"Edge table not present. Forming the edge table based on the crawl data...\")\n            self.form_graph()\n        if visualize:\n            self.visualize_graph()\n    \n    def form_graph(self):\n        '''\n        Forms edge table based on the crawled data\n        '''\n        # enforce string type to all columns to prevent dtype merge errors for empty columns\n        for col in self.df:\n            self.df[col] = self.df[col].astype(str)\n        \n        #Get reference_rs information from RTDOSE-RTPLAN connections    \n        df_filter = pd.merge(self.df, self.df[[\"instance_uid\",\"reference_rs\"]].apply(lambda x: x.astype(str), axis=1), \n                             left_on=\"reference_pl\", \n                             right_on=\"instance_uid\", \n                             how=\"left\")\n        \n        df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_x\"] = df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_y\"].values\n        df_filter.drop(columns=[\"reference_rs_y\", \"instance_uid_y\"], inplace=True)\n        df_filter.rename(columns={\"reference_rs_x\":\"reference_rs\", \"instance_uid_x\":\"instance_uid\"}, inplace=True)\n        \n        # Remove entries with no RTDOSE reference, for extra check, such cases are mostprobably removed in the earlier step\n        df_filter = df_filter.loc[~((df_filter[\"modality\"] == \"RTDOSE\") & (df_filter[\"reference_ct\"].isna()) & (df_filter[\"reference_rs\"].isna()))]\n\n        # Get all study ids\n        # all_study = df_filter.study.unique()\n        start = time.time()\n\n        # Defining Master df to store all the Edge dataframes\n        # self.df_master = []\n\n        # for i in tqdm(range(len(all_study))):\n        #     self._form_edge_study(df_filter, all_study, i)\n\n        # df_edge_patient = form_edge_study(df,all_study,i)\n        \n        self.df_edges = self._form_edges(self.df)  # pd.concat(self.df_master, axis=0, ignore_index=True)\n        end = time.time()\n        print(f\"\\nTotal time taken: {end - start}\")\n\n\n        self.df_edges.loc[self.df_edges.study_x.isna(),\"study_x\"] = self.df_edges.loc[self.df_edges.study_x.isna(), \"study\"]\n        # dropping some columns\n        self.df_edges.drop(columns=[\"study_y\", \"patient_ID_y\", \"series_description_y\", \"study_description_y\", \"study\"],inplace=True)\n        self.df_edges.sort_values(by=\"patient_ID_x\", ascending=True)\n        print(f\"Saving edge table in {self.edge_path}\")\n        self.df_edges.to_csv(self.edge_path, index=False)\n\n    def visualize_graph(self):\n        \"\"\"\n        Generates visualization using Pyviz, a wrapper around visJS. The visualization can be found at datanet.html\n        \"\"\"\n        from pyvis.network import Network  # type: ignore (PyLance)\n        print(\"Generating visualizations...\")\n        data_net = Network(height='100%', width='100%', bgcolor='#222222', font_color='white')\n\n        sources = self.df_edges[\"series_y\"]\n        targets = self.df_edges[\"series_x\"]\n        name_src = self.df_edges[\"modality_y\"] \n        name_tar = self.df_edges[\"modality_x\"]\n        patient_id = self.df_edges[\"patient_ID_x\"]\n        reference_ct = self.df_edges[\"reference_ct_y\"]\n        reference_rs = self.df_edges[\"reference_rs_y\"]\n\n        data_zip = zip(sources,targets,name_src,name_tar,patient_id,reference_ct,reference_rs)\n\n        for i in data_zip:\n            data_net.add_node(i[0],i[2],title=i[2],group=i[4])\n            data_net.add_node(i[1],i[3],title=i[3],group=i[4])\n            data_net.add_edge(i[0],i[1])\n            node = data_net.get_node(i[0])\n            node[\"title\"] = \"<br>Patient_id: {}<br>Series: {}<br>reference_ct: {}<br>reference_rs: {}\".format(i[4],i[0],i[5],i[6])\n            node = data_net.get_node(i[1])\n            node[\"title\"] = \"<br>Patient_id: {}<br>Series: {}<br>reference_ct: {}<br>reference_rs: {}\".format(i[4],i[1],i[5],i[6])\n\n        neigbour_map = data_net.get_adj_list()\n        for node in data_net.nodes:\n            node[\"title\"] += \"<br>Number of connections: {}\".format(len(neigbour_map[node['id']])) \n            node[\"value\"] = len(neigbour_map[node['id']])\n\n        vis_path = pathlib.Path(os.path.dirname(self.edge_path),\"datanet.html\").as_posix()\n        data_net.show(vis_path)\n\n    def _form_edges(self, df):\n        '''\n        For a given study id forms edge table\n        '''\n\n        df_list = []\n\n        # Split into each modality\n        plan = df[df[\"modality\"] == \"RTPLAN\"]\n        dose = df[df[\"modality\"] == \"RTDOSE\"]\n        struct = df[df[\"modality\"] == \"RTSTRUCT\"]\n        seg = df[df[\"modality\"] == \"SEG\"]\n        ct = df[df[\"modality\"] == \"CT\"]\n        mr = df[df[\"modality\"] == \"MR\"]\n        pet = df[df[\"modality\"] == \"PT\"]\n\n        edge_types = np.arange(7)\n        for edge in edge_types:\n            if edge==0:    # FORMS RTDOSE->RTSTRUCT, can be formed on both series and instance uid\n                df_comb1    = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2    = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                # Cases where both series and instance_uid are the same for struct\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n\n            elif edge==1:  # FORMS RTDOSE->CT \n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==2:  # FORMS RTSTRUCT->CT on ref_ct to series\n                df_ct = pd.merge(ct, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n\n            elif edge==3:  # FORMS RTSTRUCT->PET on ref_ct to series\n                df_combined = pd.merge(pet, struct, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==4:  # FORMS PET->CT on study\n                df_combined = pd.merge(ct, pet, left_on=\"study\", right_on=\"study\")\n\n            elif edge==5:  # FORMS RTPLAN->RTDOSE on ref_pl\n                df_combined = pd.merge(plan, dose, left_on=\"instance_uid\", right_on=\"reference_pl\")\n\n            elif edge==7:\n                df_ct = pd.merge(ct, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n\n            else:\n                df_combined = pd.merge(struct, plan, left_on=\"instance_uid\", right_on=\"reference_rs\")\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n\n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        return df_edges\n\n    def _form_edge_study(self, df, all_study, study_id):\n        '''\n        For a given study id forms edge table\n        '''\n        \n        df_study = df.loc[self.df[\"study\"] == all_study[study_id]]\n        df_list = []\n        \n        # Split into each modality\n        plan = df_study.loc[df_study[\"modality\"] == \"RTPLAN\"]\n        dose = df_study.loc[df_study[\"modality\"] == \"RTDOSE\"]\n        struct = df_study.loc[df_study[\"modality\"] == \"RTSTRUCT\"]\n        ct = df_study.loc[df_study[\"modality\"] == \"CT\"]\n        mr = df_study.loc[df_study[\"modality\"] == \"MR\"]\n        pet = df_study.loc[df_study[\"modality\"] == \"PT\"]\n        seg = df_study.loc[df_study[\"modality\"] == \"SEG\"]\n        edge_types = np.arange(8)\n\n        for edge in edge_types:\n            if edge==0:    # FORMS RTDOSE->RTSTRUCT, can be formed on both series and instance uid\n                df_comb1    = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2    = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                # Cases where both series and instance_uid are the same for struct\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n\n            elif edge==1:  # FORMS RTDOSE->CT \n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==2:  # FORMS RTSTRUCT->CT/MR on ref_ct to series\n                df_ct = pd.merge(ct, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n\n            elif edge==3:  # FORMS RTSTRUCT->PET on ref_ct to series\n                df_combined = pd.merge(pet, struct, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==4:  # FORMS PET->CT on study\n                df_combined = pd.merge(ct, pet, left_on=\"study\", right_on=\"study\")\n            \n            elif edge==5:  # FORMS RTPLAN->RTDOSE on ref_pl \n                df_combined = pd.merge(plan, dose, left_on=\"instance\", right_on=\"reference_pl\")\n\n            elif edge==7:  # FORMS SEG->CT/MR on ref_ct to series\n                df_ct_seg = pd.merge(ct, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr_seg = pd.merge(mr, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct_seg, df_mr_seg])\n\n            else:\n                df_combined = pd.merge(struct, plan, left_on=\"instance\", right_on=\"reference_rs\")\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n                \n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        self.df_master.append(df_edges)\n    \n    def parser(self, query_string: str) -> pd.DataFrame:\n        '''\n        For a given query string(Check the documentation), returns the dataframe consisting of two columns namely modality and folder location of the connected nodes\n        Parameters\n        ----------\n        df\n            Dataframe consisting of the crawled data\n        df_edges\n            Processed Dataframe forming a graph, stored in the form of edge table\n        query_string\n            Query string based on which dataset will be formed\n        \n        Query ideas:\n        There are four basic supported modalities are RTDOSE, RTSTRUCT, CT, PT, MRI\n        The options are, the string can be in any order:\n        1) RTDOSE\n        2) RTSTRUCT\n        3) CT\n        4) PT\n        5) PT,RTSTRUCT\n        6) CT,PT\n        7) CT,RTSTRUCT\n        8) CT,RTDOSE\n        9) RTDOSE,RTSTRUCT,CT\n        10) RTDOSE,CT,PT\n        11) RTSTRUCT,CT,PT\n        12) RTDOSE,RTSTRUCT,CT,PT\n        '''\n        # Basic processing of just one modality\n        supp_mods   = [\"RTDOSE\", \"RTSTRUCT\", \"CT\", \"PT\", 'MR', 'SEG']\n        edge_def    = {\"RTSTRUCT,RTDOSE\" : 0, \"CT,RTDOSE\" : 1, \"CT,RTSTRUCT\" : 2, \"PET,RTSTRUCT\" : 3, \"CT,PT\" : 4, 'MR,RTSTRUCT': 2, \"RTPLAN,RTSTRUCT\": 6, \"RTPLAN,RTDOSE\": 5, \"CT,SEG\": 7, \"MR,SEG\": 7}\n        self.mods   = query_string.split(\",\")\n        self.mods_n = len(self.mods)\n\n        # Deals with single node queries\n        if query_string in supp_mods:\n            final_df = self.df.loc[self.df.modality == query_string, [\"study\", \"patient_ID\", \"series\", \"folder\", \"subseries\"]]\n            final_df.rename(columns = {\"series\": f\"series_{query_string}\", \n                                       \"study\": f\"study_{query_string}\", \n                                       \"folder\": f\"folder_{query_string}\",\n                                       \"subseries\": f\"subseries_{query_string}\", }, inplace=True)\n        \n        elif self.mods_n == 2:\n            # Reverse the query string\n            query_string_rev = (\",\").join(self.mods[::-1])\n            if query_string in edge_def.keys():\n                edge_type = edge_def[query_string]\n                valid = query_string\n            elif query_string_rev in edge_def.keys():\n                edge_type = edge_def[query_string_rev]\n                valid = query_string_rev\n            else:\n                raise ValueError(\"Invalid Query. Select valid pairs.\")\n            \n            # For cases such as the CT-RTSTRUCT and CT-RTDOSE, there exists multiple pathways due to which just searching on the edgetype gives wrong results\n            if edge_type in [0, 1, 2]:\n                edge_list = [0, 1, 2]\n                if edge_type==0:\n                    # Search for subgraphs with edges 0 or (1 and 2)\n                    regex_term = '(((?=.*0)|(?=.*5)(?=.*6))|((?=.*1)(?=.*2)))'\n                    mod = [i for i in self.mods if i in ['CT', 'MR']][0]  # making folder_mod CT/MR agnostic <-- still needs testing\n                    final_df = self.graph_query(regex_term, edge_list, f\"folder_{mod}\")\n                elif edge_type==1:\n                    # Search for subgraphs with edges 1 or (0 and 2)\n                    regex_term = '((?=.*1)|(((?=.*0)|(?=.*5)(?=.*6))(?=.*2)))'\n                    final_df = self.graph_query(regex_term, edge_list, \"RTSTRUCT\")\n                elif edge_type==2:\n                    #Search for subgraphs with edges 2 or (1 and 0)\n                    regex_term = '((?=.*2)|(((?=.*0)|(?=.*5)(?=.*6))(?=.*1)))'\n                    final_df = self.graph_query(regex_term, edge_list, \"RTDOSE\") \n            else:\n                final_df = self.df_edges.loc[self.df_edges.edge_type == edge_type, [\"study\",\"patient_ID_x\", \"study_x\", \"study_y\", \"series_x\",\"folder_x\",\"series_y\",\"folder_y\", \"subseries_x\", \"subseries_y\"]]\n                node_dest = valid.split(\",\")[0]\n                node_origin = valid.split(\",\")[1]\n                final_df.rename(columns={\"study\": \"study\", \n                                         \"patient_ID_x\": \"patient_ID\",\n                                         \"series_x\": f\"series_{node_dest}\", \n                                         \"series_y\": f\"series_{node_origin}\", \n                                         \n                                         \"study_x\": f\"study_{node_dest}\", \n                                         \"study_y\": f\"study_{node_origin}\", \n                                         \"folder_x\": f\"folder_{node_dest}\", \n                                         \"folder_y\": f\"folder_{node_origin}\",\n                                         \n                                         \"subseries_x\": f\"subseries_{node_dest}\", \n                                         \"subseries_y\": f\"subseries_{node_origin}\", }, inplace=True)\n\n        elif self.mods_n > 2:\n            # Processing of combinations of modality\n            bads = [\"RTPLAN\"]\n            # CT/MR,RTSTRUCT,RTDOSE\n            if ((\"CT\" in query_string) or ('MR' in query_string)) & (\"RTSTRUCT\" in query_string) & (\"RTDOSE\" in query_string) & (\"PT\" not in query_string):\n                # Fetch the required data. Checks whether each study has edge 2 and (1 or 0)\n                regex_term = '((?=.*1)|(?=.*0)|(?=.*5)(?=.*6))(?=.*2)'\n                edge_list = [0, 1, 2, 5, 6]\n            # CT/MR,RTSTRUCT,RTDOSE,PT\n            elif ((\"CT\" in query_string) or ('MR' in query_string))  & (\"RTSTRUCT\" in query_string) & (\"RTDOSE\" in query_string) & (\"PT\" in query_string):\n                # Fetch the required data. Checks whether each study has edge 2,3,4 and (1 or 0)\n                regex_term = '((?=.*1)|(?=.*0)|(?=.*5)(?=.*6))(?=.*2)(?=.*3)(?=.*4)' # fix\n                edge_list = [0, 1, 2, 3, 4]\n            #CT/MR,RTSTRUCT,PT\n            elif ((\"CT\" in query_string) or ('MR' in query_string))  & (\"RTSTRUCT\" in query_string) & (\"PT\" in query_string) & (\"RTDOSE\" not in query_string):\n                # Fetch the required data. Checks whether each study has edge 2,3,4\n                regex_term = '(?=.*2)(?=.*3)(?=.*4)'\n                edge_list = [2, 3, 4]            \n            #CT/MR,RTDOSE,PT\n            elif ((\"CT\" in query_string) or ('MR' in query_string))  & (\"RTSTRUCT\" not in query_string) & (\"PT\" in query_string) & (\"RTDOSE\" in query_string):\n                # Fetch the required data. Checks whether each study has edge 4 and (1 or (2 and 0)). Remove RTSTRUCT later\n                regex_term = '(?=.*4)((?=.*1)|((?=.*2)((?=.*0)|(?=.*5)(?=.*6))))'\n                edge_list = [0, 1, 2, 4, 5, 6]\n                bads.append(\"RTSTRUCT\")\n            else:\n                raise ValueError(\"Please enter the correct query\")\n            \n            final_df = self.graph_query(regex_term, edge_list, bads)\n        else:\n            raise ValueError(\"Please enter the correct query\")\n        \n        final_df.reset_index(drop=True, inplace=True)\n        final_df[\"index_chng\"] = final_df.index.astype(str) + \"_\" + final_df[\"patient_ID\"].astype(str)\n        final_df.set_index(\"index_chng\", inplace=True)\n        final_df.rename_axis(None, inplace=True)\n        # change relative paths to absolute paths\n        for col in final_df.columns:\n            if col.startswith(\"folder\"):\n                # print(self.edge_path, os.path.dirname(self.edge_path))\n                final_df[col] = final_df[col].apply(lambda x: pathlib.Path(os.path.split(os.path.dirname(self.edge_path))[0], x).as_posix() if isinstance(x, str) else x)  # input folder joined with the rel path\n        return final_df\n    \n    def graph_query(self, \n                    regex_term: str,\n                    edge_list: List[int],\n                    change_df: List[str],\n                    return_components: bool = False,\n                    remove_less_comp: bool = True):\n        '''\n        Based on the regex forms the final dataframe. You can \n        query the edge table based on the regex to get the \n        subgraph in which the queried edges will be present.\n        \n        The components are process further to get the final \n        dataframe of the required modalities.\n        \n        Parameters\n        ----------\n        regex_term\n            To search the string in edge_type column of self.df_new which is aggregate of all the edges in a single study\n\n        edge_list\n            The list of edges that should be returned in the subgraph\n\n        return_components\n            True to return the dictionary of the componets present with the condition present in the regex\n\n        change_df\n            Use only when you want to remove columns containing that string\n\n        remove_less_comp\n            False when you want to keep components with modalities less than the modalitiy listed in the query\n        '''\n        if self.df_new is None:\n            self._form_agg()  # Form aggregates\n        \n        # Fetch the required data. Checks whether each study has edge 4 and (1 or (2 and 0)). Can remove later\n        relevant_study_id = self.df_new.loc[(self.df_new.edge_type.str.contains(regex_term)), \"study_x\"].unique()\n        \n        # Based on the correct study ids, fetches the relevant edges\n        df_processed = self.df_edges.loc[self.df_edges.study_x.isin(relevant_study_id) & (self.df_edges.edge_type.isin(edge_list))]\n        \n        # The components are deleted if it has less number of nodes than the passed modalities, change this so as to alter that condition\n        final_df = self._get_df(df_processed, relevant_study_id, remove_less_comp)\n\n        # Removing columns\n        for bad in change_df:\n            # Find columns with change_df string present\n            col_ids = [cols for cols in list(final_df.columns)[1:] if bad != cols.split(\"_\")[1]]\n            final_df = final_df[[*list(final_df.columns)[:1], *col_ids]]\n        \n        if return_components:\n            return self.final_dict\n        else:\n            return final_df\n\n    def _form_agg(self):\n        '''\n        Form aggregates for easier parsing, gets the edge types for each study and aggregates as a string. This way one can do regex based on what type of subgraph the user wants\n        '''\n        self.df_edges['edge_type_str'] = self.df_edges['edge_type'].astype(str)\n        self.df_new = self.df_edges.groupby(\"study_x\").agg({'edge_type_str':self.list_edges})\n        self.df_new.reset_index(level=0, inplace=True) \n        self.df_new[\"edge_type\"] = self.df_new[\"edge_type_str\"]\n\n    def _get_df(self, \n                df_edges_processed,\n                rel_studyids,\n                remove_less_comp = True):\n    \n        '''\n        Assumption\n        ----------\n        The components are determined based on the unique CTs. \n        Please ensure the data conforms to this case. Based on \n        our preliminary analysis, there are no cases where CT \n        and PT are present but are disconnected.\n\n        Hence this assumption should hold for most of the cases\n        This function returns dataframe consisting of folder \n        location and modality for subgraphs\n\n        Parameters\n        ----------\n        df_edges_processed\n            Dataframe processed containing only the desired edges from the full graph\n\n        rel_studyids\n            Relevant study ids to process(This operation is a bit costly \n            so better not to perform on full graph for maximum performance)\n\n        remove_less_comp\n            True for removing components with less number of edges than the query\n\n        Changelog\n        ---------\n        * June 14th, 2022: Changing from studyID-based to sample-based for loop\n        * Oct 11th, 2022: Reverted to studyID-based loop + improved readability and make CT,RTSTRUCT,RTDOSE mode pass tests\n        '''\n        # Storing all the components across all the studies\n        self.final_dict = []\n        final_df = []\n        # For checking later if all the required modalities are present in a component or not\n        mods_wanted = set(self.mods)\n        \n        # Determine the number of components\n        for i, study in enumerate(rel_studyids): # per study_id\n            df_temp   = df_edges_processed.loc[df_edges_processed.study_x == study]\n            CT_locs   = df_temp.loc[df_temp.modality_x.isin(['CT', 'MR'])]\n            CT_series = CT_locs.series_x.unique()\n            A = []\n            save_folder_comp = []\n            \n            # Initialization. For each component intialize a dictionary with the CTs and their connections\n            for ct in CT_series:\n                df_connections = CT_locs.loc[CT_locs.series_x == ct]\n                \n                if len(df_connections) > 0:\n                    row = df_connections.iloc[0]\n                else:\n                    row = df_connections\n                    \n                series   = row.series_x\n                modality = row.modality_x\n                folder   = row.folder_x\n                \n                # For each component, this loop stores the CT and its connections\n                temp = {\"study\": study,\n                          ct: {\"modality\": modality,\n                               \"folder\": folder}}\n                \n                # For saving the components in a format easier for the main pipeline\n                folder_save = {\"study\": study,\n                               'patient_ID': row.patient_ID_x,\n                                f'series_{modality}': series,\n                                f'folder_{modality}': folder}\n                \n                # This loop stores connection of the CT\n                for k in range(len(df_connections)):\n                    row_y      = df_connections.iloc[k]\n                    series_y   = row_y.series_y\n                    folder_y   = row_y.folder_y\n                    modality_y = row_y.modality_y\n                    \n                    temp[row.series_y] = {\"modality\": modality_y,\n                                          \"folder\": folder_y,\n                                          \"conn_to\": modality}\n\n                    # Checks if there is already existing connection\n                    key, key_series = self._check_save(folder_save, modality_y, modality) #CT/MR                    \n                    folder_save[key_series] = series_y\n                    folder_save[key] = folder_y\n                \n                A.append(temp)\n                save_folder_comp.append(folder_save)\n                       \n            # For rest of the edges left out, the connections are formed by going through the dictionary. For cases such as RTstruct-RTDose and PET-RTstruct\n            rest_locs = df_temp.loc[~df_temp.modality_x.isin(['CT', 'MR']), [\"series_x\", \"modality_x\",\"folder_x\", \"series_y\", \"modality_y\", \"folder_y\"]]            \n            for j in range(len(rest_locs)):\n                edge = rest_locs.iloc[j]\n                for k in range(len(CT_series)):\n                    A[k][edge['series_y']] = {\"modality\": edge['modality_y'], \n                                              \"folder\": edge['folder_y'], \n                                              \"conn_to\": edge['modality_x']}\n                    modality_origin = edge['modality_x']\n\n                    # RTDOSE is connected via either RTstruct or/and CT, but we usually don't care, so naming it commonly\n                    if edge['modality_y'] == \"RTDOSE\":\n                        modality_origin = \"CT\"\n\n                    key, key_series = self._check_save(save_folder_comp[k], edge['modality_y'], modality_origin)\n                    save_folder_comp[k][key_series] = edge['series_y']\n                    save_folder_comp[k][key] = edge['folder_y']\n                    # flag = False\n\n            remove_index = []\n            if remove_less_comp:\n                for j in range(len(CT_series)):\n                    # Check if the number of nodes in a components isn't less than the query nodes, if yes then remove that component\n                    mods_present = set([items.split(\"_\")[1] for items in save_folder_comp[j].keys() if items.split(\"_\")[0] == \"folder\"])\n                    # Checking if all the read modalities are present in a component\n                    if mods_wanted.issubset(mods_present):\n                        remove_index.append(j)\n                save_folder_comp = [save_folder_comp[idx] for idx in remove_index]        \n                A = [A[idx] for idx in remove_index]    \n\n            self.final_dict.extend(A)\n            final_df.extend(save_folder_comp)\n        \n        final_df = pd.DataFrame(final_df)\n        return final_df\n    \n    @staticmethod\n    def _check_save(save_dict,node,dest):\n        key = f\"folder_{node}_{dest}\"\n        key_series = f\"series_{node}_{dest}\"\n        i = 1\n        while key in save_dict.keys():\n            key = f\"folder_{node}_{dest}_{i}\"\n            key_series = f\"series_{node}_{dest}_{i}\"\n            i +=1\n        return key,key_series\n    \n    @staticmethod\n    def list_edges(series):\n        return reduce(lambda x, y:str(x) + str(y), series)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the DataGraph class and how does it handle different types of DICOM connections?",
        "answer": "The DataGraph class is designed to create a graph structure from a crawled dataset of DICOM files. It handles different types of DICOM connections by assigning edge types to various relationships between modalities. For example, edge_type 0 represents a connection from RTDOSE to RTSTRUCT, edge_type 1 represents RTDOSE to CT, and so on. This graph structure allows for efficient querying and retrieval of interconnected studies containing specified DICOM modalities."
      },
      {
        "question": "How does the 'parser' method in the DataGraph class work, and what types of queries does it support?",
        "answer": "The 'parser' method in the DataGraph class processes user queries to return a dataframe of connected nodes. It supports queries for single modalities (e.g., 'CT') and combinations of modalities (e.g., 'CT,RTSTRUCT,RTDOSE'). The method uses regex patterns to search for specific edge combinations in the graph, allowing for complex queries like finding studies with CT, RTSTRUCT, and RTDOSE that are all interconnected. It can handle various combinations of the supported modalities: RTDOSE, RTSTRUCT, CT, PT, and MR."
      },
      {
        "question": "What is the significance of the '_form_edges' method in the DataGraph class, and how does it contribute to the overall functionality?",
        "answer": "The '_form_edges' method is crucial for creating the edge table that represents the graph structure. It processes the input dataframe to identify and create connections between different DICOM modalities. The method handles various edge types, such as RTDOSE to RTSTRUCT, RTDOSE to CT, RTSTRUCT to CT/MR, etc. By creating this comprehensive edge table, the method enables efficient querying and traversal of the DICOM data graph, which is essential for the class's ability to find interconnected studies based on user queries."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class DataGraph:\n    def __init__(self,\n                 path_crawl: str,\n                 edge_path: str = \"./patient_id_full_edges.csv\",\n                 visualize: bool = False) -> None:\n        self.df = pd.read_csv(path_crawl, index_col=0)\n        self.edge_path = edge_path\n        self.df_new = None\n        if os.path.exists(self.edge_path):\n            print(\"Edge table is already present. Loading the data...\")\n            self.df_edges = pd.read_csv(self.edge_path)\n        else:\n            print(\"Edge table not present. Forming the edge table based on the crawl data...\")\n            self.form_graph()\n        if visualize:\n            self.visualize_graph()",
        "complete": "class DataGraph:\n    def __init__(self,\n                 path_crawl: str,\n                 edge_path: str = \"./patient_id_full_edges.csv\",\n                 visualize: bool = False) -> None:\n        self.df = pd.read_csv(path_crawl, index_col=0)\n        self.edge_path = edge_path\n        self.df_new = None\n        if os.path.exists(self.edge_path):\n            print(\"Edge table is already present. Loading the data...\")\n            self.df_edges = pd.read_csv(self.edge_path)\n        else:\n            print(\"Edge table not present. Forming the edge table based on the crawl data...\")\n            self.form_graph()\n        if visualize:\n            self.visualize_graph()\n\n    def form_graph(self):\n        for col in self.df:\n            self.df[col] = self.df[col].astype(str)\n        \n        df_filter = pd.merge(self.df, self.df[[\"instance_uid\",\"reference_rs\"]].apply(lambda x: x.astype(str), axis=1), \n                             left_on=\"reference_pl\", \n                             right_on=\"instance_uid\", \n                             how=\"left\")\n        \n        df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_x\"] = df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_y\"].values\n        df_filter.drop(columns=[\"reference_rs_y\", \"instance_uid_y\"], inplace=True)\n        df_filter.rename(columns={\"reference_rs_x\":\"reference_rs\", \"instance_uid_x\":\"instance_uid\"}, inplace=True)\n        \n        df_filter = df_filter.loc[~((df_filter[\"modality\"] == \"RTDOSE\") & (df_filter[\"reference_ct\"].isna()) & (df_filter[\"reference_rs\"].isna()))]\n\n        start = time.time()\n        \n        self.df_edges = self._form_edges(self.df)\n        end = time.time()\n        print(f\"\\nTotal time taken: {end - start}\")\n\n        self.df_edges.loc[self.df_edges.study_x.isna(),\"study_x\"] = self.df_edges.loc[self.df_edges.study_x.isna(), \"study\"]\n        self.df_edges.drop(columns=[\"study_y\", \"patient_ID_y\", \"series_description_y\", \"study_description_y\", \"study\"],inplace=True)\n        self.df_edges.sort_values(by=\"patient_ID_x\", ascending=True)\n        print(f\"Saving edge table in {self.edge_path}\")\n        self.df_edges.to_csv(self.edge_path, index=False)"
      },
      {
        "partial": "def _form_edges(self, df):\n        df_list = []\n\n        plan = df[df[\"modality\"] == \"RTPLAN\"]\n        dose = df[df[\"modality\"] == \"RTDOSE\"]\n        struct = df[df[\"modality\"] == \"RTSTRUCT\"]\n        seg = df[df[\"modality\"] == \"SEG\"]\n        ct = df[df[\"modality\"] == \"CT\"]\n        mr = df[df[\"modality\"] == \"MR\"]\n        pet = df[df[\"modality\"] == \"PT\"]\n\n        edge_types = np.arange(7)\n        for edge in edge_types:\n            if edge == 0:\n                df_comb1 = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2 = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n            elif edge == 1:\n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n            # ... (other edge cases)\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n\n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        return df_edges",
        "complete": "def _form_edges(self, df):\n        df_list = []\n\n        plan = df[df[\"modality\"] == \"RTPLAN\"]\n        dose = df[df[\"modality\"] == \"RTDOSE\"]\n        struct = df[df[\"modality\"] == \"RTSTRUCT\"]\n        seg = df[df[\"modality\"] == \"SEG\"]\n        ct = df[df[\"modality\"] == \"CT\"]\n        mr = df[df[\"modality\"] == \"MR\"]\n        pet = df[df[\"modality\"] == \"PT\"]\n\n        edge_types = np.arange(7)\n        for edge in edge_types:\n            if edge == 0:\n                df_comb1 = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2 = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n            elif edge == 1:\n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n            elif edge == 2:\n                df_ct = pd.merge(ct, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n            elif edge == 3:\n                df_combined = pd.merge(pet, struct, left_on=\"series\", right_on=\"reference_ct\")\n            elif edge == 4:\n                df_combined = pd.merge(ct, pet, left_on=\"study\", right_on=\"study\")\n            elif edge == 5:\n                df_combined = pd.merge(plan, dose, left_on=\"instance_uid\", right_on=\"reference_pl\")\n            elif edge == 7:\n                df_ct = pd.merge(ct, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n            else:\n                df_combined = pd.merge(struct, plan, left_on=\"instance_uid\", right_on=\"reference_rs\")\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n\n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        return df_edges"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "time",
        "pathlib",
        "numpy",
        "pandas"
      ],
      "from_imports": [
        "typing.List",
        "functools.reduce",
        "pyvis.network.Network"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugPerturbation.R",
    "language": "R",
    "content": "#' @importFrom stats median\n#' @importFrom stats complete.cases\n#' @importFrom stats lm\n#' @importFrom stats anova\n#' @importFrom stats pf\n#'\n\n## function computing gene-drug associations from perturbation data (CMAP)\ngeneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n## input:\n##  x: numeric vector of gene expression values\n##  concentration: numeric vector with drug concentrations/doses\n##  type: vector of factors specifying the cell lines or type types\n##  batch: vector of factors specifying the batch\n##  duration: numeric vector of measurement times (in hours)\n##  model: Should the full linear model be returned? Default set to FALSE\n##\n## output:\n##  vector reporting the effect size (estimateof the coefficient of drug concentration), standard error (se), sample size (n), t statistic, and f statistics and its corresponding p-value\n\n    ## NOTE:: The use of T/F warning from BiocCheck is a false positive on the string 'Pr(>F)'\n\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        tt <- rep(NA, length(nc))\n        names(tt) <- nc\n        return(tt)\n    }\n    ff0 <- sprintf(\"x ~ 1\")\n    ff <- sprintf(\"%s + concentration\", ff0)\n\n\n    if (length(sort(unique(type))) > 1) {\n        ff0 <- sprintf(\"%s + type\", ff0)\n        ff <- sprintf(\"%s + type\", ff)\n    }\n    if (length(sort(unique(batch))) > 1) {\n        ff0 <- sprintf(\"%s + batch\", ff0)\n        ff <- sprintf(\"%s + batch\", ff)\n    }\n\n### add experiment duration if the vector consists of more than one different value\n\n  if (length(sort(unique(duration))) > 2) {\n      ff0 <- sprintf(\"%s + duration\", ff0)\n      ff <- sprintf(\"%s + duration\", ff)\n  }\n\n    dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n        names(dd)[1]<-\"x\"\n        mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n        mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n\n        mmc <- stats::anova(mm0, mm)\n        mm <- summary(mm)\n## extract statistics\n        tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n    }\n    names(tt) <- nc\n## add tissue type/cell line statistics\n    if(length(sort(unique(type))) > 1) {\n        rr <- summary(mm0)\n        ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n        names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n    } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n    tt <- c(tt, ttype)\n## add model\n    if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n    return(tt)\n}\n\n\n## End\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `geneDrugPerturbation` function and what are its main inputs?",
        "answer": "The `geneDrugPerturbation` function computes gene-drug associations from perturbation data (CMAP). Its main inputs are: 'x' (numeric vector of gene expression values), 'concentration' (numeric vector with drug concentrations/doses), 'type' (vector of factors specifying cell lines or types), 'batch' (vector of factors specifying the batch), 'duration' (numeric vector of measurement times in hours), and 'model' (boolean indicating whether to return the full linear model, default is FALSE)."
      },
      {
        "question": "How does the function handle cases where there are insufficient drug concentrations or complete cases?",
        "answer": "The function checks for insufficient data in two ways: 1) If there are less than 2 unique drug concentrations, it issues a warning 'No drug concentrations tested' and returns a vector of NA values. 2) If there are less than 3 complete cases in the data frame, it returns a vector with NA values for most statistics, except for 'n' which reports the number of complete cases."
      },
      {
        "question": "What statistical methods does the function use to compute gene-drug associations, and what are the main output statistics?",
        "answer": "The function uses linear regression models to compute gene-drug associations. It creates two models: one with and one without the drug concentration as a predictor. It then uses ANOVA to compare these models. The main output statistics are: 'estimate' (effect size of drug concentration), 'se' (standard error), 'n' (sample size), 'tstat' (t-statistic), 'fstat' (F-statistic), and 'pvalue' (p-value from ANOVA). If multiple cell types are present, it also includes 'type.fstat' and 'type.pvalue' for the cell type effect."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        tt <- rep(NA, length(nc))\n        names(tt) <- nc\n        return(tt)\n    }\n    ff0 <- sprintf(\"x ~ 1\")\n    ff <- sprintf(\"%s + concentration\", ff0)\n\n    # Add code here to handle type, batch, and duration\n\n    dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n        # Add code here to create and analyze models\n    }\n    # Add code here to handle type statistics and model return\n    return(tt)\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        tt <- rep(NA, length(nc))\n        names(tt) <- nc\n        return(tt)\n    }\n    ff0 <- sprintf(\"x ~ 1\")\n    ff <- sprintf(\"%s + concentration\", ff0)\n\n    if (length(sort(unique(type))) > 1) {\n        ff0 <- sprintf(\"%s + type\", ff0)\n        ff <- sprintf(\"%s + type\", ff)\n    }\n    if (length(sort(unique(batch))) > 1) {\n        ff0 <- sprintf(\"%s + batch\", ff0)\n        ff <- sprintf(\"%s + batch\", ff)\n    }\n    if (length(sort(unique(duration))) > 2) {\n        ff0 <- sprintf(\"%s + duration\", ff0)\n        ff <- sprintf(\"%s + duration\", ff)\n    }\n\n    dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n        names(dd)[1] <- \"x\"\n        mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n        mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n        mmc <- stats::anova(mm0, mm)\n        mm <- summary(mm)\n        tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n    }\n    names(tt) <- nc\n    if(length(sort(unique(type))) > 1) {\n        rr <- summary(mm0)\n        ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n        names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n    } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n    tt <- c(tt, ttype)\n    if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n    return(tt)\n}"
      },
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        return(setNames(rep(NA, length(nc)), nc))\n    }\n    ff0 <- \"x ~ 1\"\n    ff <- paste(ff0, \"+ concentration\")\n\n    # Add code here to handle type, batch, and duration\n\n    dd <- data.frame(x=x, concentration=concentration, duration=duration, type=type, batch=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        return(c(estimate=NA, se=NA, n=nn, tsat=NA, fstat=NA, pvalue=NA))\n    }\n    # Add code here to create and analyze models\n    # Add code here to handle type statistics and model return\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        return(setNames(rep(NA, length(nc)), nc))\n    }\n    ff0 <- \"x ~ 1\"\n    ff <- paste(ff0, \"+ concentration\")\n\n    if (length(sort(unique(type))) > 1) {\n        ff0 <- paste(ff0, \"+ type\")\n        ff <- paste(ff, \"+ type\")\n    }\n    if (length(sort(unique(batch))) > 1) {\n        ff0 <- paste(ff0, \"+ batch\")\n        ff <- paste(ff, \"+ batch\")\n    }\n    if (length(sort(unique(duration))) > 2) {\n        ff0 <- paste(ff0, \"+ duration\")\n        ff <- paste(ff, \"+ duration\")\n    }\n\n    dd <- data.frame(x=x, concentration=concentration, duration=duration, type=type, batch=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        return(c(estimate=NA, se=NA, n=nn, tsat=NA, fstat=NA, pvalue=NA))\n    }\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm_summary <- summary(mm)\n    tt <- c(estimate=mm_summary$coefficients[\"concentration\", \"Estimate\"],\n            se=mm_summary$coefficients[\"concentration\", \"Std. Error\"],\n            n=nn,\n            tsat=mm_summary$coefficients[\"concentration\", \"t value\"],\n            fstat=mmc$F[2],\n            pvalue=mmc$'Pr(>F)'[2])\n    if(length(sort(unique(type))) > 1) {\n        rr <- summary(mm0)\n        ttype <- c(type.fstat=rr$fstatistic[\"value\"],\n                   type.pvalue=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    } else {\n        ttype <- c(type.fstat=NA, type.pvalue=NA)\n    }\n    tt <- c(tt, ttype)\n    if (model) tt <- list(stats=tt, model=mm)\n    return(tt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_modalities.py",
    "language": "py",
    "content": "'''\nThis code is for testing functioning of different modalities \n'''\n\n\nimport os\nimport pathlib\n\nimport SimpleITK as sitk\nimport pytest\nimport pydicom\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops import StructureSetToSegmentation\n\n@pytest.fixture\ndef modalities_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    print(os.listdir(curr_path))\n    \n    # qc_path = pathlib.Path(os.path.join(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-082\"))\n    # path = {}\n    # path[\"CT\"] = os.path.join(qc_path, \"08-27-1885-CA ORL FDG TEP-06980/3.000000-Merged-05195\")\n    # path[\"RTSTRUCT\"] = os.path.join(qc_path, \"08-27-1885-06980/Pinnacle POI-67882\")\n    # path[\"RTDOSE\"] = os.path.join(qc_path, \"08-27-1885-06980/89632\")\n    # path[\"PT\"] = os.path.join(qc_path, \"08-27-1885-TEP cancerologique TEP-06980/552650.000000-LOR-RAMLA-72508\")\n    qc_path = pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-052\").as_posix()\n    assert os.path.exists(qc_path), \"Dataset not found\"\n    \n    path = {}\n    path[\"CT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/3.000000-Merged-06362\").as_posix()\n    path[\"RTSTRUCT\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/Pinnacle POI-41418\").as_posix()\n    path[\"RTDOSE\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/11376\").as_posix()\n    path[\"PT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/532790.000000-LOR-RAMLA-44600\").as_posix()\n    return path\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"RTSTRUCT\", \"RTDOSE\", \"PT\"])\ndef test_modalities(modalities, modalities_path):\n    path = modalities_path\n    img = read_dicom_auto(path[\"CT\"]).image\n    if modalities != \"RTSTRUCT\":\n        #Checks for dimensions\n        dcm = pydicom.dcmread(pathlib.Path(path[modalities],os.listdir(path[modalities])[0]).as_posix()).pixel_array\n        instances = len(os.listdir(path[modalities]))\n        dicom = read_dicom_auto(path[modalities])\n        if modalities == 'CT':\n            dicom = dicom.image\n        if instances>1: #For comparing CT and PT modalities\n            assert dcm.shape == (dicom.GetHeight(),dicom.GetWidth())\n            assert instances == dicom.GetDepth()\n        else: #For comparing RTDOSE modalties\n            assert dcm.shape == (dicom.GetDepth(),dicom.GetHeight(),dicom.GetWidth())\n        if modalities == \"PT\":\n            dicom = dicom.resample_pet(img)\n            assert dicom.GetSize()==img.GetSize()\n        if modalities == \"RTDOSE\":\n            dicom = dicom.resample_dose(img)\n            assert dicom.GetSize()==img.GetSize()\n    else:\n        struc = read_dicom_auto(path[modalities])\n        make_binary_mask = StructureSetToSegmentation(roi_names=['GTV.?', 'LARYNX'], continuous=False)\n        mask = make_binary_mask(struc, img, {\"background\": 0}, False)\n        A = sitk.GetArrayFromImage(mask)\n        assert len(A.shape) == 4\n        assert A.shape[0:3] == (img.GetDepth(),img.GetHeight(),img.GetWidth())\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `modalities_path` fixture in this code, and how does it handle different imaging modalities?",
        "answer": "The `modalities_path` fixture is used to set up the file paths for different imaging modalities (CT, RTSTRUCT, RTDOSE, and PT) in a medical imaging dataset. It creates a dictionary with keys for each modality and values containing the corresponding file paths. This fixture ensures that the correct data paths are available for testing different modalities in the subsequent test function."
      },
      {
        "question": "How does the code handle the comparison of dimensions for different modalities, and what is the significance of the `instances` variable?",
        "answer": "The code compares dimensions differently based on the modality:\n1. For CT and PT: It checks if the shape of a single DICOM slice matches the height and width of the loaded image, and if the number of instances (files) matches the depth.\n2. For RTDOSE: It checks if the shape of the DICOM data matches the depth, height, and width of the loaded image.\nThe `instances` variable represents the number of DICOM files in the modality's directory. It's used to determine if the modality has multiple slices (CT, PT) or a single file (RTDOSE), which affects how the dimension comparison is performed."
      },
      {
        "question": "What special processing is done for the RTSTRUCT modality, and what assertions are made about the resulting mask?",
        "answer": "For the RTSTRUCT modality, the code:\n1. Reads the RTSTRUCT DICOM file.\n2. Creates a binary mask using the `StructureSetToSegmentation` class, focusing on ROIs with names matching 'GTV.?' and 'LARYNX'.\n3. Generates a mask aligned with the CT image.\n4. Converts the mask to a numpy array.\n5. Asserts that the resulting array has 4 dimensions.\n6. Checks if the first three dimensions of the mask array match the depth, height, and width of the CT image.\nThis process ensures that the RTSTRUCT data is correctly converted into a binary segmentation mask that aligns with the corresponding CT image."
      }
    ],
    "completion_tasks": [
      {
        "partial": "import os\nimport pathlib\nimport SimpleITK as sitk\nimport pytest\nimport pydicom\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops import StructureSetToSegmentation\n\n@pytest.fixture\ndef modalities_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    qc_path = pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-052\").as_posix()\n    assert os.path.exists(qc_path), \"Dataset not found\"\n    \n    path = {}\n    # Complete the path dictionary\n\n    return path\n",
        "complete": "import os\nimport pathlib\nimport SimpleITK as sitk\nimport pytest\nimport pydicom\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops import StructureSetToSegmentation\n\n@pytest.fixture\ndef modalities_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    qc_path = pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-052\").as_posix()\n    assert os.path.exists(qc_path), \"Dataset not found\"\n    \n    path = {}\n    path[\"CT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/3.000000-Merged-06362\").as_posix()\n    path[\"RTSTRUCT\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/Pinnacle POI-41418\").as_posix()\n    path[\"RTDOSE\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/11376\").as_posix()\n    path[\"PT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/532790.000000-LOR-RAMLA-44600\").as_posix()\n    return path\n"
      },
      {
        "partial": "@pytest.mark.parametrize(\"modalities\", [\"CT\", \"RTSTRUCT\", \"RTDOSE\", \"PT\"])\ndef test_modalities(modalities, modalities_path):\n    path = modalities_path\n    img = read_dicom_auto(path[\"CT\"]).image\n    if modalities != \"RTSTRUCT\":\n        dcm = pydicom.dcmread(pathlib.Path(path[modalities],os.listdir(path[modalities])[0]).as_posix()).pixel_array\n        instances = len(os.listdir(path[modalities]))\n        dicom = read_dicom_auto(path[modalities])\n        if modalities == 'CT':\n            dicom = dicom.image\n        # Complete the rest of the function\n",
        "complete": "@pytest.mark.parametrize(\"modalities\", [\"CT\", \"RTSTRUCT\", \"RTDOSE\", \"PT\"])\ndef test_modalities(modalities, modalities_path):\n    path = modalities_path\n    img = read_dicom_auto(path[\"CT\"]).image\n    if modalities != \"RTSTRUCT\":\n        dcm = pydicom.dcmread(pathlib.Path(path[modalities],os.listdir(path[modalities])[0]).as_posix()).pixel_array\n        instances = len(os.listdir(path[modalities]))\n        dicom = read_dicom_auto(path[modalities])\n        if modalities == 'CT':\n            dicom = dicom.image\n        if instances > 1:\n            assert dcm.shape == (dicom.GetHeight(), dicom.GetWidth())\n            assert instances == dicom.GetDepth()\n        else:\n            assert dcm.shape == (dicom.GetDepth(), dicom.GetHeight(), dicom.GetWidth())\n        if modalities == \"PT\":\n            dicom = dicom.resample_pet(img)\n            assert dicom.GetSize() == img.GetSize()\n        if modalities == \"RTDOSE\":\n            dicom = dicom.resample_dose(img)\n            assert dicom.GetSize() == img.GetSize()\n    else:\n        struc = read_dicom_auto(path[modalities])\n        make_binary_mask = StructureSetToSegmentation(roi_names=['GTV.?', 'LARYNX'], continuous=False)\n        mask = make_binary_mask(struc, img, {\"background\": 0}, False)\n        A = sitk.GetArrayFromImage(mask)\n        assert len(A.shape) == 4\n        assert A.shape[0:3] == (img.GetDepth(), img.GetHeight(), img.GetWidth())\n"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "SimpleITK",
        "pytest",
        "pydicom"
      ],
      "from_imports": [
        "imgtools.io.read_dicom_auto",
        "imgtools.ops.StructureSetToSegmentation"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_Hill.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    expect_equal(.Hill(-Inf, c(1, 0.2, 1)), 1)\n    expect_equal(.Hill(Inf, c(1, 0.2, 1)), 0.2)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .Hill function in this code snippet, and how is it being tested?",
        "answer": "The .Hill function appears to be a mathematical function used in pharmacology or biochemistry. It's being tested for its behavior under various input conditions using the testthat framework in R. The tests check the function's output for different combinations of input parameters, including edge cases like 0, infinity, and negative infinity."
      },
      {
        "question": "Explain the significance of the test case: expect_equal(.Hill(0, c(0, 0, 0)), 1/2)",
        "answer": "This test case is checking the behavior of the .Hill function when all its parameters are zero. The expected output of 1/2 suggests that the function has a built-in default or limiting behavior when faced with all-zero inputs. This could represent a mid-point or baseline response in the context of the Hill equation, which is often used to model dose-response relationships."
      },
      {
        "question": "How does the code handle extreme values, and what does this imply about the .Hill function's behavior?",
        "answer": "The code tests the .Hill function with extreme values like Inf (infinity) and -Inf (negative infinity). For example, .Hill(-Inf, c(1, 0.2, 1)) is expected to return 1, while .Hill(Inf, c(1, 0.2, 1)) is expected to return 0.2. This implies that the .Hill function has well-defined asymptotic behavior, approaching different limits as the input goes to positive or negative infinity. This is consistent with the typical behavior of the Hill equation in describing saturation phenomena."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    # Add two more test cases\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    expect_equal(.Hill(-Inf, c(1, 0.2, 1)), 1)\n    expect_equal(.Hill(Inf, c(1, 0.2, 1)), 0.2)\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    # Add five test cases for the .Hill function\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    expect_equal(.Hill(-Inf, c(1, 0.2, 1)), 1)\n    expect_equal(.Hill(Inf, c(1, 0.2, 1)), 0.2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/__init__.py",
    "language": "py",
    "content": "from .segmentation import *\nfrom .structureset import *\nfrom .pet import *\nfrom .dose import *\nfrom .datagraph import *\nfrom .sparsemask import *\nfrom .scan import *\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of using asterisks (*) in the import statements of this Python code snippet?",
        "answer": "The asterisks (*) in the import statements are used for wildcard imports. This means that all public names (functions, classes, variables) defined in the specified modules are imported directly into the current namespace. For example, 'from .segmentation import *' imports all public names from the 'segmentation' module located in the same package."
      },
      {
        "question": "What potential issue might arise from using wildcard imports as shown in this code snippet?",
        "answer": "Using wildcard imports can lead to namespace pollution, where imported names may unintentionally override existing names in the current module. This can make the code harder to understand and maintain, as it's not immediately clear where specific functions or classes are coming from. It's generally recommended to use explicit imports or import specific names to avoid these issues."
      },
      {
        "question": "What does the dot (.) before the module names in the import statements indicate?",
        "answer": "The dot (.) before the module names in the import statements indicates relative imports. It means that the imported modules are located in the same package as the current module. This is typically used in larger Python projects with a hierarchical structure to import modules from the same package or subpackages."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .segmentation import *\nfrom .structureset import *\nfrom .pet import *\nfrom .dose import *\n# Import remaining modules\n",
        "complete": "from .segmentation import *\nfrom .structureset import *\nfrom .pet import *\nfrom .dose import *\nfrom .datagraph import *\nfrom .sparsemask import *\nfrom .scan import *"
      },
      {
        "partial": "from . import (\n    segmentation,\n    structureset,\n    pet,\n    dose,\n    # Import remaining modules\n)",
        "complete": "from . import (\n    segmentation,\n    structureset,\n    pet,\n    dose,\n    datagraph,\n    sparsemask,\n    scan\n)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "segmentation.*",
        "structureset.*",
        "pet.*",
        "dose.*",
        "datagraph.*",
        "sparsemask.*",
        "scan.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_cellosaurus_helpers.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".create_cellosaurus_queries is acting as expected\", {\n  queries <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n  expect_character(queries)\n  expect_equal(queries, c(\"Accession:ID1\", \"Accession:ID2\", \"Accession:ID3\"))\n\n  queries2 <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n  expect_equal(queries2, c(\"Accession:ID1\", \"Name:ID2\", \"Species:ID3\"))\n})\n\ntest_that(\".cellosaurus_schema is acting as expected\", {\n  schema <- AnnotationGx:::.cellosaurus_schema()\n  expect_list(schema)\n  names_list <- c(\"openapi\", \"info\", \"paths\", \"components\", \"tags\")\n\n  expect_names(names(schema), subset.of = names_list)\n})\n\ntest_that(\".build_cellosaurus_request is acting as expected\", {\n  request <- AnnotationGx:::.build_cellosaurus_request()\n\n  expect_class(request, \"httr2_request\")\n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Chi%2Cca%2Csx%2Cag%2Cdi%2Cderived-from-site%2Cmisspelling&format=tsv&rows=1\"\n  expect_equal(request$url, expected)\n\n  response <- AnnotationGx:::.perform_request(request) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_class(response, \"spec_tbl_df\")\n  expect_equal(nrow(response), 1)\n\n  request2 <- AnnotationGx:::.build_cellosaurus_request(\n    query = \"id:HeLa\",\n    to = c(\n      \"id\", \"ac\", \"sy\", \"acas\", \"sx\", \"ag\", \"di\", \"dio\", \"din\", \"dr\", \"cell-type\",\n      \"derived-from-site\", \"misspelling\", \"dt\", \"dtc\", \"dtu\", \"dtv\", \"genome-ancestry\"\n    ),\n    numResults = 2, apiResource = \"search/cell-line\", output = \"TSV\"\n  )\n  expect_equal(\n    request2$url,\n    \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Csy%2Cacas%2Csx%2Cag%2Cdi%2Cdio%2Cdin%2Cdr%2Ccell-type%2Cderived-from-site%2Cmisspelling%2Cdt%2Cdtc%2Cdtu%2Cdtv%2Cgenome-ancestry&format=tsv&rows=2\"\n  )\n  response <- AnnotationGx:::.perform_request(request2) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_equal(nrow(response), 2)\n})\n\n\ntest_that(\"common_cellosaurus_fields returns the expected fields\", {\n  fields <- AnnotationGx::cellosaurus_fields(common = T, upper = T)\n  expect_character(fields)\n  expect_fields <- c(\n    \"id\", \"ac\", \"acas\", \"sy\", \"dr\", \"di\", \"din\", \"dio\", \"ox\", \"cc\",  \"sx\", \"ag\", \"oi\",\n    \"hi\", \"ch\", \"ca\",  \"dt\", \"dtc\", \"dtu\", \"dtv\", \"from\", \"group\"\n  )\n\n\n  expect_equal(fields, toupper(expect_fields))\n})\n\ntest_that(\".cellosaurus_extResources returns the expected external resources\", {\n  resources <- AnnotationGx:::.cellosaurus_extResources()\n  expect_character(resources)\n\n  expected_resources <- c(\n    \"4DN\", \"Abcam\", \"ABCD\", \"ABM\", \"AddexBio\", \"ArrayExpress\",\n    \"ATCC\", \"BCGO\", \"BCRC\", \"BCRJ\", \"BEI_Resources\",\n    \"BioGRID_ORCS_Cell_line\", \"BTO\", \"BioSample\", \"BioSamples\",\n    \"cancercelllines\", \"CancerTools\", \"CBA\", \"CCLV\", \"CCRID\",\n    \"CCTCC\", \"Cell_Biolabs\", \"Cell_Model_Passport\", \"CGH-DB\",\n    \"ChEMBL-Cells\", \"ChEMBL-Targets\", \"CLDB\", \"CLO\", \"CLS\",\n    \"ColonAtlas\", \"Coriell\", \"Cosmic\", \"Cosmic-CLP\", \"dbGAP\",\n    \"dbMHC\", \"DepMap\", \"DGRC\", \"DiscoverX\", \"DSHB\", \"DSMZ\",\n    \"DSMZCellDive\", \"EBiSC\", \"ECACC\", \"EFO\", \"EGA\", \"ENCODE\",\n    \"ESTDAB\", \"FCDI\", \"FCS-free\", \"FlyBase_Cell_line\", \"GDSC\",\n    \"GeneCopoeia\", \"GEO\", \"HipSci\", \"HIVReagentProgram\", \"Horizon_Discovery\",\n    \"hPSCreg\", \"IARC_TP53\", \"IBRC\", \"ICLC\", \"ICLDB\", \"IGRhCellID\",\n    \"IGSR\", \"IHW\", \"Imanis\", \"Innoprot\", \"IPD-IMGT/HLA\", \"ISCR\",\n    \"IZSLER\", \"JCRB\", \"KCB\", \"KCLB\", \"Kerafast\", \"KYinno\", \"LiGeA\",\n    \"LIMORE\", \"LINCS_HMS\", \"LINCS_LDP\", \"Lonza\", \"MCCL\", \"MeSH\",\n    \"MetaboLights\", \"Millipore\", \"MMRRC\", \"NCBI_Iran\", \"NCI-DTP\", \"NHCDR\",\n    \"NIHhESC\", \"NISES\", \"NRFC\", \"PerkinElmer\", \"PharmacoDB\", \"PRIDE\",\n    \"Progenetix\", \"PubChem_Cell_line\", \"RCB\", \"Rockland\", \"RSCB\", \"SKIP\",\n    \"SKY/M-FISH/CGH\", \"SLKBase\", \"TKG\", \"TNGB\", \"TOKU-E\", \"Ubigene\",\n    \"WiCell\", \"Wikidata\", \"Ximbio\"\n  )\n\n  expect_equal(resources, expected_resources)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.create_cellosaurus_queries` function in the given code snippet?",
        "answer": "The `.create_cellosaurus_queries` function is used to create formatted query strings for the Cellosaurus database. It takes two arguments: a vector of IDs and a vector of query types (e.g., 'Accession', 'Name', 'Species'). It combines these to create queries in the format 'QueryType:ID'. This function is useful for preparing search queries for the Cellosaurus API."
      },
      {
        "question": "How does the `.build_cellosaurus_request` function construct API requests, and what parameters can be customized?",
        "answer": "The `.build_cellosaurus_request` function constructs API requests for the Cellosaurus database. It creates an `httr2_request` object with a URL that includes query parameters. The function allows customization of several parameters: 'query' for the search term, 'to' for specifying fields to return, 'numResults' for the number of results, 'apiResource' for the API endpoint, and 'output' for the response format. These parameters are encoded into the URL of the request."
      },
      {
        "question": "What is the purpose of the `cellosaurus_fields` function, and how does it differ when called with `common = T, upper = T`?",
        "answer": "The `cellosaurus_fields` function returns a character vector of field names used in the Cellosaurus database. When called with `common = T, upper = T`, it returns a subset of commonly used fields with their names in uppercase. This is useful for quickly accessing the most frequently used fields in a standardized format. The function helps users specify which fields they want to retrieve from the Cellosaurus API without needing to remember all available fields."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\".create_cellosaurus_queries is acting as expected\", {\n  queries <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n  expect_character(queries)\n  expect_equal(queries, c(\"Accession:ID1\", \"Accession:ID2\", \"Accession:ID3\"))\n\n  queries2 <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n  # Complete the test for queries2\n})",
        "complete": "test_that(\".create_cellosaurus_queries is acting as expected\", {\n  queries <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n  expect_character(queries)\n  expect_equal(queries, c(\"Accession:ID1\", \"Accession:ID2\", \"Accession:ID3\"))\n\n  queries2 <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n  expect_equal(queries2, c(\"Accession:ID1\", \"Name:ID2\", \"Species:ID3\"))\n})"
      },
      {
        "partial": "test_that(\".build_cellosaurus_request is acting as expected\", {\n  request <- AnnotationGx:::.build_cellosaurus_request()\n\n  expect_class(request, \"httr2_request\")\n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Chi%2Cca%2Csx%2Cag%2Cdi%2Cderived-from-site%2Cmisspelling&format=tsv&rows=1\"\n  expect_equal(request$url, expected)\n\n  response <- AnnotationGx:::.perform_request(request) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_class(response, \"spec_tbl_df\")\n  expect_equal(nrow(response), 1)\n\n  # Complete the test for request2\n})",
        "complete": "test_that(\".build_cellosaurus_request is acting as expected\", {\n  request <- AnnotationGx:::.build_cellosaurus_request()\n\n  expect_class(request, \"httr2_request\")\n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Chi%2Cca%2Csx%2Cag%2Cdi%2Cderived-from-site%2Cmisspelling&format=tsv&rows=1\"\n  expect_equal(request$url, expected)\n\n  response <- AnnotationGx:::.perform_request(request) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_class(response, \"spec_tbl_df\")\n  expect_equal(nrow(response), 1)\n\n  request2 <- AnnotationGx:::.build_cellosaurus_request(\n    query = \"id:HeLa\",\n    to = c(\n      \"id\", \"ac\", \"sy\", \"acas\", \"sx\", \"ag\", \"di\", \"dio\", \"din\", \"dr\", \"cell-type\",\n      \"derived-from-site\", \"misspelling\", \"dt\", \"dtc\", \"dtu\", \"dtv\", \"genome-ancestry\"\n    ),\n    numResults = 2, apiResource = \"search/cell-line\", output = \"TSV\"\n  )\n  expect_equal(\n    request2$url,\n    \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Csy%2Cacas%2Csx%2Cag%2Cdi%2Cdio%2Cdin%2Cdr%2Ccell-type%2Cderived-from-site%2Cmisspelling%2Cdt%2Cdtc%2Cdtu%2Cdtv%2Cgenome-ancestry&format=tsv&rows=2\"\n  )\n  response <- AnnotationGx:::.perform_request(request2) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_equal(nrow(response), 2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/getRawSensitivityMatrix.R",
    "language": "R",
    "content": "##TODO:: Add function documentation\ngetRawSensitivityMatrix <-\n  function(pSet, cell.id, drug.id, max.conc, quality) {\n    cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n    if(!missing(quality)) {\n      if(is.element(\"quality\", colnames(sensitivityInfo(pSet)))) {\n        cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n      }\n    }\n    if(!missing(max.conc)) {\n        if(is.element(\"max.conc\", colnames(sensitivityInfo(pSet)))) {\n          if(length(max.conc) > 1) {\n            max.conc <- paste(max.conc, collapse=\"///\")\n        }\n        cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n      }\n    }\n    if(length(drug.id) > 1) {\n      drug.id <- paste(drug.id, collapse=\"///\")\n    }\n    cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n\n    exp.id <- which(eval(parse(text=cond)))\n\n    sensitivity.raw.matrix <- list()\n    if(length(exp.id) > 0) {\n      for(i in seq_len(length(exp.id))){\n        if(length(grep(\"///\", drug.id)) > 0) {\n          all.exp.id <- which(sensitivityInfo(pSet)$combination.exp.id == sensitivityInfo(pSet)[exp.id[i], \"combination.exp.id\"])\n          drug.1 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == unlist(strsplit(drug.id, split=\"///\"))[1])\n          drug.2 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == unlist(strsplit(drug.id, split=\"///\"))[2])\n          drug.1.doses <- length(which(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.1], , \"Dose\"])))\n          drug.2.doses <- length(which(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.2], , \"Dose\"])))\n\n          tt <- matrix(NA, ncol=drug.2.doses, nrow=drug.1.doses)\n          colnames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Dose\"]\n          rownames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Dose\"]\n          tt[ ,1] <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Viability\"]\n          tt[1, ] <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Viability\"]\n          tt[2:nrow(tt), 2:ncol(tt)] <- sensitivityRaw(pSet)[exp.id[i], , \"Viability\"]\n          sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- tt\n        }else{\n          sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- sensitivityRaw(pSet)[exp.id[i], , ]\n        }\n      }\n    }\n    return(sensitivity.raw.matrix)\n  }\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'cond' variable in the getRawSensitivityMatrix function, and how is it constructed?",
        "answer": "The 'cond' variable is used to build a conditional string for filtering the sensitivity data. It starts with a base condition checking if the sample ID matches the given cell.id. Additional conditions are appended based on the provided parameters (quality, max.conc, and drug.id) using string concatenation with the '&' operator. This string is later evaluated to select the appropriate experiments."
      },
      {
        "question": "How does the function handle multiple drug IDs, and what is the significance of the '///' separator?",
        "answer": "When multiple drug IDs are provided, the function joins them with '///' as a separator. This is used to handle combination experiments. Later in the function, if a '///' is detected in the drug.id, it splits the string to process each drug separately. The function then creates a matrix representing the combination of doses for both drugs, with viability data for individual drugs on the edges and combination data in the center."
      },
      {
        "question": "What is the structure of the returned 'sensitivity.raw.matrix' list, and how does it differ for single drug vs. combination experiments?",
        "answer": "The 'sensitivity.raw.matrix' is a list where each element corresponds to an experiment. For single drug experiments, each element contains the raw sensitivity data directly from sensitivityRaw(pSet). For combination experiments, each element is a matrix where rows and columns represent doses of the two drugs, cell values represent viability, and the first row and column contain individual drug viability data. The list keys are the row names from sensitivityInfo(pSet) for the selected experiments."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  if(!missing(quality) && \"quality\" %in% colnames(sensitivityInfo(pSet))) {\n    cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n  }\n  if(!missing(max.conc) && \"max.conc\" %in% colnames(sensitivityInfo(pSet))) {\n    max.conc <- if(length(max.conc) > 1) paste(max.conc, collapse=\"///) else max.conc\n    cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n  }\n  drug.id <- if(length(drug.id) > 1) paste(drug.id, collapse=\"///) else drug.id\n  cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      # TODO: Implement logic for single and combination drug experiments\n    }\n  }\n  return(sensitivity.raw.matrix)\n}",
        "complete": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  if(!missing(quality) && \"quality\" %in% colnames(sensitivityInfo(pSet))) {\n    cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n  }\n  if(!missing(max.conc) && \"max.conc\" %in% colnames(sensitivityInfo(pSet))) {\n    max.conc <- if(length(max.conc) > 1) paste(max.conc, collapse=\"///) else max.conc\n    cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n  }\n  drug.id <- if(length(drug.id) > 1) paste(drug.id, collapse=\"///) else drug.id\n  cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      if(grepl(\"///\", drug.id)) {\n        all.exp.id <- which(sensitivityInfo(pSet)$combination.exp.id == sensitivityInfo(pSet)[exp.id[i], \"combination.exp.id\"])\n        drugs <- strsplit(drug.id, \"///\")[[1]]\n        drug.1 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[1])\n        drug.2 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[2])\n        drug.1.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.1], , \"Dose\"]))\n        drug.2.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.2], , \"Dose\"]))\n        \n        tt <- matrix(NA, ncol=drug.2.doses, nrow=drug.1.doses)\n        colnames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Dose\"]\n        rownames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Dose\"]\n        tt[,1] <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Viability\"]\n        tt[1,] <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Viability\"]\n        tt[2:nrow(tt), 2:ncol(tt)] <- sensitivityRaw(pSet)[exp.id[i], , \"Viability\"]\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- tt\n      } else {\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- sensitivityRaw(pSet)[exp.id[i], , ]\n      }\n    }\n  }\n  return(sensitivity.raw.matrix)\n}"
      },
      {
        "partial": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  # TODO: Add conditions for quality, max.conc, and drug.id\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      # TODO: Implement logic for single and combination drug experiments\n    }\n  }\n  return(sensitivity.raw.matrix)\n}",
        "complete": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  if(!missing(quality) && \"quality\" %in% colnames(sensitivityInfo(pSet))) {\n    cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n  }\n  if(!missing(max.conc) && \"max.conc\" %in% colnames(sensitivityInfo(pSet))) {\n    max.conc <- if(length(max.conc) > 1) paste(max.conc, collapse=\"///) else max.conc\n    cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n  }\n  drug.id <- if(length(drug.id) > 1) paste(drug.id, collapse=\"///) else drug.id\n  cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      if(grepl(\"///\", drug.id)) {\n        all.exp.id <- which(sensitivityInfo(pSet)$combination.exp.id == sensitivityInfo(pSet)[exp.id[i], \"combination.exp.id\"])\n        drugs <- strsplit(drug.id, \"///\")[[1]]\n        drug.1 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[1])\n        drug.2 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[2])\n        drug.1.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.1], , \"Dose\"]))\n        drug.2.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.2], , \"Dose\"]))\n        \n        tt <- matrix(NA, ncol=drug.2.doses, nrow=drug.1.doses)\n        colnames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Dose\"]\n        rownames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Dose\"]\n        tt[,1] <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Viability\"]\n        tt[1,] <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Viability\"]\n        tt[2:nrow(tt), 2:ncol(tt)] <- sensitivityRaw(pSet)[exp.id[i], , \"Viability\"]\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- tt\n      } else {\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- sensitivityRaw(pSet)[exp.id[i], , ]\n      }\n    }\n  }\n  return(sensitivity.raw.matrix)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_image_processing.py",
    "language": "py",
    "content": "from readii.loaders import *\nfrom readii.image_processing import *\nimport pytest\n\n@pytest.fixture\ndef nsclcCTImage():\n    nsclcCTPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n    return loadDicomSITK(nsclcCTPath)\n\n@pytest.fixture\ndef nsclcSEGImage():\n    nsclcSEGPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n    segDictionary = loadSegmentation(nsclcSEGPath, modality = 'SEG')\n    return segDictionary['Heart']\n\n@pytest.fixture\ndef lung4DRTSTRUCTImage():\n    lung4DRTSTRUCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-47.35/1-1.dcm\"\n    lung4DCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n    segDictionary = loadSegmentation(lung4DRTSTRUCTPath, modality = 'RTSTRUCT',\n                                     baseImageDirPath = lung4DCTPath, roiNames = 'Tumor_c.*')\n    return segDictionary['Tumor_c40']\n\n\ndef test_flattenImage(nsclcSEGImage):\n    \"\"\"Test removing extra dimension of image that has size 1\"\"\"\n    actual = flattenImage(nsclcSEGImage)\n    assert isinstance(actual, sitk.Image), \\\n        \"Wrong object type, need to convert back to sitk.Image\"\n    assert actual.GetSize() == (512, 512, 304), \\\n        \"Wrong image size\"\n\n\ndef test_alignImages(nsclcCTImage, nsclcSEGImage):\n    \"\"\"Test setting segmentation origin, direction, and spacing to match the CT after flattening\"\"\"\n    flattenedSEG = flattenImage(nsclcSEGImage)\n    actual = alignImages(nsclcCTImage, flattenedSEG)\n    assert actual.GetSize() == (512, 512, 304), \\\n        \"Wrong image size\"\n    assert actual.GetSpacing() == (0.693359375, 0.693359375, 1.0), \\\n        \"Wrong spacing\"\n    assert actual.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0), \\\n        \"Wrong origin\"\n\n@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", 255),\n        (\"lung4DRTSTRUCTImage\", 1)\n    ]\n)\ndef test_getROIVoxelLabel(segImage, expected, request):\n    \"\"\"Test getting the voxel value in the ROI in a segmentation for both SEG and RTSTRUCT images\"\"\"\n    segImage = request.getfixturevalue(segImage)\n    assert getROIVoxelLabel(segImage) == expected\n\n@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", (238, 252, 124)),\n        (\"lung4DRTSTRUCTImage\", (63, 314, 318))\n    ]\n)\ndef test_getROICenterCoords(segImage, expected, request):\n    \"\"\"Test getting the center slice and coordinates for an ROI in both SEG and RTSTRUCT images\"\"\"\n    segImage = request.getfixturevalue(segImage)\n    flatSegImage = flattenImage(segImage)\n    centerSliceIdx, centerColumnPixelIdx, centerRowPixelIdx = getROICenterCoords(flatSegImage)\n    assert centerSliceIdx == expected[0], \\\n        \"Slice number is wrong\"\n    assert centerColumnPixelIdx == expected[1], \\\n        \"Center column pixel value is wrong\"\n    assert centerRowPixelIdx == expected[2], \\\n        \"Center row pixel value is wrong\"",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `flattenImage` function in the given code, and how is it tested?",
        "answer": "The `flattenImage` function is used to remove an extra dimension of an image that has a size of 1. It's tested in the `test_flattenImage` function, which checks if the returned object is an instance of `sitk.Image` and if it has the correct size of (512, 512, 304) after flattening."
      },
      {
        "question": "How does the `alignImages` function work, and what properties of the images does it align?",
        "answer": "The `alignImages` function sets the segmentation image's origin, direction, and spacing to match those of the CT image. It's tested in the `test_alignImages` function, which checks if the aligned image has the correct size (512, 512, 304), spacing (0.693359375, 0.693359375, 1.0), and origin (-182.1533203125, -314.1533203125, -305.0)."
      },
      {
        "question": "What is the purpose of the `@pytest.mark.parametrize` decorator in the `test_getROIVoxelLabel` and `test_getROICenterCoords` functions?",
        "answer": "The `@pytest.mark.parametrize` decorator is used to run the same test function with different input parameters. In this case, it's used to test the `getROIVoxelLabel` and `getROICenterCoords` functions with both SEG and RTSTRUCT image types, ensuring that these functions work correctly for different types of segmentation images."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_flattenImage(nsclcSEGImage):\n    actual = flattenImage(nsclcSEGImage)\n    assert isinstance(actual, sitk.Image)\n    assert actual.GetSize() == (512, 512, 304)",
        "complete": "def test_flattenImage(nsclcSEGImage):\n    actual = flattenImage(nsclcSEGImage)\n    assert isinstance(actual, sitk.Image), \"Wrong object type, need to convert back to sitk.Image\"\n    assert actual.GetSize() == (512, 512, 304), \"Wrong image size\""
      },
      {
        "partial": "@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", 255),\n        (\"lung4DRTSTRUCTImage\", 1)\n    ]\n)\ndef test_getROIVoxelLabel(segImage, expected, request):\n    segImage = request.getfixturevalue(segImage)\n    assert getROIVoxelLabel(segImage) == expected",
        "complete": "@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", 255),\n        (\"lung4DRTSTRUCTImage\", 1)\n    ]\n)\ndef test_getROIVoxelLabel(segImage, expected, request):\n    \"\"\"Test getting the voxel value in the ROI in a segmentation for both SEG and RTSTRUCT images\"\"\"\n    segImage = request.getfixturevalue(segImage)\n    assert getROIVoxelLabel(segImage) == expected"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest"
      ],
      "from_imports": [
        "readii.loaders.*",
        "readii.image_processing.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/__init__.py",
    "language": "py",
    "content": "from . import io, ops, utils, pipeline\n\n__all__ = [\"io\", \"ops\", \"utils\", \"pipeline\"]\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the __all__ variable in this Python module?",
        "answer": "The __all__ variable in this module explicitly defines the public interface of the module. It specifies which names will be imported when a user does 'from module import *'. In this case, it includes 'io', 'ops', 'utils', and 'pipeline', indicating that these are the main components that should be accessible when the module is imported."
      },
      {
        "question": "How does the import statement in this code differ from a standard import, and what does it imply about the module structure?",
        "answer": "The import statement 'from . import io, ops, utils, pipeline' uses a relative import (indicated by the dot '.'). This implies that the current module is part of a package, and it's importing the specified modules (io, ops, utils, pipeline) from the same package. This structure suggests a well-organized, modular codebase where related functionality is grouped together in a package."
      },
      {
        "question": "What would be the effect if the __all__ variable was omitted from this module?",
        "answer": "If the __all__ variable was omitted, it would not affect the relative imports at the top of the file. However, it would change the behavior when someone uses 'from module import *'. Without __all__, Python would import all names that don't start with an underscore. By explicitly defining __all__, the module author has more control over the public interface, potentially hiding implementation details and reducing namespace pollution."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from . import io, ops, utils, pipeline\n\n__all__ = [",
        "complete": "from . import io, ops, utils, pipeline\n\n__all__ = [\"io\", \"ops\", \"utils\", \"pipeline\"]"
      },
      {
        "partial": "from . import io, ops, utils, pipeline\n\n__all__ =",
        "complete": "from . import io, ops, utils, pipeline\n\n__all__ = [\"io\", \"ops\", \"utils\", \"pipeline\"]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "None.io"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/image.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\nfrom typing import NamedTuple, Sequence, Union\n\n# ImageGeometry = namedtuple(\"ImageGeometry\", \"origin, direction, spacing\")\n\nclass ImageGeometry(NamedTuple):\n    size:      Sequence[int]\n    origin:    Sequence[float]\n    direction: Sequence[float]\n    spacing:   Sequence[float]\n\n\ndef physical_point_to_index(point, reference_geometry, continuous=False):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type `ImageGeometry`, `imgtools.Image` or `sitk.Image`, got {type(reference_geometry)}\")\n\n    if continuous:\n        return ref_image.TransformPhysicalPointToContinuousIndex(point[::-1])[::-1]\n    else:\n        return ref_image.TransformPhysicalPointToIndex(point[::-1])[::-1]\n\n\ndef index_to_physical_point(index, reference_geometry):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type ImageGeometry, imgtools.Image or sitk.Image, got {type(reference_geometry)}\")\n\n    continuous = any([isinstance(i, float) for i in index])\n\n    if continuous:\n        return ref_image.TransformContinuousIndexToPhysicalPoint(index[::-1])[::-1]\n    else:\n        return ref_image.TransformIndexToPhysicalPoint(index[::-1])[::-1]\n\n\n# Goals for this module:\n# - One Image Class to Rule Them All (no more juggling numpy & sitk)\n# - consistent indexing (z, y, x) (maybe fancy indexing?)\n# - can be created either from array or sitk.Image\n# - implements all basic operators of sitk.Image\n# - easy conversion to array or sitk\n# - method to apply arbitrary sitk filter\n# - nicer repr\n\n# TODO:\n# - better support for masks (e.g. what happens when we pass a one-hot encoded mask?)\n# - (optional) specify indexing order when creating new Image (sitk or numpy)\n\n\nclass Image:\n    def __init__(self,\n                 image:     Union[sitk.Image, np.ndarray] = None,\n                 geometry:  ImageGeometry   = None,\n                 origin:    Sequence[float] = None,\n                 direction: Sequence[float] = None,\n                 spacing:   Sequence[float] = None):\n\n        if isinstance(image, sitk.Image):\n            self._image = image\n        elif isinstance(image, np.ndarray):\n            if geometry is None and any((origin is None, direction is None, spacing is None)):\n                raise ValueError(\n                    \"If image is a Numpy array, either geometry must be specified.\"\n                )\n\n            if geometry is not None:\n                _, origin, direction, spacing = geometry\n\n            self._image = sitk.GetImageFromArray(image)\n            self._image.SetOrigin(origin[::-1])\n            direction = tuple(direction)\n            self._image.SetDirection(direction[:-3] + direction[3:6] + direction[:3])\n            self._image.SetSpacing(spacing[::-1])\n        else:\n            raise TypeError(\n                f\"image must be either numpy.ndarray or SimpleITK.Image, not {type(image)}.\"\n            )\n\n    @property\n    def size(self):\n        return self._image.GetSize()[::-1]\n\n    @property\n    def origin(self):\n        return self._image.GetOrigin()[::-1]\n\n    @property\n    def direction(self):\n        direction = self._image.GetDirection()\n        direction = direction[:-3] + direction[3:6] + direction[:3]\n        return direction\n\n    @property\n    def spacing(self):\n        return self._image.GetSpacing()[::-1]\n\n    @property\n    def geometry(self):\n        return ImageGeometry(size=self.size,\n                             origin=self.origin,\n                             direction=self.direction,\n                             spacing=self.spacing)\n\n    @property\n    def ndim(self):\n        return len(self.size)\n\n    @property\n    def dtype(self):\n        return self._image.GetPixelIDType()\n\n    def astype(self, new_type):\n        return Image(sitk.Cast(self._image, new_type))\n\n    def to_sitk_image(self):\n        return self._image\n\n    def to_numpy(self, return_geometry=False, view=False):\n        if view:\n            array = sitk.GetArrayViewFromImage(self._image)\n        else:\n            array = sitk.GetArrayFromImage(self._image)\n        if return_geometry:\n            return array, self.geometry\n        return array\n\n    def __getitem__(self, idx):\n        if isinstance(idx, (int, slice)):\n            idx = (idx, )\n        if len(idx) < self.ndim:\n            idx += (slice(None), ) * (self.ndim - len(idx))\n\n        idx = idx[::-1]  # SimpleITK uses (x, y, z) ordering internally\n\n        value = self._image[idx]\n\n        try:  # XXX there probably is a nicer way to check if value is a scalar\n            return Image(value)\n        except TypeError:\n            return value\n\n    def __setitem__(self, idx, value):\n        if isinstance(idx, (int, slice)):\n            idx = (idx, )\n        if len(idx) < self.ndim:\n            idx += (slice(None), ) * (self.ndim - len(idx))\n\n        idx = idx[::-1]  # SimpleITK uses (x, y, z) ordering internally\n\n        value = self._image[idx]\n\n        try:  # XXX there probably is a nicer way to check if value is a scalar\n            return Image(value)\n        except TypeError:\n            return value\n\n    def apply_filter(self, sitk_filter):\n        result = sitk_filter.Execute(self._image)\n        if isinstance(result, sitk.Image):\n            return Image(result)\n        else:\n            return result\n\n    def __neg__(self):\n        return Image(-self._image)\n\n    def __abs__(self):\n        return Image(abs(self._image))\n\n    def __invert__(self):\n        return Image(~self._image)\n\n    def __add__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image + other_val)\n\n    def __sub__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image - other_val)\n\n    def __mul__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image * other_val)\n\n    def __div__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image / other_val)\n\n    def __floordiv__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image / other_val)\n\n    def __pow__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image ** other_val)\n\n    def __iadd__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image += other_val\n\n    def __isub__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image -= other_val\n\n    def __imul__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image *= other_val\n\n    def __idiv__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image /= other_val\n\n    def __ifloordiv__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image //= other_val\n\n    def __iter__(self):\n        pass\n\n    def __repr__(self):\n        return f\"Image(image={self._image}, origin={self.origin}, spacing={self.spacing}, direction={self.direction})\"\n\n    def __str__(self):\n        return f\"origin = {self.origin}\\nspacing = {self.spacing}\\ndirection = {self.direction}\\nvalues = \\n{self.to_numpy(view=True)}\"\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ImageGeometry` class in this code snippet, and how is it structured?",
        "answer": "The `ImageGeometry` class is a NamedTuple that represents the geometric properties of an image. It has four attributes: size (Sequence[int]), origin (Sequence[float]), direction (Sequence[float]), and spacing (Sequence[float]). This class provides a convenient way to store and access the geometric information of an image, which is essential for various image processing operations."
      },
      {
        "question": "Explain the functionality of the `physical_point_to_index` function and its parameters.",
        "answer": "The `physical_point_to_index` function converts a physical point in 3D space to its corresponding index in the image grid. It takes three parameters: 'point' (the physical coordinates to convert), 'reference_geometry' (the geometry of the reference image, which can be an ImageGeometry, imgtools.Image, or sitk.Image), and 'continuous' (a boolean flag to determine if the output should be a continuous index). The function handles different input types for reference_geometry, creates a SimpleITK image if necessary, and then uses SimpleITK's transformation methods to convert the point to an index."
      },
      {
        "question": "Describe the key features and functionalities of the `Image` class in this code snippet.",
        "answer": "The `Image` class is a wrapper around SimpleITK's Image class, providing a more user-friendly interface. Key features include: 1) Initialization from either a SimpleITK Image or a NumPy array with geometry information. 2) Properties for accessing image attributes (size, origin, direction, spacing, geometry, ndim, dtype). 3) Methods for type conversion (astype), exporting to SimpleITK or NumPy formats. 4) Indexing support with SimpleITK's (x, y, z) order converted to (z, y, x). 5) Application of SimpleITK filters through the apply_filter method. 6) Overloaded arithmetic operators for image operations. 7) A custom string representation for easier debugging and visualization."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def physical_point_to_index(point, reference_geometry, continuous=False):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type `ImageGeometry`, `imgtools.Image` or `sitk.Image`, got {type(reference_geometry)}\")\n\n    # Complete the function here",
        "complete": "def physical_point_to_index(point, reference_geometry, continuous=False):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type `ImageGeometry`, `imgtools.Image` or `sitk.Image`, got {type(reference_geometry)}\")\n\n    if continuous:\n        return ref_image.TransformPhysicalPointToContinuousIndex(point[::-1])[::-1]\n    else:\n        return ref_image.TransformPhysicalPointToIndex(point[::-1])[::-1]"
      },
      {
        "partial": "class Image:\n    def __init__(self,\n                 image:     Union[sitk.Image, np.ndarray] = None,\n                 geometry:  ImageGeometry   = None,\n                 origin:    Sequence[float] = None,\n                 direction: Sequence[float] = None,\n                 spacing:   Sequence[float] = None):\n\n        if isinstance(image, sitk.Image):\n            self._image = image\n        elif isinstance(image, np.ndarray):\n            if geometry is None and any((origin is None, direction is None, spacing is None)):\n                raise ValueError(\n                    \"If image is a Numpy array, either geometry must be specified.\"\n                )\n\n            if geometry is not None:\n                _, origin, direction, spacing = geometry\n\n            # Complete the initialization for numpy array here\n        else:\n            raise TypeError(\n                f\"image must be either numpy.ndarray or SimpleITK.Image, not {type(image)}.\"\n            )",
        "complete": "class Image:\n    def __init__(self,\n                 image:     Union[sitk.Image, np.ndarray] = None,\n                 geometry:  ImageGeometry   = None,\n                 origin:    Sequence[float] = None,\n                 direction: Sequence[float] = None,\n                 spacing:   Sequence[float] = None):\n\n        if isinstance(image, sitk.Image):\n            self._image = image\n        elif isinstance(image, np.ndarray):\n            if geometry is None and any((origin is None, direction is None, spacing is None)):\n                raise ValueError(\n                    \"If image is a Numpy array, either geometry must be specified.\"\n                )\n\n            if geometry is not None:\n                _, origin, direction, spacing = geometry\n\n            self._image = sitk.GetImageFromArray(image)\n            self._image.SetOrigin(origin[::-1])\n            direction = tuple(direction)\n            self._image.SetDirection(direction[:-3] + direction[3:6] + direction[:3])\n            self._image.SetSpacing(spacing[::-1])\n        else:\n            raise TypeError(\n                f\"image must be either numpy.ndarray or SimpleITK.Image, not {type(image)}.\"\n            )"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy"
      ],
      "from_imports": [
        "typing.NamedTuple"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/metadata.py",
    "language": "py",
    "content": "import os\nimport pandas as pd\nfrom typing import Optional, Literal\nfrom readii.utils import get_logger\n\nlogger = get_logger()\n\ndef createImageMetadataFile(outputDir, parentDirPath, datasetName, segType, imageFileListPath, update = False):\n    imageMetadataPath = os.path.join(outputDir, \"ct_to_seg_match_list_\" + datasetName + \".csv\")\n    if os.path.exists(imageMetadataPath) and not update:\n        logger.info(f\"Image metadata file {imageMetadataPath} already exists & update flag is {update}.\")\n        return imageMetadataPath\n    elif update:\n        logger.info(f\"{update=}, Image metadata file {imageMetadataPath} will be overwritten.\")\n    else:\n        logger.info(f\"Image metadata file {imageMetadataPath} not found.. creating...\")\n    \n    if segType == \"RTSTRUCT\":\n        imageFileEdgesPath = os.path.join(parentDirPath + \"/.imgtools/imgtools_\" + datasetName + \"_edges.csv\")\n        getCTWithSegmentation(imgFileEdgesPath = imageFileEdgesPath,\n                                segType = segType,\n                                outputFilePath = imageMetadataPath)\n    elif segType == \"SEG\":\n        matchCTtoSegmentation(imgFileListPath = imageFileListPath,\n                                segType = segType,\n                                outputFilePath = imageMetadataPath)\n    else:\n        logger.info(f\"Expecting either RTSTRUCT or SEG segmentation type. Found {segType}.\")\n        raise ValueError(\"Incorrect segmentation type or segmentation type is missing from med-imagetools output. Must be RTSTRUCT or SEG.\")\n    return imageMetadataPath\n\ndef saveDataframeCSV(\n    dataframe: pd.DataFrame, \n    outputFilePath: str\n) -> None:\n    \"\"\"Function to save a pandas Dataframe as a csv file with the index removed.\n            Checks if the path in the outputFilePath exists and will create any missing directories.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        Pandas dataframe to save out as a csv\n    outputFilePath : str\n        Full file path to save the dataframe out to.\n            \n    Raises\n    ------\n    ValueError\n        If the outputFilePath does not end in .csv, if the dataframe is not a pandas DataFrame, \n        or if an error occurs while saving the dataframe.\n    \"\"\"\n    \n    if not outputFilePath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function saves .csv files, so outputFilePath must end in .csv\"\n        )\n\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError(\"Function expects a pandas DataFrame to save out.\")\n\n    # Make directory if it doesn't exist, but don't fail if it already exists\n    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n\n    try:\n        # Save out DataFrame\n        dataframe.to_csv(outputFilePath, index=False)\n    except Exception as e:\n        error_msg = f\"An error occurred while saving the DataFrame: {str(e)}\"\n        raise ValueError(error_msg) from e\n    else:\n        return\n\n\ndef matchCTtoSegmentation(\n    imgFileListPath: str, \n    segType: str, \n    outputFilePath: Optional[str] = None,\n) -> pd.DataFrame:\n    \"\"\"From full list of image files, extract CT and corresponding segmentation files and create new table.\n    One row of the table contains both the CT and segmentation data for one patient.\n    This function currently assumes there is one segmentation for each patient.\n\n    Parameters\n    ----------\n    imgFileListPath : str\n        Path to csv containing list of image directories/paths in the dataset.\n        Expecting output from med-imagetools autopipeline .imgtools_[dataset]\n    segType : str\n        Type of file segmentation is in. Can be SEG or RTSTRUCT.\n    outputDirPath : str\n        Optional file path to save the dataframe to as a csv.\n\n    Returns\n    -------\n    pd.Dataframe\n        Dataframe containing the CT and corresponding segmentation data for each patient\n    \n    Raises\n    ------\n    ValueError\n        If the segmentation file type is not RTSTRUCT or SEG, or if the imgFileListPath does not end in .csv\n\n    Note: All subseries of CT will be kept in the dataframe in this function\n    \"\"\"\n    # Check that segmentation file type is acceptable\n    if segType not in [\"RTSTRUCT\", \"SEG\"]:\n        raise ValueError(\"Incorrect segmentation file type. Must be RTSTRUCT or SEG.\")\n\n    # Check that imgFileListPath is a csv file to properly be loaded in\n    if not imgFileListPath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function expects to load in a .csv file, so imgFileListPath must end in .csv\"\n        )\n\n    if not os.path.exists(imgFileListPath):\n        logger.error(f\"Image file list {imgFileListPath} not found.\")\n        raise FileNotFoundError(\"Image file list not found.\")\n    \n    # Load in complete list of patient image directories of all modalities (output from med-imagetools crawl)\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n\n    # Extract all CT rows\n    allCTRows: pd.DataFrame = fullDicomList.loc[fullDicomList[\"modality\"] == \"CT\"]\n\n    # Extract all SEG rows\n    allSegRows: pd.DataFrame = fullDicomList.loc[fullDicomList[\"modality\"] == segType]\n\n    # Merge the CT and segmentation dataframes based on the CT ID (referenced in the segmentation rows)\n    # Uses only segmentation keys, so no extra CTs are kept\n    # If multiple CTs have the same ID, they are both included in this table\n    samplesWSeg: pd.DataFrame = allCTRows.merge(\n        allSegRows,\n        how=\"inner\",\n        left_on=[\"series\", \"patient_ID\"],\n        right_on=[\"reference_ct\", \"patient_ID\"],\n        suffixes=(\"_CT\", \"_seg\"),\n    )\n\n    # Sort dataframe by ascending patient ID value\n    samplesWSeg.sort_values(by=\"patient_ID\", inplace=True)\n\n    # Save out the combined list\n    if outputFilePath != None:\n        saveDataframeCSV(samplesWSeg, outputFilePath)\n\n    return samplesWSeg\n\n\ndef getCTWithSegmentation(imgFileEdgesPath: str, \n                          segType: str = \"RTSTRUCT\",\n                          outputFilePath: Optional[str] = None,\n) -> pd.DataFrame:\n    \"\"\"From full list of image files edges from med-imagetools, get the list of CTs with segmentation.\n    These are marked as edge type 2 in the edges file.\n    Note: This function can only handle RTSTRUCT segmentations as this is what med-imagetools catches at this point.\n\n    Parameters\n    ----------\n    imgFileEdgesPath : str\n        Path to csv containing list of image directories/paths in the dataset with the edge types.\n        Expecting output from med-imagetools autopipeline .imgtools_[dataset]_edges\n    segType : str\n        Type of file segmentation is in. Must be RTSTRUCT.\n    outputFilePath : Optional[str]\n        Optional file path to save the dataframe to as a csv.\n\n    Returns\n    -------\n    pd.Dataframe\n        Dataframe containing the CT and corresponding segmentation data for each patient\n    \n    Raises\n    ------\n    ValueError\n        If the segmentation file type is not RTSTRUCT, or if the imgFileEdgesPath does not end in .csv\n    \"\"\"\n\n    # Check that segmentation file type is acceptable\n    if segType != \"RTSTRUCT\":\n        raise ValueError(\"Incorrect segmentation file type. Must be RTSTRUCT. For SEG, use matchCTtoSegmentation.\")\n\n    # Check that imgFileListPath is a csv file to properly be loaded in\n    if not imgFileEdgesPath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function expects to load in a .csv file, so imgFileEdgesPath must end in .csv\"\n        )\n    \n    # Load in complete list of patient image directories of all modalities and edge types(output from med-imagetools crawl)\n    fullDicomEdgeList: pd.DataFrame = pd.read_csv(imgFileEdgesPath)\n\n    # Get just the CTs with segmentations, marked as edge type 2 in the edges file\n    samplesWSeg: pd.DataFrame = fullDicomEdgeList.loc[fullDicomEdgeList[\"edge_type\"] == 2]\n\n    # Replace the _x and _y suffixes in the column names with _CT and _seg to match matchCTtoSegmentation\n    samplesWSeg.columns = samplesWSeg.columns.str.replace(\"_x\", \"_CT\", regex=True)\n    samplesWSeg.columns = samplesWSeg.columns.str.replace(\"_y\", \"_seg\", regex=True)\n\n    # Remove the _CT suffix from the patient_ID column to match matchCTtoSegmentation\n    samplesWSeg.rename(columns={\"patient_ID_CT\": \"patient_ID\"}, inplace=True)\n\n    sortedSamplesWSeg = samplesWSeg.sort_values(by=\"patient_ID\")\n\n    # Save out the combined list\n    if outputFilePath != None:\n        saveDataframeCSV(sortedSamplesWSeg, outputFilePath)\n\n    return sortedSamplesWSeg\n\n\ndef getSegmentationType(\n    imgFileListPath: str\n) -> Literal['RTSTRUCT', 'SEG']:\n    \"\"\"Find the segmentation type from the full list of image files.\n\n    Parameters\n    ----------\n    imgFileListPath : str\n        Path to csv containing list of image directories/paths in the dataset.\n        Expecting output from med-imagetools autopipeline .imgtools_[dataset]\n\n    Returns\n    -------\n    str\n        Segmentation type (RTSTRUCT or SEG)\n        \n    Raises\n    ------\n    ValueError\n        If the imgFileListPath does not end in .csv\n    RuntimeError\n        If no suitable segmentation type is found in the dataset\n    \"\"\"\n    # Check that imgFileListPath is a csv file to properly be loaded in\n    if not imgFileListPath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function expects to load in a .csv file, so imgFileListPath must end in .csv\"\n        )\n\n    # Load in complete list of patient image directories of all modalities (output from med-imagetools crawl)\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n\n    # Get list of unique modalities\n    modalities = list(fullDicomList[\"modality\"].unique())\n    logger.debug(f\"Modalities found: {modalities}\")\n    \n    if \"RTSTRUCT\" in modalities:\n        segType = \"RTSTRUCT\"\n    elif \"SEG\" in modalities:\n        segType = \"SEG\"\n    else:\n        raise RuntimeError(\n            \"No suitable segmentation type found. READII can only use RTSTRUCTs and DICOM-SEG segmentations.\"\n        )\n\n    return segType\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `createImageMetadataFile` function and how does it handle different segmentation types?",
        "answer": "The `createImageMetadataFile` function creates or updates a metadata file for CT images and their corresponding segmentations. It handles two segmentation types: RTSTRUCT and SEG. For RTSTRUCT, it calls `getCTWithSegmentation`, and for SEG, it calls `matchCTtoSegmentation`. The function checks if the file already exists and respects an update flag. It returns the path to the created or existing metadata file."
      },
      {
        "question": "Explain the purpose and functionality of the `saveDataframeCSV` function. What error handling does it include?",
        "answer": "The `saveDataframeCSV` function saves a pandas DataFrame as a CSV file without the index. It includes error handling for: 1) Ensuring the output file path ends with '.csv', 2) Verifying that the input is a pandas DataFrame, 3) Creating any missing directories in the output path, and 4) Catching and re-raising any exceptions that occur during the save operation with a custom error message. It also uses a try-except-else block for proper exception handling."
      },
      {
        "question": "How does the `getSegmentationType` function determine the segmentation type, and what types does it support?",
        "answer": "The `getSegmentationType` function determines the segmentation type by analyzing the 'modality' column in a CSV file containing image metadata. It supports two types of segmentations: RTSTRUCT and SEG. The function first checks for 'RTSTRUCT' in the unique modalities list, and if not found, it checks for 'SEG'. If neither is found, it raises a RuntimeError. The function returns a Literal type of either 'RTSTRUCT' or 'SEG'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def getSegmentationType(imgFileListPath: str) -> Literal['RTSTRUCT', 'SEG']:\n    if not imgFileListPath.endswith('.csv'):\n        raise ValueError('This function expects to load in a .csv file, so imgFileListPath must end in .csv')\n\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n    modalities = list(fullDicomList['modality'].unique())\n    logger.debug(f'Modalities found: {modalities}')\n\n    # Complete the function by checking for RTSTRUCT or SEG in modalities\n    # and returning the appropriate segmentation type\n    # Raise a RuntimeError if no suitable segmentation type is found",
        "complete": "def getSegmentationType(imgFileListPath: str) -> Literal['RTSTRUCT', 'SEG']:\n    if not imgFileListPath.endswith('.csv'):\n        raise ValueError('This function expects to load in a .csv file, so imgFileListPath must end in .csv')\n\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n    modalities = list(fullDicomList['modality'].unique())\n    logger.debug(f'Modalities found: {modalities}')\n\n    if 'RTSTRUCT' in modalities:\n        return 'RTSTRUCT'\n    elif 'SEG' in modalities:\n        return 'SEG'\n    else:\n        raise RuntimeError('No suitable segmentation type found. READII can only use RTSTRUCTs and DICOM-SEG segmentations.')"
      },
      {
        "partial": "def saveDataframeCSV(dataframe: pd.DataFrame, outputFilePath: str) -> None:\n    if not outputFilePath.endswith('.csv'):\n        raise ValueError('This function saves .csv files, so outputFilePath must end in .csv')\n\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError('Function expects a pandas DataFrame to save out.')\n\n    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n\n    # Complete the function by saving the dataframe to the specified path\n    # Handle any exceptions that may occur during the save operation",
        "complete": "def saveDataframeCSV(dataframe: pd.DataFrame, outputFilePath: str) -> None:\n    if not outputFilePath.endswith('.csv'):\n        raise ValueError('This function saves .csv files, so outputFilePath must end in .csv')\n\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError('Function expects a pandas DataFrame to save out.')\n\n    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n\n    try:\n        dataframe.to_csv(outputFilePath, index=False)\n    except Exception as e:\n        error_msg = f'An error occurred while saving the DataFrame: {str(e)}'\n        raise ValueError(error_msg) from e"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pandas"
      ],
      "from_imports": [
        "typing.Optional",
        "readii.utils.get_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/report.py",
    "language": "py",
    "content": "",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `isValidParentheses` function in the given code snippet?",
        "answer": "The `isValidParentheses` function checks if a given string of parentheses is valid. It ensures that every opening parenthesis has a corresponding closing parenthesis in the correct order."
      },
      {
        "question": "How does the function handle different types of parentheses (e.g., '(', '[', '{') in the input string?",
        "answer": "The function uses a dictionary `pairs` to map closing parentheses to their corresponding opening parentheses. This allows it to handle multiple types of parentheses, including round '()', square '[]', and curly '{}' brackets."
      },
      {
        "question": "What data structure is used in the `isValidParentheses` function to keep track of opening parentheses, and why is it appropriate for this task?",
        "answer": "The function uses a stack data structure (implemented as a list in Python) to keep track of opening parentheses. This is appropriate because a stack follows the Last-In-First-Out (LIFO) principle, which matches the nested nature of parentheses where the most recently opened parenthesis should be closed first."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            # Complete the code here\n    return -1",
        "complete": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1"
      },
      {
        "partial": "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    # Complete the code here",
        "complete": "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_view.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\n\n\ntest_that(\"AnnotationGx:::.get_all_heading_types\", {\n  res <- AnnotationGx:::.get_all_heading_types()\n  checkmate::expect_data_table(res, min.rows = 1, min.cols = 2, any.missing = FALSE)\n  checkmate::expect_names(names(res), must.include = c(\"Heading\", \"Type\"))\n})\n\n\ntest_that(\"AnnotationGx::getPubchemAnnotationHeadings\", {\n  query <- getPubchemAnnotationHeadings(\"compound\", \"ChEMBL ID\")\n  expect_data_table(query, ncols = 2, nrows = 1)\n  expect_equal(names(query), c(\"Heading\", \"Type\"))\n\n  dt <- capture.output(\n    query <- capture.output(getPubchemAnnotationHeadings(\"compound\", \"fake_placeholder\"), type = c(\"message\"))\n  )\n  assert(any(grepl(\"WARNING\", query)))\n  expect_equal(dt, \"Empty data.table (0 rows and 2 cols): Heading,Type\")\n})\n\ntest_that(\"AnnotationGx::getAnotationHeadings Failure\", {\n  expect_error(getPubchemAnnotationHeadings(\"substance\", \"ChEMBL ID\"))\n})\n\n\n\ntest_that(\"AnnotationGx::annotatePubchemCompound\", {\n  CID <- 176870 # Erlotonib\n  expected <- \"CHEMBL553\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"183321-74-6\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  query <- annotatePubchemCompound(CID, \"ChEMBL ID\", query_only=T)\n  expect_class(query[[1]], \"httr2_request\")\n\n  response <- annotatePubchemCompound(CID, \"ChEMBL ID\", raw=T)\n  expect_class(response[[1]], \"httr2_response\")\n\n  expected <- NA_character_\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"L01EB02\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT01214\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n\n  CID <- 3672 # Ibuprofen\n  expected <- \"CHEMBL521\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"15687-27-1\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  expected <- \"NSC 757073; NSC 256857\"\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"M02AA13; C01EB16; R02AX02; G02CC01; M01AE01\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT00199\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\"))\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\", parse_function = fake_parser))\n  \n  fake_parser <- function(x) {\n    return(data.table::data.table(Heading = \"CAS\", Value = \"fake_value\"))\n  }\n  annotatePubchemCompound(CID, heading = \"CAS\", parse_function = fake_parser)\n})\n\ntest_that(\"AnnotationGx:::.build_pubchem_view_query\", {\n  # Test case 1: Test with default parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(id = \"12345\")\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/12345/JSON\"\n  expect_equal(query$url, expected_url)\n\n  # Test case 2: Test with custom parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\", page = 2\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/67890/JSON?page=2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", heading = \"ChEMBL ID\", output = \"XML\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/XML?heading=ChEMBL%20ID\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", output = \"JSON\", source = \"DrugBank\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?source=DrugBank\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"substance\", version = \"1.2\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/176870/JSON?version=1.2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", version = 1\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?version=1\"\n  expect_equal(query$url, expected_url)\n})\n\n\ntest_that(\"AnnotationGx:::.build_pubchem_view_query Failure\", {\n  # Test case 1: Test with invalid annotation\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\",\n    page = 2, version = 1, heading = \"Heading1\", source = \"Source1\", output = \"XML\"\n  ))\n  expect_error(AnnotationGx:::.build_pubchem_view_query(id = \"67890\", record = \"substance\", version = 1.5))\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"substance\", version = 1\n  ))\n\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", output = \"JSON\", source = \"\"\n  ))\n\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"compound\", heading = \"fale\"\n  ))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `AnnotationGx:::.get_all_heading_types` function and how is it tested?",
        "answer": "The `AnnotationGx:::.get_all_heading_types` function likely retrieves all available heading types for PubChem annotations. It's tested using `testthat` to ensure it returns a data table with at least one row and two columns, containing 'Heading' and 'Type' columns without any missing values."
      },
      {
        "question": "How does the `annotatePubchemCompound` function handle different types of annotations, and what are some examples of its usage?",
        "answer": "The `annotatePubchemCompound` function retrieves specific annotations for a given PubChem Compound ID (CID). It can handle various annotation types such as 'ChEMBL ID', 'CAS', 'NSC Number', 'ATC Code', and 'Drug Induced Liver Injury'. The function is tested with two compounds (Erlotinib and Ibuprofen) and various annotation types. It also supports options like `query_only` to return the request object and `raw` to return the raw response."
      },
      {
        "question": "What is the purpose of the `AnnotationGx:::.build_pubchem_view_query` function and how does it handle different parameters?",
        "answer": "The `AnnotationGx:::.build_pubchem_view_query` function constructs a URL for querying the PubChem PUG View API. It can handle various parameters such as `id`, `record` type (compound or substance), `page`, `heading`, `output` format, `source`, and API `version`. The function builds the appropriate URL based on these parameters, encoding them correctly in the query string. It also includes error checking for invalid parameter combinations or values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"AnnotationGx:::.build_pubchem_view_query\", {\n  # Test case 1: Test with default parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(id = \"12345\")\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/12345/JSON\"\n  expect_equal(query$url, expected_url)\n\n  # Test case 2: Test with custom parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\", page = 2\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/67890/JSON?page=2\"\n  expect_equal(query$url, expected_url)\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"AnnotationGx:::.build_pubchem_view_query\", {\n  # Test case 1: Test with default parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(id = \"12345\")\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/12345/JSON\"\n  expect_equal(query$url, expected_url)\n\n  # Test case 2: Test with custom parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\", page = 2\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/67890/JSON?page=2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", heading = \"ChEMBL ID\", output = \"XML\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/XML?heading=ChEMBL%20ID\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", output = \"JSON\", source = \"DrugBank\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?source=DrugBank\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"substance\", version = \"1.2\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/176870/JSON?version=1.2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", version = 1\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?version=1\"\n  expect_equal(query$url, expected_url)\n})"
      },
      {
        "partial": "test_that(\"AnnotationGx::annotatePubchemCompound\", {\n  CID <- 176870 # Erlotonib\n  expected <- \"CHEMBL553\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"183321-74-6\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  query <- annotatePubchemCompound(CID, \"ChEMBL ID\", query_only=T)\n  expect_class(query[[1]], \"httr2_request\")\n\n  response <- annotatePubchemCompound(CID, \"ChEMBL ID\", raw=T)\n  expect_class(response[[1]], \"httr2_response\")\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"AnnotationGx::annotatePubchemCompound\", {\n  CID <- 176870 # Erlotonib\n  expected <- \"CHEMBL553\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"183321-74-6\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  query <- annotatePubchemCompound(CID, \"ChEMBL ID\", query_only=T)\n  expect_class(query[[1]], \"httr2_request\")\n\n  response <- annotatePubchemCompound(CID, \"ChEMBL ID\", raw=T)\n  expect_class(response[[1]], \"httr2_response\")\n\n  expected <- NA_character_\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"L01EB02\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT01214\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n  CID <- 3672 # Ibuprofen\n  expected <- \"CHEMBL521\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"15687-27-1\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  expected <- \"NSC 757073; NSC 256857\"\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"M02AA13; C01EB16; R02AX02; G02CC01; M01AE01\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT00199\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\"))\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\", parse_function = fake_parser))\n  \n  fake_parser <- function(x) {\n    return(data.table::data.table(Heading = \"CAS\", Value = \"fake_value\"))\n  }\n  annotatePubchemCompound(CID, heading = \"CAS\", parse_function = fake_parser)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/nifti_to_dicom.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nfrom rt_utils import RTStructBuilder\nimport logging\nimport numpy as np\nimport os\n\n'''You need to install rt_utils first using \n    pip install rt_utils'''\n\n\ndef segmentations_to_dicom(save_path:str,orig_series_info:dict, segmentation:sitk.Image,segmentations_labels:dict,\n                             color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    \"\"\"\n    This function takes a list of the original dicom data, moves it to the save_path and creates a corresponding rt_struct\n    @param image_set_name: name of the image set used to create the dicom sub directory\n    @param save_path: directory to save dicom data\n    @param orig_series_info: original series info see @get_dicom_series_dict_pyd\n    @param segmentation: the image containing integer segmentation data\n    @param segmentations_labels: a dictionary with segmentation label information mapping ints in @segmentation to label\n    name e.g. {1: 'Bladder', 2: 'Rectum', 3: 'Sigmoid', 4: 'SmallBowel'}\n    @return:\n    \"\"\"\n    _logger=logging.getLogger(__name__)\n    #make output dir\n    if not os.path.isdir(save_path):\n        _logger.warning(f'Making path: {save_path}')\n        os.makedirs(save_path)\n\n\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i,(key,name) in enumerate(segmentations_labels.items()):\n        mask = np.where(im_array!=key,0,im_array)\n        mask = np.array(mask,dtype=bool)\n        mask = np.transpose(mask, (1,2,0))\n\n        index = i%len(color_list)\n\n        rtstruct.add_roi(mask=mask, name=name, color=color_list[index],approximate_contours=False)\n\n\n    rtstruct_name = os.path.join(save_path,rtstruct_basefilename)\n    _logger.info(f'Saving rtstruct data: {rtstruct_name}')\n    rtstruct.save(rtstruct_name)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `segmentations_to_dicom` function, and what are its main input parameters?",
        "answer": "The `segmentations_to_dicom` function is designed to create a DICOM RT-Structure file from segmentation data. Its main input parameters are:\n1. `save_path`: directory to save the DICOM data\n2. `orig_series_info`: original series information\n3. `segmentation`: a SimpleITK Image containing integer segmentation data\n4. `segmentations_labels`: a dictionary mapping integer labels to segmentation names\n5. `color_list`: an optional list of colors for the segmentations\n6. `rtstruct_basefilename`: the filename for the output RT-Structure file"
      },
      {
        "question": "How does the function handle the conversion of the segmentation data to a format suitable for the RT-Structure file?",
        "answer": "The function converts the segmentation data to a format suitable for the RT-Structure file through these steps:\n1. It extracts the numpy array from the SimpleITK Image using `sitk.GetArrayFromImage(segmentation)`.\n2. For each label in the `segmentations_labels` dictionary:\n   a. It creates a binary mask for the current label using numpy operations.\n   b. The mask is transposed to match the expected dimensions (x,y,z).\n   c. The mask is added to the RT-Structure using `rtstruct.add_roi()`, along with the label name and a color from the `color_list`.\n3. Finally, it saves the RT-Structure file using `rtstruct.save()`."
      },
      {
        "question": "What external libraries does this code rely on, and what are their roles in the function?",
        "answer": "The code relies on several external libraries:\n1. SimpleITK (imported as sitk): Used for handling medical imaging data, particularly for reading and writing DICOM files.\n2. rt_utils: Provides the RTStructBuilder class, which is used to create and manipulate RT-Structure files.\n3. numpy (imported as np): Used for array operations on the segmentation data.\n4. os: Used for file and directory operations.\n5. logging: Used for logging information and warnings during the execution of the function.\n\nThese libraries work together to process the segmentation data, create the RT-Structure file, and handle file operations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def segmentations_to_dicom(save_path:str, orig_series_info:dict, segmentation:sitk.Image, segmentations_labels:dict, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    if not os.path.isdir(save_path):\n        os.makedirs(save_path)\n\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i, (key, name) in enumerate(segmentations_labels.items()):\n        # Complete the code to create and add ROI to rtstruct\n\n    rtstruct_name = os.path.join(save_path, rtstruct_basefilename)\n    rtstruct.save(rtstruct_name)",
        "complete": "def segmentations_to_dicom(save_path:str, orig_series_info:dict, segmentation:sitk.Image, segmentations_labels:dict, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    if not os.path.isdir(save_path):\n        os.makedirs(save_path)\n\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i, (key, name) in enumerate(segmentations_labels.items()):\n        mask = np.where(im_array != key, 0, im_array)\n        mask = np.array(mask, dtype=bool)\n        mask = np.transpose(mask, (1, 2, 0))\n        index = i % len(color_list)\n        rtstruct.add_roi(mask=mask, name=name, color=color_list[index], approximate_contours=False)\n\n    rtstruct_name = os.path.join(save_path, rtstruct_basefilename)\n    rtstruct.save(rtstruct_name)"
      },
      {
        "partial": "import SimpleITK as sitk\nfrom rt_utils import RTStructBuilder\nimport numpy as np\nimport os\n\ndef segmentations_to_dicom(save_path, orig_series_info, segmentation, segmentations_labels, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    # Complete the function implementation\n    pass",
        "complete": "import SimpleITK as sitk\nfrom rt_utils import RTStructBuilder\nimport numpy as np\nimport os\n\ndef segmentations_to_dicom(save_path, orig_series_info, segmentation, segmentations_labels, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    os.makedirs(save_path, exist_ok=True)\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i, (key, name) in enumerate(segmentations_labels.items()):\n        mask = (im_array == key).astype(bool)\n        mask = np.transpose(mask, (1, 2, 0))\n        index = i % len(color_list)\n        rtstruct.add_roi(mask=mask, name=name, color=color_list[index], approximate_contours=False)\n    rtstruct.save(os.path.join(save_path, rtstruct_basefilename))"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "logging",
        "numpy",
        "os"
      ],
      "from_imports": [
        "rt_utils.RTStructBuilder"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_unichem.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"getUnichemSources returns a data.table with the correct columns\", {\n  sources <- getUnichemSources(all_columns = TRUE)\n  \n  expected_columns <- c(\n    \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n    \"BaseURL\", \"URL\", \"Details\", \"Description\", \"ReleaseNumber\", \n    \"ReleaseDate\", \"LastUpdated\", \"UpdateComments\"\n  )\n  \n  expect_data_table(\n    sources,\n    all.missing = FALSE,\n    min.rows = 40, # As of March 2024\n    min.cols = 13, # As of March 2024\n    col.names = 'named',\n    info = \"The data.table should have the correct columns. \n        The min number of rows and columns may change over time and is set on\n        from UniChem as of March 2024.\",\n    )\n})\n\n\ntest_that(\"queryUnichemCompound returns the expected results\", {\n  # Test case 1\n  result1 <- queryUnichemCompound(type = \"sourceID\", compound = \"444795\", sourceID = 22)\n  expect_true(is.list(result1))\n  expect_true(\"External_Mappings\" %in% names(result1))\n  expect_true(\"UniChem_Mappings\" %in% names(result1))\n  \n  # Test case 2\n  expect_error(queryUnichemCompound(type = \"inchikey\", compound = \"InchiKey123\"))\n\n})\n\ntest_that(\"queryUnichemCompound returns the expected results 2\", {\n  # Test case 1\n  result1 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = T)\n\n  expect_true(is.list(result1))\n\n\n  checkmate::expect_names(\n    names(result1), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(result1$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n\n  result2 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = F)\n\n  expect_true(is.list(result2))\n\n  checkmate::expect_names(\n    names(result2$External_Mappings),\n    subset.of = c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n  )\n\n  checkmate::expect_names(\n    names(result2$UniChem_Mappings),\n    subset.of = c(\n      \"UniChem.UCI\", \"UniChem.InchiKey\", 'UniChem.Inchi',\n      'UniChem.formula','UniChem.connections','UniChem.hAtoms'\n    )\n  )\n\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getUnichemSources` function and what are the key expectations in its test?",
        "answer": "The `getUnichemSources` function retrieves information about UniChem data sources. The test expects it to return a data.table with specific columns, including 'Name', 'SourceID', 'CompoundCount', etc. It should have at least 40 rows and 13 columns as of March 2024, though these numbers may change over time. The test uses `expect_data_table` to verify the structure and content of the returned data."
      },
      {
        "question": "How does the `queryUnichemCompound` function behave with different input types, and what are the expected outputs?",
        "answer": "The `queryUnichemCompound` function accepts different input types and returns varying results. When called with `type = 'sourceID'` and valid compound and sourceID, it returns a list containing 'External_Mappings' and 'UniChem_Mappings'. When called with `type = 'inchikey'` and `raw = TRUE`, it returns a list with keys like 'compounds', 'notFound', 'response', and 'totalCompounds'. With `raw = FALSE`, it returns a list with 'External_Mappings' and 'UniChem_Mappings', each containing specific subfields. The function throws an error for invalid inputs, such as an incorrect InChIKey."
      },
      {
        "question": "What testing libraries are used in this code, and how are they applied to ensure the correctness of the UniChem-related functions?",
        "answer": "The code uses three testing libraries: testthat, AnnotationGx, and checkmate. testthat is the primary testing framework, providing the `test_that` function to group related tests. AnnotationGx likely contains the UniChem-related functions being tested. checkmate is used for additional assertions, particularly `expect_names` to verify the structure of returned objects. These libraries are applied to test the behavior of `getUnichemSources` and `queryUnichemCompound` functions, checking for correct return types, expected data structures, and proper error handling."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"getUnichemSources returns a data.table with the correct columns\", {\n  sources <- getUnichemSources(all_columns = TRUE)\n  \n  expected_columns <- c(\n    \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n    \"BaseURL\", \"URL\", \"Details\", \"Description\", \"ReleaseNumber\", \n    \"ReleaseDate\", \"LastUpdated\", \"UpdateComments\"\n  )\n  \n  expect_data_table(\n    sources,\n    all.missing = FALSE,\n    min.rows = 40,\n    min.cols = 13,\n    col.names = 'named',\n    info = \"The data.table should have the correct columns. \n        The min number of rows and columns may change over time and is set on\n        from UniChem as of March 2024.\",\n    )\n})",
        "complete": "test_that(\"getUnichemSources returns a data.table with the correct columns\", {\n  sources <- getUnichemSources(all_columns = TRUE)\n  \n  expected_columns <- c(\n    \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n    \"BaseURL\", \"URL\", \"Details\", \"Description\", \"ReleaseNumber\", \n    \"ReleaseDate\", \"LastUpdated\", \"UpdateComments\"\n  )\n  \n  expect_data_table(\n    sources,\n    all.missing = FALSE,\n    min.rows = 40,\n    min.cols = 13,\n    col.names = 'named',\n    info = \"The data.table should have the correct columns. \n        The min number of rows and columns may change over time and is set on\n        from UniChem as of March 2024.\",\n  )\n  expect_true(all(expected_columns %in% colnames(sources)))\n})"
      },
      {
        "partial": "test_that(\"queryUnichemCompound returns the expected results 2\", {\n  result1 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = T)\n\n  expect_true(is.list(result1))\n\n  checkmate::expect_names(\n    names(result1), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(result1$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n\n  result2 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = F)\n\n  expect_true(is.list(result2))\n\n  # Add assertions for result2 here\n})",
        "complete": "test_that(\"queryUnichemCompound returns the expected results 2\", {\n  result1 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = T)\n\n  expect_true(is.list(result1))\n\n  checkmate::expect_names(\n    names(result1), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(result1$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n\n  result2 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = F)\n\n  expect_true(is.list(result2))\n\n  checkmate::expect_names(\n    names(result2$External_Mappings),\n    subset.of = c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n  )\n\n  checkmate::expect_names(\n    names(result2$UniChem_Mappings),\n    subset.of = c(\n      \"UniChem.UCI\", \"UniChem.InchiKey\", 'UniChem.Inchi',\n      'UniChem.formula','UniChem.connections','UniChem.hAtoms'\n    )\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-httr2.R",
    "language": "R",
    "content": "#' Builds a URL by concatenating the input arguments and encoding it.\n#'\n#' @param ... The components of the URL.\n#' @return The encoded URL.\n#' @noRd\n#' @keywords internal\n.buildURL <- function(...) {\n  paste0(stats::na.omit(unlist(list(...))), collapse = \"/\") |> utils::URLencode()\n}\n\n#' Builds an HTTP request using the provided URL.\n#'\n#' @param url The URL for the request.\n#' @return The built HTTP request.\n#' @noRd\n#' @keywords internal\n.build_request <- function(url) {\n  httr2::request(url) |>\n    httr2::req_retry(max_tries = 5, backoff = ~ 10) |>\n    httr2::req_error(is_error = \\(resp) FALSE)\n}\n\n#' Performs an HTTP request.\n#'\n#' @param request The HTTP request to perform.\n#' @return The response of the HTTP request.\n#' @noRd\n#' @keywords internal\n.perform_request <- function(request) {\n  httr2::req_perform(request)\n}\n\n#' Performs an HTTP request in parallel.\n#' @param reqs The HTTP requests to perform.\n#' @param on_error The action to take when an error occurs. Can be \"stop\" or \"continue\".\n#' @param progress Whether to show a progress bar.\n#'\n#' @return The responses of the HTTP requests.\n#'\n#' @noRd\n#' @keywords internal\n.perform_request_parallel <- function(reqs, on_error = \"continue\", progress = TRUE, ...) {\n  httr2::req_perform_parallel(reqs, on_error = on_error, progress = progress, ...)\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @param resp The response object from the HTTP request.\n#' @return The parsed JSON response.\n#' @noRd\n#' @keywords internal\n.parse_resp_json <- function(resp, simplifyVector = TRUE) {\n  httr2::resp_body_json(resp, simplifyVector = simplifyVector)\n}\n\n\n#' Parses the TSV response from an HTTP request.\n#' @param resp The response object from the HTTP request.\n#' @return The parsed TSV response.\n#' @noRd\n#' @keywords internal\n.parse_resp_tsv <- function(resp, show_col_types = FALSE, skip = 0) {\n  readr::read_tsv(resp$body, skip = skip, show_col_types = show_col_types)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.buildURL` function and how does it handle its input?",
        "answer": "The `.buildURL` function is designed to construct a URL by concatenating its input arguments and encoding the result. It takes any number of arguments (`...`), combines them into a single URL string using '/' as a separator, and then URL-encodes the result. The function uses `stats::na.omit` and `unlist` to handle potential NA values and nested lists in the input, ensuring a clean URL string."
      },
      {
        "question": "How does the `.build_request` function enhance the reliability of HTTP requests?",
        "answer": "The `.build_request` function enhances the reliability of HTTP requests in two ways: 1) It implements a retry mechanism using `httr2::req_retry`, allowing up to 5 attempts with a backoff strategy that increases the wait time between retries. 2) It modifies the error handling behavior with `httr2::req_error(is_error = \\(resp) FALSE)`, which prevents the request from automatically throwing an error on HTTP status codes typically considered as errors. This allows for more flexible error handling in the calling code."
      },
      {
        "question": "What are the key differences between `.perform_request` and `.perform_request_parallel`, and when would you use each?",
        "answer": "`.perform_request` is used for sending a single HTTP request synchronously, while `.perform_request_parallel` is designed for sending multiple requests concurrently. The parallel version takes an array of requests, has options for error handling (`on_error`) and displaying a progress bar. You would use `.perform_request` for simple, one-off API calls, and `.perform_request_parallel` when you need to make multiple API calls efficiently, such as batch processing or when dealing with paginated API responses."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Builds a URL by concatenating the input arguments and encoding it.\n#'\n#' @param ... The components of the URL.\n#' @return The encoded URL.\n#' @noRd\n#' @keywords internal\n.buildURL <- function(...) {\n  # Complete the function body\n}",
        "complete": "#' Builds a URL by concatenating the input arguments and encoding it.\n#'\n#' @param ... The components of the URL.\n#' @return The encoded URL.\n#' @noRd\n#' @keywords internal\n.buildURL <- function(...) {\n  paste0(stats::na.omit(unlist(list(...))), collapse = \"/\") |> utils::URLencode()\n}"
      },
      {
        "partial": "#' Performs an HTTP request in parallel.\n#' @param reqs The HTTP requests to perform.\n#' @param on_error The action to take when an error occurs. Can be \"stop\" or \"continue\".\n#' @param progress Whether to show a progress bar.\n#'\n#' @return The responses of the HTTP requests.\n#'\n#' @noRd\n#' @keywords internal\n.perform_request_parallel <- function(reqs, on_error = \"continue\", progress = TRUE, ...) {\n  # Complete the function body\n}",
        "complete": "#' Performs an HTTP request in parallel.\n#' @param reqs The HTTP requests to perform.\n#' @param on_error The action to take when an error occurs. Can be \"stop\" or \"continue\".\n#' @param progress Whether to show a progress bar.\n#'\n#' @return The responses of the HTTP requests.\n#'\n#' @noRd\n#' @keywords internal\n.perform_request_parallel <- function(reqs, on_error = \"continue\", progress = TRUE, ...) {\n  httr2::req_perform_parallel(reqs, on_error = on_error, progress = progress, ...)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeSlope.R",
    "language": "R",
    "content": "#' Return Slope (normalized slope of the drug response curve) for an experiment of a pSet by taking\n#' its concentration and viability as input.\n#'\n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeSlope(dose, viability)\n#'\n#' @param concentration `numeric` A concentration range that the AUC should be computed for that range.\n#' Concentration range by default considered as not logarithmic scaled. Converted to numeric by function if necessary.\n#' @param viability `numeric` Viablities corresponding to the concentration range passed as first parameter.\n#' The range of viablity values by definition should be between 0 and 100. But the viabalities greater than\n#' 100 and lower than 0 are also accepted.\n#' @param trunc `logical(1)` A flag that identify if the viabality values should be truncated to be in the\n#' range of (0,100)\n#' @param verbose `logical(1)` If 'TRUE' the function will retrun warnings and other infomrative messages.\n#' @return Returns the normalized linear slope of the drug response curve\n#'\n#' @export\ncomputeSlope <- function(concentration, viability, trunc=TRUE, verbose=TRUE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n  ##convert to nanomolar with the assumption that always concentrations are in micro molar\n  concentration <- concentration\n  concentration <- log10(concentration) + 6\n  if(trunc) {\n    viability <- pmin(viability, 100)\n    viability <- pmax(viability, 0)\n  }\n\n  most.sensitive <- NULL\n  for(dose in concentration)\n  {\n    most.sensitive <- rbind(most.sensitive, cbind(dose,0))\n  }\n\n  slope.prime <- .optimizeRegression(x = most.sensitive[,1], y = most.sensitive[,2])\n  slope <- .optimizeRegression(x = concentration, y = viability)\n  slope <- round(slope/abs(slope.prime),digits=2)\n  return(-slope)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeSlope` function and what are its main input parameters?",
        "answer": "The `computeSlope` function calculates the normalized slope of a drug response curve. Its main input parameters are `concentration` (a numeric vector of drug concentrations) and `viability` (a numeric vector of corresponding cell viabilities). It also has optional parameters `trunc` (to truncate viability values) and `verbose` (for displaying messages)."
      },
      {
        "question": "How does the function handle concentration values of zero, and why is this important?",
        "answer": "The function removes concentration values of zero and their corresponding viability values using the line `if(length(ii) > 0) { concentration <- concentration[-ii]; viability <- viability[-ii] }`. This is important because log10(0) is undefined, and the function later applies log10 to the concentration values. Removing zero values prevents errors in the calculation."
      },
      {
        "question": "What is the purpose of the `most.sensitive` variable in the `computeSlope` function, and how is it used?",
        "answer": "The `most.sensitive` variable creates a reference curve representing the most sensitive possible drug response. It's a matrix where the first column is the log-transformed concentrations, and the second column is all zeros (representing complete cell death). This reference curve is used to normalize the actual slope by comparing it to the slope of this most sensitive case, which is calculated using `.optimizeRegression(x = most.sensitive[,1], y = most.sensitive[,2])`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeSlope <- function(concentration, viability, trunc=TRUE, verbose=TRUE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n  concentration <- log10(concentration) + 6\n  if(trunc) {\n    viability <- pmin(viability, 100)\n    viability <- pmax(viability, 0)\n  }\n\n  # Complete the code to calculate and return the slope\n}",
        "complete": "computeSlope <- function(concentration, viability, trunc=TRUE, verbose=TRUE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n  concentration <- log10(concentration) + 6\n  if(trunc) {\n    viability <- pmin(viability, 100)\n    viability <- pmax(viability, 0)\n  }\n\n  most.sensitive <- matrix(c(concentration, rep(0, length(concentration))), ncol=2)\n  slope.prime <- .optimizeRegression(x = most.sensitive[,1], y = most.sensitive[,2])\n  slope <- .optimizeRegression(x = concentration, y = viability)\n  slope <- round(slope/abs(slope.prime), digits=2)\n  return(-slope)\n}"
      },
      {
        "partial": ".optimizeRegression <- function(x, y) {\n  # Implement the regression optimization function\n}",
        "complete": ".optimizeRegression <- function(x, y) {\n  fit <- lm(y ~ x)\n  return(coef(fit)[2])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/crawl.py",
    "language": "py",
    "content": "from argparse import ArgumentParser\nimport os\nimport pathlib\nimport glob\nimport json\nimport pandas as pd\nfrom pydicom import dcmread\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\n\ndef crawl_one(folder):\n    folder_path = pathlib.Path(folder)\n    database = {}\n    for path, _, _ in os.walk(folder):\n        # find dicoms\n        dicoms = glob.glob(pathlib.Path(path, \"**\", \"*.dcm\").as_posix(), recursive=True)\n        # print('\\n', folder, dicoms)\n        # instance (slice) information\n        for dcm in dicoms:\n            try:\n                dcm_path  = pathlib.Path(dcm)\n                # parent    = dcm_path.parent#.as_posix()\n                fname     = dcm_path.name\n                rel_path  = dcm_path.relative_to(folder_path.parent.parent)                                        # rel_path of dicom from folder\n                rel_posix = rel_path.parent.as_posix()     # folder name + until parent folder of dicom\n\n                meta      = dcmread(dcm, force=True, stop_before_pixels=True)\n                patient   = str(meta.PatientID)\n                study     = str(meta.StudyInstanceUID)\n                series    = str(meta.SeriesInstanceUID)\n                instance  = str(meta.SOPInstanceUID)\n\n                reference_ct, reference_rs, reference_pl,  = \"\", \"\", \"\"\n                tr, te, tesla, scan_seq, elem = \"\", \"\", \"\", \"\", \"\"\n                try:\n                    orientation = str(meta.ImageOrientationPatient)  # (0020, 0037)\n                except:\n                    orientation = \"\"\n\n                try:\n                    orientation_type = str(meta.AnatomicalOrientationType)  # (0010, 2210)\n                except:\n                    orientation_type = \"\"\n\n                try:  # RTSTRUCT\n                    reference_ct = str(meta.ReferencedFrameOfReferenceSequence[0].RTReferencedStudySequence[0].RTReferencedSeriesSequence[0].SeriesInstanceUID)\n                except: \n                    try: # SEGMENTATION\n                        reference_ct = str(meta.ReferencedSeriesSequence[0].SeriesInstanceUID)\n                    except:\n                        try:  # RTDOSE\n                            reference_rs = str(meta.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID)\n                        except:\n                            pass\n                        try:\n                            reference_ct = str(meta.ReferencedImageSequence[0].ReferencedSOPInstanceUID)\n                        except:\n                            pass\n                        try:\n                            reference_pl = str(meta.ReferencedRTPlanSequence[0].ReferencedSOPInstanceUID)\n                        except:\n                            pass\n                \n                # MRI Tags\n                try:\n                    tr = float(meta.RepetitionTime)\n                except:\n                    pass\n                try:\n                    te = float(meta.EchoTime)\n                except:\n                    pass\n                try:\n                    scan_seq = str(meta.ScanningSequence)\n                except:\n                    pass\n                try:\n                    tesla = float(meta.MagneticFieldStrength)\n                except:\n                    pass\n                try:\n                    elem = str(meta.ImagedNucleus)\n                except:\n                    pass\n                \n                # Frame of Reference UIDs\n                try:\n                    reference_frame = str(meta.FrameOfReferenceUID)\n                except:\n                    try:\n                        reference_frame = str(meta.ReferencedFrameOfReferenceSequence[0].FrameOfReferenceUID)\n                    except:\n                        reference_frame = \"\"\n        \n                try:\n                    study_description = str(meta.StudyDescription)\n                except:\n                    study_description = \"\"\n\n                try:\n                    series_description = str(meta.SeriesDescription)\n                except:\n                    series_description = \"\"\n\n                try:\n                    subseries = str(meta.AcquisitionNumber)\n                except:\n                    subseries = \"default\"\n\n                if patient not in database:\n                    database[patient] = {}\n                if study not in database[patient]:\n                    database[patient][study] = {'description': study_description}\n                if series not in database[patient][study]:\n                    rel_crawl_path = rel_posix\n                    if meta.Modality == 'RTSTRUCT':\n                        rel_crawl_path = os.path.join(rel_crawl_path, fname)\n                    \n                    database[patient][study][series] = {'description': series_description}\n                if subseries not in database[patient][study][series]:\n                    database[patient][study][series][subseries] = {'instances': {},\n                                                                   'instance_uid': instance,\n                                                                   'modality': meta.Modality,\n                                                                   'reference_ct': reference_ct,\n                                                                   'reference_rs': reference_rs,\n                                                                   'reference_pl': reference_pl,\n                                                                   'reference_frame': reference_frame,\n                                                                   'folder': rel_crawl_path,\n                                                                   'orientation': orientation,\n                                                                   'orientation_type': orientation_type,\n                                                                   'repetition_time':tr,\n                                                                   'echo_time':te,\n                                                                   'scan_sequence': scan_seq,\n                                                                   'mag_field_strength': tesla,\n                                                                   'imaged_nucleus': elem,\n                                                                   'fname': rel_path.as_posix()  # temporary until we switch to json-based loading\n                                                                   }\n                database[patient][study][series][subseries]['instances'][instance] = rel_path.as_posix()\n            except Exception as e:\n                print(folder, e)\n                pass\n    \n    return database\n\n\ndef to_df(database_dict):\n    df = pd.DataFrame()\n    for pat in database_dict:\n        for study in database_dict[pat]:\n            for series in database_dict[pat][study]:\n                if series != 'description':  # skip description key in dict\n                    for subseries in database_dict[pat][study][series]:\n                        if subseries != 'description':  # skip description key in dict\n                            columns = ['patient_ID', 'study', 'study_description', \n                                       'series', 'series_description', 'subseries', 'modality', \n                                       'instances', 'instance_uid', \n                                       'reference_ct', 'reference_rs', 'reference_pl', 'reference_frame', 'folder',\n                                       'orientation', 'orientation_type', 'MR_repetition_time', 'MR_echo_time', \n                                       'MR_scan_sequence', 'MR_magnetic_field_strength', 'MR_imaged_nucleus', 'file_path']\n                            values = [pat, study, database_dict[pat][study]['description'], \n                                      series, database_dict[pat][study][series]['description'], \n                                      subseries, database_dict[pat][study][series][subseries]['modality'], \n                                      len(database_dict[pat][study][series][subseries]['instances']), database_dict[pat][study][series][subseries]['instance_uid'], \n                                      database_dict[pat][study][series][subseries]['reference_ct'], database_dict[pat][study][series][subseries]['reference_rs'], \n                                      database_dict[pat][study][series][subseries]['reference_pl'], database_dict[pat][study][series][subseries]['reference_frame'], database_dict[pat][study][series][subseries]['folder'],\n                                      database_dict[pat][study][series][subseries]['orientation'], database_dict[pat][study][series][subseries]['orientation_type'],\n                                      database_dict[pat][study][series][subseries]['repetition_time'], database_dict[pat][study][series][subseries]['echo_time'],\n                                      database_dict[pat][study][series][subseries]['scan_sequence'], database_dict[pat][study][series][subseries]['mag_field_strength'], database_dict[pat][study][series][subseries]['imaged_nucleus'],\n                                      database_dict[pat][study][series][subseries]['fname']\n                                      ]\n\n                            df_add = pd.DataFrame([values], columns=columns)\n                            df = pd.concat([df, df_add], ignore_index=True)\n    return df\n\n\ndef crawl(top, \n          n_jobs: int = -1):\n    # top is the input directory in the argument parser from autotest.py\n    database_list = []\n    folders = glob.glob(pathlib.Path(top, \"*\").as_posix())\n    \n    database_list = Parallel(n_jobs=n_jobs)(delayed(crawl_one)(pathlib.Path(top, folder).as_posix()) for folder in tqdm(folders))\n\n    # convert list to dictionary\n    database_dict = {}\n    for db in database_list:\n        for key in db:\n            database_dict[key] = db[key]\n    \n    # save one level above imaging folders\n    parent, dataset  = os.path.split(top)\n\n    parent_imgtools = pathlib.Path(parent, \".imgtools\").as_posix()\n\n    if not os.path.exists(parent_imgtools):\n        try:\n            os.makedirs(parent_imgtools)\n        except:\n            pass\n    \n    # save as json\n    with open(pathlib.Path(parent_imgtools, f'imgtools_{dataset}.json').as_posix(), 'w') as f:\n        json.dump(database_dict, f, indent=4)\n    \n    # save as dataframe\n    df = to_df(database_dict)\n    df_path = pathlib.Path(parent_imgtools, f'imgtools_{dataset}.csv').as_posix()\n    df.to_csv(df_path)\n    \n    return database_dict\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(\"Dataset DICOM Crawler\")\n    parser.add_argument(\"directory\",\n                        type=str,\n                        help=\"Top-level directory of the dataset.\")\n    parser.add_argument(\"--n_jobs\",\n                        type=int,\n                        default=16,\n                        help=\"Number of parallel processes for multiprocessing.\")\n\n    args = parser.parse_args()\n    db = crawl(args.directory, n_jobs=args.n_jobs)\n    print(\"# patients:\", len(db))\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `crawl_one` function in this code?",
        "answer": "The `crawl_one` function is responsible for crawling through a single folder, extracting metadata from DICOM files, and organizing the information into a nested dictionary structure. It processes various DICOM modalities, extracts relevant tags, and handles different types of references between DICOM files (e.g., CT, RTSTRUCT, RTDOSE). The function returns a dictionary containing patient, study, series, and instance-level information for all DICOM files found in the given folder."
      },
      {
        "question": "How does the code handle parallel processing of multiple folders?",
        "answer": "The code uses the `joblib` library for parallel processing. In the `crawl` function, it utilizes `Parallel` and `delayed` from joblib to apply the `crawl_one` function to multiple folders concurrently. The number of parallel jobs is controlled by the `n_jobs` parameter, which can be set by the user through the command-line argument. This approach allows for efficient processing of large datasets with multiple patient folders."
      },
      {
        "question": "What are the main outputs generated by this DICOM crawler, and where are they saved?",
        "answer": "The DICOM crawler generates two main outputs:\n1. A JSON file containing the nested dictionary structure of all DICOM metadata.\n2. A CSV file containing a flattened representation of the DICOM metadata as a pandas DataFrame.\n\nBoth outputs are saved in a hidden folder named '.imgtools' located one level above the input directory. The file names are prefixed with 'imgtools_' followed by the name of the dataset (derived from the input directory name). The JSON file is saved using `json.dump()`, while the CSV file is saved using the pandas `to_csv()` method."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def crawl_one(folder):\n    folder_path = pathlib.Path(folder)\n    database = {}\n    for path, _, _ in os.walk(folder):\n        dicoms = glob.glob(pathlib.Path(path, \"**\", \"*.dcm\").as_posix(), recursive=True)\n        for dcm in dicoms:\n            try:\n                dcm_path = pathlib.Path(dcm)\n                fname = dcm_path.name\n                rel_path = dcm_path.relative_to(folder_path.parent.parent)\n                rel_posix = rel_path.parent.as_posix()\n                meta = dcmread(dcm, force=True, stop_before_pixels=True)\n                patient = str(meta.PatientID)\n                study = str(meta.StudyInstanceUID)\n                series = str(meta.SeriesInstanceUID)\n                instance = str(meta.SOPInstanceUID)\n                # TODO: Extract additional metadata and populate the database\n            except Exception as e:\n                print(folder, e)\n                pass\n    return database",
        "complete": "def crawl_one(folder):\n    folder_path = pathlib.Path(folder)\n    database = {}\n    for path, _, _ in os.walk(folder):\n        dicoms = glob.glob(pathlib.Path(path, \"**\", \"*.dcm\").as_posix(), recursive=True)\n        for dcm in dicoms:\n            try:\n                dcm_path = pathlib.Path(dcm)\n                fname = dcm_path.name\n                rel_path = dcm_path.relative_to(folder_path.parent.parent)\n                rel_posix = rel_path.parent.as_posix()\n                meta = dcmread(dcm, force=True, stop_before_pixels=True)\n                patient = str(meta.PatientID)\n                study = str(meta.StudyInstanceUID)\n                series = str(meta.SeriesInstanceUID)\n                instance = str(meta.SOPInstanceUID)\n                subseries = str(meta.get('AcquisitionNumber', 'default'))\n                if patient not in database:\n                    database[patient] = {}\n                if study not in database[patient]:\n                    database[patient][study] = {'description': str(meta.get('StudyDescription', ''))}\n                if series not in database[patient][study]:\n                    database[patient][study][series] = {'description': str(meta.get('SeriesDescription', ''))}\n                if subseries not in database[patient][study][series]:\n                    database[patient][study][series][subseries] = {\n                        'instances': {},\n                        'instance_uid': instance,\n                        'modality': meta.Modality,\n                        'reference_ct': '',\n                        'reference_rs': '',\n                        'reference_pl': '',\n                        'reference_frame': str(meta.get('FrameOfReferenceUID', '')),\n                        'folder': rel_posix,\n                        'orientation': str(meta.get('ImageOrientationPatient', '')),\n                        'orientation_type': str(meta.get('AnatomicalOrientationType', '')),\n                        'repetition_time': float(meta.get('RepetitionTime', 0)),\n                        'echo_time': float(meta.get('EchoTime', 0)),\n                        'scan_sequence': str(meta.get('ScanningSequence', '')),\n                        'mag_field_strength': float(meta.get('MagneticFieldStrength', 0)),\n                        'imaged_nucleus': str(meta.get('ImagedNucleus', '')),\n                        'fname': rel_path.as_posix()\n                    }\n                database[patient][study][series][subseries]['instances'][instance] = rel_path.as_posix()\n            except Exception as e:\n                print(folder, e)\n                pass\n    return database"
      },
      {
        "partial": "def to_df(database_dict):\n    df = pd.DataFrame()\n    for pat in database_dict:\n        for study in database_dict[pat]:\n            for series in database_dict[pat][study]:\n                if series != 'description':\n                    for subseries in database_dict[pat][study][series]:\n                        if subseries != 'description':\n                            # TODO: Define columns and values\n                            # TODO: Create DataFrame and concatenate\n    return df",
        "complete": "def to_df(database_dict):\n    df = pd.DataFrame()\n    for pat in database_dict:\n        for study in database_dict[pat]:\n            for series in database_dict[pat][study]:\n                if series != 'description':\n                    for subseries in database_dict[pat][study][series]:\n                        if subseries != 'description':\n                            columns = ['patient_ID', 'study', 'study_description', 'series', 'series_description', 'subseries', 'modality', 'instances', 'instance_uid', 'reference_ct', 'reference_rs', 'reference_pl', 'reference_frame', 'folder', 'orientation', 'orientation_type', 'MR_repetition_time', 'MR_echo_time', 'MR_scan_sequence', 'MR_magnetic_field_strength', 'MR_imaged_nucleus', 'file_path']\n                            values = [pat, study, database_dict[pat][study]['description'], series, database_dict[pat][study][series]['description'], subseries, database_dict[pat][study][series][subseries]['modality'], len(database_dict[pat][study][series][subseries]['instances']), database_dict[pat][study][series][subseries]['instance_uid'], database_dict[pat][study][series][subseries]['reference_ct'], database_dict[pat][study][series][subseries]['reference_rs'], database_dict[pat][study][series][subseries]['reference_pl'], database_dict[pat][study][series][subseries]['reference_frame'], database_dict[pat][study][series][subseries]['folder'], database_dict[pat][study][series][subseries]['orientation'], database_dict[pat][study][series][subseries]['orientation_type'], database_dict[pat][study][series][subseries]['repetition_time'], database_dict[pat][study][series][subseries]['echo_time'], database_dict[pat][study][series][subseries]['scan_sequence'], database_dict[pat][study][series][subseries]['mag_field_strength'], database_dict[pat][study][series][subseries]['imaged_nucleus'], database_dict[pat][study][series][subseries]['fname']]\n                            df_add = pd.DataFrame([values], columns=columns)\n                            df = pd.concat([df, df_add], ignore_index=True)\n    return df"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "glob",
        "json",
        "pandas"
      ],
      "from_imports": [
        "argparse.ArgumentParser",
        "pydicom.dcmread",
        "tqdm.tqdm",
        "joblib.Parallel"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/arrayutils.py",
    "language": "py",
    "content": "import numpy as np\nimport SimpleITK as sitk\n\n\ndef array_to_image(array,\n                   origin=(0., 0., 0.),\n                   direction=(1., 0., 0., 0., 1., 0., 0., 0., 1.),\n                   spacing=(1., 1., 1.),\n                   reference_image=None):\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        image.CopyInformation(reference_image)\n    else:\n        image.SetOrigin(origin)\n        image.SetDirection(direction)\n        image.SetSpacing(spacing)\n\n    return image\n\n\ndef find_slices_with_labels(array, labels=None):\n    if not labels:\n        return np.where(array.sum(axis=(1, 2)) > 0)[0]\n    else:\n        if isinstance(labels, int):\n            labels = [labels]\n        return np.where(np.isin(array, labels).sum(axis=(1, 2)) > 0)[0]\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `array_to_image` function in the given code snippet?",
        "answer": "The `array_to_image` function converts a numpy array to a SimpleITK image. It sets the image's origin, direction, and spacing either from provided parameters or by copying information from a reference image if provided. This function is useful for creating SimpleITK images with proper metadata from numpy arrays, which is common in medical image processing."
      },
      {
        "question": "How does the `find_slices_with_labels` function handle different input types for the `labels` parameter?",
        "answer": "The `find_slices_with_labels` function is flexible in handling the `labels` parameter:\n1. If `labels` is None, it finds slices where any non-zero value exists.\n2. If `labels` is an integer, it converts it to a list containing that integer.\n3. If `labels` is a list, it uses it directly.\nThe function then uses numpy's `isin` function to check for the presence of specified labels in each slice, returning the indices of slices containing any of the specified labels."
      },
      {
        "question": "What is the significance of the `direction` parameter in the `array_to_image` function, and what does its default value represent?",
        "answer": "The `direction` parameter in the `array_to_image` function represents the direction cosines of the image axes. Its default value `(1., 0., 0., 0., 1., 0., 0., 0., 1.)` corresponds to an identity matrix, which means the image axes are aligned with the patient coordinate system. This parameter is important in medical imaging to correctly orient the image in 3D space, especially when dealing with DICOM images or other modalities where patient orientation is crucial."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def array_to_image(array,\n                   origin=(0., 0., 0.),\n                   direction=(1., 0., 0., 0., 1., 0., 0., 0., 1.),\n                   spacing=(1., 1., 1.),\n                   reference_image=None):\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        # Complete the code here\n    else:\n        # Complete the code here\n\n    return image",
        "complete": "def array_to_image(array,\n                   origin=(0., 0., 0.),\n                   direction=(1., 0., 0., 0., 1., 0., 0., 0., 1.),\n                   spacing=(1., 1., 1.),\n                   reference_image=None):\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        image.CopyInformation(reference_image)\n    else:\n        image.SetOrigin(origin)\n        image.SetDirection(direction)\n        image.SetSpacing(spacing)\n\n    return image"
      },
      {
        "partial": "def find_slices_with_labels(array, labels=None):\n    if not labels:\n        # Complete the code here\n    else:\n        if isinstance(labels, int):\n            labels = [labels]\n        # Complete the code here",
        "complete": "def find_slices_with_labels(array, labels=None):\n    if not labels:\n        return np.where(array.sum(axis=(1, 2)) > 0)[0]\n    else:\n        if isinstance(labels, int):\n            labels = [labels]\n        return np.where(np.isin(array, labels).sum(axis=(1, 2)) > 0)[0]"
      }
    ],
    "dependencies": {
      "imports": [
        "numpy",
        "SimpleITK"
      ],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugSensitivityPCorr.R",
    "language": "R",
    "content": "\ncor.boot <- function(data, w){\n  ddd <- data[w,]\n        ## A question here is what to do when our bootstrap sample has 0 variance in one of\n      ## the two variables (usually the molecular feature)\n      ## If we return NA, we effectively condition on a sampling procedure that samples at least\n      ## one expressor. If we return near 0 (the default of coop::pcor), then we effectively say that conditioned\n      ## on not sampling any expressors, there is no association. Neither is correct, but the latter is certainly more\n      ## conservative. We probably want to use a completely different model to discover \"rare\" biomarkers\n      ## We will go with the later conservative option, however we will set it to zero exactly instead of relying on\n      ## the behaviour of coop.\n\n  if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n    return(0)\n  }\n\n  return(coop::pcor(ddd[,1], ddd[,2]))\n}\n\n\n#' Calculate The Gene Drug Sensitivity\n#'\n#' This version of the function uses a partial correlation instead of standardized linear models.\n#'\n#' @param x A \\code{numeric} vector of gene expression values\n#' @param type A \\code{vector} of factors specifying the cell lines or type types\n#' @param batch A \\code{vector} of factors specifying the batch\n#' @param drugpheno A \\code{numeric} vector of drug sensitivity values (e.g.,\n#'   IC50 or AUC)\n#' @param test A \\code{character} string indicating whether resampling or analytic based tests should be used\n#' @param req_alpha \\code{numeric}, number of permutations for p value calculation\n#' @param nBoot \\code{numeric}, number of bootstrap resamplings for confidence interval estimation\n#' @param conf.level \\code{numeric}, between 0 and 1. Size of the confidence interval required\n#' @param max_perm \\code{numeric} the maximum number of permutations that QUICKSTOP can do before giving up and returning NA.\n#'   Can be set globally by setting the option \"PharmacoGx_Max_Perm\", or left at the default of \\code{ceiling(1/req_alpha*100)}.\n#' @param verbose \\code{boolean} Should the function display messages?\n#'\n#' @return A \\code{vector} reporting the effect size (estimateof the coefficient\n#'   of drug concentration), standard error (se), sample size (n), t statistic,\n#'   and F statistics and its corresponding p-value.\n#'\n#' @examples\n#' print(\"TODO::\")\n#'\n#' @importFrom stats sd complete.cases lm glm anova pf formula var pt qnorm cor residuals runif\n#' @importFrom boot boot boot.ci\n#' @importFrom coop pcor\ngeneDrugSensitivityPCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    ## not enough samples with complete information or no variation in gene expression\n    return(rest)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n\n\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  # ff1 <- sprintf(\"%s + x\", ff0)\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  ## control for tissue type\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  ## control for batch\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n  ## control for duration\n  # if(length(sort(unique(duration))) > 1){\n  #   ff0 <- sprintf(\"%s + duration\", ff0)\n  #   ff <- sprintf(\"%s + duration\", ff)\n  # }\n\n  # if(is.factor(drugpheno[,1])){\n\n  #   drugpheno <- drugpheno[,1]\n\n  # } else {\n\n  #   drugpheno <- as.matrix(drugpheno)\n\n  # }\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n\n    stop(\"Currently only continous output allowed for partial correlations\")\n\n\n  } else {\n\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L # taking the residual degrees of freedom minus 2 parameters estimated for pearson cor.\n    } else { ## doing this if statement in the case there are some numerical differences between mean centred values and raw values\n    var1 <- dd[,\"drugpheno.1\"]\n    var2 <- dd[,\"x\"]\n    df <- nn - 2L\n  }\n\n  obs.cor <- coop::pcor(var1, var2, use=\"complete.obs\")\n\n\n    ## NB: just permuting the residuals would leads to Type I error inflation,\n    ## from an underestimation due to ignoring variance in the effects of the covariates.\n    ## See: https://www.tandfonline.com/doi/abs/10.1080/00949650008812035\n    ## Note that the above paper does not provide a single method recommended in all cases\n    ## We apply the permutation of raw data method, as it is most robust to small sample sizes\n  if(test == \"resampling\"){\n      ## While the logic is equivalent regardless of if there are covariates for calculating the point estimate,\n      ## (correlation is a subcase of partial correlation), for computational efficency in permuation testing we\n      ## split here and don't do extranous calls to lm if it is unnecessay.\n\n    if(ncol(dd) > 2){\n\n      if(!getOption(\"PharmacoGx_useC\")|| ncol(dd)!=3){ ## currently implementing c code only for 1 single grouping variable\n\n        ## implementing a much more efficient method for the particular case where we have 3 columns with assumption that\n        ## column 3 is the tissue.\n        if(ncol(dd)==3){\n          sample_function <- function(){\n\n            partial.dp <- sample(dd[,1], nrow(dd))\n            partial.x <- sample(dd[,2], nrow(dd))\n\n            for(gp in unique(dd[,3])){\n              partial.x[dd[,3]==gp] <- partial.x[dd[,3]==gp]-mean(partial.x[dd[,3]==gp])\n              partial.dp[dd[,3]==gp] <- partial.dp[dd[,3]==gp]-mean(partial.dp[dd[,3]==gp])\n            }\n\n            perm.cor <- coop::pcor(partial.dp, partial.x, use=\"complete.obs\")\n            return(abs(obs.cor) < abs(perm.cor))\n          }\n        } else {\n          sample_function <- function(){\n            # browser()\n            dd2 <- dd\n            dd2[,1] <- sample(dd[,1], nrow(dd))\n            dd2[,2] <- sample(dd[,2], nrow(dd))\n\n            partial.dp <- residuals(lm(formula(ffd), dd2))\n            partial.x <- residuals(lm(formula(ffx), dd2))\n\n            perm.cor <- coop::pcor(partial.dp, partial.x, use=\"complete.obs\")\n            return(abs(obs.cor) < abs(perm.cor))\n          }\n        }\n\n        p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n        significant <- p.value$significant\n        p.value <- p.value$p.value\n\n\n      } else {\n\n        x <- dd[,1]\n        y <- dd[,2]\n        GR <- as.integer(factor(dd[,3]))-1L\n        GS <- as.integer(table(factor(dd[,3])))\n        NG <- length(table(factor(dd[,3])))\n        N <- as.numeric(length(x))\n\n        p.value <-PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, max_perm, N, req_alpha, req_alpha/100, 10L, runif(2))\n        significant <- p.value[[1]]\n        p.value <- p.value[[2]]\n      }\n\n\n    pcor.boot <- function(ddd, w){\n      ddd <- ddd[w,]\n          ## Taking care of an edge case where only one factor level is left after resampling\n          ## However, we need to keep the first two numeric columns to properly return a value, otherwise\n          ## if we remove gene expression because there were only non-detected samples, for example,\n          ## we will try to take the correlation against a character vector.\n      ddd <- ddd[,c(TRUE, TRUE, apply(ddd[,-c(1,2),drop=FALSE], 2, function(x) return(length(unique(x))))>=2)]\n\n\n      ## A question here is what to do when our bootstrap sample has 0 variance in one of\n      ## the two variables (usually the molecular feature)\n      ## If we return NA, we effectively condition on a sampling procedure that samples at least\n      ## one expressor. If we return near 0 (the default of coop::pcor), then we effectively say that conditioned\n      ## on not sampling any expressors, there is no association. Neither is correct, but the latter is certainly more\n      ## conservative. We probably want to use a completely different model to discover \"rare\" biomarkers\n      ## We will go with the later conservative option, however we will set it to zero exactly instead of relying on\n      ## the behaviour of coop.\n\n      if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n        return(0)\n      }\n\n      if(ncol(ddd)==3){\n        partial.dp <- ddd[,1]\n        partial.x <- ddd[,2]\n        for(gp in unique(ddd[,3])){\n          partial.x[ddd[,3]==gp] <- partial.x[ddd[,3]==gp]-mean(partial.x[ddd[,3]==gp])\n          partial.dp[ddd[,3]==gp] <- partial.dp[ddd[,3]==gp]-mean(partial.dp[ddd[,3]==gp])\n        }\n\n      } else if(ncol(ddd)==2){\n        partial.dp <- ddd[,1]\n        partial.x <- ddd[,2]\n      } else {\n\n        partial.dp <- residuals(lm(formula(ffd), ddd))\n        partial.x <- residuals(lm(formula(ffx), ddd))\n\n      }\n\n      return(coop::pcor(partial.dp, partial.x, use=\"complete.obs\"))\n    }\n\n    boot.out <- boot(dd, pcor.boot, R=nBoot)\n    cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n      error = function(e) {\n        if(e$message == \"estimated adjustment 'w' is infinite\"){\n          warning(\"estimated adjustment 'w' is infinite for some features\")\n          return(c(NA_real_,NA_real_))\n        } else {\n          stop(e)\n        }\n      })\n  } else {\n    if(!getOption(\"PharmacoGx_useC\")){\n      sample_function <- function(){\n        v1 <- sample(var1)\n        return(abs(obs.cor) < abs(coop::pcor(v1, var2, use=\"complete.obs\")))\n      }\n\n      p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n      significant <- p.value$significant\n      p.value <- p.value$p.value\n    } else {\n\n      x <- as.numeric(var1)\n      y <- as.numeric(var2)\n      GR <- rep(0L, length(x))\n      GS <- as.integer(length(x))\n      NG <- 1L\n      N <- as.numeric(length(x))\n\n      p.value <-PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, max_perm, N, req_alpha, req_alpha/100, 10L, runif(2))\n      significant <- p.value[[1]]\n      p.value <- p.value[[2]]\n    }\n\n\n\n\n    boot.out <- boot(data.frame(var1, var2), cor.boot, R=nBoot)\n    cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n      error = function(e) {\n        if(e$message == \"estimated adjustment 'w' is infinite\" || e$message == \"Error in if (const(t, min(1e-08, mean(t, na.rm = TRUE)/1e+06))) { : \\n  missing value where TRUE/FALSE needed\\n\"){\n          warning(\"estimated adjustment 'w' is infinite for some features\")\n          return(c(NA_real_,NA_real_))\n        } else {\n          stop(e)\n        }\n      })\n  }\n\n\n} else if(test == \"analytic\"){\n      # if(ncol(dd) > 2){\n      #   df <- nn - 2L - controlled.var\n\n      # } else {\n      #   df <- nn - 2L\n      #   # cor.test.res <- cor.test(dd[,\"drugpheno.1\"], dd[,\"x\"], method=\"pearson\", use=\"complete.obs\")\n      # }\n  stat <- sqrt(df) * obs.cor/sqrt(1-obs.cor^2) ## Note, this is implemented in same order of operations as cor.test\n  p.value <- 2*min(pt(stat, df=df), pt(stat, df=df, lower.tail = FALSE))\n      ## Implementing with fisher transform and normal dist for consistency with R's cor.test\n  z <- atanh(obs.cor)\n  sigma <- 1/sqrt(df - 1)\n  cint <- tanh(z + c(-1, 1) * sigma * qnorm((1 + conf.level)/2))\n  significant <- p.value < req_alpha\n}\n\n}\n\nrest <- c(\"estimate\"=obs.cor, \"n\"=nn, df=df, significant = as.numeric(significant), \"pvalue\"=p.value, \"lower\" = cint[1], \"upper\" = cint[2])\n\n\nreturn(rest)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cor.boot` function and how does it handle edge cases?",
        "answer": "The `cor.boot` function is used for bootstrap resampling to estimate partial correlations. It handles edge cases where the bootstrap sample has zero variance in one of the two variables by returning 0 instead of NA. This is a conservative approach for dealing with rare biomarkers or non-expressor samples."
      },
      {
        "question": "How does the `geneDrugSensitivityPCorr` function handle missing or infinite values in the input data?",
        "answer": "The function handles missing or infinite values by first removing infinite values from non-factor columns in the `drugpheno` data frame, replacing them with NA. Then, it uses `complete.cases` to filter out any rows with NA values across all input variables (x, type, batch, and drugpheno) before performing the analysis."
      },
      {
        "question": "What statistical methods does the `geneDrugSensitivityPCorr` function use for hypothesis testing and confidence interval estimation?",
        "answer": "The function offers two methods for hypothesis testing: resampling and analytic. For resampling, it uses permutation tests to calculate p-values and bootstrap resampling to estimate confidence intervals. For the analytic method, it uses the Fisher transformation and t-distribution to calculate p-values and confidence intervals. The choice between methods is controlled by the `test` parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cor.boot <- function(data, w){\n  ddd <- data[w,]\n  if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n    return(0)\n  }\n  return(coop::pcor(ddd[,1], ddd[,2]))\n}",
        "complete": "cor.boot <- function(data, w){\n  ddd <- data[w,]\n  if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n    return(0)\n  }\n  return(coop::pcor(ddd[,1], ddd[,2]))\n}"
      },
      {
        "partial": "geneDrugSensitivityPCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    return(rest)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n    stop(\"Currently only continous output allowed for partial correlations\")\n  } else {\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L\n    } else {\n      var1 <- dd[,\"drugpheno.1\"]\n      var2 <- dd[,\"x\"]\n      df <- nn - 2L\n    }\n\n    obs.cor <- coop::pcor(var1, var2, use=\"complete.obs\")\n\n    # ... Rest of the function ...\n  }\n}",
        "complete": "geneDrugSensitivityPCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    return(rest)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n    stop(\"Currently only continous output allowed for partial correlations\")\n  } else {\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L\n    } else {\n      var1 <- dd[,\"drugpheno.1\"]\n      var2 <- dd[,\"x\"]\n      df <- nn - 2L\n    }\n\n    obs.cor <- coop::pcor(var1, var2, use=\"complete.obs\")\n\n    if(test == \"resampling\"){\n      if(!getOption(\"PharmacoGx_useC\") || ncol(dd)!=3){\n        sample_function <- function(){\n          dd2 <- dd\n          dd2[,1] <- sample(dd[,1], nrow(dd))\n          dd2[,2] <- sample(dd[,2], nrow(dd))\n          partial.dp <- residuals(lm(formula(ffd), dd2))\n          partial.x <- residuals(lm(formula(ffx), dd2))\n          perm.cor <- coop::pcor(partial.dp, partial.x, use=\"complete.obs\")\n          return(abs(obs.cor) < abs(perm.cor))\n        }\n        p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n        significant <- p.value$significant\n        p.value <- p.value$p.value\n      } else {\n        x <- dd[,1]\n        y <- dd[,2]\n        GR <- as.integer(factor(dd[,3]))-1L\n        GS <- as.integer(table(factor(dd[,3])))\n        NG <- length(table(factor(dd[,3])))\n        N <- as.numeric(length(x))\n        p.value <- PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, max_perm, N, req_alpha, req_alpha/100, 10L, runif(2))\n        significant <- p.value[[1]]\n        p.value <- p.value[[2]]\n      }\n      boot.out <- boot(dd, cor.boot, R=nBoot)\n      cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n        error = function(e) {\n          if(e$message == \"estimated adjustment 'w' is infinite\"){\n            warning(\"estimated adjustment 'w' is infinite for some features\")\n            return(c(NA_real_,NA_real_))\n          } else {\n            stop(e)\n          }\n        })\n    } else if(test == \"analytic\"){\n      stat <- sqrt(df) * obs.cor/sqrt(1-obs.cor^2)\n      p.value <- 2*min(pt(stat, df=df), pt(stat, df=df, lower.tail = FALSE))\n      z <- atanh(obs.cor)\n      sigma <- 1/sqrt(df - 1)\n      cint <- tanh(z + c(-1, 1) * sigma * qnorm((1 + conf.level)/2))\n      significant <- p.value < req_alpha\n    }\n  }\n\n  rest <- c(\"estimate\"=obs.cor, \"n\"=nn, df=df, significant = as.numeric(significant), \"pvalue\"=p.value, \"lower\" = cint[1], \"upper\" = cint[2])\n  return(rest)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/updateObject-methods.R",
    "language": "R",
    "content": "#' Update the PharmacoSet class after changes in it struture or API\n#'\n#' @param object A `PharmacoSet` object to update the class structure for.\n#'\n#' @return `PharmacoSet` with update class structure.\n#' \n#' @examples\n#' data(GDSCsmall)\n#' updateObject(GDSCsmall)\n#' \n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        colnames(curation(pSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(pSet)$treatment))\n    }\n    validObject(pSet)\n    return(pSet)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `updateObject` method for the `PharmacoSet` class?",
        "answer": "The `updateObject` method is used to update the class structure of a `PharmacoSet` object after changes in its structure or API. It performs the following tasks: 1) Calls the next method in the method dispatch chain, 2) Converts the result back to a `PharmacoSet` object, 3) Updates the naming conventions in the curation data, and 4) Validates the updated object before returning it."
      },
      {
        "question": "How does this method handle the renaming of 'drug' to 'treatment' in the curation data?",
        "answer": "The method renames 'drug' to 'treatment' in the curation data using two steps: 1) It uses `gsub()` to replace 'drug' with 'treatment' in the names of the curation list. 2) If a 'treatment' element exists in the curation data, it renames the 'treatmentid' column to 'treatmentid' (which effectively does nothing but might be a placeholder for future changes)."
      },
      {
        "question": "What S4 object-oriented programming concepts are demonstrated in this code snippet?",
        "answer": "This code snippet demonstrates several S4 object-oriented programming concepts: 1) Method definition using `setMethod()`, 2) Method dispatch with `callNextMethod()`, 3) Object conversion using `as()`, 4) Slot access and modification with the `curation()` function, 5) Object validation with `validObject()`, and 6) Use of method signatures with `signature('PharmacoSet')`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        # Complete the code to update column names\n    }\n    validObject(pSet)\n    return(pSet)\n})",
        "complete": "setMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        colnames(curation(pSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(pSet)$treatment))\n    }\n    validObject(pSet)\n    return(pSet)\n})"
      },
      {
        "partial": "#' Update the PharmacoSet class after changes in it struture or API\n#'\n#' @param object A `PharmacoSet` object to update the class structure for.\n#'\n#' @return `PharmacoSet` with update class structure.\n#' \n#' @examples\n#' data(GDSCsmall)\n#' updateObject(GDSCsmall)\n#' \n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    # Complete the function body\n})",
        "complete": "#' Update the PharmacoSet class after changes in it struture or API\n#'\n#' @param object A `PharmacoSet` object to update the class structure for.\n#'\n#' @return `PharmacoSet` with update class structure.\n#' \n#' @examples\n#' data(GDSCsmall)\n#' updateObject(GDSCsmall)\n#' \n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        colnames(curation(pSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(pSet)$treatment))\n    }\n    validObject(pSet)\n    return(pSet)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/autopipeline.py",
    "language": "py",
    "content": "from aifc import Error\nimport os\nimport pathlib\nimport shutil\nimport glob\nimport pickle\nimport numpy as np\nimport warnings\n\nimport yaml\nimport json\nimport SimpleITK as sitk\n\nfrom imgtools.ops import StructureSetToSegmentation, ImageAutoInput, ImageAutoOutput, Resample\nfrom imgtools.pipeline import Pipeline\nfrom imgtools.utils.nnunet import generate_dataset_json, markdown_report_images\nfrom imgtools.utils.args import parser\nfrom joblib import Parallel, delayed\nfrom imgtools.modules import Segmentation\nfrom sklearn.model_selection import train_test_split\n\nimport dill\n###############################################################\n# Example usage:\n# python radcure_simple.py ./data/RADCURE/data ./RADCURE_output\n###############################################################\n\n\nclass AutoPipeline(Pipeline):\n    \"\"\"Example processing pipeline for the RADCURE dataset.\n    This pipeline loads the CT images and structure sets, re-samples the images,\n    and draws the GTV contour using the resampled image.\n    \"\"\"\n\n    def __init__(self,\n                 input_directory=\"\",\n                 output_directory=\"\",\n                 modalities=\"CT\",\n                 spacing=(1., 1., 0.),\n                 n_jobs=-1,\n                 visualize=False,\n                 missing_strategy=\"drop\",\n                 show_progress=False,\n                 warn_on_error=False,\n                 overwrite=False,\n                 nnunet=False,\n                 train_size=1.0,\n                 random_state=42,\n                 read_yaml_label_names=False,\n                 ignore_missing_regex=False,\n                 roi_yaml_path=\"\",\n                 custom_train_test_split=False,\n                 nnunet_inference=False,\n                 dataset_json_path=\"\",\n                 continue_processing=False,\n                 dry_run=False,\n                 verbose=False,\n                 update=False,\n                 roi_select_first=False,\n                 roi_separate=False):\n        \"\"\"Initialize the pipeline.\n\n        Parameters\n        ----------\n        input_directory: str\n            Directory containing the input data\n        output_directory: str\n            Directory where the output data will be stored\n        modalities: str, default=\"CT\"\n            Modalities to load. Can be a comma-separated list of modalities with no spaces\n        spacing: tuple of floats, default=(1., 1., 0.)\n            Spacing of the output image\n        n_jobs: int, default=-1\n            Number of jobs to run in parallel. If -1, use all cores\n        visualize: bool, default=False\n            Whether to visualize the results of the pipeline using pyvis. Outputs to an HTML file\n        missing_strategy: str, default=\"drop\"\n            How to handle missing modalities. Can be \"drop\" or \"fill\"\n        show_progress: bool, default=False\n            Whether to show progress bars\n        warn_on_error: bool, default=False\n            Whether to warn on errors\n        overwrite: bool, default=False\n            Whether to write output files even if existing output files exist\n        nnunet: bool, default=False\n            Whether to format the output for nnunet\n        train_size: float, default=1.0\n            Proportion of the dataset to use for training, as a decimal\n        random_state: int, default=42\n            Random state for train_test_split\n        read_yaml_label_names: bool, default=False\n            Whether to read dictionary representing the label that regexes are mapped to from YAML. For example, \"GTV\": \"GTV.*\" will combine all regexes that match \"GTV.*\" into \"GTV\"\n        ignore_missing_regex: bool, default=False\n            Whether to ignore missing regexes. Will raise an error if none of the regexes in label_names are found for a patient\n        roi_yaml_path: str, default=\"\"\n            The path to the yaml file defining regexes\n        custom_train_test_split: bool, default=False\n            Whether to use a custom train/test split. The remaining patients will be randomly split using train_size and random_state\n        nnunet_inference: bool, default=False\n            Whether to format the output for nnUNet inference\n        dataset_json_path: str, default=\"\"\n            The path to the dataset.json file for nnUNet inference\n        continue_processing: bool, default=False\n            Whether to continue processing a partially processed dataset\n        dry_run: bool, default=False\n            Whether to run the pipeline without writing any output files\n        \"\"\"\n\n        self.continue_processing = continue_processing\n        self.dry_run = dry_run\n        self.v = verbose\n\n        if dry_run:\n            nnunet = False\n            nnunet_inference = False\n\n        if dry_run and continue_processing:\n            raise ValueError(\"Cannot continue processing a dry run. Set --continue_processing to False to do a dry run.\")\n\n        if not dry_run and output_directory == \"\":\n            raise ValueError(\"Must specify an output directory\")\n        \n        # input/output directory configuration\n        if not os.path.isabs(input_directory):\n            input_directory = pathlib.Path(os.getcwd(), input_directory).as_posix()\n        else:\n            input_directory = pathlib.Path(input_directory).as_posix()  # consistent parsing. ensures last child directory doesn't end with slash\n        \n        if not os.path.isabs(output_directory):\n            output_directory = pathlib.Path(os.getcwd(), output_directory).as_posix()\n        else:\n            output_directory = pathlib.Path(output_directory).as_posix()  # consistent parsing. ensures last child directory doesn't end with slash\n\n        # check/make output directory\n        if not os.path.exists(output_directory):\n            os.makedirs(output_directory)\n\n        # check input directory exists\n        if not os.path.exists(input_directory):\n            raise FileNotFoundError(f\"Input directory {input_directory} does not exist\")\n        \n        self.input_directory = pathlib.Path(input_directory).as_posix()\n        self.output_directory = pathlib.Path(output_directory).as_posix()\n        \n        # if wanting to continue processing but no .temp folders\n        if not nnunet and continue_processing and not os.path.exists(pathlib.Path(output_directory, \".temp\").as_posix()):\n            raise FileNotFoundError(f\"Cannot continue processing. .temp directory does not exist in {output_directory}. Run without --continue_processing to start from scratch.\")\n\n        study_name = os.path.split(self.input_directory)[1]\n        if nnunet_inference:\n            roi_yaml_path = \"\"\n            custom_train_test_split = False\n            nnunet = False\n            if modalities != \"CT\" and modalities != \"MR\":\n                raise ValueError(\"nnUNet inference can only be run on image files. Please set modalities to 'CT' or 'MR'\")\n        if nnunet:\n            self.base_output_directory = self.output_directory\n            if not os.path.exists(pathlib.Path(self.output_directory, \"nnUNet_preprocessed\").as_posix()):\n                os.makedirs(pathlib.Path(self.output_directory, \"nnUNet_preprocessed\").as_posix())\n            if not os.path.exists(pathlib.Path(self.output_directory, \"nnUNet_trained_models\").as_posix()):\n                os.makedirs(pathlib.Path(self.output_directory, \"nnUNet_trained_models\").as_posix())\n            self.output_directory = pathlib.Path(self.output_directory, \"nnUNet_raw_data_base\",\n                                                 \"nnUNet_raw_data\").as_posix()\n            if not os.path.exists(self.output_directory):\n                os.makedirs(self.output_directory)\n            all_nnunet_folders = glob.glob(pathlib.Path(self.output_directory, \"*\", \" \").as_posix())\n            # print(all_nnunet_folders)\n            numbers = [int(os.path.split(os.path.split(folder)[0])[1][4:7]) for folder in all_nnunet_folders if os.path.split(os.path.split(folder)[0])[1].startswith(\"Task\")]\n            # print(numbers, continue_processing)\n            if (len(numbers) == 0 and continue_processing) or not continue_processing or not os.path.exists(pathlib.Path(self.output_directory, f\"Task{max(numbers)}_{study_name}\", \".temp\").as_posix()):\n                available_numbers = list(range(500, 1000))\n                for folder in all_nnunet_folders:\n                    folder_name = os.path.split(os.path.split(folder)[0])[1]\n                    if folder_name.startswith(\"Task\") and folder_name[4:7].isnumeric() and int(folder_name[4:7]) in available_numbers:\n                        available_numbers.remove(int(folder_name[4:7]))\n                if len(available_numbers) == 0:\n                    raise Error(\"There are not enough task ID's for the nnUNet output. Please make sure that there is at least one task ID available between 500 and 999, inclusive\")\n                task_folder_name = f\"Task{available_numbers[0]}_{study_name}\"\n                self.output_directory = pathlib.Path(self.output_directory, task_folder_name).as_posix()\n                self.task_id = available_numbers[0]\n            else:\n                self.task_id = max(numbers)\n                task_folder_name = f\"Task{self.task_id}_{study_name}\"\n                self.output_directory = pathlib.Path(self.output_directory, task_folder_name).as_posix()\n            if not os.path.exists(pathlib.Path(self.output_directory, \".temp\").as_posix()):\n                os.makedirs(pathlib.Path(self.output_directory, \".temp\").as_posix())\n        \n        if not dry_run:\n            # Make a directory\n            if not os.path.exists(pathlib.Path(self.output_directory,\".temp\").as_posix()):\n                os.mkdir(pathlib.Path(self.output_directory,\".temp\").as_posix())\n                \n            with open(pathlib.Path(self.output_directory, \".temp\", \"init_parameters.pkl\").as_posix(), \"wb\") as f:\n                parameters = locals()  # save all the parameters in case we need to continue processing\n                dill.dump(parameters, f)\n\n            # continue processing operations\n            self.finished_subjects = [pathlib.Path(e).name[:-4] for e in glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())]  # remove the .pkl\n            \n\n        super().__init__(n_jobs=n_jobs,\n                         missing_strategy=missing_strategy,\n                         show_progress=show_progress,\n                         warn_on_error=warn_on_error)\n        self.overwrite = overwrite\n        self.spacing = spacing\n        self.existing = [None]   # self.existing_patients()\n        self.is_nnunet = nnunet\n        if nnunet or nnunet_inference:\n            self.nnunet_info = {}\n        else:\n            self.nnunet_info = None\n        self.train_size = train_size\n        self.random_state = random_state\n        self.label_names = {}\n        self.ignore_missing_regex = ignore_missing_regex\n        self.custom_train_test_split = custom_train_test_split\n        self.is_nnunet_inference = nnunet_inference\n        self.roi_select_first = roi_select_first\n        self.roi_separate = roi_separate\n\n        if roi_yaml_path != \"\" and not read_yaml_label_names:\n            warnings.warn(\"The YAML will not be read since it has not been specified to read them. To use the file, run the CLI with --read_yaml_label_names\")\n\n        roi_path = pathlib.Path(self.input_directory, \"roi_names.yaml\").as_posix() if roi_yaml_path == \"\" else roi_yaml_path\n        if read_yaml_label_names:\n            if os.path.exists(roi_path):\n                with open(roi_path, \"r\") as f:\n                    try:\n                        self.label_names = yaml.safe_load(f)\n                    except yaml.YAMLError as exc:\n                        print(exc)\n            else:\n                raise FileNotFoundError(f\"No file named roi_names.yaml found at {roi_path}. If you did not intend on creating ROI regexes, run the CLI without --read_yaml_label_names\")\n        \n        if not isinstance(self.label_names, dict):\n            raise ValueError(\"roi_names.yaml must parse as a dictionary\")\n\n        for k, v in self.label_names.items():\n            if not isinstance(v, list) and not isinstance(v, str):\n                raise ValueError(f\"Label values must be either a list of strings or a string. Got {v} for {k}\")\n            elif isinstance(v, list):\n                for a in v:\n                    if not isinstance(a, str):\n                        raise ValueError(f\"Label values must be either a list of strings or a string. Got {a} in list {v} for {k}\")\n            elif not isinstance(k, str):\n                raise ValueError(f\"Label names must be a string. Got {k} for {v}\")\n\n        if self.train_size == 1.0 and nnunet:\n            warnings.warn(\"Train size is 1, all data will be used for training\")\n        \n        if self.train_size == 0.0 and nnunet:\n            warnings.warn(\"Train size is 0, all data will be used for testing\")\n\n        if self.train_size != 1 and not nnunet:\n            warnings.warn(\"Cannot run train/test split without nnunet, ignoring train_size\")\n\n        if self.train_size > 1 or self.train_size < 0 and nnunet:\n            raise ValueError(\"train_size must be between 0 and 1\")\n        \n        if nnunet and (not read_yaml_label_names or self.label_names == {}):\n            raise ValueError(\"YAML label names must be provided for nnunet\")\n        \n        if custom_train_test_split and not nnunet:\n            raise ValueError(\"Cannot use custom train/test split without nnunet\")\n\n        custom_train_test_split_path = pathlib.Path(self.input_directory, \"custom_train_test_split.yaml\").as_posix()\n        if custom_train_test_split and nnunet:\n            if os.path.exists(custom_train_test_split_path):\n                with open(custom_train_test_split_path, \"r\") as f:\n                    try:\n                        self.custom_split = yaml.safe_load(f)\n                        if isinstance(self.custom_split, list):\n                            for e in self.custom_split:\n                                if not isinstance(e, str):\n                                    raise ValueError(\"Custom split must be a list of strings. Place quotes around patient ID's that don't parse as YAML strings\")\n                            self.custom_split = {\"train\": [], \"test\": self.custom_split}\n                        if isinstance(self.custom_split, dict):\n                            if sorted(list(self.custom_split.keys())) != [\"test\", \"train\"] and list(self.custom_split.keys()) != [\"train\"] and list(self.custom_split.keys()) != [\"test\"]:\n                                raise ValueError(\"Custom split must be a dictionary with keys 'train' and 'test'\")\n                            for k, v in self.custom_split.items():\n                                if not isinstance(v, list):\n                                    raise ValueError(f\"Custom split must be a list of strings. Place quotes around patient ID's that don't parse as YAML strings. Got {v} for {k}\")\n                                for e in v:\n                                    if not isinstance(e, str):\n                                        raise ValueError(\"Custom split must be a list of strings. Place quotes around patient ID's that don't parse as YAML strings\")\n                            if list(self.custom_split.keys()) == [\"train\"]:\n                                self.custom_split = {\"train\": self.custom_split[\"train\"], \"test\": []}\n                            elif list(self.custom_split.keys()) == [\"test\"]:\n                                self.custom_split = {\"train\": [], \"test\": self.custom_split[\"test\"]}\n                        for e in self.custom_split[\"train\"]:\n                            if e in self.custom_split[\"test\"]:\n                                raise ValueError(\"Custom split cannot contain the same patient ID in both train and test\")\n                    except yaml.YAMLError as exc:\n                        print(exc)\n            else:\n                raise FileNotFoundError(f\"No file named custom_train_test_split.yaml found at {custom_train_test_split_path}. If you did not intend on creating a custom train-test-split, run the CLI without --custom_train_test_split\")\n\n        if self.is_nnunet:\n            self.nnunet_info[\"modalities\"] = {\"CT\": \"0000\"}  # modality to 4-digit code\n\n        if nnunet_inference:\n            if not os.path.exists(dataset_json_path):\n                raise FileNotFoundError(f\"No file named {dataset_json_path} found. Image modality definitions are required for nnUNet inference\")\n            else:\n                with open(dataset_json_path, \"r\") as f:\n                    self.nnunet_info[\"modalities\"] = {v: k.zfill(4) for k, v in json.load(f)[\"modality\"].items()}\n\n        # Input operations\n        self.input = ImageAutoInput(input_directory, modalities, n_jobs, visualize, update)\n        self.output_df_path = pathlib.Path(self.output_directory, \"dataset.csv\").as_posix()\n\n        # Output component table\n        self.output_df = self.input.df_combined\n\n        # Name of the important columns which needs to be saved\n        self.output_streams = self.input.output_streams\n        \n        # image processing ops\n        self.resample = Resample(spacing=self.spacing)\n        self.make_binary_mask = StructureSetToSegmentation(roi_names=self.label_names, continuous=False)\n\n        # output ops\n        self.output = ImageAutoOutput(self.output_directory, self.output_streams, self.nnunet_info, self.is_nnunet_inference)\n        \n        self.existing_roi_indices = {\"background\": 0}\n        if nnunet or nnunet_inference:\n            self.total_modality_counter = {}\n            self.patients_with_missing_labels = set()\n\n    def glob_checker_nnunet(self, subject_id):\n        folder_names = [\"imagesTr\", \"labelsTr\", \"imagesTs\", \"labelsTs\"]\n        files = []\n        for folder_name in folder_names:\n            if os.path.exists(pathlib.Path(self.input_directory, folder_name).as_posix()):\n                files.extend(glob.glob(pathlib.Path(self.output_directory,folder_name,\"*.nii.gz\").as_posix()))\n        for f in files:\n            if f.startswith(subject_id):\n                return True\n        return False\n\n    def process_one_subject(self, subject_id):\n        \"\"\"Define the processing operations for one subject.\n        This method must be defined for all pipelines. It is used to define\n        the preprocessing steps for a single subject (note: that might mean\n        multiple images, structures, etc.). During pipeline execution, this\n        method will receive one argument, subject_id, which can be used to\n        retrieve inputs and save outputs.\n\n        Parameters\n        ----------\n        subject_id : str\n           The ID of subject to process\n        \"\"\"\n        if self.continue_processing:\n            if subject_id in self.finished_subjects:\n                return\n        # if we want overwriting or if we don't want it and the file doesn't exist, we can process\n        if self.overwrite or (not self.overwrite and not (os.path.exists(pathlib.Path(self.output_directory, subject_id).as_posix()) or self.glob_checker_nnunet(subject_id))):\n            # Check if the subject_id has already been processed\n            if os.path.exists(pathlib.Path(self.output_directory,\".temp\",f'temp_{subject_id}.pkl').as_posix()):\n                print(f\"{subject_id} already processed\")\n                return\n\n            print(\"Processing:\", subject_id)\n\n            read_results = self.input(subject_id)\n            # print(read_results)\n\n            print(subject_id, \" start\")\n            \n            metadata = {}\n            subject_modalities = set()  # all the modalities that this subject has\n            num_rtstructs = 0\n\n            for i, colname in enumerate(self.output_streams):  # sorted(self.output_streams)):  # CT comes before MR before PT before RTDOSE before RTSTRUCT\n                modality = colname.split(\"_\")[0]\n                subject_modalities.add(modality)  # set add\n                \n                # Taking modality pairs if it exists till _{num}\n                output_stream = (\"_\").join([item for item in colname.split(\"_\") if not item.isnumeric()])\n\n                # If there are multiple connections existing, multiple connections means two modalities connected to one modality. They end with _1\n                # mult_conn = colname.split(\"_\")[-1].isnumeric()\n                # num = colname.split(\"_\")[-1]\n\n                if self.v:\n                    print(\"output_stream:\", output_stream)\n\n                if read_results[i] is None:\n                    print(\"The subject id: {} has no {}\".format(subject_id, colname))\n                    pass\n\n                # Process image (CT/MR)\n                elif modality == \"CT\" or modality == 'MR':\n                    image = read_results[i].image\n                    if len(image.GetSize()) == 4:\n                        assert image.GetSize()[-1] == 1, f\"There is more than one volume in this CT file for {subject_id}.\"\n                        extractor = sitk.ExtractImageFilter()\n                        extractor.SetSize([*image.GetSize()[:3], 0])\n                        extractor.SetIndex([0, 0, 0, 0])    \n                        \n                        image = extractor.Execute(image)\n                        if self.v:\n                            print(\"image.GetSize():\", image.GetSize())\n                    try:\n                        image = self.resample(image)\n                    except Exception as e:\n                        print(e)\n                        warnings.warn(\"Could not resample {} for subject {}\".format(colname, subject_id))\n\n                    # update the metadata for this image\n                    if hasattr(read_results[i], \"metadata\") and read_results[i].metadata is not None:\n                        metadata.update(read_results[i].metadata)\n\n                    # modality is MR and the user has selected to have nnunet output\n                    if self.is_nnunet:\n                        if modality == \"MR\":  # MR images can have various modalities like FLAIR, T1, etc.\n                            if metadata[\"AcquisitionContrast\"] not in self.total_modality_counter.keys():\n                                self.total_modality_counter[metadata[\"AcquisitionContrast\"]] = 1\n                            else:\n                                self.total_modality_counter[metadata[\"AcquisitionContrast\"]] += 1\n                            self.nnunet_info['current_modality'] = metadata[\"AcquisitionContrast\"]\n                            if metadata[\"AcquisitionContrast\"] not in self.nnunet_info[\"modalities\"].keys():  # if the modality is new\n                                self.nnunet_info[\"modalities\"][metadata[\"AcquisitionContrast\"]] = str(len(self.nnunet_info[\"modalities\"])).zfill(4)  # fill to 4 digits\n                        else:\n                            self.nnunet_info['current_modality'] = modality  # CT\n                            if modality not in self.total_modality_counter.keys():\n                                self.total_modality_counter[modality] = 1\n                            else:\n                                self.total_modality_counter[modality] += 1\n                        if \"_\".join(subject_id.split(\"_\")[1::]) in self.train:\n                            self.output(subject_id, image, output_stream, nnunet_info=self.nnunet_info)\n                        else:\n                            self.output(subject_id, image, output_stream, nnunet_info=self.nnunet_info, train_or_test=\"Ts\")\n                    elif self.is_nnunet_inference:\n                        self.nnunet_info[\"current_modality\"] = modality if modality == \"CT\" else metadata[\"AcquisitionContrast\"]\n                        if self.nnunet_info[\"current_modality\"] not in self.nnunet_info[\"modalities\"].keys():\n                            raise ValueError(f\"The modality {self.nnunet_info['current_modality']} is not in the list of modalities that are present in dataset.json.\")\n                        self.output(subject_id, image, output_stream, nnunet_info=self.nnunet_info)\n                    else:\n                        self.output(subject_id, image, output_stream)\n\n                    metadata[f\"size_{output_stream}\"] = str(image.GetSize())\n                    print(subject_id, \" SAVED IMAGE\")\n\n                # Process dose\n                elif modality == \"RTDOSE\":\n                    try:   # For cases with no image present\n                        doses = read_results[i].resample_dose(image)\n                    except:\n                        Warning(\"No CT image present. Returning dose image without resampling\")\n                        doses = read_results[i]\n                    \n                    # save output\n                    self.output(subject_id, doses, output_stream)\n                    metadata[f\"size_{output_stream}\"] = str(doses.GetSize())\n                    metadata[f\"metadata_{colname}\"] = [read_results[i].get_metadata()]\n\n                    if hasattr(doses, \"metadata\") and doses.metadata is not None:\n                        metadata.update(doses.metadata)\n\n                    print(subject_id, \" SAVED DOSE\")\n                    \n                # Process PET\n                elif modality == \"PT\":\n                    try:\n                        # For cases with no image present\n                        pet = read_results[i].resample_pet(image)\n                    except:\n                        Warning(\"No CT image present. Returning PT/PET image without resampling.\")\n                        pet = read_results[i]\n\n                    # output\n                    self.output(subject_id, pet, output_stream)\n                    metadata[f\"size_{output_stream}\"] = str(pet.GetSize())\n                    metadata[f\"metadata_{colname}\"] = [read_results[i].get_metadata()]\n\n                    if hasattr(pet, \"metadata\") and pet.metadata is not None:\n                        metadata.update(pet.metadata)\n\n                    print(subject_id, \" SAVED PET\")\n\n                # Process contour\n                elif modality == \"RTSTRUCT\":\n                    num_rtstructs += 1\n                    # For RTSTRUCT, you need image or PT\n                    structure_set = read_results[i]\n                    conn_to = output_stream.split(\"_\")[-1]\n\n                    # make_binary_mask relative to ct/pet\n                    if conn_to in [\"CT\", \"MR\", \"PT\"]:\n                        if conn_to == \"CT\" or conn_to == \"MR\":\n                            img = image\n                        elif conn_to == \"PT\":\n                            img = pet  # noqa: F821\n                        \n                        mask = self.make_binary_mask(structure_set, img, \n                                                     self.existing_roi_indices, \n                                                     self.ignore_missing_regex, \n                                                     roi_select_first=self.roi_select_first, \n                                                     roi_separate=self.roi_separate)\n\n                    else:\n                        raise ValueError(\"You need to pass a reference CT or PT/PET image to map contours to.\")\n                    \n                    if mask is None:  # ignored the missing regex, and exit the loop\n                        if self.is_nnunet:\n                            image_test_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n                            image_train_path = pathlib.Path(self.output_directory, \"imagesTr\").as_posix()\n                            if os.path.exists(image_test_path):\n                                all_files = glob.glob(pathlib.Path(image_test_path, \"*.nii.gz\").as_posix())\n                                # print(all_files)\n                                for file in all_files:\n                                    if subject_id in os.path.split(file)[1]:\n                                        os.remove(file)\n                            if os.path.exists(image_train_path):\n                                all_files = glob.glob(pathlib.Path(image_train_path, \"*.nii.gz\").as_posix())\n                                # print(all_files)\n                                for file in all_files:\n                                    if subject_id in os.path.split(file)[1]:\n                                        os.remove(file)\n                            warnings.warn(f\"Patient {subject_id} is missing a complete image-label pair\")\n                            self.patients_with_missing_labels.add(\"\".join(subject_id.split(\"_\")[1:]))\n                            return\n                        else:\n                            break\n                    \n                    for name in mask.roi_indices.keys():\n                        if name not in self.existing_roi_indices.keys():\n                            self.existing_roi_indices[name] = len(self.existing_roi_indices)\n                    mask.existing_roi_indices = self.existing_roi_indices\n\n                    if self.v:\n                        print(\"mask.GetSize():\", mask.GetSize())\n                    mask_arr = np.transpose(sitk.GetArrayFromImage(mask))\n                    \n                    if self.is_nnunet:\n                        sparse_mask = np.transpose(mask.generate_sparse_mask().mask_array)\n                        sparse_mask = sitk.GetImageFromArray(sparse_mask)  # convert the nparray to sitk image\n                        sparse_mask.CopyInformation(image)\n                        if \"_\".join(subject_id.split(\"_\")[1::]) in self.train:\n                            self.output(subject_id, sparse_mask, output_stream, nnunet_info=self.nnunet_info, label_or_image=\"labels\")  # rtstruct is label for nnunet\n                        else:\n                            self.output(subject_id, sparse_mask, output_stream, nnunet_info=self.nnunet_info, label_or_image=\"labels\", train_or_test=\"Ts\")\n                    else:\n                        # if there is only one ROI, sitk.GetArrayFromImage() will return a 3d array instead of a 4d array with one slice\n                        if len(mask_arr.shape) == 3:\n                            mask_arr = mask_arr.reshape(1, mask_arr.shape[0], mask_arr.shape[1], mask_arr.shape[2])\n                        \n                        if self.v:\n                            print(mask_arr.shape)\n\n                        roi_names_list = list(mask.roi_indices.keys())\n                        for i in range(mask_arr.shape[0]):\n                            new_mask = sitk.GetImageFromArray(np.transpose(mask_arr[i]))\n                            new_mask.CopyInformation(mask)\n                            new_mask = Segmentation(new_mask)\n                            mask_to_process = new_mask\n                            \n                            # output\n                            self.output(subject_id, mask_to_process, output_stream, True, roi_names_list[i])\n                    \n                    if hasattr(structure_set, \"metadata\") and structure_set.metadata is not None:\n                        metadata.update(structure_set.metadata)\n\n                    metadata[f\"metadata_{colname}\"] = [structure_set.roi_names]\n                    for roi, labels in mask.raw_roi_names.items():\n                        metadata[f\"raw_labels_{roi}\"] = labels                    \n\n                    print(subject_id, \"SAVED MASK ON\", conn_to)\n                \n                \n                \n                metadata[f\"output_folder_{colname}\"] = pathlib.Path(subject_id, colname).as_posix()\n            \n            # Saving all the metadata in multiple text files\n            metadata[\"Modalities\"] = str(list(subject_modalities))\n            metadata[\"numRTSTRUCTs\"] = num_rtstructs\n            if self.is_nnunet:\n                metadata[\"Train or Test\"] = \"train\" if \"_\".join(subject_id.split(\"_\")[1::]) in self.train else \"test\"\n            with open(pathlib.Path(self.output_directory,\".temp\",f'{subject_id}.pkl').as_posix(),'wb') as f:  # the continue flag depends on this being the last line in this method\n                pickle.dump(metadata,f)\n            return \n    \n    def save_data(self):\n        \"\"\"\n        Saves metadata about processing. \n        \"\"\"\n        files = glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())\n        for file in files:\n            filename = pathlib.Path(file).name\n            if filename == \"init_parameters.pkl\":\n                continue\n            subject_id = os.path.splitext(filename)[0]\n            with open(file,\"rb\") as f:\n                metadata = pickle.load(f)\n\n            self.output_df.loc[subject_id, metadata.keys()] = metadata.values()  # subject id targets the rows with that subject id and it is reassigning all the metadata values by key\n\n        folder_renames = {}\n        for col in self.output_df.columns:\n            if col.startswith(\"folder\"):\n                self.output_df[col] = self.output_df[col].apply(lambda x: x if not isinstance(x, str) else pathlib.Path(x).as_posix().split(self.input_directory)[1][1:]) # rel path, exclude the slash at the beginning\n                folder_renames[col] = f\"input_{col}\"\n        self.output_df.rename(columns=folder_renames, inplace=True)  # append input_ to the column name\n        self.output_df.to_csv(self.output_df_path)  # dataset.csv\n\n        shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n\n        # Save dataset json\n        if self.is_nnunet:  # dataset.json for nnunet and .sh file to run to process it\n            imagests_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n            images_test_location = imagests_path if os.path.exists(imagests_path) else None\n            # print(self.existing_roi_indices)\n            generate_dataset_json(pathlib.Path(self.output_directory, \"dataset.json\").as_posix(),\n                                  pathlib.Path(self.output_directory, \"imagesTr\").as_posix(),\n                                  images_test_location,\n                                  tuple(self.nnunet_info[\"modalities\"].keys()),\n                                  {v: k for k, v in self.existing_roi_indices.items()},\n                                  os.path.split(self.input_directory)[1])\n            _, child = os.path.split(self.output_directory)\n            shell_path = pathlib.Path(self.output_directory, child.split(\"_\")[1]+\".sh\").as_posix()\n            if os.path.exists(shell_path):\n                os.remove(shell_path)\n            with open(shell_path, \"w\", newline=\"\\n\") as f:\n                output = \"#!/bin/bash\\n\"\n                output += \"set -e\"\n                output += f'export nnUNet_raw_data_base=\"{self.base_output_directory}/nnUNet_raw_data_base\"\\n'\n                output += f'export nnUNet_preprocessed=\"{self.base_output_directory}/nnUNet_preprocessed\"\\n'\n                output += f'export RESULTS_FOLDER=\"{self.base_output_directory}/nnUNet_trained_models\"\\n\\n'\n                output += f'nnUNet_plan_and_preprocess -t {self.task_id} --verify_dataset_integrity\\n\\n'\n                output += 'for (( i=0; i<5; i++ ))\\n'\n                output += 'do\\n'\n                output += f'    nnUNet_train 3d_fullres nnUNetTrainerV2 {os.path.split(self.output_directory)[1]} $i --npz\\n'\n                output += 'done'\n                f.write(output)\n            markdown_report_images(self.output_directory, self.total_modality_counter)  # images saved to the output directory\n        \n        # Save summary info (factor into different file)\n        markdown_path = pathlib.Path(self.output_directory, \"report.md\").as_posix()\n        with open(markdown_path, \"w\", newline=\"\\n\") as f:\n            output = \"# Dataset Report\\n\\n\"\n            if not self.is_nnunet:\n                output += \"## Patients with broken DICOM references\\n\\n\"\n                output += \"<details>\\n\"\n                output += \"\\t<summary>Click to see the list of patients with broken DICOM references</summary>\\n\\n\\t\"\n                formatted_list = \"\\n\\t\".join(self.broken_patients)\n                output += f\"{formatted_list}\\n\"\n                output += \"</details>\\n\\n\"\n\n            if self.is_nnunet:\n                output += \"## Train Test Split\\n\\n\"\n                # pie_path = pathlib.Path(self.output_directory, \"markdown_images\", \"nnunet_train_test_pie.png\").as_posix()\n                pie_path = pathlib.Path(\"markdown_images\", \"nnunet_train_test_pie.png\").as_posix()\n                output += f\"![Pie Chart of Train Test Split]({pie_path})\\n\\n\"\n                output += \"## Image Modality Distribution\\n\\n\"\n                # bar_path = pathlib.Path(self.output_directory, \"markdown_images\", \"nnunet_modality_count.png\").as_posix()\n                bar_path = pathlib.Path(\"markdown_images\", \"nnunet_modality_count.png\").as_posix()\n                output += f\"![Pie Chart of Image Modality Distribution]({bar_path})\\n\\n\"\n            f.write(output)\n\n    def run(self):\n        \"\"\"Execute the pipeline, possibly in parallel.\n        \"\"\"\n        # Joblib prints progress to stdout if verbose > 50\n        verbose = 51 if self.v or self.show_progress else 0\n\n        subject_ids = self._get_loader_subject_ids()\n        patient_ids = []\n        if not self.dry_run:\n            for subject_id in subject_ids:\n                if subject_id.split(\"_\")[1::] not in patient_ids:\n                    patient_ids.append(\"_\".join(subject_id.split(\"_\")[1::]))\n            if self.is_nnunet:\n                custom_train = []\n                custom_test = []\n                if self.custom_train_test_split:\n                    patient_ids = [item for item in patient_ids if item not in self.custom_split[\"train\"] and item not in self.custom_split[\"test\"]]\n                    custom_test = self.custom_split[\"test\"]\n                    custom_train = self.custom_split[\"train\"]\n                if self.train_size == 1:\n                    self.train = patient_ids\n                    self.test = []\n                    self.train.extend(custom_train)\n                    self.test.extend(custom_test)\n                else:\n                    self.train, self.test = train_test_split(sorted(patient_ids), train_size=self.train_size, random_state=self.random_state)\n                    self.train.extend(custom_train)\n                    self.test.extend(custom_test)\n            else:\n                self.train, self.test = [], []\n            # Note that returning any SimpleITK object in process_one_subject is\n            # not supported yet, since they cannot be pickled\n            if os.path.exists(self.output_df_path) and not self.overwrite:\n                print(\"Dataset already processed...\")\n                shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n            else:\n                Parallel(n_jobs=self.n_jobs, verbose=verbose, require='sharedmem')(\n                        delayed(self._process_wrapper)(subject_id) for subject_id in subject_ids)\n                # for subject_id in subject_ids:\n                #     self._process_wrapper(subject_id)\n                self.broken_patients = []\n                if not self.is_nnunet:\n                    all_patient_names = glob.glob(pathlib.Path(self.input_directory, \"*\", \" \").as_posix()[0:-1])\n                    all_patient_names = [os.path.split(os.path.split(x)[0])[1] for x in all_patient_names]\n                    for e in all_patient_names:\n                        if e not in patient_ids:\n                            warnings.warn(f\"Patient {e} does not have proper DICOM references\")\n                            self.broken_patients.append(e)\n                self.save_data()\n\n\ndef main():\n    args = parser()\n    args_dict = vars(args)\n    \n    if args.continue_processing:\n        try:\n            with open(pathlib.Path(args.output_directory, \".temp\", \"init_parameters.pkl\").as_posix(), \"rb\") as f:\n                args_dict = dill.load(f)\n        except:\n            print(\"Could not resume processing. Starting processing from the beginning.\")\n\n    print('initializing AutoPipeline...')\n    pipeline = AutoPipeline(**args_dict)\n\n    if not args.dry_run:\n        print('starting AutoPipeline...')\n        pipeline.run()\n        print('finished AutoPipeline!')\n    else:\n        print('dry run complete, no processing done')\n\n    \"\"\"Print general summary info\"\"\"\n\n    \"\"\"Print nnU-Net specific info here:\n    * dataset.json can be found at /path/to/dataset/json\n    * You can train nnU-Net by cloning /path/to/nnunet/repo and run `nnUNet_plan_and_preprocess -t taskID` to let the nnU-Net package prepare\n    \"\"\"\n    print(f\"Outputted data to {args.output_directory}\")\n    csv_path = pathlib.Path(args.output_directory, \"dataset.csv\").as_posix()\n    print(f\"Dataset info found at {csv_path}\")\n    if args.nnunet:\n        json_path = pathlib.Path(args.output_directory, \"dataset.json\").as_posix()\n        print(f\"dataset.json for nnU-net can be found at {json_path}\")\n        print(\"You can train nnU-net by cloning https://github.com/MIC-DKFZ/nnUNet/ and run `nnUNet_plan_and_preprocess -t taskID` to let the nnU-Net package prepare\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "qa_pairs": [
      {
        "question": "What is the purpose of the AutoPipeline class in this code?",
        "answer": "The AutoPipeline class is designed to process medical imaging data, particularly for the RADCURE dataset. It loads CT images and structure sets, resamples the images, and draws GTV contours using the resampled image. It can also prepare data for nnUNet training and inference."
      },
      {
        "question": "How does the code handle different modalities of medical images?",
        "answer": "The code handles different modalities (CT, MR, PT, RTDOSE, RTSTRUCT) separately in the process_one_subject method. Each modality has its own processing steps, such as resampling for CT/MR, dose calculation for RTDOSE, and mask generation for RTSTRUCT. The code also supports multi-modal inputs and can prepare data for nnUNet, which requires specific formatting for different modalities."
      },
      {
        "question": "What is the purpose of the 'dry_run' parameter in the AutoPipeline class?",
        "answer": "The 'dry_run' parameter allows the pipeline to be run without actually processing or writing any output files. When set to True, it simulates the pipeline execution, which is useful for testing the setup and configuration without modifying any data. This helps in verifying the pipeline's logic and identifying potential issues before running the actual processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class AutoPipeline(Pipeline):\n    def __init__(self,\n                 input_directory=\"\",\n                 output_directory=\"\",\n                 modalities=\"CT\",\n                 spacing=(1., 1., 0.),\n                 n_jobs=-1,\n                 visualize=False,\n                 missing_strategy=\"drop\",\n                 show_progress=False,\n                 warn_on_error=False,\n                 overwrite=False,\n                 nnunet=False,\n                 train_size=1.0,\n                 random_state=42,\n                 read_yaml_label_names=False,\n                 ignore_missing_regex=False,\n                 roi_yaml_path=\"\",\n                 custom_train_test_split=False,\n                 nnunet_inference=False,\n                 dataset_json_path=\"\",\n                 continue_processing=False,\n                 dry_run=False,\n                 verbose=False,\n                 update=False,\n                 roi_select_first=False,\n                 roi_separate=False):\n        # Initialize attributes\n        self.continue_processing = continue_processing\n        self.dry_run = dry_run\n        self.v = verbose\n\n        if dry_run:\n            nnunet = False\n            nnunet_inference = False\n\n        if dry_run and continue_processing:\n            raise ValueError(\"Cannot continue processing a dry run. Set --continue_processing to False to do a dry run.\")\n\n        if not dry_run and output_directory == \"\":\n            raise ValueError(\"Must specify an output directory\")\n        \n        # Configure input/output directories\n        self.input_directory = self.configure_directory(input_directory)\n        self.output_directory = self.configure_directory(output_directory)\n        \n        # Check/create output directory\n        if not os.path.exists(self.output_directory):\n            os.makedirs(self.output_directory)\n\n        # Check input directory exists\n        if not os.path.exists(self.input_directory):\n            raise FileNotFoundError(f\"Input directory {self.input_directory} does not exist\")\n        \n        # Continue processing checks\n        if not nnunet and continue_processing and not os.path.exists(pathlib.Path(self.output_directory, \".temp\").as_posix()):\n            raise FileNotFoundError(f\"Cannot continue processing. .temp directory does not exist in {self.output_directory}. Run without --continue_processing to start from scratch.\")\n\n        # Initialize other attributes and perform additional checks\n        # ...\n\n    def configure_directory(self, directory):\n        if not os.path.isabs(directory):\n            return pathlib.Path(os.getcwd(), directory).as_posix()\n        return pathlib.Path(directory).as_posix()",
        "complete": "class AutoPipeline(Pipeline):\n    def __init__(self,\n                 input_directory=\"\",\n                 output_directory=\"\",\n                 modalities=\"CT\",\n                 spacing=(1., 1., 0.),\n                 n_jobs=-1,\n                 visualize=False,\n                 missing_strategy=\"drop\",\n                 show_progress=False,\n                 warn_on_error=False,\n                 overwrite=False,\n                 nnunet=False,\n                 train_size=1.0,\n                 random_state=42,\n                 read_yaml_label_names=False,\n                 ignore_missing_regex=False,\n                 roi_yaml_path=\"\",\n                 custom_train_test_split=False,\n                 nnunet_inference=False,\n                 dataset_json_path=\"\",\n                 continue_processing=False,\n                 dry_run=False,\n                 verbose=False,\n                 update=False,\n                 roi_select_first=False,\n                 roi_separate=False):\n        # Initialize attributes\n        self.continue_processing = continue_processing\n        self.dry_run = dry_run\n        self.v = verbose\n\n        if dry_run:\n            nnunet = False\n            nnunet_inference = False\n\n        if dry_run and continue_processing:\n            raise ValueError(\"Cannot continue processing a dry run. Set --continue_processing to False to do a dry run.\")\n\n        if not dry_run and output_directory == \"\":\n            raise ValueError(\"Must specify an output directory\")\n        \n        # Configure input/output directories\n        self.input_directory = self.configure_directory(input_directory)\n        self.output_directory = self.configure_directory(output_directory)\n        \n        # Check/create output directory\n        if not os.path.exists(self.output_directory):\n            os.makedirs(self.output_directory)\n\n        # Check input directory exists\n        if not os.path.exists(self.input_directory):\n            raise FileNotFoundError(f\"Input directory {self.input_directory} does not exist\")\n        \n        # Continue processing checks\n        if not nnunet and continue_processing and not os.path.exists(pathlib.Path(self.output_directory, \".temp\").as_posix()):\n            raise FileNotFoundError(f\"Cannot continue processing. .temp directory does not exist in {self.output_directory}. Run without --continue_processing to start from scratch.\")\n\n        # Initialize other attributes\n        self.overwrite = overwrite\n        self.spacing = spacing\n        self.is_nnunet = nnunet\n        self.nnunet_info = {} if nnunet or nnunet_inference else None\n        self.train_size = train_size\n        self.random_state = random_state\n        self.label_names = {}\n        self.ignore_missing_regex = ignore_missing_regex\n        self.custom_train_test_split = custom_train_test_split\n        self.is_nnunet_inference = nnunet_inference\n        self.roi_select_first = roi_select_first\n        self.roi_separate = roi_separate\n\n        # Perform additional checks and initializations\n        self.check_nnunet_settings(nnunet, nnunet_inference, modalities, dataset_json_path)\n        self.load_roi_names(read_yaml_label_names, roi_yaml_path)\n        self.check_train_size()\n        self.check_custom_train_test_split()\n\n        # Initialize pipeline components\n        self.initialize_pipeline_components(input_directory, modalities, n_jobs, visualize, update, output_directory)\n\n        super().__init__(n_jobs=n_jobs,\n                         missing_strategy=missing_strategy,\n                         show_progress=show_progress,\n                         warn_on_error=warn_on_error)\n\n    def configure_directory(self, directory):\n        if not os.path.isabs(directory):\n            return pathlib.Path(os.getcwd(), directory).as_posix()\n        return pathlib.Path(directory).as_posix()\n\n    def check_nnunet_settings(self, nnunet, nnunet_inference, modalities, dataset_json_path):\n        if nnunet_inference:\n            if modalities not in [\"CT\", \"MR\"]:\n                raise ValueError(\"nnUNet inference can only be run on image files. Please set modalities to 'CT' or 'MR'\")\n            self.load_nnunet_inference_info(dataset_json_path)\n\n    def load_roi_names(self, read_yaml_label_names, roi_yaml_path):\n        if read_yaml_label_names:\n            self.label_names = self.load_yaml_file(roi_yaml_path or pathlib.Path(self.input_directory, \"roi_names.yaml\").as_posix())\n            self.validate_label_names()\n\n    def check_train_size(self):\n        if self.train_size == 1.0 and self.is_nnunet:\n            warnings.warn(\"Train size is 1, all data will be used for training\")\n        if self.train_size == 0.0 and self.is_nnunet:\n            warnings.warn(\"Train size is 0, all data will be used for testing\")\n        if self.train_size != 1 and not self.is_nnunet:\n            warnings.warn(\"Cannot run train/test split without nnunet, ignoring train_size\")\n        if self.train_size > 1 or self.train_size < 0 and self.is_nnunet:\n            raise ValueError(\"train_size must be between 0 and 1\")\n\n    def check_custom_train_test_split(self):\n        if self.custom_train_test_split and not self.is_nnunet:\n            raise ValueError(\"Cannot use custom train/test split without nnunet\")\n        if self.custom_train_test_split and self.is_nnunet:\n            self.load_custom_split()\n\n    def initialize_pipeline_components(self, input_directory, modalities, n_jobs, visualize, update, output_directory):\n        self.input = ImageAutoInput(input_directory, modalities, n_jobs, visualize, update)\n        self.output_df_path = pathlib.Path(output_directory, \"dataset.csv\").as_posix()\n        self.output_df = self.input.df_combined\n        self.output_streams = self.input.output_streams\n        self.resample = Resample(spacing=self.spacing)\n        self.make_binary_mask = StructureSetToSegmentation(roi_names=self.label_names, continuous=False)\n        self.output = ImageAutoOutput(output_directory, self.output_streams, self.nnunet_info, self.is_nnunet_inference)\n        self.existing_roi_indices = {\"background\": 0}\n\n    def load_yaml_file(self, file_path):\n        if os.path.exists(file_path):\n            with open(file_path, \"r\") as f:\n                try:\n                    return yaml.safe_load(f)\n                except yaml.YAMLError as exc:\n                    print(exc)\n        else:\n            raise FileNotFoundError(f\"No file named {os.path.basename(file_path)} found at {file_path}\")\n\n    def validate_label_names(self):\n        if not isinstance(self.label_names, dict):\n            raise ValueError(\"roi_names.yaml must parse as a dictionary\")\n        for k, v in self.label_names.items():\n            if not isinstance(v, (list, str)) or (isinstance(v, list) and not all(isinstance(a, str) for a in v)):\n                raise ValueError(f\"Label values must be either a list of strings or a string. Got {v} for {k}\")\n            if not isinstance(k, str):\n                raise ValueError(f\"Label names must be a string. Got {k} for {v}\")\n\n    def load_nnunet_inference_info(self, dataset_json_path):\n        if not os.path.exists(dataset_json_path):\n            raise FileNotFoundError(f\"No file named {dataset_json_path} found. Image modality definitions are required for nnUNet inference\")\n        with open(dataset_json_path, \"r\") as f:\n            self.nnunet_info[\"modalities\"] = {v: k.zfill(4) for k, v in json.load(f)[\"modality\"].items()}\n\n    def load_custom_split(self):\n        custom_split_path = pathlib.Path(self.input_directory, \"custom_train_test_split.yaml\").as_posix()\n        self.custom_split = self.load_yaml_file(custom_split_path)\n        self.validate_custom_split()"
      },
      {
        "partial": "def process_one_subject(self, subject_id):\n    if self.continue_processing and subject_id in self.finished_subjects:\n        return\n    \n    if self.overwrite or not self.subject_exists(subject_id):\n        if self.subject_already_processed(subject_id):\n            print(f\"{subject_id} already processed\")\n            return\n\n        print(\"Processing:\", subject_id)\n        read_results = self.input(subject_id)\n        print(subject_id, \" start\")\n        \n        metadata = {}\n        subject_modalities = set()\n        num_rtstructs = 0\n\n        for i, colname in enumerate(self.output_streams):\n            modality = colname.split(\"_\")[0]\n            subject_modalities.add(modality)\n            \n            output_stream = (\"_\").join([item for item in colname.split(\"_\") if not item.isnumeric()])\n\n            if self.v:\n                print(\"output_stream:\", output_stream)\n\n            if read_results[i] is None:\n                print(f\"The subject id: {subject_id} has no {colname}\")\n                continue\n\n            # Process image (CT/MR)\n            if modality in [\"CT\", \"MR\"]:\n                image ="
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "shutil",
        "glob",
        "pickle",
        "numpy",
        "warnings",
        "yaml",
        "json",
        "SimpleITK",
        "dill"
      ],
      "from_imports": [
        "aifc.Error",
        "imgtools.ops.StructureSetToSegmentation",
        "imgtools.pipeline.Pipeline",
        "imgtools.utils.nnunet.generate_dataset_json",
        "imgtools.utils.args.parser",
        "joblib.Parallel",
        "imgtools.modules.Segmentation",
        "sklearn.model_selection.train_test_split"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/filterNoisyCurves.R",
    "language": "R",
    "content": "#' Viability measurements in dose-reponse curves must remain stable or decrease\n#' monotonically reflecting response to the drug being tested.\n#' filterNoisyCurves flags dose-response curves that strongly violate these\n#' assumptions.\n#'\n#' @examples\n#' data(GDSCsmall)\n#' filterNoisyCurves(GDSCsmall)\n#'\n#' @param pSet [PharmacoSet] a PharmacoSet object\n#' @param epsilon `numeric` a value indicates assumed threshold for the\n#'   distance between to consecutive viability values on the drug-response curve\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv\n#' @param positive.cutoff.percent `numeric` This value indicates that function\n#'   may violate epsilon rule for how many points on drug-response curve\n#' @param mean.viablity `numeric` average expected viability value\n#' @param nthread `numeric` if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#'\n#' @return a list with two elements 'noisy' containing the rownames of the\n#'   noisy curves, and 'ok' containing the rownames of the non-noisy curves\n#'\n#' @export\nfilterNoisyCurves <- function(pSet, epsilon=25 , positive.cutoff.percent=.80,\n        mean.viablity=200, nthread=1) {\n\n    acceptable <- mclapply(rownames(sensitivityInfo(pSet)), function(xp) {\n        #for(xp in rownames(sensitivityInfo(pSet))){\n        drug.responses <- as.data.frame(apply(sensitivityRaw(pSet)[xp , ,], 2, as.numeric), stringsAsFactors=FALSE)\n        drug.responses <- drug.responses[complete.cases(drug.responses), ]\n        doses.no <- nrow(drug.responses)\n\n        drug.responses[, \"delta\"] <- .computeDelta(drug.responses$Viability)\n\n        delta.sum <- sum(drug.responses$delta, na.rm = TRUE)\n\n        max.cum.sum <- .computeCumSumDelta(drug.responses$Viability)\n\n        if (\n            (table(drug.responses$delta < epsilon)[\"TRUE\"] >=\n                (doses.no * positive.cutoff.percent)) &\n            (delta.sum < epsilon) &\n            (max.cum.sum < (2 * epsilon)) &\n            (mean(drug.responses$Viability) < mean.viablity)\n        ) {\n            return (xp)\n        }\n    }, mc.cores=nthread)\n    acceptable <- unlist(acceptable)\n    noisy <- setdiff(rownames(sensitivityInfo(pSet)), acceptable)\n    return(list(\"noisy\"=noisy, \"ok\"=acceptable))\n}\n\n.computeDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if (trunc) {\n        return(c(pmin(100, xx[seq(2,length(xx))]) - pmin(100, xx[seq_along(xx)-1]), 0))\n    } else {\n        return(c(xx[seq(2, length(xx))] - xx[seq_along(xx) - 1]), 0)\n    }\n}\n\n#' @importFrom utils combn\n.computeCumSumDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if(trunc) {\n        xx <- pmin(xx, 100)\n    }\n    tt <- t(combn(seq_along(xx), 2 , simplify = TRUE))\n    tt <- tt[which(((tt[,2] - tt[,1]) >= 2) == TRUE),]\n    cum.sum <- unlist(lapply(seq_len(nrow(tt)), function(x) { xx[tt[x, 2]] - xx[tt[x, 1]]}))\n    return(max(cum.sum))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `filterNoisyCurves` function and what are its main input parameters?",
        "answer": "The `filterNoisyCurves` function is designed to flag dose-response curves that violate assumptions about viability measurements in drug response experiments. It takes a PharmacoSet object as its main input, along with optional parameters like epsilon (threshold for consecutive viability differences), positive.cutoff.percent (allowed violation percentage), mean.viability (average expected viability), and nthread (for parallelization)."
      },
      {
        "question": "Explain the logic behind determining if a curve is 'acceptable' in the `filterNoisyCurves` function.",
        "answer": "A curve is considered 'acceptable' if it meets four criteria: 1) At least 80% (by default) of the delta values are less than epsilon, 2) The sum of all delta values is less than epsilon, 3) The maximum cumulative sum of deltas is less than twice epsilon, and 4) The mean viability is less than the specified mean.viability threshold. These criteria ensure that the curve is mostly monotonically decreasing and within expected viability ranges."
      },
      {
        "question": "What is the purpose of the `.computeCumSumDelta` function and how does it contribute to the noise filtering process?",
        "answer": "The `.computeCumSumDelta` function calculates the maximum cumulative sum of differences between all pairs of viability values that are at least two positions apart in the dose-response curve. This helps identify large increases in viability over the course of the experiment, which would be unexpected in a typical dose-response scenario. By comparing this value to a threshold in the main function, it helps filter out curves with unusual or noisy patterns."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filterNoisyCurves <- function(pSet, epsilon=25, positive.cutoff.percent=.80, mean.viablity=200, nthread=1) {\n    acceptable <- mclapply(rownames(sensitivityInfo(pSet)), function(xp) {\n        drug.responses <- as.data.frame(apply(sensitivityRaw(pSet)[xp , ,], 2, as.numeric), stringsAsFactors=FALSE)\n        drug.responses <- drug.responses[complete.cases(drug.responses), ]\n        doses.no <- nrow(drug.responses)\n\n        drug.responses[, \"delta\"] <- .computeDelta(drug.responses$Viability)\n\n        delta.sum <- sum(drug.responses$delta, na.rm = TRUE)\n\n        max.cum.sum <- .computeCumSumDelta(drug.responses$Viability)\n\n        if (\n            # Complete the condition here\n        ) {\n            return (xp)\n        }\n    }, mc.cores=nthread)\n    acceptable <- unlist(acceptable)\n    noisy <- setdiff(rownames(sensitivityInfo(pSet)), acceptable)\n    return(list(\"noisy\"=noisy, \"ok\"=acceptable))\n}",
        "complete": "filterNoisyCurves <- function(pSet, epsilon=25, positive.cutoff.percent=.80, mean.viablity=200, nthread=1) {\n    acceptable <- mclapply(rownames(sensitivityInfo(pSet)), function(xp) {\n        drug.responses <- as.data.frame(apply(sensitivityRaw(pSet)[xp , ,], 2, as.numeric), stringsAsFactors=FALSE)\n        drug.responses <- drug.responses[complete.cases(drug.responses), ]\n        doses.no <- nrow(drug.responses)\n\n        drug.responses[, \"delta\"] <- .computeDelta(drug.responses$Viability)\n\n        delta.sum <- sum(drug.responses$delta, na.rm = TRUE)\n\n        max.cum.sum <- .computeCumSumDelta(drug.responses$Viability)\n\n        if (\n            (table(drug.responses$delta < epsilon)[\"TRUE\"] >= (doses.no * positive.cutoff.percent)) &&\n            (delta.sum < epsilon) &&\n            (max.cum.sum < (2 * epsilon)) &&\n            (mean(drug.responses$Viability) < mean.viablity)\n        ) {\n            return (xp)\n        }\n    }, mc.cores=nthread)\n    acceptable <- unlist(acceptable)\n    noisy <- setdiff(rownames(sensitivityInfo(pSet)), acceptable)\n    return(list(\"noisy\"=noisy, \"ok\"=acceptable))\n}"
      },
      {
        "partial": ".computeCumSumDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if(trunc) {\n        xx <- pmin(xx, 100)\n    }\n    tt <- t(combn(seq_along(xx), 2 , simplify = TRUE))\n    tt <- tt[which(((tt[,2] - tt[,1]) >= 2) == TRUE),]\n    # Complete the function here\n}",
        "complete": ".computeCumSumDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if(trunc) {\n        xx <- pmin(xx, 100)\n    }\n    tt <- t(combn(seq_along(xx), 2 , simplify = TRUE))\n    tt <- tt[which(((tt[,2] - tt[,1]) >= 2) == TRUE),]\n    cum.sum <- unlist(lapply(seq_len(nrow(tt)), function(x) { xx[tt[x, 2]] - xx[tt[x, 1]]} ))\n    return(max(cum.sum))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_4_split_utils.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\n\ntest_that(\"strSplit splits a character vector into a matrix based on a delimiter\", {\n  input <- \"Hello,World\"\n  expected <- matrix(c(\"Hello\", \"World\"), ncol = 2, byrow = TRUE)\n  result <- strSplit(input, \",\")\n  expect_equal(result, expected)\n})\n\ntest_that(\".strSplitFinite splits a string into multiple substrings based on a delimiter\", {\n  input <- \"Hello,World,How,Are,You\"\n  result <- .strSplitFinite(input, \",\", 3, fixed = TRUE)\n  expected <- c(\"Hello\", \"World\", \"How,Are,You\")\n  expect_equal(result[[1]], expected)\n\n  result2 <- .strSplitFinite(input, \",\", 4, fixed = TRUE)\n  expected2 <- c(\"Hello\", \"World\", \"How\", \"Are,You\")\n  expect_equal(result2[[1]], expected2)\n\n  result3 <- .strSplitFinite(input, \",\", 3, fixed = FALSE)\n  expected3 <- c(\"Hello\", \"World\", \"How,Are,You\")\n  expect_equal(result3[[1]], expected3)\n})\n\ntest_that(\".strSplitInfinite splits a character vector into substrings based on a delimiter\", {\n  input <- c(\"apple,banana,orange\", \"cat,dog,rabbit\")\n  expected <- list(c(\"apple\", \"banana\", \"orange\"), c(\"cat\", \"dog\", \"rabbit\"))\n  result <- .strSplitInfinite(input, \",\", fixed = TRUE)\n  expect_equal(result, expected)\n})\n\ntest_that(\".splitCol splits a column into a character list\", {\n  input <- data.table(col = c(\"apple;banana\", \"orange;grape\"))\n  result <- .splitCol(input, \"col\", split = \";\")\n  expected <- data.table(col = list(c(\"apple\", \"banana\"), c(\"orange\", \"grape\")))\n  expect_equal(result, expected)\n\n  result2 <- .splitCol(input, \"col\", split = \"; \")\n  expected2 <- data.table(col = c(list(\"apple;banana\"), list(\"orange;grape\")))\n  expect_equal(result2, expected2)\n  \n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `strSplit` function in the given code snippet, and how does it differ from `.strSplitFinite`?",
        "answer": "The `strSplit` function splits a character vector into a matrix based on a delimiter. It returns a matrix with each element of the input vector split into columns. On the other hand, `.strSplitFinite` splits a string into a specified number of substrings based on a delimiter. The main difference is that `strSplit` always splits the entire string, while `.strSplitFinite` allows you to control the number of splits and returns the remaining unsplit portion as the last element."
      },
      {
        "question": "How does the `fixed` parameter in `.strSplitFinite` and `.strSplitInfinite` functions affect their behavior?",
        "answer": "The `fixed` parameter in both `.strSplitFinite` and `.strSplitInfinite` functions determines whether the delimiter should be interpreted as a fixed string or as a regular expression. When `fixed = TRUE`, the delimiter is treated as a literal string. When `fixed = FALSE`, the delimiter is interpreted as a regular expression pattern. This allows for more flexible splitting options, especially when dealing with complex patterns or multiple delimiters."
      },
      {
        "question": "What is the purpose of the `.splitCol` function, and how does it handle different types of delimiters?",
        "answer": "The `.splitCol` function is designed to split a column in a data.table into a character list based on a specified delimiter. It takes a data.table, a column name, and a split parameter as input. The function can handle different types of delimiters, including single characters (like ';') and multi-character strings (like '; '). When the delimiter exactly matches the split parameter, it splits the column values into a list of character vectors. If the delimiter doesn't match exactly, it treats the entire column value as a single element in the resulting list."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"strSplit splits a character vector into a matrix based on a delimiter\", {\n  input <- \"Hello,World\"\n  expected <- matrix(c(\"Hello\", \"World\"), ncol = 2, byrow = TRUE)\n  result <- strSplit(input, \",\")\n  # Add assertion here\n})",
        "complete": "test_that(\"strSplit splits a character vector into a matrix based on a delimiter\", {\n  input <- \"Hello,World\"\n  expected <- matrix(c(\"Hello\", \"World\"), ncol = 2, byrow = TRUE)\n  result <- strSplit(input, \",\")\n  expect_equal(result, expected)\n})"
      },
      {
        "partial": "test_that(\".splitCol splits a column into a character list\", {\n  input <- data.table(col = c(\"apple;banana\", \"orange;grape\"))\n  result <- .splitCol(input, \"col\", split = \";\")\n  # Add assertion for the first test case\n\n  result2 <- .splitCol(input, \"col\", split = \"; \")\n  # Add assertion for the second test case\n})",
        "complete": "test_that(\".splitCol splits a column into a character list\", {\n  input <- data.table(col = c(\"apple;banana\", \"orange;grape\"))\n  result <- .splitCol(input, \"col\", split = \";\")\n  expected <- data.table(col = list(c(\"apple\", \"banana\"), c(\"orange\", \"grape\")))\n  expect_equal(result, expected)\n\n  result2 <- .splitCol(input, \"col\", split = \"; \")\n  expected2 <- data.table(col = c(list(\"apple;banana\"), list(\"orange;grape\")))\n  expect_equal(result2, expected2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/connectivityScore.R",
    "language": "R",
    "content": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  CoreGx::connectivityScore(x, y, method, nperm, nthread, gwc.method, ...)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@inherit` and `@inheritParams` tags in the function documentation?",
        "answer": "The `@inherit` tag is used to inherit the entire documentation from the `CoreGx::connectivityScore` function, while `@inheritParams` is used to inherit the parameter documentation from the same function. This allows the new function to reuse the documentation from the original function without duplicating it."
      },
      {
        "question": "What does the `...` argument in the function signature represent, and why is it important?",
        "answer": "The `...` (ellipsis) argument allows additional arguments to be passed to the function. It's important because it enables the function to accept and forward any extra parameters to the `CoreGx::connectivityScore` function, maintaining flexibility and compatibility with the original function's interface."
      },
      {
        "question": "How does this function relate to the `CoreGx::connectivityScore` function, and what is its main purpose?",
        "answer": "This function is a wrapper for the `CoreGx::connectivityScore` function. Its main purpose is to provide a simplified interface to the original function, potentially within a different package or namespace. It maintains the same functionality and parameters as the original function but allows for easier access or customization within the current package context."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  # Complete the function body\n}",
        "complete": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  CoreGx::connectivityScore(x, y, method, nperm, nthread, gwc.method, ...)\n}"
      },
      {
        "partial": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, ...) \n{\n  # Complete the function signature and body\n}",
        "complete": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  CoreGx::connectivityScore(x, y, method, nperm, nthread, gwc.method, ...)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/cellosaurus_helpers.R",
    "language": "R",
    "content": "#' Create a query list for Cellosaurus database\n#'\n#' This function creates a query list for the Cellosaurus database based on the provided IDs and fields.\n#' The query list is used to retrieve specific information from the database.\n#'\n#' @param ids A vector of IDs to be included in the query list.\n#' @param from A character vector specifying the fields to be included in the query list.\n#'             If the length of 'from' is 1, the same field will be used for all IDs.\n#'             If the length of 'from' is equal to the length of 'ids', each ID will be paired with its corresponding field.\n#'             Otherwise, an error will be thrown.\n#' @param fuzzy A logical value indicating whether to perform a fuzzy search. Default is FALSE.\n#'\n#' @return A character vector representing the query list.\n#'\n#' @examples\n#' AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n#' # Returns: \"Accession:ID1\" \"Accession:ID2\" \"Accession:ID3\"\n#'\n#' AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n#' # Returns: \"Accession:ID1\" \"Name:ID2\" \"Species:ID3\"\n#'\n#' @keywords internal\n#' @noRd\n.create_cellosaurus_queries <- function(ids, from, fuzzy = FALSE) {\n\n  if(fuzzy){\n    ids <- paste0(cleanCharacterStrings(ids), \"~\")\n  }\n\n  # either from has to be one field, or the same length as ids\n  if (length(from) == 1) {\n    return(paste0(from, \":\", ids))\n  }\n  if (length(from) != length(ids)) {\n    stop(\"Length of 'from' must be 1 or the same length as 'ids'\")\n  }\n  sapply(1:length(ids), function(i) {\n    paste(from[i], ids[i], sep = \":\")\n  })\n}\n\n\n\n#' Build a Cellosaurus API request\n#'\n#' This function builds a Cellosaurus API request based on the provided parameters.\n#'\n#' @param query A character vector specifying the query terms for the Cellosaurus API.\n#' @param to A character vector specifying the fields to include in the API response.\n#' @param numResults An integer specifying the maximum number of results to return.\n#' @param apiResource A character string specifying the API resource to query.\n#' @param output A character string specifying the desired output format of the API response.\n#' @param sort A character string specifying the field to sort the results by.\n#' @param query_only A logical value indicating whether to return only the constructed URL without making the request.\n#' @param ... Additional arguments to be passed to the function.\n#'\n#' @return A character string representing the constructed URL for the Cellosaurus API request.\n#'\n#' @examples\n#' .build_cellosaurus_request(query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n#'                           numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n#'                           query_only = FALSE)\n#' \n#' @keywords internal\n#' @noRd\n.build_cellosaurus_request <- function(\n    query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n    numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n    query_only = FALSE,  ...) {\n  \n  checkmate::assert_character(c(query, output))\n  checkmate::assert_choice(apiResource, c(\"search/cell-line\", \"cell-line\", \"release-info\"))\n  checkmate::assert_choice(output, c(\"TSV\", \"TXT\", \"JSON\", \"XML\"))\n\n  base_url <- \"https://api.cellosaurus.org\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, apiResource)\n\n  opts <- list()\n  \n  if(apiResource == \"search/cell-line\"){\n  \"https://api.cellosaurus.org/search/cell-line?q=idsy%3ADOR%2013&sort=ac%20asc&fields=ac%2Cid%2Csy%2Cmisspelling%2Cdr%2Ccc&format=txt&rows=10000\"\n    opts$q <- paste0(query, collapse = \" \")\n  } else if(apiResource == \"cell-line\"){\n    url$path <- .buildURL(url$path, query)\n  }\n\n  if (!is.null(sort)) {\n    opts$sort <- paste0(sort, \" asc\")\n  }\n\n  opts$fields <- paste0(to, collapse = \",\")\n  opts$format <- tolower(output)\n  opts$rows <- numResults\n\n\n  url$query <- opts\n  url <- url |> httr2::url_build()\n  if (query_only) {\n    return(url)\n  }\n  url |> .build_request()\n}\n\n\n\n#' Get the Cellosaurus schema\n#'\n#' This function retrieves the Cellosaurus schema from the Cellosaurus API.\n#' It internally calls the `.buildURL()`, `.build_request()`, `.perform_request()`,\n#' and `.parse_resp_json()` functions to construct the API URL, send the request,\n#' and parse the response.\n#'\n#' @return A list representing the Cellosaurus schema.\n#'\n#' @keywords internal\n#' @noRd\n.cellosaurus_schema <- function() {\n  url <- .buildURL(\"https://api.cellosaurus.org/openapi.json\")\n  request <- .build_request(url)\n\n  resp <- .perform_request(request)\n  .parse_resp_json(resp)\n}\n\n\n\n\n\n#' Internal function to return the list of external resources available in Cellosaurus\n#' @return A character vector of external resources available in Cellosaurus\n#'\n#' @keywords internal\n#' @noRd\n.cellosaurus_extResources <- function() {\n  c(\n    \"4DN\", \"Abcam\", \"ABCD\", \"ABM\", \"AddexBio\", \"ArrayExpress\",\n    \"ATCC\", \"BCGO\", \"BCRC\", \"BCRJ\", \"BEI_Resources\",\n    \"BioGRID_ORCS_Cell_line\", \"BTO\", \"BioSample\", \"BioSamples\",\n    \"cancercelllines\", \"CancerTools\", \"CBA\", \"CCLV\", \"CCRID\",\n    \"CCTCC\", \"Cell_Biolabs\", \"Cell_Model_Passport\", \"CGH-DB\",\n    \"ChEMBL-Cells\", \"ChEMBL-Targets\", \"CLDB\", \"CLO\", \"CLS\",\n    \"ColonAtlas\", \"Coriell\", \"Cosmic\", \"Cosmic-CLP\", \"dbGAP\",\n    \"dbMHC\", \"DepMap\", \"DGRC\", \"DiscoverX\", \"DSHB\", \"DSMZ\",\n    \"DSMZCellDive\", \"EBiSC\", \"ECACC\", \"EFO\", \"EGA\", \"ENCODE\",\n    \"ESTDAB\", \"FCDI\", \"FCS-free\", \"FlyBase_Cell_line\", \"GDSC\",\n    \"GeneCopoeia\", \"GEO\", \"HipSci\", \"HIVReagentProgram\", \"Horizon_Discovery\",\n    \"hPSCreg\", \"IARC_TP53\", \"IBRC\", \"ICLC\", \"ICLDB\", \"IGRhCellID\",\n    \"IGSR\", \"IHW\", \"Imanis\", \"Innoprot\", \"IPD-IMGT/HLA\", \"ISCR\",\n    \"IZSLER\", \"JCRB\", \"KCB\", \"KCLB\", \"Kerafast\", \"KYinno\", \"LiGeA\",\n    \"LIMORE\", \"LINCS_HMS\", \"LINCS_LDP\", \"Lonza\", \"MCCL\", \"MeSH\",\n    \"MetaboLights\", \"Millipore\", \"MMRRC\", \"NCBI_Iran\", \"NCI-DTP\", \"NHCDR\",\n    \"NIHhESC\", \"NISES\", \"NRFC\", \"PerkinElmer\", \"PharmacoDB\", \"PRIDE\",\n    \"Progenetix\", \"PubChem_Cell_line\", \"RCB\", \"Rockland\", \"RSCB\", \"SKIP\",\n    \"SKY/M-FISH/CGH\", \"SLKBase\", \"TKG\", \"TNGB\", \"TOKU-E\", \"Ubigene\",\n    \"WiCell\", \"Wikidata\", \"Ximbio\"\n  )\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.create_cellosaurus_queries` function and how does it handle different input scenarios?",
        "answer": "The `.create_cellosaurus_queries` function creates a query list for the Cellosaurus database based on provided IDs and fields. It handles three scenarios: 1) If a single field is provided, it pairs it with all IDs. 2) If the number of fields matches the number of IDs, it pairs them one-to-one. 3) If the lengths don't match, it throws an error. The function also supports fuzzy search by appending '~' to cleaned IDs when the `fuzzy` parameter is TRUE."
      },
      {
        "question": "How does the `.build_cellosaurus_request` function construct the API URL, and what are its key parameters?",
        "answer": "The `.build_cellosaurus_request` function constructs a Cellosaurus API URL by combining a base URL with various parameters. Key parameters include: `query` for search terms, `to` for specifying response fields, `numResults` for limiting results, `apiResource` for choosing the API endpoint, `output` for response format, and `sort` for result ordering. It uses `httr2::url_parse` and `httr2::url_build` to construct the URL, and can optionally return just the URL without making the request if `query_only` is TRUE."
      },
      {
        "question": "What is the purpose of the `.cellosaurus_extResources` function and how might it be used in the context of the Cellosaurus API?",
        "answer": "The `.cellosaurus_extResources` function returns a character vector containing a list of external resources available in Cellosaurus. This function could be used to validate or suggest external resource names when querying the Cellosaurus API. For example, it might be used to filter or validate the 'to' parameter in the `.build_cellosaurus_request` function, ensuring that only valid external resources are requested in the API call."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".create_cellosaurus_queries <- function(ids, from, fuzzy = FALSE) {\n  if(fuzzy){\n    ids <- paste0(cleanCharacterStrings(ids), \"~\")\n  }\n\n  if (length(from) == 1) {\n    return(paste0(from, \":\", ids))\n  }\n  if (length(from) != length(ids)) {\n    stop(\"Length of 'from' must be 1 or the same length as 'ids'\")\n  }\n  # Complete the function here\n}",
        "complete": ".create_cellosaurus_queries <- function(ids, from, fuzzy = FALSE) {\n  if(fuzzy){\n    ids <- paste0(cleanCharacterStrings(ids), \"~\")\n  }\n\n  if (length(from) == 1) {\n    return(paste0(from, \":\", ids))\n  }\n  if (length(from) != length(ids)) {\n    stop(\"Length of 'from' must be 1 or the same length as 'ids'\")\n  }\n  sapply(1:length(ids), function(i) {\n    paste(from[i], ids[i], sep = \":\")\n  })\n}"
      },
      {
        "partial": ".build_cellosaurus_request <- function(\n    query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n    numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n    query_only = FALSE,  ...) {\n  \n  checkmate::assert_character(c(query, output))\n  checkmate::assert_choice(apiResource, c(\"search/cell-line\", \"cell-line\", \"release-info\"))\n  checkmate::assert_choice(output, c(\"TSV\", \"TXT\", \"JSON\", \"XML\"))\n\n  base_url <- \"https://api.cellosaurus.org\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, apiResource)\n\n  opts <- list()\n  \n  # Complete the function here\n}",
        "complete": ".build_cellosaurus_request <- function(\n    query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n    numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n    query_only = FALSE,  ...) {\n  \n  checkmate::assert_character(c(query, output))\n  checkmate::assert_choice(apiResource, c(\"search/cell-line\", \"cell-line\", \"release-info\"))\n  checkmate::assert_choice(output, c(\"TSV\", \"TXT\", \"JSON\", \"XML\"))\n\n  base_url <- \"https://api.cellosaurus.org\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, apiResource)\n\n  opts <- list()\n  \n  if(apiResource == \"search/cell-line\"){\n    opts$q <- paste0(query, collapse = \" \")\n  } else if(apiResource == \"cell-line\"){\n    url$path <- .buildURL(url$path, query)\n  }\n\n  if (!is.null(sort)) {\n    opts$sort <- paste0(sort, \" asc\")\n  }\n\n  opts$fields <- paste0(to, collapse = \",\")\n  opts$format <- tolower(output)\n  opts$rows <- numResults\n\n  url$query <- opts\n  url <- url |> httr2::url_build()\n  if (query_only) {\n    return(url)\n  }\n  url |> .build_request()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_unichem_helpers.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Valid endpoint returns correct URL\", {\n  endpoint <- \"compounds\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  actual_url <- .build_unichem_query(endpoint)\n  expect_equal(actual_url, expected_url)\n})\n\ntest_that(\"Invalid endpoint throws an error\", {\n  endpoint <- \"invalid_endpoint\"\n  expect_error(.build_unichem_query(endpoint))\n})\n\ntest_that(\"Query only option returns httr2::httr2_url object\", {\n  endpoint <- \"images\"\n  query_only <- TRUE\n  expected_class <- \"httr2_url\"\n  actual_url <- .build_unichem_query(endpoint, query_only)\n  expect_class(actual_url, expected_class)\n})\n\n\ntest_that(\"Valid compound request is built correctly\", {\n  type <- \"uci\"\n  compound <- \"538323\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound\n  )\n  actual_request <- .build_unichem_compound_req(type, compound)\n  expect_equal(actual_request$url, expected_url)\n  expect_equal(actual_request$body$data, expected_body)\n})\n\ntest_that(\"Valid sourceID compound request is built correctly\", {\n  type <- \"sourceID\"\n  compound <- \"2244\"\n  sourceID <- 22\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound,\n    sourceID = sourceID\n  )\n  actual_request <- .build_unichem_compound_req(type, compound, sourceID)\n  expect_equal(actual_request$url, expected_url)\n  expect_equal(actual_request$body$data, expected_body)\n\n\n  response <- actual_request |> \n    .perform_request() |>  \n    .parse_resp_json()  \n\n  checkmate::expect_names(\n    names(response), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(response$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n  \n\n})\n\ntest_that(\"Invalid type throws an error\", {\n  type <- \"invalid_type\"\n  compound <- \"538323\"\n  expect_error(.build_unichem_compound_req(type, compound))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_unichem_query()` function in this code, and how is it being tested?",
        "answer": "The `.build_unichem_query()` function appears to construct URLs for the UniChem API. It's being tested for three scenarios: 1) It correctly builds a URL for a valid endpoint, 2) It throws an error for an invalid endpoint, and 3) It returns an `httr2_url` object when the `query_only` parameter is set to TRUE."
      },
      {
        "question": "How does the `.build_unichem_compound_req()` function handle different types of compound requests, and what are its parameters?",
        "answer": "The `.build_unichem_compound_req()` function builds requests for compound information. It takes parameters `type`, `compound`, and optionally `sourceID`. It constructs a request with a URL and a body containing these parameters. The function handles different types of requests, including 'uci' and 'sourceID', adjusting the body of the request accordingly."
      },
      {
        "question": "What assertions are made about the structure of the API response in the last test case, and why might these be important?",
        "answer": "The last test case checks the structure of the API response using `checkmate::expect_names()`. It verifies that the response contains expected top-level keys ('compounds', 'notFound', 'response', 'totalCompounds') and that the 'compounds' object has expected keys ('inchi', 'sources', 'standardInchiKey', 'uci'). These assertions are important to ensure the API response maintains a consistent structure, which is crucial for reliable data parsing and processing in the application."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Valid endpoint returns correct URL\", {\n  endpoint <- \"compounds\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  actual_url <- .build_unichem_query(endpoint)\n  # Complete the test\n})",
        "complete": "test_that(\"Valid endpoint returns correct URL\", {\n  endpoint <- \"compounds\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  actual_url <- .build_unichem_query(endpoint)\n  expect_equal(actual_url, expected_url)\n})"
      },
      {
        "partial": "test_that(\"Valid sourceID compound request is built correctly\", {\n  type <- \"sourceID\"\n  compound <- \"2244\"\n  sourceID <- 22\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound,\n    sourceID = sourceID\n  )\n  actual_request <- .build_unichem_compound_req(type, compound, sourceID)\n  # Complete the test assertions and response checking\n})",
        "complete": "test_that(\"Valid sourceID compound request is built correctly\", {\n  type <- \"sourceID\"\n  compound <- \"2244\"\n  sourceID <- 22\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound,\n    sourceID = sourceID\n  )\n  actual_request <- .build_unichem_compound_req(type, compound, sourceID)\n  expect_equal(actual_request$url, expected_url)\n  expect_equal(actual_request$body$data, expected_body)\n\n  response <- actual_request |> \n    .perform_request() |>  \n    .parse_resp_json()  \n\n  checkmate::expect_names(\n    names(response), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(response$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/args.py",
    "language": "py",
    "content": "from argparse import ArgumentParser\n\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    # arguments\n    parser.add_argument(\"input_directory\", type=str,\n                        help=\"Path to top-level directory of dataset.\")\n\n    parser.add_argument(\"output_directory\", type=str,\n                        help=\"Path to output directory to save processed images.\")\n\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\",\n                        help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n\n    parser.add_argument(\"--visualize\", default=False, action=\"store_true\",\n                        help=\"Whether to visualize the data graph\")\n\n    parser.add_argument(\"--spacing\", nargs=3, type=float, default=(1., 1., 0.),\n                        help=\"The resampled voxel spacing in  (x, y, z) directions.\")\n\n    parser.add_argument(\"--n_jobs\", type=int, default=-1,\n                        help=\"The number of parallel processes to use.\")\n\n    parser.add_argument(\"--show_progress\", action=\"store_true\",\n                        help=\"Whether to print progress to standard output.\")\n\n    parser.add_argument(\"--warn_on_error\", default=False, action=\"store_true\",\n                        help=\"Whether to warn on error.\")\n\n    parser.add_argument(\"--overwrite\", default=False, action=\"store_true\",\n                        help=\"Whether to write output files even if existing output files exist.\")\n    \n    parser.add_argument(\"--nnunet\", default=False, action=\"store_true\",\n                        help=\"Whether to make the output conform to nnunet requirements.\")\n\n    parser.add_argument(\"--train_size\", type=float, default=1.0,\n                        help=\"The proportion of data to be used for training, as a decimal.\")\n\n    parser.add_argument(\"--random_state\", type=int, default=42,\n                        help=\"The random state to be used for the train-test-split.\")\n\n    parser.add_argument(\"--read_yaml_label_names\", default=False, action=\"store_true\",\n                        help=\"Whether to read the label names from roi_names.yaml in the input directory.\")\n\n    parser.add_argument(\"--ignore_missing_regex\", default=False, action=\"store_true\",\n                        help=\"Whether to ignore patients with no ROI regexes that match the given ones. Will throw an error on patients without matches if this is not set.\")\n\n    parser.add_argument(\"--roi_yaml_path\", type=str, default=\"\",\n                        help=\"Path to the YAML file defining ROI regexes\")\n\n    parser.add_argument(\"--custom_train_test_split\", default=False, action=\"store_true\",\n                        help=\"Whether to use a custom train-test-split, stored in custom_train_test_split.yaml in the input directory.\")\n\n    parser.add_argument(\"--nnunet_inference\", default=False, action=\"store_true\",\n                        help=\"Whether to generate data for nnUNet inference.\")\n    \n    parser.add_argument(\"--dataset_json_path\", type=str, default=\"\",\n                        help=\"Path to the dataset.json file defining image modality indices for nnUNet inference.\")\n\n    parser.add_argument(\"--continue_processing\", default=False, action=\"store_true\",\n                        help=\"Whether to continue processing a partially completed dataset.\")\n    \n    parser.add_argument(\"--dry_run\", default=False, action=\"store_true\",\n                        help=\"Make a dry run of the pipeline, only producing the edge table and dataset.csv.\")\n\n    parser.add_argument(\"--verbose\", default=False, action=\"store_true\",\n                        help=\"Verbose output flag.\")\n\n    parser.add_argument(\"--update\", default=False, action=\"store_true\",\n                        help=\"Update crawled index. In other words, process from scratch.\")\n\n    parser.add_argument(\"--roi_select_first\", default=False, action=\"store_true\",\n                        help=\"Only select first matching regex/ROI name.\")\n\n    parser.add_argument(\"--roi_separate\", default=False, action=\"store_true\",\n                        help=\"Process each matching regex/ROI into separate masks. Each matched mask will be saved as ROI_n. (n = index from list of regex/ROIs)\")\n\n    # parser.add_argument(\"--custom_train_test_split_path\", type=str,\n    #                     help=\"Path to the YAML file defining the custom train-test-split.\")\n\n    return parser.parse_known_args()[0]\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'parser()' function in this code snippet?",
        "answer": "The 'parser()' function creates and configures an ArgumentParser object for a command-line interface. It defines various command-line arguments for an image processing pipeline, including input and output directories, processing options, and data handling preferences. The function returns the parsed arguments as an object."
      },
      {
        "question": "How does the code handle optional boolean flags, such as '--visualize' or '--overwrite'?",
        "answer": "Optional boolean flags are handled using the 'action=\"store_true\"' parameter in the add_argument() method. This means that if the flag is present in the command-line arguments, its value will be set to True; otherwise, it defaults to False. For example, '--visualize' and '--overwrite' are both implemented this way, allowing users to enable these options by simply including the flag in their command."
      },
      {
        "question": "What is the purpose of the '--spacing' argument, and how is it configured?",
        "answer": "The '--spacing' argument is used to specify the resampled voxel spacing in the x, y, and z directions for image processing. It is configured using 'nargs=3' to accept exactly three float values, with a default of (1., 1., 0.). This allows users to customize the voxel spacing of the processed images, which can be crucial for maintaining consistency across different imaging modalities or datasets."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    # arguments\n    parser.add_argument(\"input_directory\", type=str,\n                        help=\"Path to top-level directory of dataset.\")\n\n    parser.add_argument(\"output_directory\", type=str,\n                        help=\"Path to output directory to save processed images.\")\n\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\",\n                        help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n\n    parser.add_argument(\"--visualize\", default=False, action=\"store_true\",\n                        help=\"Whether to visualize the data graph\")\n\n    # Add more arguments here\n\n    return parser.parse_known_args()[0]",
        "complete": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    parser.add_argument(\"input_directory\", type=str, help=\"Path to top-level directory of dataset.\")\n    parser.add_argument(\"output_directory\", type=str, help=\"Path to output directory to save processed images.\")\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\", help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n    parser.add_argument(\"--visualize\", default=False, action=\"store_true\", help=\"Whether to visualize the data graph\")\n    parser.add_argument(\"--spacing\", nargs=3, type=float, default=(1., 1., 0.), help=\"The resampled voxel spacing in (x, y, z) directions.\")\n    parser.add_argument(\"--n_jobs\", type=int, default=-1, help=\"The number of parallel processes to use.\")\n    parser.add_argument(\"--show_progress\", action=\"store_true\", help=\"Whether to print progress to standard output.\")\n    parser.add_argument(\"--warn_on_error\", default=False, action=\"store_true\", help=\"Whether to warn on error.\")\n    parser.add_argument(\"--overwrite\", default=False, action=\"store_true\", help=\"Whether to write output files even if existing output files exist.\")\n    parser.add_argument(\"--nnunet\", default=False, action=\"store_true\", help=\"Whether to make the output conform to nnunet requirements.\")\n    parser.add_argument(\"--train_size\", type=float, default=1.0, help=\"The proportion of data to be used for training, as a decimal.\")\n    parser.add_argument(\"--random_state\", type=int, default=42, help=\"The random state to be used for the train-test-split.\")\n    parser.add_argument(\"--read_yaml_label_names\", default=False, action=\"store_true\", help=\"Whether to read the label names from roi_names.yaml in the input directory.\")\n    parser.add_argument(\"--ignore_missing_regex\", default=False, action=\"store_true\", help=\"Whether to ignore patients with no ROI regexes that match the given ones. Will throw an error on patients without matches if this is not set.\")\n    parser.add_argument(\"--roi_yaml_path\", type=str, default=\"\", help=\"Path to the YAML file defining ROI regexes\")\n    parser.add_argument(\"--custom_train_test_split\", default=False, action=\"store_true\", help=\"Whether to use a custom train-test-split, stored in custom_train_test_split.yaml in the input directory.\")\n    parser.add_argument(\"--nnunet_inference\", default=False, action=\"store_true\", help=\"Whether to generate data for nnUNet inference.\")\n    parser.add_argument(\"--dataset_json_path\", type=str, default=\"\", help=\"Path to the dataset.json file defining image modality indices for nnUNet inference.\")\n    parser.add_argument(\"--continue_processing\", default=False, action=\"store_true\", help=\"Whether to continue processing a partially completed dataset.\")\n    parser.add_argument(\"--dry_run\", default=False, action=\"store_true\", help=\"Make a dry run of the pipeline, only producing the edge table and dataset.csv.\")\n    parser.add_argument(\"--verbose\", default=False, action=\"store_true\", help=\"Verbose output flag.\")\n    parser.add_argument(\"--update\", default=False, action=\"store_true\", help=\"Update crawled index. In other words, process from scratch.\")\n    parser.add_argument(\"--roi_select_first\", default=False, action=\"store_true\", help=\"Only select first matching regex/ROI name.\")\n    parser.add_argument(\"--roi_separate\", default=False, action=\"store_true\", help=\"Process each matching regex/ROI into separate masks. Each matched mask will be saved as ROI_n. (n = index from list of regex/ROIs)\")\n\n    return parser.parse_known_args()[0]"
      },
      {
        "partial": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    # Add arguments here\n\n    return parser.parse_known_args()[0]",
        "complete": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    parser.add_argument(\"input_directory\", type=str, help=\"Path to top-level directory of dataset.\")\n    parser.add_argument(\"output_directory\", type=str, help=\"Path to output directory to save processed images.\")\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\", help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Whether to visualize the data graph\")\n    parser.add_argument(\"--spacing\", nargs=3, type=float, default=(1., 1., 0.), help=\"The resampled voxel spacing in (x, y, z) directions.\")\n    parser.add_argument(\"--n_jobs\", type=int, default=-1, help=\"The number of parallel processes to use.\")\n    parser.add_argument(\"--show_progress\", action=\"store_true\", help=\"Whether to print progress to standard output.\")\n    parser.add_argument(\"--warn_on_error\", action=\"store_true\", help=\"Whether to warn on error.\")\n    parser.add_argument(\"--overwrite\", action=\"store_true\", help=\"Whether to write output files even if existing output files exist.\")\n    parser.add_argument(\"--nnunet\", action=\"store_true\", help=\"Whether to make the output conform to nnunet requirements.\")\n    parser.add_argument(\"--train_size\", type=float, default=1.0, help=\"The proportion of data to be used for training, as a decimal.\")\n    parser.add_argument(\"--random_state\", type=int, default=42, help=\"The random state to be used for the train-test-split.\")\n    parser.add_argument(\"--read_yaml_label_names\", action=\"store_true\", help=\"Whether to read the label names from roi_names.yaml in the input directory.\")\n    parser.add_argument(\"--ignore_missing_regex\", action=\"store_true\", help=\"Whether to ignore patients with no ROI regexes that match the given ones. Will throw an error on patients without matches if this is not set.\")\n    parser.add_argument(\"--roi_yaml_path\", type=str, default=\"\", help=\"Path to the YAML file defining ROI regexes\")\n    parser.add_argument(\"--custom_train_test_split\", action=\"store_true\", help=\"Whether to use a custom train-test-split, stored in custom_train_test_split.yaml in the input directory.\")\n    parser.add_argument(\"--nnunet_inference\", action=\"store_true\", help=\"Whether to generate data for nnUNet inference.\")\n    parser.add_argument(\"--dataset_json_path\", type=str, default=\"\", help=\"Path to the dataset.json file defining image modality indices for nnUNet inference.\")\n    parser.add_argument(\"--continue_processing\", action=\"store_true\", help=\"Whether to continue processing a partially completed dataset.\")\n    parser.add_argument(\"--dry_run\", action=\"store_true\", help=\"Make a dry run of the pipeline, only producing the edge table and dataset.csv.\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output flag.\")\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"Update crawled index. In other words, process from scratch.\")\n    parser.add_argument(\"--roi_select_first\", action=\"store_true\", help=\"Only select first matching regex/ROI name.\")\n    parser.add_argument(\"--roi_separate\", action=\"store_true\", help=\"Process each matching regex/ROI into separate masks. Each matched mask will be saved as ROI_n. (n = index from list of regex/ROIs)\")\n\n    return parser.parse_known_args()[0]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "argparse.ArgumentParser"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_rest_2.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n# Basic Tests\noptions(\"log_level\" = \"DEBUG\")\ntest_that(\"AnnotationGx:::.build_pubchem_rest_query\", {\n  res <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\")\n  expect_class(res, \"httr2_request\")\n\n  res2 <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\", namespace = \"name\", operation = \"cids\", output = \"JSON\")\n  expect_class(res2, \"httr2_request\")\n\n  expect_equal(res, res2)\n\n  res3 <- AnnotationGx:::.build_pubchem_rest_query(3672, namespace = \"cid\", operation = \"property/InChIKey\", output = \"JSON\")\n  expect_class(res3, \"httr2_request\")\n\n  res4 <- AnnotationGx:::.build_pubchem_rest_query(3672,\n    namespace = \"cid\",\n    operation = \"property/InChIKey\", output = \"JSON\", query_only = T\n  )\n  expect_class(res4, \"character\")\n})\n\noptions(\"log_level\" = \"WARN\")\n\ntest_that(\"AnnotationGx:::.build_pubchem_rest_query Failure\", {\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(NA))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query())\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"subStance\", namespace = \"cid\", operation = \"record\", output = \"JSON\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, operation = \"fake\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(1, domain = \"substance\", namespace = \"cid\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"compound\", namespace = \"cid\", operation = \"Title\", output = \"JSON\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(c(\"TRETINOIN\", \"erlotinib\", \"TRAMETINIB\"),\n    domain = \"compound\", namespace = \"name\",\n    operation = \"cids\", output = \"JSON\"\n  ))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, raw = \"TRUE\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, query_only = \"TRUE\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"substance\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"assay\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"cell\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"gene\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"protein\", namespace = \"not choice\"))\n\n  lapply(c(\"TSV\", \"PDF\", \"XLSX\"), function(x) expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, output = x)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_pubchem_rest_query` function in the AnnotationGx package, and how is it being tested in this code snippet?",
        "answer": "The `.build_pubchem_rest_query` function in the AnnotationGx package is likely used to construct REST API queries for the PubChem database. The code snippet tests this function's behavior under various input conditions. It checks if the function correctly builds httr2_request objects for valid inputs, handles different parameters (like namespace, operation, and output format), and properly throws errors for invalid inputs or unsupported options."
      },
      {
        "question": "How does the code handle error cases for the `.build_pubchem_rest_query` function, and what types of errors are being tested?",
        "answer": "The code uses the `expect_error()` function from the testthat package to check for various error conditions. It tests for errors when: passing NA or no arguments, using invalid domain/namespace combinations, specifying non-existent operations, passing multiple inputs for single-input parameters, using unsupported output formats, and providing invalid data types for boolean parameters. This comprehensive error testing ensures the function behaves correctly under various edge cases and invalid inputs."
      },
      {
        "question": "What is the significance of the `options(\"log_level\" = ...)` statements in the code, and how might they affect the test execution?",
        "answer": "The `options(\"log_level\" = ...)` statements are used to set the logging level for the tests. At the beginning, it's set to \"DEBUG\", which likely enables more verbose logging for the first set of tests. Later, it's changed to \"WARN\", probably to reduce the verbosity for the error-case tests. This approach allows for more detailed logging during the positive test cases while keeping the output cleaner for the expected error cases, helping to focus on the relevant information during different parts of the test suite."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"AnnotationGx:::.build_pubchem_rest_query\", {\n  res <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\")\n  expect_class(res, \"httr2_request\")\n\n  res2 <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\", namespace = \"name\", operation = \"cids\", output = \"JSON\")\n  expect_class(res2, \"httr2_request\")\n\n  expect_equal(res, res2)\n\n  # Complete the test for res3 and res4\n})",
        "complete": "test_that(\"AnnotationGx:::.build_pubchem_rest_query\", {\n  res <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\")\n  expect_class(res, \"httr2_request\")\n\n  res2 <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\", namespace = \"name\", operation = \"cids\", output = \"JSON\")\n  expect_class(res2, \"httr2_request\")\n\n  expect_equal(res, res2)\n\n  res3 <- AnnotationGx:::.build_pubchem_rest_query(3672, namespace = \"cid\", operation = \"property/InChIKey\", output = \"JSON\")\n  expect_class(res3, \"httr2_request\")\n\n  res4 <- AnnotationGx:::.build_pubchem_rest_query(3672, namespace = \"cid\", operation = \"property/InChIKey\", output = \"JSON\", query_only = TRUE)\n  expect_class(res4, \"character\")\n})"
      },
      {
        "partial": "test_that(\"AnnotationGx:::.build_pubchem_rest_query Failure\", {\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(NA))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query())\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"subStance\", namespace = \"cid\", operation = \"record\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, operation = \"fake\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(1, domain = \"substance\", namespace = \"cid\"))\n  # Complete the remaining error tests\n})",
        "complete": "test_that(\"AnnotationGx:::.build_pubchem_rest_query Failure\", {\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(NA))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query())\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"subStance\", namespace = \"cid\", operation = \"record\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, operation = \"fake\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(1, domain = \"substance\", namespace = \"cid\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"compound\", namespace = \"cid\", operation = \"Title\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(c(\"TRETINOIN\", \"erlotinib\", \"TRAMETINIB\"), domain = \"compound\", namespace = \"name\", operation = \"cids\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, raw = \"TRUE\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, query_only = \"TRUE\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"substance\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"assay\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"cell\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"gene\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"protein\", namespace = \"not choice\"))\n  lapply(c(\"TSV\", \"PDF\", \"XLSX\"), function(x) expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, output = x)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/CancerTargetDiscovery.R",
    "language": "R",
    "content": "# CancerTargetDiscoveryDevelopment Functions\n\n#' Map Compound to CTD\n#'\n#' This function maps a drug compound to the Cancer Target Discovery (CTD) database.\n#' It retrieves information about the compound from the CTD database and returns the results as a data table.\n#'\n#' @param compounds A character vector of drug compounds to map to the CTD database.\n#' @param base_url The base URL of the CTD API. Default is \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\".\n#' @param endpoint The API endpoint for the compound mapping. Default is \"compound\".\n#' @param nParallel The number of parallel processes to use. Default is one less than the number of available cores.\n#' @param raw Logical indicating whether to return the raw response from the API. Default is FALSE.\n#' @param query_only Logical indicating whether to only return the API request URL without making the actual request. Default is FALSE.\n#'\n#' @return A data table containing the mapped information for the drug compound.\n#' If the API request fails, a data table with the drug compound name will be returned.\n#' If \\code{raw} is set to TRUE, the raw response from the API will be returned.\n#'\n#' @examples\n#' mapCompound2CTD(\"Bax channel blocker\", nParallel = 1)\n#'\n#' @export\nmapCompound2CTD <- function(\n    compounds,\n    base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", \n    endpoint = \"compound\",\n    nParallel = parallel::detectCores() - 1,\n    raw = FALSE,\n    query_only = FALSE\n) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                dt <- data.table::data.table(\n                    displayName = compound)\n                return(dt)\n            }\n\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n\n            original_dt <- .asDT(resp$xrefs)[, c(\"databaseId\", \"databaseName\")]\n            original_dt[, \"displayName\" := resp$displayName]\n\n            dt <- data.table::dcast(\n                    original_dt, \n                    formula = displayName ~ databaseName, \n                    value.var = \"databaseId\"\n                )\n            return(dt)\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) {\n        names(results) <- compounds\n        return(results)\n    }\n\n    return(data.table::rbindlist(results, fill = TRUE))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CTD` function and what are its main input parameters?",
        "answer": "The `mapCompound2CTD` function maps drug compounds to the Cancer Target Discovery (CTD) database. It retrieves information about the compounds from the CTD database and returns the results as a data table. The main input parameters are:\n1. `compounds`: A character vector of drug compounds to map.\n2. `base_url`: The base URL of the CTD API (default provided).\n3. `endpoint`: The API endpoint for compound mapping (default is 'compound').\n4. `nParallel`: Number of parallel processes to use.\n5. `raw`: Boolean to return raw API response.\n6. `query_only`: Boolean to return only the API request URL without making the actual request."
      },
      {
        "question": "How does the function handle parallel processing of multiple compound requests, and what happens if a request fails?",
        "answer": "The function uses parallel processing through the `parallel::mclapply` function to handle multiple compound requests simultaneously. The number of parallel processes is determined by the `nParallel` parameter, which defaults to one less than the number of available cores.\n\nIf a request fails or returns a non-200 status code, the function handles it gracefully:\n1. It creates a data table with only the compound name (displayName).\n2. This ensures that even if some requests fail, the function still returns a result for all input compounds.\n3. The successful requests are processed normally, extracting the relevant information from the API response."
      },
      {
        "question": "Explain the data transformation process in the function when `raw = FALSE`. How is the final output structured?",
        "answer": "When `raw = FALSE` (default), the function performs the following data transformation:\n1. It extracts the 'xrefs' data from the API response and converts it to a data table with 'databaseId' and 'databaseName' columns.\n2. It adds a 'displayName' column with the compound name.\n3. The data is then reshaped using `data.table::dcast` to create a wide format table where:\n   - Each row represents a compound\n   - Columns are created for each unique 'databaseName'\n   - Cell values contain the corresponding 'databaseId'\n4. Finally, all compound results are combined using `data.table::rbindlist` with `fill = TRUE` to handle missing columns.\n\nThe final output is a single data table where each row represents a compound, and columns represent different database identifiers for that compound."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                dt <- data.table::data.table(\n                    displayName = compound)\n                return(dt)\n            }\n\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n\n            # Complete the code here\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) {\n        names(results) <- compounds\n        return(results)\n    }\n\n    return(data.table::rbindlist(results, fill = TRUE))\n}",
        "complete": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                dt <- data.table::data.table(\n                    displayName = compound)\n                return(dt)\n            }\n\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n\n            original_dt <- .asDT(resp$xrefs)[, c(\"databaseId\", \"databaseName\")]\n            original_dt[, \"displayName\" := resp$displayName]\n\n            dt <- data.table::dcast(\n                    original_dt, \n                    formula = displayName ~ databaseName, \n                    value.var = \"databaseId\"\n                )\n            return(dt)\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) {\n        names(results) <- compounds\n        return(results)\n    }\n\n    return(data.table::rbindlist(results, fill = TRUE))\n}"
      },
      {
        "partial": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    # Complete the code here\n}",
        "complete": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                return(data.table::data.table(displayName = compound))\n            }\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n            original_dt <- .asDT(resp$xrefs)[, c(\"databaseId\", \"databaseName\")]\n            original_dt[, \"displayName\" := resp$displayName]\n            data.table::dcast(original_dt, formula = displayName ~ databaseName, value.var = \"databaseId\")\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) return(setNames(results, compounds))\n    data.table::rbindlist(results, fill = TRUE)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-logging.R",
    "language": "R",
    "content": "#' Default Log formatter\n#'\n#' @title Default Log formatter\n#' @description log_fmt function to format log messages\n#' @param level `character` The log level\n#' @param ... `character` The messages to log\n#' @keywords internal\n#' @noRd\n.log_fmt <- function(level, ...) {\n  time_str <- format(Sys.time(), \"[%H:%M:%S]\")\n  level_str <- paste0(\"[\", level, \"]\")\n  msg <- paste(..., sep = \" \")\n  paste0(time_str, level_str, msg, sep = \" \", collapse = \"\\n\")\n}\n\n\n#' Info message function\n#' \n#' This function is used to print messages when the verbose option is enabled.\n#' \n#' @param ... `character` The messages to print\n#' \n#' @keywords internal\n#' @noRd\n#' @export\n#' @examples\n#' \\dontrun{\n#' .info(\"This is an info message\")\n#' }\n.info <- function(...) {\n  msg <- .log_fmt(\"INFO\", ...)\n  optionIsTRUE <- options(\"log_level\") == \"INFO\" || (options(\"log_level\") %in% c(\"WARN\", \"DEBUG\", \"ERROR\"))\n  if(optionIsTRUE) {\n    message(crayon::green(msg))\n  }\n}\n\n#' Custom message function for verbose output\n#'\n#' This function is used to print messages when the verbose option is enabled.\n#' It checks if the package-specific verbose option is set or if the global verbose option is set.\n#' If either of these options is TRUE, the message is printed in blue and bold format.\n#'\n#' @param ... `character` The messages to print\n#'\n#' @examples\n#' \\dontrun{\n#' options(\"myPackage.verbose\" = TRUE)\n#' }\n#'\n#' @keywords internal\n#' @noRd\n.debug <- function(...) {\n  msg <- .log_fmt(\"DEBUG\", ...)\n  optionIsTRUE <- options(\"log_level\") == \"DEBUG\"\n  if (optionIsTRUE) {\n    message(crayon::blue(msg))\n  }\n}\n\n#' @keywords internal\n#' @noRd\n.warn <- function(...) {\n  msg <- .log_fmt(\"WARNING\", ...)\n  optionIsTRUE <- options(\"log_level\") != \"ERROR\"\n  message(crayon::yellow(msg))\n}\n\n#' @keywords internal\n#' @noRd\n.err <- function(...) {\n  msg <- .log_fmt(\"ERROR\", ...)\n  optionIsTRUE <- options(\"log_level\") != NULL\n  stop(\"\\r\", crayon::red(msg), call. = FALSE)\n}\n\n#' Generate a function context string\n#'\n#' This function takes the name of a function and returns a string that\n#' represents the function context.\n#' The string is formatted as [packageName functionName].\n#'\n#' @keywords internal\n#' @noRd\n.funContext <- function(funName) paste0(\"[\", utils::packageName(), \"::\", funName, \"]\")\n\n# # test\n# .debug(\"This is a debug message\")\n# .info(\"This is an info message\")\n# .warn(\"This is a warning message\")\n# .error(\"This is an error message\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.log_fmt` function in the given code snippet?",
        "answer": "The `.log_fmt` function is an internal utility function used to format log messages. It takes a log level and additional message arguments, and returns a formatted string that includes the current time, log level, and the message. The function combines these elements into a single string, separating them with spaces and collapsing multiple lines with newline characters."
      },
      {
        "question": "How does the `.info` function determine whether to display a message or not?",
        "answer": "The `.info` function checks the 'log_level' option to determine whether to display the message. It will display the message if the 'log_level' option is set to 'INFO', 'WARN', 'DEBUG', or 'ERROR'. The function uses the `optionIsTRUE` variable to store this condition. If `optionIsTRUE` is TRUE, the function displays the message in green color using the `crayon::green` function."
      },
      {
        "question": "What is the difference between how the `.warn` and `.err` functions handle message display?",
        "answer": "The `.warn` function displays a warning message in yellow color using `crayon::yellow` whenever the 'log_level' option is not set to 'ERROR'. On the other hand, the `.err` function stops the execution with an error message in red color using `crayon::red` as long as the 'log_level' option is not NULL. The `.err` function uses `stop()` to halt execution, while `.warn` uses `message()` to display the warning without stopping the program."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Default Log formatter\n#'\n#' @title Default Log formatter\n#' @description log_fmt function to format log messages\n#' @param level `character` The log level\n#' @param ... `character` The messages to log\n#' @keywords internal\n#' @noRd\n.log_fmt <- function(level, ...) {\n  time_str <- format(Sys.time(), \"[%H:%M:%S]\")\n  level_str <- paste0(\"[\", level, \"]\")\n  msg <- paste(..., sep = \" \")\n  # Complete the function to format the log message\n}",
        "complete": "#' Default Log formatter\n#'\n#' @title Default Log formatter\n#' @description log_fmt function to format log messages\n#' @param level `character` The log level\n#' @param ... `character` The messages to log\n#' @keywords internal\n#' @noRd\n.log_fmt <- function(level, ...) {\n  time_str <- format(Sys.time(), \"[%H:%M:%S]\")\n  level_str <- paste0(\"[\", level, \"]\")\n  msg <- paste(..., sep = \" \")\n  paste0(time_str, level_str, msg, sep = \" \", collapse = \"\\n\")\n}"
      },
      {
        "partial": "#' Custom message function for verbose output\n#'\n#' This function is used to print messages when the verbose option is enabled.\n#' It checks if the package-specific verbose option is set or if the global verbose option is set.\n#' If either of these options is TRUE, the message is printed in blue and bold format.\n#'\n#' @param ... `character` The messages to print\n#'\n#' @examples\n#' \\dontrun{\n#' options(\"myPackage.verbose\" = TRUE)\n#' }\n#'\n#' @keywords internal\n#' @noRd\n.debug <- function(...) {\n  msg <- .log_fmt(\"DEBUG\", ...)\n  # Complete the function to check the log level and print the message if appropriate\n}",
        "complete": "#' Custom message function for verbose output\n#'\n#' This function is used to print messages when the verbose option is enabled.\n#' It checks if the package-specific verbose option is set or if the global verbose option is set.\n#' If either of these options is TRUE, the message is printed in blue and bold format.\n#'\n#' @param ... `character` The messages to print\n#'\n#' @examples\n#' \\dontrun{\n#' options(\"myPackage.verbose\" = TRUE)\n#' }\n#'\n#' @keywords internal\n#' @noRd\n.debug <- function(...) {\n  msg <- .log_fmt(\"DEBUG\", ...)\n  if (options(\"log_level\") == \"DEBUG\") {\n    message(crayon::blue(msg))\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-[.R",
    "language": "R",
    "content": "# ==== PharmacoSet Class\n#'`[`\n#'\n#' @examples\n#' data(CCLEsmall)\n#' CCLEsmall[\"WM1799\", \"Sorafenib\"]\n#'\n#' @param x object\n#' @param i Cell lines to keep in object\n#' @param j Drugs to keep in object\n#' @param ... further arguments\n#' @param drop A boolean flag of whether to drop single dimensions or not\n#'\n#'@return Returns the subsetted object\n#'\n#' @export\nsetMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i)&&is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],  molecular.data.cells=sampleNames(x)[i]))\n  }\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `[` method defined for the PharmacoSet class?",
        "answer": "The `[` method is defined to allow subsetting of a PharmacoSet object. It enables users to extract specific cell lines and drugs from the dataset, either by their names (as character vectors) or by their indices (as numeric vectors)."
      },
      {
        "question": "How does the method handle different types of input for the `i` and `j` parameters?",
        "answer": "The method checks if both `i` and `j` are character vectors or if both are numeric vectors. For character vectors, it directly uses them as cell and drug names. For numeric vectors, it treats them as indices and retrieves the corresponding cell and drug names from the object before subsetting."
      },
      {
        "question": "What is the significance of the `drop` parameter in this method, and what is its default value?",
        "answer": "The `drop` parameter is a boolean flag that determines whether to drop single dimensions in the resulting subset. Its default value is set to `FALSE`, meaning that by default, the method will preserve the dimensionality of the subset even if it contains only one cell line or drug."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i) && is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    # Complete the code here\n  }\n})",
        "complete": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i) && is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],  molecular.data.cells=sampleNames(x)[i]))\n  }\n})"
      },
      {
        "partial": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  # Complete the code here\n})",
        "complete": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i) && is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],  molecular.data.cells=sampleNames(x)[i]))\n  }\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-summarizeMolecularProfiles.R",
    "language": "R",
    "content": "#' Takes molecular data from a PharmacoSet, and summarises them\n#' into one entry per drug\n#'\n#' Given a PharmacoSet with molecular data, this function will summarize\n#' the data into one profile per cell line, using the chosen summary.stat. Note\n#' that this does not really make sense with perturbation type data, and will\n#' combine experiments and controls when doing the summary if run on a\n#' perturbation dataset.\n#'\n#' @examples\n#' data(GDSCsmall)\n#' GDSCsmall <- summarizeMolecularProfiles(GDSCsmall, mDataType = \"rna\", cell.lines=sampleNames(GDSCsmall), summary.stat = 'median', fill.missing = TRUE, verbose=TRUE)\n#' GDSCsmall\n#'\n#' @param object \\code{PharmacoSet} The PharmacoSet to summarize\n#' @param mDataType \\code{character} which one of the molecular data types\n#' to use in the analysis, out of all the molecular data types available for the pset\n#' for example: rna, rnaseq, snp\n#' @param cell.lines \\code{character} The cell lines to be summarized.\n#'   If any cell.line has no data, missing values will be created\n#' @param features \\code{caracter} A vector of the feature names to include in the summary\n#' @param summary.stat \\code{character} which summary method to use if there are repeated\n#'   cell.lines? Choices are \"mean\", \"median\", \"first\", or \"last\"\n#'   In case molecular data type is mutation or fusion \"and\" and \"or\" choices are available\n#' @param fill.missing \\code{boolean} should the missing cell lines not in the\n#'   molecular data object be filled in with missing values?\n#' @param summarize A flag which when set to FALSE (defaults to TRUE) disables summarizing and\n#'   returns the data unchanged as a ExpressionSet\n#' @param verbose \\code{boolean} should messages be printed\n#' @param binarize.threshold \\code{numeric} A value on which the molecular data is binarized.\n#'   If NA, no binarization is done.\n#' @param binarize.direction \\code{character} One of \"less\" or \"greater\", the direction of binarization on\n#'   binarize.threshold, if it is not NA.\n#' @param removeTreated \\code{logical} If treated/perturbation experiments are present, should they\n#'   be removed? Defaults to yes.\n#'\n#' @return \\code{matrix} An updated PharmacoSet with the molecular data summarized\n#'   per cell line.\n#'\n#' @importMethodsFrom CoreGx summarizeMolecularProfiles\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom SummarizedExperiment SummarizedExperiment rowData rowData<- colData colData<- assays assays<- assayNames assayNames<-\n#' @importFrom Biobase AnnotatedDataFrame\n#' @keywords internal\n#' @export\nsetMethod('summarizeMolecularProfiles', signature(object='PharmacoSet'),\n    function(object, mDataType, cell.lines, features,\n        summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n        fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n        binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n        removeTreated=TRUE)\n{\n\n    mDataTypes <- mDataNames(object)\n    if (!(mDataType %in% mDataTypes)) {\n      stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n    }\n\n    if(summarize==FALSE){\n      return(molecularProfilesSlot(object)[[mDataType]])\n    }\n\n    if (missing(features)) {\n      features <- rownames(featureInfo(object, mDataType))\n    } else {\n      fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n      if (verbose && !all(fix)) {\n        warning (sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n      }\n      features <- features[fix]\n    }\n\n    summary.stat <- match.arg(summary.stat)\n    binarize.direction <- match.arg(binarize.direction)\n\n    if((!S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) {\n      stop (\"Invalid summary.stat, choose among: mean, median, first, last\" )\n    }\n    if((S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"and\", \"or\"))) {\n      stop (\"Invalid summary.stat, choose among: and, or\" )\n    }\n\n    if (missing(cell.lines)) {\n      cell.lines <- sampleNames(object)\n    }\n\n    if(datasetType(object) %in% c(\"perturbation\", \"both\") && removeTreated){\n      if(!\"xptype\" %in% colnames(phenoInfo(object, mDataType))) {\n        warning(\"The passed in molecular data had no column: xptype.\n                 \\rEither the mDataType does not include perturbations, or the PSet is malformed.\n                 \\rAssuming the former and continuing.\")\n      } else {\n        keepCols <- phenoInfo(object, mDataType)$xptype %in% c(\"control\", \"untreated\")\n        molecularProfilesSlot(object)[[mDataType]] <- molecularProfilesSlot(object)[[mDataType]][,keepCols]\n      }\n    }\n\n\n    ##TODO:: have less confusing variable names\n    dd <- molecularProfiles(object, mDataType)\n    pp <- phenoInfo(object, mDataType)\n\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation == \"mutation\") {\n      tt <- dd\n      tt[which(!is.na(dd) & dd ==\"wt\")] <- FALSE\n      tt[which(!is.na(dd) & dd !=\"wt\")] <- TRUE\n      tt <- apply(tt, 2, as.logical)\n      dimnames(tt) <- dimnames(dd)\n      dd <- tt\n    }\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation == \"fusion\") {\n      tt <- dd\n      tt[which(!is.na(dd) & dd ==\"0\")] <- FALSE\n      tt[which(!is.na(dd) & dd !=\"0\")] <- TRUE\n      tt <- apply(tt, 2, as.logical)\n      dimnames(tt) <- dimnames(dd)\n      dd <- tt\n    }\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"cnv\", \"rna\", \"rnaseq\", \"isoform\")\n       && !is.na(binarize.threshold)) {\n      tt <- dd\n      switch(binarize.direction, \"less\" = {\n            tt[which(!is.na(dd) & dd < binarize.threshold)] <- TRUE\n            tt[which(!is.na(dd) & dd >= binarize.threshold)] <- FALSE\n      }, \"greater\" = {\n            tt[which(!is.na(dd) & dd > binarize.threshold)] <- TRUE\n            tt[which(!is.na(dd) & dd <= binarize.threshold)] <- FALSE\n      })\n      tt <- apply(tt, 2, as.logical)\n      dimnames(tt) <- dimnames(dd)\n      dd <- tt\n    }\n    if (any(colnames(dd) != rownames(pp))) {\n      warning (\"Samples in phenodata and expression matrices must be ordered the same way\")\n      dd <- dd[ , rownames(pp), drop=FALSE]\n    }\n    if (!fill.missing) {\n      cell.lines <- intersect(cell.lines, unique(pp[!is.na(pp[ , \"sampleid\"]), \"sampleid\"]))\n    }\n    if (length(cell.lines) == 0) {\n      stop (\"No cell lines in common\")\n    }\n\n    ## select profiles with no replicates\n    duplix <- unique(pp[!is.na(pp[ , \"sampleid\"]) & duplicated(pp[ , \"sampleid\"]), \"sampleid\"])\n    ucell <- setdiff(cell.lines, duplix)\n\n    ## keep the non ambiguous cases\n    dd2 <- dd[ , match(ucell, pp[ , \"sampleid\"]), drop=FALSE]\n    pp2 <- pp[match(ucell, pp[ , \"sampleid\"]), , drop=FALSE]\n    if (length(duplix) > 0) {\n      if (verbose) {\n        message(sprintf(\"Summarizing %s molecular data for:\\t%s\", mDataType, annotation(object)$name))\n        total <- length(duplix)\n        # create progress bar\n        pb <- utils::txtProgressBar(min=0, max=total, style=3)\n        i <- 1\n      }\n      ## replace factors by characters to allow for merging duplicated experiments\n      pp2 <- apply(pp2, 2, function (x) {\n        if (is.factor(x)) {\n          return (as.character(x))\n        } else {\n          return (x)\n        }\n      })\n      ## there are some replicates to collapse\n      for (x in duplix) {\n        myx <- which(!is.na(pp[ , \"sampleid\"]) & is.element(pp[ , \"sampleid\"], x))\n        switch(summary.stat,\n          \"mean\" = {\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, mean)\n          },\n          \"median\"={\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, median)\n          },\n          \"first\"={\n            ddt <- dd[ , myx[1], drop=FALSE]\n          },\n          \"last\" = {\n            ddt <- dd[ , myx[length(myx)], drop=FALSE]\n          },\n          \"and\" = {\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, function(x) do.call(`&`, as.list(x)))\n          },\n          \"or\" = {\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, function(x) do.call(`|`, as.list(x)))\n          }\n        )\n        ppt <- apply(pp[myx, , drop=FALSE], 2, function (x) {\n          x <- paste(unique(as.character(x[!is.na(x)])), collapse=\"///\")\n          return (x)\n        })\n        ppt[!is.na(ppt) & ppt == \"\"] <- NA\n        dd2 <- cbind(dd2, ddt)\n        pp2 <- rbind(pp2, ppt)\n        if (verbose){\n          utils::setTxtProgressBar(pb, i)\n          i <- i + 1\n        }\n      }\n      if (verbose) {\n        close(pb)\n      }\n    }\n    colnames(dd2) <- rownames(pp2) <- c(ucell, duplix)\n\n    ## reorder cell lines\n    dd2 <- dd2[ , cell.lines, drop=FALSE]\n    pp2 <- pp2[cell.lines, , drop=FALSE]\n    pp2[ , \"sampleid\"] <- cell.lines\n    res <- molecularProfilesSlot(object)[[mDataType]]\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\", \"fusion\")) {\n      tt <- dd2\n      tt[which(!is.na(dd2) & dd2)] <- \"1\"\n      tt[which(!is.na(dd2) & !dd2)] <- \"0\"\n      dd2 <- tt\n    }\n    res <- SummarizedExperiment::SummarizedExperiment(dd2)\n    pp2 <- S4Vectors::DataFrame(pp2, row.names=rownames(pp2))\n    pp2$tissueid <- sampleInfo(object)[pp2$sampleid, \"tissueid\"]\n    SummarizedExperiment::colData(res) <- pp2\n    SummarizedExperiment::rowData(res) <- featureInfo(object, mDataType)\n    ##TODO:: Generalize this to multiple assay SummarizedExperiments!\n    # if(!is.null(SummarizedExperiment::assay(res, 1))) {\n    #   SummarizedExperiment::assay(res, 2) <- matrix(rep(NA,\n    #                                                     length(assay(res, 1))\n    #                                                     ),\n    #                                                     nrow=nrow(assay(res, 1)),\n    #                                                     ncol=ncol(assay(res, 1)),\n    #                                                 dimnames=dimnames(assay(res, 1))\n    #                                                 )\n    # }\n    assayNames(res) <- assayNames(molecularProfilesSlot(object)[[mDataType]])[[1]]\n    res <- res[features,]\n    S4Vectors::metadata(res) <- S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])\n    return(res)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeMolecularProfiles` function in this code snippet?",
        "answer": "The `summarizeMolecularProfiles` function is designed to summarize molecular data from a PharmacoSet object into one entry per cell line. It processes various types of molecular data (e.g., RNA, mutations, fusions) and can apply different summary statistics (mean, median, first, last, and, or) to handle replicate experiments. The function also allows for data binarization and can handle perturbation experiments."
      },
      {
        "question": "How does the function handle different types of molecular data, such as mutations or fusions?",
        "answer": "For mutation and fusion data, the function converts the data to a logical format. Mutations are transformed so that 'wt' (wild type) becomes FALSE, and any other value becomes TRUE. For fusions, '0' becomes FALSE, and any other value becomes TRUE. The function then uses 'and' or 'or' operations to summarize replicate experiments for these data types, instead of mean or median used for other data types."
      },
      {
        "question": "What is the purpose of the `binarize.threshold` and `binarize.direction` parameters in this function?",
        "answer": "The `binarize.threshold` and `binarize.direction` parameters are used to binarize continuous molecular data types such as CNV, RNA, RNAseq, or isoform data. If a `binarize.threshold` is provided (not NA), the function will convert the data to TRUE/FALSE based on this threshold. The `binarize.direction` parameter (either 'less' or 'greater') determines whether values less than or greater than the threshold are set to TRUE. This allows for flexible transformation of continuous data into binary format for specific analysis needs."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('summarizeMolecularProfiles', signature(object='PharmacoSet'),\n    function(object, mDataType, cell.lines, features,\n        summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n        fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n        binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n        removeTreated=TRUE)\n{\n    mDataTypes <- mDataNames(object)\n    if (!(mDataType %in% mDataTypes)) {\n      stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n    }\n\n    if(summarize==FALSE){\n      return(molecularProfilesSlot(object)[[mDataType]])\n    }\n\n    # Complete the rest of the function\n}",
        "complete": "setMethod('summarizeMolecularProfiles', signature(object='PharmacoSet'),\n    function(object, mDataType, cell.lines, features,\n        summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n        fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n        binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n        removeTreated=TRUE)\n{\n    mDataTypes <- mDataNames(object)\n    if (!(mDataType %in% mDataTypes)) {\n      stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n    }\n\n    if(summarize==FALSE){\n      return(molecularProfilesSlot(object)[[mDataType]])\n    }\n\n    if (missing(features)) {\n      features <- rownames(featureInfo(object, mDataType))\n    } else {\n      fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n      if (verbose && !all(fix)) {\n        warning (sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n      }\n      features <- features[fix]\n    }\n\n    summary.stat <- match.arg(summary.stat)\n    binarize.direction <- match.arg(binarize.direction)\n\n    if((!S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) {\n      stop (\"Invalid summary.stat, choose among: mean, median, first, last\" )\n    }\n    if((S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"and\", \"or\"))) {\n      stop (\"Invalid summary.stat, choose among: and, or\" )\n    }\n\n    if (missing(cell.lines)) {\n      cell.lines <- sampleNames(object)\n    }\n\n    # ... (rest of the function implementation)\n}"
      },
      {
        "partial": "# Function to summarize molecular profiles\nsummarizeMolecularProfiles <- function(object, mDataType, cell.lines, features,\n                                       summary.stat, fill.missing, summarize, verbose,\n                                       binarize.threshold, binarize.direction, removeTreated) {\n  # Implement the function body\n}",
        "complete": "# Function to summarize molecular profiles\nsummarizeMolecularProfiles <- function(object, mDataType, cell.lines, features,\n                                       summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                                       fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n                                       binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n                                       removeTreated = TRUE) {\n  mDataTypes <- mDataNames(object)\n  if (!(mDataType %in% mDataTypes)) {\n    stop(sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n  }\n\n  if (!summarize) return(molecularProfilesSlot(object)[[mDataType]])\n\n  features <- if (missing(features)) rownames(featureInfo(object, mDataType)) else {\n    fix <- features %in% rownames(featureInfo(object, mDataType))\n    if (verbose && !all(fix)) warning(sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n    features[fix]\n  }\n\n  summary.stat <- match.arg(summary.stat)\n  binarize.direction <- match.arg(binarize.direction)\n\n  annotation <- S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation\n  if (!annotation %in% c(\"mutation\", \"fusion\") && !summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\")) {\n    stop(\"Invalid summary.stat, choose among: mean, median, first, last\")\n  }\n  if (annotation %in% c(\"mutation\", \"fusion\") && !summary.stat %in% c(\"and\", \"or\")) {\n    stop(\"Invalid summary.stat, choose among: and, or\")\n  }\n\n  cell.lines <- if (missing(cell.lines)) sampleNames(object) else cell.lines\n\n  # ... (rest of the function implementation)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/unichem_helpers.R",
    "language": "R",
    "content": "#' Build a UniChem query URL\n#'\n#' This function builds a UniChem query URL based on the specified endpoint.\n#'\n#' @param endpoint The UniChem endpoint to query (valid options: \"compounds\", \"connectivity\", \"images\", \"sources\")\n#' @param query_only Logical indicating whether to return only the query URL without building it (default: FALSE)\n#'\n#' @return `httr2::httr2_url` object if `query_only` is TRUE, otherwise the built URL.\n#'\n#' @examples\n#' .build_unichem_query(\"sources\")\n#' .build_unichem_query(\"connectivity\", query_only = TRUE)\n#' \n#' @noRd\n#' @keywords internal\n.build_unichem_query <- function(\n    endpoint, query_only = FALSE\n) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_query\")\n\n    valid_endpoints <- c(\"compounds\", \"connectivity\", \"images\", \"sources\")\n    checkmate::assert_subset(endpoint, valid_endpoints)\n\n    unichem_api <- \"https://www.ebi.ac.uk/unichem/api/v1\"\n    url <- httr2::url_parse(unichem_api)\n    url$path <- .buildURL(url$path, endpoint)\n\n    output <- httr2::url_build(url)\n\n    .debug(funContext, \"URL: \", output )\n\n    if (query_only) return(url)\n    httr2::url_build(url) \n}\n\n\n#' Build a UniChem compound request\n#'\n#' This function builds a UniChem compound request based on the provided parameters.\n#'\n#' @param type The type of compound identifier to search for. Valid types are \"uci\", \"inchi\", \"inchikey\", and \"sourceID\".\n#' @param compound The compound identifier to search for.\n#' @param sourceID The source ID to search for if the type is \"sourceID\". Defaults to NULL.\n#' @param ... Additional arguments.\n#'\n#' @return A `httr2_request`  request object for the UniChem compound query.\n#'\n#' @examples\n#' .build_unichem_compound_req(type = \"uci\", compound = \"538323\")\n#' .build_unichem_compound_req(type = \"sourceID\", sourceID = 22, compound = \"2244\")\n#' \n#' @noRd\n#' @keywords internal\n.build_unichem_compound_req <- function(\n    type, compound, sourceID = NULL, ...\n){\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_compound_req\")\n\n    valid_types <- c(\"uci\", \"inchi\", \"inchikey\", \"sourceID\")\n    checkmate::assert_subset(type, valid_types)\n\n    base_url <- .build_unichem_query(\"compounds\")\n\n    .debug(funContext, \"Base URL: \", base_url)\n\n    body <- list(\n        type = type,\n        compound = compound\n    )\n\n    body$sourceID <- if (type == \"sourceID\") {\n        checkmate::assert_integerish(\n            x = sourceID,\n            lower = 1,\n            upper = max(getUnichemSources()$SourceID),\n            len = 1\n            )\n        sourceID\n    } else NULL\n\n\n    request <- base_url |> \n        .build_request() |>\n        httr2::req_body_json(body) \n\n    .debug(funContext, \"Request: \", request)\n    return(request)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_unichem_query` function and what are its parameters?",
        "answer": "The `.build_unichem_query` function builds a UniChem query URL based on a specified endpoint. It takes two parameters: `endpoint` (a string specifying the UniChem endpoint to query, with valid options being 'compounds', 'connectivity', 'images', or 'sources') and `query_only` (a logical value indicating whether to return only the query URL without building it, defaulting to FALSE)."
      },
      {
        "question": "In the `.build_unichem_compound_req` function, how is the `sourceID` parameter handled?",
        "answer": "The `sourceID` parameter is handled conditionally in the `.build_unichem_compound_req` function. If the `type` parameter is 'sourceID', the function checks that `sourceID` is an integer within a valid range (between 1 and the maximum SourceID from `getUnichemSources()`). If `type` is not 'sourceID', the `sourceID` is set to NULL in the request body."
      },
      {
        "question": "How does the `.build_unichem_compound_req` function construct and return the HTTP request?",
        "answer": "The `.build_unichem_compound_req` function constructs the HTTP request by first calling `.build_unichem_query('compounds')` to get the base URL. It then creates a request body with the `type` and `compound` parameters, and `sourceID` if applicable. Finally, it uses `httr2::req_body_json()` to add the JSON-encoded body to the request, and returns the resulting `httr2_request` object."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".build_unichem_query <- function(endpoint, query_only = FALSE) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_query\")\n    valid_endpoints <- c(\"compounds\", \"connectivity\", \"images\", \"sources\")\n    checkmate::assert_subset(endpoint, valid_endpoints)\n    unichem_api <- \"https://www.ebi.ac.uk/unichem/api/v1\"\n    url <- httr2::url_parse(unichem_api)\n    url$path <- .buildURL(url$path, endpoint)\n    output <- httr2::url_build(url)\n    .debug(funContext, \"URL: \", output)\n    if (query_only) return(url)\n    # Complete the function here\n}",
        "complete": ".build_unichem_query <- function(endpoint, query_only = FALSE) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_query\")\n    valid_endpoints <- c(\"compounds\", \"connectivity\", \"images\", \"sources\")\n    checkmate::assert_subset(endpoint, valid_endpoints)\n    unichem_api <- \"https://www.ebi.ac.uk/unichem/api/v1\"\n    url <- httr2::url_parse(unichem_api)\n    url$path <- .buildURL(url$path, endpoint)\n    output <- httr2::url_build(url)\n    .debug(funContext, \"URL: \", output)\n    if (query_only) return(url)\n    httr2::url_build(url)\n}"
      },
      {
        "partial": ".build_unichem_compound_req <- function(type, compound, sourceID = NULL, ...) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_compound_req\")\n    valid_types <- c(\"uci\", \"inchi\", \"inchikey\", \"sourceID\")\n    checkmate::assert_subset(type, valid_types)\n    base_url <- .build_unichem_query(\"compounds\")\n    .debug(funContext, \"Base URL: \", base_url)\n    body <- list(type = type, compound = compound)\n    # Complete the function here\n}",
        "complete": ".build_unichem_compound_req <- function(type, compound, sourceID = NULL, ...) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_compound_req\")\n    valid_types <- c(\"uci\", \"inchi\", \"inchikey\", \"sourceID\")\n    checkmate::assert_subset(type, valid_types)\n    base_url <- .build_unichem_query(\"compounds\")\n    .debug(funContext, \"Base URL: \", base_url)\n    body <- list(type = type, compound = compound)\n    body$sourceID <- if (type == \"sourceID\") {\n        checkmate::assert_integerish(\n            x = sourceID,\n            lower = 1,\n            upper = max(getUnichemSources()$SourceID),\n            len = 1\n        )\n        sourceID\n    } else NULL\n    request <- base_url |>\n        .build_request() |>\n        httr2::req_body_json(body)\n    .debug(funContext, \"Request: \", request)\n    return(request)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat.R",
    "language": "R",
    "content": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\nlibrary(AnnotationGx)\n\ntest_check(\"AnnotationGx\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of this code snippet in the context of R package development?",
        "answer": "This code snippet is typically found in the 'tests/testthat.R' file of an R package. Its purpose is to set up the testing environment for the package 'AnnotationGx' using the testthat framework. It loads necessary libraries (testthat, checkmate, and AnnotationGx) and then calls test_check() to run all the tests for the package."
      },
      {
        "question": "Why does the comment suggest not modifying this file, and where should additional test configuration be done?",
        "answer": "The comment advises against modifying this file because it's part of the standard setup for testthat. Additional test configuration should be done in separate test files within the 'tests/testthat/' directory. This separation helps maintain a clean and organized testing structure, keeping the main test runner file untouched while allowing developers to add specific tests in individual files."
      },
      {
        "question": "What is the significance of including the 'checkmate' library in this test setup?",
        "answer": "The inclusion of the 'checkmate' library in this test setup suggests that it's being used for additional assertion and validation in the package's tests. Checkmate provides a comprehensive set of assertion functions that can be used to validate function inputs and outputs, enhancing the robustness of the tests. Its presence indicates that the package developers are likely using these advanced assertion capabilities in their test suite for 'AnnotationGx'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_check(\"",
        "complete": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\nlibrary(AnnotationGx)\n\ntest_check(\"AnnotationGx\")"
      },
      {
        "partial": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\n",
        "complete": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\nlibrary(AnnotationGx)\n\ntest_check(\"AnnotationGx\")"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/src/rCPP_bridge.cpp",
    "language": "cpp",
    "content": "#include <Rcpp.h>\n\n\n//' QUICKSTOP significance testing for partial correlation\n//'\n//' This function will test whether the observed partial correlation is significant\n//' at a level of req_alpha, doing up to MaxIter permutations. Currently, it\n//' supports only grouping by discrete categories when calculating a partial correlation.\n//' Currenlty, only does two sided tests.\n//'\n//' @param pin_x one of the two vectors to correlate.\n//' @param pin_y the other vector to calculate\n//' @param pobsCor the observed (partial) correlation between these varaiables\n//' @param pGroupFactor an integer vector labeling group membership, to correct\n//' for in the partial correlation. NEEDS TO BE ZERO BASED!\n//' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n//' the number of members of each group (basically table(pGroupFactor)) as integer vector\n//' @param pnumGroup how many groups are there (len(pGroupSize))\n//' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n//' @param pn length of x and y, as a REAL NUMBER\n//' @param preq_alpha the required alpha for significance\n//' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n//' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n//' per quickstop paper (used to determine the log-odds)\n//' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n//' random number generator. Note that currently, these values get modified per call, so pass in a copy\n//' if you wish to keep a seed for running same simulation twice\n//'\n//' @return a double vector of length 4, entry 1 is either 0, 1 (for TRUE/FALSE) or NA_REAL_ for significance determination\n//' NA_REAL_ is returned when the MaxIter were reached before a decision is made. Usually, this occurs when the real p value is close to, or\n//' falls within the tolerance region of (req_alpha, req_alpha+tolerance_par). Entry 2 is the current p value estimate. entry 3 is the total\n//' number of iterations performed. Entry 4 is the number of time a permuted value was larger in absolute value than the observed cor.\n//'\n//' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP\n//'\n//'\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed);\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `partialCorQUICKSTOP` function in this Rcpp code?",
        "answer": "The `partialCorQUICKSTOP` function performs significance testing for partial correlation using the QUICKSTOP method. It tests whether the observed partial correlation is significant at a given alpha level, performing up to a maximum number of permutations. The function supports grouping by discrete categories when calculating partial correlation and currently only performs two-sided tests."
      },
      {
        "question": "What are the key input parameters for the `partialCorQUICKSTOP` function and their purposes?",
        "answer": "The key input parameters include: `pin_x` and `pin_y` (vectors to correlate), `pobsCor` (observed partial correlation), `pGroupFactor` (group membership labels), `pGroupSize` (count of members in each group), `pMaxIter` (maximum number of iterations), `preq_alpha` (required alpha for significance), `ptolerance_par` (tolerance region for quickstop), and `pseed` (seed for random number generator). These parameters allow the function to perform the partial correlation analysis with specific constraints and settings."
      },
      {
        "question": "What does the function return, and how should the output be interpreted?",
        "answer": "The function returns a double vector of length 4. The first entry is 0, 1, or NA_REAL_ for the significance determination (FALSE, TRUE, or undetermined). NA_REAL_ is returned when MaxIter is reached before a decision. The second entry is the current p-value estimate. The third entry is the total number of iterations performed. The fourth entry is the number of times a permuted value was larger in absolute value than the observed correlation. This output provides both the significance result and additional information about the testing process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#include <Rcpp.h>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    // Implementation here\n}",
        "complete": "#include <Rcpp.h>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    Rcpp::NumericVector x(pin_x);\n    Rcpp::NumericVector y(pin_y);\n    double obsCor = Rcpp::as<double>(pobsCor);\n    Rcpp::IntegerVector groupFactor(pGroupFactor);\n    Rcpp::IntegerVector groupSize(pGroupSize);\n    int numGroup = Rcpp::as<int>(pnumGroup);\n    double maxIter = Rcpp::as<double>(pMaxIter);\n    double n = Rcpp::as<double>(pn);\n    double req_alpha = Rcpp::as<double>(preq_alpha);\n    double tolerance_par = Rcpp::as<double>(ptolerance_par);\n    double log_decision_boundary = Rcpp::as<double>(plog_decision_boundary);\n    Rcpp::NumericVector seed(pseed);\n\n    // Implement QUICKSTOP algorithm here\n    // ...\n\n    Rcpp::NumericVector result(4);\n    // Set result values\n    // ...\n\n    return result;\n}"
      },
      {
        "partial": "#include <Rcpp.h>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    // Convert input parameters\n    // ...\n\n    // Implement QUICKSTOP algorithm\n    double pValue = 0.0;\n    int iterations = 0;\n    int exceedCount = 0;\n\n    // Main loop\n    while (iterations < maxIter) {\n        // Permutation and correlation calculation\n        // ...\n\n        // Update pValue and exceedCount\n        // ...\n\n        // Check for early stopping condition\n        // ...\n\n        iterations++;\n    }\n\n    // Prepare and return result\n    // ...\n}",
        "complete": "#include <Rcpp.h>\n#include <random>\n#include <algorithm>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    Rcpp::NumericVector x(pin_x), y(pin_y);\n    double obsCor = Rcpp::as<double>(pobsCor);\n    Rcpp::IntegerVector groupFactor(pGroupFactor), groupSize(pGroupSize);\n    int numGroup = Rcpp::as<int>(pnumGroup);\n    double maxIter = Rcpp::as<double>(pMaxIter);\n    double n = Rcpp::as<double>(pn);\n    double req_alpha = Rcpp::as<double>(preq_alpha);\n    double tolerance_par = Rcpp::as<double>(ptolerance_par);\n    double log_decision_boundary = Rcpp::as<double>(plog_decision_boundary);\n    Rcpp::NumericVector seed(pseed);\n\n    std::mt19937 gen(seed[0]);\n    double pValue = 0.0;\n    int iterations = 0;\n    int exceedCount = 0;\n\n    while (iterations < maxIter) {\n        std::vector<double> permuted_x = Rcpp::as<std::vector<double>>(x);\n        for (int i = 0; i < numGroup; ++i) {\n            std::shuffle(permuted_x.begin() + groupFactor[i], \n                         permuted_x.begin() + groupFactor[i] + groupSize[i], gen);\n        }\n\n        double permCor = calculatePartialCorrelation(permuted_x, y, groupFactor);\n        if (std::abs(permCor) >= std::abs(obsCor)) exceedCount++;\n\n        pValue = (exceedCount + 1.0) / (iterations + 1.0);\n        iterations++;\n\n        if (pValue <= req_alpha - tolerance_par || pValue > req_alpha + tolerance_par) {\n            if (std::log(pValue / (1 - pValue)) <= -log_decision_boundary) break;\n        }\n    }\n\n    Rcpp::NumericVector result = Rcpp::NumericVector::create(\n        (pValue <= req_alpha) ? 1.0 : (iterations == (int)maxIter ? NA_REAL : 0.0),\n        pValue, iterations, exceedCount\n    );\n    return result;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/mergePSets.R",
    "language": "R",
    "content": "#' @include PharmacoSet-class.R\nNULL\n\nmergePSets <- function(mDataPSet, sensDataPSet, commonCellsOnly=FALSE, ...){\n\n\tif(commonCellsOnly){\n\t\tcommon <- intersectPSet(list(mDataPSet, sensDataPSet), intersectOn=c(\"celllines\"))\n\t\tmDataPSet <- common[[1]]\n\t\tsensDataPSet <- common[[2]]\n\t}\n\n  ### union of cell lines\n\tucell <- union(sampleNames(sensDataPSet), sampleNames(mDataPSet))\n\n  ### molecular profiles\n\tmergePSet <- sensDataPSet\n\tmolecularProfilesSlot(mergePSet) <- molecularProfilesSlot(mDataPSet)\n\n  ### number of sensitivity experiments\n  #acell <- setdiff(rownames(sampleInfo(mDataPSet)), rownames(sampleInfo(sensDataPSet)))\n\n\t### cell line annotations\n\tnew.cell.info <- c(union(colnames(sampleInfo(sensDataPSet)), colnames(sampleInfo(mDataPSet))), \"dataset\")\n\tcell.info.df <- data.frame(matrix(NA, nrow=length(ucell), ncol=length(new.cell.info), dimnames=list(ucell, new.cell.info)), check.names=FALSE)\n\tcell.info.df[rownames(sampleInfo(mDataPSet)), colnames(sampleInfo(mDataPSet))] <- sampleInfo(mDataPSet)\n\tcell.info.df[rownames(sampleInfo(sensDataPSet)), colnames(sampleInfo(sensDataPSet))] <- sampleInfo(sensDataPSet)\n\tcell.info.df[setdiff(rownames(sampleInfo(sensDataPSet)), rownames(sampleInfo(mDataPSet))), \"dataset\"] <- name(sensDataPSet)\n\tcell.info.df[setdiff(rownames(sampleInfo(mDataPSet)), rownames(sampleInfo(sensDataPSet))), \"dataset\"] <- name(mDataPSet)\n\tcell.info.df[intersect(rownames(sampleInfo(mDataPSet)), rownames(sampleInfo(sensDataPSet))), \"dataset\"] <- paste0(name(sensDataPSet), \"///\", name(mDataPSet))\n\tsampleInfo(mergePSet) <- cell.info.df\n\n\t### curation of cell line names\n\tnew.cell.curation <- c(union(colnames(curation(sensDataPSet)$sample), colnames(curation(mDataPSet)$sample)), \"dataset\")\n\tcell.curation.df <- data.frame(matrix(NA, nrow=length(ucell), ncol=length(new.cell.curation), dimnames=list(ucell, new.cell.curation)), check.names=FALSE)\n\tcell.curation.df[rownames(curation(mDataPSet)$sample), colnames(curation(mDataPSet)$sample)] <- curation(mDataPSet)$sample\n\tcell.curation.df[rownames(curation(sensDataPSet)$sample), colnames(curation(sensDataPSet)$sample)] <- curation(sensDataPSet)$sample\n\tcell.curation.df[setdiff(rownames(curation(sensDataPSet)$sample), rownames(curation(mDataPSet)$sample)), \"dataset\"] <- name(sensDataPSet)\n\tcell.curation.df[setdiff(rownames(curation(mDataPSet)$sample), rownames(curation(sensDataPSet)$sample)), \"dataset\"] <- name(mDataPSet)\n\tcell.curation.df[intersect(rownames(curation(mDataPSet)$sample), rownames(curation(sensDataPSet)$sample)), \"dataset\"] <- paste0(name(sensDataPSet), \"///\", name(mDataPSet))\n\tcuration(mergePSet)$sample <- cell.curation.df\n\n\t### curation of tissue names\n\tnew.tissue.curation <- c(union(colnames(curation(sensDataPSet)$tissue), colnames(curation(mDataPSet)$tissue)), \"dataset\")\n\ttissue.curation.df <- data.frame(matrix(NA, nrow=length(ucell), ncol=length(new.tissue.curation), dimnames=list(ucell, new.tissue.curation)), check.names=FALSE)\n\ttissue.curation.df[rownames(curation(mDataPSet)$tissue), colnames(curation(mDataPSet)$tissue)] <- curation(mDataPSet)$tissue\n\ttissue.curation.df[rownames(curation(sensDataPSet)$tissue), colnames(curation(sensDataPSet)$tissue)] <- curation(sensDataPSet)$tissue\n\ttissue.curation.df[setdiff(rownames(curation(sensDataPSet)$tissue), rownames(curation(mDataPSet)$tissue)), \"dataset\"] <- name(sensDataPSet)\n\ttissue.curation.df[setdiff(rownames(curation(mDataPSet)$tissue), rownames(curation(sensDataPSet)$tissue)), \"dataset\"] <- name(mDataPSet)\n\ttissue.curation.df[intersect(rownames(curation(mDataPSet)$tissue), rownames(curation(sensDataPSet)$tissue)), \"dataset\"] <- paste0(name(sensDataPSet), \"///\", name(mDataPSet))\n\tcuration(mergePSet)$tissue <- tissue.curation.df\n\n\tsensNumber(mergePSet) <- PharmacoGx:::.summarizeSensitivityNumbers(mergePSet)\n\n\tannotation(mergePSet)$name <- paste(name(mDataPSet), name(sensDataPSet), sep=\".\")\n\n\tannotation(mergePSet)$dateCreated <- date()\n\n\tcheckPSetStructure(mergePSet)\n\n    return(mergePSet)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mergePSets` function and what are its main input parameters?",
        "answer": "The `mergePSets` function is designed to merge two PharmacoSet objects: one containing molecular data (mDataPSet) and another containing sensitivity data (sensDataPSet). The main input parameters are:\n1. mDataPSet: A PharmacoSet object with molecular data\n2. sensDataPSet: A PharmacoSet object with sensitivity data\n3. commonCellsOnly: A boolean parameter to determine if only common cell lines should be kept (default is FALSE)\n\nThe function combines the molecular profiles from mDataPSet with the sensitivity data from sensDataPSet, merging cell line annotations, curations, and other relevant information."
      },
      {
        "question": "How does the function handle the merging of cell line information when there are differences between the two input PharmacoSet objects?",
        "answer": "The function handles merging cell line information as follows:\n1. It creates a union of cell lines from both input PharmacoSets.\n2. For sample information, it combines columns from both inputs into a new data frame.\n3. It fills in values from both input PharmacoSets, giving priority to sensDataPSet when there's an overlap.\n4. A new 'dataset' column is added to track the origin of each cell line:\n   - Unique to sensDataPSet: labeled with sensDataPSet's name\n   - Unique to mDataPSet: labeled with mDataPSet's name\n   - Common to both: labeled with both names separated by '///'\n5. This process is repeated for both sample information and curation data (sample and tissue).\n\nThis approach ensures that all cell line information is preserved and the source of each piece of data is traceable."
      },
      {
        "question": "What steps does the function take to ensure the integrity and completeness of the merged PharmacoSet object?",
        "answer": "The function takes several steps to ensure the integrity and completeness of the merged PharmacoSet object:\n1. It uses the `union` function to ensure all unique cell lines from both input sets are included.\n2. It merges molecular profiles, sample information, and curation data comprehensively.\n3. It handles potential conflicts in cell line data by preserving information from both input sets and adding a 'dataset' column to track the origin.\n4. It updates the sensitivity numbers using the `.summarizeSensitivityNumbers` function.\n5. It creates a new name for the merged set by combining names of input sets.\n6. It adds a 'dateCreated' annotation to record when the merge occurred.\n7. Finally, it calls `checkPSetStructure` to verify that the resulting merged PharmacoSet object has a valid structure.\n\nThese steps ensure that the merged object contains all necessary data, maintains traceability, and adheres to the expected PharmacoSet structure."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_view_helpers.R",
    "language": "R",
    "content": "#' Get all available annotation headings\n#'\n#' https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON will return a list of all available headings\n#'\n#' @keywords internal\n#' @noRd\n.get_all_heading_types_base <- function() {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON\"\n  req <- .build_pubchem_request(url)\n  response <- httr2::req_perform(req) |> .parse_resp_json()\n  .asDT(response[[1]][[1]])\n}\n\n#' @keywords internal\n.get_all_heading_types <- memoise::memoise(.get_all_heading_types_base)\n\n#' Build a PubChem REST query URL\n#'\n#' pubchem<DOT>ncbi<DOT>nlm<DOT>nih<DOT>gov/rest/pug_view/<ANNOTATION>/<RECORD>/<ID>/<OUTPUT><?OPTIONS>\n#'\n#' Optional Parameters:\n#'   - annotation: data, index, annotations, categories, neighbors, literature, structure, image, qr, linkout\n#'  - record: compound, substance, assay, cell, gene, protein\n#' - filter (Parameter, Value, Description):\n#'      - Page, n <integer>, The page number to retrieve. Retrieves the nth page of the output data when the data is paginated.\n#'          This option is useful when downloading the annotations under a specific heading for all compounds\n#'          By default, retrieve all pages of the output data.\n#'      - Version, n <int> for substance | n.m <int.int> for assay, data for a particular version of a substance or assay record\n#'      - Heading, (Sub)heading name, The name of the annotation heading or subheading to retrieve\n#'      - Source, source name, The name of the annotation source to retrieve from a specified source\n#'\n#' @param id The identifier for the query.\n#' @param annotation The type of annotation to retrieve. Options include \"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", or \"linkout\".\n#' @param record The type of record to retrieve. Options include \"compound\", \"substance\", \"assay\", \"cell\", \"gene\", or \"protein\".\n#' @param page The page number to retrieve. Retrieves the nth page of the output data when the data is paginated. By default, retrieve all pages of the output data.\n#' @param version The version of the record to retrieve. For substance, this is an integer. For assay, this is a string in the format \"n.m\".\n#' @param heading The name of the annotation heading or subheading to retrieve.\n#' @param source The name of the annotation source to retrieve from a specified source.\n#' @param output The desired output format. Options are \"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\".\n#'\n#' @return The query URL\n#'\n#' @keywords internal\n#' @noRd\n.build_pubchem_view_query <- function(\n    id, annotation = \"data\", record = \"compound\", \n    page = NULL, version = NULL, heading = NULL, source = NULL,\n    output = \"JSON\", ...\n) {\n  funContext <- .funContext(\".build_pubchem_view_query\")\n\n  # Check the inputs\n  checkmate::assert_choice(\n    annotation,\n    c(\"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", \"linkout\")\n  )\n  checkmate::assert_choice(record, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n\n  # Configure the options for the query\n  opts_ <- list()\n  if (!is.null(heading)) {\n    if (record == \"substance\") {\n      .debug(\n        funContext,\n        \" fyi: https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON\n                 has no substance headings\"\n      )\n    } else {\n      checkmate::assert(heading %in% .get_all_heading_types()$Heading)\n    }\n    opts_ <- c(opts_, list(heading = heading))\n  }\n  if (!is.null(version)) {\n    if (record == \"substance\") {\n      checkmate::assert_string(version, min.chars = 1)\n    } else {\n      checkmate::assert_numeric(version, lower = 1)\n    }\n    opts_ <- c(opts_, list(version = version))\n  }\n\n  if (!is.null(source)) {\n    checkmate::assert_string(source, min.chars = 1)\n    opts_ <- c(opts_, list(source = source))\n  }\n\n  if (!is.null(page)) {\n    checkmate::assert_numeric(page, lower = 1)\n    opts_ <- c(opts_, list(page = page))\n  }\n\n  base_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, annotation, record, id, output)\n  url$query <- opts_\n\n  url |>\n    httr2::url_build() |>\n    httr2::request()\n  \n  # url |>\n    # httr2::url_build() |>\n    # .build_request()\n}\n\n#' Generic function to parse one of the annotation helpers\n#'\n#' @noRd\n#' @keywords internal\n.clean_parsed_annotation <- function(result) {\n  # If returned value is a list, concatenate the elements into a single string with \";\"\n  if (length(result) > 1) {\n    return(paste(result, collapse = \"; \"))\n  }\n  return(result)\n}\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseCHEMBLresponse <- function(result) {\n  gsub(\"Compound::\", \"\", result[[\"Record\"]][[\"Reference\"]][[\"SourceID\"]]) |>\n    .clean_parsed_annotation()\n}\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseCASresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"CAS Common Chemistry\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseNSCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"DTP/NCI\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseATCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"WHO Anatomical Therapeutic Chemical (ATC) Classification\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseDILIresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"Drug Induced Liver Injury Rank (DILIrank) Dataset\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.get_all_heading_types_base` function and how is it used in conjunction with `.get_all_heading_types`?",
        "answer": "The `.get_all_heading_types_base` function retrieves all available annotation headings from the PubChem API. It sends a request to the specified URL, parses the JSON response, and returns the result as a data table. The `.get_all_heading_types` function is a memoized version of `.get_all_heading_types_base`, which caches the results for improved performance on subsequent calls."
      },
      {
        "question": "In the `.build_pubchem_view_query` function, what are the key parameters and how are they validated?",
        "answer": "The key parameters for `.build_pubchem_view_query` include `id`, `annotation`, `record`, `page`, `version`, `heading`, `source`, and `output`. The function validates these parameters using `checkmate::assert_choice` for `annotation` and `record`, ensuring they match predefined options. It also checks if the `heading` is valid by comparing it against the list of available headings retrieved from `.get_all_heading_types()`. The `version` parameter is validated differently for 'substance' and other record types."
      },
      {
        "question": "How does the `.clean_parsed_annotation` function handle different types of input, and where is it used in the code snippet?",
        "answer": "The `.clean_parsed_annotation` function is designed to handle both single values and lists. If the input `result` has a length greater than 1, it concatenates the elements into a single string using semicolons as separators. Otherwise, it returns the single value as-is. This function is used in various parsing functions like `.parseCHEMBLresponse`, `.parseCASresponse`, `.parseNSCresponse`, `.parseATCresponse`, and `.parseDILIresponse` to standardize the output format of parsed annotations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Partial code for .build_pubchem_view_query function\n.build_pubchem_view_query <- function(\n    id, annotation = \"data\", record = \"compound\", \n    page = NULL, version = NULL, heading = NULL, source = NULL,\n    output = \"JSON\", ...\n) {\n  funContext <- .funContext(\".build_pubchem_view_query\")\n\n  # Check the inputs\n  checkmate::assert_choice(\n    annotation,\n    c(\"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", \"linkout\")\n  )\n  checkmate::assert_choice(record, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n\n  # Configure the options for the query\n  opts_ <- list()\n  # Add code here to handle options\n\n  base_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view\"\n  url <- httr2::url_parse(base_url)\n  # Complete the URL construction\n\n  # Return the request\n}",
        "complete": "# Complete code for .build_pubchem_view_query function\n.build_pubchem_view_query <- function(\n    id, annotation = \"data\", record = \"compound\", \n    page = NULL, version = NULL, heading = NULL, source = NULL,\n    output = \"JSON\", ...\n) {\n  funContext <- .funContext(\".build_pubchem_view_query\")\n\n  # Check the inputs\n  checkmate::assert_choice(\n    annotation,\n    c(\"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", \"linkout\")\n  )\n  checkmate::assert_choice(record, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n\n  # Configure the options for the query\n  opts_ <- list()\n  if (!is.null(heading)) {\n    if (record == \"substance\") {\n      .debug(funContext, \" fyi: https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON has no substance headings\")\n    } else {\n      checkmate::assert(heading %in% .get_all_heading_types()$Heading)\n    }\n    opts_ <- c(opts_, list(heading = heading))\n  }\n  if (!is.null(version)) {\n    if (record == \"substance\") {\n      checkmate::assert_string(version, min.chars = 1)\n    } else {\n      checkmate::assert_numeric(version, lower = 1)\n    }\n    opts_ <- c(opts_, list(version = version))\n  }\n  if (!is.null(source)) {\n    checkmate::assert_string(source, min.chars = 1)\n    opts_ <- c(opts_, list(source = source))\n  }\n  if (!is.null(page)) {\n    checkmate::assert_numeric(page, lower = 1)\n    opts_ <- c(opts_, list(page = page))\n  }\n\n  base_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, annotation, record, id, output)\n  url$query <- opts_\n\n  url |>\n    httr2::url_build() |>\n    httr2::request()\n}"
      },
      {
        "partial": "# Partial code for parsing functions\n.clean_parsed_annotation <- function(result) {\n  # Implement the function\n}\n\n.parseCHEMBLresponse <- function(result) {\n  # Implement the function\n}\n\n.parseCASresponse <- function(result) {\n  # Implement the function\n}\n\n.parseNSCresponse <- function(result) {\n  # Implement the function\n}\n\n.parseATCresponse <- function(result) {\n  # Implement the function\n}\n\n.parseDILIresponse <- function(result) {\n  # Implement the function\n}",
        "complete": "# Complete code for parsing functions\n.clean_parsed_annotation <- function(result) {\n  if (length(result) > 1) {\n    return(paste(result, collapse = \"; \"))\n  }\n  return(result)\n}\n\n.parseCHEMBLresponse <- function(result) {\n  gsub(\"Compound::\", \"\", result[[\"Record\"]][[\"Reference\"]][[\"SourceID\"]]) |>\n    .clean_parsed_annotation()\n}\n\n.parseCASresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"CAS Common Chemistry\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n.parseNSCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"DTP/NCI\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n.parseATCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"WHO Anatomical Therapeutic Chemical (ATC) Classification\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n.parseDILIresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"Drug Induced Liver Injury Rank (DILIrank) Dataset\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/ops/ops.py",
    "language": "py",
    "content": "from typing import List, TypeVar, Sequence, Union, Tuple, Optional, Any\n\nimport numpy as np\nimport SimpleITK as sitk\n\nfrom .functional import *\nfrom ..io import *\nfrom ..utils import image_to_array, array_to_image, crawl, physical_points_to_idxs\nfrom ..modules import map_over_labels\nfrom ..modules import DataGraph\n\n\nLoaderFunction = TypeVar('LoaderFunction')\nImageFilter = TypeVar('ImageFilter')\nFunction = TypeVar('Function')\nStructureSet = TypeVar('StructureSet')\n\n\n# Base class\nclass BaseOp:\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def __repr__(self):\n        attrs = [(k, v) for k, v in self.__dict__.items()\n                 if not k.startswith(\"_\")]\n        attrs = [(k, f\"'{v}'\") if isinstance(v, str) else (k, v) for k, v in attrs]\n        args = \", \".join(f\"{k}={v}\" for k, v in attrs)\n        return f\"{self.__class__.__name__}({args})\"\n\n\n# Input/output\nclass BaseInput(BaseOp):\n    def __init__(self, loader):\n        if not isinstance(loader, BaseLoader):\n            raise ValueError(\n                f\"loader must be a subclass of io.BaseLoader, got {type(loader)}\"\n            )\n        self._loader = loader\n\n    def __call__(self, key):\n        inputs = self._loader.get(key)\n        return inputs\n\n\nclass BaseOutput(BaseOp):\n    def __init__(self, writer):\n        if not isinstance(writer, BaseWriter):\n            raise ValueError(\n                f\"writer must be a subclass of io.BaseWriter, got {type(writer)}\"\n            )\n        self._writer = writer\n\n    def __call__(self, key, *args, **kwargs):\n        self._writer.put(key, *args, **kwargs)\n\n\nclass BetaAutoInput(BaseInput):\n    \"\"\"ImageAutoInput class is a wrapper class around ImgCSVloader which looks for the specified directory and crawls through it as the first step. Using the crawled output data, a graph on modalties present in the dataset is formed\n    which stores the relation between all the modalities.\n    Based on the user provided modalities, this class loads the information of the user provided modalities\n\n    Parameters\n    ----------\n    dir_path: str\n        Path to dataset top-level directory. The crawler/indexer will start at this directory.\n\n    modalities: str\n        List of modalities to process. Only samples with ALL modalities will be processed. Make sure there are no space between list elements as it is parsed as a string.\n\n    visualize: bool\n        Whether to return visualization of the data graph\n\n    update: bool\n        Whether to update crawled index\n    \"\"\"\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        # To be changed later\n        df_crawl_path   = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        tree_crawl_path = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.json\").as_posix()\n\n        if not os.path.exists(df_crawl_path) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs = n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        import json\n        with open(tree_crawl_path, 'r') as f:\n            tree_db = json.load(f)  # currently unused, TO BE implemented in the future\n            assert tree_db is not None, \"There was no crawler output\" # dodging linter\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent, \".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=df_crawl_path, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [(\"_\").join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.study_names  = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"study\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        self.subseries_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"subseries\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]\n\n        loader = ImageCSVLoader(self.df_combined,\n                                col_names=self.column_names,\n                                study_names=self.study_names,\n                                series_names=self.series_names,\n                                subseries_names=self.subseries_names,\n                                id_column=None,\n                                expand_paths=False,\n                                readers=self.readers)\n        \n        super().__init__(loader)\n\n\nclass ImageAutoInput(BaseInput):\n    \"\"\"ImageAutoInput class is a wrapper class around ImgCSVloader which looks for the specified directory and crawls through it as the first step. Using the crawled output data, a graph on modalties present in the dataset is formed\n    which stores the relation between all the modalities.\n    Based on the user provided modalities, this class loads the information of the user provided modalities\n\n    Parameters\n    ----------\n    dir_path: str\n        Path to dataset top-level directory. The crawler/indexer will start at this directory.\n\n    modalities: str\n        List of modalities to process. Only samples with ALL modalities will be processed. Make sure there are no space between list elements as it is parsed as a string.\n\n    visualize: bool\n        Whether to return visualization of the data graph\n\n    update: bool\n        Whether to update crawled index\n    \"\"\"\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        # To be changed later\n        path_crawl = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        if not os.path.exists(path_crawl) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs=n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent,\".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=path_crawl, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [(\"_\").join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]\n\n        loader = ImageCSVLoader(self.df_combined,\n                                colnames=self.column_names,\n                                seriesnames=self.series_names,\n                                id_column=None,\n                                expand_paths=False,\n                                readers=self.readers)\n        \n        super().__init__(loader)\n\n\nclass ImageCSVInput(BaseInput):\n    \"\"\"ImageCSVInput class looks for a CSV file in a specified directory and loads images based on the information in the file.\n\n    Parameters\n    ----------\n    csv_path: str\n        Directory where the CSV file is stored.\n\n    colnames: List[str]\n        List of column names in the CSV file to be used for image loading.\n\n    id_column: str, optional\n        A column name to be used as the subject ID.\n\n    reader: LoaderFunction\n        The functions used to read images.\n        The functions are implemented in the imgtools.io module.\n        The options are:\n        - read_image\n        - read_dicom_series\n        - read_dicom_rtstruct\n        - read_segmentation\n    \"\"\"\n    def __init__(self,\n                 csv_path_or_dataframe: str,\n                 colnames: List[str] = None,\n                 id_column: Optional[str] = None,\n                 expand_paths: bool = True,\n                 readers: List[LoaderFunction] = None):  # no mutable defaults: https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n        if colnames is None:\n            colnames = []\n        if readers is None:\n            readers = [read_image]\n        self.csv_path_or_dataframe = csv_path_or_dataframe\n        self.colnames = colnames\n        self.id_column = id_column\n        self.expand_paths = expand_paths\n        self.readers = readers\n        loader = ImageCSVLoader(self.csv_path_or_dataframe,\n                                colnames=self.colnames,\n                                id_column=self.id_column,\n                                expand_paths=self.expand_paths,\n                                readers=self.readers)\n        super().__init__(loader)\n\n\nclass ImageFileInput(BaseInput):\n    \"\"\"ImageFileInput class looks for images in a specified directory and reads them by using a reader function.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the image files are stored.\n\n    get_subject_id_from: str\n        How to extract the subject ID. The options are:\n        - 'filename' indicates the image reader to use filename as the subject ID. (default)\n        - 'subject_directory' indicates the image reader to use the name of the subject directory.\n\n    subdir_path: str, optional\n        Path to a subdirectory where the image is stored.\n\n    exclude_paths: List[str], optional\n        Directory paths to be excluded when searching for the image files to be loaded.\n\n    reader: LoaderFunction\n        The function used to read individual images.\n        The functions are implemented in the imgtools.io module.\n        The options are:\n        - read_image\n        - read_dicom_series\n        - read_dicom_rtstruct\n        - read_segmentation\n        - read_dicom_auto\n        - read_dicom_rtdose\n        - read_dicom_pet\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 get_subject_id_from: str = \"filename\",\n                 subdir_path: Optional[str] = None,\n                 exclude_paths: Optional[List[str]] = None,  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n                 reader: LoaderFunction = None):\n        if exclude_paths is None:\n            exclude_paths = []\n        if reader is None:\n            reader = read_image\n        self.root_directory = root_directory\n        self.get_subject_id_from = get_subject_id_from\n        self.subdir_path = subdir_path\n        self.exclude_paths = exclude_paths\n        self.reader = reader\n        loader = ImageFileLoader(self.root_directory,\n                                 self.get_subject_id_from,\n                                 self.subdir_path,\n                                 self.exclude_paths,\n                                 self.reader)\n        super().__init__(loader)\n\n\nclass ImageFileOutput(BaseOutput):\n    \"\"\"ImageFileOutput class outputs processed images as one of the image file formats.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed image files will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.nrrd as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    compress: bool, optional\n        Specify whether to enable compression for NRRD format.\n        Set to be true as default.\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.nrrd\",\n                 create_dirs: Optional[bool] =True,\n                 compress: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        \n        if \".seg\" in filename_format:  # from .seg.nrrd bc it is now .nii.gz\n            writer_class = SegNrrdWriter\n        else:\n            writer_class = ImageFileWriter\n            \n        writer = writer_class(self.root_directory,\n                              self.filename_format,\n                              self.create_dirs,\n                              self.compress)\n        \n        super().__init__(writer)\n\n\n# Resampling ops\nclass ImageSubjectFileOutput(BaseOutput):\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] = \"{subject_id}.nii.gz\",\n                 create_dirs: Optional[bool] = True,\n                 compress: Optional[bool] = True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n\n        writer = BaseSubjectWriter(self.root_directory,\n                                   self.filename_format,\n                                   self.create_dirs,\n                                   self.compress)\n        \n        super().__init__(writer)\n\n\nclass ImageAutoOutput:\n    \"\"\"\n    Wrapper class around ImageFileOutput. This class supports multiple modalities writers and calls ImageFileOutput for writing the files\n    \n    Parameters\n    ----------\n    root_directory: str\n        The directory where all the processed files will be stored in the form of nrrd\n\n    output_streams: List[str]\n        The modalties that should be stored. This is typically equal to the column names of the table returned after graph querying. Examples is provided in the \n        dictionary file_name\n    \"\"\"\n    def __init__(self,\n                 root_directory: str,\n                 output_streams: List[str],\n                 nnunet_info: Dict = None,\n                 inference: bool = False,):\n                 \n        self.output = {}\n        for colname in output_streams:\n            # Not considering colnames ending with alphanumeric\n            colname_process = (\"_\").join([item for item in colname.split(\"_\") if not item.isnumeric()])\n            colname_process = colname  # temproary force #\n            if not nnunet_info and not inference:\n                self.output[colname_process] = ImageSubjectFileOutput(pathlib.Path(root_directory,\"{subject_id}\",colname_process.split(\".\")[0]).as_posix(),\n                                                                      filename_format=\"{}.nii.gz\".format(colname_process))\n            elif inference:\n                self.output[colname_process] = ImageSubjectFileOutput(root_directory,\n                                                                      filename_format=\"{subject_id}_{modality_index}.nii.gz\")\n            else:\n                self.output[colname_process] = ImageSubjectFileOutput(pathlib.Path(root_directory,\"{label_or_image}{train_or_test}\").as_posix(),\n                                                                      filename_format=\"{subject_id}_{modality_index}.nii.gz\")\n    \n    def __call__(self, \n                 subject_id: str,\n                 img: sitk.Image,\n                 output_stream,\n                 is_mask: bool = False,\n                 mask_label: Optional[str] = \"\",\n                 label_or_image: str=\"images\",\n                 train_or_test: str=\"Tr\",\n                 nnunet_info: Dict=None):\n                 \n        self.output[output_stream](subject_id, img, is_mask=is_mask, mask_label=mask_label, label_or_image=label_or_image, train_or_test=train_or_test, nnunet_info=nnunet_info)\n\n\nclass NumpyOutput(BaseOutput):\n    \"\"\"NumpyOutput class processed images as NumPy files.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed NumPy files will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.npy as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.npy\",\n                 create_dirs: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        writer = NumpyWriter(self.root_directory, self.filename_format, self.create_dirs)\n        super().__init__(writer)\n\n\nclass HDF5Output(BaseOutput):\n    \"\"\"HDF5Output class outputs the processed image data in HDF5 format.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed .h5 file will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.h5 as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    save_geometry: bool, optional\n        Specify whether to save geometry data.\n        Set to be True as default.\n\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.h5\",\n                 create_dirs: Optional[bool] =True,\n                 save_geometry: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.save_geometry = save_geometry\n        writer = HDF5Writer(self.root_directory,\n                            self.filename_format,\n                            self.create_dirs,\n                            self.save_geometry)\n        super().__init__(writer)\n\n\nclass MetadataOutput(BaseOutput):\n    \"\"\"MetadataOutput class outputs the metadata of processed image files in .json format.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed .json file will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.json as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.json\",\n                 create_dirs: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        writer = MetadataWriter(self.root_directory, self.filename_format, self.create_dirs)\n        super().__init__(writer)\n\n\n# Resampling ops\nclass Resample(BaseOp):\n    \"\"\"Resample operation class:\n    A callable class that resamples image to a given spacing, optionally applying a transformation.\n\n    To instantiate:\n        obj = Resample(spacing, interpolation, anti_alias, anti_alias_sigma, transform, output_size)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    spacing\n        The new image spacing. If float, assumes the same spacing in all\n        directions. Alternatively, a sequence of floats can be passed to\n        specify spacing along each dimension. Passing 0 at any position will\n        keep the original spacing along that dimension (useful for in-plane\n        resampling).\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `spacing < image.GetSpacing()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n    transform, optional\n        Transform to apply to input coordinates when resampling. If None,\n        defaults to identity transformation.\n\n    output_size, optional\n        Size of the output image. If None, it is computed to preserve the\n        whole extent of the input image.\n    \"\"\"\n\n    def __init__(self,\n                 spacing: Union[float, Sequence[float], np.ndarray],\n                 interpolation: str =\"linear\",\n                 anti_alias: bool =True,\n                 anti_alias_sigma: Optional[float] =None,\n                 transform: Optional[sitk.Transform] =None,\n                 output_size: Optional[Sequence[float]] =None):\n        self.spacing = spacing\n        self.interpolation = interpolation\n        self.anti_alias = anti_alias\n        self.anti_alias_sigma = anti_alias_sigma\n        self.transform = transform\n        self.output_size = output_size\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Resample callable object:\n        Resamples image to a given spacing, optionally applying a transformation..\n\n        Parameters\n        ----------\n        image\n            The image to resample.\n\n        Returns\n        -------\n        sitk.Image\n            The resampled image.\n        \"\"\"\n\n        return resample(image,\n                        spacing=self.spacing,\n                        interpolation=self.interpolation,\n                        anti_alias=self.anti_alias,\n                        anti_alias_sigma=self.anti_alias_sigma,\n                        transform=self.transform,\n                        output_size=self.output_size)\n\n\nclass Resize(BaseOp):\n    \"\"\"Resize operation class:\n    A callable class that resizes image to a given size by resampling coordinates.\n\n    To instantiate:\n        obj = Resize(size, interpolation, anti_alias, anti_alias_sigma)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    size\n        The new image size. If float, assumes the same size in all directions.\n        Alternatively, a sequence of floats can be passed to specify size along\n        each dimension. Passing 0 at any position will keep the original\n        size along that dimension.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `size < image.GetSize()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n    \"\"\"\n\n    def __init__(self,\n                 size: Union[int, Sequence[int], np.ndarray],\n                 interpolation: str =\"linear\",\n                 anti_alias: bool =True,\n                 anti_alias_sigma: Optional[float] =None):\n        self.size = size\n        self.interpolation = interpolation\n        self.anti_alias = anti_alias\n        self.anti_alias_sigma = anti_alias_sigma\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Resize callable object: Resizes image to a given size by resampling coordinates.\n\n        Parameters\n        ----------\n        image\n            The image to resize.\n\n        Returns\n        -------\n        sitk.Image\n            The resized image.\n        \"\"\"\n\n        return resize(image,\n                      size=self.size,\n                      interpolation=self.interpolation,\n                      anti_alias_sigma=self.anti_alias_sigma)\n\n\nclass Zoom(BaseOp):\n    \"\"\"Zoom operation class: A callable class that rescales image, preserving its spatial extent.\n\n    To instantiate:\n        obj = Zoom(scale_factor, interpolation, anti_alias, anti_alias_sigma)\n\n    To call:\n        result = obj(image)\n\n    The rescaled image will have the same spatial extent (size) but will be\n    rescaled by `scale_factor` in each dimension. Alternatively, a separate\n    scale factor for each dimension can be specified by passing a sequence\n    of floats.\n\n    Parameters\n    ----------\n    scale_factor\n        If float, each dimension will be scaled by that factor. If tuple, each\n        dimension will be scaled by the corresponding element.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `size < image.GetSize()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n    \"\"\"\n\n    def __init__(self,\n                 scale_factor: Union[float, Sequence[float]],\n                 interpolation: str =\"linear\",\n                 anti_alias: bool =True,\n                 anti_alias_sigma: Optional[float] =None):\n        self.scale_factor = scale_factor\n        self.interpolation = interpolation\n        self.anti_alias = anti_alias\n        self.anti_alias_sigma = anti_alias_sigma\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Zoom callable object: Rescales image, preserving its spatial extent.\n\n        The rescaled image will have the same spatial extent (size) but will be\n        rescaled by `scale_factor` in each dimension. Alternatively, a separate\n        scale factor for each dimension can be specified by passing a sequence\n        of floats.\n\n        Parameters\n        ----------\n        image\n            The image to rescale.\n\n        Returns\n        -------\n        sitk.Image\n            The rescaled image.\n        \"\"\"\n\n        return zoom(image,\n                    self.scale_factor,\n                    interpolation=self.interpolation,\n                    anti_alias=self.anti_alias,\n                    anti_alias_sigma=self.anti_alias_sigma)\n\n\nclass Rotate(BaseOp):\n    \"\"\"Rotate operation class: A callable class that rotates an image around a given centre.\n\n    To instantiate:\n        obj = Rotate(rotation_centre, angles, interpolation)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    rotation_centre\n        The centre of rotation in image coordinates.\n\n    angles\n        The angles of rotation around x, y and z axes.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n    \"\"\"\n\n    def __init__(self,\n                 rotation_centre: Sequence[float],\n                 angles: Union[float, Sequence[float]],\n                 interpolation: str =\"linear\"):\n        self.rotation_centre = rotation_centre\n        self.angles = angles\n        self.interpolation = interpolation\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Rotate callable object: Rotates an image around a given centre.\n\n        Parameters\n        ----------\n        image\n            The image to rotate.\n\n        Returns\n        -------\n        sitk.Image\n            The rotated image.\n        \"\"\"\n\n        return rotate(image,\n                      rotation_centre=self.rotation_centre,\n                      angles=self.angles,\n                      interpolation=self.interpolation)\n\n\nclass InPlaneRotate(BaseOp):\n    \"\"\"InPlaneRotate operation class: A callable class that rotates an image on a plane.\n\n    To instantiate:\n        obj = InPlaneRotate(angle, interpolation)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    angle\n        The angle of rotation.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n    \"\"\"\n    def __init__(self, angle: float, interpolation: str =\"linear\"):\n        self.angle = angle\n        self.interpolation = interpolation\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"InPlaneRotate callable object: Rotates an image on a plane.\n\n        Parameters\n        ----------\n        image\n            The image to rotate.\n\n        Returns\n        -------\n        sitk.Image\n            The rotated image.\n        \"\"\"\n\n        image_size = np.array(image.GetSize())\n        image_centre = image_size // 2\n        angles = (0., 0., self.angle)\n        return rotate(image,\n                      rotation_centre=image_centre.tolist(),\n                      angles=angles,\n                      interpolation=self.interpolation)\n\n\n# Cropping & mask ops\nclass Crop(BaseOp):\n    \"\"\"Crop operation class: A callable class that crops an image to\n    the desired size around a given centre.\n\n    To instantiate:\n        obj = Crop(crop_centre, size)\n\n    To call:\n        result = obj(image)\n\n    Note that the cropped image might be smaller than size in a particular\n    direction if the cropping window exceeds image boundaries.\n\n    Parameters\n    ----------\n\n    crop_centre\n        The centre of the cropping window in image coordinates.\n\n    size\n        The size of the cropping window along each dimension in pixels. If\n        float, assumes the same size in all directions. Alternatively, a\n        sequence of floats can be passed to specify size along x, y and z\n        dimensions. Passing 0 at any position will keep the original size along\n        that dimension.\n    \"\"\"\n\n    def __init__(self, crop_centre: Sequence[float], size: Union[int, Sequence[int], np.ndarray]):\n        self.crop_centre = crop_centre\n        self.size = size\n\n    def __call__(self, image) -> sitk.Image:\n        \"\"\"Crop callable object: Crops an image to the desired size around a given centre.\n\n        Note that the cropped image might be smaller than size in a particular\n        direction if the cropping window exceeds image boundaries.\n\n        Parameters\n        ----------\n        image\n            The image to crop.\n\n        Returns\n        -------\n        sitk.Image\n            The cropped image.\n        \"\"\"\n\n        return crop(image, crop_centre=self.crop_centre, size=self.size)\n\n\nclass CentreCrop(BaseOp):\n    \"\"\"CentreCrop operation class: A callable class that crops an image to the desired size\n    around the centre of an image.\n\n    To instantiate:\n        obj = CentreCrop(size)\n\n    To call:\n        result = obj(image)\n\n    Note that the cropped image might be smaller than size in a particular\n    direction if the cropping window exceeds image boundaries.\n\n    Parameters\n    ----------\n    size\n        The size of the cropping window along each dimension in pixels. If\n        float, assumes the same size in all directions. Alternatively, a\n        sequence of floats can be passed to specify size along x, y and z\n        dimensions. Passing 0 at any position will keep the original size along\n        that dimension.\n    \"\"\"\n\n    def __init__(self, size: Union[int, Sequence[int]]):\n        self.size = size\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"CentreCrop callable object: Crops an image to the desired size\n        around the centre of an image.\n\n        Note that the cropped image might be smaller than size in a particular\n        direction if the cropping window exceeds image boundaries.\n\n        Parameters\n        ----------\n        image\n            The image to crop.\n\n        Returns\n        -------\n        sitk.Image\n            The cropped image.\n        \"\"\"\n        image_size = np.array(image.GetSize())\n        image_centre = image_size // 2\n        return crop(image, crop_centre=image_centre, size=self.size)\n\n\nclass BoundingBox(BaseOp):\n    \"\"\"BoundingBox opetation class: A callable class that find the axis-aligned\n    bounding box of a region descriibed by a segmentation mask.\n\n    To instantiate:\n        obj = BoundingBox()\n\n    To call:\n        result = obj(mask, label)\n    \"\"\"\n\n    def __call__(self, mask: sitk.Image, label: int = 1) -> Tuple[Tuple, Tuple]:\n        \"\"\"BoundingBox callable object: Find the axis-aligned\n        bounding box of a region descriibed by a segmentation mask.\n\n        Parameters\n        ----------\n        mask\n            Segmentation mask describing the region of interest. Can be an image of\n            type unsigned int representing a label map or `segmentation.Segmentation`.\n\n        label, optional\n            Label to use when computing bounding box if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        tuple of tuples\n            The bounding box location and size. The first tuple gives the\n            coordinates of the corner closest to the origin and the second\n            gives the size in pixels along each dimension.\n        \"\"\"\n\n        return bounding_box(mask, label=label)\n\n\nclass Centroid(BaseOp):\n    \"\"\"Centroid operation class: A callable class that finds the centroid of\n    a labelled region specified by a segmentation mask.\n\n    To instantiate:\n        obj = Centroid(world_coordinates)\n\n    To call:\n        result = obj(mask, label)\n\n    Parameters\n    ----------\n    world_coordinates, optional\n        If True, return centroid in world coordinates, otherwise in image\n        (voxel) coordinates (default).\n    \"\"\"\n\n    def __init__(self, world_coordinates: bool = False):\n                 self.world_coordinates = world_coordinates\n\n    def __call__(self,\n                 mask: sitk.Image,\n                 label: Optional[int] = 1) -> tuple:\n        \"\"\"Centroid callable object: Finds the centroid of\n        a labelled region specified by a segmentation mask.\n\n        Parameters\n        ----------\n        mask\n            Segmentation mask describing the region of interest. Can be an image of\n            type unsigned int representing a label map or `segmentation.Segmentation`.\n\n        label, optional\n            Label to use when computing the centroid if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        tuple\n            The centroid coordinates.\n        \"\"\"\n\n        return centroid(mask,\n                        label=label,\n                        world_coordinates=self.world_coordinates)\n\n\nclass CropToMaskBoundingBox(BaseOp):\n    \"\"\"CropToMaskBoundingBox opetation class:\n    A callable class that crops the image using the bounding box of a region of interest specified\n    by a segmentation mask.\n\n    To instantiate:\n        obj = CropToMaskBoundingBox(margin)\n\n    To call:\n        result = obj(image, mask, label)\n\n    Parameters\n    ----------\n    margin\n        A margin that will be added to each dimension when cropping. If int,\n        add the same margin to each dimension. A sequence of ints can also be\n        passed to specify the margin separately along each dimension.\n    \"\"\"\n\n    def __init__(self, margin: Union[int, Sequence[int], np.ndarray]):\n                 self.margin = margin\n\n    def __call__(self,\n                 image: sitk.Image,\n                 mask: Union[int, Sequence[int], np.ndarray] = None,\n                 label: Optional[int] = 1) -> Tuple[sitk.Image]:\n        \"\"\"CropToMaskBoundingBox callable object:\n        Crops the image using the bounding box of a region of interest specified\n        by a segmentation mask.\n\n        Parameters\n        ----------\n        image\n            The image to crop.\n\n        mask\n            Segmentation mask describing the region of interest. Can be an image of\n            type unsigned int representing a label map or `segmentation.Segmentation`.\n\n        label, optional\n            Label to use when computing the centroid if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        tuple of sitk.Image\n            The cropped image and mask.\n        \"\"\"\n\n        return crop_to_mask_bounding_box(image,\n                                         mask,\n                                         margin=self.margin,\n                                         label=label)\n\n\n# Intensity ops\nclass ClipIntensity(BaseOp):\n    \"\"\"ClipIntensity operation class:\n    A callable class that clips image grey level intensities to specified range.\n\n    To instantiate:\n        obj = ClipIntensity(lower, upper)\n\n    To call:\n        result = obj(image)\n\n    The grey level intensities in the resulting image will fall in the range\n    [lower, upper].\n\n    Parameters\n    ----------\n    lower\n        The lower bound on grey level intensity. Voxels with lower intensity\n        will be set to this value.\n\n    upper\n        The upper bound on grey level intensity. Voxels with higer intensity\n        will be set to this value.\n    \"\"\"\n\n    def __init__(self, lower: float, upper: float):\n        self.lower = lower\n        self.upper = upper\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"ClipIntensity callable object:\n        Clips image grey level intensities to specified range.\n\n        Parameters\n        ----------\n        image\n            The intensity image to clip.\n\n        Returns\n        -------\n        sitk.Image\n            The clipped intensity image.\n        \"\"\"\n        return clip_intensity(image, self.lower, self.upper)\n\n\nclass WindowIntensity(BaseOp):\n    \"\"\"WindowIntensity operation class:\n    A callable class that restricts image grey level intensities to a given window and level.\n\n    To instantiate:\n        obj = WindowIntensity(window, level)\n\n    To call:\n        result = obj(image)\n\n    The grey level intensities in the resulting image will fall in the range\n    [level - window / 2, level + window / 2].\n\n    Parameters\n    ----------\n    window\n        The width of the intensity window.\n\n    level\n        The mid-point of the intensity window.\n    \"\"\"\n\n    def __init__(self, window: float, level: float):\n        self.window = window\n        self.level = level\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"WindowIntensity callable object:\n        Restricts image grey level intensities to a given window and level.\n\n        Parameters\n        ----------\n        image\n            The intensity image to window.\n\n        Returns\n        -------\n        sitk.Image\n            The windowed intensity image.\n        \"\"\"\n\n        return window_intensity(image, self.window, self.level)\n\n\nclass ImageStatistics(BaseOp):\n    \"\"\"ImageStatistics operation class:\n    A callable class that computes the intensity statistics of an image.\n\n    To instantiate:\n        obj = ImageStatistics()\n\n    To call:\n        result = obj(image, mask, label)\n\n    Returns the minimum, maximum, sum, mean, variance and standard deviation\n    of image intensities.\n    This function also supports computing the statistics in a specific\n    region of interest if `mask` and `label` are passed.\n    \"\"\"\n\n    def __call__(self,\n                 image: sitk.Image,\n                 mask: Optional[sitk.Image] = None,\n                 label: Optional[int] =1) -> float:\n        \"\"\"ImageStatistics callable object:\n        Computes the intensity statistics of an image.\n\n        Returns the minimum, maximum, sum, mean, variance and standard deviation\n        of image intensities.\n        This function also supports computing the statistics in a specific\n        region of interest if `mask` and `label` are passed.\n\n        Parameters\n        ----------\n        image\n            The image used to compute the statistics.\n\n        mask, optional\n            Segmentation mask specifying a region of interest used in computation.\n            Can be an image of type unsigned int representing a label map or\n            `segmentation.Segmentation`. Only voxels falling within the ROI will\n            be considered. If None, use the whole image.\n\n        label, optional\n            Label to use when computing the statistics if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        collections.namedtuple\n            The computed intensity statistics in the image or region.\n        \"\"\"\n\n        return image_statistics(image, mask, label=label)\n\n\nclass StandardScale(BaseOp):\n    \"\"\"StandardScale operation class:\n    A callable class that rescales image intensities by subtracting\n    the mean and dividing by standard deviation.\n\n    To instantiate:\n        obj = StandardScale(rescale_mean, rescale_std)\n    To call\n        result = obj(image, mask, label)\n\n    If `rescale_mean` and `rescale_std` are None, image mean and standard\n    deviation will be used, i.e. the resulting image intensities will have\n    0 mean and unit variance. Alternatively, a specific mean and standard\n    deviation can be passed to e.g. standardize a whole dataset of images.\n    If a segmentation mask is passed, only the voxels falling within the mask\n    will be considered when computing the statistics. However, the whole image\n    will still be normalized using the computed values.\n\n    Parameters\n    ----------\n\n    rescale_mean, optional\n        The mean intensity used in rescaling. If None, image mean will be used.\n\n    rescale_std, optional\n        The standard deviation used in rescaling. If None, image standard\n        deviation will be used.\n    \"\"\"\n\n    def __init__(self, rescale_mean: Optional[float] = 0., rescale_std: Optional[float] = 1.):\n        self.rescale_mean = rescale_mean\n        self.rescale_std = rescale_std\n\n    def __call__(self, image: sitk.Image, mask: Optional[sitk.Image] = None, label: Optional[int] =1) -> sitk.Image:\n        \"\"\"StandardScale callable object:\n        A callable class that rescales image intensities by subtracting\n        the mean and dividing by standard deviation.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be rescaled.\n\n        mask, optional\n            Segmentation mask specifying a region of interest used in computation.\n            Can be an image of type unsigned int representing a label map or\n            `segmentation.Segmentation`. Only voxels falling within the ROI will\n            be considered. If None, use the whole image.\n\n        label, optional\n            Label to use when computing the statistics if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        sitk.Image\n            The rescaled image.\n        \"\"\"\n\n        return standard_scale(image, mask, self.rescale_mean, self.rescale_std,\n                              label)\n\n\nclass MinMaxScale(BaseOp):\n    \"\"\"MinMaxScale operation class:\n    A callable class that rescales image intensities to a given minimum and maximum.\n\n    Applies a linear transformation to image intensities such that the minimum\n    and maximum intensity values in the resulting image are equal to minimum\n    (default 0) and maximum (default 1) respectively.\n\n    To instantiante:\n        obj = MinMaxScale(minimum, maximum)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n\n    minimum, optional\n        The minimum intensity in the rescaled image.\n\n    maximum, optional\n        The maximum intensity in the rescaled image.\n    \"\"\"\n\n    def __init__(self, minimum: float, maximum: float):\n        self.minimum = minimum\n        self.maximum = maximum\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"MinMaxScale callable object:\n        Rescales image intensities to a given minimum and maximum.\n\n        Applies a linear transformation to image intensities such that the minimum\n        and maximum intensity values in the resulting image are equal to minimum\n        (default 0) and maximum (default 1) respectively.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be rescaled.\n\n        Returns\n        -------\n        sitk.Image\n            The rescaled image.\n        \"\"\"\n        return min_max_scale(image, self.minimum, self.maximum)\n\n\n# Lambda ops\n\nclass SimpleITKFilter(BaseOp):\n    \"\"\"SimpleITKFilter operation class:\n    A callable class that accepts an sitk.ImageFilter object to add a filter to an image.\n\n    To instantiate:\n        obj = SimpleITKFilter(sitk_filter, *execute_args)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    sitk_filter\n        An ImageFilter object in sitk library.\n\n    execute_args, optional\n        Any arguments to be passed to the Execute() function of the selected ImageFilter object.\n    \"\"\"\n\n    def __init__(self, sitk_filter: ImageFilter, *execute_args: Optional[Any]):\n        self.sitk_filter = sitk_filter\n        self.execute_args = execute_args\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"SimpleITKFilter callable object:\n        A callable class that uses an sitk.ImageFilter object to add a filter to an image.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be processed.\n\n        Returns\n        -------\n        sitk.Image\n            The processed image with a given filter.\n        \"\"\"\n        return self.sitk_filter.Execute(image, *self.execute_args)\n\n\nclass ImageFunction(BaseOp):\n    \"\"\"ImageFunction operation class:\n    A callable class that takens in a function to be used to process an image,\n    and executes it.\n\n    To instantiate:\n        obj = ImageFunction(function, copy_geometry, **kwargs)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    function\n        A function to be used for image processing.\n        This function needs to have the following signature:\n        - function(image: sitk.Image, **args)\n        - The first argument needs to be an sitkImage, followed by optional arguments.\n\n    copy_geometry, optional\n        An optional argument to specify whether information about the image should be copied to the\n        resulting image. Set to be true as a default.\n\n    kwargs, optional\n        Any number of arguements used in the given function.\n    \"\"\"\n\n    def __init__(self, function: Function, copy_geometry: bool = True, **kwargs: Optional[Any]):\n        self.function = function\n        self.copy_geometry = copy_geometry\n        self.kwargs = kwargs\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"ImageFunction callable object:\n        Process an image based on a given function.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be processed.\n\n        Returns\n        -------\n        sitk.Image\n            The image processed with the given function.\n        \"\"\"\n\n        result = self.function(image, **self.kwargs)\n        if self.copy_geometry:\n            result.CopyInformation(image)\n        return result\n\n\nclass ArrayFunction(BaseOp):\n    \"\"\"ArrayFunction operation class:\n    A callable class that takes in a function to be used to process an image from numpy array,\n    and executes it.\n\n    To instantiate:\n        obj = ArrayFunction(function, copy_geometry, **kwargs)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    function\n        A function to be used for image processing.\n        This function needs to have the following signature:\n        - function(image: sitk.Image, **args)\n        - The first argument needs to be an sitkImage, followed by optional arguments.\n\n    copy_geometry, optional\n        An optional argument to specify whether information about the image should be copied to the\n        resulting image. Set to be true as a default.\n\n    kwargs, optional\n        Any number of arguements used in the given function.\n    \"\"\"\n\n    def __init__(self, function: Function, copy_geometry: bool =True, **kwargs: Optional[Any]):\n        self.function = function\n        self.copy_geometry = copy_geometry\n        self.kwargs = kwargs\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"ArrayFunction callable object:\n        Processes an image from numpy array.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be processed.\n\n        Returns\n        -------\n        sitk.Image\n            The image processed with a given function.\n        \"\"\"\n\n        array, origin, direction, spacing = image_to_array(image)\n        result = self.function(array, **self.kwargs)\n        if self.copy_geometry:\n            result = array_to_image(result, origin, direction, spacing)\n        else:\n            result = array_to_image(result)\n        return result\n\n\n# Segmentation ops\n\nclass StructureSetToSegmentation(BaseOp):\n    \"\"\"StructureSetToSegmentation operation class:\n    A callable class that accepts ROI names, a StrutureSet object, and a reference image, and\n    returns Segmentation mask.\n\n    To instantiate:\n        obj = StructureSet(roi_names)\n    To call:\n        mask = obj(structure_set, reference_image)\n\n    Parameters\n    ----------\n    roi_names\n        List of Region of Interests\n    \"\"\"\n\n    def __init__(self, \n                 roi_names: Dict[str, str], \n                 continuous: bool = True):\n        \"\"\"Initialize the op.\n\n        Parameters\n        ----------\n        roi_names\n            List of ROI names to export. Both full names and\n            case-insensitive regular expressions are allowed.\n            All labels within one sublist will be assigned\n            the same label.\n        continuous\n            flag passed to 'physical_points_to_idxs' in 'StructureSet.to_segmentation'. \n            Helps to resolve errors caused by ContinuousIndex > Index. \n\n        Notes\n        -----\n        If `self.roi_names` contains lists of strings, each matching\n        name within a sublist will be assigned the same label. This means\n        that `roi_names=['pat']` and `roi_names=[['pat']]` can lead\n        to different label assignments, depending on how many ROI names\n        match the pattern. E.g. if `self.roi_names = ['fooa', 'foob']`,\n        passing `roi_names=['foo(a|b)']` will result in a segmentation with \n        two labels, but passing `roi_names=[['foo(a|b)']]` will result in\n        one label for both `'fooa'` and `'foob'`.\n\n        If `roi_names` is kept empty ([]), the pipeline will process all ROIs/contours \n        found according to their original names.\n\n        In general, the exact ordering of the returned labels cannot be\n        guaranteed (unless all patterns in `roi_names` can only match\n        a single name or are lists of strings).\n        \"\"\"\n        self.roi_names = roi_names\n        self.continuous = continuous\n\n    def __call__(self, \n                 structure_set: StructureSet, \n                 reference_image: sitk.Image, \n                 existing_roi_indices: Dict[str, int], \n                 ignore_missing_regex: bool,\n                 roi_select_first: bool = False,\n                 roi_separate: bool = False) -> Segmentation:\n        \"\"\"Convert the structure set to a Segmentation object.\n\n        Parameters\n        ----------\n        structure_set\n            The structure set to convert.\n        reference_image\n            Image used as reference geometry.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation object.\n        \"\"\"\n        return structure_set.to_segmentation(reference_image,\n                                             roi_names=self.roi_names,\n                                             continuous=self.continuous,\n                                             existing_roi_indices=existing_roi_indices,\n                                             ignore_missing_regex=ignore_missing_regex,\n                                             roi_select_first=roi_select_first,\n                                             roi_separate=roi_separate)\n\n\nclass FilterSegmentation():\n    \"\"\"FilterSegmentation operation class:\n    A callable class that accepts ROI names, a Segmentation mask with all labels\n    and returns only the desired Segmentation masks based on accepted ROI names.\n\n    To instantiate:\n        obj = StructureSet(roi_names)\n    To call:\n        mask = obj(structure_set, reference_image)\n\n    Parameters\n    ----------\n    roi_names\n        List of Region of Interests\n    \"\"\"\n\n    def __init__(self, \n                 roi_patterns: Dict[str, str],\n                 continuous: bool = False):\n        \"\"\"Initialize the op.\n\n        Parameters\n        ----------\n        roi_names\n            List of ROI names to export. Both full names and\n            case-insensitive regular expressions are allowed.\n            All labels within one sublist will be assigned\n            the same label.\n        \n        \"\"\"\n        self.roi_patterns = roi_patterns\n        self.continuous = continuous\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        \"\"\"\n        Parameters\n        ----\n        roi_select_first\n            Select the first matching ROI/regex for each OAR, no duplicate matches. \n\n        roi_separate\n            Process each matching ROI/regex as individual masks, instead of consolidating into one mask\n            Each mask will be named ROI_n, where n is the nth regex/name/string.\n        \"\"\"\n        labels = {}\n        cur_label = 0\n        if names == self.roi_patterns:\n            for i, name in enumerate(self.roi_patterns):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):  # checks if all ROIs have already been processed.\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:  # if multiple regex/names to match\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched: \n                            break  # break if roi_select_first and we're matched\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels\n\n    def get_mask(self, reference_image, seg, mask, label, idx, continuous):\n        size = seg.GetSize()\n        seg_arr = sitk.GetArrayFromImage(seg)\n        if len(size) == 5:\n            size = size[:-1]\n        elif len(size) == 3:\n            size = size.append(1)\n\n        idx_seg = self.roi_names[label] - 1         # SegmentSequence numbering starts at 1 instead of 0\n        if size[:-1] == reference_image.GetSize():  # Assumes `size` is length of 4: (x, y, z, channels)\n            mask[:,:,:,idx] += seg[:,:,:,idx_seg]\n        else:                                       # if 2D segmentations on 3D images\n            frame        = seg.frame_groups[idx_seg]\n            ref_uid      = frame.DerivationImageSequence[0].SourceImageSequence[0].ReferencedSOPInstanceUID  # unused but references InstanceUID of slice\n            assert ref_uid is not None, \"There was no ref_uid\" # dodging linter\n\n            frame_coords = np.array(frame.PlanePositionSequence[0].ImagePositionPatient)\n            img_coords   = physical_points_to_idxs(reference_image, np.expand_dims(frame_coords, (0, 1)))[0][0]\n            z            = img_coords[0]\n\n            mask[z,:,:,idx] += seg_arr[0,idx_seg,:,:]\n\n    def __call__(self, \n                 reference_image: sitk.Image,\n                 seg: Segmentation, \n                 existing_roi_indices: Dict[str, int], \n                 ignore_missing_regex: bool = False,\n                 roi_select_first: bool = False,\n                 roi_separate: bool = False) -> Segmentation:\n        \"\"\"Convert the structure set to a Segmentation object.\n\n        Parameters\n        ----------\n        structure_set\n            The structure set to convert.\n        reference_image\n            Image used as reference geometry.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation object.\n        \"\"\"\n        from itertools import groupby\n\n        # variable name isn't ideal, but follows StructureSet.to_segmentation convention\n        self.roi_names    = seg.raw_roi_names \n        labels = {}\n        \n        # `roi_names` in .to_segmentation() method = self.roi_patterns\n        if self.roi_patterns is None or self.roi_patterns == {}:\n            self.roi_patterns = self.roi_names\n            labels = self._assign_labels(self.roi_patterns, roi_select_first, roi_separate) #only the ones that match the regex\n        elif isinstance(self.roi_patterns, dict):\n            for name, pattern in self.roi_patterns.items():\n                if isinstance(pattern, str):\n                    matching_names = list(self._assign_labels([pattern], roi_select_first).keys())\n                    if matching_names:\n                        labels[name] = matching_names  # {\"GTV\": [\"GTV1\", \"GTV2\"]} is the result of _assign_labels()\n                elif isinstance(pattern, list):        # for inputs that have multiple patterns for the input, e.g. {\"GTV\": [\"GTV.*\", \"HTVI.*\"]}\n                    labels[name] = []\n                    for pattern_one in pattern:\n                        matching_names = list(self._assign_labels([pattern_one], roi_select_first).keys())\n                        if matching_names:\n                            labels[name].extend(matching_names)  # {\"GTV\": [\"GTV1\", \"GTV2\"]}\n        elif isinstance(self.roi_patterns, list):      # won't this always trigger after the previous?\n            labels = self._assign_labels(self.roi_patterns, roi_select_first)\n        else:\n            raise ValueError(f\"{self.roi_patterns} not expected datatype\")\n        print(\"labels:\", labels)\n        \n        # removing empty labels from dictionary to prevent processing empty masks\n        all_empty = True\n        for v in labels.values():\n            if v != []:\n                all_empty = False\n        if all_empty:\n            if not ignore_missing_regex:\n                raise ValueError(f\"No ROIs matching {self.roi_patterns} found in {self.roi_names}.\")\n            else:\n                return None\n\n        labels = {k:v for (k,v) in labels.items() if v != [] }\n        size = reference_image.GetSize()[::-1] + (len(labels),)\n        mask = np.zeros(size, dtype=np.uint8)\n\n        seg_roi_indices = {}\n        if self.roi_patterns != {} and isinstance(self.roi_patterns, dict):\n            for i, (name, label_list) in enumerate(labels.items()):\n                for label in label_list:\n                    self.get_mask(reference_image, seg, mask, label, i, self.continuous)\n                seg_roi_indices[name] = i\n        else:\n            for name, label in labels.items():\n                self.get_mask(reference_image, seg, mask, name, label, self.continuous)\n            seg_roi_indices = {\"_\".join(k): v for v, k in groupby(labels, key=lambda x: labels[x])}\n\n        mask[mask > 1] = 1\n        mask = sitk.GetImageFromArray(mask, isVector=True)\n        mask.CopyInformation(reference_image)\n        return Segmentation(mask, \n                            roi_indices=seg_roi_indices, \n                            existing_roi_indices=existing_roi_indices, \n                            raw_roi_names=labels)\n\n\nclass MapOverLabels(BaseOp):\n    \"\"\"MapOverLabels operation class:\n\n    To instantiate:\n        obj = MapOverLabels(op, include_background, return_segmentation)\n    To call:\n        mask = obj(segmentation, **kwargs)\n\n    Parameters\n    ----------\n    op\n        A processing function to be used for the operation.\n\n    \"\"\"\n\n    def __init__(self, op, include_background: bool = False, return_segmentation: bool =True):\n        self.op = op\n        self.include_background = include_background\n        self.return_seg = return_segmentation\n\n    def __call__(self, segmentation: Segmentation, **kwargs: Optional[Any]) -> Segmentation:\n        \"\"\"MapOverLabels callable object:\n\n        Parameters\n        ----------\n        include_background\n            Specify whether to include background. Set to be false as a default.\n\n        return_segmentation\n            Specify whether to return segmentation. Set to be true as a default.\n\n        **kwargs\n            Arguments used for the processing function.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation mask.\n        \"\"\"\n\n        return map_over_labels(segmentation,\n                               self.op,\n                               include_background=self.include_background,\n                               return_segmentation=self.return_seg,\n                               **kwargs)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the BaseOp class in this code?",
        "answer": "The BaseOp class serves as a base class for all operation classes in the code. It defines a common interface with a __call__ method that must be implemented by subclasses, and a __repr__ method for string representation of the object's attributes."
      },
      {
        "question": "How does the Resample class handle anti-aliasing when downsampling an image?",
        "answer": "The Resample class uses a Gaussian kernel for anti-aliasing when downsampling an image. This is controlled by the 'anti_alias' parameter, which is set to True by default. The 'anti_alias_sigma' parameter allows specifying the standard deviation of the Gaussian kernel used for anti-aliasing."
      },
      {
        "question": "What is the purpose of the ImageAutoInput class and how does it differ from ImageCSVInput?",
        "answer": "The ImageAutoInput class is a wrapper around ImgCSVloader that automatically crawls through a specified directory, forms a graph of modalities present in the dataset, and loads information for user-provided modalities. It differs from ImageCSVInput in that it doesn't require a pre-existing CSV file, but instead generates the necessary information from the directory structure and file contents."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class ImageAutoInput(BaseInput):\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        path_crawl = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        if not os.path.exists(path_crawl) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs=n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent,\".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=path_crawl, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [\"_\".join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]",
        "complete": "class ImageAutoInput(BaseInput):\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        path_crawl = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        if not os.path.exists(path_crawl) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs=n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent,\".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=path_crawl, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [\"_\".join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]\n\n        loader = ImageCSVLoader(self.df_combined,\n                                colnames=self.column_names,\n                                seriesnames=self.series_names,\n                                id_column=None,\n                                expand_paths=False,\n                                readers=self.readers)\n        \n        super().__init__(loader)"
      },
      {
        "partial": "class FilterSegmentation():\n    def __init__(self, \n                 roi_patterns: Dict[str, str],\n                 continuous: bool = False):\n        self.roi_patterns = roi_patterns\n        self.continuous = continuous\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        labels = {}\n        cur_label = 0\n        if names == self.roi_patterns:\n            for i, name in enumerate(self.roi_patterns):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched: \n                            break\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels",
        "complete": "class FilterSegmentation():\n    def __init__(self, \n                 roi_patterns: Dict[str, str],\n                 continuous: bool = False):\n        self.roi_patterns = roi_patterns\n        self.continuous = continuous\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        labels = {}\n        cur_label = 0\n        if names == self.roi_patterns:\n            for i, name in enumerate(self.roi_patterns):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched: \n                            break\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels\n\n    def get_mask(self, reference_image, seg, mask, label, idx, continuous):\n        size = seg.GetSize()\n        seg_arr = sitk.GetArrayFromImage(seg)\n        if len(size) == 5:\n            size = size[:-1]\n        elif len(size) == 3:\n            size = size.append(1)\n\n        idx_seg = self.roi_names[label] - 1\n        if size[:-1] == reference_image.GetSize():\n            mask[:,:,:,idx] += seg[:,:,:,idx_seg]\n        else:\n            frame = seg.frame_groups[idx_seg]\n            ref_uid = frame.DerivationImageSequence[0].SourceImageSequence[0].ReferencedSOPInstanceUID\n            assert ref_uid is not None\n\n            frame_coords = np.array(frame.PlanePositionSequence[0].ImagePositionPatient)\n            img_coords = physical_points_to_idxs(reference_image, np.expand_dims(frame_coords, (0, 1)))[0][0]\n            z = img_coords[0]\n\n            mask[z,:,:,idx] += seg_arr[0,idx_seg,:,:]\n\n    def __call__(self, \n                 reference_image: sitk.Image,\n                 seg: Segmentation, \n                 existing_roi_indices: Dict[str, int], \n                 ignore_missing_regex: bool = False,\n                 roi_select_first: bool = False,\n                 roi_separate: bool = False) -> Segmentation:\n        from itertools import groupby\n\n        self.roi_names = seg.raw_roi_names \n        labels = {}\n        \n        if self.roi_patterns is None or self.roi_patterns == {}:\n            self.roi_patterns = self.roi_names\n            labels = self._assign_labels(self.roi_patterns, roi_select_first, roi_separate)\n        elif isinstance(self.roi_patterns, dict):\n            for name, pattern in self.roi_patterns.items():\n                if isinstance(pattern, str):\n                    matching_names = list(self._assign_labels([pattern], roi_select_first).keys())\n                    if matching_names:\n                        labels[name] = matching_names\n                elif isinstance(pattern, list):\n                    labels[name] = []\n                    for pattern_one in pattern:\n                        matching_names = list(self._assign_labels([pattern_one], roi_select_first).keys())\n                        if matching_names:\n                            labels[name].extend(matching_names)\n        elif isinstance(self.roi_patterns, list):\n            labels = self._assign_labels(self.roi_patterns, roi_select_first)\n        else:\n            raise ValueError(f\"{self.roi_patterns} not expected datatype\")\n        print(\"labels:\", labels)\n        \n        all_empty = True\n        for v in labels.values():\n            if v != []:\n                all_empty = False\n        if all_empty:\n            if not ignore_missing_regex:\n                raise ValueError(f\"No ROIs matching {self.roi_patterns} found in {self.roi_names}.\")\n            else:\n                return None\n\n        labels = {k:v for (k,v) in labels.items() if v != [] }\n        size = reference_image.GetSize()[::-1] + (len(labels),)\n        mask = np.zeros(size, dtype=np.uint8)\n\n        seg_roi_indices = {}\n        if self.roi_patterns != {} and isinstance(self.roi_patterns, dict):\n            for i, (name, label_list) in enumerate(labels.items()):\n                for label in label_list:\n                    self.get_mask(reference_image, seg, mask, label, i, self.continuous)\n                seg_roi_indices[name] = i\n        else:\n            for name, label in labels.items():\n                self.get_mask(reference_image, seg, mask, name, label, self.continuous)\n            seg_roi_indices = {\"_\".join(k): v for v, k in groupby(labels, key=lambda x: labels[x])}\n\n        mask[mask > 1] = 1\n        mask = sitk.GetImageFromArray(mask, isVector=True)\n        mask.CopyInformation(reference_image)\n        return Segmentation(mask, \n                            roi_indices=seg_roi_indices, \n                            existing_roi_indices=existing_roi_indices, \n                            raw_roi_names=labels)"
      }
    ],
    "dependencies": {
      "imports": [
        "numpy",
        "SimpleITK",
        "json"
      ],
      "from_imports": [
        "typing.List",
        "functional.*",
        "io.*",
        "utils.image_to_array",
        "modules.map_over_labels",
        "modules.DataGraph",
        "itertools.groupby"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/drugPerturbationSig.R",
    "language": "R",
    "content": "#' Creates a signature representing gene expression (or other molecular profile)\n#' change induced by administrating a drug, for use in drug effect analysis.\n#'\n#' Given a Pharmacoset of the perturbation experiment type, and a list of drugs,\n#' the function will compute a signature for the effect of drug concentration on\n#' the molecular profile of a cell. The algorithm uses a regression model which\n#' corrects for experimental batch effects, cell specific differences, and\n#' duration of experiment to isolate the effect of the concentration of the drug\n#' applied. The function returns the estimated coefficient for concentration,\n#' the t-stat, the p-value and the false discovery rate associated with that\n#' coefficient, in a 3 dimensional array, with genes in the first direction,\n#' drugs in the second, and the selected return values in the third.\n#'\n#' @examples\n#' data(CMAPsmall)\n#' drug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\n#' print(drug.perturbation)\n#'\n#' @param pSet [PharmacoSet] a PharmacoSet of the perturbation experiment type\n#' @param mDataType `character` which one of the molecular data types to use\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv\n#' @param drugs `character` a vector of drug names for which to compute the\n#'   signatures. Should match the names used in the PharmacoSet.\n#' @param cells `character` a vector of cell names to use in computing the\n#'   signatures. Should match the names used in the PharmacoSet.\n#' @param features `character` a vector of features for which to compute the\n#'   signatures. Should match the names used in correspondant molecular data in PharmacoSet.\n#' @param nthread `numeric` if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#' @param returnValues `character` Which of estimate, t-stat, p-value and fdr\n#'   should the function return for each gene drug pair?\n#' @param verbose `logical(1)` Should diagnostive messages be printed? (default false)\n#'\n#' @return `list` a 3D array with genes in the first dimension, drugs in the\n#'   second, and return values in the third.\n#'\n#' @export\ndrugPerturbationSig <- function(pSet, mDataType, drugs, cells, features,\n  nthread=1, returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE)\n{\n\tavailcore <- parallel::detectCores()\n\tif ( nthread > availcore) {\n\t  nthread <- availcore\n\t}\n  options(\"mc.cores\"=nthread)\n  if(!missing(cells)){\n    if(!all(cells%in%sampleNames(pSet))){\n      stop(\"The cell names should match to the names used in sampleNames(pSet)\")\n    }\n    pSet <- subsetTo(pSet, cells=cells)\n  }\n  if (mDataType %in% names(pSet@molecularProfiles)) {\n    #eset <- pSet@molecularProfiles[[mDataType]]\n\t\tif(S4Vectors::metadata(pSet@molecularProfiles[[mDataType]])$annotation != \"rna\"){\n\t\t\tstop(sprintf(\"Only rna data type perturbations are currently implemented\"))\n\t\t}\n  } else {\n    stop (sprintf(\"This pSet does not have any molecular data of type %s, choose among: %s\", mDataType), paste(names(pSet@molecularProfiles), collapse=\", \"))\n  }\n\n\n  if (missing(drugs)) {\n    drugn <- treatmentNames(pSet)\n  } else {\n    drugn <- drugs\n  }\n  dix <- is.element(drugn, PharmacoGx::phenoInfo(pSet, mDataType)[ , \"treatmentid\"])\n  if (verbose && !all(dix)) {\n    warning (sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) {\n    stop(\"None of the drugs were found in the dataset\")\n  }\n  drugn <- drugn[dix]\n\n  if (missing(features)) {\n    features <- rownames(featureInfo(pSet, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(pSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning (sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  # splitix <- parallel::splitIndices(nx=length(drugn), ncl=nthread)\n  # splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n  mcres <- lapply(drugn, function(x, exprs, sampleinfo) {\n    res <- NULL\n    i = x\n    ## using a linear model (x ~ concentration + cell + batch + duration)\n    res <- rankGeneDrugPerturbation(data=exprs, drug=i, drug.id=as.character(sampleinfo[ , \"treatmentid\"]), drug.concentration=as.numeric(sampleinfo[ , \"concentration\"]), type=as.character(sampleinfo[ , \"sampleid\"]), xp=as.character(sampleinfo[ , \"xptype\"]), batch=as.character(sampleinfo[ , \"batchid\"]), duration=as.character(sampleinfo[ , \"duration\"]) ,single.type=FALSE, nthread=nthread, verbose=FALSE)$all[ , returnValues, drop=FALSE]\n    res <- list(res)\n    names(res) <- i\n    return(res)\n  }, exprs=t(molecularProfiles(pSet, mDataType)[features, , drop=FALSE]), sampleinfo=PharmacoGx::phenoInfo(pSet, mDataType))\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE=logical(1))]\n  drug.perturbation <- array(NA, dim=c(nrow(featureInfo(pSet, mDataType)[features,, drop=FALSE]), length(res), ncol(res[[1]])), dimnames=list(rownames(featureInfo(pSet, mDataType)[features,,drop=FALSE]), names(res), colnames(res[[1]])))\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n              xx <- array(NA, dim=length(k), dimnames=list(k))\n              xx[rownames(x)] <- x[ , j, drop=FALSE]\n              return (xx)\n              }, j=j, k=rownames(featureInfo(pSet, mDataType)[features,, drop=FALSE]),\n            FUN.VALUE=numeric(dim(drug.perturbation)[1]))\n    drug.perturbation[rownames(featureInfo(pSet, mDataType)[features,, drop=FALSE]), names(res), j] <- ttt\n  }\n\n  drug.perturbation <- PharmacoSig(drug.perturbation, PSetName = name(pSet), Call = as.character(match.call()), SigType='Perturbation')\n\n  return(drug.perturbation)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `drugPerturbationSig` function?",
        "answer": "The main purpose of the `drugPerturbationSig` function is to create a signature representing gene expression (or other molecular profile) changes induced by administering a drug. It computes a signature for the effect of drug concentration on the molecular profile of a cell, using a regression model that corrects for experimental batch effects, cell-specific differences, and duration of experiment to isolate the effect of the drug concentration."
      },
      {
        "question": "How does the function handle multiple cores for parallel processing?",
        "answer": "The function uses the `nthread` parameter to determine the number of cores for parallel processing. It first detects the available cores using `parallel::detectCores()`. If the specified `nthread` is greater than the available cores, it sets `nthread` to the number of available cores. Then it sets the 'mc.cores' option to the determined number of threads for parallel processing."
      },
      {
        "question": "What is the structure of the returned object from the `drugPerturbationSig` function?",
        "answer": "The function returns a `PharmacoSig` object, which is a 3-dimensional array. The first dimension represents genes, the second dimension represents drugs, and the third dimension contains the selected return values (such as estimate, t-stat, p-value, and fdr). The array is wrapped in a `PharmacoSig` object that includes additional metadata like the PharmacoSet name and the function call."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugPerturbationSig <- function(pSet, mDataType, drugs, cells, features,\n  nthread=1, returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE)\n{\n  # Check and set number of threads\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) {\n    nthread <- availcore\n  }\n  options(\"mc.cores\"=nthread)\n\n  # Subset pSet if cells are specified\n  if(!missing(cells)){\n    if(!all(cells %in% sampleNames(pSet))){\n      stop(\"The cell names should match to the names used in sampleNames(pSet)\")\n    }\n    pSet <- subsetTo(pSet, cells=cells)\n  }\n\n  # Check mDataType\n  if (mDataType %in% names(pSet@molecularProfiles)) {\n    if(S4Vectors::metadata(pSet@molecularProfiles[[mDataType]])$annotation != \"rna\"){\n      stop(\"Only rna data type perturbations are currently implemented\")\n    }\n  } else {\n    stop(sprintf(\"This pSet does not have any molecular data of type %s, choose among: %s\", mDataType, paste(names(pSet@molecularProfiles), collapse=\", \")))\n  }\n\n  # Set drug names\n  if (missing(drugs)) {\n    drugn <- treatmentNames(pSet)\n  } else {\n    drugn <- drugs\n  }\n  dix <- is.element(drugn, PharmacoGx::phenoInfo(pSet, mDataType)[ , \"treatmentid\"])\n  if (verbose && !all(dix)) {\n    warning(sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) {\n    stop(\"None of the drugs were found in the dataset\")\n  }\n  drugn <- drugn[dix]\n\n  # Set features\n  if (missing(features)) {\n    features <- rownames(featureInfo(pSet, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(pSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning(sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  # Main computation\n  mcres <- lapply(drugn, function(x, exprs, sampleinfo) {\n    # TODO: Implement the main computation logic here\n  }, exprs=t(molecularProfiles(pSet, mDataType)[features, , drop=FALSE]), sampleinfo=PharmacoGx::phenoInfo(pSet, mDataType))\n\n  # TODO: Process results and create drug.perturbation object\n\n  return(drug.perturbation)\n}",
        "complete": "drugPerturbationSig <- function(pSet, mDataType, drugs, cells, features,\n  nthread=1, returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE)\n{\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) {\n    nthread <- availcore\n  }\n  options(\"mc.cores\"=nthread)\n\n  if(!missing(cells)){\n    if(!all(cells %in% sampleNames(pSet))){\n      stop(\"The cell names should match to the names used in sampleNames(pSet)\")\n    }\n    pSet <- subsetTo(pSet, cells=cells)\n  }\n\n  if (mDataType %in% names(pSet@molecularProfiles)) {\n    if(S4Vectors::metadata(pSet@molecularProfiles[[mDataType]])$annotation != \"rna\"){\n      stop(\"Only rna data type perturbations are currently implemented\")\n    }\n  } else {\n    stop(sprintf(\"This pSet does not have any molecular data of type %s, choose among: %s\", mDataType, paste(names(pSet@molecularProfiles), collapse=\", \")))\n  }\n\n  drugn <- if(missing(drugs)) treatmentNames(pSet) else drugs\n  dix <- is.element(drugn, PharmacoGx::phenoInfo(pSet, mDataType)[ , \"treatmentid\"])\n  if (verbose && !all(dix)) {\n    warning(sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) stop(\"None of the drugs were found in the dataset\")\n  drugn <- drugn[dix]\n\n  features <- if(missing(features)) rownames(featureInfo(pSet, mDataType)) else {\n    fix <- is.element(features, rownames(featureInfo(pSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning(sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features[fix]\n  }\n\n  mcres <- lapply(drugn, function(x, exprs, sampleinfo) {\n    res <- rankGeneDrugPerturbation(data=exprs, drug=x, drug.id=as.character(sampleinfo[ , \"treatmentid\"]),\n                                    drug.concentration=as.numeric(sampleinfo[ , \"concentration\"]),\n                                    type=as.character(sampleinfo[ , \"sampleid\"]),\n                                    xp=as.character(sampleinfo[ , \"xptype\"]),\n                                    batch=as.character(sampleinfo[ , \"batchid\"]),\n                                    duration=as.character(sampleinfo[ , \"duration\"]),\n                                    single.type=FALSE, nthread=nthread, verbose=FALSE)$all[ , returnValues, drop=FALSE]\n    list(res) %>% setNames(x)\n  }, exprs=t(molecularProfiles(pSet, mDataType)[features, , drop=FALSE]), sampleinfo=PharmacoGx::phenoInfo(pSet, mDataType))\n\n  res <- do.call(c, mcres) %>% .[!vapply(., is.null, logical(1))]\n  drug.perturbation <- array(NA, dim=c(nrow(featureInfo(pSet, mDataType)[features,, drop=FALSE]), length(res), ncol(res[[1]])),\n                             dimnames=list(rownames(featureInfo(pSet, mDataType)[features,,drop=FALSE]), names(res), colnames(res[[1]])))\n\n  for (j in seq_len(ncol(res[[1]]))) {\n    drug.perturbation[,,j] <- vapply(res, function(x) {\n      xx <- array(NA, dim=nrow(drug.perturbation), dimnames=list(rownames(drug.perturbation)))\n      xx[rownames(x)] <- x[ , j, drop=FALSE]\n      xx\n    }, FUN.VALUE=numeric(nrow(drug.perturbation)))\n  }\n\n  PharmacoSig(drug.perturbation, PSetName = name(pSet), Call = as.character(match.call()), SigType='Perturbation')\n}"
      },
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  # TODO: Implement the function body\n  # This function should perform the following tasks:\n  # 1. Prepare the data for analysis\n  # 2. Fit a linear model for each gene\n  # 3. Calculate statistics (estimate, t-stat, p-value, FDR)\n  # 4. Return the results\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  require(limma)\n  require(stats)\n\n  # Prepare data\n  drug.ix <- drug.id == drug\n  X <- data[, drug.ix]\n  conc <- drug.concentration[drug.ix]\n  type <- type[drug.ix]\n  xp <- xp[drug.ix]\n  batch <- batch[drug.ix]\n  duration <- duration[drug.ix]\n\n  # Create design matrix\n  design <- model.matrix(~ conc + type + xp + batch + duration)\n\n  # Fit linear model for each gene\n  fit <- lmFit(X, design)\n  fit <- eBayes(fit)\n\n  # Extract results\n  results <- topTable(fit, coef=\"conc\", number=Inf, sort.by=\"none\")\n\n  # Prepare output\n  out <- list(\n    estimate = results$logFC,\n    tstat = results$t,\n    pvalue = results$P.Value,\n    fdr = results$adj.P.Val\n  )\n\n  list(all = as.data.frame(out))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/adaptiveMatthewCor.R",
    "language": "R",
    "content": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    CoreGx::amcc(x, y, step.prct, min.cat, nperm, nthread)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@inherit` tag in the function documentation?",
        "answer": "The `@inherit` tag is used to inherit documentation from another function. In this case, it inherits the documentation from the `amcc` function in the `CoreGx` package. This allows the developer to reuse existing documentation without duplicating it, ensuring consistency and reducing maintenance overhead."
      },
      {
        "question": "Why might a developer choose to create a wrapper function like this instead of directly using the `CoreGx::amcc` function?",
        "answer": "A developer might create a wrapper function like this for several reasons: 1) To provide a simplified interface within their own package, 2) To set default values for certain parameters, 3) To add additional functionality or pre/post-processing, or 4) To make the function available in the global namespace of their package without requiring users to specify the `CoreGx::` prefix each time. In this case, it appears to be primarily for simplifying the interface and potentially setting package-specific defaults."
      },
      {
        "question": "What does the `#' @export` tag do in this code snippet?",
        "answer": "The `#' @export` tag is used in R package development to indicate that this function should be made available to users of the package. When the package is built, functions marked with `@export` will be included in the package's namespace and can be called by users after they load the package. Without this tag, the function would be internal to the package and not directly accessible to users."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    # Complete the function body\n}",
        "complete": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    CoreGx::amcc(x, y, step.prct, min.cat, nperm, nthread)\n}"
      },
      {
        "partial": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, ...) {\n    # Complete the function signature and body\n}",
        "complete": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    CoreGx::amcc(x, y, step.prct, min.cat, nperm, nthread)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeIC50.R",
    "language": "R",
    "content": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose=TRUE,\n                    trunc=TRUE))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeIC50` function and how does it relate to the `computeICn` function?",
        "answer": "The `computeIC50` function is a specific implementation of the more general `computeICn` function. It calculates the IC50 (Inhibitory Concentration 50%) of a drug dose-response curve. The IC50 is the concentration of a drug that reduces the response by 50%. This function essentially calls `computeICn` with `n` set to 50 (or 0.5 if `viability_as_pct` is FALSE), which represents the 50% inhibition point."
      },
      {
        "question": "How does the `viability_as_pct` parameter affect the behavior of the `computeIC50` function?",
        "answer": "The `viability_as_pct` parameter determines how the viability data is interpreted and how the IC50 is calculated. If `viability_as_pct` is TRUE (default), the function assumes the viability data is in percentages, and sets `n` to 50 when calling `computeICn`. If FALSE, it assumes the viability data is in decimal form (0 to 1), and sets `n` to 0.5. This ensures that the IC50 is correctly calculated regardless of how the viability data is represented."
      },
      {
        "question": "What is the significance of the `@describeIn` and `@export` tags in the function documentation?",
        "answer": "The `@describeIn` tag indicates that this function is part of a family of related functions, specifically the `computeICn` family. It suggests that there's a main documentation page for `computeICn`, and this function (`computeIC50`) will be described in that documentation. The `@export` tag means that this function should be made publicly available when the package is built and installed. It allows users of the package to access and use the `computeIC50` function directly."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ,\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}"
      },
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn())\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration, viability, Hill_fit,\n                    ifelse(viability_as_pct, 50, .5),\n                    conc_as_log, viability_as_pct, verbose, trunc))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_PharmacoSetClass.R",
    "language": "R",
    "content": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking PharmacoSet Class Methods.\")\n\n\ntest_that(\"cellInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sampleInfo(GDSCsmall), \"cellInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"drugInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(treatmentInfo(GDSCsmall), \"drugInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"phenoInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(phenoInfo(GDSCsmall, \"rna\"), \"phenoInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"molecularProfiles result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(molecularProfiles(GDSCsmall, \"rna\"), \"molecularProfiles.GDSCsmall.rds\")\n})\n\ntest_that(\"featureInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(featureInfo(GDSCsmall, \"rna\"), \"featureInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"sensitivityInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sensitivityInfo(GDSCsmall), \"sensitivityInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"sensitivityProfiles result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sensitivityProfiles(GDSCsmall), \"sensitivityProfiles.GDSCsmall.rds\")\n})\n\n\ntest_that(\"sensitivityMeasures result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sensitivityMeasures(GDSCsmall), \"sensitivityMeasures.GDSCsmall.rds\")\n})\n\n\ntest_that(\"drugNames result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(treatmentNames(GDSCsmall), \"drugNames.GDSCsmall.rds\")\n})\n\ntest_that(\"cellNames result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sampleNames(GDSCsmall), \"cellNames.GDSCsmall.rds\")\n})\n\n\ntest_that(\"fNames result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(fNames(GDSCsmall, \"rna\"), \"fNames.GDSCsmall.rds\")\n})\n\n\ntest_that(\"name result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal(name(GDSCsmall), \"GDSC\")\n})\n\ntest_that(\"updateSampleId works without duplicates\", {\n\tdata(GDSCsmall)\n\tnewNames <- c(\"Test\",\"Test2\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n\n\tsampleNames(GDSCsmall) <- newNames\n\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n\texpect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(newNames))\n\texpect_equal(sort(rownames(sensNumber(GDSCsmall))), sort(newNames))\n\n})\n\n\ntest_that(\"updateSampleId works with duplicates\", {\n\tdata(GDSCsmall)\n\tnewNames <- c(\"Test\",\"Test\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n\n\texpect_warning(sampleNames(GDSCsmall) <- newNames, \"Duplicated ids passed to updateSampleId. Merging old ids into the same identifier\")\n\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n\texpect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(unique(newNames)))\n\texpect_equal(sort(rownames(sensNumber(GDSCsmall))), sort(unique(newNames)))\n\n})\n\n\n\ntest_that(\"updateTreatmentId works without duplicates\", {\n\tdata(GDSCsmall)\n\tnewNames <- c(\"Test\",\"Test2\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n\ttreatmentNames(GDSCsmall) <- newNames\n\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n\texpect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(newNames))\n\texpect_equal(sort(colnames(sensNumber(GDSCsmall))), sort(newNames))\n\n})\n\n\ntest_that(\"updateTreatmentId works with duplicates\", {\n    data(GDSCsmall)\n    newNames <- c(\"Test\",\"Test\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n    expect_warning(treatmentNames(GDSCsmall) <- newNames,\n\t\t\"Duplicated ids passed to updateTreatmentId. Merging old ids into the same identifier\")\n\n    expect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n    expect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(unique(newNames)))\n    expect_equal(sort(colnames(sensNumber(GDSCsmall))), sort(unique(newNames)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'expect_equal_to_reference' function used throughout this code snippet?",
        "answer": "The 'expect_equal_to_reference' function is used to compare the output of various PharmacoSet class methods with previously saved reference data. This ensures that the results of these methods have not changed since the last time the tests were run, maintaining consistency in the package's functionality."
      },
      {
        "question": "How does the code handle the case of duplicate sample IDs when updating sample names?",
        "answer": "When updating sample names with duplicates, the code issues a warning message: 'Duplicated ids passed to updateSampleId. Merging old ids into the same identifier'. It then proceeds to update the sample names, merging the data for duplicate IDs. This is tested in the 'updateSampleId works with duplicates' test case, where it checks if the updated sample names are correctly reflected in various components of the PharmacoSet object."
      },
      {
        "question": "What is the significance of the 'GDSCsmall' dataset used in these tests?",
        "answer": "The 'GDSCsmall' dataset is a sample PharmacoSet object used throughout the test cases. It is loaded at the beginning of each test using the 'data(GDSCsmall)' command. This dataset provides a consistent and known set of data to test various methods and functions of the PharmacoGx package, ensuring that the package's functionality works correctly with real-world data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"updateSampleId works without duplicates\", {\n  data(GDSCsmall)\n  newNames <- c(\"Test\",\"Test2\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n  sampleNames(GDSCsmall) <- newNames\n\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(newNames))\n  # Complete the test by adding an expectation for sensNumber\n})",
        "complete": "test_that(\"updateSampleId works without duplicates\", {\n  data(GDSCsmall)\n  newNames <- c(\"Test\",\"Test2\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n  sampleNames(GDSCsmall) <- newNames\n\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(newNames))\n  expect_equal(sort(rownames(sensNumber(GDSCsmall))), sort(newNames))\n})"
      },
      {
        "partial": "test_that(\"updateTreatmentId works with duplicates\", {\n    data(GDSCsmall)\n    newNames <- c(\"Test\",\"Test\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n    expect_warning(treatmentNames(GDSCsmall) <- newNames,\n        \"Duplicated ids passed to updateTreatmentId. Merging old ids into the same identifier\")\n\n    expect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n    expect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(unique(newNames)))\n    # Complete the test by adding an expectation for sensNumber\n})",
        "complete": "test_that(\"updateTreatmentId works with duplicates\", {\n    data(GDSCsmall)\n    newNames <- c(\"Test\",\"Test\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n    expect_warning(treatmentNames(GDSCsmall) <- newNames,\n        \"Duplicated ids passed to updateTreatmentId. Merging old ids into the same identifier\")\n\n    expect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n    expect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(unique(newNames)))\n    expect_equal(sort(colnames(sensNumber(GDSCsmall))), sort(unique(newNames)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/GR.R",
    "language": "R",
    "content": ".GR <- function(x, pars, tau) {\n  #GR takes in a vector of log concentrations, a vector of DRC parameters from the .Hill()\n  #function, and a coefficient tau equal to the number of doubling times occuring between\n  #the start of the experiment and the taking of the viability measurements. It then returns\n  #the GR-value associated with those conditions.\n  return((.Hill(x, pars)) ^ (1 / tau))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .GR function and what are its input parameters?",
        "answer": "The .GR function calculates the GR-value (Growth Rate value) based on given conditions. It takes three input parameters: 'x' (a vector of log concentrations), 'pars' (a vector of DRC parameters from the .Hill() function), and 'tau' (a coefficient representing the number of doubling times occurring between the start of the experiment and the viability measurements)."
      },
      {
        "question": "How does the .GR function calculate the GR-value?",
        "answer": "The .GR function calculates the GR-value by first calling the .Hill() function with the input parameters 'x' and 'pars'. It then raises the result to the power of (1 / tau). The formula used is: GR-value = (.Hill(x, pars)) ^ (1 / tau)."
      },
      {
        "question": "What is the significance of the 'tau' parameter in the .GR function?",
        "answer": "The 'tau' parameter in the .GR function represents the number of doubling times occurring between the start of the experiment and the taking of the viability measurements. It is used as an exponent in the calculation (1 / tau) to adjust the result of the .Hill() function, likely to account for the growth rate over the specified time period."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".GR <- function(x, pars, tau) {\n  # Function description\n  return((___(x, pars)) ^ (1 / tau))\n}",
        "complete": ".GR <- function(x, pars, tau) {\n  # GR calculates growth rate value based on Hill function\n  return((.Hill(x, pars)) ^ (1 / tau))\n}"
      },
      {
        "partial": ".GR <- function(x, pars, tau) {\n  # Calculate GR value\n  hill_result <- ___\n  return(hill_result ^ (1 / tau))\n}",
        "complete": ".GR <- function(x, pars, tau) {\n  # Calculate GR value\n  hill_result <- .Hill(x, pars)\n  return(hill_result ^ (1 / tau))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/setup.py",
    "language": "py",
    "content": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `with open('README.md', 'r') as fh:` block in this setup script?",
        "answer": "This block reads the contents of the README.md file and assigns it to the `long_description` variable. The long description is typically used to provide detailed information about the package, which is then displayed on package repositories like PyPI. It allows package authors to include comprehensive documentation directly from their README file."
      },
      {
        "question": "How does this setup script handle package dependencies, and what are the optional dependency groups defined?",
        "answer": "The script reads package dependencies from a 'requirements.txt' file using `with open('requirements.txt', 'r') as fh:`. These are set as the main install requirements. Additionally, it defines optional dependency groups using the `extras_require` parameter. Two groups are defined: 'debug' (which includes 'pyvis') and 'torch' (which includes 'torch' and 'torchio'). Users can install these optional dependencies by specifying the group name when installing the package."
      },
      {
        "question": "What command-line tools does this package provide, and how are they defined in the setup script?",
        "answer": "The package provides two command-line tools: 'autopipeline' and 'betapipeline'. These are defined in the `entry_points` parameter of the setup function, under the 'console_scripts' key. 'autopipeline' maps to the `main` function in the `imgtools.autopipeline` module, while 'betapipeline' maps to the `main` function in the `imgtools.autopipeline_refactored` module. This allows users to run these tools directly from the command line after installing the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        # Add classifiers here\n    ],\n    python_requires='>=3.7',\n)",
        "complete": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)"
      },
      {
        "partial": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        # Add extras_require here\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)",
        "complete": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "setuptools.setup"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-general.R",
    "language": "R",
    "content": "#' simple wrapper for the data.table::as.data.table() function\n#' @param x object to convert to a data.table\n#' @param ... additional arguments to pass to data.table::as.data.table()\n#' @return a data.table\n#' @keywords internal\n#' @noRd\n.asDT <- function(x, ...) data.table::as.data.table(x, ...)\n\n#' Parses the query response into a data table\n#'\n#' This function takes a query response and converts it into a data table using the `as.data.table` function from the `data.table` package.\n#'\n#' @param resp The query response to be parsed\n#' @return A data table containing the parsed query response\n#'\n#' @noRd\n#' @keywords internal\n.parseQueryToDT <- function(resp) {\n  data.table::as.data.table(resp[[1]][[1]])\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.asDT` function in the given code snippet?",
        "answer": "The `.asDT` function is a simple wrapper for the `data.table::as.data.table()` function. It takes an object `x` and any additional arguments, then converts `x` to a data.table using the `as.data.table()` function from the data.table package. This wrapper function provides a convenient way to convert objects to data.tables within the package."
      },
      {
        "question": "Explain the purpose and functionality of the `.parseQueryToDT` function.",
        "answer": "The `.parseQueryToDT` function is designed to parse a query response into a data table. It takes a `resp` parameter, which is expected to be a query response object. The function extracts the first element of the first element of `resp` using double square bracket notation (`resp[[1]][[1]]`), and then converts this extracted data into a data.table using the `data.table::as.data.table()` function. This function is likely used to standardize the format of query responses for further processing or analysis within the package."
      },
      {
        "question": "What do the `@noRd` and `@keywords internal` tags indicate in the function documentation?",
        "answer": "The `@noRd` and `@keywords internal` tags in the function documentation are Roxygen2 tags used for R package development. `@noRd` stands for 'no Rd file' and indicates that no separate documentation file should be generated for this function. `@keywords internal` marks the function as internal, meaning it's not intended for direct use by package users. These tags suggest that both `.asDT` and `.parseQueryToDT` are helper functions meant for internal use within the package rather than part of the public API."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".asDT <- function(x, ...) {\n  # Complete the function body\n}",
        "complete": ".asDT <- function(x, ...) data.table::as.data.table(x, ...)"
      },
      {
        "partial": ".parseQueryToDT <- function(resp) {\n  # Complete the function body\n}",
        "complete": ".parseQueryToDT <- function(resp) {\n  data.table::as.data.table(resp[[1]][[1]])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_metadata.py",
    "language": "py",
    "content": "import pytest\nimport os\n\nfrom readii.metadata import (\n    matchCTtoSegmentation,\n    getSegmentationType,\n    saveDataframeCSV,\n    getCTWithSegmentation\n)\n\n@pytest.fixture\ndef nsclcSummaryFilePath():\n    return \"tests/.imgtools/imgtools_NSCLC_Radiogenomics.csv\"\n\n@pytest.fixture\ndef lung4DSummaryFilePath():\n    return \"tests/.imgtools/imgtools_4D-Lung.csv\"\n\n@pytest.fixture\ndef lung4DEdgesSummaryFilePath():\n    return \"tests/.imgtools/imgtools_4D-Lung_edges.csv\"\n\n\ndef test_matchCTtoSEG(nsclcSummaryFilePath):\n    \"\"\"Test generating matched summary file for CT and DICOM SEG\"\"\"\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\")\n    assert len(actual) == 1, \\\n        \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.4334.1501.312037286778380630549945195741', \\\n        \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \\\n        \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'SEG', \\\n        \"Incorrect segmentation type has been found\"\n\n\ndef test_matchCTtoRTSTRUCT(lung4DSummaryFilePath):\n    \"\"\"Test generating matched summary file for CT and RTSTRUCT\"\"\"\n    actual = matchCTtoSegmentation(lung4DSummaryFilePath, \n                                   segType = \"RTSTRUCT\")\n    assert len(actual) == 1, \\\n        \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.6834.5010.339023390306606021995936229543', \\\n        \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \\\n        \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'RTSTRUCT', \\\n        \"Incorrect segmentation type has been found\"\n\n\ndef test_matchCTtoSegmentation_output(nsclcSummaryFilePath):\n    \"\"\"Test saving output of summary file\"\"\"\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\",\n                                   outputFilePath = \"tests/output/ct_to_seg_match_list_NSCLC_Radiogenomics.csv\")\n    assert os.path.exists(\"tests/output/ct_to_seg_match_list_NSCLC_Radiogenomics.csv\") == True, \\\n        \"Output does not exist, double check output file is named correctly.\"\n\n\ndef test_getCTWithRTTRUCT(lung4DEdgesSummaryFilePath):\n    \"\"\"Test getting CTs with RTSTRUCT segmentation from edges file\"\"\"\n    actual = getCTWithSegmentation(lung4DEdgesSummaryFilePath, \n                                   segType = \"RTSTRUCT\")\n    assert len(actual) == 1, \\\n        \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.6834.5010.339023390306606021995936229543', \\\n        \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \\\n        \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'RTSTRUCT', \\\n        \"Incorrect segmentation type has been found\"\n\n\ndef test_getCTtoSegmentation_output(lung4DEdgesSummaryFilePath):\n    \"\"\"Test saving output of summary file\"\"\"\n    actual = getCTWithSegmentation(lung4DEdgesSummaryFilePath, \n                                   segType = \"RTSTRUCT\",\n                                   outputFilePath = \"tests/output/ct_to_seg_match_list_4D-Lung.csv\")\n    assert os.path.exists(\"tests/output/ct_to_seg_match_list_4D-Lung.csv\") == True, \\\n        \"Output does not exist, double check output file is named correctly.\"\n\n\n@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_matchCTtoSegmentation_error(nsclcSummaryFilePath, wrongSeg):\n    \"\"\"Check ValueError is raised when incorrect segType is passed\"\"\"\n    with pytest.raises(ValueError):\n        matchCTtoSegmentation(nsclcSummaryFilePath,\n                              segType = wrongSeg)\n\n\ndef test_saveDataframeCSV_outputFilePath_error(nsclcSummaryFilePath):\n    \"\"\"Check ValueError is raised when incorrect outputFilePath is passed\"\"\"\n    testDataframe = matchCTtoSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\")\n    badFilePath = \"notacsv.xlsx\"\n    with pytest.raises(ValueError):\n        saveDataframeCSV(testDataframe, badFilePath)\n\n@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_getCTWithRTSTRUCT_error(lung4DEdgesSummaryFilePath, wrongSeg):\n    \"\"\"Check ValueError is raised when incorrect segType is passed\"\"\"\n    with pytest.raises(ValueError):\n        getCTWithSegmentation(lung4DEdgesSummaryFilePath, \n                                   segType = wrongSeg)\n\ndef test_getCTWithSEG_error(nsclcSummaryFilePath):\n    \"\"\"Check ValueError is raised when incorrect segType is passed\"\"\"\n    with pytest.raises(ValueError):\n        getCTWithSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\")\n\n\n@pytest.mark.parametrize(\n    \"notADataFrame\",\n    [\n        ['list', 'of', 'features'],\n        \"Just a string\",\n        {\"feat1\": 34, \"feat2\": 10000, \"feat3\": 3.141592}\n    ]\n)\ndef test_saveDataframeCSV_dataframe_error(notADataFrame):\n    \"\"\"Check ValueError is raised when something other than pd.DataFrame is passed\"\"\"\n    goodFilePath = \"tests/output/badDataframeExample.csv\"\n    with pytest.raises(ValueError):\n        saveDataframeCSV(notADataFrame, goodFilePath)\n\n\ndef test_getSegmentationType_SEG(nsclcSummaryFilePath):\n    \"\"\"Test getting segmentation type from summary file with SEG and CT\"\"\"\n    actual = getSegmentationType(nsclcSummaryFilePath)\n    assert actual == \"SEG\", \\\n        \"Wrong segmentation type found\"\n\n\ndef test_getSegmentationType_RTSTRUCT(lung4DSummaryFilePath):\n    \"\"\"Test getting segmentation type from summary file with RTSTRUCT and CT\"\"\"\n    actual = getSegmentationType(lung4DSummaryFilePath)\n    assert actual == \"RTSTRUCT\", \\\n        \"Wrong segmentation type found\"\n\n\n@pytest.mark.parametrize(\n    \"notACSV\",\n    [\n        \"tests/.imgtools/imgtools_NSCLC_Radiogenomics.json\",\n        \"Just a string\",\n        \"tests/.imgtools/imgtools_4D-Lung.json\"\n    ]\n)\ndef test_getSegmentation_dataframe_error(notACSV):\n    \"\"\"Check ValueError is raised when something other than a csv file is passed\"\"\"\n    with pytest.raises(ValueError):\n        getSegmentationType(notACSV)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `matchCTtoSegmentation` function and how does it handle different segmentation types?",
        "answer": "The `matchCTtoSegmentation` function is designed to match CT images with their corresponding segmentation data. It can handle two types of segmentations: 'SEG' (DICOM Segmentation) and 'RTSTRUCT' (Radiotherapy Structure Set). The function takes a summary file path and a segmentation type as input, and returns a DataFrame containing matched CT and segmentation information. It ensures that the segmentation's reference CT ID matches the CT series ID and verifies the correct segmentation type is found."
      },
      {
        "question": "How does the code handle error cases in the `matchCTtoSegmentation` and `getCTWithSegmentation` functions?",
        "answer": "The code uses pytest's parametrize decorator to test multiple error cases. For both `matchCTtoSegmentation` and `getCTWithSegmentation` functions, it checks if a ValueError is raised when an incorrect segmentation type is passed. The test cases include invalid segmentation types like 'CT', 'Nonsense', and an empty string. This ensures that the functions properly validate their input parameters and raise appropriate exceptions when given invalid data."
      },
      {
        "question": "What is the purpose of the `getSegmentationType` function and how is it tested?",
        "answer": "The `getSegmentationType` function is used to determine the type of segmentation (either 'SEG' or 'RTSTRUCT') present in a given summary file. It is tested using two separate test functions: `test_getSegmentationType_SEG` and `test_getSegmentationType_RTSTRUCT`. These tests check if the function correctly identifies the segmentation type for different input files. Additionally, there's an error case test `test_getSegmentation_dataframe_error` that ensures the function raises a ValueError when given an invalid file format (non-CSV)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_matchCTtoSEG(nsclcSummaryFilePath):\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, segType=\"SEG\")\n    assert len(actual) == 1\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.4334.1501.312037286778380630549945195741'\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0]\n    # Complete the assertion for modality_seg",
        "complete": "def test_matchCTtoSEG(nsclcSummaryFilePath):\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, segType=\"SEG\")\n    assert len(actual) == 1, \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.4334.1501.312037286778380630549945195741', \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'SEG', \"Incorrect segmentation type has been found\""
      },
      {
        "partial": "@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_matchCTtoSegmentation_error(nsclcSummaryFilePath, wrongSeg):\n    # Complete the function body",
        "complete": "@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_matchCTtoSegmentation_error(nsclcSummaryFilePath, wrongSeg):\n    with pytest.raises(ValueError):\n        matchCTtoSegmentation(nsclcSummaryFilePath, segType=wrongSeg)"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest",
        "os"
      ],
      "from_imports": [
        "readii.metadata.matchCTtoSegmentation"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/RcppExports.R",
    "language": "R",
    "content": "# Generated by using Rcpp::compileAttributes() -> do not edit by hand\n# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393\n\n#' QUICKSTOP significance testing for partial correlation\n#'\n#' This function will test whether the observed partial correlation is significant\n#' at a level of req_alpha, doing up to MaxIter permutations. Currently, it\n#' supports only grouping by discrete categories when calculating a partial correlation.\n#' Currenlty, only does two sided tests.\n#'\n#' @param pin_x one of the two vectors to correlate.\n#' @param pin_y the other vector to calculate\n#' @param pobsCor the observed (partial) correlation between these varaiables\n#' @param pGroupFactor an integer vector labeling group membership, to correct\n#' for in the partial correlation. NEEDS TO BE ZERO BASED!\n#' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n#' the number of members of each group (basically table(pGroupFactor)) as integer vector\n#' @param pnumGroup how many groups are there (len(pGroupSize))\n#' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n#' @param pn length of x and y, as a REAL NUMBER\n#' @param preq_alpha the required alpha for significance\n#' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n#' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n#' per quickstop paper (used to determine the log-odds)\n#' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n#' random number generator. Note that currently, these values get modified per call, so pass in a copy\n#' if you wish to keep a seed for running same simulation twice\n#'\n#' @return a double vector of length 4, entry 1 is either 0, 1 (for TRUE/FALSE) or NA_REAL_ for significance determination\n#' NA_REAL_ is returned when the MaxIter were reached before a decision is made. Usually, this occurs when the real p value is close to, or\n#' falls within the tolerance region of (req_alpha, req_alpha+tolerance_par). Entry 2 is the current p value estimate. entry 3 is the total\n#' number of iterations performed. Entry 4 is the number of time a permuted value was larger in absolute value than the observed cor.\n#'\n#' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP\n#'\n#'\npartialCorQUICKSTOP <- function(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed) {\n    .Call('_PharmacoGx_partialCorQUICKSTOP', PACKAGE = 'PharmacoGx', pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `partialCorQUICKSTOP` function and what statistical test does it perform?",
        "answer": "The `partialCorQUICKSTOP` function performs significance testing for partial correlation. It tests whether the observed partial correlation is significant at a specified alpha level using permutation testing. The function implements the QUICKSTOP algorithm, which allows for early stopping of the permutation test when significance can be determined with high confidence. It specifically supports grouping by discrete categories when calculating partial correlations and currently only performs two-sided tests."
      },
      {
        "question": "What are the key parameters of the `partialCorQUICKSTOP` function and their purposes?",
        "answer": "The key parameters of the `partialCorQUICKSTOP` function include:\n1. `pin_x` and `pin_y`: The two vectors to correlate.\n2. `pobsCor`: The observed (partial) correlation between the variables.\n3. `pGroupFactor`: An integer vector labeling group membership (zero-based).\n4. `pGroupSize`: An integer vector counting the number of members in each group.\n5. `pMaxIter`: Maximum number of iterations to perform.\n6. `preq_alpha`: The required alpha for significance.\n7. `ptolerance_par`: The tolerance region for quickstop.\n8. `plog_decision_boundary`: Log of 1/probability of incorrectly calling significance.\n9. `pseed`: A numeric vector of length 2 for seeding the random number generator."
      },
      {
        "question": "What does the function return, and how should the output be interpreted?",
        "answer": "The function returns a double vector of length 4:\n1. Entry 1: 0 (FALSE), 1 (TRUE), or NA_REAL_ for significance determination. NA_REAL_ indicates that MaxIter was reached before a decision was made.\n2. Entry 2: The current p-value estimate.\n3. Entry 3: The total number of iterations performed.\n4. Entry 4: The number of times a permuted value was larger in absolute value than the observed correlation.\n\nThe significance determination (entry 1) is based on the QUICKSTOP algorithm, which may terminate early if significance can be confidently determined before reaching MaxIter iterations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "partialCorQUICKSTOP <- function(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed) {\n    # Complete the function body\n}",
        "complete": "partialCorQUICKSTOP <- function(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed) {\n    .Call('_PharmacoGx_partialCorQUICKSTOP', PACKAGE = 'PharmacoGx', pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed)\n}"
      },
      {
        "partial": "#' QUICKSTOP significance testing for partial correlation\n#'\n#' @param pin_x one of the two vectors to correlate.\n#' @param pin_y the other vector to calculate\n#' @param pobsCor the observed (partial) correlation between these varaiables\n#' @param pGroupFactor an integer vector labeling group membership, to correct\n#' for in the partial correlation. NEEDS TO BE ZERO BASED!\n#' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n#' the number of members of each group (basically table(pGroupFactor)) as integer vector\n#' @param pnumGroup how many groups are there (len(pGroupSize))\n#' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n#' @param pn length of x and y, as a REAL NUMBER\n#' @param preq_alpha the required alpha for significance\n#' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n#' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n#' per quickstop paper (used to determine the log-odds)\n#' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n#' random number generator. Note that currently, these values get modified per call, so pass in a copy\n#' if you wish to keep a seed for running same simulation twice\n#'\n#' @return # Complete the return description\n#'\n#' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP",
        "complete": "#' QUICKSTOP significance testing for partial correlation\n#'\n#' @param pin_x one of the two vectors to correlate.\n#' @param pin_y the other vector to calculate\n#' @param pobsCor the observed (partial) correlation between these varaiables\n#' @param pGroupFactor an integer vector labeling group membership, to correct\n#' for in the partial correlation. NEEDS TO BE ZERO BASED!\n#' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n#' the number of members of each group (basically table(pGroupFactor)) as integer vector\n#' @param pnumGroup how many groups are there (len(pGroupSize))\n#' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n#' @param pn length of x and y, as a REAL NUMBER\n#' @param preq_alpha the required alpha for significance\n#' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n#' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n#' per quickstop paper (used to determine the log-odds)\n#' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n#' random number generator. Note that currently, these values get modified per call, so pass in a copy\n#' if you wish to keep a seed for running same simulation twice\n#'\n#' @return a double vector of length 4, entry 1 is either 0, 1 (for TRUE/FALSE) or NA_REAL_ for significance determination\n#' NA_REAL_ is returned when the MaxIter were reached before a decision is made. Usually, this occurs when the real p value is close to, or\n#' falls within the tolerance region of (req_alpha, req_alpha+tolerance_par). Entry 2 is the current p value estimate. entry 3 is the total\n#' number of iterations performed. Entry 4 is the number of time a permuted value was larger in absolute value than the observed cor.\n#'\n#' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/utils/logging_config.py",
    "language": "py",
    "content": "import logging\nimport logging.config\nimport os\nfrom typing import Optional\n\nBASE_LOGGING: dict = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        # 'json': {\n        #     'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',\n        #     'format': '%(asctime)s %(name)s %(levelname)s %(module)s %(message)s %(pathname)s %(lineno)s %(funcName)s %(threadName)s %(thread)s %(process)s %(processName)s',  # noqa: E501\n        #     'datefmt': '%Y-%m-%d %H:%M:%S',\n        # },\n        'stdout': {\n            'format': '%(asctime)s %(levelname)s: %(message)s (%(module)s:%(funcName)s:%(lineno)d)',\n            'datefmt': '%Y-%m-%d %H:%M:%S',\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'stdout',\n        },\n    },\n    'loggers': {\n        'devel': {\n            'handlers': ['console'],\n            'level': os.getenv('READII_VERBOSITY', 'INFO'),\n            'propagate': True,\n        },\n    },\n}\n\n\ndef setup_logger(\n    logger_name: str = 'root',\n    config: Optional[dict] = None,\n    extra_handlers: Optional[list] = None,\n  ) -> logging.Logger:\n    \"\"\"Set up a logger with optional custom configuration and log file.\"\"\"\n    valid_loggers = ['devel', 'prod', 'root']\n    assert (\n        logger_name in valid_loggers\n    ), f'Invalid logger name. Available options are {valid_loggers}'\n    \n    # Merge the base config with any custom config\n    logging_config = BASE_LOGGING.copy()\n    if config:\n        logging_config.update(config)\n    logging.config.dictConfig(logging_config)\n\n    logger = logging.getLogger(logger_name)\n\n    # Add any extra handlers provided\n    if extra_handlers:\n        for handler in extra_handlers:\n            logger.addHandler(handler)\n\n    return logger\n\ndef get_logger(config: Optional[dict] = None) -> logging.Logger:\n    \"\"\"Retrieve logger based on the environment, with an optional configuration.\"\"\"\n    env = os.getenv('READII_ENV', 'development')\n\n    logger_name = 'devel' if env in ['devel', 'development'] else 'prod'\n    return setup_logger(logger_name=logger_name, config=config)\n\n# Example usage\nif __name__ == \"__main__\":\n    logger = get_logger()\n    logger.info(\"This is an informational message.\")\n    logger.debug(\"This is a debug message.\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `BASE_LOGGING` dictionary in the given code snippet?",
        "answer": "The `BASE_LOGGING` dictionary defines the basic configuration for the logging system. It specifies the logging format, handlers, and default logger settings. This dictionary serves as a template for setting up loggers with consistent formatting and behavior across the application."
      },
      {
        "question": "Explain the function of the `setup_logger` function and its parameters.",
        "answer": "The `setup_logger` function is responsible for creating and configuring a logger. It takes three parameters: `logger_name` (default 'root'), `config` (optional custom configuration), and `extra_handlers` (optional additional log handlers). The function merges the base configuration with any custom config, sets up the logger using `logging.config.dictConfig`, and adds any extra handlers provided. It also validates the logger name against a list of valid options."
      },
      {
        "question": "How does the `get_logger` function determine which logger to use, and what is its purpose?",
        "answer": "The `get_logger` function determines the logger to use based on the `READII_ENV` environment variable. If the environment is 'devel' or 'development', it uses the 'devel' logger; otherwise, it uses the 'prod' logger. This function serves as a convenient way to retrieve the appropriate logger for the current environment, allowing for different logging configurations in development and production settings."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [
        "logging",
        "logging.config",
        "os"
      ],
      "from_imports": [
        "typing.Optional"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeAUC.R",
    "language": "R",
    "content": "#' Computes the AUC for a Drug Dose Viability Curve\n#' \n#' Returns the AUC (Area Under the drug response Curve) given concentration and viability as input, normalized by the concentration\n#' range of the experiment. The area returned is the response (1-Viablility) area, i.e. area under the curve when the response curve \n#' is plotted on a log10 concentration scale, with high AUC implying high sensitivity to the drug. The function can calculate both \n#' the area under a fitted Hill Curve to the data, and a trapz numeric integral of the actual data provided. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known. \n#' \n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8) \n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeAUC(dose, viability)\n#' \n#' \n#' @param concentration `numeric` is a vector of drug concentrations.\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells. \n#' @param Hill_fit `list` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope \n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal, \n#' and EC50 as a concentration. \n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and returns AUC as a decimal. Otherwise, viability is interpreted as percent, and AUC is returned 0-100.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param area.type Should the area be computed using the actual data (\"Actual\"), or a fitted curve (\"Fitted\")\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return Numeric AUC value\n#' \n#' @export\n#' @import caTools\ncomputeAUC <- function (concentration,\n   viability,\n   Hill_fit,\n   conc_as_log = FALSE,\n   viability_as_pct = TRUE,\n   trunc = TRUE,\n   area.type = c(\"Fitted\", \"Actual\"),\n   verbose = TRUE\n   #, ...\n   ) {\n\n  if (missing(concentration)) {\n\n    stop(\"The concentration values to integrate over must always be provided.\")\n\n  }\nif (missing(area.type)) {\n    area.type <- \"Fitted\"\n} else {\n    area.type <- match.arg(area.type)\n}\nif (area.type == \"Fitted\" && missing(Hill_fit)) {\n\n    Hill_fit <- logLogisticRegression(concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n} else if (area.type == \"Fitted\" && !missing(Hill_fit)) {\n\n  cleanData <- sanitizeInput(conc = concentration,\n    viability = viability,\n    Hill_fit = Hill_fit,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n} else if (area.type == \"Actual\" && !missing(viability)){\n  cleanData <- sanitizeInput(conc = concentration,\n     viability = viability,\n     conc_as_log = conc_as_log,\n     viability_as_pct = viability_as_pct,\n     trunc = trunc,\n     verbose = verbose)\n  concentration <- cleanData[[\"log_conc\"]]\n  viability <- cleanData[[\"viability\"]]\n} else if (area.type == \"Actual\" && missing(viability)) {\n\n  stop(\"To calculate the actual area using a trapezoid integral, the raw viability values are needed!\")\n}\n\nif (length(concentration) < 2) {\n  return(NA)\n}\n\na <- min(concentration)\nb <- max(concentration)\nif (area.type == \"Actual\") {\n  trapezoid.integral <- caTools::trapz(concentration, viability)\n  AUC <- 1 - trapezoid.integral / (b - a)\n}\nelse {\n    if (pars[2] == 1) {\n        AUC <- 0\n    } else if (pars[1] == 0){\n        AUC <- (1 - pars[2]) / 2\n    } else {\n        AUC <- as.numeric(\n        (1 - pars[2]) / (pars[1] * (b - a)) *\n        log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) /\n            (1 + (10 ^ (a - pars[3])) ^ pars[1])))\n    }\n}\n\nif(viability_as_pct){\n\n  AUC <- AUC*100\n\n}\n\nreturn(AUC)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC` function and what are its main input parameters?",
        "answer": "The `computeAUC` function computes the Area Under the Curve (AUC) for a Drug Dose Viability Curve. It calculates the response area (1-Viability) when plotted on a log10 concentration scale, with a high AUC implying high drug sensitivity. The main input parameters are:\n1. `concentration`: a vector of drug concentrations\n2. `viability`: a vector of viability values corresponding to the concentrations\n3. `Hill_fit`: optional parameters of a Hill Slope\n4. `conc_as_log`: boolean indicating if concentration is given in log scale\n5. `viability_as_pct`: boolean indicating if viability is given as a percentage\n6. `area.type`: specifies whether to use actual data or a fitted curve for calculation"
      },
      {
        "question": "How does the function handle different types of input data and what data preprocessing steps are performed?",
        "answer": "The function handles different types of input data through several preprocessing steps:\n1. It checks if concentration data is provided, which is mandatory.\n2. It uses the `sanitizeInput` function to clean and standardize the input data.\n3. If `conc_as_log` is FALSE, it converts concentration to log scale.\n4. If `viability_as_pct` is TRUE, it converts viability from percentage to decimal.\n5. If `trunc` is TRUE, it truncates viability data to be between 0 and 1.\n6. It handles both raw data ('Actual' area.type) and fitted curve data ('Fitted' area.type).\n7. For 'Fitted' area.type, it either uses provided Hill_fit parameters or calculates them using `logLogisticRegression` function."
      },
      {
        "question": "How is the AUC calculated differently for 'Actual' and 'Fitted' area types, and what is the significance of the calculation method?",
        "answer": "The AUC calculation differs for 'Actual' and 'Fitted' area types:\n\n1. For 'Actual' area type:\n   - Uses the `trapz` function from the caTools package to perform a trapezoidal integration of the raw data.\n   - Calculates AUC as: 1 - (trapezoid integral / concentration range)\n\n2. For 'Fitted' area type:\n   - Uses the Hill Slope parameters (slope, E_inf, EC50) to calculate the AUC analytically.\n   - Handles special cases where the slope is 0 or E_inf is 1.\n   - For normal cases, uses a logarithmic formula to calculate the area.\n\nThe significance of these methods:\n- 'Actual' provides a direct measure of the observed data but may be sensitive to noise.\n- 'Fitted' smooths out noise and can interpolate between data points, but assumes the data follows a Hill Slope model.\n\nThe choice between methods depends on the data quality and the specific requirements of the analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC <- function (concentration,\n   viability,\n   Hill_fit,\n   conc_as_log = FALSE,\n   viability_as_pct = TRUE,\n   trunc = TRUE,\n   area.type = c(\"Fitted\", \"Actual\"),\n   verbose = TRUE\n   ) {\n\n  if (missing(concentration)) {\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(area.type)) {\n    area.type <- \"Fitted\"\n  } else {\n    area.type <- match.arg(area.type)\n  }\n  \n  # TODO: Implement the main logic for AUC calculation\n  \n  # Return the calculated AUC\n  return(AUC)\n}",
        "complete": "computeAUC <- function (concentration,\n   viability,\n   Hill_fit,\n   conc_as_log = FALSE,\n   viability_as_pct = TRUE,\n   trunc = TRUE,\n   area.type = c(\"Fitted\", \"Actual\"),\n   verbose = TRUE\n   ) {\n\n  if (missing(concentration)) {\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(area.type)) {\n    area.type <- \"Fitted\"\n  } else {\n    area.type <- match.arg(area.type)\n  }\n  \n  cleanData <- sanitizeInput(conc = concentration,\n    viability = viability,\n    Hill_fit = Hill_fit,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  \n  concentration <- cleanData[[\"log_conc\"]]\n  if (area.type == \"Actual\") {\n    viability <- cleanData[[\"viability\"]]\n  } else {\n    pars <- cleanData[[\"Hill_fit\"]]\n  }\n  \n  if (length(concentration) < 2) return(NA)\n  \n  a <- min(concentration)\n  b <- max(concentration)\n  \n  if (area.type == \"Actual\") {\n    AUC <- 1 - caTools::trapz(concentration, viability) / (b - a)\n  } else {\n    if (pars[2] == 1) {\n      AUC <- 0\n    } else if (pars[1] == 0) {\n      AUC <- (1 - pars[2]) / 2\n    } else {\n      AUC <- (1 - pars[2]) / (pars[1] * (b - a)) *\n        log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) /\n          (1 + (10 ^ (a - pars[3])) ^ pars[1]))\n    }\n  }\n  \n  if (viability_as_pct) AUC <- AUC * 100\n  \n  return(AUC)\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n  # TODO: Implement input sanitization logic\n  \n  # Return a list with sanitized inputs\n  return(list(log_conc = log_conc,\n               viability = viability,\n               Hill_fit = Hill_fit))\n}",
        "complete": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n  if (!conc_as_log) {\n    log_conc <- log10(conc)\n  } else {\n    log_conc <- conc\n  }\n  \n  if (!missing(viability)) {\n    if (viability_as_pct) {\n      viability <- viability / 100\n    }\n    if (trunc) {\n      viability <- pmin(pmax(viability, 0), 1)\n    }\n  }\n  \n  if (!missing(Hill_fit)) {\n    if (viability_as_pct) {\n      Hill_fit[2] <- Hill_fit[2] / 100\n    }\n    if (!conc_as_log) {\n      Hill_fit[3] <- log10(Hill_fit[3])\n    }\n  }\n  \n  return(list(log_conc = log_conc,\n               viability = viability,\n               Hill_fit = Hill_fit))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_match-methods.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"unlistNested returns the correct result for a nested list\", {\n  nested_list <- list(list(1, 2), list(3, 4), list(5, 6))\n  expected_result <- c(1, 2, 3, 4, 5, 6)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with NA values\", {\n  nested_list <- list(list(1, NA), list(3, 4), list(NA, 6))\n  expected_result <- c(1, 3, 4, 6)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with duplicate values\", {\n  nested_list <- list(list(1, 2), list(2, 3), list(3, 4))\n  expected_result <- c(1, 2, 3, 4)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns an empty vector for an empty nested list\", {\n  nested_list <- list()\n  expected_result <- NULL\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with character elements\", {\n  nested_list <- list(list(\"a\", \"b\"), list(\"c\", \"d\"), list(\"e\", \"f\"))\n  expected_result <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with mixed data types\", {\n  nested_list <- list(list(1, \"a\"), list(TRUE, 2.5), list(\"b\", FALSE))\n  expected_result <- c(1, \"a\", TRUE, 2.5, \"b\", FALSE)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a deeply nested list\", {\n  nested_list <- list(list(list(1, 2), list(3, 4)), list(list(5, 6), list(7, 8)))\n  expected_result <- c(1, 2, 3, 4, 5, 6, 7, 8)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with NULL values\", {\n  nested_list <- list(list(NULL, 1), list(2, NULL), list(NULL, NULL))\n  expected_result <- c(1, 2)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with empty sublists\", {\n  nested_list <- list(list(), list(), list())\n  expected_result <- NULL\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with a single element\", {\n  nested_list <- list(list(42))\n  expected_result <- 42\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\n####################################################################################################\n# matchNested,list\n####################################################################################################\n\ntest_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(1, 2), list(3, 4), list(5, 6))\n  x <- 3\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\ntest_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(\"1\", \"2\"), list(\"3\", \"4\"), list(\"5\", \"6\"))\n  x <- \"3\"\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\n\ntest_that(\"matchNested,list returns the correct index for a nested list with NA values\", {\n  table <- list(list(1, NA), list(3, 4), list(NA, 6))\n  x <- 4\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\ntest_that(\"matchNested,list returns the correct index for a nested list with duplicate values\", {\n  table <- list(list(1, 2), list(2, 3), list(3, 4))\n  x <- 2\n  expected_result <- 1\n  expected_result_dups <- c(1, 2)\n\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result_dups)\n\n  x <- 4\n  expected_result <- 3\n  expected_result_dups <- c(3)\n\n  expect_equal(matchNested(x, table), expected_result)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result_dups)\n})\n\n####################################################################################################\n# matchNested,data.table\n####################################################################################################\n# Test case 1: Matching a character value in a data.table\ntest_that(\"matchNested,data.table returns the correct index for a character value\", {\n  table <- data.table(col1 = c(\"apple\", \"banana\", \"orange\"), col2 = c(1, 2, 3))\n  x <- \"banana\"\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\n# Test case 2: Matching a character value in a data.table with NA values\ntest_that(\"matchNested,data.table returns the correct index for a character value with NA values\", {\n  table <- data.table(col1 = c(\"apple\", NA, \"orange\"), col2 = c(1, 2, 3))\n  x <- \"orange\"\n  expected_result <- 3\n  expect_equal(matchNested(x, table), expected_result)\n})\n\n# Test case 3: Matching a character value in a data.table with duplicate values\ntest_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(col1 = c(\"apple\", \"banana\", \"banana\"), col2 = c(1, 2, 3))\n  x <- \"banana\"\n  expected_result <- c(2, 3)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result)\n  \n  expected_result <- 2\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n  expect_equal(matchNested(x, table), expected_result)\n\n  idx <- matchNested(x, table, keep_duplicates = FALSE)\n\n  data.table::setkeyv(table, \"col1\")\n  matched <- table[idx]  \n\n  # make sure that x is in one of the columns\n  expect_true(any(matched$col1 == x | matched$col2 == x))\n})\n\ntest_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(\n    col1 = list(\n      list(\"apple\", \"banana\"), \n      list(\"mango\", \"orange\"), \n      list(\"banana\", \"orange\")), \n    col2 = c(1, \"banana\", 3)\n  )\n  x <- \"banana\"\n  expected_result <- c(1, 2, 3)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result)\n\n  expected_result <- 1\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n\n  x <- \"orange\"\n  expected_result <- c(2, 3)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result)\n\n  expected_result <- 2\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n\n})\n\n# Test case 4: Matching a character value in an empty data.table\ntest_that(\"matchNested,data.table returns NULL for an empty data.table\", {\n  table <- data.table()\n  x <- \"apple\"\n  expected_result <- NULL\n  expect_error(matchNested(x, table))\n})\n\ntest_that(\"matchNested returns the correct matches for character and data.frame inputs\", {\n  # Test case 1: Matching single character with data.frame\n  x1 <- \"apple\"\n  table1 <- data.frame(fruit = c(\"apple\", \"banana\", \"orange\"), color = c(\"red\", \"yellow\", \"orange\"))\n  result1 <- matchNested(x1, table1)\n  expect_equal(result1, 1)\n\n  # Test case 2: Matching multiple characters with data.frame\n  x2 <- c(\"apple\", \"banana\")\n  table2 <- data.frame(fruit = c(\"apple\", \"banana\", \"orange\"), color = c(\"red\", \"yellow\", \"orange\"))\n  expect_warning(result2 <-matchNested(x2, table2))\n\n  expect_equal(result2,  1)\n\n\n  x3 <- c(\"apple\", \"orange\")\n  expect_warning(result3 <- matchNested(x3, table2))\n  expect_equal(result3,  1)\n\n  x4 <- c(\"red\", \"yellow\")\n  expect_warning(result4 <- matchNested(x4, table2))\n  expect_equal(result4,  2)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `unlistNested` function based on the test cases provided?",
        "answer": "The `unlistNested` function is designed to flatten a nested list structure into a single vector. It removes NA and NULL values, eliminates duplicates, and can handle various data types including numeric, character, and logical values. The function works with lists of different depths and returns NULL for empty lists."
      },
      {
        "question": "How does the `matchNested` function behave differently when applied to a list versus a data.table?",
        "answer": "When applied to a list, `matchNested` searches for a value `x` within the nested structure and returns the index of the first matching sublist. For a data.table, it searches across all columns and can return multiple indices if `keep_duplicates = TRUE`. With a data.table, it can also handle more complex nested structures within columns."
      },
      {
        "question": "What is the significance of the `keep_duplicates` parameter in the `matchNested` function?",
        "answer": "The `keep_duplicates` parameter determines whether `matchNested` returns all matching indices or just the first one. When set to `TRUE`, it returns a vector of all indices where the value is found. When `FALSE` (default), it returns only the first matching index. This is particularly useful when dealing with data structures that may contain duplicate values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(1, 2), list(3, 4), list(5, 6))\n  x <- 3\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})",
        "complete": "test_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(1, 2), list(3, 4), list(5, 6))\n  x <- 3\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n\n  x <- 6\n  expected_result <- 3\n  expect_equal(matchNested(x, table), expected_result)\n\n  x <- 7\n  expect_equal(matchNested(x, table), NULL)\n})"
      },
      {
        "partial": "test_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(\n    col1 = list(\n      list(\"apple\", \"banana\"), \n      list(\"mango\", \"orange\"), \n      list(\"banana\", \"orange\")), \n    col2 = c(1, \"banana\", 3)\n  )\n  x <- \"banana\"\n  # Add test cases here\n})",
        "complete": "test_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(\n    col1 = list(\n      list(\"apple\", \"banana\"), \n      list(\"mango\", \"orange\"), \n      list(\"banana\", \"orange\")), \n    col2 = c(1, \"banana\", 3)\n  )\n  x <- \"banana\"\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), c(1, 2, 3))\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), 1)\n\n  x <- \"orange\"\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), c(2, 3))\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), 2)\n\n  x <- \"grape\"\n  expect_equal(matchNested(x, table), NULL)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/loaders.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport json\nimport glob\nimport re\nfrom typing import Optional\nfrom collections import namedtuple\n# import copy\n\nimport pandas as pd\nimport SimpleITK as sitk\nfrom pydicom import dcmread\n\n# from joblib import Parallel, delayed\n# from tqdm.auto import tqdm\n\nfrom ..modules import StructureSet, Dose, PET, Scan, Segmentation\nfrom ..utils.crawl import *\nfrom ..utils.dicomutils import *\n\n\ndef read_image(path):\n    return sitk.ReadImage(path)\n\n\ndef read_dicom_series(path: str,\n                      series_id: Optional[str] = None,\n                      recursive: bool = False, \n                      file_names: list = None):\n    \"\"\"Read DICOM series as SimpleITK Image.\n\n    Parameters\n    ----------\n    path\n       Path to directory containing the DICOM series.\n\n    recursive, optional\n       Whether to recursively parse the input directory when searching for\n       DICOM series,\n\n    series_id, optional\n       Specifies the DICOM series to load if multiple series are present in\n       the directory. If None and multiple series are present, loads the first\n       series found.\n\n    file_names, optional\n        If there are multiple acquisitions/\"subseries\" for an individual series,\n        use the provided list of file_names to set the ImageSeriesReader.\n\n    Returns\n    -------\n    The loaded image.\n\n    \"\"\"\n    reader = sitk.ImageSeriesReader()\n    if file_names is None:\n        file_names = reader.GetGDCMSeriesFileNames(path,\n                                                   seriesID=series_id if series_id else \"\",\n                                                   recursive=recursive)\n        # extract the names of the dicom files that are in the path variable, which is a directory\n    \n    reader.SetFileNames(file_names)\n    \n    # Configure the reader to load all of the DICOM tags (public+private):\n    # By default tags are not loaded (saves time).\n    # By default if tags are loaded, the private tags are not loaded.\n    # We explicitly configure the reader to load tags, including the\n    # private ones.\n    reader.MetaDataDictionaryArrayUpdateOn()\n    reader.LoadPrivateTagsOn()\n\n    return reader.Execute()\n\n    \ndef read_dicom_scan(path, series_id=None, recursive: bool=False, file_names=None) -> Scan:\n    image = read_dicom_series(path, series_id=series_id, recursive=recursive, file_names=file_names)\n    return Scan(image, {})\n\n\ndef read_dicom_rtstruct(path):\n    return StructureSet.from_dicom_rtstruct(path)\n\n\ndef read_dicom_rtdose(path):\n    return Dose.from_dicom_rtdose(path)\n\n\ndef read_dicom_pet(path, series=None):\n    return PET.from_dicom_pet(path, series, \"SUV\")\n\n\ndef read_dicom_seg(path, meta, series=None):\n    seg_img = read_dicom_series(path, series)\n    return Segmentation.from_dicom_seg(seg_img, meta)\n\n\ndef read_dicom_auto(path, series=None, file_names=None):\n    if path is None:\n        return None\n    if path.endswith(\".dcm\"):\n        dcms = [path]\n    else:\n        dcms = glob.glob(pathlib.Path(path, \"*.dcm\").as_posix())\n        \n    for dcm in dcms:\n        meta = dcmread(dcm)\n        if meta.SeriesInstanceUID != series and series is not None:\n            continue\n        \n        modality = meta.Modality\n        if modality in ['CT', 'MR']:\n            obj = read_dicom_scan(path, series, file_names=file_names)\n        elif modality == 'PT':\n            obj = read_dicom_pet(path, series)\n        elif modality == 'RTSTRUCT':\n            obj = read_dicom_rtstruct(dcm)\n        elif modality == 'RTDOSE':\n            obj = read_dicom_rtdose(dcm)\n        elif modality == 'SEG':\n            obj = read_dicom_seg(path, meta, series)\n        else:\n            if len(dcms) == 1:\n                print(modality, 'at', dcms[0], 'is NOT implemented yet.')\n                raise NotImplementedError\n            else:\n                print(\"There were no dicoms in this path.\")\n                return None\n        \n        obj.metadata.update(get_modality_metadata(meta, modality))\n        return obj\n\n\nclass BaseLoader:\n    def __getitem__(self, subject_id):\n        raise NotImplementedError\n\n    def __len__(self):\n        return len(self.keys())\n\n    def keys(self):\n        raise NotImplementedError\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n\n    def values(self):\n        return (self[k] for k in self.keys())\n\n    def get(self, subject_id, default=None):\n        try:\n            return self[subject_id]\n        except KeyError:\n            return default\n\n\nclass ImageTreeLoader(BaseLoader):\n    def __init__(self,\n                 json_path,\n                 csv_path_or_dataframe,\n                 col_names=[],\n                 study_names=[],\n                 series_names=[],\n                 subseries_names=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n\n        if readers is None:\n            readers = [read_image]  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n\n        self.expand_paths = expand_paths\n        self.readers = readers\n        self.colnames = col_names\n        self.studynames = study_names\n        self.seriesnames = series_names\n        self.subseriesnames = subseries_names\n\n        if isinstance(csv_path_or_dataframe, str):\n            if id_column is not None and id_column not in self.colnames:\n                self.colnames.append(id_column)\n            self.paths = pd.read_csv(csv_path_or_dataframe,\n                                     index_col=id_column)\n        elif isinstance(csv_path_or_dataframe, pd.DataFrame):\n            self.paths = csv_path_or_dataframe\n            if id_column:\n                self.paths = self.paths.set_index(id_column)\n            if len(self.colnames) == 0:\n                self.colnames = self.paths.columns\n        else:\n            raise ValueError(f\"Expected a path to csv file or pd.DataFrame, not {type(csv_path_or_dataframe)}.\")\n        \n        if isinstance(json_path, str):\n            with open(json_path, 'r') as f:\n                self.tree = json.load(f)\n        else:\n            raise ValueError(f\"Expected a path to a json file, not {type(json_path)}.\")\n\n        if not isinstance(readers, list):\n            readers = [readers] * len(self.colnames)\n\n        self.output_tuple = namedtuple(\"Output\", self.colnames)\n\n    def __getitem__(self, subject_id):\n        row = self.paths.loc[subject_id]\n        paths = {col: row[col] for col in self.colnames}\n        study = {col: row[col] for col in self.studynames}\n        series = {col: row[col] for col in self.seriesnames}\n        subseries = {col: row[col] for col in self.subseriesnames}\n        paths = {k: v if pd.notna(v) else None for k, v in paths.items()}\n        \n        if self.expand_paths:\n            # paths = {col: glob.glob(path)[0] for col, path in paths.items()}\n            paths = {col: glob.glob(path)[0] if pd.notna(path) else None for col, path in paths.items()}\n        \n        for i, (col, path) in enumerate(paths.items()):\n            files = self.tree[subject_id][study[\"study_\"+(\"_\").join(col.split(\"_\")[1:])]][series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])]][subseries[\"subseries_\"+(\"_\").join(col.split(\"_\")[1:])]]\n            self.readers[i](path, series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])])\n        outputs = {col: self.readers[i](path, series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])], file_names=files) for i, (col, path) in enumerate(paths.items())}\n        return self.output_tuple(**outputs)\n\n    def keys(self):\n        return list(self.paths.index)\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n    \n\nclass ImageCSVLoader(BaseLoader):\n    def __init__(self,\n                 csv_path_or_dataframe,\n                 colnames=[],\n                 seriesnames=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n\n        if readers is None:\n            readers = [read_image]  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n\n        self.expand_paths = expand_paths\n        self.readers = readers\n\n        self.colnames = colnames\n        self.seriesnames = seriesnames\n        if isinstance(csv_path_or_dataframe, str):\n            if id_column is not None and id_column not in colnames:\n                colnames.append(id_column)\n            self.paths = pd.read_csv(csv_path_or_dataframe,\n                                     index_col=id_column)\n        elif isinstance(csv_path_or_dataframe, pd.DataFrame):\n            self.paths = csv_path_or_dataframe\n            if id_column:\n                self.paths = self.paths.set_index(id_column)\n            if len(self.colnames) == 0:\n                self.colnames = self.paths.columns\n        else:\n            raise ValueError(f\"Expected a path to csv file or pd.DataFrame, not {type(csv_path_or_dataframe)}.\")\n\n        if not isinstance(readers, list):\n            readers = [readers] * len(self.colnames)\n\n        self.output_tuple = namedtuple(\"Output\", self.colnames)\n\n    def __getitem__(self, subject_id):\n        row = self.paths.loc[subject_id]\n        paths = {col: row[col] for col in self.colnames}\n        series = {col: row[col] for col in self.seriesnames}\n        paths = {k: v if pd.notna(v) else None for k, v in paths.items()}\n        if self.expand_paths:\n            # paths = {col: glob.glob(path)[0] for col, path in paths.items()}\n            paths = {col: glob.glob(path)[0] if pd.notna(path) else None for col, path in paths.items()}\n        \n        outputs = {col: self.readers[i](path,series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])]) for i, (col, path) in enumerate(paths.items())}\n        return self.output_tuple(**outputs)\n\n    def keys(self):\n        return list(self.paths.index)\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n\n\nclass ImageFileLoader(BaseLoader):\n    def __init__(self,\n                 root_directory,\n                 get_subject_id_from=\"filename\",\n                 subdir_path=None,\n                 exclude_paths=None,\n                 reader=None):\n\n        if exclude_paths is None:\n            exclude_paths = []\n        if reader is None:\n            reader = read_image  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n\n        self.root_directory = root_directory\n        self.get_subject_id_from = get_subject_id_from\n        self.subdir_path = subdir_path\n        self.exclude_paths = []\n        for path in exclude_paths:\n            if not path.startswith(self.root_directory):\n                full_paths = glob.glob(pathlib.Path(root_directory, path).as_posix())\n                self.exclude_paths.extend(full_paths)\n            else:\n                full_path = path\n                self.exclude_paths.append(full_path)\n        self.reader = reader\n\n        self.paths = self._generate_paths()\n\n    def _generate_paths(self):\n        paths = {}\n        for f in os.scandir(self.root_directory):\n            if f.path in self.exclude_paths:\n                continue\n            subject_dir_path = f.path\n            if self.subdir_path:\n                full_path = pathlib.Path(subject_dir_path, self.subdir_path).as_posix()\n            else:\n                full_path = subject_dir_path\n            try:\n                full_path = glob.glob(full_path)[0]\n            except IndexError:\n                continue\n            if os.path.isdir(full_path):\n                full_path = pathlib.Path(full_path, \"\").as_posix()\n            subject_dir_name = os.path.basename(os.path.normpath(subject_dir_path))\n            subject_id = self._extract_subject_id_from_path(full_path, subject_dir_name)\n            paths[subject_id] = full_path\n        return paths\n\n    def _extract_subject_id_from_path(self, full_path, subject_dir_name):\n        filename, _ = os.path.splitext(os.path.basename(full_path))\n        if isinstance(self.get_subject_id_from, str):\n            if self.get_subject_id_from == \"filename\":\n                subject_id = filename\n            elif self.get_subject_id_from == \"subject_directory\":\n                subject_id = subject_dir_name\n            else:\n                subject_id = re.search(self.get_subject_id_from, full_path)[0]\n        else:\n            return self.get_subject_id_from(full_path, filename, subject_dir_name)\n        return subject_id\n\n    def __getitem__(self, subject_id):\n        path = self.paths[subject_id]\n        return self.reader(path)\n\n    def keys(self):\n        return self.paths.keys()\n\n\n# class CombinedLoader(BaseLoader):\n#     def __init__(self, **kwargs):\n#         self.loaders = kwargs\n#         self.output_tuple = namedtuple(\"Output\", list(self.loaders.keys()))\n\n#     def __getitem__(self, subject_id):\n#         outputs = {name: loader[subject_id] for name, loader in self.loaders.items()}\n#         return self.output_tuple(**outputs)\n\n#     def keys(self):\n#         return set(chain.from_iterable(loader.keys() for loader in self.loaders))\n\n#     def items(self):\n#         return ((k, self[k]) for k in self.keys())\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read_dicom_series` function in the given code snippet?",
        "answer": "The `read_dicom_series` function is designed to read a DICOM series as a SimpleITK Image. It takes a path to a directory containing DICOM files, optionally a specific series ID, and other parameters. The function uses SimpleITK's ImageSeriesReader to load the DICOM series, including all metadata and private tags. It returns the loaded image as a SimpleITK Image object."
      },
      {
        "question": "How does the `ImageTreeLoader` class handle different types of input for the `csv_path_or_dataframe` parameter?",
        "answer": "The `ImageTreeLoader` class can handle two types of input for the `csv_path_or_dataframe` parameter: 1) A string representing the path to a CSV file, in which case it reads the CSV using pandas' `read_csv` function. 2) A pandas DataFrame object, which it uses directly. If an id_column is specified, it sets that column as the index of the DataFrame. If neither a string nor a DataFrame is provided, it raises a ValueError."
      },
      {
        "question": "What is the purpose of the `_extract_subject_id_from_path` method in the `ImageFileLoader` class?",
        "answer": "The `_extract_subject_id_from_path` method in the `ImageFileLoader` class is responsible for extracting a subject ID from a given file path. It can extract the ID in three ways based on the `get_subject_id_from` parameter: 1) From the filename, 2) From the subject directory name, or 3) Using a regular expression pattern. If `get_subject_id_from` is a function, it calls that function with the full path, filename, and subject directory name to extract the ID. This method allows flexible identification of subjects based on the file structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class ImageCSVLoader(BaseLoader):\n    def __init__(self,\n                 csv_path_or_dataframe,\n                 colnames=[],\n                 seriesnames=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n        # TODO: Implement the constructor\n        pass\n\n    def __getitem__(self, subject_id):\n        # TODO: Implement the item retrieval\n        pass\n\n    def keys(self):\n        # TODO: Implement the keys method\n        pass",
        "complete": "class ImageCSVLoader(BaseLoader):\n    def __init__(self,\n                 csv_path_or_dataframe,\n                 colnames=[],\n                 seriesnames=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n        self.expand_paths = expand_paths\n        self.readers = readers or [read_image]\n        self.colnames = colnames\n        self.seriesnames = seriesnames\n\n        if isinstance(csv_path_or_dataframe, str):\n            if id_column is not None and id_column not in colnames:\n                colnames.append(id_column)\n            self.paths = pd.read_csv(csv_path_or_dataframe, index_col=id_column)\n        elif isinstance(csv_path_or_dataframe, pd.DataFrame):\n            self.paths = csv_path_or_dataframe\n            if id_column:\n                self.paths = self.paths.set_index(id_column)\n            if len(self.colnames) == 0:\n                self.colnames = self.paths.columns\n        else:\n            raise ValueError(f\"Expected a path to csv file or pd.DataFrame, not {type(csv_path_or_dataframe)}.\")\n\n        if not isinstance(self.readers, list):\n            self.readers = [self.readers] * len(self.colnames)\n\n        self.output_tuple = namedtuple(\"Output\", self.colnames)\n\n    def __getitem__(self, subject_id):\n        row = self.paths.loc[subject_id]\n        paths = {col: row[col] for col in self.colnames}\n        series = {col: row[col] for col in self.seriesnames}\n        paths = {k: v if pd.notna(v) else None for k, v in paths.items()}\n        \n        if self.expand_paths:\n            paths = {col: glob.glob(path)[0] if pd.notna(path) else None for col, path in paths.items()}\n        \n        outputs = {col: self.readers[i](path, series.get(f\"series_{'_'.join(col.split('_')[1:])}\")) for i, (col, path) in enumerate(paths.items())}\n        return self.output_tuple(**outputs)\n\n    def keys(self):\n        return list(self.paths.index)"
      },
      {
        "partial": "def read_dicom_auto(path, series=None, file_names=None):\n    # TODO: Implement the function to automatically read DICOM files\n    pass",
        "complete": "def read_dicom_auto(path, series=None, file_names=None):\n    if path is None:\n        return None\n    if path.endswith(\".dcm\"):\n        dcms = [path]\n    else:\n        dcms = glob.glob(pathlib.Path(path, \"*.dcm\").as_posix())\n        \n    for dcm in dcms:\n        meta = dcmread(dcm)\n        if meta.SeriesInstanceUID != series and series is not None:\n            continue\n        \n        modality = meta.Modality\n        if modality in ['CT', 'MR']:\n            obj = read_dicom_scan(path, series, file_names=file_names)\n        elif modality == 'PT':\n            obj = read_dicom_pet(path, series)\n        elif modality == 'RTSTRUCT':\n            obj = read_dicom_rtstruct(dcm)\n        elif modality == 'RTDOSE':\n            obj = read_dicom_rtdose(dcm)\n        elif modality == 'SEG':\n            obj = read_dicom_seg(path, meta, series)\n        else:\n            if len(dcms) == 1:\n                print(modality, 'at', dcms[0], 'is NOT implemented yet.')\n                raise NotImplementedError\n            else:\n                print(\"There were no dicoms in this path.\")\n                return None\n        \n        obj.metadata.update(get_modality_metadata(meta, modality))\n        return obj"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "json",
        "glob",
        "re",
        "pandas",
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Optional",
        "collections.namedtuple",
        "pydicom.dcmread",
        "modules.StructureSet",
        "utils.crawl.*",
        "utils.dicomutils.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_helpers.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"getPubchemStatus works\", {\n  output <- capture.output(getPubchemStatus(printMessage = TRUE), type = \"message\")\n  expect_match(output[1], \"Throttling status:\")\n  result <- getPubchemStatus(printMessage = FALSE)\n  expect_equal(result, NULL)\n\n  result <- getPubchemStatus(returnMessage = TRUE, printMessage = FALSE)\n\n  expect_class(result, \"list\")\n  expect_equal(names(result), c(\"request_count\", \"request_time\", \"service\"))\n})\n\n\ntest_that(\"checkThrottlingStatus Works\", {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\"\n\n\n\n  response <- AnnotationGx:::.buildURL(url) |>\n    AnnotationGx:::.build_pubchem_request() |>\n    httr2::req_perform()\n  message <- \"Request Count status: Yellow (60%), Request Time status: Yellow (60%), Service status: Yellow (60%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = TRUE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Yellow\", percent = 60),\n    request_time = list(status = \"Yellow\", percent = 60),\n    service = list(status = \"Yellow\", percent = 60)\n  ))\n\n\n  message <- \"Request Count status: Red (80%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Red\", percent = 80),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n\n  message <- \"Request Count status: Black (100%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Black\", percent = 100),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemStatus` function and how is it tested in the given code snippet?",
        "answer": "The `getPubchemStatus` function is used to retrieve the throttling status of PubChem API requests. It is tested in two ways: 1) It checks if the function prints a message containing 'Throttling status:' when `printMessage = TRUE`. 2) It verifies that the function returns a list with three specific keys ('request_count', 'request_time', 'service') when `returnMessage = TRUE` and `printMessage = FALSE`."
      },
      {
        "question": "How does the `.checkThrottlingStatus2` function parse the throttling status message, and what data structure does it return?",
        "answer": "The `.checkThrottlingStatus2` function parses a throttling status message and returns a nested list. The returned list contains three main keys: 'request_count', 'request_time', and 'service'. Each of these keys contains a sub-list with 'status' (a string like 'Yellow' or 'Red') and 'percent' (an integer representing the usage percentage). The function can handle different status levels, including 'Yellow', 'Red', and 'Black'."
      },
      {
        "question": "What testing framework and assertions are used in this code snippet, and how are they applied?",
        "answer": "This code snippet uses the `testthat` framework for unit testing in R. The main assertions used are `expect_match`, `expect_equal`, and `expect_class`. `expect_match` is used to check if the output contains a specific string. `expect_equal` is used to compare the actual output with expected values, such as comparing parsed throttling status information. `expect_class` is used to verify that the returned object is of the expected class (in this case, a list)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"getPubchemStatus works\", {\n  output <- capture.output(getPubchemStatus(printMessage = TRUE), type = \"message\")\n  expect_match(output[1], \"Throttling status:\")\n  result <- getPubchemStatus(printMessage = FALSE)\n  expect_equal(result, NULL)\n\n  result <- getPubchemStatus(returnMessage = TRUE, printMessage = FALSE)\n\n  expect_class(result, \"list\")\n  # Complete the test by adding an expectation for the names of the result\n})",
        "complete": "test_that(\"getPubchemStatus works\", {\n  output <- capture.output(getPubchemStatus(printMessage = TRUE), type = \"message\")\n  expect_match(output[1], \"Throttling status:\")\n  result <- getPubchemStatus(printMessage = FALSE)\n  expect_equal(result, NULL)\n\n  result <- getPubchemStatus(returnMessage = TRUE, printMessage = FALSE)\n\n  expect_class(result, \"list\")\n  expect_equal(names(result), c(\"request_count\", \"request_time\", \"service\"))\n})"
      },
      {
        "partial": "test_that(\"checkThrottlingStatus Works\", {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\"\n\n  response <- AnnotationGx:::.buildURL(url) |>\n    AnnotationGx:::.build_pubchem_request() |>\n    httr2::req_perform()\n  message <- \"Request Count status: Yellow (60%), Request Time status: Yellow (60%), Service status: Yellow (60%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = TRUE)\n  # Complete the test by adding an expectation for the parsed_info\n\n  # Add two more test cases for different status messages\n})",
        "complete": "test_that(\"checkThrottlingStatus Works\", {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\"\n\n  response <- AnnotationGx:::.buildURL(url) |>\n    AnnotationGx:::.build_pubchem_request() |>\n    httr2::req_perform()\n  message <- \"Request Count status: Yellow (60%), Request Time status: Yellow (60%), Service status: Yellow (60%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = TRUE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Yellow\", percent = 60),\n    request_time = list(status = \"Yellow\", percent = 60),\n    service = list(status = \"Yellow\", percent = 60)\n  ))\n\n  message <- \"Request Count status: Red (80%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Red\", percent = 80),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n\n  message <- \"Request Count status: Black (100%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Black\", percent = 100),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_helpers.R",
    "language": "R",
    "content": "#' Parses PubChem REST responses\n#'\n#' This function takes a list of PubChem REST responses and parses them into a\n#' standardized format. It checks the input for validity and handles error\n#' responses appropriately.\n#'\n#' @param responses A list of PubChem REST responses.\n#' @return A list of parsed PubChem responses, with each response parsed into a\n#'         data table format.\n#'\n#' @noRd\n#' @keywords internal\n.parse_pubchem_rest_responses <- function(responses) {\n  checkmate::assert_list(\n    x = responses,\n    any.missing = FALSE,\n    names = \"named\",\n    min.len = 1\n  )\n\n  responses_parsed <- lapply(names(responses), function(i) {\n    resp <- responses[[i]]\n    body <- .parse_resp_json(resp)\n    if (httr2::resp_is_error(resp)) {\n      return(.parseQueryToDT(NA_integer_))\n    }\n\n    return(.parseQueryToDT(body))\n  })\n  names(responses_parsed) <- names(responses)\n  return(responses_parsed)\n}\n\n\n\n#' Build a query for the PubChem REST API\n#'\n#' This function builds a query for the PubChem REST API based on the provided parameters.\n#'\n#' @param id The identifier(s) for the query. If namespace is 'name', id must be a single value.\n#' @param domain The domain of the query. Options are 'compound', 'substance', 'assay', 'cell', 'gene', 'protein'.\n#' @param namespace The namespace of the query. Options depend on the chosen domain.\n#' @param operation The operation to perform. Options depend on the chosen domain and namespace.\n#' @param output The desired output format. Options are 'JSON', 'XML', 'SDF', 'TXT', 'CSV'.\n#' @param url The base URL for the PubChem REST API.\n#' @param raw Logical indicating whether to return the raw response or parse it.\n#' @param query_only Logical indicating whether to return the query URL only.\n#' @param ... Additional arguments to be passed to the query.\n#'\n#' @return The query URL or the parsed response, depending on the arguments.\n#'\n#' @importFrom checkmate assert assert_choice assert_logical assert_atomic test_choice assert_integerish test_atomic\n#'\n#' @noRd\n#' @keywords internal\n.build_pubchem_rest_query <- function(id, domain = \"compound\", namespace = \"name\", operation = \"cids\",\n                                      output = \"JSON\", url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\",\n                                      raw = FALSE, query_only = FALSE, ...) {\n  # -------------------------------------- Argument checking --------------------------------------\n  assert_choice(domain, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n  switch(domain,\n    \"compound\" = {\n      assert_choice(namespace, c(\"cid\", \"name\", \"smiles\", \"inchi\", \"sdf\", \"inchikey\", \"formula\"))\n      assert(test_choice(\n        operation, c(\"record\", \"synonyms\", \"sids\", \"cids\", \"aids\", \"assaysummary\")\n      ) ||\n        grepl(\"property\", operation))\n    },\n    \"substance\" = assert_choice(namespace, c(\"sid\", \"sourceid\", \"sourceall\", \"name\")),\n    \"assay\" = assert_choice(namespace, c(\"aid\", \"listkey\", \"type\", \"sourceall\", \"target\", \"activity\")),\n    \"cell\" = assert_choice(namespace, c(\"cellacc\", \"synonym\")),\n    \"gene\" = assert_choice(namespace, c(\"geneid\", \"genesymbol\", \"synonym\")),\n    \"protein\" = assert_choice(namespace, c(\"accession\", \"gi\", \"synonym\"))\n  )\n  assert_choice(output, c(\"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\"))\n  assert_logical(raw, query_only)\n  if (!test_atomic(id, any.missing = FALSE)) .err(\"id must be an atomic vector with no missing/NA values\")\n\n  if (namespace == \"cid\") assert_integerish(id)\n\n  # -------------------------------------- Function context --------------------------------------\n  funContext <- .funContext(\"query_pubchem_rest\")\n  if (length(id) > 1 && namespace == \"name\") .err(funContext, \" id must be a single value when namespace is 'name'\")\n\n  url <- .buildURL(url, domain, namespace, id, operation, output)\n  .debug(funContext, \" Query URL: \", url)\n  if (query_only) {\n    return(url)\n  }\n\n  # -------------------------------------- Querying PubChem REST API --------------------------------------\n  .build_pubchem_request(url)\n}\n\n\n#' Builds a PubChem HTTP request using the provided URL.\n#'\n#' @param url The URL for the request.\n#' @return The built PubChem HTTP request.\n#' @noRd\n.build_pubchem_request <- function(url) {\n  .build_request(url) |>\n    httr2::req_throttle(rate = 1000 / 60)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.parse_pubchem_rest_responses` function and what are its key components?",
        "answer": "The `.parse_pubchem_rest_responses` function is designed to parse a list of PubChem REST responses into a standardized format. Its key components include: 1) Input validation using `checkmate::assert_list`, 2) Iterating through the responses using `lapply`, 3) Parsing each response body with `.parse_resp_json`, 4) Handling error responses by returning `NA_integer_`, and 5) Parsing successful responses using `.parseQueryToDT`."
      },
      {
        "question": "In the `.build_pubchem_rest_query` function, how does the code handle different domains and their corresponding namespaces?",
        "answer": "The function uses a `switch` statement to handle different domains and their corresponding namespaces. For each domain (compound, substance, assay, cell, gene, protein), it asserts that the provided namespace is valid using `assert_choice`. For example, if the domain is 'compound', it checks that the namespace is one of 'cid', 'name', 'smiles', 'inchi', 'sdf', 'inchikey', or 'formula'. This ensures that only valid combinations of domains and namespaces are used in the query construction."
      },
      {
        "question": "What is the purpose of the `.build_pubchem_request` function and how does it contribute to responsible API usage?",
        "answer": "The `.build_pubchem_request` function builds an HTTP request for the PubChem API using the provided URL. It contributes to responsible API usage by implementing rate limiting through the `httr2::req_throttle` function. Specifically, it sets a throttle rate of 1000 milliseconds per 60 requests (approximately 1 request per second), which helps prevent overwhelming the PubChem server and ensures compliance with their usage guidelines."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".parse_pubchem_rest_responses <- function(responses) {\n  checkmate::assert_list(\n    x = responses,\n    any.missing = FALSE,\n    names = \"named\",\n    min.len = 1\n  )\n\n  responses_parsed <- lapply(names(responses), function(i) {\n    resp <- responses[[i]]\n    body <- .parse_resp_json(resp)\n    if (httr2::resp_is_error(resp)) {\n      return(.parseQueryToDT(NA_integer_))\n    }\n\n    # Complete the function here\n  })\n  # Complete the function here\n}",
        "complete": ".parse_pubchem_rest_responses <- function(responses) {\n  checkmate::assert_list(\n    x = responses,\n    any.missing = FALSE,\n    names = \"named\",\n    min.len = 1\n  )\n\n  responses_parsed <- lapply(names(responses), function(i) {\n    resp <- responses[[i]]\n    body <- .parse_resp_json(resp)\n    if (httr2::resp_is_error(resp)) {\n      return(.parseQueryToDT(NA_integer_))\n    }\n\n    return(.parseQueryToDT(body))\n  })\n  names(responses_parsed) <- names(responses)\n  return(responses_parsed)\n}"
      },
      {
        "partial": ".build_pubchem_rest_query <- function(id, domain = \"compound\", namespace = \"name\", operation = \"cids\",\n                                      output = \"JSON\", url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\",\n                                      raw = FALSE, query_only = FALSE, ...) {\n  # Argument checking\n  assert_choice(domain, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n  switch(domain,\n    \"compound\" = {\n      assert_choice(namespace, c(\"cid\", \"name\", \"smiles\", \"inchi\", \"sdf\", \"inchikey\", \"formula\"))\n      assert(test_choice(\n        operation, c(\"record\", \"synonyms\", \"sids\", \"cids\", \"aids\", \"assaysummary\")\n      ) ||\n        grepl(\"property\", operation))\n    },\n    \"substance\" = assert_choice(namespace, c(\"sid\", \"sourceid\", \"sourceall\", \"name\")),\n    \"assay\" = assert_choice(namespace, c(\"aid\", \"listkey\", \"type\", \"sourceall\", \"target\", \"activity\")),\n    \"cell\" = assert_choice(namespace, c(\"cellacc\", \"synonym\")),\n    \"gene\" = assert_choice(namespace, c(\"geneid\", \"genesymbol\", \"synonym\")),\n    \"protein\" = assert_choice(namespace, c(\"accession\", \"gi\", \"synonym\"))\n  )\n  assert_choice(output, c(\"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\"))\n  assert_logical(raw, query_only)\n  if (!test_atomic(id, any.missing = FALSE)) .err(\"id must be an atomic vector with no missing/NA values\")\n\n  if (namespace == \"cid\") assert_integerish(id)\n\n  # Function context\n  funContext <- .funContext(\"query_pubchem_rest\")\n  if (length(id) > 1 && namespace == \"name\") .err(funContext, \" id must be a single value when namespace is 'name'\")\n\n  # Complete the function here\n}",
        "complete": ".build_pubchem_rest_query <- function(id, domain = \"compound\", namespace = \"name\", operation = \"cids\",\n                                      output = \"JSON\", url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\",\n                                      raw = FALSE, query_only = FALSE, ...) {\n  # Argument checking\n  assert_choice(domain, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n  switch(domain,\n    \"compound\" = {\n      assert_choice(namespace, c(\"cid\", \"name\", \"smiles\", \"inchi\", \"sdf\", \"inchikey\", \"formula\"))\n      assert(test_choice(\n        operation, c(\"record\", \"synonyms\", \"sids\", \"cids\", \"aids\", \"assaysummary\")\n      ) ||\n        grepl(\"property\", operation))\n    },\n    \"substance\" = assert_choice(namespace, c(\"sid\", \"sourceid\", \"sourceall\", \"name\")),\n    \"assay\" = assert_choice(namespace, c(\"aid\", \"listkey\", \"type\", \"sourceall\", \"target\", \"activity\")),\n    \"cell\" = assert_choice(namespace, c(\"cellacc\", \"synonym\")),\n    \"gene\" = assert_choice(namespace, c(\"geneid\", \"genesymbol\", \"synonym\")),\n    \"protein\" = assert_choice(namespace, c(\"accession\", \"gi\", \"synonym\"))\n  )\n  assert_choice(output, c(\"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\"))\n  assert_logical(raw, query_only)\n  if (!test_atomic(id, any.missing = FALSE)) .err(\"id must be an atomic vector with no missing/NA values\")\n\n  if (namespace == \"cid\") assert_integerish(id)\n\n  # Function context\n  funContext <- .funContext(\"query_pubchem_rest\")\n  if (length(id) > 1 && namespace == \"name\") .err(funContext, \" id must be a single value when namespace is 'name'\")\n\n  url <- .buildURL(url, domain, namespace, id, operation, output)\n  .debug(funContext, \" Query URL: \", url)\n  if (query_only) return(url)\n\n  .build_pubchem_request(url)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/utils/__init__.py",
    "language": "py",
    "content": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]",
    "qa_pairs": [
      {
        "question": "What is the purpose of the __all__ list in this Python module?",
        "answer": "The __all__ list in this module explicitly defines which names should be imported when using 'from module import *'. In this case, it specifies that only 'setup_logger' and 'get_logger' should be available for import, providing better control over the module's public interface."
      },
      {
        "question": "How does this code snippet demonstrate the use of relative imports in Python?",
        "answer": "The code uses a relative import with the line 'from .logging_config import setup_logger, get_logger'. The dot (.) before 'logging_config' indicates that the 'logging_config' module is in the same package as the current module, demonstrating how to import from sibling modules within the same package."
      },
      {
        "question": "What is the relationship between the imported functions and the __all__ list in this code?",
        "answer": "The functions 'setup_logger' and 'get_logger' are imported from the 'logging_config' module and then explicitly listed in the __all__ variable. This pattern is often used to re-export specific functions from a submodule, making them appear as if they were defined in the current module while maintaining a clean internal structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    # Complete the __all__ list\n]",
        "complete": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]"
      },
      {
        "partial": "# Import the necessary functions from logging_config\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]",
        "complete": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "logging_config.setup_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CCLE/CCLE_treatmentdata.R",
    "language": "R",
    "content": "# https://data.broadinstitute.org/ccle_legacy_data/pharmacological_profiling/CCLE_NP24.2009_profiling_2012.02.20.csv\nfilePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code, and how is it being used?",
        "answer": "The `system.file()` function is used to locate and construct the file path for a specific CSV file within the 'AnnotationGx' package. It's being used to find the file 'CCLE_NP24.2009_profiling_2012.02.20.csv' in the 'extdata/CCLE' directory of the package. This allows the code to access package-specific data files in a portable manner, regardless of where the package is installed on the user's system."
      },
      {
        "question": "How is the `data.table::fread()` function being used in this code snippet, and what are its parameters?",
        "answer": "The `data.table::fread()` function is being used to read the CSV file specified by `filePath` into R. It's called with two parameters: 'input', which is set to the `filePath` variable containing the path to the CSV file, and 'encoding', which is set to 'Latin-1' to specify the character encoding of the file. The `fread()` function is part of the data.table package and is known for its fast reading of large data files."
      },
      {
        "question": "What does the code do with the `rawdata` after reading it, and what is the purpose of the `CCLE_treatmentMetadata` variable?",
        "answer": "After reading the raw data, the code creates a new data table called `CCLE_treatmentMetadata`. This new table contains only one column, renamed as 'CCLE.treatmentid', which is extracted from the 'Compound (code or generic name)' column of the original `rawdata`. The purpose of `CCLE_treatmentMetadata` is likely to create a simplified dataset containing only the treatment IDs from the original data, which can be used for further analysis or as a reference table."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = # Complete this line\n\n# Complete the code to save the data",
        "complete": "filePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the CSV file and create CCLE_treatmentMetadata\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)",
        "complete": "filePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_standardize_names.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\n\ntest_that(\"standardize_names converts names to lowercase, removes trailing information, removes non-alphanumeric characters, replaces empty names with 'invalid', and converts names to uppercase\", {\n  # Test case 1: Standardize names without any special characters\n  names1 <- c(\"John Doe\", \"Jane Smith\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with trailing information\n  names2 <- c(\"John Doe, Manager\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n  # Test case 3: Standardize names with square brackets and parentheses\n  names3 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected3 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result3 <- standardize_names(names3)\n  expect_equal(result3, expected3)\n\n  # Test case 5: Standardize names with empty names\n  names5 <- c(\"John Doe\", \"\", \"Alice\")\n  expected5 <- c(\"JOHNDOE\", NA, \"ALICE\")\n  result5 <- standardize_names(names5)\n  expect_equal(result5, expected5)\n\n\n  # Test case 7: Standardize names with leading and trailing spaces\n  names7 <- c(\"  John Doe  \", \" Jane Smith \", \" Alice \")\n  expected7 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result7 <- standardize_names(names7)\n  expect_equal(result7, expected7)\n\n  # Test case 8: Standardize names with numbers\n  names8 <- c(\"John Doe 1\", \"Jane Smith 2\", \"Alice 3\")\n  expected8 <- c(\"JOHNDOE1\", \"JANESMITH2\", \"ALICE3\")\n  result8 <- standardize_names(names8)\n  expect_equal(result8, expected8)\n\n  # Test case 11: Standardize names with non-alphanumeric characters and numbers\n  names11 <- c(\"John Doe\", 1, \"Alice\")\n  expected11 <- c(\"JOHNDOE\", \"1\", \"ALICE\")\n  result11 <- standardize_names(names11)\n  expect_equal(result11, expected11)\n})\n\ntest_that(\"standardize_names Error\", {\n  names <- c(\"John Doe\", NA, \"Alice\")\n  expect_error(standardize_names(names))\n\n  names <- c(1, 1, 1)\n  expect_error(standardize_names(names))\n}) \n\n\n# Test case 6: Standardize names with special characters\ntest_that(\"cleanCharacterStrings removes special characters, formatting, and unwanted substrings\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n\n  # Test case 5: Clean string with hyphen\n  input5 <- \"Bio-informatics\"\n  expected5 <- \"BIOINFORMATICS\"\n  result5 <- cleanCharacterStrings(input5)\n  expect_equal(result5, expected5)\n})\n\ntest_that(\"standardize_names handles different scenarios\", {\n  # Test case 1: Standardize names with trailing information after a comma\n  names1 <- c(\"John Doe, Manager\", \"Jane Smith, PhD\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with information within square brackets or parentheses\n  names2 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n\n  # Test case 4: Standardize names with empty names\n  names4 <- c(\"John Doe\", \"\", \"Alice\")\n  expected4 <- c(\"JOHNDOE\", NA, \"ALICE\")\n  result4 <- standardize_names(names4)\n  expect_equal(result4, expected4)\n\n  # Test case 5: Standardize names with leading and trailing spaces\n  names5 <- c(\"  John Doe  \", \" Jane Smith \", \" Alice \")\n  expected5 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result5 <- standardize_names(names5)\n  expect_equal(result5, expected5)\n\n  # Test case 6: Standardize names with numbers\n  names6 <- c(\"John Doe 1\", \"Jane Smith 2\", \"Alice 3\")\n  expected6 <- c(\"JOHNDOE1\", \"JANESMITH2\", \"ALICE3\")\n  result6 <- standardize_names(names6)\n  expect_equal(result6, expected6)\n\n  # Test case 9: Standardize names with non-alphanumeric characters and numbers\n  names9 <- c(\"John Doe\", 1, \"Alice\")\n  expected9 <- c(\"JOHNDOE\", \"1\", \"ALICE\")\n  result9 <- standardize_names(names9)\n  expect_equal(result9, expected9)\n})\n\ntest_that(\"cleanCharacterStrings handles different scenarios\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n\n  # Test case 5: Clean string with hyphen\n  input5 <- \"Bio-informatics\"\n  expected5 <- \"BIOINFORMATICS\"\n  result5 <- cleanCharacterStrings(input5)\n  expect_equal(result5, expected5)\n\n  # Test case 6: Clean numeric input\n  input6 <- 12345\n  expected6 <- \"12345\"\n  result6 <- cleanCharacterStrings(input6)\n  expect_equal(result6, expected6)\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `standardize_names` function based on the test cases provided?",
        "answer": "The `standardize_names` function is designed to standardize names by converting them to uppercase, removing trailing information (such as titles or qualifications), removing non-alphanumeric characters, replacing empty names with NA, and trimming leading and trailing spaces. It handles various scenarios including names with special characters, numbers, and different formatting."
      },
      {
        "question": "How does the `cleanCharacterStrings` function differ from `standardize_names`, and what specific task does it perform?",
        "answer": "The `cleanCharacterStrings` function is more focused on cleaning individual strings rather than standardizing a vector of names. It removes special characters, formatting, and unwanted substrings from input strings. Unlike `standardize_names`, it doesn't handle empty strings or convert them to NA. It also appears to handle numeric inputs by converting them to strings."
      },
      {
        "question": "What are the key differences in how `standardize_names` handles empty strings versus non-string inputs?",
        "answer": "For empty strings, `standardize_names` converts them to NA values. For non-string inputs like numbers, it converts them to string representations. However, the function throws an error when encountering NA values in the input vector, as shown in the error test case."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"standardize_names handles different scenarios\", {\n  # Test case 1: Standardize names with trailing information after a comma\n  names1 <- c(\"John Doe, Manager\", \"Jane Smith, PhD\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with information within square brackets or parentheses\n  names2 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"standardize_names handles different scenarios\", {\n  # Test case 1: Standardize names with trailing information after a comma\n  names1 <- c(\"John Doe, Manager\", \"Jane Smith, PhD\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with information within square brackets or parentheses\n  names2 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n  # Test case 3: Standardize names with empty names\n  names3 <- c(\"John Doe\", \"\", \"Alice\")\n  expected3 <- c(\"JOHNDOE\", NA, \"ALICE\")\n  result3 <- standardize_names(names3)\n  expect_equal(result3, expected3)\n\n  # Test case 4: Standardize names with leading and trailing spaces\n  names4 <- c(\"  John Doe  \", \" Jane Smith \", \" Alice \")\n  expected4 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result4 <- standardize_names(names4)\n  expect_equal(result4, expected4)\n\n  # Test case 5: Standardize names with numbers\n  names5 <- c(\"John Doe 1\", \"Jane Smith 2\", \"Alice 3\")\n  expected5 <- c(\"JOHNDOE1\", \"JANESMITH2\", \"ALICE3\")\n  result5 <- standardize_names(names5)\n  expect_equal(result5, expected5)\n\n  # Test case 6: Standardize names with non-alphanumeric characters and numbers\n  names6 <- c(\"John Doe\", 1, \"Alice\")\n  expected6 <- c(\"JOHNDOE\", \"1\", \"ALICE\")\n  result6 <- standardize_names(names6)\n  expect_equal(result6, expected6)\n})"
      },
      {
        "partial": "test_that(\"cleanCharacterStrings handles different scenarios\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"cleanCharacterStrings handles different scenarios\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n  # Test case 3: Clean string with hyphen\n  input3 <- \"Bio-informatics\"\n  expected3 <- \"BIOINFORMATICS\"\n  result3 <- cleanCharacterStrings(input3)\n  expect_equal(result3, expected3)\n\n  # Test case 4: Clean numeric input\n  input4 <- 12345\n  expected4 <- \"12345\"\n  result4 <- cleanCharacterStrings(input4)\n  expect_equal(result4, expected4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_logLogisticRegression.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Testing LogLogisticRegression.\")\n\n##TO-DO::Supress print to console from this test file\n\ntest_that(\"Errors are checked.\",{\n\t\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60))) #should complain\n    expect_warning(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), viability_as_pct = FALSE)) #should complain\n    expect_error(logLogisticRegression(c(-1, 2, 3), c(70, 60, 50), conc_as_log = FALSE)) #should complain\n\n    expect_error(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), median_n = 0)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), median_n = 3/2)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1))) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1, -1))) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), precision = 0)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), scale = 0)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), lower_bounds = c(0, 0, 0), upper_bounds = c(1, 1, -1))) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), family = \"The Addams Family\")) #should complain\n})\n\ntest_that(\"Values returned as expected (previous runs of function).\",{\n\t\n\texpect_equivalent(\n\t\tlogLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0))\n\t\t\t, conc_as_log=TRUE, viability_as_pct = FALSE), list(1,0,0))\n\n\texpect_equivalent(\n\t\tlogLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0))\n\t\t\t, conc_as_log=TRUE, viability_as_pct = FALSE, family=\"Cauchy\"), list(1,0,0))\n\n\texpect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40)), \n        structure(list(HS = 1.28651256396627, E_inf = 36.2653101620223, EC50 = 1.1810533048852), Rsquare = 0.994776857838702), \n        tolerance=1e-3) #should run with no objections\n\n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40), family=\"Cauchy\"), \n        structure(list(HS = 1.29137210106265, E_inf = 36.32166034246, EC50 = 1.17645746710051), Rsquare = 0.994810069836551), \n        tolerance=1e-3) #should run with no objections\n\n\texpect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE), \n        structure(structure(list(HS = 1.33880390747459, E_inf = 36.8315342784204, \n        EC50 = 1.17467467087487), Rsquare = 0.993325611444731), Rsquare = 0.992907719144335), \n        tolerance=1e-3) #should run with no objections\n\n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE, family=\"Cauchy\"), \n        structure(list(HS = 1.33972330068866, E_inf = 36.8279821440339, EC50 = 1.17127888613006), Rsquare = 0.993391659104375),\n        tolerance=1e-3) #should run with no objections\n\n    ## These next few tests make sure trunc is doing something sensible\n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=FALSE), \n        structure(list(HS = 1.83941027802297, E_inf = 46.2252841409534, EC50 = 0.929240163785174), Rsquare = 0.950154102951421),\n        tolerance=1e-3) #should run with no objections\n    \n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=TRUE), \n        structure(list(HS = 2.06741101065827, E_inf = 48.0764684303728, EC50 = 0.900808050726654), Rsquare = 0.986745954925997),\n        tolerance=1e-3) #should run with no objections\n\n    expect_equivalent(logLogisticRegression(c(0.1, 1, 2, 3), c(100, 70, 60, 50), trunc=TRUE), \n        logLogisticRegression(c(0.1, 1, 2, 3), c(500, 70, 60, 50), trunc=TRUE))\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logLogisticRegression` function in this code, and what are its main parameters?",
        "answer": "The `logLogisticRegression` function appears to be a custom implementation for fitting a log-logistic regression model. Its main parameters include:\n1. A vector of concentrations\n2. A vector of corresponding viability or response values\n3. `conc_as_log`: Boolean indicating if concentrations are already log-transformed\n4. `viability_as_pct`: Boolean indicating if viability is expressed as a percentage\n5. `family`: The distribution family (e.g., 'Cauchy' or default)\n6. `trunc`: Boolean for truncation of values\n7. Other parameters like `median_n`, `density`, `precision`, `scale`, and `lower_bounds`/`upper_bounds` for fine-tuning the regression."
      },
      {
        "question": "How does this test suite handle different types of input validation for the `logLogisticRegression` function?",
        "answer": "The test suite handles input validation for `logLogisticRegression` in several ways:\n1. It checks for mismatched vector lengths between concentrations and viability values.\n2. It verifies that warnings are raised when `viability_as_pct` is set incorrectly.\n3. It ensures errors are thrown for invalid concentration values when `conc_as_log` is false.\n4. It tests for proper error handling with invalid `median_n`, `density`, `precision`, and `scale` values.\n5. It checks that appropriate errors are raised for invalid `lower_bounds` and `upper_bounds`.\n6. It verifies that an error is thrown when an invalid `family` parameter is provided.\nThese tests use `expect_error()` and `expect_warning()` functions to assert that the appropriate exceptions are raised for invalid inputs."
      },
      {
        "question": "What are the key aspects of the `logLogisticRegression` function's output that are being tested in this code?",
        "answer": "The test suite checks several key aspects of the `logLogisticRegression` function's output:\n1. Correctness: It verifies that the function returns expected values for known inputs, using `expect_equivalent()` and `expect_equal()` with predefined tolerance levels.\n2. Consistency: It ensures that the function produces similar results for different input scales when using the `trunc` parameter.\n3. Parameter estimation: The tests check if the function correctly estimates the Hill Slope (HS), E_inf (minimum effect), and EC50 (half maximal effective concentration) parameters.\n4. R-squared values: The tests verify the goodness of fit by checking the returned R-squared values.\n5. Different families: It compares results between the default family and the 'Cauchy' family to ensure both options work correctly.\n6. Truncation effects: The tests examine how the `trunc` parameter affects the output, especially for edge cases with values outside the expected range."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Errors are checked.\", {\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60)))\n    expect_warning(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), viability_as_pct = FALSE))\n    expect_error(logLogisticRegression(c(-1, 2, 3), c(70, 60, 50), conc_as_log = FALSE))\n    # Add more expect_error statements here\n})",
        "complete": "test_that(\"Errors are checked.\", {\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60)))\n    expect_warning(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), viability_as_pct = FALSE))\n    expect_error(logLogisticRegression(c(-1, 2, 3), c(70, 60, 50), conc_as_log = FALSE))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), median_n = 0))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), median_n = 3/2))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1)))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1, -1)))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), precision = 0))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), scale = 0))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), lower_bounds = c(0, 0, 0), upper_bounds = c(1, 1, -1)))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), family = \"The Addams Family\"))\n})"
      },
      {
        "partial": "test_that(\"Values returned as expected (previous runs of function).\", {\n    expect_equivalent(\n        logLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0)),\n            conc_as_log=TRUE, viability_as_pct = FALSE), list(1,0,0))\n    \n    # Add more expect_equivalent and expect_equal statements here\n})",
        "complete": "test_that(\"Values returned as expected (previous runs of function).\", {\n    expect_equivalent(\n        logLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0)),\n            conc_as_log=TRUE, viability_as_pct = FALSE), list(1,0,0))\n    \n    expect_equivalent(\n        logLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0)),\n            conc_as_log=TRUE, viability_as_pct = FALSE, family=\"Cauchy\"), list(1,0,0))\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40)), \n        structure(list(HS = 1.28651256396627, E_inf = 36.2653101620223, EC50 = 1.1810533048852), Rsquare = 0.994776857838702), \n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40), family=\"Cauchy\"), \n        structure(list(HS = 1.29137210106265, E_inf = 36.32166034246, EC50 = 1.17645746710051), Rsquare = 0.994810069836551), \n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE), \n        structure(structure(list(HS = 1.33880390747459, E_inf = 36.8315342784204, \n        EC50 = 1.17467467087487), Rsquare = 0.993325611444731), Rsquare = 0.992907719144335), \n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE, family=\"Cauchy\"), \n        structure(list(HS = 1.33972330068866, E_inf = 36.8279821440339, EC50 = 1.17127888613006), Rsquare = 0.993391659104375),\n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=FALSE), \n        structure(list(HS = 1.83941027802297, E_inf = 46.2252841409534, EC50 = 0.929240163785174), Rsquare = 0.950154102951421),\n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=TRUE), \n        structure(list(HS = 2.06741101065827, E_inf = 48.0764684303728, EC50 = 0.900808050726654), Rsquare = 0.986745954925997),\n        tolerance=1e-3)\n    \n    expect_equivalent(logLogisticRegression(c(0.1, 1, 2, 3), c(100, 70, 60, 50), trunc=TRUE), \n        logLogisticRegression(c(0.1, 1, 2, 3), c(500, 70, 60, 50), trunc=TRUE))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/matthewCor.R",
    "language": "R",
    "content": "## Matthews correlatipon coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    CoreGx::mcc(x, y, nperm, nthread)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mcc` function in this code snippet, and what package does it rely on?",
        "answer": "The `mcc` function is designed to compute a Matthews Correlation Coefficient. It relies on the `CoreGx` package, specifically calling the `CoreGx::mcc` function internally. This function appears to be a wrapper around the `CoreGx::mcc` function, providing the same functionality while potentially making it more accessible or consistent with the current package's naming conventions."
      },
      {
        "question": "What does the `#' @inherit CoreGx::mcc` comment do in the context of R package documentation?",
        "answer": "The `#' @inherit CoreGx::mcc` comment is a roxygen2 documentation tag. It indicates that this function should inherit the documentation from the `mcc` function in the `CoreGx` package. This is useful when creating wrapper functions, as it allows you to reuse existing documentation without duplicating it, ensuring consistency and reducing maintenance overhead."
      },
      {
        "question": "What are the parameters of the `mcc` function, and how does it handle them?",
        "answer": "The `mcc` function takes four parameters: `x`, `y`, `nperm` (with a default value of 1000), and `nthread` (with a default value of 1). These parameters are passed directly to the `CoreGx::mcc` function without any modification. This suggests that the wrapper function maintains the same interface as the original function, allowing users to specify the same parameters they would use with `CoreGx::mcc`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    # Complete the function body\n}",
        "complete": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    CoreGx::mcc(x, y, nperm, nthread)\n}"
      },
      {
        "partial": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, ...) {\n    # Complete the function signature and body\n}",
        "complete": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    CoreGx::mcc(x, y, nperm, nthread)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/downloadSignatures.R",
    "language": "R",
    "content": "#' Download Drug Perturbation Signatures\n#' \n#' This function allows you to download an array of drug perturbation\n#' signatures, as would be computed by the `drugPerturbationSig` function,\n#' for the available perturbation `PharmacoSets`. This function allows the\n#' user to skip these very lengthy calculation steps for the datasets available,\n#' and start their analysis from the already computed signatures\n#' \n#' @examples\n#'\n#' \\dontrun{\n#'     if (interactive()) downloadPertSig(\"CMAP_2016\")\n#' }\n#' \n#' @param name A `character(1)` string, the name of the PharmacoSet for which\n#'   to download signatures. The name should match the names returned in the\n#'   `PSet Name` column of `availablePSets(canonical=FALSE)`.\n#' @param saveDir A `character(1)` string with the folder path where the\n#'   PharmacoSet should be saved. Defaults to `\"./PSets/Sigs/\"`. Will\n#'   create directory if it does not exist.\n#' @param fileName `character(1)` What to name the downloaded file. Defaults\n#' to '`name`_signature.RData' when excluded.\n#' @param verbose `logical(1)` Should `downloader` show detailed messages?\n#' @param ... `pairlist` Force subsequent arguments to be named.\n#' @param myfn `character(1)` A deprecated version of `fileName`. Still works\n#' for now, but will be deprecated in future releases.\n#'\n#' @return An array type object contaning the signatures\n#'\n#' @export\n#' @importFrom CoreGx .warning .funContext\n#' @import downloader\ndownloadPertSig <- function(name, saveDir=file.path(\".\", \"PSets\", \"Sigs\"), \n    fileName, verbose=TRUE, ..., myfn) \n{\n    funContext <- .funContext('::downloadPertSig')\n    if (missing(fileName) && !missing(myfn)) {\n        .warning(funContext, 'The `myfn` parameter is being deprecated in \n            favour of `fileName`. It still works for now, but will be retired\n            in a future release.')\n        fileName <- myfn\n    }\n\n    # change the download timeout since the files are big\n    opts <- options()\n    options(timeout=600)\n    on.exit(options(opts))\n\n    # get the annotations for available data\n    pSetTable <- availablePSets(canonical=FALSE)\n\n    # pick a signature from the list\n    whichx <- match(name, pSetTable[, 3])\n    if (is.na(whichx)){\n        stop('Unknown Dataset. Please use the `Dataset Name` column in the\n            data.frame returned by the availablePSet function to select a\n            PharmacoSet')\n    }\n    if (!pSetTable[whichx, \"type\"] %in% c(\"perturbation\", \"both\")){\n        stop('Signatures are available only for perturbation type datasets')\n    }\n\n    if(!file.exists(saveDir)) {\n        dir.create(saveDir, recursive=TRUE)\n    }\n\n    if (missing(fileName)) {\n        fileName <- paste(pSetTable[whichx, ]$`Dataset Name`, \n            \"_signatures.RData\", sep=\"\")\n    }\n\n    downloader::download(\n        paste(\"https://www.pmgenomics.ca/bhklab/sites/default/files/downloads\", \n            fileName, sep='/'), \n        destfile=file.path(saveDir, fileName), quiet=!verbose, mode='wb')\n\n    sig <- load(file.path(saveDir, fileName))\n\n    return(get(sig))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `downloadPertSig` function and what type of data does it return?",
        "answer": "The `downloadPertSig` function is designed to download pre-computed drug perturbation signatures for available PharmacoSets. It returns an array-type object containing the signatures, allowing users to skip lengthy calculation steps and start their analysis with already computed signatures."
      },
      {
        "question": "How does the function handle the deprecated `myfn` parameter, and what is the recommended alternative?",
        "answer": "The function checks if `fileName` is missing and `myfn` is provided. If so, it issues a warning that `myfn` is being deprecated in favor of `fileName`. It then assigns the value of `myfn` to `fileName`. The recommended alternative is to use `fileName` instead of `myfn` for specifying the name of the downloaded file."
      },
      {
        "question": "What checks does the function perform before downloading the signatures, and what errors might it throw?",
        "answer": "The function performs several checks: 1) It verifies if the provided `name` exists in the available PharmacoSets. 2) It checks if the dataset is of type 'perturbation' or 'both'. If either check fails, it throws an error. The function may stop with an 'Unknown Dataset' error if the name is not found, or a 'Signatures are available only for perturbation type datasets' error if the dataset type is incorrect."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/inst/extdata/test_cellosaurus_detailed.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\nids <- c(\"HT\")\nto = cellosaurus_fields(common=T)\nfrom <- \"idsy\"\nfuzzy <- FALSE\nnumResults <- 1000\nsort = \"ac\"\nparsed = FALSE\nkeep_duplicates = FALSE\nquery <- AnnotationGx:::.create_cellosaurus_queries(ids, from, fuzzy)\nnames(query) <- ids\nrequests <- AnnotationGx:::.build_cellosaurus_request(\n    query = query,\n    to = to,\n    numResults = numResults,\n    sort = sort,\n    output = \"TXT\",\n    fuzzy = fuzzy\n)\nresponses <- AnnotationGx:::.perform_request_parallel(list(requests))\nnames(responses) <- as.character(ids)\n\nlines <- httr2::resp_body_string(responses[[ids[1]]]) |>\n    strsplit(\"\\n\") |>\n    unlist()\n\n# Test case 1: Test with a valid cell line name\nlines <- readRDS(system.file(\"extdata\", \"cellosaurus_HT_raw_lines.RDS\", package = \"AnnotationGx\"))\n\nparsed_lines <- \n    Map(\n    f = function(lines, i, j) {\n        lines[i:(j - 1L)]\n    },\n    i = grep(pattern = \"^ID\\\\s+\", x = lines, value = FALSE),\n    j = grep(pattern = \"^//$\", x = lines, value = FALSE),\n    MoreArgs = list(\"lines\" = lines),\n    USE.NAMES = FALSE\n)\n\n\n\nrequiredKeys = c(\"AC\", \"CA\", \"DT\", \"ID\")\nnestedKeys = c(\"DI\", \"DR\", \"HI\", \"OI\", \"OX\", \"WW\")\noptionalKeys = c(\"AG\", \"SX\", \"SY\", \"ACAS\", \"DIN\", \"DIO\", \"CH\", \"DTC\", \"DTU\", \"DTV\", \"FROM\", \"GROUP\")\nspecialKeys = c(\"CC\")\n\nx <- strSplit(parsed_lines[[1]], split = \"   \")\nx <- split(x[, 2L], f = x[, 1L])\n\n\ntest_that(\".formatComments works as expected\", {\n    # cc_column <- AnnotationGx:::.formatComments(x)\n\n    dt <- data.table::data.table(rbind(x))\n\n    da(); .formatComments(dt)\n\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cellosaurus_fields` function in this code snippet, and how is it used?",
        "answer": "The `cellosaurus_fields` function is used to retrieve a list of common fields from the Cellosaurus database. In this code, it's called with the argument `common=T`, which likely means it returns only the commonly used fields. The result is stored in the `to` variable, which is later used as a parameter in the `.build_cellosaurus_request` function to specify which fields should be included in the request."
      },
      {
        "question": "Explain the purpose and functionality of the `.perform_request_parallel` function in this code.",
        "answer": "The `.perform_request_parallel` function is used to execute multiple HTTP requests in parallel. It takes a list of requests (in this case, `list(requests)`) as an argument. The function likely sends these requests concurrently to improve efficiency when querying the Cellosaurus database. The responses are then stored in the `responses` variable, with the names of the responses set to the corresponding cell line IDs."
      },
      {
        "question": "What is the purpose of the `Map` function used near the end of the code snippet, and what does it do with the `lines` variable?",
        "answer": "The `Map` function is used here to parse the raw response lines into separate entries. It applies a custom function to each set of lines between 'ID' and '//' markers in the response. This function extracts each individual cell line entry from the raw response. The result, `parsed_lines`, is a list where each element contains the lines for a single cell line entry, effectively separating the raw response into structured data for further processing."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeAmax.R",
    "language": "R",
    "content": "#' Fits dose-response curves to data given by the user\n#' and returns the Amax of the fitted curve.\n#' Amax: 100 - viability at maximum concentarion (in fitted curve)\n#'\n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeAmax(dose, viability)\n#'\n#' @param concentration `numeric` is a vector of drug concentrations.\n#'\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of the log_conc, expressed as percentages\n#' of viability in the absence of any drug.\n#'\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical` should warnings be printed\n#' @return The numerical Amax\n#' @export\ncomputeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n\n  #CHECK THAT FUNCTION INPUTS ARE APPROPRIATE\n  if (!all(is.finite(concentration))) {\n    print(concentration)\n    stop(\"Concentration vector contains elements which are not real numbers.\")\n  }\n\n  if (!all(is.finite(viability))) {\n    print(viability)\n    stop(\"Viability vector contains elements which are not real numbers.\")\n  }\n\n  if (is.logical(trunc) == FALSE) {\n    print(trunc)\n    stop(\"'trunc' is not a logical.\")\n  }\n\n  if (length(concentration) != length(viability)) {\n    print(concentration)\n    print(viability)\n    stop(\"Concentration vector is not of same length as viability vector.\")\n  }\n\n  if (min(concentration) < 0) {\n    stop(\"Concentration vector contains negative data.\")\n  }\n\n  if (min(viability) < 0 & verbose) {\n    warning(\"Warning: Negative viability data.\")\n  }\n\n  if (max(viability) > 100 & verbose) {\n    warning(\"Warning: Viability data exceeds negative control.\")\n  }\n\n  #CONVERT DOSE-RESPONSE DATA TO APPROPRIATE INTERNAL REPRESENTATION\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc == TRUE) {\n    viability[which(viability < 0)] <- 0\n    viability[which(viability > 1)] <- 1\n  }\n\n  #FIT CURVE AND CALCULATE IC50\n  pars <- unlist(logLogisticRegression(log_conc,\n                                       viability,\n                                       conc_as_log = TRUE,\n                                       viability_as_pct = FALSE,\n                                       trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAmax` function and what does it return?",
        "answer": "The `computeAmax` function fits dose-response curves to given data and returns the Amax of the fitted curve. Amax is defined as 100 minus the viability at the maximum concentration in the fitted curve. The function returns a numerical value representing Amax."
      },
      {
        "question": "How does the function handle missing or zero concentration values in the input data?",
        "answer": "The function removes NA values from both concentration and viability vectors using `!is.na()`. It also removes entries where the concentration is zero. This is done to ensure that only valid, non-zero concentration data is used for curve fitting."
      },
      {
        "question": "What data preprocessing steps does the function perform before fitting the curve?",
        "answer": "The function performs several preprocessing steps: 1) It converts concentration to log10 scale. 2) It normalizes viability data by dividing by 100. 3) If `trunc=TRUE`, it truncates viability values to be between 0 and 1. 4) It checks for negative concentrations, negative viabilities, and viabilities exceeding 100%, issuing warnings or errors as appropriate."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n\n  # Input validation checks\n  # ...\n\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc == TRUE) {\n    viability[which(viability < 0)] <- 0\n    viability[which(viability > 1)] <- 1\n  }\n\n  # Fit curve and calculate Amax\n  # ...\n\n  return(x)\n}",
        "complete": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n\n  if (!all(is.finite(concentration)) || !all(is.finite(viability))) {\n    stop(\"Concentration or viability vector contains non-finite elements.\")\n  }\n  if (!is.logical(trunc)) stop(\"'trunc' must be logical.\")\n  if (length(concentration) != length(viability)) stop(\"Concentration and viability vectors must have the same length.\")\n  if (min(concentration) < 0) stop(\"Concentration vector contains negative data.\")\n  if (verbose) {\n    if (min(viability) < 0) warning(\"Negative viability data.\")\n    if (max(viability) > 100) warning(\"Viability data exceeds negative control.\")\n  }\n\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc) {\n    viability <- pmax(pmin(viability, 1), 0)\n  }\n\n  pars <- unlist(logLogisticRegression(log_conc, viability, conc_as_log = TRUE, viability_as_pct = FALSE, trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n}"
      },
      {
        "partial": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  # Data preprocessing\n  # ...\n\n  # Input validation\n  # ...\n\n  # Data transformation\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc) {\n    # Truncate viability\n    # ...\n  }\n\n  # Curve fitting and Amax calculation\n  pars <- unlist(logLogisticRegression(log_conc, viability, conc_as_log = TRUE, viability_as_pct = FALSE, trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n}",
        "complete": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  concentration <- concentration[concentration != 0]\n  viability <- viability[concentration != 0]\n\n  if (!all(is.finite(c(concentration, viability)))) stop(\"Non-finite elements in concentration or viability.\")\n  if (!is.logical(trunc)) stop(\"'trunc' must be logical.\")\n  if (length(concentration) != length(viability)) stop(\"Mismatched vector lengths.\")\n  if (min(concentration) < 0) stop(\"Negative concentration data.\")\n  if (verbose) {\n    if (min(viability) < 0) warning(\"Negative viability data.\")\n    if (max(viability) > 100) warning(\"Viability exceeds 100%.\")\n  }\n\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc) {\n    viability <- pmax(pmin(viability, 1), 0)\n  }\n\n  pars <- unlist(logLogisticRegression(log_conc, viability, conc_as_log = TRUE, viability_as_pct = FALSE, trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/unichem.R",
    "language": "R",
    "content": "\n# Unichem API documentation: https://www.ebi.ac.uk/unichem/info/webservices\n\n#' Get the list of sources in UniChem.\n#' \n#' @param all_columns `boolean` Whether to return all columns. Defaults to FALSE.\n#' \n\n#' \n#' Returns a `data.table` with the following columns:\n#' - `CompoundCount` (integer): Total of compounds provided by that source\n#' - `BaseURL` (string): Source Base URL for compounds\n#' - `Description` (string): Source database description\n#' - `LastUpdated` (string): Date in which the source database was last updated\n#' - `Name` (string): Short name of the source database\n#' - `NameLabel` (string): Machine readable label name of the source database\n#' - `NameLong` (string): Full name of the source database\n#' - `SourceID` (integer): Unique ID for the source database\n#' - `Details` (string): Notes about the source\n#' - `ReleaseDate` (string): Date in which the source database was released\n#' - `ReleaseNumber` (integer): Release number of the source database data stored in UniChEM\n#' - `URL` (string): Main URL for the source\n#' - `UpdateComments` (string): Notes about the update process of that source to UniChEM\n#' \n#' \n#' @return A data.table with the list of sources in UniChem.\n#' \n#' @export\ngetUnichemSources <- function(all_columns = FALSE) {\n    funContext <- .funContext(\"AnnotationGx::getUnichemSources\")\n\n    response <- .build_unichem_query(\"sources\") |>\n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() \n    \n    if(response$response != \"Success\"){\n        .err(funContext, \"Unichem API request failed.\")\n    }\n\n    .debug(funContext, sprintf(\"Unichem sourceCount: %s\", response$totalSources))\n\n    sources_dt <- .asDT(response$sources)\n\n    old_names <- c(\n        \"UCICount\", \"baseIdUrl\", \"description\", \"lastUpdated\", \"name\", \n        \"nameLabel\", \"nameLong\", \"sourceID\", \"srcDetails\", \"srcReleaseDate\", \n        \"srcReleaseNumber\", \"srcUrl\", \"updateComments\")\n\n    new_names <- c(\n        \"CompoundCount\", \"BaseURL\", \"Description\", \"LastUpdated\", \"Name\", \n        \"NameLabel\", \"NameLong\", \"SourceID\", \"Details\", \"ReleaseDate\",\n        \"ReleaseNumber\", \"URL\", \"UpdateComments\")\n    \n    data.table::setnames(sources_dt, old_names, new_names)\n\n    new_order <- c(\n        \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n        \"BaseURL\", \"URL\", \"Details\",\n        \"Description\", \"ReleaseNumber\", \"ReleaseDate\", \"LastUpdated\", \n        \"UpdateComments\"\n    )\n\n\n    sources_dt <- sources_dt[, new_order, with = FALSE]\n\n    if(all_columns) return(sources_dt)\n\n    sources_dt[, c(\"Name\", \"SourceID\")]\n\n}\n\n#' Query UniChem for a compound.\n#' \n#' This function queries the UniChem API for a compound based on the provided parameters.\n#' \n#' @param compound `character` or `integer` The compound identifier to search for.\n#' @param type `character` The type of compound identifier to search for. Valid types are \"uci\", \"inchi\", \"inchikey\", and \"sourceID\".\n#' @param sourceID `integer` The source ID to search for if the type is \"sourceID\". Defaults to NULL.\n#' @param request_only `boolean` Whether to return the request only. Defaults to FALSE.\n#' @param raw `boolean` Whether to return the raw response. Defaults to FALSE.\n#' @param ... Additional arguments.\n#' \n#' @return A list with the external mappings and the UniChem mappings.\n#' \n#' @examples\n#' queryUnichemCompound(type = \"sourceID\", compound = \"444795\", sourceID = 22)\n#' \n#' @export\nqueryUnichemCompound <- function(\n    compound, type, sourceID = NA_integer_, request_only = FALSE, raw = FALSE, ...\n){\n    checkmate::assert_string(type)\n    checkmate::assert_atomic(compound)\n    checkmate::assert_integerish(sourceID)\n    checkmate::assertLogical(request_only)\n    checkmate::assertLogical(raw)\n\n    request <- .build_unichem_compound_req(type, compound, sourceID,...)\n    if(request_only) return(request)\n\n    response <- request |> \n        .perform_request() |>  \n        .parse_resp_json() \n    \n    if(raw) return(response)\n\n    if(response$response != \"Success\"){\n        msg <- paste(\n            \"Unichem API request failed for compound\", compound, \"\n            with type\", type, \n            \" . Error:\", response$error\n        )\n        .err(.funContext(\"AnnotationGx::queryUnichemCompound\"), msg)\n    }\n\n    # Mapping names to be consistent with other API calls\n    mapped_sources_dt <- .asDT(response$compounds$sources)\n    old_names <- c(\"compoundId\", \"shortName\", \"longName\", \"id\", \"url\")\n\n    new_names <- c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n    data.table::setnames(mapped_sources_dt, old = old_names, new = new_names)\n\n    External_Mappings <- mapped_sources_dt[, new_names, with = FALSE]\n    \n    UniChem_Mappings <- list(\n        UniChem.UCI = response$compounds$uci,\n        UniChem.InchiKey = response$compounds$standardInchiKey,\n        UniChem.Inchi = response$compounds$inchi$inchi,\n        UniChem.formula = response$compounds$inchi$formula,\n        UniChem.connections = response$compounds$inchi$connections,\n        UniChem.hAtoms = response$compounds$inchi$hAtoms\n    )\n\n    list(\n        External_Mappings = External_Mappings,\n        UniChem_Mappings = UniChem_Mappings\n    )\n\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getUnichemSources` function and what are its key parameters?",
        "answer": "The `getUnichemSources` function retrieves a list of sources from the UniChem database. It has one key parameter: `all_columns` (boolean), which determines whether to return all columns (when TRUE) or just the 'Name' and 'SourceID' columns (when FALSE, which is the default)."
      },
      {
        "question": "In the `queryUnichemCompound` function, what are the valid types for the `type` parameter, and how does the function handle different types of compound identifiers?",
        "answer": "The valid types for the `type` parameter in `queryUnichemCompound` are 'uci', 'inchi', 'inchikey', and 'sourceID'. The function builds a request based on the specified type and compound identifier. If the type is 'sourceID', an additional `sourceID` parameter is required. The function then performs the request and parses the JSON response, returning a list with External_Mappings and UniChem_Mappings."
      },
      {
        "question": "How does the `getUnichemSources` function handle column renaming and reordering, and why might this be important?",
        "answer": "The `getUnichemSources` function renames columns using `data.table::setnames()` to provide more descriptive and consistent names (e.g., 'UCICount' to 'CompoundCount'). It then reorders the columns using a predefined `new_order` vector. This renaming and reordering is important for improving readability, maintaining consistency across the API, and ensuring that the most relevant information (like 'Name' and 'SourceID') appears first in the output."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getUnichemSources <- function(all_columns = FALSE) {\n    funContext <- .funContext(\"AnnotationGx::getUnichemSources\")\n\n    response <- .build_unichem_query(\"sources\") |>\n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() \n    \n    if(response$response != \"Success\"){\n        .err(funContext, \"Unichem API request failed.\")\n    }\n\n    .debug(funContext, sprintf(\"Unichem sourceCount: %s\", response$totalSources))\n\n    sources_dt <- .asDT(response$sources)\n\n    old_names <- c(\n        \"UCICount\", \"baseIdUrl\", \"description\", \"lastUpdated\", \"name\", \n        \"nameLabel\", \"nameLong\", \"sourceID\", \"srcDetails\", \"srcReleaseDate\", \n        \"srcReleaseNumber\", \"srcUrl\", \"updateComments\")\n\n    new_names <- c(\n        \"CompoundCount\", \"BaseURL\", \"Description\", \"LastUpdated\", \"Name\", \n        \"NameLabel\", \"NameLong\", \"SourceID\", \"Details\", \"ReleaseDate\",\n        \"ReleaseNumber\", \"URL\", \"UpdateComments\")\n    \n    data.table::setnames(sources_dt, old_names, new_names)\n\n    new_order <- c(\n        \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n        \"BaseURL\", \"URL\", \"Details\",\n        \"Description\", \"ReleaseNumber\", \"ReleaseDate\", \"LastUpdated\", \n        \"UpdateComments\"\n    )\n\n\n    sources_dt <- sources_dt[, new_order, with = FALSE]\n\n    if(all_columns) return(sources_dt)\n\n    # Complete the function by returning the appropriate columns\n}",
        "complete": "getUnichemSources <- function(all_columns = FALSE) {\n    funContext <- .funContext(\"AnnotationGx::getUnichemSources\")\n\n    response <- .build_unichem_query(\"sources\") |>\n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() \n    \n    if(response$response != \"Success\"){\n        .err(funContext, \"Unichem API request failed.\")\n    }\n\n    .debug(funContext, sprintf(\"Unichem sourceCount: %s\", response$totalSources))\n\n    sources_dt <- .asDT(response$sources)\n\n    old_names <- c(\n        \"UCICount\", \"baseIdUrl\", \"description\", \"lastUpdated\", \"name\", \n        \"nameLabel\", \"nameLong\", \"sourceID\", \"srcDetails\", \"srcReleaseDate\", \n        \"srcReleaseNumber\", \"srcUrl\", \"updateComments\")\n\n    new_names <- c(\n        \"CompoundCount\", \"BaseURL\", \"Description\", \"LastUpdated\", \"Name\", \n        \"NameLabel\", \"NameLong\", \"SourceID\", \"Details\", \"ReleaseDate\",\n        \"ReleaseNumber\", \"URL\", \"UpdateComments\")\n    \n    data.table::setnames(sources_dt, old_names, new_names)\n\n    new_order <- c(\n        \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n        \"BaseURL\", \"URL\", \"Details\",\n        \"Description\", \"ReleaseNumber\", \"ReleaseDate\", \"LastUpdated\", \n        \"UpdateComments\"\n    )\n\n\n    sources_dt <- sources_dt[, new_order, with = FALSE]\n\n    if(all_columns) return(sources_dt)\n\n    sources_dt[, c(\"Name\", \"SourceID\")]\n}"
      },
      {
        "partial": "queryUnichemCompound <- function(\n    compound, type, sourceID = NA_integer_, request_only = FALSE, raw = FALSE, ...\n){\n    checkmate::assert_string(type)\n    checkmate::assert_atomic(compound)\n    checkmate::assert_integerish(sourceID)\n    checkmate::assertLogical(request_only)\n    checkmate::assertLogical(raw)\n\n    request <- .build_unichem_compound_req(type, compound, sourceID,...)\n    if(request_only) return(request)\n\n    response <- request |> \n        .perform_request() |>  \n        .parse_resp_json() \n    \n    if(raw) return(response)\n\n    if(response$response != \"Success\"){\n        msg <- paste(\n            \"Unichem API request failed for compound\", compound, \"\n            with type\", type, \n            \" . Error:\", response$error\n        )\n        .err(.funContext(\"AnnotationGx::queryUnichemCompound\"), msg)\n    }\n\n    # Mapping names to be consistent with other API calls\n    mapped_sources_dt <- .asDT(response$compounds$sources)\n    old_names <- c(\"compoundId\", \"shortName\", \"longName\", \"id\", \"url\")\n\n    new_names <- c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n    data.table::setnames(mapped_sources_dt, old = old_names, new = new_names)\n\n    External_Mappings <- mapped_sources_dt[, new_names, with = FALSE]\n    \n    # Complete the function by creating the UniChem_Mappings list and returning the result\n}",
        "complete": "queryUnichemCompound <- function(\n    compound, type, sourceID = NA_integer_, request_only = FALSE, raw = FALSE, ...\n){\n    checkmate::assert_string(type)\n    checkmate::assert_atomic(compound)\n    checkmate::assert_integerish(sourceID)\n    checkmate::assertLogical(request_only)\n    checkmate::assertLogical(raw)\n\n    request <- .build_unichem_compound_req(type, compound, sourceID,...)\n    if(request_only) return(request)\n\n    response <- request |> \n        .perform_request() |>  \n        .parse_resp_json() \n    \n    if(raw) return(response)\n\n    if(response$response != \"Success\"){\n        msg <- paste(\n            \"Unichem API request failed for compound\", compound, \"\n            with type\", type, \n            \" . Error:\", response$error\n        )\n        .err(.funContext(\"AnnotationGx::queryUnichemCompound\"), msg)\n    }\n\n    # Mapping names to be consistent with other API calls\n    mapped_sources_dt <- .asDT(response$compounds$sources)\n    old_names <- c(\"compoundId\", \"shortName\", \"longName\", \"id\", \"url\")\n\n    new_names <- c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n    data.table::setnames(mapped_sources_dt, old = old_names, new = new_names)\n\n    External_Mappings <- mapped_sources_dt[, new_names, with = FALSE]\n    \n    UniChem_Mappings <- list(\n        UniChem.UCI = response$compounds$uci,\n        UniChem.InchiKey = response$compounds$standardInchiKey,\n        UniChem.Inchi = response$compounds$inchi$inchi,\n        UniChem.formula = response$compounds$inchi$formula,\n        UniChem.connections = response$compounds$inchi$connections,\n        UniChem.hAtoms = response$compounds$inchi$hAtoms\n    )\n\n    list(\n        External_Mappings = External_Mappings,\n        UniChem_Mappings = UniChem_Mappings\n    )\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeABC.R",
    "language": "R",
    "content": "#' Fits dose-response curves to data given by the user\n#' and returns the ABC of the fitted curves.\n#'\n#' @examples\n#' dose1 <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability1 <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' dose2 <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability2 <- c(100.94,112.5,86,104.16,75,68,48,29)\n#' computeABC(dose1, dose2, viability1, viability2)\n#'\n#' @param conc1 `numeric` is a vector of drug concentrations.\n#' @param conc2 `numeric` is a vector of drug concentrations.\n#' @param viability1 `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc1, expressed as percentages\n#' of viability in the absence of any drug.\n#' @param viability2 `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc2, expressed as percentages\n#' of viability in the absence of any drug.\n#' @param Hill_fit1 `list` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param Hill_fit2 `lis` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and returns ABC as a decimal. Otherwise, viability is interpreted as percent, and AUC is returned 0-100.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#'\n#' @author Mark Freeman\n#'\n#' @return The numeric area of the absolute difference between the two hill slopes\n#'\n#' @importFrom CoreGx .getSupportVec\n#' @export\ncomputeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n\nif (missing(conc1) | missing(conc2)){\n\n    stop(\"Both Concentration vectors the drugs were tested on must always be provided.\")\n\n}\nif (missing(Hill_fit1) | missing(Hill_fit2)) {\n\n    Hill_fit1 <- logLogisticRegression(conc1,\n      viability1,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=conc1,\n      Hill_fit=Hill_fit1,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars1 <- cleanData[[\"Hill_fit\"]]\n    log_conc1 <- cleanData[[\"log_conc\"]]\n    Hill_fit2 <- logLogisticRegression(conc2,\n      viability2,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=conc2,\n      Hill_fit=Hill_fit2,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars2 <- cleanData[[\"Hill_fit\"]]\n    log_conc2 <- cleanData[[\"log_conc\"]]\n\n} else {\n\n  cleanData <- sanitizeInput(conc = conc1,\n    viability = viability1,\n    Hill_fit = Hill_fit1,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  pars1 <- cleanData[[\"Hill_fit\"]]\n  log_conc1 <- cleanData[[\"log_conc\"]]\n  cleanData <- sanitizeInput(conc = conc2,\n    viability = viability2,\n    Hill_fit = Hill_fit2,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  pars2 <- cleanData[[\"Hill_fit\"]]\n  log_conc2 <- cleanData[[\"log_conc\"]]\n}\n\n  #FIT CURVE AND CALCULATE IC50\n  if (max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)) {\n    return(NA)\n  } else {\n    extrema <- sort(c(min(log_conc1), max(log_conc1), min(log_conc2), max(log_conc2)))\n    support <- .getSupportVec(c(extrema[2], extrema[3]))\n    ABC <- as.numeric(caTools::trapz(support, abs(.Hill(support, pars1) - .Hill(support, pars2))) / (extrema[3] - extrema[2]))\n    if(viability_as_pct){\n      ABC <- ABC*100\n    }\n    return(ABC)\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeABC` function and what does it return?",
        "answer": "The `computeABC` function fits dose-response curves to given data and returns the Area Between Curves (ABC) of the fitted curves. It calculates the numeric area of the absolute difference between two Hill slopes, which represents the difference in drug response between two conditions or treatments."
      },
      {
        "question": "How does the function handle missing `Hill_fit` parameters?",
        "answer": "If `Hill_fit1` or `Hill_fit2` are missing, the function automatically calculates them using the `logLogisticRegression` function. It then sanitizes the input data using the `sanitizeInput` function before proceeding with the ABC calculation. This allows the function to work with either pre-calculated Hill fits or raw concentration and viability data."
      },
      {
        "question": "What condition causes the function to return NA, and why is this check important?",
        "answer": "The function returns NA if the maximum concentration of one dataset is less than the minimum concentration of the other dataset (i.e., `max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)`). This check is important because it ensures that there is an overlapping concentration range between the two datasets, which is necessary for a meaningful ABC calculation. Without this overlap, the comparison would not be valid."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n\n  if (missing(conc1) | missing(conc2)){\n    stop(\"Both Concentration vectors the drugs were tested on must always be provided.\")\n  }\n  if (missing(Hill_fit1) | missing(Hill_fit2)) {\n    # Code to handle missing Hill fits\n  } else {\n    # Code to handle provided Hill fits\n  }\n\n  # Fit curve and calculate IC50\n  if (max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)) {\n    return(NA)\n  } else {\n    # Code to calculate ABC\n  }\n}",
        "complete": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n\n  if (missing(conc1) | missing(conc2)){\n    stop(\"Both Concentration vectors the drugs were tested on must always be provided.\")\n  }\n  if (missing(Hill_fit1) | missing(Hill_fit2)) {\n    Hill_fit1 <- logLogisticRegression(conc1, viability1, conc_as_log, viability_as_pct, trunc, verbose)\n    cleanData <- sanitizeInput(conc1, Hill_fit=Hill_fit1, conc_as_log, viability_as_pct, trunc, verbose)\n    pars1 <- cleanData[\"Hill_fit\"]\n    log_conc1 <- cleanData[\"log_conc\"]\n    Hill_fit2 <- logLogisticRegression(conc2, viability2, conc_as_log, viability_as_pct, trunc, verbose)\n    cleanData <- sanitizeInput(conc2, Hill_fit=Hill_fit2, conc_as_log, viability_as_pct, trunc, verbose)\n    pars2 <- cleanData[\"Hill_fit\"]\n    log_conc2 <- cleanData[\"log_conc\"]\n  } else {\n    cleanData <- sanitizeInput(conc1, viability1, Hill_fit1, conc_as_log, viability_as_pct, trunc, verbose)\n    pars1 <- cleanData[\"Hill_fit\"]\n    log_conc1 <- cleanData[\"log_conc\"]\n    cleanData <- sanitizeInput(conc2, viability2, Hill_fit2, conc_as_log, viability_as_pct, trunc, verbose)\n    pars2 <- cleanData[\"Hill_fit\"]\n    log_conc2 <- cleanData[\"log_conc\"]\n  }\n\n  if (max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)) {\n    return(NA)\n  } else {\n    extrema <- sort(c(min(log_conc1), max(log_conc1), min(log_conc2), max(log_conc2)))\n    support <- .getSupportVec(c(extrema[2], extrema[3]))\n    ABC <- as.numeric(caTools::trapz(support, abs(.Hill(support, pars1) - .Hill(support, pars2))) / (extrema[3] - extrema[2]))\n    if(viability_as_pct) ABC <- ABC * 100\n    return(ABC)\n  }\n}"
      },
      {
        "partial": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n  # Check if concentration vectors are provided\n  \n  # Handle missing or provided Hill fits\n  \n  # Calculate ABC\n  \n  # Return result\n}",
        "complete": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n  if (missing(conc1) | missing(conc2)) stop(\"Both Concentration vectors must be provided.\")\n  \n  process_data <- function(conc, viability, Hill_fit) {\n    if (missing(Hill_fit)) {\n      Hill_fit <- logLogisticRegression(conc, viability, conc_as_log, viability_as_pct, trunc, verbose)\n    }\n    cleanData <- sanitizeInput(conc, viability, Hill_fit, conc_as_log, viability_as_pct, trunc, verbose)\n    list(pars = cleanData[\"Hill_fit\"], log_conc = cleanData[\"log_conc\"])\n  }\n  \n  data1 <- process_data(conc1, viability1, Hill_fit1)\n  data2 <- process_data(conc2, viability2, Hill_fit2)\n  \n  if (max(data1$log_conc) < min(data2$log_conc) | max(data2$log_conc) < min(data1$log_conc)) return(NA)\n  \n  extrema <- sort(c(range(data1$log_conc), range(data2$log_conc)))\n  support <- .getSupportVec(extrema[2:3])\n  ABC <- as.numeric(caTools::trapz(support, abs(.Hill(support, data1$pars) - .Hill(support, data2$pars))) / diff(extrema[2:3]))\n  \n  if(viability_as_pct) ABC <- ABC * 100\n  ABC\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-split.R",
    "language": "R",
    "content": "\n# The following functions are taken from the AcidBase package by acidgenomics using their\n# license. Adding the package as a dependency is the better approach but fails on the \n# CI/CD pipeline as the package is not available on CRAN.\n# TODO:: Add the package as a dependency and remove the following functions.\n# TODO:: reach out to the author to discuss the license and the possibility of \n#        adding the package as a dependency.\n\n#' Split a character vector into a matrix based on a delimiter\n#'\n#' This function splits a character vector into a matrix based on a specified delimiter.\n#' It can handle both finite and infinite splits.\n#'\n#' @param x A character vector to be split\n#' @param split A character string specifying the delimiter\n#' @param fixed A logical value indicating whether the delimiter should be treated as a fixed string\n#' @param n An integer specifying the maximum number of splits to be performed\n#'\n#' @return A matrix where each row represents a split element\n#'\n#' @examples\n#' strSplit(\"Hello,World\", \",\")\n#' # Output:\n#' #      [,1]    [,2]   \n#' # [1,] \"Hello\" \"World\"\n#'\n#' @export\nstrSplit <- function(x, split, fixed = TRUE, n = Inf) {\n\n  if (is.finite(n)) {\n    x <- .strSplitFinite(x = x, split = split, n = n, fixed = fixed)\n  } else {\n    x <- .strSplitInfinite(x = x, split = split, fixed = fixed)\n  }\n  n2 <- lengths(x)\n  assert(\n    length(unique(n2)) == 1L,\n    msg = sprintf(\n      \"Split mismatch detected: %s.\",\n      toString(which(n2 != n2[[1L]]))\n    )\n  )\n  n2 <- n2[[1L]]\n  x <- unlist(x = x, recursive = FALSE, use.names = FALSE)\n  x <- matrix(data = x, ncol = n2, byrow = TRUE)\n  x\n}\n\n\n#' Split a string into multiple substrings based on a delimiter\n#'\n#' This function splits a given string into multiple substrings based on a specified delimiter.\n#' The number of resulting substrings can be controlled using the 'n' parameter.\n#'\n#' @param x The input string to be split.\n#' @param split The delimiter used to split the string.\n#' @param n The maximum number of substrings to be generated.\n#' @param fixed A logical value indicating whether the 'split' parameter should be treated as a fixed string or a regular expression.\n#'\n#' @return A character vector containing the resulting substrings.\n#'\n#' @examples\n#' str <- \"Hello,World,How,Are,You\"\n#' .strSplitFinite(str, \",\", 3, fixed = TRUE)\n#'\n#' @noRd\n#' @keywords internal\n.strSplitFinite <- function(x, split, n, fixed) {\n\n    checkmate::assertString(split)\n    checkmate::assertFlag(fixed)\n    checkmate::assert_integerish(n, lower = 2L, upper = Inf)\n    checkmate::assert_character(x)\n\n    m <- gregexpr(pattern = split, text = x, fixed = fixed)\n    ln <- lengths(m)\n    assert(\n        all((ln + 1L) >= n),\n        msg = sprintf(\n            \"Not enough to split: %s.\",\n            toString(which((ln + 1L) < n))\n        )\n    )\n    Map(\n        x = x,\n        m = m,\n        n = n,\n        f = function(x, m, n) {\n            ml <- attr(m, \"match.length\")\n            nl <- seq_len(n)\n            m <- m[nl]\n            ml <- ml[nl]\n            out <- substr(x = x, start = 1L, stop = m[[1L]] - 1L)\n            i <- 1L\n            while (i < (length(m) - 1L)) {\n                out <- append(\n                    x = out,\n                    values = substr(\n                        x = x,\n                        start = m[[i]] + ml[[i]],\n                        stop = m[[i + 1L]] - 1L\n                    )\n                )\n                i <- i + 1L\n            }\n            out <- append(\n                x = out,\n                values = substr(\n                    x = x,\n                    start = m[[n - 1L]] + ml[[n - 1L]],\n                    stop = nchar(x)\n                )\n            )\n            out\n        },\n        USE.NAMES = FALSE\n    )\n}\n\n\n#' Split a character vector into substrings based on a delimiter\n#'\n#' This function splits a character vector into substrings based on a specified delimiter.\n#' It uses the `strsplit` function from the base R package.\n#'\n#' @param x A character vector to be split.\n#' @param split A character string specifying the delimiter to use for splitting.\n#' @param fixed A logical value indicating whether the delimiter should be treated as a fixed string.\n#'              If `TRUE`, the delimiter is treated as a fixed string; if `FALSE`, it is treated as a regular expression.\n#'\n#' @return A list of character vectors, where each element of the list corresponds to the substrings obtained from splitting the input vector.\n#'\n#' @examples\n#' x <- c(\"apple,banana,orange\", \"cat,dog,rabbit\")\n#' .strSplitInfinite(x, \",\", fixed = TRUE)\n#'\n#' @noRd\n#' @keywords internal\n.strSplitInfinite <- function(x, split, fixed) {\n    checkmate::assertCharacter(x)\n    checkmate::assertString(split)\n    checkmate::assertFlag(fixed)\n    strsplit(x = x, split = split, fixed = fixed)\n}\n\n\n\n#' Split a column into a character list\n#'\n#' @note Updated 2023-09-22.\n#' @noRd\n.splitCol <- function(object, colName, split = \"; \") {\n  checkmate::assert_class(object, \"data.table\")\n  object[[colName]] <- strsplit(object[[colName]], split = split, fixed = TRUE)\n  object\n}\n\n#' Split a nested column by key\n#'\n#' Don't format key names into camel case -- too CPU intensive.\n#'\n#' @note Updated 2023-09-22.\n#' @noRd\n.splitNestedCol <- function(object, colName, split) {\n    # assert(\n    #     is(object, \"DFrame\"),\n    #     is(object[[colName]], \"CharacterList\"),\n    #     isString(split)\n    # )\n    lst <- lapply(\n        X = object[[colName]],\n        split = split,\n        FUN = function(x, split) {\n            if (identical(x, character())) {\n                return(list())\n            }\n            x <- strSplit(x = x, split = split, n = 2L)\n            ## Formatting into camel case takes too long.\n            ## > x[, 1L] <- camelCase(x[, 1L])\n            x <- split(x = x[, 2L], f = x[, 1L])\n            x\n        }\n    ) |> unlist(recursive = F)\n    object[[colName]] <- lst\n    object\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `strSplit` function and how does it handle different types of splits?",
        "answer": "The `strSplit` function is designed to split a character vector into a matrix based on a specified delimiter. It handles two types of splits: finite and infinite. For finite splits (when `n` is a finite number), it uses the `.strSplitFinite` helper function. For infinite splits (when `n` is `Inf`), it uses the `.strSplitInfinite` helper function. The function ensures that all splits result in the same number of elements and returns a matrix where each row represents a split element."
      },
      {
        "question": "How does the `.strSplitFinite` function differ from the `.strSplitInfinite` function in terms of implementation and use case?",
        "answer": "The `.strSplitFinite` function is used when a specific number of splits is desired. It uses `gregexpr` to find split positions and then manually extracts substrings using `substr`. This allows for precise control over the number of resulting substrings. The `.strSplitInfinite` function, on the other hand, is simpler and uses the base R `strsplit` function to split the input without a limit on the number of resulting substrings. `.strSplitFinite` is more complex but offers more control, while `.strSplitInfinite` is simpler but less flexible in terms of controlling the number of splits."
      },
      {
        "question": "What is the purpose of the `.splitNestedCol` function and how does it process nested data?",
        "answer": "The `.splitNestedCol` function is designed to split a nested column in a data structure (likely a data.frame or data.table) by a specified key. It processes each element of the nested column by splitting it into key-value pairs using the provided split character. The function then creates a list where each key becomes a list element containing its corresponding value. This is useful for unpacking nested data structures within a column, transforming them into a more easily accessible format. The function notably avoids formatting key names into camel case to save processing time."
      }
    ],
    "completion_tasks": [
      {
        "partial": "strSplit <- function(x, split, fixed = TRUE, n = Inf) {\n  if (is.finite(n)) {\n    x <- .strSplitFinite(x = x, split = split, n = n, fixed = fixed)\n  } else {\n    x <- .strSplitInfinite(x = x, split = split, fixed = fixed)\n  }\n  n2 <- lengths(x)\n  assert(\n    length(unique(n2)) == 1L,\n    msg = sprintf(\n      \"Split mismatch detected: %s.\",\n      toString(which(n2 != n2[[1L]]))\n    )\n  )\n  n2 <- n2[[1L]]\n  x <- unlist(x = x, recursive = FALSE, use.names = FALSE)\n  # Complete the function by adding the final step\n}",
        "complete": "strSplit <- function(x, split, fixed = TRUE, n = Inf) {\n  if (is.finite(n)) {\n    x <- .strSplitFinite(x = x, split = split, n = n, fixed = fixed)\n  } else {\n    x <- .strSplitInfinite(x = x, split = split, fixed = fixed)\n  }\n  n2 <- lengths(x)\n  assert(\n    length(unique(n2)) == 1L,\n    msg = sprintf(\n      \"Split mismatch detected: %s.\",\n      toString(which(n2 != n2[[1L]]))\n    )\n  )\n  n2 <- n2[[1L]]\n  x <- unlist(x = x, recursive = FALSE, use.names = FALSE)\n  matrix(data = x, ncol = n2, byrow = TRUE)\n}"
      },
      {
        "partial": ".strSplitFinite <- function(x, split, n, fixed) {\n  checkmate::assertString(split)\n  checkmate::assertFlag(fixed)\n  checkmate::assert_integerish(n, lower = 2L, upper = Inf)\n  checkmate::assert_character(x)\n\n  m <- gregexpr(pattern = split, text = x, fixed = fixed)\n  ln <- lengths(m)\n  assert(\n    all((ln + 1L) >= n),\n    msg = sprintf(\n      \"Not enough to split: %s.\",\n      toString(which((ln + 1L) < n))\n    )\n  )\n  # Complete the function by adding the Map operation\n}",
        "complete": ".strSplitFinite <- function(x, split, n, fixed) {\n  checkmate::assertString(split)\n  checkmate::assertFlag(fixed)\n  checkmate::assert_integerish(n, lower = 2L, upper = Inf)\n  checkmate::assert_character(x)\n\n  m <- gregexpr(pattern = split, text = x, fixed = fixed)\n  ln <- lengths(m)\n  assert(\n    all((ln + 1L) >= n),\n    msg = sprintf(\n      \"Not enough to split: %s.\",\n      toString(which((ln + 1L) < n))\n    )\n  )\n  Map(\n    x = x,\n    m = m,\n    n = n,\n    f = function(x, m, n) {\n      ml <- attr(m, \"match.length\")\n      nl <- seq_len(n)\n      m <- m[nl]\n      ml <- ml[nl]\n      out <- substr(x = x, start = 1L, stop = m[[1L]] - 1L)\n      i <- 1L\n      while (i < (length(m) - 1L)) {\n        out <- append(\n          x = out,\n          values = substr(\n            x = x,\n            start = m[[i]] + ml[[i]],\n            stop = m[[i + 1L]] - 1L\n          )\n        )\n        i <- i + 1L\n      }\n      out <- append(\n        x = out,\n        values = substr(\n          x = x,\n          start = m[[n - 1L]] + ml[[n - 1L]],\n          stop = nchar(x)\n        )\n      )\n      out\n    },\n    USE.NAMES = FALSE\n  )\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CTRP/CTRP_sampleMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]\n\n\nusethis::use_data(CTRP_sampleMetadata, overwrite = TRUE)\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'system.file()' function in the first line of the code snippet?",
        "answer": "The 'system.file()' function is used to locate and construct the file path for a specific file within an R package. In this case, it's locating the 'CTRPv2_meta_per_cell_line.txt' file in the 'extdata/CTRP' directory of the 'AnnotationGx' package. This function is particularly useful for accessing files that are distributed with a package."
      },
      {
        "question": "How does the code extract specific columns from the 'rawdata' data table?",
        "answer": "The code uses the data.table syntax to extract specific columns from 'rawdata'. The line 'CTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]' selects only the 'master_ccl_id' and 'ccl_name' columns from the 'rawdata' data table and assigns them to a new variable 'CTRP_sampleMetadata'."
      },
      {
        "question": "What is the purpose of the 'usethis::use_data()' function in the last line of the code?",
        "answer": "The 'usethis::use_data()' function is used to save R objects (in this case, 'CTRP_sampleMetadata') to an .rda file in the 'data/' directory of a package. The 'overwrite = TRUE' argument allows the function to overwrite an existing file with the same name. This is typically used when creating or updating datasets that will be included in an R package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]\n\n# Complete the code to save CTRP_sampleMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]\n\nusethis::use_data(CTRP_sampleMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the file and extract specific columns\n\nfilePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\n# Read the file and extract columns",
        "complete": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/common.py",
    "language": "py",
    "content": "import os\nimport pathlib\nfrom typing import Dict\n\nfrom pydicom.misc import is_dicom\n\n\ndef find_dicom_paths(root_path: str, yield_directories: bool = False) -> str:\n    \"\"\"Find DICOM file paths in the specified root directory file tree.\n\n    Parameters\n    ----------\n    root_path\n        Path to the root directory specifying the file hierarchy.\n\n    yield_directories, optional\n        Whether to yield paths to directories containing DICOM files\n        or separately to each file (default).\n\n\n    Yields\n    ------\n    The paths to DICOM files or DICOM-containing directories (if\n    `yield_directories` is True).\n\n    \"\"\"\n    # TODO add some filtering options\n    for root, _, files in os.walk(root_path):\n        if yield_directories:\n            if any((is_dicom(pathlib.Path(root, f).as_posix()) for f in files)):\n                yield root\n        else:\n            for f in files:\n                fpath = pathlib.Path(root, f).as_posix()\n                if is_dicom(fpath):\n                    yield fpath\n\n\ndef file_name_convention() -> Dict:\n    \"\"\"\n    This function returns the file name taxonomy which is used by ImageAutoOutput and Dataset class\n    \"\"\"\n    file_name_convention = {\"CT\": \"image\",\n                            \"MR\": \"image\",\n                            \"RTDOSE_CT\": \"dose\", \n                            \"RTSTRUCT_CT\": \"mask_ct\", \n                            \"RTSTRUCT_MR\": \"mask_mr\", \n                            \"RTSTRUCT_PT\": \"mask_pt\", \n                            \"PT_CT\": \"pet\", \n                            \"PT\": \"pet\", \n                            \"RTDOSE\": \"dose\", \n                            \"RTSTRUCT\": \"mask\"}\n                            \n    return file_name_convention\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `find_dicom_paths` function and how does it handle the `yield_directories` parameter?",
        "answer": "The `find_dicom_paths` function is designed to find DICOM file paths in a specified root directory file tree. It uses the `os.walk` function to traverse the directory structure. The `yield_directories` parameter determines the function's behavior: if set to True, it yields paths to directories containing DICOM files; if False (default), it yields paths to individual DICOM files. The function uses the `is_dicom` function from the pydicom library to check if a file is a DICOM file."
      },
      {
        "question": "How does the `file_name_convention` function contribute to the overall functionality of the code, and what type of data does it return?",
        "answer": "The `file_name_convention` function returns a dictionary that defines a naming convention for different types of medical imaging files. It maps modality-specific keys (e.g., 'CT', 'MR', 'RTDOSE_CT') to standardized file type names (e.g., 'image', 'dose', 'mask_ct'). This function is likely used by other parts of the system (such as ImageAutoOutput and Dataset classes) to maintain consistent file naming across different imaging modalities. The function returns a Dict type, which is imported from the typing module."
      },
      {
        "question": "What potential improvement is suggested in the `find_dicom_paths` function, and how might this enhance its functionality?",
        "answer": "The `find_dicom_paths` function includes a TODO comment suggesting the addition of filtering options. This improvement could enhance the function's flexibility by allowing users to specify criteria for including or excluding certain DICOM files or directories. Possible filtering options might include file size, date modified, or specific DICOM tags. Implementing these filters would make the function more versatile and efficient, especially when dealing with large datasets or when only specific types of DICOM files are needed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def find_dicom_paths(root_path: str, yield_directories: bool = False) -> str:\n    for root, _, files in os.walk(root_path):\n        if yield_directories:\n            if any((is_dicom(pathlib.Path(root, f).as_posix()) for f in files)):\n                yield root\n        else:\n            # Complete the code to yield DICOM file paths",
        "complete": "def find_dicom_paths(root_path: str, yield_directories: bool = False) -> str:\n    for root, _, files in os.walk(root_path):\n        if yield_directories:\n            if any((is_dicom(pathlib.Path(root, f).as_posix()) for f in files)):\n                yield root\n        else:\n            for f in files:\n                fpath = pathlib.Path(root, f).as_posix()\n                if is_dicom(fpath):\n                    yield fpath"
      },
      {
        "partial": "def file_name_convention() -> Dict:\n    file_name_convention = {\n        \"CT\": \"image\",\n        \"MR\": \"image\",\n        # Complete the dictionary with the remaining key-value pairs\n    }\n    return file_name_convention",
        "complete": "def file_name_convention() -> Dict:\n    file_name_convention = {\n        \"CT\": \"image\",\n        \"MR\": \"image\",\n        \"RTDOSE_CT\": \"dose\",\n        \"RTSTRUCT_CT\": \"mask_ct\",\n        \"RTSTRUCT_MR\": \"mask_mr\",\n        \"RTSTRUCT_PT\": \"mask_pt\",\n        \"PT_CT\": \"pet\",\n        \"PT\": \"pet\",\n        \"RTDOSE\": \"dose\",\n        \"RTSTRUCT\": \"mask\"\n    }\n    return file_name_convention"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib"
      ],
      "from_imports": [
        "typing.Dict",
        "pydicom.misc.is_dicom"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_oncotree.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `expect_data_table()` function in these test cases, and what parameters are being used to validate the results?",
        "answer": "The `expect_data_table()` function is used to validate that the result of each AnnotationGx function call is a data table with specific characteristics. It checks the number of columns (ncols), minimum number of rows (min.rows), ensures there are no missing values (all.missing = FALSE), and in some cases, verifies that column names are present (col.names = 'named'). This ensures that the returned data structures meet the expected format and content requirements."
      },
      {
        "question": "How do the test cases for `getOncotreeVersions()`, `getOncotreeMainTypes()`, and `getOncotreeTumorTypes()` differ in terms of their expected output structure?",
        "answer": "The test cases differ in their expected output structure as follows:\n1. `getOncotreeVersions()`: Expects 4 columns and at least 25 rows.\n2. `getOncotreeMainTypes()`: Expects 1 column, at least 100 rows, and named columns.\n3. `getOncotreeTumorTypes()`: Expects 12 columns, at least 800 rows, and named columns.\nThese differences reflect the varying complexity and amount of data returned by each function in the AnnotationGx package."
      },
      {
        "question": "What libraries are being used in this test suite, and what is their purpose in the context of these tests?",
        "answer": "The test suite uses three libraries:\n1. `testthat`: A popular R package for unit testing, providing functions like `test_that()` for organizing tests.\n2. `AnnotationGx`: The package being tested, which contains functions for retrieving oncology-related data.\n3. `checkmate`: Although not explicitly used in the shown code, it's likely used for additional assertion and validation functions within the tests.\nThese libraries work together to create a robust testing environment for the AnnotationGx package's data retrieval functions."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  # Complete the expect_data_table function call\n})",
        "complete": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})"
      },
      {
        "partial": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  # Complete the expect_data_table function call\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})",
        "complete": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_cellosaurus.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\n\n\ntest_that(\"mapCell2Accession works as expected\", {\n  # Test case 1: Test with a valid cell line name\n  cell_line1 <- \"Hela\"\n  expected1 <- \"CVCL_0030\"\n  result1 <- mapCell2Accession(cell_line1)\n  expect_data_table(result1)\n  expect_equal(result1$accession, expected1)\n\n})\n\ntest_that(\"mapCell2Accession prioritizePatient works as expected\", {\n  cell_line <- \"BT474\"\n\n  result1 <- mapCell2Accession(\n    cell_line)\n\n  expect_data_table(result1, nrows = 1, ncols = 3)\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n  expect_equal(result1$accession, \"CVCL_0179\")\n  expect_equal(result1$cellLineName, \"BT-474\")\n\n  expect_error(mapCell2Accession(\"BT474\", numResults = -1, from = \"idsy\"))\n})\n\ntest_that(\"mapCell2Accession fuzzy search works as expected\", {\n  cell_line <- \"BT474\"\n\n  result1 <- mapCell2Accession(\n    cell_line, fuzzy = TRUE)\n\n  expect_data_table(result1, nrows = 1, ncols = 3)\n  expect_equal(result1$accession, \"CVCL_0179\")\n  expect_equal(result1$cellLineName, \"BT-474\")\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n\n  expect_error(mapCell2Accession(\"BT474\", numResults = -1, from = \"idsy\"))\n})\n\ntest_that(\"mapCell2Accession with multiple cell lines works as expected\", {\n  cell_lines <- c(\"BT474\", \"Hela\")\n\n  result1 <- mapCell2Accession(\n    cell_lines)\n\n  expect_data_table(result1, nrows = 2, ncols = 3)\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n  expect_equal(result1$accession, c(\"CVCL_0179\", \"CVCL_0030\"))\n  expect_equal(result1$cellLineName, c(\"BT-474\", \"HeLa\"))\n\n  expect_error(mapCell2Accession(\"BT474\", numResults = -1, from = \"idsy\"))\n})\n\ntest_that(\"mapCell DOR 13 works\", {\n  name <- \"DOR 13\"\n\n  result1 <- mapCell2Accession(name)\n  result2 <- mapCell2Accession(name, fuzzy = T) \n  result3 <- mapCell2Accession(c(name, \"HT\"))\n  \n  expect_data_table(result1, nrows = 1, ncols = 1) # fails\n  expect_data_table(result2, nrows = 1, ncols = 3) # works\n  expect_data_table(result3, nrows = 2, ncols = 3) # works\n\n\n  expect_equal(result2$accession, \"CVCL_6774\")\n  expect_equal(result2$cellLineName, \"DOV13\")\n\n  expect_equal(result3$accession, c(NA_character_, \"CVCL_1290\"))\n  expect_equal(result3$cellLineName, c(NA_character_, \"HT\"))\n  expect_equal(result3$query, c(\"DOR 13\", \"HT\"))\n})\n\n\ntest_that(\"query only paramater works\",{\n  result1 <- mapCell2Accession(\"DOR 13\", query_only = TRUE)\n  \n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=idsy%3ADOR%2013&sort=ac%20asc&fields=ac%2Cid%2Csy%2Cmisspelling%2Cdr%2Ccc%2Cca%2Cdi%2Cag%2Csx%2Chi&format=txt&rows=10000\"\n  expect_equal(result1[[1]], expected)\n  expect_equal(names(result1), \"DOR 13\")\n})\n\ntest_that(\"raw param works\",{\n  \n  result1 <- mapCell2Accession(\"HT\", raw = TRUE)\n  expect_class(result1[[1]], \"httr2_response\")\n  expect_equal(names(result1), \"HT\")\n\n  resp <- result1$HT\n  lines <- httr2::resp_body_string(resp)  |>\n            strsplit(\"\\n\") |> \n            unlist()\n\n  checkmate::expect_character(lines)\n  expect_true(length(lines) > 2000 & 10000 > length(lines) )\n\n\n  parsed_lines <- \n    Map(\n      f = function(lines, i, j) {\n          lines[i:(j - 1L)]\n      },\n      i = grep(pattern = \"^ID\\\\s+\", x = lines, value = FALSE),\n      j = grep(pattern = \"^//$\", x = lines, value = FALSE),\n      MoreArgs = list(\"lines\" = lines),\n      USE.NAMES = FALSE\n    )\n  x <- parsed_lines[[1]]\n  result <- AnnotationGx:::.processEntry(x)\n\n  expect_data_table(result, min.rows = 1, min.cols = 9)\n  expect_true(\n    all(\n      c(\"cellLineName\", \"accession\", \"comments\", \"synonyms\") %in% colnames(result)\n      )\n    )\n\n})\n\n\ntest_that(\"parsed works\", {\n  ( result1 <- mapCell2Accession(\"22RV1\", parsed = FALSE))$diseases\n   expect_data_table(result1, min.rows = 1, min.cols = 3)\n  expect_true(\n    all(\n      c(\"cellLineName\", \"accession\", \"query\") %in% colnames(result1)\n      )\n    )\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCell2Accession` function based on the test cases provided?",
        "answer": "The `mapCell2Accession` function appears to map cell line names to their corresponding accession numbers in the Cellosaurus database. It can handle single or multiple cell line inputs, perform fuzzy searches, and return various levels of detail about the cell lines."
      },
      {
        "question": "How does the `fuzzy` parameter affect the behavior of `mapCell2Accession`?",
        "answer": "When the `fuzzy` parameter is set to TRUE, the function performs a fuzzy search, which allows for approximate matching of cell line names. This is demonstrated in the test case where 'DOR 13' is successfully matched to 'DOV13' when fuzzy search is enabled, but fails to return a result when fuzzy search is not used."
      },
      {
        "question": "What are the different output formats available for the `mapCell2Accession` function, and how are they controlled?",
        "answer": "The `mapCell2Accession` function offers several output formats controlled by different parameters: 1) Default output is a data table with cellLineName, accession, and query columns. 2) Setting `query_only = TRUE` returns the API query URL. 3) Setting `raw = TRUE` returns the raw HTTP response. 4) Setting `parsed = FALSE` returns a simplified data table with fewer columns. These options allow for flexibility in how the data is returned and processed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"mapCell2Accession works as expected\", {\n  cell_line1 <- \"Hela\"\n  expected1 <- \"CVCL_0030\"\n  result1 <- mapCell2Accession(cell_line1)\n  # Add expectations here\n})",
        "complete": "test_that(\"mapCell2Accession works as expected\", {\n  cell_line1 <- \"Hela\"\n  expected1 <- \"CVCL_0030\"\n  result1 <- mapCell2Accession(cell_line1)\n  expect_data_table(result1)\n  expect_equal(result1$accession, expected1)\n})"
      },
      {
        "partial": "test_that(\"mapCell2Accession with multiple cell lines works as expected\", {\n  cell_lines <- c(\"BT474\", \"Hela\")\n  result1 <- mapCell2Accession(cell_lines)\n  # Add expectations here\n})",
        "complete": "test_that(\"mapCell2Accession with multiple cell lines works as expected\", {\n  cell_lines <- c(\"BT474\", \"Hela\")\n  result1 <- mapCell2Accession(cell_lines)\n  expect_data_table(result1, nrows = 2, ncols = 3)\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n  expect_equal(result1$accession, c(\"CVCL_0179\", \"CVCL_0030\"))\n  expect_equal(result1$cellLineName, c(\"BT-474\", \"HeLa\"))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/sparsemask.py",
    "language": "py",
    "content": "from typing import Dict\nimport numpy as np\n\n\nclass SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        self.roi_name_dict = roi_name_dict\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the SparseMask class in this code snippet?",
        "answer": "The SparseMask class is designed to represent a sparse mask, likely for image processing or region of interest (ROI) operations. It stores a mask array and a dictionary mapping ROI names to integer values, allowing for efficient storage and manipulation of sparse mask data."
      },
      {
        "question": "What are the types of the parameters in the __init__ method of the SparseMask class?",
        "answer": "The __init__ method takes two parameters: mask_array of type np.ndarray (NumPy array), and roi_name_dict of type Dict[str, int] (a dictionary with string keys and integer values)."
      },
      {
        "question": "Why might the typing module be imported in this code snippet?",
        "answer": "The typing module is imported to use type hints, specifically for the Dict type used in the roi_name_dict parameter. This improves code readability and allows for better static type checking, which can help catch potential errors early in development."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        # Complete the initialization",
        "complete": "class SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        self.roi_name_dict = roi_name_dict"
      },
      {
        "partial": "from typing import Dict\nimport numpy as np\n\nclass SparseMask:\n    # Complete the class definition",
        "complete": "from typing import Dict\nimport numpy as np\n\nclass SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        self.roi_name_dict = roi_name_dict"
      }
    ],
    "dependencies": {
      "imports": [
        "numpy"
      ],
      "from_imports": [
        "typing.Dict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeGR.R",
    "language": "R",
    "content": "#' @keywords internal\n#' @noRd\ngrRegression <- function(conc,\n                  viability,\n                  duration,\n                  Hill_fit,\n                  dbl_time,\n                  conc_as_log = FALSE,\n                  viability_as_pct = TRUE,\n                  verbose = FALSE,\n                  density = c(2, 10, 2),\n                  step = .5 / density,\n                  precision = 0.05,\n                  lower_bounds = c(0, 0, -6),\n                  upper_bounds = c(4, 1, 6),\n                  scale = 0.07,\n                  family = c(\"normal\", \"Cauchy\"),\n                  scale_01 = FALSE,\n                  trunc=TRUE) { #If true, fits parameters to a transformed GR curve\n  #with image [0, 1]. If false, fits parameters to Sorger's original [-1, 1]-image curve.\n\n  #GRFit takes in dose-response data and a bunch of formatting parameters, then returns\n  #the GHS, GEC_50, and G_inf values associated with them in accordance with the Sorger\n  #paper. However, the definitions are corrected in accordance with my adjustment of the\n  #relevant Sorger equations. While this may change the form of the equations, it does\n  #not affect their intuitive meanings. G_inf is still the GR in the presence of arbitrarily\n  #large drug concentration, GEC_50 is the dose that produces a half-minimal GR-value,\n  #and GHS is the magnitude of the slope of the tangent to the log dose-response curve\n  #when the drug concentration is GEC_50.\n\n  #DO SANITY CHECKS ON INPUT\n  # if(missing(concentration)){\n\n  #   stop(\"The concentration values the drug was tested on must always be provided.\")\n  family <- match.arg(family)\n\n\nif (missing(Hill_fit)) {\n\n    Hill_fit <- logLogisticRegression(conc,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose,\n      density = density,\n      step = step,\n      precision = precision,\n      lower_bounds = lower_bounds,\n      upper_bounds = upper_bounds,\n      scale = scale,\n      family = family\n      )\n    cleanData <- sanitizeInput(conc=conc,\n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    Hill_fit <- cleanData[[\"Hill_fit\"]]\n    log_conc <- cleanData[[\"log_conc\"]]\n} else if (!missing(Hill_fit)){\n\n  cleanData <- sanitizeInput(conc = conc,\n    viability = viability,\n    Hill_fit = Hill_fit,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  Hill_fit <- cleanData[[\"Hill_fit\"]]\n  # log_conc <- cleanData[[\"log_conc\"]]\n}\n\n  if (missing(viability) && missing(Hill_fit)) {\n    stop(\"Please enter viability data and/or Hill equation parameters.\")\n  }\n\n  if(missing(duration)){\n    stop(\"Cannot calculate GR without duration of experiment\")\n  }\n  if(missing(dbl_time)){\n    stop(\"Cannot calculate GR without cell doubling time\")\n  }\n\n  tau <- duration / dbl_time\n\n  #CALCULATE GR STATISTICS\n\n  Ginf <- (Hill_fit[2]) ^ (1 / tau)\n  GEC50 <- 10 ^ Hill_fit[3] * ((2 ^ tau - (1 + Ginf) ^ tau) / ((1 + Ginf) ^ tau - (2 * Ginf) ^ tau)) ^ (1 / Hill_fit[1])\n  GHS <- (1 - Hill_fit[2]) / tau *\n    (.Hill(log10(GEC50), Hill_fit)) ^ (1 / tau - 1) *\n    1 / (1 + (GEC50 / 10 ^ Hill_fit[3]) ^ Hill_fit[1]) ^ 2 * Hill_fit[1] / 10 ^ Hill_fit[3] *\n    (GEC50 / 10 ^ Hill_fit[3]) ^ (Hill_fit[1] - 1) * GEC50 * log(10)\n\n  #CONVERT OUTPUT TO CONFORM TO FORMATTING PARAMETERS\n\n  if (scale_01 == FALSE) {\n    Ginf <- 2 * Ginf - 1\n    GHS <- 2 * GHS\n  }\n\n  if (viability_as_pct == TRUE) {\n    Ginf <- 100 * Ginf\n    GHS <- 100 * GHS\n  }\n\n  if (conc_as_log == TRUE) {\n    GEC50 <- log10(GEC50)\n  }\n\n  return(list(GHS = as.numeric(GHS), Ginf = as.numeric(Ginf), GEC50 = as.numeric(GEC50)))\n\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `grRegression` function and what are its main input parameters?",
        "answer": "The `grRegression` function is used to calculate GR (Growth Rate) statistics from dose-response data. Its main input parameters include:\n- `conc`: drug concentration values\n- `viability`: cell viability data\n- `duration`: experiment duration\n- `Hill_fit`: parameters for the Hill equation (optional)\n- `dbl_time`: cell doubling time\nThe function also accepts various optional parameters for data formatting and fitting options."
      },
      {
        "question": "How does the function handle missing input data, and what error messages are displayed?",
        "answer": "The function performs several checks for missing input data:\n1. If both `viability` and `Hill_fit` are missing, it stops with the message: \"Please enter viability data and/or Hill equation parameters.\"\n2. If `duration` is missing, it stops with: \"Cannot calculate GR without duration of experiment\"\n3. If `dbl_time` is missing, it stops with: \"Cannot calculate GR without cell doubling time\"\nThese checks ensure that all necessary data for GR calculation is provided."
      },
      {
        "question": "What are the main output values of the `grRegression` function, and how are they calculated?",
        "answer": "The main output values of the `grRegression` function are:\n1. GHS (Growth rate inhibition Hill Slope)\n2. Ginf (G infinity, the GR value at infinite drug concentration)\n3. GEC50 (the drug concentration producing half-maximal GR)\n\nThese values are calculated using the Hill equation parameters and the input data. The function applies various transformations based on the input parameters (e.g., `scale_01`, `viability_as_pct`, `conc_as_log`) to adjust the output format. The calculations involve complex mathematical formulas derived from the Sorger paper on drug response metrics, with some adjustments made by the function author."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/vignettes/DetectingDrugSynergyAndAntagonism.R",
    "language": "R",
    "content": "## ----eval=TRUE, include=FALSE-------------------------------------------------\n# convenience variables\ncgx <- BiocStyle::Biocpkg(\"CoreGx\")\npgx <- BiocStyle::Biocpkg(\"PharmacoGx\")\ndt <- BiocStyle::CRANpkg(\"data.table\")\n\n# knitr options\nknitr::opts_chunk$set(warning=FALSE)\n\n## ----load_dependencies_eval, eval=TRUE, echo=FALSE----------------------------\nsuppressPackageStartupMessages({\n    library(PharmacoGx)\n    library(CoreGx)\n    library(data.table)\n    library(ggplot2)\n})\n\n## ----load_dependencies_echo, eval=FALSE, echo=TRUE----------------------------\n#  library(PharmacoGx)\n#  library(CoreGx)\n#  library(data.table)\n#  library(ggplot2)\n\n## -----------------------------------------------------------------------------\ninput_file <- system.file(\"extdata/mathews_griner.csv.tar.gz\",\n    package=\"PharmacoGx\")\nmathews_griner <- fread(input_file)\n\n## ----experiment_design_hypothesis---------------------------------------------\ngroups <- list(\n    rowDataMap=c(\n        treatment1id=\"RowName\", treatment2id=\"ColName\",\n        treatment1dose=\"RowConcs\", treatment2dose=\"ColConcs\"\n    ),\n    colDataMap=c(\"sampleid\")\n)\ngroups[[\"assayMap\"]] <- c(groups$rowDataMap, groups$colDataMap)\n(groups)\n\n## ----handling_technical_replicates--------------------------------------------\n# The := operator modifies a data.table by reference (i.e., without making a copy)\nmathews_griner[, tech_rep := seq_len(.N), by=c(groups[[\"assayMap\"]])]\nif (max(mathews_griner[[\"tech_rep\"]]) > 1) {\n    groups[[\"colDataMap\"]] <- c(groups[[\"colDataMap\"]], \"tech_rep\")\n    groups[[\"assayMap\"]] <- c(groups[[\"assayMap\"]], \"tech_rep\")\n} else {\n    # delete the additional column if not needed\n    message(\"No technical replicates in this dataset!\")\n    mathews_griner[[\"tech_reps\"]] <- NULL\n}\n\n## ----build_tredatamapper------------------------------------------------------\n(treMapper <- TREDataMapper(rawdata=mathews_griner))\n\n## ----evaluate_tre_mapping_guess-----------------------------------------------\n(guess <- guessMapping(treMapper, groups, subset=TRUE))\n\n## ----update_tredatamapper_with_guess------------------------------------------\nmetadataMap(treMapper) <- list(experiment_metadata=guess$metadata$mapped_columns)\nrowDataMap(treMapper) <- guess$rowDataMap\ncolDataMap(treMapper) <- guess$colDataMap\nassayMap(treMapper) <- list(raw=guess$assayMap)\ntreMapper\n\n## ----metaconstruct_the_tre----------------------------------------------------\n(tre <- metaConstruct(treMapper))\n\n## ----normalize_to_dose_0_0_control--------------------------------------------\nraw <- tre[[\"raw\"]]\nraw[,\n    viability := viability / .SD[treatment1dose == 0 & treatment2dose == 0, viability],\n    by=c(\"treatment1id\", \"treatment2id\", \"sampleid\", \"tech_rep\")\n]\nraw[, viability := pmax(0, viability)]  # truncate min viability at 0\ntre[[\"raw\"]] <- raw\n\n## ----sanity_check_viability---------------------------------------------------\ntre[[\"raw\"]][, range(viability)]\n\n## ----find_bad_viability_treatment, warning=FALSE------------------------------\n(bad_treatments <- tre[[\"raw\"]][viability > 2, unique(treatment1id)])\n\n## ----remove_bad_viability_treatment, warning=FALSE----------------------------\n(tre <- subset(tre, !(treatment1id %in% bad_treatments)))\n\n## ----sanity_check_viability2--------------------------------------------------\ntre[[\"raw\"]][, range(viability)]\n\n## ----creating_monotherapy_assay-----------------------------------------------\ntre_qc <- tre |>\n    endoaggregate(\n        subset=treatment2dose == 0,  # filter to only monotherapy rows\n        assay=\"raw\",\n        target=\"mono_viability\",  # create a new assay named mono_viability\n        mean_viability=pmin(1, mean(viability)),\n        by=c(\"treatment1id\", \"treatment1dose\", \"sampleid\")\n    )\n\n## ----monotherapy_curve_fits, messages=FALSE-----------------------------------\ntre_fit <- tre_qc |>\n    endoaggregate(\n        {  # the entire code block is evaluated for each group in our group by\n            # 1. fit a log logistic curve over the dose range\n            fit <- PharmacoGx::logLogisticRegression(treatment1dose, mean_viability,\n                viability_as_pct=FALSE)\n            # 2. compute curve summary metrics\n            ic50 <- computeIC50(treatment1dose, Hill_fit=fit)\n            aac <- computeAUC(treatment1dose, Hill_fit=fit)\n            # 3. assemble the results into a list, each item will become a\n            #   column in the target assay.\n            list(\n                HS=fit[[\"HS\"]],\n                E_inf = fit[[\"E_inf\"]],\n                EC50 = fit[[\"EC50\"]],\n                Rsq=as.numeric(unlist(attributes(fit))),\n                aac_recomputed=aac,\n                ic50_recomputed=ic50\n            )\n        },\n        assay=\"mono_viability\",\n        target=\"mono_profiles\",\n        enlist=FALSE,  # this option enables the use of a code block for aggregation\n        by=c(\"treatment1id\", \"sampleid\"),\n        nthread=2  # parallelize over multiple cores to speed up the computation\n    )\n\n## ----create_combo_viability, message=FALSE------------------------------------\ntre_combo <- tre_fit |>\n    endoaggregate(\n        assay=\"raw\",\n        target=\"combo_viability\",\n        mean(viability),\n        by=c(\"treatment1id\", \"treatment2id\", \"treatment1dose\", \"treatment2dose\",\n            \"sampleid\")\n    )\n\n## ----add_monotherapy_fits_to_combo_viability----------------------------------\ntre_combo <- tre_combo |>\n    mergeAssays(\n        x=\"combo_viability\",\n        y=\"mono_profiles\",\n        by=c(\"treatment1id\", \"sampleid\")\n    ) |>\n    mergeAssays(\n        x=\"combo_viability\",\n        y=\"mono_profiles\",\n        by.x=c(\"treatment2id\", \"sampleid\"),\n        by.y=c(\"treatment1id\", \"sampleid\"),\n        suffixes=c(\"_1\", \"_2\")  # add sufixes to duplicate column names\n    )\n\n## -----------------------------------------------------------------------------\ntre_combo <- tre_combo |>\n    endoaggregate(\n        viability_1=.SD[treatment2dose == 0, mean_viability],\n        assay=\"combo_viability\",\n        by=c(\"treatment1id\", \"treatment1dose\", \"sampleid\")\n    ) |>\n    endoaggregate(\n        viability_2=.SD[treatment1dose == 0, mean_viability],\n        assay=\"combo_viability\",\n        by=c(\"treatment1id\", \"treatment2dose\", \"sampleid\")\n    )\n\n## ----compute_synergy_null_hypotheses, message=FALSE---------------------------\ntre_synergy <- tre_combo |>\n    endoaggregate(\n        assay=\"combo_viability\",\n        HSA_ref=computeHSA(viability_1, viability_2),\n        Bliss_ref=computeBliss(viability_1, viability_2),\n        Loewe_ref=computeLoewe(\n            treatment1dose, HS_1=HS_1, EC50_1=EC50_1, E_inf_1=E_inf_1,\n            treatment2dose, HS_2=HS_2, EC50_2=EC50_2, E_inf_2=E_inf_2\n        ),\n        ZIP_ref=computeZIP(\n            treatment1dose, HS_1=HS_1, EC50_1=EC50_1, E_inf_1=E_inf_1,\n            treatment2dose, HS_2=HS_2, EC50_2=EC50_2, E_inf_2=E_inf_2\n        ),\n        by=assayKeys(tre_combo, \"combo_viability\"),\n        nthread=2\n    )\n\n## ----synergy_score_vs_reference-----------------------------------------------\ntre_synergy <- tre_synergy |>\n    endoaggregate(\n        assay=\"combo_viability\",\n        HSA_score=HSA_ref - mean_viability,\n        Bliss_score=Bliss_ref - mean_viability,\n        Loewe_score=Loewe_ref - mean_viability,\n        ZIP_score=ZIP_ref - mean_viability,\n        by=assayKeys(tre_synergy, \"combo_viability\")\n    )\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `guessMapping` function in this code, and how is its output used?",
        "answer": "The `guessMapping` function is used to automatically infer the mapping between the raw data columns and the TRE (Treatment Response Experiment) data structure. Its output, stored in the `guess` variable, is used to update the `TREDataMapper` object (`treMapper`) with the inferred mappings for metadata, row data, column data, and assay data. This mapping is crucial for correctly structuring the raw data into a standardized TRE format."
      },
      {
        "question": "How does the code handle technical replicates in the dataset, and what happens if there are no technical replicates?",
        "answer": "The code checks for technical replicates by adding a 'tech_rep' column to the `mathews_griner` data table, assigning a sequence number to each unique combination of assay map variables. If the maximum value of 'tech_rep' is greater than 1, indicating the presence of technical replicates, 'tech_rep' is added to the colDataMap and assayMap. If there are no technical replicates, the code removes the 'tech_rep' column and displays a message saying 'No technical replicates in this dataset!'."
      },
      {
        "question": "What synergy scores are calculated in this code, and how are they computed?",
        "answer": "The code calculates four synergy scores: HSA (Highest Single Agent), Bliss, Loewe, and ZIP (Zero Interaction Potency). These scores are computed by comparing the observed combination effect (mean_viability) to reference values calculated using different synergy models. The synergy scores are calculated as the difference between the reference value and the observed mean viability. For example, HSA_score = HSA_ref - mean_viability. The reference values are computed using specific functions like computeHSA, computeBliss, computeLoewe, and computeZIP, which implement the respective synergy models."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/drugDoseResponseCurve.R",
    "language": "R",
    "content": "#' Plot drug response curve of a given drug and a given cell for a list of pSets (objects of the PharmacoSet class).\n#'\n#' Given a list of PharmacoSets, the function will plot the drug_response curve,\n#' for a given drug/cell pair. The y axis of the plot is the viability percentage\n#' and x axis is the log transformed concentrations. If more than one pSet is\n#' provided, a light gray area would show the common concentration range between pSets.\n#' User can ask for type of sensitivity measurment to be shown in the plot legend.\n#' The user can also provide a list of their own concentrations and viability values,\n#' as in the examples below, and it will be treated as experiments equivalent to values coming\n#' from a pset. The names of the concentration list determine the legend labels.\n#'\n#' @examples\n##TODO:: How do you pass PSets to this?\n#' if (interactive()) {\n#' # Manually enter the plot parameters\n#' drugDoseResponseCurve(concentrations=list(\"Experiment 1\"=c(.008, .04, .2, 1)),\n#'  viabilities=list(c(100,50,30,1)), plot.type=\"Both\")\n#'\n#' # Generate a plot from one or more PSets\n#' data(GDSCsmall)\n#' drugDoseResponseCurve(drug=\"Doxorubicin\", cellline=\"22RV1\", pSets=GDSCsmall)\n#' }\n#'\n#' @param drug `character(1)` A drug name for which the drug response curve should be\n#' plotted. If the plot is desirable for more than one pharmaco set, A unique drug id\n#' should be provided.\n#' @param cellline `character(1)` A cell line name for which the drug response curve should be\n#' plotted. If the plot is desirable for more than one pharmaco set, A unique cell id\n#' should be provided.\n#' @param pSets `list` a list of PharmacoSet objects, for which the function\n#' should plot the curves.\n#' @param concentrations,viabilities `list` A list of concentrations and viabilities to plot, the function assumes that\n#' `concentrations[[i]]` is plotted against `viabilities[[i]]`. The names of the concentration list are used to create the legend labels\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(ICn) should be returned instead of ICn. Applies only to the concentrations parameter.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf passed in as decimal. Applies only to the viabilities parameter.\n#' @param legends.label `numeric` A vector of sensitivity measurment types which could\n#' be any combination of  ic50_published, auc_published, auc_recomputed and auc_recomputed_star.\n#' A legend will be displayed on the top right of the plot which each line of the legend is\n#' the values of requested sensitivity measerments for one of the requested pSets.\n#' If this parameter is missed no legend would be provided for the plot.\n#' @param ylim `numeric` A vector of two numerical values to be used as ylim of the plot.\n#' If this parameter would be missed c(0,100) would be used as the ylim of the plot.\n#' @param xlim `numeric` A vector of two numerical values to be used as xlim of the plot.\n#' If this parameter would be missed the minimum and maximum comncentrations between all\n#' the pSets would be used as plot xlim.\n#' @param mycol `numeric` A vector with the same lenght of the pSets parameter which\n#' will determine the color of the curve for the pharmaco sets. If this parameter is\n#' missed default colors from Rcolorbrewer package will be used as curves color.\n#' @param plot.type `character` Plot type which can be the actual one (\"Actual\") or\n#' the one fitted by logl logistic regression (\"Fitted\") or both of them (\"Both\").\n#' If this parameter is missed by default actual curve is plotted.\n#' @param summarize.replicates `character` If this parameter is set to true replicates\n#' are summarized and replicates are plotted individually otherwise\n#' @param title `character` The title of the graph. If no title is provided, then it defaults to\n#' 'Drug':'Cell Line'.\n#' @param lwd `numeric` The line width to plot with\n#' @param cex `numeric` The cex parameter passed to plot\n#' @param cex.main `numeric` The cex.main parameter passed to plot, controls the size of the titles\n#' @param legend.loc And argument passable to xy.coords for the position to place the legend.\n#' @param trunc `logical(1)` Should the viability values be truncated to lie in \\[0-100\\] before doing the fitting\n#' @param verbose `logical(1)` Should warning messages about the data passed in be printed?\n#' @param sample_col `character(1)` The name of the column in the profiles assay that contains the sample IDs.\n#' @param treatment_col `character(1)` The name of the column in the profiles assay that contains the treatment IDs.\n#'\n#' @return Plots to the active graphics device and returns an invisible NULL.\n#'\n#' @import RColorBrewer\n#'\n#' @importFrom graphics plot rect points lines legend\n#' @importFrom grDevices rgb\n# # ' @importFrom magicaxis magaxis\n#' @importFrom CoreGx .getSupportVec\n#'\n#' @export\ndrugDoseResponseCurve <-\nfunction(drug,\n         cellline,\n         pSets=list(),\n         concentrations=list(),\n         viabilities=list(),\n         conc_as_log = FALSE,\n         viability_as_pct = TRUE,\n         trunc=TRUE,\n         legends.label = c(\"ic50_published\", \"gi50_published\",\"auc_published\",\"auc_recomputed\",\"ic50_recomputed\"),\n         ylim=c(0,100),\n         xlim, mycol,\n         title,\n         plot.type=c(\"Fitted\",\"Actual\", \"Both\"),\n         summarize.replicates=TRUE,\n         lwd = 0.5,\n         cex = 0.7,\n         cex.main = 0.9,\n         legend.loc = \"topright\",\n         verbose=TRUE,\n         sample_col = \"sampleid\",\n         treatment_col = \"treatmentid\") {\n  if(!missing(pSets)){\n    if (!is(pSets, \"list\")) {\n      if (is(pSets, \"PharmacoSet\")) {\n        temp <- name(pSets)\n        pSets <- list(pSets)\n        names(pSets) <- temp\n      } else {\n        stop(\"Type of pSets parameter should be either a pSet or a list of pSets.\")\n      }\n    }\n  }\n  if(!missing(pSets) && (missing(drug) || missing(cellline))){\n    stop(\"If you pass in a pSet then drug and cellline must be set\") }\n  # } else {\n  #   if(missing(drug)){\n  #   drug <- \"Drug\"}\n  #   if(missing(cellline))\n  #   cellline <- \"Cell Line\"\n  # }\n  if(!missing(concentrations)){\n    if(missing(viabilities)){\n\n      stop(\"Please pass in the viabilities to Plot with the concentrations.\")\n\n    }\n    if (!is(concentrations, \"list\")) {\n      if (mode(concentrations) == \"numeric\") {\n        if(mode(viabilities)!=\"numeric\"){\n          stop(\"Passed in 1 vector of concentrations but the viabilities are not numeric!\")\n        }\n        cleanData <- sanitizeInput(concentrations,\n          viabilities,\n          conc_as_log = conc_as_log,\n          viability_as_pct = viability_as_pct,\n          trunc = trunc,\n          verbose = verbose)\n        concentrations <- 10^cleanData[[\"log_conc\"]]\n        concentrations <- list(concentrations)\n        viabilities <- 100*cleanData[[\"viability\"]]\n        viabilities <- list(viabilities)\n        names(concentrations) <- \"Exp1\"\n        names(viabilities) <- \"Exp1\"\n      } else {\n        stop(\"Mode of concentrations parameter should be either numeric or a list of numeric vectors\")\n      }\n    } else{\n      if(length(viabilities)!= length(concentrations)){\n        stop(\"The number of concentration and viability vectors passed in differs\")\n      }\n      if(is.null(names(concentrations))){\n        names(concentrations) <- paste(\"Exp\", seq_len(length(concentrations)))\n      }\n      for(i in seq_len(length(concentrations))){\n\n        if (mode(concentrations[[i]]) == \"numeric\") {\n          if(mode(viabilities[[i]])!=\"numeric\"){\n            stop(sprintf(\"concentrations[[%d]] are numeric but the viabilities[[%d]] are not numeric!\",i,i))\n          }\n          cleanData <- sanitizeInput(concentrations[[i]],\n            viabilities[[i]],\n            conc_as_log = conc_as_log,\n            viability_as_pct = viability_as_pct,\n            trunc = trunc,\n            verbose = verbose)\n          concentrations[[i]] <- 10^cleanData[[\"log_conc\"]]\n          viabilities[[i]] <- 100*cleanData[[\"viability\"]]\n        } else {\n          stop(sprintf(\"Mode of concentrations[[%d]] parameter should be numeric\",i))\n        }\n\n      }\n\n    }\n  }\n\n  if (missing(plot.type)) {\n    plot.type <- \"Actual\"\n  }\n\n  if(is(treatmentResponse(pSets[[1]]), \"LongTable\")){\n    pSets[[1]] <- subsetByTreatment(pSets[[1]], treatments=drug)\n  }\n  pSets[[1]] <- subsetBySample(pSets[[1]], samples=cellline)\n\n  doses <- list(); responses <- list(); legend.values <- list(); j <- 0; pSetNames <- list()\n  if(!missing(pSets)){\n    for(i in seq_len(length(pSets))) {\n      exp_i <- which(sensitivityInfo(pSets[[i]])[ ,sample_col] == cellline & sensitivityInfo(pSets[[i]])[ ,treatment_col] == drug)\n      if(length(exp_i) > 0) {\n        if (summarize.replicates) {\n          pSetNames[[i]] <- name(pSets[[i]])\n          drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"])),\n              \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"]))), stringsAsFactors=FALSE)\n          drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # tryCatch(\n          #   drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"])),\n          #     \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"]))), stringsAsFactors=FALSE)\n          #   drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # , error = function(e) {\n          #   if (length(exp_i) == 1) {\n          #   drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"])),\n          #     \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"]))), stringsAsFactors=FALSE)\n          #   drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # }else{\n          #   drug.responses <- as.data.frame(cbind(\"Dose\"=apply(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"], 1, function(x){median(as.numeric(x), na.rm=TRUE)}),\n          #     \"Viability\"=apply(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"], 2, function(x){median(as.numeric(x), na.rm=TRUE)})), stringsAsFactors=FALSE)\n          #   drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # }\n          # })\n\n          \n          doses[[i]] <- drug.responses$Dose\n          responses[[i]] <- drug.responses$Viability\n          names(doses[[i]]) <- names(responses[[i]]) <- seq_len(length(doses[[i]]))\n          if (!missing(legends.label)) {\n            if (length(legends.label) > 1) {\n              legend.values[[i]] <- paste(unlist(lapply(legends.label, function(x){\n                sprintf(\"%s = %s\", x, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp_i,x]), digits=2))\n              })), collapse = \", \")\n            } else {\n              legend.values[[i]] <- sprintf(\"%s = %s\", legends.label, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp_i, legends.label]), digits=2))\n            }\n          } else {\n            legend.values[i] <- \"\"\n          }\n        }else {\n          for (exp in exp_i) {\n            j <- j + 1\n            pSetNames[[j]] <- name(pSets[[i]])\n\n            drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp, , \"Dose\"])),\n              \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp, , \"Viability\"]))), stringsAsFactors=FALSE)\n            drug.responses <- drug.responses[complete.cases(drug.responses), ]\n            doses[[j]] <- drug.responses$Dose\n            responses[[j]] <- drug.responses$Viability\n            names(doses[[j]]) <- names(responses[[j]]) <- seq_len(length(doses[[j]]))\n            if (!missing(legends.label)) {\n              if (length(legends.label) > 1) {\n                legend.values[[j]] <- paste(unlist(lapply(legends.label, function(x){\n                  sprintf(\"%s = %s\", x, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp, x]), digits=2))\n                })), collapse = \", \")\n              } else {\n                legend.values[[j]] <- sprintf(\" Exp %s %s = %s\", rownames(sensitivityInfo(pSets[[i]]))[exp], legends.label, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp, legends.label]), digits=2))\n              }\n            } else {\n              tt <- unlist(strsplit(rownames(sensitivityInfo(pSets[[i]]))[exp], split=\"_\"))\n              if (tt[1] == treatment_col) {\n                legend.values[[j]] <- tt[2]\n              }else{\n                legend.values[[j]] <- rownames(sensitivityInfo(pSets[[i]]))[exp]\n              }\n            }\n          }\n        }\n      } else {\n        warning(\"The cell line and drug combo were not tested together. Aborting function.\")\n        return()\n      }\n    }\n  }\n\n  if(!missing(concentrations)){\n    doses2 <- list(); responses2 <- list(); legend.values2 <- list(); j <- 0; pSetNames2 <- list();\n    for (i in seq_len(length(concentrations))){\n      doses2[[i]] <- concentrations[[i]]\n      responses2[[i]] <- viabilities[[i]]\n      if(length(legends.label)>0){\n        if(any(grepl(\"AUC\", x=toupper(legends.label)))){\n          legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"AUC\", round(computeAUC(concentrations[[i]],viabilities[[i]], conc_as_log=FALSE, viability_as_pct=TRUE)/100, digits=2)), sep=\", \")\n        }\n        if(any(grepl(\"IC50\", x=toupper(legends.label)))){\n          legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"IC50\", round(computeIC50(concentrations[[i]],viabilities[[i]], conc_as_log=FALSE, viability_as_pct=TRUE), digits=2)), sep=\", \")\n        }\n\n      } else{ legend.values2[[i]] <- \"\"}\n\n      pSetNames2[[i]] <- names(concentrations)[[i]]\n    }\n    doses <- c(doses, doses2)\n    responses <- c(responses, responses2)\n    legend.values <- c(legend.values, legend.values2)\n    pSetNames <- c(pSetNames, pSetNames2)\n  }\n\n  if (missing(mycol)) {\n    # require(RColorBrewer) || stop(\"Library RColorBrewer is not available!\")\n    mycol <- RColorBrewer::brewer.pal(n=7, name=\"Set1\")\n  }\n\n  dose.range <- c(10^100 , 0)\n  viability.range <- c(0 , 10)\n  for(i in seq_len(length(doses))) {\n    dose.range <- c(min(dose.range[1], min(doses[[i]], na.rm=TRUE), na.rm=TRUE), max(dose.range[2], max(doses[[i]], na.rm=TRUE), na.rm=TRUE))\n    viability.range <- c(0, max(viability.range[2], max(responses[[i]], na.rm=TRUE), na.rm=TRUE))\n  }\n  x1 <- 10 ^ 10; x2 <- 0\n\n  if(length(doses) > 1) {\n    common.ranges <- .getCommonConcentrationRange(doses)\n\n    for(i in seq_len(length(doses))) {\n      x1 <- min(x1, min(common.ranges[[i]]))\n      x2 <- max(x2, max(common.ranges[[i]]))\n    }\n  }\n  if (!missing(xlim)) {\n    dose.range <- xlim\n  }\n  if (!missing(ylim)) {\n    viability.range <- ylim\n  }\n  if(missing(title)){\n    if(!missing(drug)&&!missing(cellline)){\n      title <- sprintf(\"%s:%s\", drug, cellline)\n    } else {\n      title <- \"Drug Dose Response Curve\"\n    }\n\n  }\n  plot(NA, xlab=\"Concentration (uM)\", ylab=\"% Viability\", axes =FALSE, main=title, log=\"x\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n  magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,3), minorn=c(5,2))\n  legends <- NULL\n  legends.col <- NULL\n  if (length(doses) > 1) {\n    rect(xleft=x1, xright=x2, ybottom=viability.range[1] , ytop=viability.range[2] , col=rgb(240, 240, 240, maxColorValue = 255), border=FALSE)\n  }\n\n  for (i in seq_len(length(doses))) {\n    points(doses[[i]],responses[[i]],pch=20,col = mycol[i], cex=cex)\n\n    switch(plot.type , \"Actual\"={\n      lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i])\n    }, \"Fitted\"={\n      log_logistic_params <- logLogisticRegression(conc=doses[[i]], viability=responses[[i]])\n      log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n      lines(10 ^ log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100 ,lty=1, lwd=lwd, col=mycol[i])\n    },\"Both\"={\n      lines(doses[[i]],responses[[i]],lty=1,lwd=lwd,col = mycol[i])\n      log_logistic_params <- logLogisticRegression(conc = doses[[i]], viability = responses[[i]])\n      log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n      lines(10 ^ log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100 ,lty=1, lwd=lwd, col=mycol[i])\n    })\n    legends<- c(legends, sprintf(\"%s%s\", pSetNames[[i]], legend.values[[i]]))\n    legends.col <-  c(legends.col, mycol[i])\n  }\n\n  legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=c(15,15))\n  return(invisible(NULL))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `drugDoseResponseCurve` function?",
        "answer": "The primary purpose of the `drugDoseResponseCurve` function is to plot drug response curves for a given drug and cell line combination across one or more PharmacoSet objects. It visualizes the relationship between drug concentration and cell viability, allowing for comparison of drug responses across different datasets or experiments."
      },
      {
        "question": "How does the function handle multiple PharmacoSet objects when plotting the drug response curve?",
        "answer": "When multiple PharmacoSet objects are provided, the function plots curves for each set using different colors. It also adds a light gray area to show the common concentration range between the PharmacoSets. The legend includes sensitivity measurements (e.g., IC50, AUC) for each PharmacoSet if requested."
      },
      {
        "question": "What options does the function provide for customizing the plot output?",
        "answer": "The function offers several customization options, including: specifying plot type (Actual, Fitted, or Both), setting y-axis and x-axis limits, customizing colors, adjusting line width and point size, setting the plot title, controlling legend location and content, and choosing whether to summarize replicates or plot them individually."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugDoseResponseCurve <- function(drug, cellline, pSets=list(), concentrations=list(), viabilities=list(), conc_as_log=FALSE, viability_as_pct=TRUE, trunc=TRUE, legends.label=c(\"ic50_published\", \"gi50_published\",\"auc_published\",\"auc_recomputed\",\"ic50_recomputed\"), ylim=c(0,100), xlim, mycol, title, plot.type=c(\"Fitted\",\"Actual\", \"Both\"), summarize.replicates=TRUE, lwd=0.5, cex=0.7, cex.main=0.9, legend.loc=\"topright\", verbose=TRUE, sample_col=\"sampleid\", treatment_col=\"treatmentid\") {\n  # Input validation and data preparation\n  # ...\n\n  # Plot setup\n  if(missing(title)) {\n    title <- if(!missing(drug) && !missing(cellline)) sprintf(\"%s:%s\", drug, cellline) else \"Drug Dose Response Curve\"\n  }\n  plot(NA, xlab=\"Concentration (uM)\", ylab=\"% Viability\", axes=FALSE, main=title, log=\"x\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n  magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,3), minorn=c(5,2))\n\n  # Plot data points and curves\n  for (i in seq_len(length(doses))) {\n    points(doses[[i]], responses[[i]], pch=20, col=mycol[i], cex=cex)\n    # Plot lines based on plot.type\n    # ...\n  }\n\n  # Add legend\n  legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=c(15,15))\n  return(invisible(NULL))\n}",
        "complete": "drugDoseResponseCurve <- function(drug, cellline, pSets=list(), concentrations=list(), viabilities=list(), conc_as_log=FALSE, viability_as_pct=TRUE, trunc=TRUE, legends.label=c(\"ic50_published\", \"gi50_published\",\"auc_published\",\"auc_recomputed\",\"ic50_recomputed\"), ylim=c(0,100), xlim, mycol, title, plot.type=c(\"Fitted\",\"Actual\", \"Both\"), summarize.replicates=TRUE, lwd=0.5, cex=0.7, cex.main=0.9, legend.loc=\"topright\", verbose=TRUE, sample_col=\"sampleid\", treatment_col=\"treatmentid\") {\n  if(!missing(pSets)) {\n    if (!is(pSets, \"list\")) {\n      if (is(pSets, \"PharmacoSet\")) {\n        temp <- name(pSets)\n        pSets <- list(pSets)\n        names(pSets) <- temp\n      } else {\n        stop(\"Type of pSets parameter should be either a pSet or a list of pSets.\")\n      }\n    }\n  }\n  if(!missing(pSets) && (missing(drug) || missing(cellline))) {\n    stop(\"If you pass in a pSet then drug and cellline must be set\")\n  }\n\n  # Data preparation and validation\n  doses <- list(); responses <- list(); legend.values <- list(); pSetNames <- list()\n  if(!missing(pSets)) {\n    for(i in seq_len(length(pSets))) {\n      # Extract and process data from pSets\n      # ...\n    }\n  }\n\n  if(!missing(concentrations)) {\n    # Process manually entered concentrations and viabilities\n    # ...\n  }\n\n  # Calculate dose and viability ranges\n  dose.range <- range(unlist(doses), na.rm=TRUE)\n  viability.range <- c(0, max(unlist(responses), na.rm=TRUE))\n\n  if (!missing(xlim)) dose.range <- xlim\n  if (!missing(ylim)) viability.range <- ylim\n\n  # Set up plot\n  if(missing(title)) {\n    title <- if(!missing(drug) && !missing(cellline)) sprintf(\"%s:%s\", drug, cellline) else \"Drug Dose Response Curve\"\n  }\n  plot(NA, xlab=\"Concentration (uM)\", ylab=\"% Viability\", axes=FALSE, main=title, log=\"x\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n  magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,3), minorn=c(5,2))\n\n  # Plot data points and curves\n  legends <- character(length(doses))\n  legends.col <- mycol[seq_along(doses)]\n  for (i in seq_along(doses)) {\n    points(doses[[i]], responses[[i]], pch=20, col=mycol[i], cex=cex)\n    switch(plot.type,\n      \"Actual\" = lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i]),\n      \"Fitted\" = {\n        log_logistic_params <- logLogisticRegression(conc=doses[[i]], viability=responses[[i]])\n        log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n        lines(10^log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100, lty=1, lwd=lwd, col=mycol[i])\n      },\n      \"Both\" = {\n        lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i])\n        log_logistic_params <- logLogisticRegression(conc=doses[[i]], viability=responses[[i]])\n        log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n        lines(10^log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100, lty=1, lwd=lwd, col=mycol[i])\n      }\n    )\n    legends[i] <- sprintf(\"%s%s\", pSetNames[[i]], legend.values[[i]])\n  }\n\n  # Add legend\n  legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=15)\n  return(invisible(NULL))\n}"
      },
      {
        "partial": "logLogisticRegression <- function(conc, viability, trunc=TRUE, viability_as_pct=TRUE, conc_as_log=FALSE, verbose=TRUE) {\n  # Input validation and data preparation\n  # ...\n\n  # Perform log-logistic regression\n  fit <- try(stats::nls(viability ~ SSlogis(log10_conc, Asym, xmid, scal)), silent=TRUE)\n\n  # Extract and return parameters\n  # ...\n}",
        "complete": "logLogisticRegression <- function(conc, viability, trunc=TRUE, viability_as_pct=TRUE, conc_as_log=FALSE, verbose=TRUE) {\n  if (length(conc) != length(viability)) {\n    stop(\"Length of concentration and viability vectors must be the same.\")\n  }\n\n  # Convert concentration to log10 if necessary\n  log10_conc <- if (conc_as_log) conc else log10(conc)\n\n  # Normalize viability if necessary\n  if (viability_as_pct) {\n    viability <- viability / 100\n  }\n\n  # Truncate viability values if requested\n  if (trunc) {\n    viability[viability > 1] <- 1\n    viability[viability < 0] <- 0\n  }\n\n  # Remove NA and infinite values\n  valid_indices <- which(!is.na(log10_conc) & !is.na(viability) & is.finite(log10_conc) & is.finite(viability))\n  log10_conc <- log10_conc[valid_indices]\n  viability <- viability[valid_indices]\n\n  # Perform log-logistic regression\n  fit <- try(stats::nls(viability ~ SSlogis(log10_conc, Asym, xmid, scal)), silent=TRUE)\n\n  if (class(fit) == \"try-error\") {\n    if (verbose) warning(\"NLS fitting failed. Returning NA for all parameters.\")\n    return(list(HS=NA, E_inf=NA, EC50=NA))\n  }\n\n  # Extract parameters\n  params <- stats::coef(fit)\n  HS <- -1 / params[\"scal\"]\n  E_inf <- params[\"Asym\"] * 100\n  EC50 <- 10^params[\"xmid\"]\n\n  return(list(HS=HS, E_inf=E_inf, EC50=EC50))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/dose.py",
    "language": "py",
    "content": "import warnings\nfrom typing import Dict, Optional, TypeVar\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport SimpleITK as sitk\nfrom pydicom import dcmread\n\nT = TypeVar('T')\n\n\ndef read_image(path):\n    reader = sitk.ImageSeriesReader()\n    dicom_names = reader.GetGDCMSeriesFileNames(path)\n    reader.SetFileNames(dicom_names)\n    reader.MetaDataDictionaryArrayUpdateOn()\n    reader.LoadPrivateTagsOn()\n\n    return reader.Execute()\n\n\nclass Dose(sitk.Image):\n    def __init__(self, img_dose, df, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_dose)\n        self.img_dose = img_dose\n        self.df = df\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n        \n    @classmethod\n    def from_dicom_rtdose(cls, path):\n        '''\n        Reads the data and returns the data frame and the image dosage in SITK format\n        '''\n        # change log (2022-10-12)\n        if \".dcm\" in path:\n            dose = sitk.ReadImage(path)\n        else:\n            dose = read_image(path) \n        \n        # if 4D, make 3D\n        if dose.GetDimension() == 4:\n            dose = dose[:,:,:,0]\n        \n        # Get the metadata\n        df = dcmread(path)\n\n        # Convert to SUV\n        factor = float(df.DoseGridScaling)\n        img_dose = sitk.Cast(dose, sitk.sitkFloat32)\n        img_dose = img_dose * factor\n\n        metadata = {}\n\n        return cls(img_dose, df, metadata)\n\n    def resample_dose(self,\n                      ct_scan: sitk.Image) -> sitk.Image:\n        '''\n        Resamples the RTDOSE information so that it can be overlayed with CT scan. The beginning and end slices of the \n        resampled RTDOSE scan might be empty due to the interpolation\n        '''\n        resampled_dose = sitk.Resample(self.img_dose, ct_scan)  # , interpolator=sitk.sitkNearestNeighbor)\n        return resampled_dose\n\n    def show_overlay(self,\n                     ct_scan: sitk.Image,\n                     slice_number: int):\n        '''\n        For a given slice number, the function resamples RTDOSE scan and overlays on top of the CT scan and returns the figure of the\n        overlay\n        '''\n        resampled_dose = self.resample_dose(ct_scan)\n        fig = plt.figure(\"Overlayed RTdose image\", figsize=[15, 10])\n        dose_arr = sitk.GetArrayFromImage(resampled_dose)\n        plt.subplot(1,3,1)\n        plt.imshow(dose_arr[slice_number,:,:])\n        plt.subplot(1,3,2)\n        ct_arr = sitk.GetArrayFromImage(ct_scan)\n        plt.imshow(ct_arr[slice_number,:,:])\n        plt.subplot(1,3,3)\n        plt.imshow(ct_arr[slice_number,:,:], cmap=plt.cm.gray)\n        plt.imshow(dose_arr[slice_number,:,:], cmap=plt.cm.hot, alpha=.4)\n        return fig\n        \n    def get_metadata(self):\n        '''\n        Forms Dose-Value Histogram (DVH) from DICOM metadata\n        {\n            dvh_type\n            dose_type\n            dose_units\n            vol_units\n            ROI_ID: {\n                vol: different volume values for different dosage bins\n                dose_bins: different dose bins\n                max_dose: max dose value\n                mean_dose : mean dose value\n                min_dose: min dose value\n                total_vol: total volume of the ROI\n            }\n        }\n        '''\n        try:\n            n_ROI =  len(self.df.DVHSequence)\n            self.dvh = {}\n            # These properties are uniform across all the ROIs\n            self.dvh[\"dvh_type\"] = self.df.DVHSequence[0].DVHType   \n            self.dvh[\"dose_units\"] = self.df.DVHSequence[0].DoseUnits\n            self.dvh[\"dose_type\"] = self.df.DVHSequence[0].DoseType\n            self.dvh[\"vol_units\"] = self.df.DVHSequence[0].DVHVolumeUnits\n            # ROI specific properties\n            for i in range(n_ROI):\n                raw_data = np.array(self.df.DVHSequence[i].DVHData)\n                n = len(raw_data)\n\n                # ROI ID\n                ROI_reference = self.df.DVHSequence[i].DVHReferencedROISequence[0].ReferencedROINumber\n\n                # Make dictionary for each ROI ID\n                self.dvh[ROI_reference] = {}\n\n                # DVH specifc properties\n                doses_bin = np.cumsum(raw_data[0:n:2])\n                vol = raw_data[1:n:2]\n                self.dvh[ROI_reference][\"dose_bins\"] = doses_bin.tolist()\n                self.dvh[ROI_reference][\"vol\"] = vol.tolist()\n                \n                # ROI specific properties\n                tot_vol = np.sum(vol)\n                non_zero_index = np.where(vol != 0)[0]\n                min_dose = doses_bin[non_zero_index[0]]\n                max_dose = doses_bin[non_zero_index[-1]]\n                mean_dose = np.sum(doses_bin * (vol / np.sum(vol)))\n                self.dvh[ROI_reference][\"max_dose\"] = max_dose\n                self.dvh[ROI_reference][\"mean_dose\"] = mean_dose\n                self.dvh[ROI_reference][\"min_dose\"] = min_dose\n                self.dvh[ROI_reference][\"total_vol\"] = tot_vol\n        except:\n            # TO-DO: more nuanced error catch instead of returning None\n            warnings.warn(\"No DVH information present in the DICOM. Returning empty dictionary\")\n            self.dvh = {}\n            \n        return self.dvh\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read_image` function in the given code snippet?",
        "answer": "The `read_image` function is designed to read a series of DICOM images from a specified path. It uses SimpleITK's ImageSeriesReader to load the DICOM series, enabling metadata dictionary updates and loading of private tags. The function returns the executed reader, which contains the loaded image series."
      },
      {
        "question": "Explain the `from_dicom_rtdose` class method in the `Dose` class. What does it do and how does it handle different input scenarios?",
        "answer": "The `from_dicom_rtdose` class method is a factory method that creates a `Dose` object from a DICOM RT dose file. It handles two scenarios: 1) If the path ends with '.dcm', it reads a single DICOM file. 2) Otherwise, it assumes the path is a directory and reads a series of DICOM files. The method also converts 4D images to 3D if necessary, applies dose grid scaling, and returns a new `Dose` instance with the processed image, DICOM dataset, and metadata."
      },
      {
        "question": "What is the purpose of the `get_metadata` method in the `Dose` class, and what kind of information does it extract?",
        "answer": "The `get_metadata` method extracts and organizes Dose-Volume Histogram (DVH) information from the DICOM metadata. It creates a dictionary structure containing general DVH properties (like dvh_type, dose_units, etc.) and ROI-specific information. For each ROI, it calculates and stores dose bins, volumes, max dose, mean dose, min dose, and total volume. If no DVH information is present, it returns an empty dictionary and issues a warning."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class Dose(sitk.Image):\n    def __init__(self, img_dose, df, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_dose)\n        self.img_dose = img_dose\n        self.df = df\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n    \n    @classmethod\n    def from_dicom_rtdose(cls, path):\n        # TODO: Implement method to read DICOM RT dose file\n        pass\n\n    def resample_dose(self, ct_scan: sitk.Image) -> sitk.Image:\n        # TODO: Implement dose resampling\n        pass",
        "complete": "class Dose(sitk.Image):\n    def __init__(self, img_dose, df, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_dose)\n        self.img_dose = img_dose\n        self.df = df\n        self.metadata = metadata or {}\n    \n    @classmethod\n    def from_dicom_rtdose(cls, path):\n        dose = sitk.ReadImage(path) if \".dcm\" in path else read_image(path)\n        dose = dose[:,:,:,0] if dose.GetDimension() == 4 else dose\n        df = dcmread(path)\n        factor = float(df.DoseGridScaling)\n        img_dose = sitk.Cast(dose, sitk.sitkFloat32) * factor\n        return cls(img_dose, df, {})\n\n    def resample_dose(self, ct_scan: sitk.Image) -> sitk.Image:\n        return sitk.Resample(self.img_dose, ct_scan)"
      },
      {
        "partial": "def get_metadata(self):\n    try:\n        n_ROI = len(self.df.DVHSequence)\n        self.dvh = {\n            \"dvh_type\": self.df.DVHSequence[0].DVHType,\n            \"dose_units\": self.df.DVHSequence[0].DoseUnits,\n            \"dose_type\": self.df.DVHSequence[0].DoseType,\n            \"vol_units\": self.df.DVHSequence[0].DVHVolumeUnits\n        }\n        # TODO: Implement ROI-specific properties\n    except:\n        warnings.warn(\"No DVH information present in the DICOM. Returning empty dictionary\")\n        self.dvh = {}\n    return self.dvh",
        "complete": "def get_metadata(self):\n    try:\n        n_ROI = len(self.df.DVHSequence)\n        self.dvh = {\n            \"dvh_type\": self.df.DVHSequence[0].DVHType,\n            \"dose_units\": self.df.DVHSequence[0].DoseUnits,\n            \"dose_type\": self.df.DVHSequence[0].DoseType,\n            \"vol_units\": self.df.DVHSequence[0].DVHVolumeUnits\n        }\n        for i in range(n_ROI):\n            raw_data = np.array(self.df.DVHSequence[i].DVHData)\n            ROI_reference = self.df.DVHSequence[i].DVHReferencedROISequence[0].ReferencedROINumber\n            doses_bin = np.cumsum(raw_data[0::2])\n            vol = raw_data[1::2]\n            non_zero_index = np.nonzero(vol)[0]\n            self.dvh[ROI_reference] = {\n                \"dose_bins\": doses_bin.tolist(),\n                \"vol\": vol.tolist(),\n                \"max_dose\": doses_bin[non_zero_index[-1]],\n                \"mean_dose\": np.sum(doses_bin * (vol / np.sum(vol))),\n                \"min_dose\": doses_bin[non_zero_index[0]],\n                \"total_vol\": np.sum(vol)\n            }\n    except:\n        warnings.warn(\"No DVH information present in the DICOM. Returning empty dictionary\")\n        self.dvh = {}\n    return self.dvh"
      }
    ],
    "dependencies": {
      "imports": [
        "warnings",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Dict",
        "matplotlib.pyplot",
        "pydicom.dcmread"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/scan.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nfrom typing import Dict, TypeVar\n\nT = TypeVar('T')\n\n\nclass Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        self.metadata = metadata\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `TypeVar('T')` in this code, and how is it used in the `Scan` class?",
        "answer": "The `TypeVar('T')` creates a type variable 'T' which is used to define a generic type. In the `Scan` class, it's used in the type hint for the `metadata` parameter, allowing the dictionary values to be of any consistent type. This provides flexibility while maintaining type safety, as the actual type will be determined when the class is instantiated."
      },
      {
        "question": "How does the `Scan` class combine SimpleITK functionality with custom metadata storage?",
        "answer": "The `Scan` class combines SimpleITK functionality with custom metadata storage by having two attributes: `image` of type `sitk.Image`, which likely contains medical imaging data that can be processed using SimpleITK functions, and `metadata` which is a dictionary that can store any additional information about the scan. This design allows for both image processing capabilities and flexible metadata management within a single object."
      },
      {
        "question": "What are the potential benefits and drawbacks of using a generic type for the `metadata` dictionary values in the `Scan` class?",
        "answer": "Benefits of using a generic type for `metadata` values include: 1) Flexibility to store different types of data for different scans, 2) Type safety when accessing the metadata, as the type is consistent within a single instance. Drawbacks might include: 1) Lack of specific type information at compile-time, which could make it harder to catch type-related errors early, 2) Potential for runtime type errors if not used carefully, 3) Less clear documentation of expected metadata types without additional comments or type hints in method signatures that use the metadata."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        # Complete the initialization",
        "complete": "class Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        self.metadata = metadata"
      },
      {
        "partial": "import SimpleITK as sitk\nfrom typing import Dict, TypeVar\n\nT = TypeVar('T')\n\n# Complete the Scan class definition",
        "complete": "import SimpleITK as sitk\nfrom typing import Dict, TypeVar\n\nT = TypeVar('T')\n\nclass Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        self.metadata = metadata"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Dict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/sanityCheck.R",
    "language": "R",
    "content": "sanitizeInput <- function(conc,\n\tviability,\n\tHill_fit,\n\tconc_as_log = FALSE,\n\tviability_as_pct = TRUE,\n\ttrunc = TRUE,\n\tverbose = TRUE # Set to 2 to see debug printouts\n\t) {\n\n\n\n\tif (is.logical(conc_as_log) == FALSE) {\n\t\tprint(conc_as_log)\n\t\tstop(\"'conc_as_log' is not a logical.\")\n\t}\n\n\tif (is.logical(viability_as_pct) == FALSE) {\n\t\tprint(viability_as_pct)\n\t\tstop(\"'viability_as_pct' is not a logical.\")\n\t}\n\n\tif (is.logical(trunc) == FALSE) {\n\t\tprint(trunc)\n\t\tstop(\"'trunc' is not a logical.\")\n\t}\n\tif(!is.finite(verbose)){\n\t\tstop(\"'verbose' should be a logical (or numerical) argument.\")\n\t}\n\tif(!missing(viability)&&!missing(conc)&&missing(Hill_fit))\n\t{\n\t  if (length(conc) != length(viability)) {\n\t    if(verbose==2){\n\t      print(conc)\n\t      print(viability)\n\t    }\n\t    stop(\"Log concentration vector is not of same length as viability vector.\")\n\t  }\n\t\tif( any(is.na(conc)&(!is.na(viability)))){\n\t\t\twarning(\"Missing concentrations with non-missing viability values encountered. Removing viability values correspoding to those concentrations\")\n\n\t\t\tmyx <- !is.na(conc)\n\t\t\tconc <- as.numeric(conc[myx])\n\t\t\tviability <- as.numeric(viability[myx])\n\n\t\t}\n\t\tif(any((!is.na(conc))&is.na(viability))){\n\n\t\t\twarning(\"Missing viability with non-missing concentrations values encountered. Removing concentrations values correspoding to those viabilities\")\n\t\t\tmyx <- !is.na(viability)\n\t\t\tconc <- as.numeric(conc[myx])\n\t\t\tviability <- as.numeric(viability[myx])\n\n\t\t}\n\t\tconc <- as.numeric(conc[!is.na(conc)])\n\t\tviability <- as.numeric(viability[!is.na(viability)])\n\n  #CHECK THAT FUNCTION INPUTS ARE APPROPRIATE\n\t\tif (prod(is.finite(conc)) != 1) {\n\t\t\tprint(conc)\n\t\t\tstop(\"Concentration vector contains elements which are not real numbers.\")\n\t\t}\n\n\t\tif (prod(is.finite(viability)) != 1) {\n\t\t\tprint(viability)\n\t\t\tstop(\"Viability vector contains elements which are not real numbers.\")\n\t\t}\n\n\n\t\tif (min(viability) < 0) {\n\t\t\tif (verbose) {\n\n\t\t\t\twarning(\"Warning: Negative viability data.\")\n\t\t\t}\n\t\t}\n\n\t\tif (max(viability) > (1 + 99 * viability_as_pct)) {\n\t\t\tif (verbose) {\n\t\t\t\twarning(\"Warning: Viability data exceeds negative control.\")\n\t\t\t}\n\t\t}\n\n\n\t\tif (conc_as_log == FALSE && min(conc) < 0) {\n\t\t\tif (verbose == 2) {\n\t\t\t\tprint(conc)\n\t\t\t\tprint(conc_as_log)\n\t\t\t}\n\t\t\tstop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n\t\t}\n\n\t\tif (viability_as_pct == TRUE && max(viability) < 5) {\n\t\t\twarning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\t\t\tif (verbose == 2) {\n\n\t\t\t\tprint(viability)\n\t\t\t\tprint(viability_as_pct)\n\t\t\t}\n\t\t}\n\n\t\tif (viability_as_pct == FALSE && max(viability) > 5) {\n\t\t\twarning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\t\t\tif (verbose == 2) {\n\t\t\t\tprint(viability)\n\t\t\t\tprint(viability_as_pct)\n\t\t\t}\n\t\t}\n\n\t\tif(is.unsorted(conc)){\n\t\t\twarning(\"Concentration Values were unsorted. Sorting concentration and ordering viability in same order\")\n\t\t\tmyx <- order(conc)\n\t\t\tconc <- conc[myx]\n\t\t\tviability <- viability[myx]\n\t\t}\n\n  #CONVERT DOSE-RESPONSE DATA TO APPROPRIATE INTERNAL REPRESENTATION\n\t\tif (conc_as_log == FALSE ) {\n\t\t  ii <- which(conc == 0)\n\t\t  if(length(ii) > 0) {\n\t\t    conc <- conc[-ii]\n\t\t    viability <- viability[-ii]\n\t\t  }\n\n\t\t\tlog_conc <- log10(conc)\n\t\t} else {\n\t\t\tlog_conc <- conc\n\t\t}\n\n\t\tif (viability_as_pct == TRUE) {\n\t\t\tviability <- viability / 100\n\t\t}\n\t\tif (trunc) {\n\t\t\tviability = pmin(as.numeric(viability), 1)\n\t\t\tviability = pmax(as.numeric(viability), 0)\n\t\t}\n\n\t\treturn(list(\"log_conc\"=log_conc, \"viability\"=viability))\n\t}\n\tif(!missing(Hill_fit) && missing(viability)){\n\t\tif(is.list(Hill_fit)){\n\n\t\t\tHill_fit <- unlist(Hill_fit)\n\t\t}\n\t\tif (conc_as_log == FALSE && Hill_fit[[3]] < 0) {\n\t\t\tprint(\"EC50 passed in as:\")\n\t\t\tprint(Hill_fit[[3]])\n\t\t\tstop(\"'conc_as_log' flag may be set incorrectly, as the EC50 is negative when positive value is expected.\")\n\t\t}\n\n\n\t\tif (viability_as_pct == FALSE && Hill_fit[[2]] > 1) {\n\t\t\tprint(\"Einf passed in as:\")\n\t\t\tprint(Hill_fit[[2]])\n\n\t\t\twarning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\n\t\t}\n\t\tif (conc_as_log == FALSE){\n\t\t\tHill_fit[[3]] <- log10(Hill_fit[[3]])\n\t\t}\n\t\tif (viability_as_pct == TRUE){\n\t\t\tHill_fit[[2]] <- Hill_fit[[2]]/100\n\t\t}\n\t\tif(missing(conc)){\n\t\t\treturn(list(\"Hill_fit\"=Hill_fit))\n\t\t} else {\n\t\t\tconc <- as.numeric(conc[!is.na(conc)])\n\n\t\t\tif (prod(is.finite(conc)) != 1) {\n\t\t\t\tprint(conc)\n\t\t\t\tstop(\"Concentration vector contains elements which are not real numbers.\")\n\t\t\t}\n\t\t\tif (conc_as_log == FALSE && min(conc) < 0) {\n\t\t\t\tprint(conc)\n\t\t\t\tprint(conc_as_log)\n\t\t\t\tstop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n\t\t\t}\n\n\t\t\tif (conc_as_log == FALSE ) {\n\t\t\t\tii <- which(conc == 0)\n\t\t\t\tif(length(ii) > 0) {\n\t\t\t\t\tconc <- conc[-ii]\n\t\t\t\t}\n\t\t\t\tlog_conc <- log10(conc)\n\t\t\t} else {\n\t\t\t\tlog_conc <- conc\n\t\t\t}\n\t\t\tif(is.unsorted(conc)){\n\t\t\t\tmyx <- order(conc)\n\t\t\t\tconc <- conc[myx]\n\t\t\t}\n\t\t\treturn(list(\"Hill_fit\"=Hill_fit, \"log_conc\" = log_conc))\n\t\t}\n\n\n\t}\n\tif(!missing(Hill_fit)&&!missing(viability)){\n\n\t\tstop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n\t}\n\tif(missing(Hill_fit)&&missing(viability)){\n\n\t\tstop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n\t}\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'sanitizeInput' function and what are its main input parameters?",
        "answer": "The 'sanitizeInput' function is designed to validate and preprocess input data for dose-response analysis. Its main input parameters are:\n1. conc: concentration values\n2. viability: corresponding viability values\n3. Hill_fit: parameters for Hill equation fitting\n4. conc_as_log: boolean indicating if concentrations are in log scale\n5. viability_as_pct: boolean indicating if viability is in percentage\n6. trunc: boolean to truncate viability values between 0 and 1\n7. verbose: controls the level of warning messages"
      },
      {
        "question": "How does the function handle missing or NA values in the concentration and viability data?",
        "answer": "The function handles missing or NA values as follows:\n1. If there are missing concentrations with non-missing viability values, it removes the corresponding viability values and issues a warning.\n2. If there are missing viability values with non-missing concentrations, it removes the corresponding concentration values and issues a warning.\n3. After these checks, it removes any remaining NA values from both concentration and viability vectors using the following lines:\n   conc <- as.numeric(conc[!is.na(conc)])\n   viability <- as.numeric(viability[!is.na(viability)])\nThis ensures that only complete pairs of concentration and viability data are used in further analysis."
      },
      {
        "question": "How does the function handle the case when 'Hill_fit' is provided instead of 'viability' data, and what transformations are applied?",
        "answer": "When 'Hill_fit' is provided instead of 'viability' data, the function:\n1. Checks if Hill_fit is a list and unlist it if necessary.\n2. Validates the EC50 (Hill_fit[[3]]) and Einf (Hill_fit[[2]]) values based on the 'conc_as_log' and 'viability_as_pct' flags.\n3. If 'conc_as_log' is FALSE, it converts the EC50 to log scale: Hill_fit[[3]] <- log10(Hill_fit[[3]]).\n4. If 'viability_as_pct' is TRUE, it converts Einf to a proportion: Hill_fit[[2]] <- Hill_fit[[2]]/100.\n5. If 'conc' is also provided, it processes the concentration data similarly to the case with viability data (converting to log scale if necessary and sorting).\n6. Returns a list containing the processed Hill_fit parameters and, if applicable, the processed log_conc values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"Input flags must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  # Add code to handle missing arguments and perform data validation\n  \n  # Add code to process and sanitize input data\n  \n  # Return sanitized data\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"Input flags must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    if (length(conc) != length(viability)) {\n      stop(\"Log concentration vector is not of same length as viability vector.\")\n    }\n    \n    # Remove NA values\n    valid_data <- complete.cases(conc, viability)\n    conc <- as.numeric(conc[valid_data])\n    viability <- as.numeric(viability[valid_data])\n    \n    # Validate data\n    if (!all(is.finite(conc)) || !all(is.finite(viability))) {\n      stop(\"Input vectors contain non-finite elements.\")\n    }\n    \n    # Convert and sanitize data\n    log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    # Sort data\n    sorted_indices <- order(log_conc)\n    log_conc <- log_conc[sorted_indices]\n    viability <- viability[sorted_indices]\n    \n    return(list(log_conc = log_conc, viability = viability))\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill_fit data\n    Hill_fit <- unlist(Hill_fit)\n    if (conc_as_log == FALSE) Hill_fit[3] <- log10(Hill_fit[3])\n    if (viability_as_pct) Hill_fit[2] <- Hill_fit[2] / 100\n    \n    if (!missing(conc)) {\n      log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n      return(list(Hill_fit = Hill_fit, log_conc = sort(log_conc)))\n    }\n    \n    return(list(Hill_fit = Hill_fit))\n  } else {\n    stop(\"Invalid input combination. Provide either 'viability' and 'conc', or 'Hill_fit'.\")\n  }\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  # Validate input flags\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Process concentration and viability data\n    \n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill_fit data\n    \n  } else {\n    stop(\"Invalid input combination.\")\n  }\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  # Validate input flags\n  if (!all(sapply(list(conc_as_log, viability_as_pct, trunc), is.logical)) || !is.finite(verbose)) {\n    stop(\"Invalid input flags or verbose argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Process concentration and viability data\n    if (length(conc) != length(viability)) stop(\"Mismatched vector lengths.\")\n    \n    valid_data <- complete.cases(conc, viability)\n    conc <- as.numeric(conc[valid_data])\n    viability <- as.numeric(viability[valid_data])\n    \n    if (!all(is.finite(c(conc, viability)))) stop(\"Non-finite values in input.\")\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    sorted_indices <- order(log_conc)\n    return(list(log_conc = log_conc[sorted_indices], viability = viability[sorted_indices]))\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill_fit data\n    Hill_fit <- unlist(Hill_fit)\n    if (!conc_as_log) Hill_fit[3] <- log10(Hill_fit[3])\n    if (viability_as_pct) Hill_fit[2] <- Hill_fit[2] / 100\n    \n    if (!missing(conc)) {\n      log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n      return(list(Hill_fit = Hill_fit, log_conc = sort(log_conc)))\n    }\n    return(list(Hill_fit = Hill_fit))\n  } else {\n    stop(\"Invalid input combination.\")\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/PharmacoSet-class.R",
    "language": "R",
    "content": "#' @importClassesFrom MultiAssayExperiment MultiAssayExperiment\n#' @export\nNULL\n\nsetClassUnion('list_OR_MAE', c('list', 'MultiAssayExperiment'))\n\n# #' @importClassesFrom CoreGx LongTable TreatmentResponseExperiment\n# setClassUnion('list_OR_LongTable', c('list', 'LongTable'))\n\n.local_class=\"PharmacoSet\"\n\n#' A Class to Contain PharmacoGenomic datasets together with their curations\n#'\n#' The PharmacoSet (pSet) class was developed to contain and organise large\n#' PharmacoGenomic datasets, and aid in their metanalysis. It was designed\n#' primarily to allow bioinformaticians and biologists to work with data at the\n#' level of genes, drugs and cell lines, providing a more naturally intuitive\n#' interface and simplifying analyses between several datasets. As such, it was\n#' designed to be flexible enough to hold datasets of two different natures\n#' while providing a common interface. The class can accomidate datasets\n#' containing both drug dose response data, as well as datasets contaning\n#' genetic profiles of cell lines pre and post treatement with compounds, known\n#' respecitively as sensitivity and perturbation datasets.\n#'\n#' @param object A \\code{PharmacoSet} object\n#' @param mDataType A \\code{character} with the type of molecular data to\n#'   return/update\n#' @param value A replacement value\n#'\n#' @slot annotation A \\code{list} of annotation data about the PharmacoSet,\n#'    including the \\code{$name} and the session information for how the object\n#'    was creating, detailing the exact versions of R and all the packages used\n#' @slot molecularProfiles A \\code{list} containing \\code{SummarizedExperiment}\n#'   type object for holding data for RNA, DNA, SNP and CNV\n#'   measurements, with associated \\code{fData} and \\code{pData}\n#'   containing the row and column metadata\n#' @slot sample A \\code{data.frame} containing the annotations for all the cell\n#'   lines profiled in the data set, across all data types\n#' @slot treatment A \\code{data.frame} containg the annotations for all the drugs\n#'   profiled in the data set, across all data types\n#' @slot treatmentResponse A \\code{list} containing all the data for the\n#'   sensitivity experiments, including \\code{$info}, a \\code{data.frame}\n#'   containing the experimental info,\\code{$raw} a 3D \\code{array} containing\n#'   raw data, \\code{$profiles}, a \\code{data.frame} containing sensitivity\n#'   profiles statistics, and \\code{$n}, a \\code{data.frame} detailing the\n#'   number of experiments for each cell-drug pair\n#' @slot perturbation A \\code{list} containting \\code{$n}, a \\code{data.frame}\n#'   summarizing the available perturbation data,\n#' @slot curation A \\code{list} containing mappings for \\code{$treatment},\n#'   \\code{cell}, \\code{tissue} names  used in the data set to universal\n#'   identifiers used between different PharmacoSet objects\n#' @slot datasetType A \\code{character} string of 'sensitivity',\n#'   'perturbation', or both detailing what type of data can be found in the\n#'   PharmacoSet, for proper processing of the data\n#'\n#' @importClassesFrom CoreGx CoreSet\n#' @importClassesFrom CoreGx LongTable\n#' @importClassesFrom CoreGx TreatmentResponseExperiment\n#'\n#' @return An object of the PharmacoSet class\n.PharmacoSet <- setClass('PharmacoSet',\n    contains='CoreSet')\n\n\n# The default constructor above does a poor job of explaining the required\n# structure of a PharmacoSet. The constructor function defined below guides the\n# user into providing the required components of the curation and senstivity\n# lists and hides the annotation slot which the user does not need to manually\n# fill. This also follows the design of the Expression Set class.\n\n#####\n# CONSTRUCTOR -----\n#####\n\n#' PharmacoSet constructor\n#'\n#' A constructor that simplifies the process of creating PharmacoSets, as well\n#' as creates empty objects for data not provided to the constructor. Only\n#' objects returned by this constructor are expected to work with the PharmacoSet\n#' methods. For a much more detailed instruction on creating PharmacoSets, please\n#' see the \"CreatingPharmacoSet\" vignette.\n#'\n#' @examples\n#' ## For help creating a PharmacoSet object, please see the following vignette:\n#' browseVignettes(\"PharmacoGx\")\n#'\n#' @inheritParams CoreGx::CoreSet\n#'\n#' @return An object of class `PharmacoSet`\n#\n#' @import methods\n#' @importFrom utils sessionInfo\n#' @importFrom stats na.omit\n#' @importFrom SummarizedExperiment rowData colData assay assays assayNames Assays\n#' @importFrom S4Vectors DataFrame SimpleList metadata\n#' @importFrom CoreGx CoreSet\n#'\n#' @export\nPharmacoSet <-  function(name, molecularProfiles=list(), sample=data.frame(),\n        treatment=data.frame(), sensitivityInfo=data.frame(),\n        sensitivityRaw=array(dim=c(0,0,0)), sensitivityProfiles=matrix(),\n        sensitivityN=matrix(nrow=0, ncol=0), perturbationN=array(NA, dim=c(0,0,0)),\n        curationTreatment=data.frame(), curationSample = data.frame(),\n        curationTissue = data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n        verify = TRUE, ...) {\n\n    #.Deprecated(\"PharmacoSet2\", )\n\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        molecularProfiles = molecularProfiles,\n        sample=sample,\n        treatment=treatment,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify,\n        ...\n    )\n\n    pSet  <- .PharmacoSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) checkPsetStructure(pSet)\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n        pSet@treatmentResponse$n <- .summarizeSensitivityNumbers(pSet)\n    }\n    if (!length(perturbationN) &&\n            datasetType %in% c(\"perturbation\", \"both\")) {\n        pSet@perturbation$n <- .summarizePerturbationNumbers(pSet)\n    }\n    return(pSet)\n}\n\n#' @eval CoreGx:::.docs_CoreSet2_constructor(class_=.local_class,\n#' sx_=\"Samples in a `PharmacoSet` represent cancer cell-lines.\",\n#' tx_=\"Treatments in a `PharmacoSet` represent pharmaceutical compounds.\",\n#' cx_=\"This class requires an additional curation item, tissue, which maps\n#' from published to standardized tissue idenifiers.\",\n#' data_=.local_data)\n#' @importFrom CoreGx CoreSet2 LongTable TreatmentResponseExperiment\n#' @export\nPharmacoSet2 <- function(name=\"emptySet\", treatment=data.frame(),\n        sample=data.frame(), molecularProfiles=MultiAssayExperiment(),\n        treatmentResponse=TreatmentResponseExperiment(),\n        perturbation=list(),\n        curation=list(sample=data.frame(), treatment=data.frame(),\n        tissue=data.frame()), datasetType=\"sensitivity\"\n) {\n    # -- Leverage existing checks in CoreSet constructor\n    cSet <- CoreSet2(name=name, treatment=treatment,\n        sample=sample, treatmentResponse=treatmentResponse,\n        molecularProfiles=molecularProfiles, curation=curation,\n        perturbation=perturbation, datasetType=datasetType)\n\n    ## -- data integrity\n    # treatment\n    ## TODO\n\n    .PharmacoSet(\n        annotation=cSet@annotation,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        molecularProfiles=cSet@molecularProfiles,\n        treatmentResponse=cSet@treatmentResponse,\n        datasetType=cSet@datasetType,\n        curation=cSet@curation,\n        perturbation=cSet@perturbation\n    )\n}\n\n# Constructor Helper Functions ----------------------------------------------\n\n#' @keywords internal\n#' @importFrom CoreGx idCols . .errorMsg .collapse\n.summarizeSensitivityNumbers <- function(object) {\n    ## TODO:: Checks don't like assigning to global evnironment. Can we return this?\n    assign('object_sumSenNum', object) # Removed envir=.GlobalEnv\n    if (datasetType(object) != 'sensitivity' && datasetType(object) != 'both') {\n        stop ('Data type must be either sensitivity or both')\n    }\n    ## consider all drugs\n    drugn <- treatmentNames(object)\n    ## consider all cell lines\n    celln <- sampleNames(object)\n    sensitivity.info <- matrix(0, nrow=length(celln), ncol=length(drugn),\n        dimnames=list(celln, drugn))\n    drugids <- sensitivityInfo(object)[ , \"treatmentid\"]\n    sampleids <- sensitivityInfo(object)[ , \"sampleid\"]\n    sampleids <- sampleids[grep('///', drugids, invert=TRUE)]\n    drugids <- drugids[grep('///', drugids, invert=TRUE)]\n    tt <- table(sampleids, drugids)\n    sensitivity.info[rownames(tt), colnames(tt)] <- tt\n\n    return(sensitivity.info)\n}\n\n#' @importFrom CoreGx .summarizeMolecularNumbers\n.summarizeMolecularNumbers <- function(object) {\n    CoreGx::.summarizeMolecularNumbers(object)\n}\n\n#' @importFrom CoreGx treatmentNames sampleNames\n.summarizePerturbationNumbers <- function(object) {\n\n    if (datasetType(object) != 'perturbation' && datasetType(object) != 'both') {\n        stop('Data type must be either perturbation or both')\n    }\n\n    ## consider all drugs\n    drugn <- treatmentNames(object)\n\n    ## consider all cell lines\n    celln <- sampleNames(object)\n\n    mprof <- molecularProfilesSlot(object)\n    perturbation.info <- array(0, dim=c(length(celln), length(drugn),\n        length(mprof)),\n        dimnames=list(celln, drugn, names(mprof))\n    )\n    for (i in seq_len(length(mprof))) {\n        if (nrow(colData(mprof[[i]])) > 0 &&\n                all(c(\"sampleid\", \"treatmentid\") %in%\n                    colnames(mprof[[i]]))) {\n            tt <- table(\n                colData(mprof[[i]])[, \"sampleid\"],\n                colData(mprof[[i]])[, \"treatmentid\"]\n            )\n        perturbation.info[rownames(tt), colnames(tt), names(mprof)[i]] <- tt\n        }\n    }\n\n    return(perturbation.info)\n}\n\n### -------------------------------------------------------------------------\n### Class Validity ----------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' A function to verify the structure of a PharmacoSet\n#'\n#' This function checks the structure of a PharamcoSet, ensuring that the\n#' correct annotations are in place and all the required slots are filled so\n#' that matching of cells and drugs can be properly done across different types\n#' of data and with other studies.\n#'\n#' @examples\n#' data(CCLEsmall)\n#' checkPsetStructure(CCLEsmall)\n#'\n#' @param object A \\code{PharmacoSet} to be verified\n#' @param plotDist Should the function also plot the distribution of molecular data?\n#' @param result.dir The path to the directory for saving the plots as a string\n#'\n#' @return Prints out messages whenever describing the errors found in the\n#'   structure of the object object passed in.\n#'\n#' @importFrom graphics hist\n#' @importFrom grDevices dev.off pdf\n#'\n#' @export\ncheckPsetStructure <-\n  function(object, plotDist=FALSE, result.dir='.') {\n\n    # Make directory to store results if it doesn't exist\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n\n    #####\n    # Checking molecularProfiles\n    #####\n    # Can this be parallelized or does it mess with the order of printing warnings?\n    mprof <- molecularProfilesSlot(object)\n    for( i in seq_along(mprof)) {\n      profile <- mprof[[i]]\n      nn <- names(mprof)[i]\n\n      # Testing plot rendering for rna and rnaseq\n      if((S4Vectors::metadata(profile)$annotation == 'rna' || S4Vectors::metadata(profile)$annotation == 'rnaseq') && plotDist)\n      {\n        pdf(file=file.path(result.dir, sprintf('%s.pdf', nn)))\n        hist(assays(profile)[[1]], breaks = 100)\n        dev.off()\n      }\n\n      ## Test if sample and feature annotations dimensions match the assay\n      warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                     sprintf('%s: number of features in fData is different from\n                             SummarizedExperiment slots', nn),\n                     sprintf('%s: rowData dimension is OK', nn)\n                     )\n              )\n      warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                     sprintf('%s: number of cell lines in pData is different\n                             from expression slots', nn),\n                     sprintf('%s: colData dimension is OK', nn)\n                     )\n              )\n\n\n      # Checking sample metadata for required columns\n      warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), '',\n                     sprintf('%s: sampleid does not exist in colData (samples)\n                             columns', nn)))\n      warning(ifelse('batchid' %in% colnames(colData(profile)), '',\n                     sprintf('%s: batchid does not exist in colData (samples)\n                             columns', nn)))\n\n      # Checking mDataType of the SummarizedExperiment for required columns\n      if(S4Vectors::metadata(profile)$annotation == 'rna' |\n         S4Vectors::metadata(profile)$annotation == 'rnaseq')\n      {\n        warning(ifelse('BEST' %in% colnames(rowData(profile)), 'BEST is OK',\n                       sprintf('%s: BEST does not exist in rowData (features)\n                               columns', nn)))\n        warning(ifelse('Symbol' %in% colnames(rowData(profile)), 'Symbol is OK',\n                       sprintf('%s: Symbol does not exist in rowData (features)\n                               columns', nn)))\n      }\n\n      # Check that all sampleids from the object are included in molecularProfiles\n      if(\"sampleid\" %in% colnames(colData(profile))) {\n        if (!all(colData(profile)[,\"sampleid\"] %in% sampleNames(object))) {\n          warning(sprintf('%s: not all the cell lines in this profile are in\n                          cell lines slot', nn))\n        }\n      }else {\n        warning(sprintf('%s: sampleid does not exist in colData (samples)', nn))\n      }\n    }\n\n}\n\n\n### -------------------------------------------------------------------------\n### Method Definitions ------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' Show a PharamcoSet\n#'\n#' @param object \\code{PharmacoSet}\n#'\n#' @examples\n#' data(CCLEsmall)\n#' CCLEsmall\n#'\n#' @return Prints the PharmacoSet object to the output stream, and returns\n#'   invisible NULL.\n#'\n#'  @importFrom CoreGx show\n#'  @importFrom methods callNextMethod\n#'\n#' @export\nsetMethod('show', signature=signature(object='PharmacoSet'), function(object) {\n    callNextMethod(object)\n})\n\n#' Get the dimensions of a PharmacoSet\n#'\n#' @param x PharmacoSet\n#' @return A named vector with the number of Cells and Drugs in the PharmacoSet\n#' @export\nsetMethod('dim', signature=signature(x='PharmacoSet'), function(x){\n    return(c(Cells=length(sampleNames(x)), Drugs=length(treatmentNames(x))))\n})\n\n\n### TODO:: Add updating of sensitivity Number tables\n#' @importFrom CoreGx updateSampleId\n#' @aliases updateCellId\nupdateSampleId <- updateCellId <- function(object, new.ids = vector('character')){\n    CoreGx::updateSampleId(object, new.ids)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `PharmacoSet` class and what types of data can it accommodate?",
        "answer": "The `PharmacoSet` class is designed to contain and organize large pharmacogenomic datasets. It can accommodate two types of datasets: 1) drug dose response data (sensitivity datasets), and 2) genetic profiles of cell lines pre and post treatment with compounds (perturbation datasets). The class provides a common interface for working with data at the level of genes, drugs, and cell lines, simplifying analyses between several datasets."
      },
      {
        "question": "What are the key slots in the `PharmacoSet` class and what do they contain?",
        "answer": "The key slots in the `PharmacoSet` class include: 1) `annotation`: a list of metadata about the PharmacoSet, 2) `molecularProfiles`: a list containing `SummarizedExperiment` objects for RNA, DNA, SNP, and CNV measurements, 3) `sample`: a data frame with annotations for all cell lines, 4) `treatment`: a data frame with annotations for all drugs, 5) `treatmentResponse`: a list containing sensitivity experiment data, 6) `perturbation`: a list containing perturbation data, 7) `curation`: a list of mappings for treatment, cell, and tissue names, and 8) `datasetType`: a character string specifying the type of data in the PharmacoSet."
      },
      {
        "question": "How does the `PharmacoSet` constructor function simplify the process of creating PharmacoSet objects?",
        "answer": "The `PharmacoSet` constructor function simplifies the creation of PharmacoSet objects by: 1) Guiding the user to provide the required components of the curation and sensitivity lists, 2) Hiding the annotation slot which the user doesn't need to manually fill, 3) Creating empty objects for data not provided to the constructor, 4) Performing structure verification if the `verify` parameter is set to `TRUE`, and 5) Automatically summarizing sensitivity and perturbation numbers if not provided. This approach ensures that only objects returned by this constructor are expected to work with PharmacoSet methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "PharmacoSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n        treatment=data.frame(), sensitivityInfo=data.frame(),\n        sensitivityRaw=array(dim=c(0,0,0)), sensitivityProfiles=matrix(),\n        sensitivityN=matrix(nrow=0, ncol=0), perturbationN=array(NA, dim=c(0,0,0)),\n        curationTreatment=data.frame(), curationSample = data.frame(),\n        curationTissue = data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n        verify = TRUE, ...) {\n\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        molecularProfiles = molecularProfiles,\n        sample=sample,\n        treatment=treatment,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify,\n        ...\n    )\n\n    pSet  <- .PharmacoSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) checkPsetStructure(pSet)\n    # Complete the function here\n}",
        "complete": "PharmacoSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n        treatment=data.frame(), sensitivityInfo=data.frame(),\n        sensitivityRaw=array(dim=c(0,0,0)), sensitivityProfiles=matrix(),\n        sensitivityN=matrix(nrow=0, ncol=0), perturbationN=array(NA, dim=c(0,0,0)),\n        curationTreatment=data.frame(), curationSample = data.frame(),\n        curationTissue = data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n        verify = TRUE, ...) {\n\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        molecularProfiles = molecularProfiles,\n        sample=sample,\n        treatment=treatment,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify,\n        ...\n    )\n\n    pSet  <- .PharmacoSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) checkPsetStructure(pSet)\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n        pSet@treatmentResponse$n <- .summarizeSensitivityNumbers(pSet)\n    }\n    if (!length(perturbationN) &&\n            datasetType %in% c(\"perturbation\", \"both\")) {\n        pSet@perturbation$n <- .summarizePerturbationNumbers(pSet)\n    }\n    return(pSet)\n}"
      },
      {
        "partial": "checkPsetStructure <- function(object, plotDist=FALSE, result.dir='.') {\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n    mprof <- molecularProfilesSlot(object)\n    for( i in seq_along(mprof)) {\n        profile <- mprof[[i]]\n        nn <- names(mprof)[i]\n        if((S4Vectors::metadata(profile)$annotation == 'rna' || S4Vectors::metadata(profile)$annotation == 'rnaseq') && plotDist) {\n            pdf(file=file.path(result.dir, sprintf('%s.pdf', nn)))\n            hist(assays(profile)[[1]], breaks = 100)\n            dev.off()\n        }\n        warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                        sprintf('%s: number of features in fData is different from SummarizedExperiment slots', nn),\n                        sprintf('%s: rowData dimension is OK', nn)))\n        warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                        sprintf('%s: number of cell lines in pData is different from expression slots', nn),\n                        sprintf('%s: colData dimension is OK', nn)))\n        warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), '',\n                        sprintf('%s: sampleid does not exist in colData (samples) columns', nn)))\n        warning(ifelse('batchid' %in% colnames(colData(profile)), '',\n                        sprintf('%s: batchid does not exist in colData (samples) columns', nn)))\n        # Complete the function here\n    }\n}",
        "complete": "checkPsetStructure <- function(object, plotDist=FALSE, result.dir='.') {\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n    mprof <- molecularProfilesSlot(object)\n    for( i in seq_along(mprof)) {\n        profile <- mprof[[i]]\n        nn <- names(mprof)[i]\n        if((S4Vectors::metadata(profile)$annotation == 'rna' || S4Vectors::metadata(profile)$annotation == 'rnaseq') && plotDist) {\n            pdf(file=file.path(result.dir, sprintf('%s.pdf', nn)))\n            hist(assays(profile)[[1]], breaks = 100)\n            dev.off()\n        }\n        warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                        sprintf('%s: number of features in fData is different from SummarizedExperiment slots', nn),\n                        sprintf('%s: rowData dimension is OK', nn)))\n        warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                        sprintf('%s: number of cell lines in pData is different from expression slots', nn),\n                        sprintf('%s: colData dimension is OK', nn)))\n        warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), '',\n                        sprintf('%s: sampleid does not exist in colData (samples) columns', nn)))\n        warning(ifelse('batchid' %in% colnames(colData(profile)), '',\n                        sprintf('%s: batchid does not exist in colData (samples) columns', nn)))\n        if(S4Vectors::metadata(profile)$annotation == 'rna' | S4Vectors::metadata(profile)$annotation == 'rnaseq') {\n            warning(ifelse('BEST' %in% colnames(rowData(profile)), 'BEST is OK',\n                            sprintf('%s: BEST does not exist in rowData (features) columns', nn)))\n            warning(ifelse('Symbol' %in% colnames(rowData(profile)), 'Symbol is OK',\n                            sprintf('%s: Symbol does not exist in rowData (features) columns', nn)))\n        }\n        if(\"sampleid\" %in% colnames(colData(profile))) {\n            if (!all(colData(profile)[,\"sampleid\"] %in% sampleNames(object))) {\n                warning(sprintf('%s: not all the cell lines in this profile are in cell lines slot', nn))\n            }\n        } else {\n            warning(sprintf('%s: sampleid does not exist in colData (samples)', nn))\n        }\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_sanitizeInput.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking the sanitization of input to curve fitting and sensitivity summary funcitons\")\n\ntest_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(100,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70),\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70)/100,\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = FALSE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40)/100,\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE, trunc = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(110,90,80,70,60,50,40)/100))\n})\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "test_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  # Complete the test cases here\n})",
        "complete": "test_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(100,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70),\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70)/100,\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = FALSE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40)/100,\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE, trunc = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(110,90,80,70,60,50,40)/100))\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking the sanitization of input to curve fitting and sensitivity summary funcitons\")\n\ntest_that(\"Function sanitizeInput handles no input correctly.\", {\n  # Add test case here\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  # Add test cases here\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  # Add test cases here\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking the sanitization of input to curve fitting and sensitivity summary funcitons\")\n\ntest_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(100,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70),\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70)/100,\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = FALSE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40)/100,\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE, trunc = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(110,90,80,70,60,50,40)/100))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_loaders.py",
    "language": "py",
    "content": "from readii.loaders import *\nimport pytest\n\n@pytest.fixture\ndef nsclcCTPath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n\n@pytest.fixture\ndef nsclcSEGPath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n\n@pytest.fixture\ndef lung4DCTPath():\n    return \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n\n@pytest.fixture\ndef lung4DRTSTRUCTPath():\n    return \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-47.35/1-1.dcm\"\n\n\ndef test_loadDicomSITK(nsclcCTPath):\n    \"\"\"Test loading DICOM from directory.\"\"\"\n    actual = loadDicomSITK(nsclcCTPath)\n    assert isinstance(actual, sitk.Image), \\\n        \"Wrong object type\"\n    assert actual.GetSize() == (512, 512, 304), \\\n        \"Wrong image size\"\n    assert actual.GetSpacing() == (0.693359375, 0.693359375, 1.0), \\\n        \"Wrong spacing\"\n    assert actual.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0), \\\n        \"Wrong origin\"\n\n\ndef test_loadSegmentationSEG(nsclcSEGPath):\n    \"\"\"Test loading a DICOM SEG file\"\"\"\n    actual = loadSegmentation(segImagePath = nsclcSEGPath,\n                              modality = 'SEG')\n\n    assert isinstance(actual, dict), \\\n        \"Wrong object type, should be dictionary\"\n    assert list(actual.keys()) == ['Heart'], \\\n        \"Segmentation label is wrong, should be Heart\"\n\n    actualImage = actual['Heart']\n\n    assert isinstance(actualImage, sitk.Image), \\\n        \"Wrong object type\"\n    assert actualImage.GetSize() == (512, 512, 304, 1), \\\n        \"Wrong image size\"\n    assert actualImage.GetSpacing() == (0.693359375, 0.693359375, 1.0, 1.0), \\\n        \"Wrong spacing\"\n    assert actualImage.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0, 0.0), \\\n        \"Wrong origin\"\n\n\ndef test_loadSegmentationRTSTRUCT(lung4DRTSTRUCTPath, lung4DCTPath):\n    \"\"\"Test loading a RTSTRUCT file\"\"\"\n    actual = loadSegmentation(segImagePath = lung4DRTSTRUCTPath,\n                              modality = 'RTSTRUCT',\n                              baseImageDirPath = lung4DCTPath,\n                              roiNames = 'Tumor_c.*')\n\n    assert isinstance(actual, dict), \\\n        \"Wrong object type, should be dictionary\"\n    assert list(actual.keys()) == ['Tumor_c40'], \\\n        \"Segmentation label is wrong, should be Heart\"\n    \n    actualImage = actual['Tumor_c40']\n\n    assert isinstance(actualImage, sitk.Image), \\\n        \"Wrong object type\"\n    assert actualImage.GetSize() == (512, 512, 99), \\\n        \"Wrong image size\"\n    assert actualImage.GetSpacing() == (0.9766, 0.9766, 3.0), \\\n        \"Wrong spacing\"\n    assert actualImage.GetOrigin() == (-250.0, -163.019, -1132.0), \\\n        \"Wrong origin\"\n\n\ndef test_loadSegmentation_error(nsclcSEGPath):\n    \"\"\"Check ValueError raised when wrong segmentation type is passed\"\"\"\n    with pytest.raises(ValueError):\n        loadSegmentation(segImagePath = nsclcSEGPath,\n                         modality = 'CT')",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@pytest.fixture` decorators in this code, and how are they used in the test functions?",
        "answer": "The `@pytest.fixture` decorators are used to define reusable test fixtures. These fixtures provide paths to different types of medical imaging files (CT scans and segmentation files) that are used in the test functions. The fixtures are injected into the test functions as arguments, allowing easy access to the file paths without hardcoding them in each test. This promotes code reusability and makes it easier to manage test data across multiple test functions."
      },
      {
        "question": "In the `test_loadDicomSITK` function, what assertions are made to verify the correctness of the loaded DICOM image?",
        "answer": "The `test_loadDicomSITK` function makes four assertions to verify the correctness of the loaded DICOM image:\n1. It checks if the returned object is an instance of `sitk.Image`.\n2. It verifies that the image size is (512, 512, 304).\n3. It checks if the image spacing is (0.693359375, 0.693359375, 1.0).\n4. It ensures that the image origin is (-182.1533203125, -314.1533203125, -305.0).\nThese assertions validate the type, dimensions, spatial properties, and positioning of the loaded image."
      },
      {
        "question": "How does the `loadSegmentation` function handle different types of segmentation files, and what error checking is implemented?",
        "answer": "The `loadSegmentation` function handles different types of segmentation files through the `modality` parameter. It supports 'SEG' and 'RTSTRUCT' modalities, as seen in the `test_loadSegmentationSEG` and `test_loadSegmentationRTSTRUCT` functions. The function returns a dictionary containing the segmentation labels as keys and corresponding `sitk.Image` objects as values. Error checking is implemented in the `test_loadSegmentation_error` function, which verifies that a `ValueError` is raised when an unsupported modality (e.g., 'CT') is passed to the `loadSegmentation` function. This ensures that the function properly handles invalid input and provides appropriate error messages."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_loadDicomSITK(nsclcCTPath):\n    actual = loadDicomSITK(nsclcCTPath)\n    assert isinstance(actual, sitk.Image)\n    # Add assertions for size, spacing, and origin",
        "complete": "def test_loadDicomSITK(nsclcCTPath):\n    actual = loadDicomSITK(nsclcCTPath)\n    assert isinstance(actual, sitk.Image), \"Wrong object type\"\n    assert actual.GetSize() == (512, 512, 304), \"Wrong image size\"\n    assert actual.GetSpacing() == (0.693359375, 0.693359375, 1.0), \"Wrong spacing\"\n    assert actual.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0), \"Wrong origin\""
      },
      {
        "partial": "def test_loadSegmentation_error(nsclcSEGPath):\n    # Implement test to check if ValueError is raised\n    # when wrong segmentation type is passed",
        "complete": "def test_loadSegmentation_error(nsclcSEGPath):\n    with pytest.raises(ValueError):\n        loadSegmentation(segImagePath=nsclcSEGPath, modality='CT')"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest"
      ],
      "from_imports": [
        "readii.loaders.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-standardize_names.R",
    "language": "R",
    "content": "#' Standardize Names\n#'\n#' This function takes a character vector and standardizes the names by converting them to lowercase,\n#' removing any trailing information after a comma, removing any information within square brackets or parentheses,\n#' removing any non-alphanumeric characters, replacing empty names with \"invalid\", and converting the names to uppercase.\n#'\n#' @param object A character vector containing the names to be standardized.\n#' @return A character vector with the standardized names.\n#' @examples\n#' standardize_names(c(\"John Doe\", \"Jane Smith (Manager)\", \"Alice, PhD\"))\n#' # Output: [1] \"JOHNDOE\" \"JANESMITH\" \"ALICE\"\n#' @export\nstandardize_names <- function(object) {\n  checkmate::assert_character(object, all.missing = F)\n  object <- tolower(object)\n  object <- gsub(\n    pattern = \",\\\\s.+$\",\n    replacement = \"\",\n    x = object\n  )\n  object <- sub(\n    pattern = \"\\\\s[\\\\[\\\\(].+$\",\n    replacement = \"\",\n    x = object\n  )\n  object <- gsub(\n    pattern = \"[^[:alnum:]]+\",\n    replacement = \"\",\n    x = object\n  )\n  if (any(object == \"\")) {\n    object[object == \"\"] <- NA\n  }\n  object <- toupper(object)\n  object\n}\n\n\n#' Clean character strings by removing special characters and formatting.\n#'\n#' This function takes a character string as input and performs several cleaning operations\n#' to remove special characters, formatting, and unwanted substrings. The cleaned string\n#' is then returned as the output.\n#'\n#' @param name A character string to be cleaned.\n#' @param space_action A character vector specifying the actions to be taken for space characters.\n#'                     One of c(\"\", \"-\", \" \").\n#' @return The cleaned character string.\n#'\n#' @examples\n#' cleanCharacterStrings(\"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\")\n#'\n#' @export\ncleanCharacterStrings <- function(name, space_action = \"\") {\n\n  # make sure name is a string\n  name <- as.character(name)\n\n  # replace space characters based on space_action\n  if (space_action == \"-\") {\n    name <- gsub(\" \", \"-\", name)\n  } else if (space_action == \" \") {\n    name <- gsub(\" \", \" \", name)\n  }else{\n    name <- gsub(\" \", \"\", name)\n  }\n\n  # remove the ~ character\n  name <- gsub(\"~\", \"\", name)\n\n  # if there is a colon like in \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  # remove everything after the colon\n  name <- gsub(\":.*\", \"\", name)\n\n  # remove ,  ;  -  +  *  $  %  #  ^  _  as well as any spaces\n  name <- gsub(\"[\\\\,\\\\;\\\\+\\\\*\\\\$\\\\%\\\\#\\\\^\\\\_]\", \"\", name, perl = TRUE)\n\n  # remove hyphen \n  if (!space_action == \"-\")  name <- gsub(\"-\", \"\", name)\n\n  # remove substring of round brackets and contents\n  name <- gsub(\"\\\\s*\\\\(.*\\\\)\", \"\", name)\n\n  # remove substring of square brackets and contents\n  name <- gsub(\"\\\\s*\\\\[.*\\\\]\", \"\", name)\n\n  # remove substring of curly brackets and contents\n  name <- gsub(\"\\\\s*\\\\{.*\\\\}\", \"\", name)\n\n\n\n  # convert entire string to uppercase\n  name <- toupper(name)\n\n  # dealing with unicode characters \n  name <- gsub(\"Unicode\", \"\", iconv(name, \"LATIN1\", \"ASCII\", \"Unicode\"), perl=TRUE)\n\n  name\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `standardize_names` function, and how does it handle empty names?",
        "answer": "The `standardize_names` function standardizes a character vector of names by converting them to lowercase, removing trailing information after commas, removing information within brackets or parentheses, removing non-alphanumeric characters, and converting the names to uppercase. Empty names are replaced with NA values."
      },
      {
        "question": "How does the `cleanCharacterStrings` function handle spaces in the input string, and what are the possible options for the `space_action` parameter?",
        "answer": "The `cleanCharacterStrings` function handles spaces based on the `space_action` parameter. The possible options are: '' (remove spaces), '-' (replace spaces with hyphens), and ' ' (keep spaces as is). If no `space_action` is specified, spaces are removed by default."
      },
      {
        "question": "What are the main differences between the `standardize_names` and `cleanCharacterStrings` functions in terms of their approach to cleaning strings?",
        "answer": "The main differences are: 1) `standardize_names` focuses on name standardization, while `cleanCharacterStrings` is more general-purpose. 2) `standardize_names` always converts to uppercase, while `cleanCharacterStrings` allows for different space handling options. 3) `cleanCharacterStrings` removes more types of brackets and special characters, and handles Unicode characters, which `standardize_names` does not."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cleanCharacterStrings <- function(name, space_action = \"\") {\n  name <- as.character(name)\n  if (space_action == \"-\") {\n    name <- gsub(\" \", \"-\", name)\n  } else if (space_action == \" \") {\n    name <- gsub(\" \", \" \", name)\n  } else {\n    name <- gsub(\" \", \"\", name)\n  }\n  name <- gsub(\"~\", \"\", name)\n  name <- gsub(\":.*\", \"\", name)\n  # Complete the function by adding code to remove special characters,\n  # brackets and their contents, and convert to uppercase\n}",
        "complete": "cleanCharacterStrings <- function(name, space_action = \"\") {\n  name <- as.character(name)\n  if (space_action == \"-\") {\n    name <- gsub(\" \", \"-\", name)\n  } else if (space_action == \" \") {\n    name <- gsub(\" \", \" \", name)\n  } else {\n    name <- gsub(\" \", \"\", name)\n  }\n  name <- gsub(\"~\", \"\", name)\n  name <- gsub(\":.*\", \"\", name)\n  name <- gsub(\"[\\\\,\\\\;\\\\+\\\\*\\\\$\\\\%\\\\#\\\\^\\\\_]\", \"\", name, perl = TRUE)\n  if (!space_action == \"-\") name <- gsub(\"-\", \"\", name)\n  name <- gsub(\"\\\\s*\\\\(.*\\\\)\", \"\", name)\n  name <- gsub(\"\\\\s*\\\\[.*\\\\]\", \"\", name)\n  name <- gsub(\"\\\\s*\\\\{.*\\\\}\", \"\", name)\n  name <- toupper(name)\n  name <- gsub(\"Unicode\", \"\", iconv(name, \"LATIN1\", \"ASCII\", \"Unicode\"), perl=TRUE)\n  name\n}"
      },
      {
        "partial": "standardize_names <- function(object) {\n  checkmate::assert_character(object, all.missing = F)\n  object <- tolower(object)\n  object <- gsub(\",\\\\s.+$\", \"\", object)\n  # Complete the function by adding code to remove information within brackets,\n  # remove non-alphanumeric characters, replace empty names with NA, and convert to uppercase\n}",
        "complete": "standardize_names <- function(object) {\n  checkmate::assert_character(object, all.missing = F)\n  object <- tolower(object)\n  object <- gsub(\",\\\\s.+$\", \"\", object)\n  object <- sub(\"\\\\s[\\\\[\\\\(].+$\", \"\", object)\n  object <- gsub(\"[^[:alnum:]]+\", \"\", object)\n  object[object == \"\"] <- NA\n  toupper(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_view.R",
    "language": "R",
    "content": "#' Get annotation headings (name only) based on type and heading criteria.\n#'\n#' @param type The type of annotation headings to retrieve.\n#' Options include \"Compound\", \"Gene\", \"Taxonomy\", \"Element\", \"Assay\", \"Protein\", \"Cell\", \"Pathway\", or \"all\" (default).\n#' @param heading The specific heading to filter the results by. Defaults to NULL, which retrieves all headings.\n#'\n#' @return A `data.table` containing the annotation headings and types.\n#'\n#' @examples\n#' getPubchemAnnotationHeadings()\n#' getPubchemAnnotationHeadings(type = \"Compound\")\n#' getPubchemAnnotationHeadings(heading = \"ChEMBL*\")\n#' getPubchemAnnotationHeadings(type = \"Compound\", heading = \"ChEMBL*\")\n#'\n#' @export\ngetPubchemAnnotationHeadings <- function(\n    type = \"all\", heading = NULL) {\n  funContext <- .funContext(\"getPubchemAnnotationHeadings\")\n\n  .debug(funContext, \" type: \", type, \" heading: \", heading)\n  # TODO:: messy...\n  checkmate::assert(\n    checkmate::test_choice(\n      tolower(type), tolower(c(\n        \"Compound\", \"Gene\", \"Taxonomy\", \"Element\",\n        \"Assay\", \"Protein\", \"Cell\", \"Pathway\"\n      ))\n    ) || type == \"all\"\n  )\n\n  ann_dt <- .get_all_heading_types()\n  .debug(funContext, \" ann_dt: \", utils::capture.output(utils::str(ann_dt)))\n  if (type != \"all\") {\n    ann_dt <- ann_dt[grepl(type, ann_dt$Type, ignore.case = T), ]\n  }\n  if (!is.null(heading)) {\n    ann_dt <- ann_dt[grepl(heading, ann_dt$Heading, ignore.case = F), ]\n  }\n\n  if (nrow(ann_dt) == 0) {\n    .warn(\n      funContext, \" No headings found for type: `\", type, \"` and heading: `\", heading,\n      \"`.\\nTry getPubchemAnnotationHeadings(type = 'all') for available headings and types\"\n    )\n  }\n  ann_dt\n}\n\n#' Annotate PubChem Compound\n#'\n#' This function retrieves information about a PubChem compound based on the provided compound ID (CID).\n#'\n#' @param cids The compound ID (CID) of the PubChem compound.\n#' @param heading The type of information to retrieve. Default is \"ChEMBL ID\".\n#' @param source The data source to use. Default is NULL.\n#' @param parse_function A custom parsing function to process the response. Default is the identity function.\n#' @param query_only Logical indicating whether to return the query URL only. Default is FALSE.\n#' @param raw Logical indicating whether to return the raw response. Default is FALSE.\n#' @param nParallel The number of parallel processes to use. Default is 1.\n#'\n#' @return The annotated information about the PubChem compound.\n#'\n#' @examples\n#' annotatePubchemCompound(cid = 2244)\n#' annotatePubchemCompound(cid = c(2244, 67890), heading = \"CAS\")\n#'\n#' @export\nannotatePubchemCompound <- function(\n    cids, heading = \"ChEMBL ID\", source = NULL, parse_function = identity,\n    query_only = FALSE, raw = FALSE, nParallel = 1\n  ) {\n  funContext <- .funContext(\"annotatePubchemCompound\")\n\n  .info(funContext, sprintf(\"Building requests for %s CIDs\", length(cids)))\n  requests <- lapply(cids, function(cid) {\n    .build_pubchem_view_query(\n      id = cid, record = \"compound\", heading = heading,\n      output = \"JSON\", source = source\n      )\n   }\n  )\n\n  .debug(funContext, paste0(\"query: \", sapply(requests, `[[`, i = \"url\")))\n  if (query_only) return(requests)\n\n  tryCatch({\n    resp_raw <- httr2::req_perform_sequential(\n      reqs = requests, \n      on_error = \"continue\",\n      progress = \"Performing API requests...\"\n  )}, error = function(e) {\n    .err(funContext, \"An error occurred while performing requests:\\n\", e)\n  })\n\n  if (raw) return(resp_raw)\n\n  responses <- lapply(seq_along(resp_raw), function(i){\n    resp <- resp_raw[[i]]\n    if(is.null(resp)) return(NA_character_)\n    tryCatch(\n      {\n        .parse_resp_json(resp)\n      },\n      error = function(e) {\n        warnmsg <- sprintf(\n          \"\\nThe response could not be parsed:\\n\\t%s\\tReturning NA instead for CID: %s for the heading: %s\",\n          e, cids[i], heading\n        )\n        .warn(\n          funContext, warnmsg\n        )\n        resp\n      }\n    )\n  })\n\n  # apply the parse function to each response depending on heading\n  parsed_responses <- parallel::mclapply(responses, function(response) {\n    switch(heading,\n      \"ChEMBL ID\" = .parseCHEMBLresponse(response),\n      \"CAS\" = .parseCASresponse(response),\n      \"NSC Number\" = .parseNSCresponse(response),\n      \"ATC Code\" = .parseATCresponse(response),\n      \"Drug Induced Liver Injury\" = .parseDILIresponse(response),\n      tryCatch(\n        {\n          parse_function(response)\n        },\n        error = function(e) {\n          .warn(\n            funContext, \"The parseFUN function failed: \", e,\n            \". Returning unparsed results instead. Please test the parseFUN\n                  on the returned data.\"\n          )\n          response\n        }\n      )\n    )\n  },\n  mc.cores = nParallel \n)\n  \n\n  sapply(parsed_responses, .replace_null)\n\n}\n\n# helper function to replace NULL with NA\n.replace_null <- function(x) {\n  ifelse(is.null(x), NA_character_, x)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemAnnotationHeadings` function and what are its main parameters?",
        "answer": "The `getPubchemAnnotationHeadings` function retrieves annotation headings based on type and heading criteria from PubChem. Its main parameters are `type` (default 'all'), which specifies the type of annotation headings to retrieve (e.g., 'Compound', 'Gene', 'Taxonomy'), and `heading` (default NULL), which allows filtering results by a specific heading."
      },
      {
        "question": "How does the `annotatePubchemCompound` function handle multiple compound IDs (CIDs) and what is the purpose of the `nParallel` parameter?",
        "answer": "The `annotatePubchemCompound` function can handle multiple CIDs by creating a list of requests using `lapply`. It then uses `httr2::req_perform_sequential` to execute these requests. The `nParallel` parameter is used in the `mclapply` function when parsing responses, allowing for parallel processing of the results to improve performance when dealing with multiple compounds."
      },
      {
        "question": "What is the purpose of the `parse_function` parameter in the `annotatePubchemCompound` function, and how does the function handle different types of headings?",
        "answer": "The `parse_function` parameter allows users to provide a custom parsing function to process the API response. The function handles different types of headings using a `switch` statement, which applies specific parsing functions (e.g., `.parseCHEMBLresponse`, `.parseCASresponse`) based on the heading. If the heading doesn't match any predefined cases, it applies the custom `parse_function` provided by the user, with error handling to return unparsed results if the custom function fails."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getPubchemAnnotationHeadings <- function(type = \"all\", heading = NULL) {\n  funContext <- .funContext(\"getPubchemAnnotationHeadings\")\n\n  .debug(funContext, \" type: \", type, \" heading: \", heading)\n  checkmate::assert(\n    checkmate::test_choice(\n      tolower(type), tolower(c(\n        \"Compound\", \"Gene\", \"Taxonomy\", \"Element\",\n        \"Assay\", \"Protein\", \"Cell\", \"Pathway\"\n      ))\n    ) || type == \"all\"\n  )\n\n  ann_dt <- .get_all_heading_types()\n  .debug(funContext, \" ann_dt: \", utils::capture.output(utils::str(ann_dt)))\n  if (type != \"all\") {\n    ann_dt <- ann_dt[grepl(type, ann_dt$Type, ignore.case = T), ]\n  }\n  if (!is.null(heading)) {\n    ann_dt <- ann_dt[grepl(heading, ann_dt$Heading, ignore.case = F), ]\n  }\n\n  # Complete the function by adding the warning message and returning ann_dt\n}",
        "complete": "getPubchemAnnotationHeadings <- function(type = \"all\", heading = NULL) {\n  funContext <- .funContext(\"getPubchemAnnotationHeadings\")\n\n  .debug(funContext, \" type: \", type, \" heading: \", heading)\n  checkmate::assert(\n    checkmate::test_choice(\n      tolower(type), tolower(c(\n        \"Compound\", \"Gene\", \"Taxonomy\", \"Element\",\n        \"Assay\", \"Protein\", \"Cell\", \"Pathway\"\n      ))\n    ) || type == \"all\"\n  )\n\n  ann_dt <- .get_all_heading_types()\n  .debug(funContext, \" ann_dt: \", utils::capture.output(utils::str(ann_dt)))\n  if (type != \"all\") {\n    ann_dt <- ann_dt[grepl(type, ann_dt$Type, ignore.case = T), ]\n  }\n  if (!is.null(heading)) {\n    ann_dt <- ann_dt[grepl(heading, ann_dt$Heading, ignore.case = F), ]\n  }\n\n  if (nrow(ann_dt) == 0) {\n    .warn(\n      funContext, \" No headings found for type: `\", type, \"` and heading: `\", heading,\n      \"`.\nTry getPubchemAnnotationHeadings(type = 'all') for available headings and types\"\n    )\n  }\n  ann_dt\n}"
      },
      {
        "partial": "annotatePubchemCompound <- function(cids, heading = \"ChEMBL ID\", source = NULL, parse_function = identity,\n    query_only = FALSE, raw = FALSE, nParallel = 1) {\n  funContext <- .funContext(\"annotatePubchemCompound\")\n\n  .info(funContext, sprintf(\"Building requests for %s CIDs\", length(cids)))\n  requests <- lapply(cids, function(cid) {\n    .build_pubchem_view_query(\n      id = cid, record = \"compound\", heading = heading,\n      output = \"JSON\", source = source\n    )\n  })\n\n  .debug(funContext, paste0(\"query: \", sapply(requests, `[[`, i = \"url\")))\n  if (query_only) return(requests)\n\n  # Complete the function by adding the API request, response parsing, and result processing\n}",
        "complete": "annotatePubchemCompound <- function(cids, heading = \"ChEMBL ID\", source = NULL, parse_function = identity,\n    query_only = FALSE, raw = FALSE, nParallel = 1) {\n  funContext <- .funContext(\"annotatePubchemCompound\")\n\n  .info(funContext, sprintf(\"Building requests for %s CIDs\", length(cids)))\n  requests <- lapply(cids, function(cid) {\n    .build_pubchem_view_query(\n      id = cid, record = \"compound\", heading = heading,\n      output = \"JSON\", source = source\n    )\n  })\n\n  .debug(funContext, paste0(\"query: \", sapply(requests, `[[`, i = \"url\")))\n  if (query_only) return(requests)\n\n  tryCatch({\n    resp_raw <- httr2::req_perform_sequential(\n      reqs = requests, \n      on_error = \"continue\",\n      progress = \"Performing API requests...\"\n    )\n  }, error = function(e) {\n    .err(funContext, \"An error occurred while performing requests:\\n\", e)\n  })\n\n  if (raw) return(resp_raw)\n\n  responses <- lapply(seq_along(resp_raw), function(i){\n    resp <- resp_raw[[i]]\n    if(is.null(resp)) return(NA_character_)\n    tryCatch(\n      {\n        .parse_resp_json(resp)\n      },\n      error = function(e) {\n        warnmsg <- sprintf(\n          \"\\nThe response could not be parsed:\\n\\t%s\\tReturning NA instead for CID: %s for the heading: %s\",\n          e, cids[i], heading\n        )\n        .warn(funContext, warnmsg)\n        resp\n      }\n    )\n  })\n\n  parsed_responses <- parallel::mclapply(responses, function(response) {\n    switch(heading,\n      \"ChEMBL ID\" = .parseCHEMBLresponse(response),\n      \"CAS\" = .parseCASresponse(response),\n      \"NSC Number\" = .parseNSCresponse(response),\n      \"ATC Code\" = .parseATCresponse(response),\n      \"Drug Induced Liver Injury\" = .parseDILIresponse(response),\n      tryCatch(\n        {\n          parse_function(response)\n        },\n        error = function(e) {\n          .warn(\n            funContext, \"The parseFUN function failed: \", e,\n            \". Returning unparsed results instead. Please test the parseFUN on the returned data.\"\n          )\n          response\n        }\n      )\n    )\n  }, mc.cores = nParallel)\n\n  sapply(parsed_responses, .replace_null)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/PharmacoSet-accessors.R",
    "language": "R",
    "content": "## Navigating this file:\n## - Slot section names start with ----\n## - Method section names start with ==\n##\n## As a result, you can use Ctrl + f to find the slot or method you are looking\n## for quickly, assuming you know its name.\n##\n## For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n## method, while Ctrl +f '---- molecularProfiles' would take you to the slot\n## section.\n\n#' @include PharmacoSet-class.R\nNULL\n\n## Variables for dynamic inheritted roxygen2 docs\n\n.local_class <- 'PharmacoSet'\n.local_data <- 'CCLEsmall'\n\n#### CoreGx inherited methods\n####\n#### Note: The raw documentation lives in CoreGx, see the functions called\n#### in @eval tags for the content of the metaprogrammed roxygen2 docs.\n####\n#### See .parseToRoxygen method in utils-messages.R file of CoreGx to\n#### create similar metaprogrammed docs.\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n#' @title .parseToRoxygen\n#'\n#' @description\n#' Helper for metaprogramming roxygen2 documentation\n#'\n#' @details\n#' Takes a string block of roxygen2 tags sepearated by new-line\n#'   characteres and parses it to the appropriate format for the @eval tag,\n#'   subtituting any string in { } for the argument of the same name in `...`.\n#'\n#' @keywords internal\n#' @importFrom CoreGx .parseToRoxygen\n#' @export\n#' @noRd\n.parseToRoxygen <- function(string, ...) {\n    CoreGx::.parseToRoxygen(string, ...)\n}\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n\n#' @name PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_accessors(class_=.local_class)\n#' @eval .parseToRoxygen(\n#'      \"@examples data({data_})\n#'      \", data_=.local_data)\n#' @importFrom methods callNextMethod\nNULL\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ==============\n## ---- drug slot\n## --------------\n\n\n##\n## == drugInfo\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo\n#' @aliases drugInfo\n#' @export\ndrugInfo <- function(...) treatmentInfo(...)\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo<-\n#' @aliases drugInfo<-\n#' @export\n`drugInfo<-` <- function(..., value) `treatmentInfo<-`(..., value=value)\n\n\n\n##\n## == drugNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames\n#' @aliases drugNames\n#' @export\ndrugNames <- function(...) treatmentNames(...)\n\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames<-\n#' @aliases drugNames<-\n#' @export\n`drugNames<-` <- function(..., value) `treatmentNames<-`(..., value=value)\n\n\n\n\n## ====================\n## ---- annotation slot\n## --------------------\n\n\n##\n## == annotation\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation\n#' @export\nsetMethod('annotation', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation<-\n#' @export\nsetReplaceMethod(\"annotation\", signature(\"PharmacoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == dateCreated\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated\n#' @export\nsetMethod('dateCreated', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated<-\n#' @export\nsetReplaceMethod('dateCreated', signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## === name\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name\nsetMethod('name', signature(\"PharmacoSet\"), function(object){\n    callNextMethod(object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name<-\nsetReplaceMethod('name', signature(\"PharmacoSet\"), function(object, value){\n    object <- callNextMethod(object, value=value)\n    return(invisible(object))\n})\n\n## ==============\n## ---- sample slot\n## --------------\n\n\n##\n## == sampleInfo\n\n.local_sample <- \"cell\"\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleInfo(class_=.local_class, sample_=.local_sample)\n#' @importFrom CoreGx sampleInfo\n#' @export\nsetMethod(\"sampleInfo\", \"PharmacoSet\", function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleInfo(class_=.local_class,\n#' data_=.local_data, sample_=\"cell\")\n#' @importFrom CoreGx sampleInfo<-\n#' @export\nsetReplaceMethod(\"sampleInfo\", signature(object=\"PharmacoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\n\n##\n## == sampleNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames\nsetMethod(\"sampleNames\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames<-\nsetReplaceMethod(\"sampleNames\", signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n\n## ------------------\n## ---- curation slot\n\n\n##\n## == curation\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_curation(class_=.local_class,\n#' data_=.local_data, details_=\"Contains three `data.frame`s, 'cell' with\n#' cell-line ids and 'tissue' with tissue ids and 'drug' with drug ids.\")\n#' @importMethodsFrom CoreGx curation\nsetMethod('curation', signature(object=\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_curation(class_=.local_class,\n#' data_=.local_data, details_=\"For a `PharmacoSet` object the slot should\n#' contain tissue, cell-line and drug id `data.frame`s.\")\n#' @importMethodsFrom CoreGx curation<-\nsetReplaceMethod(\"curation\", signature(object=\"PharmacoSet\", value=\"list\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ----------------------\n## ---- datasetType slot\n\n\n#\n# == datasetType\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType\nsetMethod(\"datasetType\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType<-\nsetReplaceMethod(\"datasetType\", signature(object=\"PharmacoSet\",\n    value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ---------------------------\n## ---- molecularProfiles slot\n\n\n##\n## == molecularProfiles\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles\nsetMethod(molecularProfiles, \"PharmacoSet\", function(object, mDataType, assay)\n{\n    callNextMethod()\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles<-\nsetReplaceMethod(\"molecularProfiles\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\", assay=\"character\", value=\"matrix\"),\n    function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\nsetReplaceMethod(\"molecularProfiles\",\n    signature(object=\"PharmacoSet\", mDataType =\"character\", assay=\"missing\",\n        value=\"matrix\"), function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\n\n\n##\n## == featureInfo\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_featureInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx featureInfo\nsetMethod(featureInfo, \"PharmacoSet\", function(object, mDataType) {\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_featureInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx featureInfo<-\nsetReplaceMethod(\"featureInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\",value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"featureInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\",value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n\n##\n## == phenoInfo\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo\nsetMethod('phenoInfo', signature(object='PharmacoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo<-\nsetReplaceMethod(\"phenoInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\", value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"phenoInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\", value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == fNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames\nsetMethod('fNames', signature(object='PharmacoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames<-\nsetReplaceMethod('fNames', signature(object='PharmacoSet', mDataType='character',\n    value='character'), function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == mDataNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames\nsetMethod(\"mDataNames\", \"PharmacoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames<-\nsetReplaceMethod(\"mDataNames\", \"PharmacoSet\", function(object, value){\n    callNextMethod(object=object, value=value)\n})\n\n\n\n##\n## == molecularProfilesSlot\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot\nsetMethod(\"molecularProfilesSlot\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot<-\nsetReplaceMethod(\"molecularProfilesSlot\", signature(\"PharmacoSet\", \"list_OR_MAE\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n# ---------------------\n## ---- sensitivity slot\n\n\n##\n## == sensitivityInfo\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo\nsetMethod('sensitivityInfo', signature(\"PharmacoSet\"),\n    function(object, dimension, ...)\n{\n    callNextMethod(object=object, dimension=dimension, ...)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo<-\nsetReplaceMethod(\"sensitivityInfo\", signature(object=\"PharmacoSet\",\n    value=\"data.frame\"), function(object, dimension, ..., value)\n{\n    callNextMethod(object=object, dimension=dimension, ..., value=value)\n})\n\n\n##\n## == sensitvityMeasures\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityMeasures(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityMeasures\nsetMethod('sensitivityMeasures', signature(object=\"PharmacoSet\"),\n    function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitityMeasures(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('sensitivityMeasures',\n    signature(object='PharmacoSet', value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensitivityProfiles\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles\nsetMethod('sensitivityProfiles', signature(object=\"PharmacoSet\"), function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles<-\nsetReplaceMethod(\"sensitivityProfiles\",\n    signature(object=\"PharmacoSet\", value=\"data.frame\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n#\n# == sensitivityRaw\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw\nsetMethod(\"sensitivityRaw\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw<-\nsetReplaceMethod('sensitivityRaw', signature(\"PharmacoSet\", \"array\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n#\n# == treatmentResponse\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentResponse(class_=.local_class,\n#'   data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentResponse\nsetMethod(\"treatmentResponse\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n\n\n#' @rdname PharmacoSet-accessors\n#' @importMethodsFrom CoreGx treatmentResponse<-\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentResponse(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('treatmentResponse', signature(object='PharmacoSet',\n    value='list_OR_LongTable'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensNumber\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber\nsetMethod('sensNumber', \"PharmacoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber<-\nsetReplaceMethod('sensNumber', signature(object=\"PharmacoSet\", value=\"matrix\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ======================\n## ---- perturbation slot\n\n\n##\n## == pertNumber\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber\nsetMethod('pertNumber', signature(object='PharmacoSet'), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber<-\nsetReplaceMethod('pertNumber', signature(object='PharmacoSet', value=\"array\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.parseToRoxygen` function in this code?",
        "answer": "The `.parseToRoxygen` function is a helper for metaprogramming roxygen2 documentation. It takes a string block of roxygen2 tags separated by new-line characters and parses it to the appropriate format for the @eval tag, substituting any string in { } for the argument of the same name in `...`."
      },
      {
        "question": "How does the `drugInfo` method relate to the `treatmentInfo` method in this code?",
        "answer": "The `drugInfo` method is an alias for the `treatmentInfo` method. It uses the same implementation as `treatmentInfo` but provides a more specific name for the PharmacoSet context. This is evident from the line `drugInfo <- function(...) treatmentInfo(...)` and similarly for the setter method."
      },
      {
        "question": "What is the purpose of the `callNextMethod` function used in many of the accessor methods?",
        "answer": "The `callNextMethod` function is used to call the next method in the method dispatch chain. In this context, it's used to delegate the implementation to the parent class (likely CoreSet) while allowing for potential additional behavior specific to the PharmacoSet class. This promotes code reuse and maintains the inheritance hierarchy."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('annotation', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"PharmacoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})",
        "complete": "setMethod('annotation', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"PharmacoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\nsetMethod('dateCreated', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod('dateCreated', signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})"
      },
      {
        "partial": "setMethod(\"sampleInfo\", \"PharmacoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"PharmacoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})",
        "complete": "setMethod(\"sampleInfo\", \"PharmacoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"PharmacoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\nsetMethod(\"sampleNames\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleNames\", signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/pipeline.py",
    "language": "py",
    "content": "import warnings\nfrom itertools import chain\n\nfrom joblib import Parallel, delayed\n\nfrom .ops import BaseOp, BaseInput, BaseOutput\n\n\nclass Pipeline:\n    \"\"\"Base class for image processing pipelines.\n\n    A pipeline can be created by subclassing, instantiating the required\n    data loaders and operations from `ops` in the constructor and implementing\n    the `process_one_case` method, which defines the processing steps for one\n    case (i.e. one subject_id from data loaders).\n    \"\"\"\n    def __init__(self, n_jobs=1, missing_strategy=\"drop\", show_progress=True, warn_on_error=False):\n        \"\"\"Initialize the base class.\n\n        Parameters\n        ----------\n        n_jobs : int, optional\n            The number of worker processes to use for parallel computation\n            (default 0).\n        \"\"\"\n        self.n_jobs = n_jobs\n        self.missing_strategy = missing_strategy.lower()\n        self.show_progress = show_progress\n        self.warn_on_error = warn_on_error\n        if self.missing_strategy not in [\"drop\", \"pass\"]:\n            raise ValueError(f\"missing_strategy must be either of 'drop' or 'pass', got {missing_strategy}\")\n\n    def _get_loader_subject_ids(self):\n        loaders = (v._loader for v in self.__dict__.values() if isinstance(v, BaseInput))\n        all_subject_ids = [loader.keys() for loader in loaders]\n        unique_subject_ids = set(chain.from_iterable(all_subject_ids))\n\n        if not all_subject_ids:\n            raise AttributeError(\"Pipeline must define at least one input op (subclass of ops.BaseInput)\")\n\n        result = []\n        for subject_id in unique_subject_ids:\n            if not all((subject_id in subject_ids for subject_ids in all_subject_ids)):\n                # TODO give more details about which input data is missing\n                message = f\"Subject {subject_id} is missing some of the input data \"\n                if self.missing_strategy == \"drop\":\n                    message += f\"and will be dropped according to current missing strategy ('{self.missing_strategy}').\"\n                elif self.missing_strategy == \"pass\":\n                    message += f\"but will be passed according to current missing strategy ('{self.missing_strategy}').\"\n                    result.append(subject_id)\n                warnings.warn(message, category=RuntimeWarning)\n                continue\n            result.append(subject_id)\n\n        return result\n\n    @property\n    def ops(self):\n        # TODO (Michal) return ops in actual order of execution\n        return [v for v in self.__dict__.values() if isinstance(v, BaseOp)]\n\n    def __repr__(self):\n        attrs = [(k, v) for k, v in self.__dict__.items() if not isinstance(v, BaseOp) and not k.startswith(\"_\")]\n        args = \", \".join(f\"{k}={v}\" for k, v in attrs)\n        return f\"{self.__class__.__module__}.{self.__class__.__name__}({args})\"\n\n    def __str__(self):\n        repr_ = self.__repr__()\n        ops = self.ops()\n        if not ops:\n            ops = \"<none>\"\n        else:\n            ops = \",\\n\".join(ops)\n        return repr_ + \"\\n\" + \"ops: (\\n\" + ops + \"\\n)\"\n\n    def process_one_subject(self, subject_id):\n        \"\"\"Define the processing steps for one case.\n\n        Parameters\n        ----------\n        subject_id : str\n            The ID of the subject to be processed.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_wrapper(self, subject_id):\n        try:\n            print(subject_id)\n            self.process_one_subject(subject_id)\n        except Exception as e:\n            message = f\"{type(e).__name__} while processing subject {subject_id}: \" + str(e)\n            if self.warn_on_error:\n                warnings.warn(message, category=RuntimeWarning)\n            else:\n                raise RuntimeError(message) from e\n\n    def run(self):\n        \"\"\"Execute the pipeline, possibly in parallel.\n        \"\"\"\n        # Joblib prints progress to stdout if verbose > 50\n        verbose = 51 if self.show_progress else 0\n\n        subject_ids = self._get_loader_subject_ids()\n        # Note that returning any SimpleITK object in process_one_subject is\n        # not supported yet, since they cannot be pickled\n        Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n            delayed(self._process_wrapper)(subject_id) for subject_id in subject_ids)\n\n\nclass SequentialPipeline(Pipeline):\n    def __init__(self, ops_list):\n        self.source = ops_list.pop(0)\n        self.sink = ops_list.pop() if isinstance(ops_list[-1], BaseOutput) else None\n        self.ops_list = ops_list\n\n    def process_one_subject(self, subject_id):\n        image = self.source(subject_id)\n        for op in self.ops_list:\n            image = op(image)\n        if self.sink is not None:\n            self.sink(image)\n        return image\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `_get_loader_subject_ids` method in the `Pipeline` class?",
        "answer": "The `_get_loader_subject_ids` method is responsible for gathering and filtering subject IDs from all input loaders. It ensures that only subject IDs present in all input data sources are included, based on the specified missing strategy ('drop' or 'pass'). It also raises warnings for missing data and handles cases where subjects are missing from some input sources."
      },
      {
        "question": "How does the `SequentialPipeline` class differ from the base `Pipeline` class in terms of operation execution?",
        "answer": "The `SequentialPipeline` class implements a specific processing flow where operations are executed sequentially. It takes a list of operations, with the first operation being the source (input) and the last potentially being the sink (output). The `process_one_subject` method applies each operation in the list to the image data, passing the result of each operation to the next one in the sequence."
      },
      {
        "question": "What is the purpose of the `_process_wrapper` method in the `Pipeline` class, and how does it handle errors?",
        "answer": "The `_process_wrapper` method is a helper function used to execute `process_one_subject` for each subject ID. It provides error handling functionality, catching any exceptions that occur during processing. If `warn_on_error` is set to True, it will issue a warning for any errors encountered. Otherwise, it will raise a `RuntimeError` with details about the exception. This method allows for more robust error handling and reporting when processing multiple subjects."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class SequentialPipeline(Pipeline):\n    def __init__(self, ops_list):\n        self.source = ops_list.pop(0)\n        self.sink = ops_list.pop() if isinstance(ops_list[-1], BaseOutput) else None\n        self.ops_list = ops_list\n\n    def process_one_subject(self, subject_id):\n        # TODO: Implement the processing logic\n        pass",
        "complete": "class SequentialPipeline(Pipeline):\n    def __init__(self, ops_list):\n        self.source = ops_list.pop(0)\n        self.sink = ops_list.pop() if isinstance(ops_list[-1], BaseOutput) else None\n        self.ops_list = ops_list\n\n    def process_one_subject(self, subject_id):\n        image = self.source(subject_id)\n        for op in self.ops_list:\n            image = op(image)\n        if self.sink is not None:\n            self.sink(image)\n        return image"
      },
      {
        "partial": "class Pipeline:\n    def __init__(self, n_jobs=1, missing_strategy=\"drop\", show_progress=True, warn_on_error=False):\n        self.n_jobs = n_jobs\n        self.missing_strategy = missing_strategy.lower()\n        self.show_progress = show_progress\n        self.warn_on_error = warn_on_error\n        # TODO: Implement validation for missing_strategy\n\n    def run(self):\n        # TODO: Implement the run method\n        pass",
        "complete": "class Pipeline:\n    def __init__(self, n_jobs=1, missing_strategy=\"drop\", show_progress=True, warn_on_error=False):\n        self.n_jobs = n_jobs\n        self.missing_strategy = missing_strategy.lower()\n        self.show_progress = show_progress\n        self.warn_on_error = warn_on_error\n        if self.missing_strategy not in [\"drop\", \"pass\"]:\n            raise ValueError(f\"missing_strategy must be either of 'drop' or 'pass', got {missing_strategy}\")\n\n    def run(self):\n        verbose = 51 if self.show_progress else 0\n        subject_ids = self._get_loader_subject_ids()\n        Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n            delayed(self._process_wrapper)(subject_id) for subject_id in subject_ids)"
      }
    ],
    "dependencies": {
      "imports": [
        "warnings"
      ],
      "from_imports": [
        "itertools.chain",
        "joblib.Parallel",
        "ops.BaseOp"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-summarizeSensitivityProfiles.R",
    "language": "R",
    "content": "#' Takes the sensitivity data from a PharmacoSet, and summarises them into a\n#' drug vs cell line table\n#'\n#' This function creates a table with cell lines as rows and drugs as columns,\n#' summarising the drug senstitivity data of a PharmacoSet into drug-cell line\n#' pairs\n#'\n#' @examples\n#' data(GDSCsmall)\n#' GDSCauc <- summarizeSensitivityProfiles(GDSCsmall,\n#'     sensitivity.measure='auc_published')\n#'\n#' @param object [PharmacoSet] The PharmacoSet from which to extract the data\n#' @param sensitivity.measure [character] The sensitivity measure to use. Use the sensitivityMeasures function to find out what measures are available for each object.\n#' @param cell.lines [character] The cell lines to be summarized. If any cell lines have no data, they will be filled with missing values.\n#' @param profiles_assay [character] The name of the assay in the PharmacoSet object that contains the sensitivity profiles.\n#' @param treatment_col [character] The name of the column in the profiles assay that contains the treatment IDs.\n#' @param sample_col [character] The name of the column in the profiles assay that contains the sample IDs.\n#' @param drugs [character] The drugs to be summarized. If any drugs have no data, they will be filled with missing values.\n#' @param summary.stat [character] The summary method to use if there are repeated cell line-drug experiments. Choices are \"mean\", \"median\", \"first\", \"last\", \"max\", or \"min\".\n#' @param fill.missing Should the missing cell lines not in the molecular data object be filled in with missing values?\n#' @param verbose Should the function print progress messages?\n#'\n#' @return [matrix] A matrix with cell lines going down the rows, drugs across the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @importMethodsFrom CoreGx summarizeSensitivityProfiles\n#' @export\nsetMethod(\"summarizeSensitivityProfiles\", signature(object=\"PharmacoSet\"),\n    function(\n      object, \n      sensitivity.measure=\"auc_recomputed\", \n      cell.lines, \n      profiles_assay = \"profiles\",\n      treatment_col = \"treatmentid\", \n      sample_col = \"sampleid\",\n      drugs, \n      summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n      fill.missing=TRUE, \n      verbose=TRUE\n  ) {\n  if (is(treatmentResponse(object), 'LongTable'))\n    .summarizeSensProfiles(object, sensitivity.measure, profiles_assay = profiles_assay,\n      treatment_col, sample_col, cell.lines, drugs, summary.stat, fill.missing)\n  else\n    .summarizeSensitivityProfilesPharmacoSet(object,\n      sensitivity.measure, cell.lines, drugs, summary.stat,\n      fill.missing, verbose)\n})\n\n#' Summarize the sensitivity profiles when the sensitivity slot is a LongTable\n#'\n#' @return [matrix] A matrix with cell lines going down the rows, drugs across\n#'   the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @import data.table\n#' @keywords internal\n.summarizeSensProfiles <- function(object,\n        sensitivity.measure='auc_recomputed', profiles_assay = \"profiles\", \n        treatment_col = \"treatmentid\", sample_col = \"sampleid\", cell.lines, drugs, summary.stat,\n        fill.missing=TRUE) {\n\n    # handle missing\n    if (missing(cell.lines)) cell.lines <- sampleNames(object)\n    if (missing(drugs)) drugs <- treatmentNames(object)\n    if (missing(summary.stat) || length(summary.stat)>1) summary.stat <- 'mean'\n\n    checkmate::assert_class(treatmentResponse(object), 'LongTable')\n    checkmate::assert_string(sensitivity.measure)\n    checkmate::assert_string(profiles_assay)\n    # get LongTable object\n    longTable <- treatmentResponse(object)\n\n    checkmate::assert((profiles_assay %in% names(longTable)),\n      msg = paste0(\"[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] \",\n        \"The assay '\", profiles_assay, \"' is not in the LongTable object.\"))\n\n    # extract the sensitivty profiles\n    sensProfiles <- assay(longTable, profiles_assay, withDimnames=TRUE, key=FALSE)\n    profileOpts <- setdiff(colnames(sensProfiles), idCols(longTable))\n\n    # compute max concentration and add it to the profiles\n    if (sensitivity.measure == 'max.conc') {\n        dose <- copy(assay(longTable, 'dose', withDimnames=TRUE, key=FALSE))\n        dose[, max.conc := max(.SD, na.rm=TRUE),\n            .SDcols=grep('dose\\\\d+id', colnames(dose))]\n        dose <- dose[, .SD, .SDcols=!grepl('dose\\\\d+id', colnames(dose))]\n        sensProfiles <- dose[sensProfiles, on=idCols(longTable)]\n    }\n\n    # deal with drug combo methods\n    if (sensitivity.measure == 'Synergy_score')\n        drugs <- grep('///', drugs, value=TRUE)\n\n    # ensure selected measure is an option\n    if (!(sensitivity.measure %in% profileOpts))\n        stop(.errorMsg('[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] ',\n            'there is no measure ', sensitivity.measure, ' in this PharmacoSet.',\n            ' Please select one of: ', .collapse(profileOpts)))\n\n    # match summary function\n    ## TODO:: extend this function to support passing in a custom summary function\n    summary.function <- function(x) {\n        if (all(is.na(x))) {\n            return(NA_real_)\n        }\n        switch(summary.stat,\n            \"mean\" = { mean(as.numeric(x), na.rm=TRUE) },\n            \"median\" = { median(as.numeric(x), na.rm=TRUE) },\n            \"first\" = { as.numeric(x)[[1]] },\n            \"last\" = { as.numeric(x)[[length(x)]] },\n            \"max\"= { max(as.numeric(x), na.rm=TRUE) },\n            \"min\" = { min(as.numeric(x), na.rm=TRUE)}\n            )\n    }\n    sensProfiles <- data.table::as.data.table(sensProfiles)\n\n    # do the summary\n    profSummary <- sensProfiles[, summary.function(get(sensitivity.measure)),\n        by=c(treatment_col, sample_col)]\n\n    print(profSummary)\n    \n    # NA pad the missing cells and drugs\n    if (fill.missing) {\n        allCombos <- data.table(expand.grid(drugs, cell.lines))\n        colnames(allCombos) <- c(treatment_col, sample_col)\n        profSummary <- profSummary[allCombos, on=c(treatment_col, sample_col)]\n        print(profSummary)\n    }\n\n    # reshape and convert to matrix\n    setorderv(profSummary, c(sample_col, treatment_col))\n    profSummary <- dcast(profSummary, get(treatment_col) ~ get(sample_col), value.var='V1')\n    summaryMatrix <- as.matrix(profSummary, rownames='treatment_col')\n    return(summaryMatrix)\n\n}\n\n\n\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom stats median\n#' @importFrom reshape2 acast\n#' @keywords internal\n.summarizeSensitivityProfilesPharmacoSet <- function(object,\n                                         sensitivity.measure=\"aac_recomputed\",\n                                         cell.lines,\n                                         drugs,\n                                         summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE) {\n\n\tsummary.stat <- match.arg(summary.stat)\n  #sensitivity.measure <- match.arg(sensitivity.measure)\n  if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(object)), \"max.conc\"))) {\n    stop (sprintf(\"Invalid sensitivity measure for %s, choose among: %s\", annotation(object)$name, paste(colnames(sensitivityProfiles(object)), collapse=\", \")))\n  }\n  if (missing(cell.lines)) {\n    cell.lines <- sampleNames(object)\n  }\n  if (missing(drugs)) {\n    if (sensitivity.measure != \"Synergy_score\")\n    {\n      drugs <- treatmentNames(object)\n    }else{\n      drugs <- sensitivityInfo(object)[grep(\"///\", sensitivityInfo(object)$treatmentid), \"treatmentid\"]\n    }\n  }\n\n  pp <- sensitivityInfo(object)\n  ppRows <- which(pp$sampleid %in% cell.lines & pp$treatmentid %in% drugs) ### NEEDED to deal with duplicated rownames!!!!!!!\n  if(sensitivity.measure != \"max.conc\") {\n    dd <- sensitivityProfiles(object)\n  } else {\n\n    if(!\"max.conc\" %in% colnames(sensitivityInfo(object))) {\n\n      object <- updateMaxConc(object)\n\n    }\n    dd <- sensitivityInfo(object)\n\n  }\n\n  result <- matrix(NA_real_, nrow=length(drugs), ncol=length(cell.lines))\n  rownames(result) <- drugs\n  colnames(result) <- cell.lines\n\n  if(is.factor(dd[, sensitivity.measure]) | is.character(dd[, sensitivity.measure])){\n    warning(\"Sensitivity measure is stored as a factor or character in the pSet. This is incorrect.\\n\n             Please correct this and/or file an issue. Fixing in the call of this function.\")\n    dd[, sensitivity.measure] <- as.numeric(as.character(dd[, sensitivity.measure]))\n  }\n\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\")], \"sensitivity.measure\"=dd[, sensitivity.measure])\n\n\n  summary.function <- function(x) {\n    if(all(is.na(x))){\n      return(NA_real_)\n    }\n    switch(summary.stat,\n        \"mean\" = { mean(as.numeric(x), na.rm=TRUE) },\n        \"median\" = { median(as.numeric(x), na.rm=TRUE) },\n        \"first\" = { as.numeric(x)[[1]] },\n        \"last\" = { as.numeric(x)[[length(x)]] },\n        \"max\"= { max(as.numeric(x), na.rm=TRUE) },\n        \"min\" = { min(as.numeric(x), na.rm=TRUE)}\n        )\n  }\n\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell.lines & pp_dd[,\"treatmentid\"]%in%drugs,]\n\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, fun.aggregate=summary.function, value.var=\"sensitivity.measure\")\n\n  result[rownames(tt), colnames(tt)] <- tt\n\n\tif (!fill.missing) {\n\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n\t}\n  return(result)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeSensitivityProfiles` function in this code?",
        "answer": "The `summarizeSensitivityProfiles` function takes sensitivity data from a PharmacoSet object and summarizes it into a drug vs cell line table. It creates a matrix with cell lines as rows and drugs as columns, containing the selected sensitivity statistic for each drug-cell line pair."
      },
      {
        "question": "How does the function handle missing data or repeated experiments?",
        "answer": "The function handles missing data by filling in missing values for cell lines or drugs not present in the data if the `fill.missing` parameter is set to TRUE. For repeated cell line-drug experiments, it uses a summary statistic specified by the `summary.stat` parameter, which can be 'mean', 'median', 'first', 'last', 'max', or 'min'."
      },
      {
        "question": "What is the difference between `.summarizeSensProfiles` and `.summarizeSensitivityProfilesPharmacoSet` functions?",
        "answer": "The `.summarizeSensProfiles` function is used when the sensitivity slot is a LongTable object, while `.summarizeSensitivityProfilesPharmacoSet` is used for other cases. The former uses data.table operations for efficient processing of large datasets, while the latter uses more traditional R data manipulation techniques."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"summarizeSensitivityProfiles\", signature(object=\"PharmacoSet\"),\n    function(\n      object, \n      sensitivity.measure=\"auc_recomputed\", \n      cell.lines, \n      profiles_assay = \"profiles\",\n      treatment_col = \"treatmentid\", \n      sample_col = \"sampleid\",\n      drugs, \n      summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n      fill.missing=TRUE, \n      verbose=TRUE\n  ) {\n  if (is(treatmentResponse(object), 'LongTable'))\n    # Complete the function call\n  else\n    # Complete the function call\n})",
        "complete": "setMethod(\"summarizeSensitivityProfiles\", signature(object=\"PharmacoSet\"),\n    function(\n      object, \n      sensitivity.measure=\"auc_recomputed\", \n      cell.lines, \n      profiles_assay = \"profiles\",\n      treatment_col = \"treatmentid\", \n      sample_col = \"sampleid\",\n      drugs, \n      summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n      fill.missing=TRUE, \n      verbose=TRUE\n  ) {\n  if (is(treatmentResponse(object), 'LongTable'))\n    .summarizeSensProfiles(object, sensitivity.measure, profiles_assay = profiles_assay,\n      treatment_col, sample_col, cell.lines, drugs, summary.stat, fill.missing)\n  else\n    .summarizeSensitivityProfilesPharmacoSet(object,\n      sensitivity.measure, cell.lines, drugs, summary.stat,\n      fill.missing, verbose)\n})"
      },
      {
        "partial": ".summarizeSensProfiles <- function(object,\n        sensitivity.measure='auc_recomputed', profiles_assay = \"profiles\", \n        treatment_col = \"treatmentid\", sample_col = \"sampleid\", cell.lines, drugs, summary.stat,\n        fill.missing=TRUE) {\n\n    if (missing(cell.lines)) cell.lines <- sampleNames(object)\n    if (missing(drugs)) drugs <- treatmentNames(object)\n    if (missing(summary.stat) || length(summary.stat)>1) summary.stat <- 'mean'\n\n    checkmate::assert_class(treatmentResponse(object), 'LongTable')\n    checkmate::assert_string(sensitivity.measure)\n    checkmate::assert_string(profiles_assay)\n    longTable <- treatmentResponse(object)\n\n    checkmate::assert((profiles_assay %in% names(longTable)),\n      msg = paste0(\"[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] \",\n        \"The assay '\", profiles_assay, \"' is not in the LongTable object.\"))\n\n    sensProfiles <- assay(longTable, profiles_assay, withDimnames=TRUE, key=FALSE)\n    profileOpts <- setdiff(colnames(sensProfiles), idCols(longTable))\n\n    # Complete the function\n}",
        "complete": ".summarizeSensProfiles <- function(object,\n        sensitivity.measure='auc_recomputed', profiles_assay = \"profiles\", \n        treatment_col = \"treatmentid\", sample_col = \"sampleid\", cell.lines, drugs, summary.stat,\n        fill.missing=TRUE) {\n\n    if (missing(cell.lines)) cell.lines <- sampleNames(object)\n    if (missing(drugs)) drugs <- treatmentNames(object)\n    if (missing(summary.stat) || length(summary.stat)>1) summary.stat <- 'mean'\n\n    checkmate::assert_class(treatmentResponse(object), 'LongTable')\n    checkmate::assert_string(sensitivity.measure)\n    checkmate::assert_string(profiles_assay)\n    longTable <- treatmentResponse(object)\n\n    checkmate::assert((profiles_assay %in% names(longTable)),\n      msg = paste0(\"[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] \",\n        \"The assay '\", profiles_assay, \"' is not in the LongTable object.\"))\n\n    sensProfiles <- assay(longTable, profiles_assay, withDimnames=TRUE, key=FALSE)\n    profileOpts <- setdiff(colnames(sensProfiles), idCols(longTable))\n\n    if (!(sensitivity.measure %in% profileOpts))\n        stop(.errorMsg('[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] ',\n            'there is no measure ', sensitivity.measure, ' in this PharmacoSet.',\n            ' Please select one of: ', .collapse(profileOpts)))\n\n    summary.function <- function(x) {\n        if (all(is.na(x))) return(NA_real_)\n        switch(summary.stat,\n            \"mean\" = mean(as.numeric(x), na.rm=TRUE),\n            \"median\" = median(as.numeric(x), na.rm=TRUE),\n            \"first\" = as.numeric(x)[[1]],\n            \"last\" = as.numeric(x)[[length(x)]],\n            \"max\" = max(as.numeric(x), na.rm=TRUE),\n            \"min\" = min(as.numeric(x), na.rm=TRUE))\n    }\n\n    sensProfiles <- data.table::as.data.table(sensProfiles)\n    profSummary <- sensProfiles[, summary.function(get(sensitivity.measure)),\n        by=c(treatment_col, sample_col)]\n\n    if (fill.missing) {\n        allCombos <- data.table(expand.grid(drugs, cell.lines))\n        colnames(allCombos) <- c(treatment_col, sample_col)\n        profSummary <- profSummary[allCombos, on=c(treatment_col, sample_col)]\n    }\n\n    setorderv(profSummary, c(sample_col, treatment_col))\n    profSummary <- dcast(profSummary, get(treatment_col) ~ get(sample_col), value.var='V1')\n    summaryMatrix <- as.matrix(profSummary, rownames='treatment_col')\n    return(summaryMatrix)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/chembl.R",
    "language": "R",
    "content": "#' A general function for creating Queries to the ChEMBL API\n#'\n#' @description A general function for creating Queries to the ChEMBL API\n#' www DOT ebi DOT ac DOT uk/chembl/api/data/ <resource>?<field>__<filter_type>=<value>\n#' |       Resource Name       |                                                                                                   Description                                                                                                   |\n#' |:-------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n#' | activity                  | Activity values recorded in an Assay                                                                                                                                                                            |\n#' | assay                     | Assay details as reported in source Document/Dataset\n#' | atc_class                 | WHO ATC Classification for drugs                                                                                                                                                                                |                                                                                                                                                           |\n#' | binding_site              | WHO ATC Classification for drugs                                                                                                                                                                                |\n#' | biotherapeutic            | Biotherapeutic molecules, which includes HELM notation and sequence data                                                                                                                                        |\n#' | cell_line                 | Cell line information                                                                                                                                                                                           |\n#' | chembl_id_lookup          | Look up ChEMBL Id entity type                                                                                                                                                                                   |\n#' | compound_record           | Occurence of a given compound in a spcecific document                                                                                                                                                           |\n#' | compound_structural_alert | Indicates certain anomaly in compound structure                                                                                                                                                                 |\n#' | document                  | Document/Dataset from which Assays have been extracted                                                                                                                                                          |\n#' | document_similarity       | Provides documents similar to a given one                                                                                                                                                                       |\n#' | document_term             | Provides keywords extracted from a document using the TextRank algorithm                                                                                                                                        |\n#' | drug                      | Approved drugs information, including (but not limited to) applicants, patent numbers and research codes. This endpoint aggregates data on the parent, please use the parent chembl id found in other endpoints |\n#' | drug_indication           | Joins drugs with diseases providing references to relevant sources                                                                                                                                              |\n#' | drug_warning              | Safety information for drugs withdrawn from one or more regions of the world and drugs that carry a warning for severe or life threatening adverse effects                                                      |\n#' | go_slim                   | GO slim ontology                                                                                                                                                                                                |\n#' | image                     | Graphical (svg) representation of Molecule                                                                                                                                                                      |\n#' | mechanism                 | Mechanism of action information for approved drugs                                                                                                                                                              |\n#' | metabolism                | Metabolic pathways with references                                                                                                                                                                              |\n#' | molecule                  | Molecule information, including properties, structural representations and synonyms                                                                                                                             |\n#' | molecule_form             | Relationships between molecule parents and salts                                                                                                                                                                |\n#' | organism                  | Simple organism classification                                                                                                                                                                                  |\n#' | protein_classification    | Protein family classification of TargetComponents                                                                                                                                                               |\n#' | similarity                | Molecule similarity search                                                                                                                                                                                      |\n#' | source                    | Document/Dataset source                                                                                                                                                                                         |\n#' | status                    | API status with ChEMBL DB version number and API software version number                                                                                                                                        |\n#' | substructure              | Molecule substructure search                                                                                                                                                                                    |\n#' | target                    | Targets (protein and non-protein) defined in Assay                                                                                                                                                              |\n#' | target_component          | Target sequence information (A Target may have 1 or more sequences)                                                                                                                                             |\n#' | target_relation           | Describes relations between targets                                                                                                                                                                             |\n#' | tissue                    | Tissue classification                                                                                                                                                                                           |\n#' | xref_source               | Cross references to other resources for compounds                                                                                                                                                               |\n#'\n#'\n#'\n#' |        Filter Type       |                                                  Description                                                 |\n#' |:------------------------:|:------------------------------------------------------------------------------------------------------------:|\n#' | exact (iexact)           | Exact match with query (case insensitive equivalent)                                                         |\n#' | contains (icontains)     | Wild card search with query (case insensitive equivalent)                                                    |\n#' | startswith (istartswith) | Starts with query (case insensitive equivalent)                                                              |\n#' | endswith (iendswith)     | Ends with query (case insensitive equivalent)                                                                |\n#' | regex (iregex)           | Regular expression query (case insensitive equivalent)                                                       |\n#' | gt (gte)                 | Greater than (or equal)                                                                                      |\n#' | lt (lte)                 | Less than (or equal)                                                                                         |\n#' | range                    | Within a range of values                                                                                     |\n#' | in                       | Appears within list of query values                                                                          |\n#' | isnull                   | Field is null                                                                                                |\n#' | search                   | Special type of filter allowing a full text search based on elastic search queries                           |\n#' | only                     | Select specific properties from the original endpoint and returns only the desired properties on each record |                                                                                                         |\n#'\n#' @param resource `character(1)` Resource to query\n#' @param field `character(1)` Field to query\n#' @param filter_type `character(1)` Filter type\n#' @param value `character(1)` Value to query\n#' @param format `character(1)` Format of the response\n#'\n#' @noRd\n#' @keywords internal\n.build_chembl_request <- function(\n    resource,\n    field = NULL, filter_type = NULL, value = NULL, format = \"json\") {\n  # possible formats for now are XML, JSON and YAML\n  checkmate::assert_choice(resource, c(.chembl_resources(), paste0(.chembl_resources(), \"/schema\")))\n  checkmate::assert_choice(field, getChemblResourceFields(resource), null.ok = TRUE)\n  checkmate::assert_choice(filter_type, .chembl_filter_types(), null.ok = TRUE)\n  checkmate::assert_character(value, null.ok = TRUE)\n  checkmate::assert_choice(format, c(\"json\", \"xml\", \"yaml\"))\n\n  # Construct the URL\n  base_url <- .buildURL(\"https://www.ebi.ac.uk/chembl/api/data\", resource)\n  url <- httr2::url_parse(base_url)\n\n  # Add the query parameters\n  query <- list()\n  fld <- paste0(field, \"__\", filter_type)\n  query[[fld]] <- value\n  query[[\"format\"]] <- format\n  url$query <- query\n\n  final_url <- httr2::url_build(url)\n  final_url |> .build_request()\n}\n\n\n#' Query the ChEMBL API\n#'\n#' This function queries the ChEMBL API using the specified parameters and returns the response in JSON format.\n#'\n#' @param resource The resource to query in the ChEMBL API.\n#' @param field The field to filter on in the ChEMBL API.\n#' @param filter_type The type of filter to apply in the ChEMBL API.\n#' @param value The value to filter on in the ChEMBL API.\n#' @param format The format of the response (default is \"json\").\n#'\n#' @return The response from the ChEMBL API in JSON format.\n#'\n#' @examples\n#' queryChemblAPI(\"mechanism\", \"molecule_chembl_id\", \"in\", \"CHEMBL1413\")\n#'\n#' @export\nqueryChemblAPI <- function(resource, field, filter_type, value, format = \"json\") {\n  .build_chembl_request(resource, field, filter_type, value, format) |> \n    .perform_request() |> \n    .parse_resp_json()\n}\n\n\n#' Get ChEMBL Mechanism\n#'\n#' This function retrieves information about the mechanism of action for a given ChEMBL ID.\n#'\n#' @param chembl.ID The ChEMBL ID of the molecule.\n#' @param resources The ChEMBL resource to query (default: \"mechanism\").\n#' @param field The field to filter on (default: \"molecule_chembl_id\").\n#' @param filter_type The filter type to use (default: \"in\").\n#' @param returnURL Logical indicating whether to return the constructed URL (default: FALSE).\n#' @param raw Logical indicating whether to return the raw response JSON (default: FALSE).\n#'\n#' @return A data.table containing the retrieved mechanism information.\n#'\n#' @examples\n#' getChemblMechanism(\"CHEMBL1413\")\n#' getChemblMechanism(\"CHEMBL1413\",\n#'   resources = \"mechanism\", field = \"molecule_chembl_id\",\n#'   filter_type = \"in\", returnURL = FALSE, raw = FALSE\n#' )\n#'\n#' @export\ngetChemblMechanism <- function(\n    chembl.ID, resources = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\",\n    returnURL = FALSE, raw = FALSE) {\n\n  funContext <- .funContext(\"getChemblMechanism\")\n  # constructChemblQuery(resource = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\", value = \"CHEMBL1413\")\n  # urls <- constructChemblQuery(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n  # urls <- URLencode(urls)\n\n  .info(funContext, \"Retrieving ChEMBL Mechanism information for \", length(chembl.ID), \" ChEMBL IDs.\")\n  response_dts <- lapply(chembl.ID, function(chembl.ID) {\n    request <- .build_chembl_request(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n\n    if (returnURL) {\n      return(request$url)\n    }\n    response <- .perform_request(request)\n\n    response_json <- .parse_resp_json(response)\n    if (raw) {\n      return(response_json)\n    }\n    .asDT(response_json[[\"mechanisms\"]])\n  })\n\n  if (returnURL || raw) {\n    return(response_dts)\n  }\n  all_cols <- .chembl_mechanism_cols()\n  # If any cols are missing, fill with NA\n  response_dts <- lapply(response_dts, function(x) {\n    missing_cols <- setdiff(all_cols, names(x))\n    if (length(missing_cols) > 0) {\n      x[, (missing_cols) := NA]\n    }\n    x\n  })\n  \n  data.table::rbindlist(response_dts, fill = TRUE)\n}\n\n\n#' Get the fields of a Chembl resource\n#'\n#' This function retrieves the fields of a Chembl resource.\n#'\n#' @param resource The Chembl resource.\n#' @return A character vector containing the names of the fields.\n#'\n#' @examples\n#' getChemblResourceFields(\"molecule\")\n#'\n#' @export\ngetChemblResourceFields <- function(resource) {\n  checkmate::assert_choice(resource, .chembl_resources())\n  .chembl_resource_schema(resource)[[\"fields\"]] |> names()\n}\n\n#' getChemblResources function\n#'\n#' This function retrieves the Chembl resources.\n#'\n#' @return A list of Chembl resources.\n#'\n#' @examples\n#' getChemblResources()\n#'\n#' @export\ngetChemblResources <- function(){\n  .chembl_resources()\n}\n\n#' Get the Chembl filter types\n#'\n#' This function retrieves the Chembl filter types.\n#'\n#' @return A list of Chembl filter types.\n#'\n#' @examples\n#' getChemblFilterTypes()\n#'\n#' @export\ngetChemblFilterTypes <- function(){\n  .chembl_filter_types()\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_chembl_request` function and what are its main parameters?",
        "answer": "The `.build_chembl_request` function is designed to create queries for the ChEMBL API. Its main parameters are: 'resource' (the API endpoint to query), 'field' (the field to filter on), 'filter_type' (the type of filter to apply), 'value' (the value to filter by), and 'format' (the response format, defaulting to JSON)."
      },
      {
        "question": "How does the `getChemblMechanism` function handle multiple ChEMBL IDs and what does it return?",
        "answer": "The `getChemblMechanism` function can handle multiple ChEMBL IDs by using `lapply` to process each ID. It returns a data.table containing mechanism information for all provided ChEMBL IDs. If any columns are missing for a particular ID, they are filled with NA values to ensure consistent output structure."
      },
      {
        "question": "What is the purpose of the `getChemblResourceFields` function and how does it utilize the `.chembl_resource_schema` function?",
        "answer": "The `getChemblResourceFields` function retrieves the field names for a given ChEMBL resource. It uses the `.chembl_resource_schema` function to get the schema for the specified resource, then extracts and returns the names of the fields from this schema."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getChemblMechanism <- function(chembl.ID, resources = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\", returnURL = FALSE, raw = FALSE) {\n  funContext <- .funContext(\"getChemblMechanism\")\n  .info(funContext, \"Retrieving ChEMBL Mechanism information for \", length(chembl.ID), \" ChEMBL IDs.\")\n  response_dts <- lapply(chembl.ID, function(chembl.ID) {\n    request <- .build_chembl_request(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n\n    if (returnURL) {\n      return(request$url)\n    }\n    response <- .perform_request(request)\n\n    response_json <- .parse_resp_json(response)\n    if (raw) {\n      return(response_json)\n    }\n    .asDT(response_json[[\"mechanisms\"]])\n  })\n\n  if (returnURL || raw) {\n    return(response_dts)\n  }\n  all_cols <- .chembl_mechanism_cols()\n  # Complete the function here\n}",
        "complete": "getChemblMechanism <- function(chembl.ID, resources = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\", returnURL = FALSE, raw = FALSE) {\n  funContext <- .funContext(\"getChemblMechanism\")\n  .info(funContext, \"Retrieving ChEMBL Mechanism information for \", length(chembl.ID), \" ChEMBL IDs.\")\n  response_dts <- lapply(chembl.ID, function(chembl.ID) {\n    request <- .build_chembl_request(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n\n    if (returnURL) {\n      return(request$url)\n    }\n    response <- .perform_request(request)\n\n    response_json <- .parse_resp_json(response)\n    if (raw) {\n      return(response_json)\n    }\n    .asDT(response_json[[\"mechanisms\"]])\n  })\n\n  if (returnURL || raw) {\n    return(response_dts)\n  }\n  all_cols <- .chembl_mechanism_cols()\n  response_dts <- lapply(response_dts, function(x) {\n    missing_cols <- setdiff(all_cols, names(x))\n    if (length(missing_cols) > 0) {\n      x[, (missing_cols) := NA]\n    }\n    x\n  })\n  \n  data.table::rbindlist(response_dts, fill = TRUE)\n}"
      },
      {
        "partial": "queryChemblAPI <- function(resource, field, filter_type, value, format = \"json\") {\n  # Complete the function here\n}",
        "complete": "queryChemblAPI <- function(resource, field, filter_type, value, format = \"json\") {\n  .build_chembl_request(resource, field, filter_type, value, format) |> \n    .perform_request() |> \n    .parse_resp_json()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/zzz.R",
    "language": "R",
    "content": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE) # setting default option for using c code across package\n\n}\n# Package Start-up Functions\n\n.onAttach <- function(libname, pkgname) {\n\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\nPharmacoGx package brought to you by:\n\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\nFor more of our work visit bhklab.ca!\n\nLike PharmacoGx? Check out our companion web-app at PharmacoDB.ca.\n        \"\n        )\n        # Prevent repeated messages when loading multiple lab packages\n        options(bhklab.startup_=FALSE)\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.onLoad` function in this R package?",
        "answer": "The `.onLoad` function is used to set default options when the package is loaded. In this case, it sets the 'PharmacoGx_useC' option to TRUE, which likely enables the use of C code across the package for improved performance."
      },
      {
        "question": "How does the `.onAttach` function control the display of the startup message?",
        "answer": "The `.onAttach` function checks if the session is interactive and if the 'bhklab.startup_' option is null. If both conditions are met, it displays the startup message. It also sets 'bhklab.startup_' to FALSE after displaying the message, preventing repeated messages when loading multiple lab packages."
      },
      {
        "question": "What technique is used in the `.onAttach` function to temporarily suppress warnings?",
        "answer": "The function uses `options(warn=-1)` to temporarily suppress warnings. It stores the old options in `oldOpts` and uses `on.exit(options(oldOpts))` to ensure that the original warning settings are restored when the function exits, regardless of how it exits."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE)\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    oldOpts <- options()\n    options(warn=-1)\n    on.exit(options(oldOpts))\n\n    packageStartupMessage(\n      # Message content here\n    )\n\n    # Complete the code\n  }\n}",
        "complete": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE)\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    oldOpts <- options()\n    options(warn=-1)\n    on.exit(options(oldOpts))\n\n    packageStartupMessage(\n      # Message content here\n    )\n\n    options(bhklab.startup_=FALSE)\n  }\n}"
      },
      {
        "partial": ".onLoad <- function(libname, pkgname) {\n  # Set default option\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    # Set options and display message\n  }\n}",
        "complete": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE)\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    oldOpts <- options()\n    options(warn=-1)\n    on.exit(options(oldOpts))\n    packageStartupMessage(\"PharmacoGx package loaded. Visit bhklab.ca for more.\")\n    options(bhklab.startup_=FALSE)\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/gCSI/gCSI_treatmentMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))\n\nusethis::use_data(gCSI_treatmentMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in the first line of the code snippet?",
        "answer": "The `system.file()` function is used to locate and construct the file path for a specific file within an R package. In this case, it's locating the file 'gCSI_GRmetrics_v1.3.tsv' in the 'extdata/gCSI' directory of the 'AnnotationGx' package. This function is particularly useful for accessing files that are distributed with a package."
      },
      {
        "question": "How does the code ensure unique entries in the `gCSI_treatmentMetadata` data frame?",
        "answer": "The code uses the `unique()` function on a subset of the `rawdata` data frame. Specifically, it selects the columns 'DrugName' and 'Norm_DrugName' using `rawdata[,c('DrugName', 'Norm_DrugName')]`, and then applies `unique()` to this selection. This operation removes any duplicate rows, ensuring that each combination of DrugName and Norm_DrugName appears only once in the resulting `gCSI_treatmentMetadata` data frame."
      },
      {
        "question": "What is the purpose of the `usethis::use_data()` function call at the end of the snippet?",
        "answer": "The `usethis::use_data()` function is used to save R objects (in this case, `gCSI_treatmentMetadata`) to an .rda file in the 'data/' directory of the current package. The `overwrite = TRUE` argument allows the function to overwrite an existing file with the same name if it exists. This is typically used in package development to include data with the package, making it easily accessible to users of the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))\n\n# Complete the code to save the gCSI_treatmentMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))\n\nusethis::use_data(gCSI_treatmentMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the file, extract unique drug metadata, and rename columns\n\nfilePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\n# Add code here",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_intersectPSet.R",
    "language": "R",
    "content": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\n# ###TO-DO:: This test takes forever to run; consider making the intersections smaller\n# test_that(\"Intersection result did not change since last time\", {\n# \tdata(GDSCsmall)\n# \tdata(CCLEsmall)\n# \tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"))\n# \texpect_equal_to_reference(common, \"intersectedSmallData.rds\", tolerance=1e-3)\n# \texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n# \texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n# \texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n# })\n#\n# test_that(\"Strict Intersection result did not change since last time\", {\n# \tdata(GDSCsmall)\n# \tdata(CCLEsmall)\n# \tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"), strictIntersect=TRUE)\n# \texpect_equal_to_reference(common, \"intersectedSmallDataStrict.rds\", tolerance=1e-3)\n# \texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n# \texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n# \texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n# \texpect_equal(rownames(sensitivityProfiles(common$GDSC)), rownames(common$sensitivityProfiles(CCLE)))\n# })\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'intersectPSet' function in this code snippet?",
        "answer": "The 'intersectPSet' function is used to find the intersection between two PharmacoSet objects (GDSCsmall and CCLEsmall) based on specified criteria such as drugs, cell lines, and concentrations. It allows for comparing and combining data from different pharmacogenomic datasets."
      },
      {
        "question": "Why are the test cases commented out in this code snippet?",
        "answer": "The test cases are commented out because, as mentioned in the TODO comment, they take a long time to run. The comment suggests considering making the intersections smaller to improve performance. This is likely a temporary measure during development to avoid long execution times when running tests frequently."
      },
      {
        "question": "What is the difference between the two commented-out test cases in terms of the 'intersectPSet' function call?",
        "answer": "The main difference is in the 'strictIntersect' parameter. The first test case uses the default value (FALSE), while the second test case explicitly sets 'strictIntersect=TRUE'. This likely affects how the intersection is performed, with the strict version potentially requiring exact matches across all specified criteria."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"))\n\texpect_equal_to_reference(common, \"intersectedSmallData.rds\", tolerance=1e-3)\n\t# Add more expectations here\n})",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"))\n\texpect_equal_to_reference(common, \"intersectedSmallData.rds\", tolerance=1e-3)\n\texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n\texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n\texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n})"
      },
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Strict Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"), strictIntersect=TRUE)\n\texpect_equal_to_reference(common, \"intersectedSmallDataStrict.rds\", tolerance=1e-3)\n\t# Add more expectations here\n})",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Strict Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"), strictIntersect=TRUE)\n\texpect_equal_to_reference(common, \"intersectedSmallDataStrict.rds\", tolerance=1e-3)\n\texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n\texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n\texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n\texpect_equal(rownames(sensitivityProfiles(common$GDSC)), rownames(common$sensitivityProfiles(CCLE)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeDSS.R",
    "language": "R",
    "content": "##TODO:: Add function documentation\ncomputeDSS <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       t_param = 10,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc = TRUE,\n                       verbose = TRUE,\n                       dss_type = 3,\n                       censor = FALSE\n                       #, ...\n) {\n  \n  if(missing(concentration)){\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(Hill_fit)) {\n    \n    Hill_fit <- logLogisticRegression(concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, \n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n    \n  } else {\n    \n    cleanData <- sanitizeInput(conc = concentration, \n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose) #is this coercing the concentration to log?\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n    \n  }\n  \n  if(pars[[3]] > max(concentration)) {\n    return(0)\n  }\n  \n  if(!viability_as_pct){\n    t_param = t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 = max(concentration)\n  x1 = computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)){return(0)}\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor) {\n    if (pars[[2]] > 50) {\n      return(NA)\n    } else if (all(concentration < pars[[3]])) {\n      return(0)\n    }\n  }\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) {\n    return(DSS)\n  }\n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) {\n    return(DSS)\n  }\n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) {\n    return(DSS)\n  } else {\n    stop(\"Invalid DSS type entered.\")\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeDSS` function and what are its main input parameters?",
        "answer": "The `computeDSS` function is designed to compute the Drug Sensitivity Score (DSS) based on concentration-response data. Its main input parameters are:\n1. `concentration`: The concentration values to integrate over (required)\n2. `viability`: The corresponding viability values (optional if Hill_fit is provided)\n3. `Hill_fit`: A pre-computed Hill equation fit (optional)\n4. `t_param`: A threshold parameter (default: 10)\n5. `conc_as_log`: Boolean indicating if concentration is in log scale (default: FALSE)\n6. `viability_as_pct`: Boolean indicating if viability is in percentage (default: TRUE)\n7. `dss_type`: An integer (1, 2, or 3) specifying the type of DSS calculation (default: 3)"
      },
      {
        "question": "How does the function handle missing or incomplete input data?",
        "answer": "The function handles missing or incomplete input data in several ways:\n1. If `concentration` is missing, it stops execution with an error message.\n2. If `Hill_fit` is missing, it computes the Hill fit using the `logLogisticRegression` function with the provided concentration and viability data.\n3. It uses the `sanitizeInput` function to clean and validate the input data, ensuring consistency in data format and scale.\n4. If the computed IC50 (pars[[3]]) is greater than the maximum concentration, it returns 0.\n5. If `censor` is TRUE, it returns NA if the maximum effect (pars[[2]]) is greater than 50%, and returns 0 if all concentrations are less than the IC50."
      },
      {
        "question": "Explain the different types of DSS calculations (dss_type 1, 2, and 3) implemented in this function.",
        "answer": "The function implements three types of DSS calculations based on the `dss_type` parameter:\n1. DSS Type 1 (dss_type = 1):\n   - Basic DSS calculation: (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n2. DSS Type 2 (dss_type = 2):\n   - DSS Type 1 result divided by log(100 - pars[[2]]), where pars[[2]] is the minimum viability\n3. DSS Type 3 (dss_type = 3):\n   - DSS Type 2 result multiplied by (x2 - x1) / (max(concentration) - min(concentration))\n\nEach type progressively adjusts the DSS calculation to account for different aspects of the dose-response curve. Type 3 is the default and most comprehensive calculation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)){\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  } else {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  }\n  \n  if(pars[[3]] > max(concentration)) {\n    return(0)\n  }\n  \n  if(!viability_as_pct){\n    t_param = t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 = max(concentration)\n  x1 = computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)){return(0)}\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor) {\n    if (pars[[2]] > 50) {\n      return(NA)\n    } else if (all(concentration < pars[[3]])) {\n      return(0)\n    }\n  }\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) {\n    return(DSS)\n  }\n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) {\n    return(DSS)\n  }\n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) {\n    return(DSS)\n  } else {\n    stop(\"Invalid DSS type entered.\")\n  }\n}",
        "complete": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)) stop(\"The concentration values to integrate over must always be provided.\")\n  \n  if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  } else {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  }\n  \n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n  \n  if(pars[[3]] > max(concentration)) return(0)\n  \n  if(!viability_as_pct) {\n    t_param <- t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 <- max(concentration)\n  x1 <- computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)) return(0)\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor && (pars[[2]] > 50 || all(concentration < pars[[3]]))) return(if(pars[[2]] > 50) NA else 0)\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) return(DSS)\n  \n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) return(DSS)\n  \n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) return(DSS)\n  \n  stop(\"Invalid DSS type entered.\")\n}"
      },
      {
        "partial": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)) stop(\"The concentration values to integrate over must always be provided.\")\n  \n  cleanData <- if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  } else {\n    sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  }\n  \n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n  \n  if(pars[[3]] > max(concentration)) return(0)\n  \n  if(!viability_as_pct) {\n    t_param <- t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 <- max(concentration)\n  x1 <- computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)) return(0)\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor && (pars[[2]] > 50 || all(concentration < pars[[3]]))) return(if(pars[[2]] > 50) NA else 0)\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) return(DSS)\n  \n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) return(DSS)\n  \n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) return(DSS)\n  \n  stop(\"Invalid DSS type entered.\")\n}",
        "complete": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)) stop(\"The concentration values to integrate over must always be provided.\")\n  \n  cleanData <- if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  } else {\n    sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  }\n  \n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n  \n  if(pars[[3]] > max(concentration) || (censor && (pars[[2]] > 50 || all(concentration < pars[[3]]))))\n    return(if(censor && pars[[2]] > 50) NA else 0)\n  \n  if(!viability_as_pct) {\n    t_param <- t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 <- max(concentration)\n  x1 <- max(computeICn(concentration, unlist(pars), t_param, conc_as_log = TRUE, viability_as_pct = TRUE), min(concentration))\n  if(!is.finite(x1)) return(0)\n  \n  AUC <- computeAUC(c(x1, x2), unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) return(DSS)\n  \n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) return(DSS)\n  \n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) return(DSS)\n  \n  stop(\"Invalid DSS type entered.\")\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeICn.R",
    "language": "R",
    "content": "#' Computes the ICn for any n in 0-100 for a Drug Dose Viability Curve\n#' \n#' Returns the ICn for any given nth percentile when given concentration and viability as input, normalized by the concentration\n#' range of the experiment. A Hill Slope is first fit to the data, and the ICn is inferred from the fitted curve. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known. \n#' \n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8) \n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeIC50(dose, viability)\n#' computeICn(dose, viability, n=10)\n#' \n#' @param concentration `numeric` is a vector of drug concentrations.\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells. \n#' @param Hill_fit `list` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope \n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal, \n#' and EC50 as a concentration. \n#' @param n `numeric` The percentile concentration to compute. If viability_as_pct set, assumed to be percentage, otherwise\n#' assumed to be a decimal value.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(ICn) should be returned instead of ICn.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf passed in as decimal.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return a numeric value for the concentration of the nth precentile viability reduction \n#' @export\ncomputeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE, \n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n\n    Hill_fit <- logLogisticRegression(conc = concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  } else if (!missing(Hill_fit)){\n\n    cleanData <- sanitizeInput(conc = concentration, \n      viability = viability,\n      Hill_fit = Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n  } else {\n\n    stop(\"Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.\")\n\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n\n  \n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) {\n    return(NA_real_)\n  } else if (n == pars[2]) {\n\n    return(Inf)\n\n  } else if (n == 1) {\n\n    return(ifelse(conc_as_log, -Inf, 0))\n\n  } else {\n\n    return(ifelse(conc_as_log,\n      log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n      10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n\n  }\n\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeICn` function and what are its main input parameters?",
        "answer": "The `computeICn` function computes the ICn (Inhibitory Concentration) for any nth percentile of a Drug Dose Viability Curve. It takes the following main input parameters: `concentration` (a vector of drug concentrations), `viability` (a vector of corresponding viability values), `Hill_fit` (optional, parameters of a Hill Slope), `n` (the percentile concentration to compute), and several boolean flags for input format and processing options."
      },
      {
        "question": "How does the function handle different input formats for concentration and viability data?",
        "answer": "The function uses boolean flags to handle different input formats: `conc_as_log` determines if the concentration data is given as log10 values, `viability_as_pct` specifies if viability is given as a percentage or decimal, and `trunc` indicates whether to truncate viability data between 0 and 1. These flags allow the function to adapt to various input formats and ensure correct processing of the data."
      },
      {
        "question": "What is the significance of the `Hill_fit` parameter, and how does the function behave if it's missing?",
        "answer": "The `Hill_fit` parameter represents the parameters of a Hill Slope curve (Hill Slope, E_inf, EC50). If `Hill_fit` is missing and both `concentration` and `viability` are provided, the function calls `logLogisticRegression` to compute the Hill Slope parameters. If `Hill_fit` is provided, it uses these parameters directly. This allows flexibility in using pre-computed Hill Slope parameters or calculating them from raw data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)){\n    cleanData <- sanitizeInput(conc = concentration, \n      viability = viability,\n      Hill_fit = Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  # Complete the function by adding the logic to calculate and return the ICn value\n  \n}",
        "complete": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)){\n    cleanData <- sanitizeInput(conc = concentration, \n      viability = viability,\n      Hill_fit = Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) {\n    return(NA_real_)\n  } else if (n == pars[2]) {\n    return(Inf)\n  } else if (n == 1) {\n    return(ifelse(conc_as_log, -Inf, 0))\n  } else {\n    return(ifelse(conc_as_log,\n      log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n      10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n  }\n}"
      },
      {
        "partial": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  # Add input validation and data preparation\n  \n  # Calculate n value\n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  # Implement the ICn calculation logic\n  \n}",
        "complete": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, Hill_fit=Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)){\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) return(NA_real_)\n  if (n == pars[2]) return(Inf)\n  if (n == 1) return(ifelse(conc_as_log, -Inf, 0))\n  \n  return(ifelse(conc_as_log,\n    log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n    10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-intersect.R",
    "language": "R",
    "content": "# ==== PharmacoSet Class\n\n## TODO:: Can we implement intersect method for PSets?\n\n# ==== LongTable Class\n\n## TODO:: Implement intersection of LongTable objects",
    "qa_pairs": [
      {
        "question": "What are the main classes mentioned in this code snippet, and what is the purpose of the TODO comments associated with them?",
        "answer": "The main classes mentioned in this code snippet are PharmacoSet and LongTable. The TODO comments suggest implementing an 'intersect' method for PSets (likely an abbreviation for PharmacoSet) and an 'intersection' method for LongTable objects. These comments indicate that the developers are planning to add functionality to find common elements or data between instances of these classes."
      },
      {
        "question": "What is the difference between 'intersect' and 'intersection' as mentioned in the TODO comments?",
        "answer": "While 'intersect' and 'intersection' are semantically similar, their usage in the TODO comments suggests a subtle difference in implementation or naming convention between the two classes. For the PharmacoSet class, the comment uses 'intersect' as a method name, while for the LongTable class, it uses 'intersection'. This could indicate either a slight difference in functionality or simply a variation in naming convention between the two classes. In both cases, the intended purpose is likely to find common elements or data between instances of the respective classes."
      },
      {
        "question": "Based on the class names and TODO comments, what kind of application or domain might this code be part of?",
        "answer": "The class names and TODO comments suggest that this code is likely part of a bioinformatics or pharmaceutical research application. The 'PharmacoSet' class name implies a collection or set of pharmacological data, while 'LongTable' suggests a data structure for handling large datasets, which are common in scientific research. The planned intersection functionality could be useful for comparing different drug sets or experimental results, which is a common task in pharmaceutical research and data analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class PharmacoSet:\n    def __init__(self):\n        self.data = []\n    \n    def add(self, item):\n        self.data.append(item)\n    \n    def intersect(self, other):\n        # TODO: Implement intersection of PharmacoSet objects\n        pass",
        "complete": "class PharmacoSet:\n    def __init__(self):\n        self.data = []\n    \n    def add(self, item):\n        self.data.append(item)\n    \n    def intersect(self, other):\n        return PharmacoSet([item for item in self.data if item in other.data])"
      },
      {
        "partial": "class LongTable:\n    def __init__(self):\n        self.data = {}\n    \n    def add(self, key, value):\n        self.data[key] = value\n    \n    def intersection(self, other):\n        # TODO: Implement intersection of LongTable objects\n        pass",
        "complete": "class LongTable:\n    def __init__(self):\n        self.data = {}\n    \n    def add(self, key, value):\n        self.data[key] = value\n    \n    def intersection(self, other):\n        return LongTable({k: v for k, v in self.data.items() if k in other.data and v == other.data[k]})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/pet.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport warnings\nimport datetime\nfrom typing import Optional, Dict, TypeVar\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport SimpleITK as sitk\nfrom pydicom import dcmread\n\nT = TypeVar('T')\n\n\ndef read_image(path:str,series_id: Optional[str]=None):\n    reader = sitk.ImageSeriesReader()\n    dicom_names = reader.GetGDCMSeriesFileNames(path,seriesID=series_id if series_id else \"\")\n    reader.SetFileNames(dicom_names)\n    reader.MetaDataDictionaryArrayUpdateOn()\n    reader.LoadPrivateTagsOn()\n\n    return reader.Execute()\n\n\nclass PET(sitk.Image):\n    def __init__(self, img_pet, df, factor, calc, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_pet)\n        self.img_pet = img_pet\n        self.df = df\n        self.factor = factor\n        self.calc = calc\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n    \n    @classmethod\n    def from_dicom_pet(cls, path,series_id=None,type=\"SUV\"):\n        '''\n        Reads the PET scan and returns the data frame and the image dosage in SITK format\n        There are two types of existing formats which has to be mentioned in the type\n        type:\n            SUV: gets the image with each pixel value having SUV value\n            ACT: gets the image with each pixel value having activity concentration\n        SUV = Activity concenteration/(Injected dose quantity/Body weight)\n\n        Please refer to the pseudocode: https://qibawiki.rsna.org/index.php/Standardized_Uptake_Value_(SUV) \n        If there is no data on SUV/ACT then backup calculation is done based on the formula in the documentation, although, it may\n        have some error.\n        '''\n        pet      = read_image(path,series_id)\n        path_one = pathlib.Path(path,os.listdir(path)[0]).as_posix()\n        df       = dcmread(path_one)\n        calc     = False\n        try:\n            if type==\"SUV\":\n                factor = df.to_json_dict()['70531000'][\"Value\"][0]\n            else:\n                factor = df.to_json_dict()['70531009']['Value'][0]\n        except:\n            warnings.warn(\"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\")\n            factor = cls.calc_factor(df,type)\n            calc = True\n        img_pet = sitk.Cast(pet, sitk.sitkFloat32)\n\n        # SimpleITK reads some pixel values as negative but with correct value\n        img_pet = sitk.Abs(img_pet * factor)\n\n        metadata = {}\n        return cls(img_pet, df, factor, calc, metadata)\n        # return cls(img_pet, df, factor, calc)\n        \n    def get_metadata(self):\n        '''\n        Forms the important metadata for reference in the dictionary format\n        {\n            scan_time (in seconds): AcquisitionTime \n            injection_time (in seconds): RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime\n            weight (in kg): PatientWeight\n            half_life (in seconds): RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife\n            injected_dose: RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose\n            Values_Assumed: True when some values are not available and are assumed for the calculation of SUV factor\n            factor: factor used for rescaling to bring it to SUV or ACT\n        }\n        '''\n        self.metadata = {}\n        try:\n            self.metadata[\"weight\"] = float(self.df.PatientWeight)\n        except:\n            pass\n        try:\n            self.metadata[\"scan_time\"] = datetime.datetime.strptime(self.df.AcquisitionTime, '%H%M%S.%f')\n            self.metadata[\"injection_time\"] = datetime.datetime.strptime(self.df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n            self.metadata[\"half_life\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n            self.metadata[\"injected_dose\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n        except:\n            pass\n        self.metadata[\"factor\"] = self.factor\n        self.metadata[\"Values_Assumed\"] = self.calc\n        return self.metadata\n\n    def resample_pet(self,\n                     ct_scan: sitk.Image) -> sitk.Image:\n        '''\n        Resamples the PET scan so that it can be overlayed with CT scan. The beginning and end slices of the \n        resampled PET scan might be empty due to the interpolation\n        '''\n        resampled_pt = sitk.Resample(self.img_pet, ct_scan)  # , interpolator=sitk.sitkNearestNeighbor) # commented interporator due to error\n        return resampled_pt\n\n    def show_overlay(self,\n                     ct_scan: sitk.Image,\n                     slice_number: int) -> plt.figure:\n        '''\n        For a given slice number, the function resamples PET scan and overlays on top of the CT scan and returns the figure of the\n        overlay\n        '''\n        resampled_pt = self.resample_pet(ct_scan)\n        fig = plt.figure(\"Overlayed image\", figsize=[15, 10])\n        pt_arr = sitk.GetArrayFromImage(resampled_pt)\n        plt.subplot(1,3,1)\n        plt.imshow(pt_arr[slice_number,:,:])\n        plt.subplot(1,3,2)\n        ct_arr = sitk.GetArrayFromImage(ct_scan)\n        plt.imshow(ct_arr[slice_number,:,:])\n        plt.subplot(1,3,3)\n        plt.imshow(ct_arr[slice_number,:,:], cmap=plt.cm.gray)\n        plt.imshow(pt_arr[slice_number,:,:], cmap=plt.cm.hot, alpha=.4)\n        return fig\n\n    @staticmethod\n    def calc_factor(df, type: str):\n        '''\n        Following the calculation formula stated in https://gist.github.com/pangyuteng/c6a075ba9aa00bb750468c30f13fc603\n        '''\n        # Fetching some required Meta Data\n        try:\n            weight = float(df.PatientWeight) * 1000\n        except:\n            warnings.warn(\"Patient Weight Not Present. Taking 75Kg\")\n            weight = 75000\n        try:\n            scan_time = datetime.datetime.strptime(df.AcquisitionTime, '%H%M%S.%f')\n            injection_time = datetime.datetime.strptime(df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n            half_life = float(df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n            injected_dose = float(df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n\n            # Calculate activity concenteration factor\n            a = np.exp(-np.log(2) * ((scan_time - injection_time).seconds / half_life))\n\n            # Calculate SUV factor\n            injected_dose_decay = a * injected_dose\n        except:\n            warnings.warn(\"Not enough data available, taking average values\")\n            a = np.exp(-np.log(2) * (1.75 * 3600) / 6588)  # 90 min waiting time, 15 min preparation\n            injected_dose_decay = 420000000 * a  # 420 MBq\n\n        suv = weight/injected_dose_decay\n        if type == \"SUV\":\n            return suv\n        else:\n            return 1/a\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read_image` function in the given code snippet, and how does it handle DICOM series?",
        "answer": "The `read_image` function is designed to read a DICOM image series from a specified path. It uses SimpleITK's ImageSeriesReader to load DICOM files. The function performs the following steps:\n1. Creates an ImageSeriesReader object.\n2. Gets the file names of the DICOM series using GetGDCMSeriesFileNames.\n3. Sets the file names for the reader.\n4. Enables metadata dictionary array updates and loading of private tags.\n5. Executes the reader to load the image series.\n\nThe function can optionally filter by a specific series ID if provided. This allows for reading specific DICOM series from a directory containing multiple series."
      },
      {
        "question": "Explain the `PET` class's `from_dicom_pet` method. What are its parameters, and how does it handle different PET image types?",
        "answer": "The `from_dicom_pet` class method is a factory method for creating a `PET` object from DICOM files. Its parameters are:\n- `path`: The directory containing the DICOM files.\n- `series_id` (optional): To specify a particular series.\n- `type`: Either 'SUV' or 'ACT' to determine the image format.\n\nThe method handles two types of PET images:\n1. SUV (Standardized Uptake Value): Pixel values represent SUV.\n2. ACT (Activity Concentration): Pixel values represent activity concentration.\n\nIt reads the image using `read_image`, extracts metadata from the first DICOM file, and attempts to get the scaling factor from the DICOM tags. If the scaling factor is not available, it calculates it using the `calc_factor` method. The method then scales the image values, converts them to absolute values, and returns a new `PET` object with the processed image and metadata."
      },
      {
        "question": "How does the `resample_pet` method work, and why is it necessary when overlaying PET images on CT scans?",
        "answer": "The `resample_pet` method in the `PET` class is used to resample the PET image to match the dimensions and spacing of a CT scan. Here's how it works:\n\n1. It takes a CT scan (as a SimpleITK Image) as input.\n2. It uses SimpleITK's `Resample` function to resample the PET image (`self.img_pet`) to match the CT scan's properties.\n3. The resampled PET image is returned.\n\nThis method is necessary when overlaying PET images on CT scans because:\n1. PET and CT scans often have different resolutions and slice thicknesses.\n2. To accurately overlay the images, they need to have the same dimensions and spatial alignment.\n3. Resampling ensures that each voxel in the PET image corresponds to the same anatomical location as in the CT image.\n\nBy resampling the PET image to match the CT scan, it allows for proper fusion and visualization of the two imaging modalities, which is crucial for accurate interpretation in medical imaging."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class PET(sitk.Image):\n    def __init__(self, img_pet, df, factor, calc, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_pet)\n        self.img_pet = img_pet\n        self.df = df\n        self.factor = factor\n        self.calc = calc\n        self.metadata = metadata or {}\n    \n    @classmethod\n    def from_dicom_pet(cls, path, series_id=None, type=\"SUV\"):\n        pet = read_image(path, series_id)\n        path_one = pathlib.Path(path, os.listdir(path)[0]).as_posix()\n        df = dcmread(path_one)\n        calc = False\n        try:\n            factor = df.to_json_dict()['70531000' if type == \"SUV\" else '70531009']['Value'][0]\n        except:\n            warnings.warn(\"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\")\n            factor = cls.calc_factor(df, type)\n            calc = True\n        img_pet = sitk.Cast(pet, sitk.sitkFloat32)\n        img_pet = sitk.Abs(img_pet * factor)\n        # Complete the method",
        "complete": "class PET(sitk.Image):\n    def __init__(self, img_pet, df, factor, calc, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_pet)\n        self.img_pet = img_pet\n        self.df = df\n        self.factor = factor\n        self.calc = calc\n        self.metadata = metadata or {}\n    \n    @classmethod\n    def from_dicom_pet(cls, path, series_id=None, type=\"SUV\"):\n        pet = read_image(path, series_id)\n        path_one = pathlib.Path(path, os.listdir(path)[0]).as_posix()\n        df = dcmread(path_one)\n        calc = False\n        try:\n            factor = df.to_json_dict()['70531000' if type == \"SUV\" else '70531009']['Value'][0]\n        except:\n            warnings.warn(\"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\")\n            factor = cls.calc_factor(df, type)\n            calc = True\n        img_pet = sitk.Cast(pet, sitk.sitkFloat32)\n        img_pet = sitk.Abs(img_pet * factor)\n        return cls(img_pet, df, factor, calc)"
      },
      {
        "partial": "def get_metadata(self):\n    self.metadata = {}\n    try:\n        self.metadata[\"weight\"] = float(self.df.PatientWeight)\n    except:\n        pass\n    try:\n        self.metadata[\"scan_time\"] = datetime.datetime.strptime(self.df.AcquisitionTime, '%H%M%S.%f')\n        self.metadata[\"injection_time\"] = datetime.datetime.strptime(self.df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n        self.metadata[\"half_life\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n        self.metadata[\"injected_dose\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n    except:\n        pass\n    # Complete the method",
        "complete": "def get_metadata(self):\n    self.metadata = {}\n    try:\n        self.metadata[\"weight\"] = float(self.df.PatientWeight)\n    except:\n        pass\n    try:\n        self.metadata[\"scan_time\"] = datetime.datetime.strptime(self.df.AcquisitionTime, '%H%M%S.%f')\n        self.metadata[\"injection_time\"] = datetime.datetime.strptime(self.df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n        self.metadata[\"half_life\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n        self.metadata[\"injected_dose\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n    except:\n        pass\n    self.metadata[\"factor\"] = self.factor\n    self.metadata[\"Values_Assumed\"] = self.calc\n    return self.metadata"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "warnings",
        "datetime",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Optional",
        "matplotlib.pyplot",
        "pydicom.dcmread"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_pipeline.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport shutil\nimport warnings\nfrom multiprocessing import cpu_count\n\nimport numpy as np\nimport SimpleITK as sitk\nimport pytest\n\nfrom imgtools.io import ImageFileLoader, ImageFileWriter\nfrom imgtools.ops import BaseInput as Input\nfrom imgtools.ops import BaseOutput as Output\nfrom imgtools.pipeline import Pipeline\n\n\n@pytest.fixture\ndef sample_input_output(tmp_path):\n    input_paths = []\n    output_paths = []\n    for i in range(2):\n        input_path = tmp_path / f\"input_{i}\"\n        output_path = tmp_path / f\"output_{i}\"\n        input_path.mkdir(exist_ok=True)\n        output_path.mkdir(exist_ok=True)\n        input_paths.append(input_path)\n        output_paths.append(output_path)\n\n        # generate some test data\n        test_inputs = [sitk.GetImageFromArray(np.random.random((10, 10, 2))) for _ in range(4)]\n        for j, img in enumerate(test_inputs):\n            path = input_path / f\"test{j}.nrrd\"\n            sitk.WriteImage(img, str(path))\n\n    yield input_paths, output_paths\n    # clean up\n    shutil.rmtree(tmp_path)\n\n\nclass SimplePipelineTest(Pipeline):\n    def __init__(self, input_path, output_path, n_jobs):\n        super().__init__(n_jobs=n_jobs, show_progress=False)\n        self.input_path = input_path\n        self.output_path = output_path\n        self.image_input = Input(\n            ImageFileLoader(self.input_path))\n        self.image_output = Output(\n            ImageFileWriter(self.output_path))\n\n    def process_one_subject(self, subject_id):\n        image = self.image_input(subject_id)\n        self.image_output(subject_id, image)\n\n@pytest.mark.parametrize(\"n_jobs\", [1, 2])\ndef test_output(n_jobs, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    pipeline = SimplePipelineTest(input_paths[0], output_paths[0], n_jobs)\n    pipeline.run()\n\n    input_dir, output_dir = input_paths[0], output_paths[0]\n\n    for input_file, output_file in zip(sorted(os.listdir(input_dir)), sorted(os.listdir(output_dir))):\n        assert os.path.exists(pathlib.Path(output_dir, output_file))\n        test_output = sitk.GetArrayFromImage(sitk.ReadImage(pathlib.Path(output_dir, output_file).as_posix()))\n        true_output = sitk.GetArrayFromImage(sitk.ReadImage(pathlib.Path(input_dir, input_file).as_posix()))\n        assert np.allclose(test_output, true_output)\n        print('passed assert')\n\n\nclass MultiInputPipelineTest(Pipeline):\n    def __init__(self, input_path_0, input_path_1, output_path_0, output_path_1, n_jobs, missing_strategy):\n        super().__init__(n_jobs=n_jobs, missing_strategy=missing_strategy, show_progress=False)\n        self.input_path_0 = input_path_0\n        self.input_path_1 = input_path_1\n        self.output_path_0 = output_path_0\n        self.output_path_1 = output_path_1\n        self.image_input_0 = Input(\n            ImageFileLoader(self.input_path_0))\n        self.image_input_1 = Input(\n            ImageFileLoader(self.input_path_1))\n        self.image_output_0 = Output(\n            ImageFileWriter(self.output_path_0))\n        self.image_output_1 = Output(\n            ImageFileWriter(self.output_path_1))\n\n    def process_one_subject(self, subject_id):\n        image_0 = self.image_input_0(subject_id)\n        image_1 = self.image_input_1(subject_id)\n        if image_0 is not None:\n            self.image_output_0(subject_id, image_0)\n        if image_1 is not None:\n            self.image_output_1(subject_id, image_1)\n\n@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"missing_strategy\", [\"pass\", \"drop\"])\ndef test_missing_handling(n_jobs, missing_strategy, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    # simulate partial missing data\n    os.remove(pathlib.Path(input_paths[0], \"test0.nrrd\").as_posix())\n    print(input_paths, output_paths)\n\n    pipeline = MultiInputPipelineTest(input_paths[0],\n                                      input_paths[1],\n                                      output_paths[0],\n                                      output_paths[1],\n                                      n_jobs,\n                                      missing_strategy)\n    with warnings.catch_warnings(record=True) as w:\n        pipeline.run()\n        assert len(w) == 1\n        assert missing_strategy in str(w[-1].message)\n\n    print(os.listdir(input_paths[1]), os.listdir(output_paths[1]))\n    if missing_strategy == \"drop\":\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nrrd\").as_posix()),\n            not os.path.exists(pathlib.Path(output_paths[1], \"test0.nrrd\").as_posix())\n        ])\n    else:\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nii.gz\").as_posix()),\n            os.path.exists(pathlib.Path(output_paths[1], \"test0.nii.gz\").as_posix())\n        ])\n    \n    \n    \n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `sample_input_output` fixture in the given code?",
        "answer": "The `sample_input_output` fixture is used to create temporary input and output directories for testing. It generates random test data (SimpleITK images) in the input directories, yields the paths to these directories for use in tests, and then cleans up the temporary directories after the tests are completed."
      },
      {
        "question": "How does the `SimplePipelineTest` class handle parallel processing, and what parameter controls this behavior?",
        "answer": "The `SimplePipelineTest` class inherits from the `Pipeline` class, which likely implements parallel processing. The `n_jobs` parameter in the constructor controls the number of parallel jobs. When `n_jobs` is set to a value greater than 1, the pipeline will attempt to process multiple subjects concurrently. The actual parallelization is handled by the parent `Pipeline` class."
      },
      {
        "question": "In the `test_missing_handling` function, what are the two different strategies for handling missing data, and how do they affect the output?",
        "answer": "The two strategies for handling missing data are 'pass' and 'drop', controlled by the `missing_strategy` parameter. When set to 'pass', the pipeline will process all subjects, skipping the missing input but still processing other available inputs for that subject. When set to 'drop', the pipeline will completely skip processing any subject with missing input data. This is verified in the test by checking the existence of output files for the deliberately removed input file."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class SimplePipelineTest(Pipeline):\n    def __init__(self, input_path, output_path, n_jobs):\n        super().__init__(n_jobs=n_jobs, show_progress=False)\n        self.input_path = input_path\n        self.output_path = output_path\n        self.image_input = Input(\n            ImageFileLoader(self.input_path))\n        self.image_output = Output(\n            ImageFileWriter(self.output_path))\n\n    def process_one_subject(self, subject_id):\n        # TODO: Implement the processing logic\n        pass",
        "complete": "class SimplePipelineTest(Pipeline):\n    def __init__(self, input_path, output_path, n_jobs):\n        super().__init__(n_jobs=n_jobs, show_progress=False)\n        self.input_path = input_path\n        self.output_path = output_path\n        self.image_input = Input(\n            ImageFileLoader(self.input_path))\n        self.image_output = Output(\n            ImageFileWriter(self.output_path))\n\n    def process_one_subject(self, subject_id):\n        image = self.image_input(subject_id)\n        self.image_output(subject_id, image)"
      },
      {
        "partial": "@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"missing_strategy\", [\"pass\", \"drop\"])\ndef test_missing_handling(n_jobs, missing_strategy, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    # simulate partial missing data\n    os.remove(pathlib.Path(input_paths[0], \"test0.nrrd\").as_posix())\n\n    pipeline = MultiInputPipelineTest(input_paths[0],\n                                      input_paths[1],\n                                      output_paths[0],\n                                      output_paths[1],\n                                      n_jobs,\n                                      missing_strategy)\n    # TODO: Implement the test logic\n    pass",
        "complete": "@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"missing_strategy\", [\"pass\", \"drop\"])\ndef test_missing_handling(n_jobs, missing_strategy, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    # simulate partial missing data\n    os.remove(pathlib.Path(input_paths[0], \"test0.nrrd\").as_posix())\n\n    pipeline = MultiInputPipelineTest(input_paths[0],\n                                      input_paths[1],\n                                      output_paths[0],\n                                      output_paths[1],\n                                      n_jobs,\n                                      missing_strategy)\n    with warnings.catch_warnings(record=True) as w:\n        pipeline.run()\n        assert len(w) == 1\n        assert missing_strategy in str(w[-1].message)\n\n    if missing_strategy == \"drop\":\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nrrd\").as_posix()),\n            not os.path.exists(pathlib.Path(output_paths[1], \"test0.nrrd\").as_posix())\n        ])\n    else:\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nii.gz\").as_posix()),\n            os.path.exists(pathlib.Path(output_paths[1], \"test0.nii.gz\").as_posix())\n        ])"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "shutil",
        "warnings",
        "numpy",
        "SimpleITK",
        "pytest"
      ],
      "from_imports": [
        "multiprocessing.cpu_count",
        "imgtools.io.ImageFileLoader",
        "imgtools.ops.BaseInput",
        "imgtools.ops.BaseOutput",
        "imgtools.pipeline.Pipeline"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_rest_3.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"mapcompound\",{\n  result <- mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})\n\ntest_that(\"mapproperties\",{\n  props <- mapCID2Properties(ids = c(123, 456), properties = c(\"MolecularWeight\", \"CanonicalSMILES\"))\n\n  expect_data_table(\n    x = props,\n    types = c(\"integer\", \"character\", \"character\"),\n    ncols = 3,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})\n\ntest_that(\"getPubchemCompound\", {\n  result <- getPubchemCompound(2244)\n  expect_class(result, \"data.table\")\n\n  res2 <- getPubchemCompound(c(3672), query_only = T)\n  expect_class(res2, \"list\")\n\n  res3 <- getPubchemCompound(c(3672), raw = T)\n  expect_class(res3, \"list\")\n  expect_class(res3[[1]], \"httr2_response\")\n\n  res4 <- getPubchemCompound(\"erlotinib\", \"name\", \"cids\")\n  expect_class(res4, \"data.table\")\n})\n\n\ntest_that(\"getPubchemCompound Failure\", {\n  expect_error(getPubchemCompound(2244, properties = NULL))\n  expect_error(getPubchemCompound(2244, properties = c(1234, 1542)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CID` function in the given code snippet, and what does the test case verify about its output?",
        "answer": "The `mapCompound2CID` function appears to map compound names to their corresponding CID (Compound ID) in PubChem. The test case verifies that the function returns a data table with 2 columns (likely compound name and CID) and 2 rows, where the first column is of character type and the second is of integer type. This suggests the function takes compound names as input and returns their corresponding CIDs."
      },
      {
        "question": "How does the `mapCID2Properties` function differ from `mapCompound2CID`, and what properties are being requested in the test case?",
        "answer": "The `mapCID2Properties` function maps CIDs to specific properties of the compounds. Unlike `mapCompound2CID` which goes from names to CIDs, this function starts with CIDs and retrieves properties. In the test case, it's requesting 'MolecularWeight' and 'CanonicalSMILES' for the CIDs 123 and 456. The test verifies that the output is a data table with 3 columns (likely CID, MolecularWeight, and CanonicalSMILES) and 2 rows, with the first column being integer type and the other two being character type."
      },
      {
        "question": "What are the different ways the `getPubchemCompound` function can be called based on the test cases, and how do the return types differ?",
        "answer": "The `getPubchemCompound` function can be called in several ways:\n1. With a single CID (e.g., 2244), which returns a data.table.\n2. With a CID and `query_only = TRUE`, which returns a list.\n3. With a CID and `raw = TRUE`, which returns a list containing an httr2_response object.\n4. With a compound name, 'name' as the second argument, and 'cids' as the third argument, which returns a data.table.\n\nThe return types differ based on the arguments passed: it can return a data.table, a list, or a list containing an httr2_response object, depending on how it's called."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"mapcompound\", {\n  result <- mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})",
        "complete": "test_that(\"mapcompound\", {\n  result <- mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})"
      },
      {
        "partial": "test_that(\"getPubchemCompound\", {\n  result <- getPubchemCompound(2244)\n  expect_class(result, \"data.table\")\n\n  res2 <- getPubchemCompound(c(3672), query_only = T)\n  expect_class(res2, \"list\")\n\n  res3 <- getPubchemCompound(c(3672), raw = T)\n  expect_class(res3, \"list\")\n  expect_class(res3[[1]], \"httr2_response\")\n\n  res4 <- getPubchemCompound(\"erlotinib\", \"name\", \"cids\")\n  expect_class(res4, \"data.table\")\n})",
        "complete": "test_that(\"getPubchemCompound\", {\n  result <- getPubchemCompound(2244)\n  expect_class(result, \"data.table\")\n\n  res2 <- getPubchemCompound(c(3672), query_only = T)\n  expect_class(res2, \"list\")\n\n  res3 <- getPubchemCompound(c(3672), raw = T)\n  expect_class(res3, \"list\")\n  expect_class(res3[[1]], \"httr2_response\")\n\n  res4 <- getPubchemCompound(\"erlotinib\", \"name\", \"cids\")\n  expect_class(res4, \"data.table\")\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/oncotree.R",
    "language": "R",
    "content": "\n#' Get data from Oncotree API\n#'\n#' This function retrieves data from the Oncotree API based on the specified target.\n#'\n#' @param target A character vector specifying the target data to retrieve. \n#'              Valid options are \"versions\", \"mainTypes\", and \"tumorTypes\".\n#'\n#' @return A data table containing the retrieved data.\n#'\n#' @noRd \n#' @keywords internal\n.getRequestOncotree <- function(\n    target = c(\"versions\", \"mainTypes\", \"tumorTypes\")\n) {\n    \n    url <- \"http://oncotree.mskcc.org\"\n    targetClean <- match.arg(target)\n    .buildURL(url, \"api\", targetClean) |> \n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() |> \n        .asDT()\n}\n#' Get available Oncotree versions\n#'\n#' This function retrieves the available versions of Oncotree.\n#'\n#' @return A `data.table` containing available Oncotree versions.\n#'\n#' @export\ngetOncotreeVersions <- function() {\n    .getRequestOncotree(target=\"versions\")\n}\n\n#' Get the main types from the Oncotree database.\n#'\n#' This function retrieves the main types from the Oncotree database.\n#' \n#' @return A `data.table` containing the main types from the Oncotree database.\n#' \n#' @export\ngetOncotreeMainTypes <- function() {\n    res <- .getRequestOncotree(target=\"mainTypes\") \n    data.table::setnames(res, \"mainType\")\n    return(res)\n}\n\n\n#' Get the tumor types from the Oncotree database.\n#' \n#' This function retrieves the tumor types from the Oncotree database.\n#' \n#' @return A `data.table` containing the tumor types from the Oncotree database.\n#' \n#' @export\ngetOncotreeTumorTypes <- function() {\n    .getRequestOncotree(target=\"tumorTypes\")\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.getRequestOncotree()` function and how does it handle different target parameters?",
        "answer": "The `.getRequestOncotree()` function is an internal function designed to retrieve data from the Oncotree API. It accepts a 'target' parameter which can be 'versions', 'mainTypes', or 'tumorTypes'. The function uses `match.arg()` to validate and clean the input, constructs the appropriate URL, sends a request to the API, parses the JSON response, and returns the result as a data table. This function serves as the core mechanism for the three exported functions: `getOncotreeVersions()`, `getOncotreeMainTypes()`, and `getOncotreeTumorTypes()`."
      },
      {
        "question": "How does the `getOncotreeMainTypes()` function differ from the other two exported functions in terms of its implementation?",
        "answer": "The `getOncotreeMainTypes()` function differs from `getOncotreeVersions()` and `getOncotreeTumorTypes()` in that it performs additional data manipulation after retrieving the data. Specifically, it uses `data.table::setnames(res, \"mainType\")` to rename the column in the returned data table. This suggests that the API response for main types might have a different column name that needs to be standardized. The other two functions simply return the data as received from the `.getRequestOncotree()` function without any further processing."
      },
      {
        "question": "What is the significance of the `#' @noRd` and `#' @keywords internal` tags in the documentation for the `.getRequestOncotree()` function?",
        "answer": "The `#' @noRd` tag stands for 'no Rd', which means this function should not have its own help page in the package documentation. The `#' @keywords internal` tag indicates that this function is intended for internal use within the package and should not be exposed to end users. These tags are used to hide implementation details from users while still allowing package developers to document the function for maintenance purposes. This is in contrast to the other three functions (`getOncotreeVersions()`, `getOncotreeMainTypes()`, and `getOncotreeTumorTypes()`), which are marked with `#' @export` and are intended to be used by end users of the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Get data from Oncotree API\n#'\n#' This function retrieves data from the Oncotree API based on the specified target.\n#'\n#' @param target A character vector specifying the target data to retrieve. \n#'              Valid options are \"versions\", \"mainTypes\", and \"tumorTypes\".\n#'\n#' @return A data table containing the retrieved data.\n#'\n#' @noRd \n#' @keywords internal\n.getRequestOncotree <- function(\n    target = c(\"versions\", \"mainTypes\", \"tumorTypes\")\n) {\n    \n    url <- \"http://oncotree.mskcc.org\"\n    targetClean <- match.arg(target)\n    # Complete the function body\n}",
        "complete": "#' Get data from Oncotree API\n#'\n#' This function retrieves data from the Oncotree API based on the specified target.\n#'\n#' @param target A character vector specifying the target data to retrieve. \n#'              Valid options are \"versions\", \"mainTypes\", and \"tumorTypes\".\n#'\n#' @return A data table containing the retrieved data.\n#'\n#' @noRd \n#' @keywords internal\n.getRequestOncotree <- function(\n    target = c(\"versions\", \"mainTypes\", \"tumorTypes\")\n) {\n    \n    url <- \"http://oncotree.mskcc.org\"\n    targetClean <- match.arg(target)\n    .buildURL(url, \"api\", targetClean) |> \n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() |> \n        .asDT()\n}"
      },
      {
        "partial": "#' Get the main types from the Oncotree database.\n#'\n#' This function retrieves the main types from the Oncotree database.\n#' \n#' @return A `data.table` containing the main types from the Oncotree database.\n#' \n#' @export\ngetOncotreeMainTypes <- function() {\n    # Complete the function body\n}",
        "complete": "#' Get the main types from the Oncotree database.\n#'\n#' This function retrieves the main types from the Oncotree database.\n#' \n#' @return A `data.table` containing the main types from the Oncotree database.\n#' \n#' @export\ngetOncotreeMainTypes <- function() {\n    res <- .getRequestOncotree(target=\"mainTypes\") \n    data.table::setnames(res, \"mainType\")\n    return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/segmentation.py",
    "language": "py",
    "content": "from functools import wraps\nimport warnings\n\nimport numpy as np\nimport SimpleITK as sitk\n\nfrom .sparsemask import SparseMask\n\nfrom ..utils import array_to_image, image_to_array\nfrom typing import Optional, Tuple, Set\n\n\ndef accepts_segmentations(f):\n    @wraps(f)\n    def wrapper(img, *args, **kwargs):\n        result = f(img, *args, **kwargs)\n        if isinstance(img, Segmentation):\n            result = sitk.Cast(result, sitk.sitkVectorUInt8)\n            return Segmentation(result, roi_indices=img.roi_indices, raw_roi_names=img.raw_roi_names)\n        else:\n            return result\n    return wrapper\n\n\ndef map_over_labels(segmentation, f, include_background=False, return_segmentation=True, **kwargs):\n    if include_background:\n        labels = range(segmentation.num_labels + 1)\n    else:\n        labels = range(1, segmentation.num_labels + 1)\n    res = [f(segmentation.get_label(label=label), **kwargs) for label in labels]\n    if return_segmentation and isinstance(res[0], sitk.Image):\n        res = [sitk.Cast(r, sitk.sitkUInt8) for r in res]\n        res = Segmentation(sitk.Compose(*res), roi_indices=segmentation.roi_indices, raw_roi_names=segmentation.raw_roi_names)\n    return res\n\n\nclass Segmentation(sitk.Image):\n    def __init__(self, \n                 segmentation, \n                 metadata: Optional[dict] = {}, \n                 roi_indices=None, \n                 existing_roi_indices=None, \n                 raw_roi_names={},\n                 frame_groups=None):\n        \"\"\"Initializes the Segmentation class\n        \n        Parameters\n        ----------\n        roi_indices\n            Dictionary of {\"ROI\": label number}\n        \n        existing_roi_indices\n            Dictionary of {\"ROI\": label number} of the existing ROIs\n\n        raw_roi_names\n            Dictionary of {\"ROI\": original countor names}\n\n        frame_groups\n            PerFrameFunctionalGroupsSequence (5200, 9230) DICOM metadata\n        \"\"\"\n        super().__init__(segmentation)\n        self.num_labels = self.GetNumberOfComponentsPerPixel()\n        self.raw_roi_names = raw_roi_names\n        self.metadata = metadata\n        self.frame_groups = frame_groups\n\n        if not roi_indices:\n            self.roi_indices = {f\"label_{i}\": i for i in range(1, self.num_labels+1)}\n        else:\n            self.roi_indices = roi_indices\n            if 0 in self.roi_indices.values():\n                self.roi_indices = {k : v+1 for k, v in self.roi_indices.items()}\n        \n        if len(self.roi_indices) != self.num_labels:\n            for i in range(1, self.num_labels+1):\n                if i not in self.roi_indices.values():\n                    self.roi_indices[f\"label_{i}\"] = i\n\n        self.existing_roi_indices = existing_roi_indices\n\n    def get_label(self, label=None, name=None, relabel=False):\n        if label is None and name is None:\n            raise ValueError(\"Must pass either label or name.\")\n\n        if label is None:\n            label = self.roi_indices[name]\n\n        if label == 0:\n            # background is stored implicitly and needs to be computed\n            label_arr = sitk.GetArrayViewFromImage(self)\n            label_img = sitk.GetImageFromArray((label_arr.sum(-1) == 0).astype(np.uint8))\n        else:\n            label_img = sitk.VectorIndexSelectionCast(self, label - 1)\n            if relabel:\n                label_img *= label\n\n        return label_img\n\n    def to_label_image(self):\n        arr, *_ = image_to_array(self)\n        # TODO handle overlapping labels\n        label_arr = np.where(arr.sum(-1) != 0, arr.argmax(-1) + 1, 0)\n        label_img = array_to_image(label_arr, reference_image=self)\n        return label_img\n\n    # TODO also overload other operators (arithmetic, etc.)\n    # with some sensible behaviour\n\n    def __getitem__(self, idx):\n        res = super().__getitem__(idx)\n        if isinstance(res, sitk.Image):\n            res = Segmentation(res, roi_indices=self.roi_indices, raw_roi_names=self.raw_roi_names)\n        return res\n\n    def __repr__(self):\n        return f\"<Segmentation with ROIs: {self.roi_indices!r}>\"\n         \n    def generate_sparse_mask(self, verbose=False) -> SparseMask:\n        \"\"\"\n        Generate a sparse mask from the contours, taking the argmax of all overlaps\n\n        Parameters\n        ----------\n        mask\n            Segmentation object to build sparse mask from\n\n        Returns\n        -------\n        SparseMask\n            The sparse mask object.\n        \"\"\"\n        # print(\"asdlkfjalkfsjg\", self.roi_indices)\n        mask_arr = np.transpose(sitk.GetArrayFromImage(self))\n        for name in self.roi_indices.keys():\n            self.roi_indices[name] = self.existing_roi_indices[name]\n        # print(self.roi_indices)\n        \n        sparsemask_arr = np.zeros(mask_arr.shape[1:])\n        \n        if verbose:\n            voxels_with_overlap = set()\n\n        if len(mask_arr.shape) == 4:\n            for i in range(mask_arr.shape[0]):\n                slice = mask_arr[i, :, :, :]\n                slice *= list(self.roi_indices.values())[i]  # everything is 0 or 1, so this is fine to convert filled voxels to label indices\n                if verbose:\n                    res = self._max_adder(sparsemask_arr, slice)\n                    sparsemask_arr = res[0]\n                    for e in res[1]:\n                        voxels_with_overlap.add(e)\n                else:\n                    sparsemask_arr = np.fmax(sparsemask_arr, slice)  # elementwise maximum\n        else:\n            sparsemask_arr = mask_arr\n        \n        sparsemask = SparseMask(sparsemask_arr, self.roi_indices)\n\n        if verbose:\n            if len(voxels_with_overlap) != 0:\n                warnings.warn(f\"{len(voxels_with_overlap)} voxels have overlapping contours.\")\n        return sparsemask\n\n    def _max_adder(self, arr_1: np.ndarray, arr_2: np.ndarray) -> Tuple[np.ndarray, Set[Tuple[int, int, int]]]:\n        \"\"\"\n        Takes the maximum of two 3D arrays elementwise and returns the resulting array and a list of voxels that have overlapping contours in a set\n\n        Parameters\n        ----------\n        arr_1\n            First array to take maximum of\n        arr_2\n            Second array to take maximum of\n        \n        Returns\n        -------\n        Tuple[np.ndarray, Set[Tuple[int, int, int]]]\n            The resulting array and a list of voxels that have overlapping contours in a set\n        \"\"\"\n        res = np.zeros(arr_1.shape)\n        overlaps = {}  # set of tuples of the coords that have overlap\n        for i in range(arr_1.shape[0]):\n            for j in range(arr_1.shape[1]):\n                for k in range(arr_1.shape[2]):\n                    if arr_1[i, j, k] != 0 and arr_2[i, j, k] != 0:\n                        overlaps.add((i, j, k))\n                    res[i, j, k] = max(arr_1[i, j, k], arr_2[i, j, k])\n        return res, overlaps\n\n    @classmethod\n    def from_dicom_seg(cls, mask, meta):\n        # get duplicates\n        label_counters = {i.SegmentLabel: 1 for i in meta.SegmentSequence}\n        raw_roi_names  = {}  # {i.SegmentLabel: i.SegmentNumber for n, i in meta.SegmentSequence}\n        for n, i in enumerate(meta.SegmentSequence):\n            label = i.SegmentLabel\n            num   = i.SegmentNumber\n\n            if label not in raw_roi_names:\n                raw_roi_names[label] = num\n            else:\n                raw_roi_names[f\"{label}_{label_counters[label]}\"] = num\n                label_counters[label] += 1\n        \n        frame_groups  = meta.PerFrameFunctionalGroupsSequence\n        return cls(mask, raw_roi_names=raw_roi_names, frame_groups=frame_groups)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `accepts_segmentations` decorator in the given code snippet?",
        "answer": "The `accepts_segmentations` decorator is used to wrap functions that operate on images. It ensures that if the input is a Segmentation object, the result is cast to a VectorUInt8 type and returned as a new Segmentation object with the same ROI indices and names. This allows functions to work seamlessly with both regular images and Segmentation objects, maintaining the segmentation-specific properties when applicable."
      },
      {
        "question": "How does the `map_over_labels` function handle background labels in a segmentation?",
        "answer": "The `map_over_labels` function processes each label in a segmentation separately. It has an `include_background` parameter that determines whether to process the background (label 0) or not. If `include_background` is True, it processes labels from 0 to num_labels. If False (default), it processes labels from 1 to num_labels, excluding the background. This allows for flexible handling of background labels in segmentation processing tasks."
      },
      {
        "question": "What is the purpose of the `generate_sparse_mask` method in the Segmentation class, and how does it handle overlapping contours?",
        "answer": "The `generate_sparse_mask` method creates a SparseMask object from the Segmentation. It converts the multi-label segmentation into a single-label representation by taking the argmax of overlapping labels. When `verbose` is True, it uses the `_max_adder` method to detect and report overlapping contours. This method returns both the resulting sparse mask and a set of voxel coordinates where overlaps occur, allowing for analysis of conflicting segmentations while generating a deterministic single-label representation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def map_over_labels(segmentation, f, include_background=False, return_segmentation=True, **kwargs):\n    if include_background:\n        labels = range(segmentation.num_labels + 1)\n    else:\n        labels = range(1, segmentation.num_labels + 1)\n    res = [f(segmentation.get_label(label=label), **kwargs) for label in labels]\n    if return_segmentation and isinstance(res[0], sitk.Image):\n        # Complete the code here\n    return res",
        "complete": "def map_over_labels(segmentation, f, include_background=False, return_segmentation=True, **kwargs):\n    if include_background:\n        labels = range(segmentation.num_labels + 1)\n    else:\n        labels = range(1, segmentation.num_labels + 1)\n    res = [f(segmentation.get_label(label=label), **kwargs) for label in labels]\n    if return_segmentation and isinstance(res[0], sitk.Image):\n        res = [sitk.Cast(r, sitk.sitkUInt8) for r in res]\n        res = Segmentation(sitk.Compose(*res), roi_indices=segmentation.roi_indices, raw_roi_names=segmentation.raw_roi_names)\n    return res"
      },
      {
        "partial": "class Segmentation(sitk.Image):\n    def __init__(self, \n                 segmentation, \n                 metadata: Optional[dict] = {}, \n                 roi_indices=None, \n                 existing_roi_indices=None, \n                 raw_roi_names={},\n                 frame_groups=None):\n        super().__init__(segmentation)\n        self.num_labels = self.GetNumberOfComponentsPerPixel()\n        self.raw_roi_names = raw_roi_names\n        self.metadata = metadata\n        self.frame_groups = frame_groups\n\n        if not roi_indices:\n            self.roi_indices = {f\"label_{i}\": i for i in range(1, self.num_labels+1)}\n        else:\n            # Complete the code here\n\n        if len(self.roi_indices) != self.num_labels:\n            for i in range(1, self.num_labels+1):\n                if i not in self.roi_indices.values():\n                    self.roi_indices[f\"label_{i}\"] = i\n\n        self.existing_roi_indices = existing_roi_indices",
        "complete": "class Segmentation(sitk.Image):\n    def __init__(self, \n                 segmentation, \n                 metadata: Optional[dict] = {}, \n                 roi_indices=None, \n                 existing_roi_indices=None, \n                 raw_roi_names={},\n                 frame_groups=None):\n        super().__init__(segmentation)\n        self.num_labels = self.GetNumberOfComponentsPerPixel()\n        self.raw_roi_names = raw_roi_names\n        self.metadata = metadata\n        self.frame_groups = frame_groups\n\n        if not roi_indices:\n            self.roi_indices = {f\"label_{i}\": i for i in range(1, self.num_labels+1)}\n        else:\n            self.roi_indices = roi_indices\n            if 0 in self.roi_indices.values():\n                self.roi_indices = {k : v+1 for k, v in self.roi_indices.items()}\n\n        if len(self.roi_indices) != self.num_labels:\n            for i in range(1, self.num_labels+1):\n                if i not in self.roi_indices.values():\n                    self.roi_indices[f\"label_{i}\"] = i\n\n        self.existing_roi_indices = existing_roi_indices"
      }
    ],
    "dependencies": {
      "imports": [
        "warnings",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "functools.wraps",
        "sparsemask.SparseMask",
        "utils.array_to_image",
        "typing.Optional"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-matchNested.R",
    "language": "R",
    "content": "#' Match inside nested elements\n#' \n#' @export\n#'\n#' @details\n#' Intentionally only performs exact matching. Refer to `filterNested` function\n#' for partial matching support with regular expressions.\n#'\n#' @param x\n#' The values to be matched.\n#'\n#' @param table\n#' The values to be matched against.\n#' Applies across rows for `DataFrame` method.\n#' \n#' @param ...\n#' Additional arguments to be passed to the method.\n#' \n#' @param keep_duplicates\n#' A logical value indicating whether to keep duplicates.\n#' \n#' @return `integer`.\n#' A positional vector corresponding to values defined in `table` the same\n#' size as `x`.\n#'\n#' @examples\n#' showMethods(\"matchNested\")\nsetGeneric(\n    name = \"matchNested\",\n    def = function(x, table, ..., keep_duplicates = FALSE) standardGeneric(\"matchNested\"),\n    signature = c(\"x\", \"table\", \"keep_duplicates\")\n)\n\n`matchNested,list` <- \n    function(x, table, keep_duplicates){\n        dt <- lapply(table, unlistNested) |> \n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n}\n\n`matchNested,data.table` <- \n    function(x, table, keep_duplicates){\n        checkmate::assert_data_table(table, min.rows = 1)\n\n        dt <- apply(\n            X = table,\n            MARGIN = 1L,\n            FUN = unlistNested,\n            simplify = FALSE\n            ) |>\n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n}\n\n\n`matchNested,data.frame`  <-\n    function(x, table, keep_duplicates){\n        checkmate::assert_data_frame(table, min.rows = 1)\n\n        dt <- apply(\n            X = table,\n            MARGIN = 1L,\n            FUN = unlistNested,\n            simplify = FALSE\n            ) |>\n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n}\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"list\"\n    ),\n    definition = `matchNested,list`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"numeric\",\n        table = \"list\"\n    ),\n    definition = `matchNested,list`\n)\n\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"data.table\"\n    ),\n    definition = `matchNested,data.table`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"numeric\",\n        table = \"data.table\"\n    ),\n    definition = `matchNested,data.table`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"data.frame\"\n    ),\n    definition = `matchNested,data.frame`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"data.frame\"\n    ),\n    definition = `matchNested,data.frame`\n)\n\n#' Convert Nested List to Data Table\n#'\n#' This function converts a nested list into a data table with two columns: \"idx\" and \"value\".\n#'\n#' @param unlisted_elements A nested list to be converted into a data table.\n#' @return A data table with two columns: \"idx\" and \"value\".\n#' @noRd \n#' @keywords internal\n.convert_nested_list_to_dt <- function(unlisted_elements){\n    idx <- rep(seq_along(unlisted_elements), times = lengths(unlisted_elements))\n    elements <- unlist(unlisted_elements, recursive = FALSE, use.names = FALSE)\n    data.table::data.table(\"idx\" = idx, \"value\" = elements)\n}\n\n\n#' Unlists a nested list and removes NA values and duplicates.\n#'\n#' This function takes a nested list as input and unlists it recursively. \n#' It then removes any NA values and duplicates from the resulting vector.\n#'\n#' @param element The nested list to be unlisted.\n#' @return A vector with NA values and duplicates removed.\n#' @examples\n#' nested_list <- list(a = list(1, 2, NA), b = list(3, 4, 5))\n#' unlistNested(nested_list)\n#' # Output: [1] 1 2 3 4 5\n#'\n#' @export\nunlistNested <- function(element){\n    unlist(element, recursive = TRUE, use.names = FALSE) |>\n        stats::na.omit() |>\n        unique()\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `matchNested` function and how does it handle different input types?",
        "answer": "The `matchNested` function is designed to match values inside nested elements. It uses method dispatch to handle different input types, including lists, data.tables, and data.frames. The function returns an integer vector corresponding to the positions of matched values in the `table` argument. It supports both character and numeric input for `x` and can work with various table types."
      },
      {
        "question": "How does the `keep_duplicates` parameter affect the behavior of the `matchNested` function?",
        "answer": "The `keep_duplicates` parameter is a logical value that determines whether duplicate values should be retained in the matching process. When `keep_duplicates` is set to `FALSE` (the default), the function removes duplicates from the converted data table using `dt <- dt[!duplicated(dt$value), , drop = FALSE]`. This ensures that only unique values are considered for matching, potentially improving performance and avoiding redundant matches."
      },
      {
        "question": "What is the role of the `.convert_nested_list_to_dt` helper function in the `matchNested` implementation?",
        "answer": "The `.convert_nested_list_to_dt` function is an internal helper function that converts a nested list into a data table with two columns: 'idx' and 'value'. It is used within the `matchNested` function to transform the input `table` into a standardized format for efficient matching. The function creates an index column ('idx') that corresponds to the original positions in the input list, and a 'value' column containing the flattened elements from the nested structure. This conversion allows for easier and more efficient matching operations in the main function."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setGeneric(\n    name = \"matchNested\",\n    def = function(x, table, ..., keep_duplicates = FALSE) standardGeneric(\"matchNested\"),\n    signature = c(\"x\", \"table\", \"keep_duplicates\")\n)\n\n`matchNested,list` <- \n    function(x, table, keep_duplicates){\n        dt <- lapply(table, unlistNested) |> \n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        # Complete the function here\n    }",
        "complete": "setGeneric(\n    name = \"matchNested\",\n    def = function(x, table, ..., keep_duplicates = FALSE) standardGeneric(\"matchNested\"),\n    signature = c(\"x\", \"table\", \"keep_duplicates\")\n)\n\n`matchNested,list` <- \n    function(x, table, keep_duplicates){\n        dt <- lapply(table, unlistNested) |> \n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n    }"
      },
      {
        "partial": "#' Convert Nested List to Data Table\n#'\n#' This function converts a nested list into a data table with two columns: \"idx\" and \"value\".\n#'\n#' @param unlisted_elements A nested list to be converted into a data table.\n#' @return A data table with two columns: \"idx\" and \"value\".\n#' @noRd \n#' @keywords internal\n.convert_nested_list_to_dt <- function(unlisted_elements){\n    # Complete the function here\n}",
        "complete": "#' Convert Nested List to Data Table\n#'\n#' This function converts a nested list into a data table with two columns: \"idx\" and \"value\".\n#'\n#' @param unlisted_elements A nested list to be converted into a data table.\n#' @return A data table with two columns: \"idx\" and \"value\".\n#' @noRd \n#' @keywords internal\n.convert_nested_list_to_dt <- function(unlisted_elements){\n    idx <- rep(seq_along(unlisted_elements), times = lengths(unlisted_elements))\n    elements <- unlist(unlisted_elements, recursive = FALSE, use.names = FALSE)\n    data.table::data.table(\"idx\" = idx, \"value\" = elements)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/codecover.R",
    "language": "R",
    "content": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\ncovr::codecov(quiet = FALSE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `Sys.unsetenv(\"R_TESTS\")` command in this code snippet?",
        "answer": "The `Sys.unsetenv(\"R_TESTS\")` command is used to unset the R_TESTS environment variable. This is often done to prevent certain tests from running or to modify the behavior of R during package checks or installations. In this context, it's likely used to ensure that the code coverage analysis runs without interference from R's built-in testing mechanisms."
      },
      {
        "question": "What does the `options(covr.fix_parallel_mcexit=TRUE)` line do in this code?",
        "answer": "This line sets an option for the 'covr' package. Specifically, it enables a fix for issues that can occur when running code coverage in parallel. The `covr.fix_parallel_mcexit=TRUE` option helps prevent errors related to parallel processing during code coverage analysis, ensuring more reliable results when using multiple cores or threads."
      },
      {
        "question": "What is the purpose of the `covr::codecov(quiet = FALSE)` function call in this snippet?",
        "answer": "The `covr::codecov(quiet = FALSE)` function call is used to run code coverage analysis and submit the results to Codecov, a service for tracking code coverage. The `quiet = FALSE` argument means that the function will provide verbose output during its execution, which can be useful for debugging or monitoring the progress of the coverage analysis. This is typically used in continuous integration pipelines to ensure that new code changes maintain or improve code coverage."
      }
    ],
    "completion_tasks": [
      {
        "partial": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\ncovr::codecov(",
        "complete": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\ncovr::codecov(quiet = FALSE)"
      },
      {
        "partial": "library(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\n\n# Run codecov with specific options\ncovr::codecov(",
        "complete": "library(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\n\n# Run codecov with specific options\ncovr::codecov(quiet = FALSE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/imageutils.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\n\ndef physical_points_to_idxs(image, points, continuous=False):\n    if continuous:\n        transform = image.TransformPhysicalPointToContinuousIndex\n    else:\n        transform = image.TransformPhysicalPointToIndex\n    \n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    \n    # transform indices to ContourSequence/ContourData-wise\n    t_points = []\n    for slc in points:\n        t_points.append(vectorized_transform(slc)[:,::-1])\n    return t_points\n\n\ndef idxs_to_physical_points(image, idxs):\n    continuous = any([isinstance(i, float) for i in idxs])\n\n    if continuous:\n        transform = image.TransformContinuousIndexToPhysicalPoint\n    else:\n        transform = image.TransformIndexToPhysicalPoint\n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    return vectorized_transform(idxs)\n\n\ndef image_to_array(image):\n    origin, direction, spacing = image.GetOrigin(), image.GetDirection(), image.GetSpacing()\n    array = sitk.GetArrayFromImage(image)\n    return array, origin, direction, spacing\n\n\ndef show_image(image, mask=None, ax=None):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.subplots()\n\n    image_array, *_ = image_to_array(image)\n\n    ax.imshow(image_array, cmap=\"bone\", interpolation=\"bilinear\")\n\n    if mask is not None:\n        mask_array, *_ = image_to_array(mask)\n        mask_array = np.ma.masked_where(mask_array == 0, mask_array)\n\n        ax.imshow(mask_array, cmap=\"tab20\")\n\n    return ax\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `physical_points_to_idxs` function, and how does it handle continuous vs. non-continuous transformations?",
        "answer": "The `physical_points_to_idxs` function converts physical points to image indices. It uses SimpleITK's transformation methods to perform this conversion. The function has a `continuous` parameter that determines whether to use `TransformPhysicalPointToContinuousIndex` (for continuous indices) or `TransformPhysicalPointToIndex` (for discrete indices). It applies the chosen transformation to each point using NumPy's vectorize function and returns the transformed points with reversed coordinates."
      },
      {
        "question": "Explain the purpose of the `idxs_to_physical_points` function and how it determines whether to use continuous or discrete transformation.",
        "answer": "The `idxs_to_physical_points` function converts image indices to physical points. It automatically determines whether to use continuous or discrete transformation by checking if any of the input indices are floats. If floats are present, it uses `TransformContinuousIndexToPhysicalPoint`; otherwise, it uses `TransformIndexToPhysicalPoint`. The function then applies the chosen transformation to all indices using NumPy's vectorize function and returns the physical points."
      },
      {
        "question": "How does the `show_image` function visualize both the image and an optional mask? What plotting libraries does it use?",
        "answer": "The `show_image` function visualizes a SimpleITK image and an optional mask using matplotlib. It first converts the SimpleITK image to a NumPy array using the `image_to_array` function. The image is displayed using `imshow` with a 'bone' colormap. If a mask is provided, it's also converted to a NumPy array, and areas with zero values are masked. The mask is then overlaid on the image using a different colormap ('tab20'). This function allows for easy visualization of medical images with segmentation masks."
      }
    ],
    "completion_tasks": [
      {
        "partial": "import SimpleITK as sitk\nimport numpy as np\n\ndef physical_points_to_idxs(image, points, continuous=False):\n    if continuous:\n        transform = image.TransformPhysicalPointToContinuousIndex\n    else:\n        transform = image.TransformPhysicalPointToIndex\n    \n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    \n    # transform indices to ContourSequence/ContourData-wise\n    t_points = []\n    for slc in points:\n        # Complete the transformation of points\n    return t_points",
        "complete": "import SimpleITK as sitk\nimport numpy as np\n\ndef physical_points_to_idxs(image, points, continuous=False):\n    if continuous:\n        transform = image.TransformPhysicalPointToContinuousIndex\n    else:\n        transform = image.TransformPhysicalPointToIndex\n    \n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    \n    # transform indices to ContourSequence/ContourData-wise\n    t_points = []\n    for slc in points:\n        t_points.append(vectorized_transform(slc)[:,::-1])\n    return t_points"
      },
      {
        "partial": "import SimpleITK as sitk\nimport numpy as np\n\ndef show_image(image, mask=None, ax=None):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.subplots()\n\n    image_array, *_ = image_to_array(image)\n\n    ax.imshow(image_array, cmap=\"bone\", interpolation=\"bilinear\")\n\n    if mask is not None:\n        # Complete the mask overlay\n\n    return ax",
        "complete": "import SimpleITK as sitk\nimport numpy as np\n\ndef show_image(image, mask=None, ax=None):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.subplots()\n\n    image_array, *_ = image_to_array(image)\n\n    ax.imshow(image_array, cmap=\"bone\", interpolation=\"bilinear\")\n\n    if mask is not None:\n        mask_array, *_ = image_to_array(mask)\n        mask_array = np.ma.masked_where(mask_array == 0, mask_array)\n        ax.imshow(mask_array, cmap=\"tab20\")\n\n    return ax"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy",
        "matplotlib.pyplot"
      ],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/structureset.py",
    "language": "py",
    "content": "import re\nfrom warnings import warn\nfrom typing import Dict, List, Optional, TypeVar\n\nimport numpy as np\nimport SimpleITK as sitk\nfrom pydicom import dcmread\nfrom itertools import groupby\nfrom skimage.draw import polygon2mask\n\nfrom .segmentation import Segmentation\nfrom ..utils import physical_points_to_idxs\n\nT = TypeVar('T')\n\n\ndef _get_roi_points(rtstruct, roi_index):\n    return [np.array(slc.ContourData).reshape(-1, 3) for slc in rtstruct.ROIContourSequence[roi_index].ContourSequence]\n\n\nclass StructureSet:\n    def __init__(self, roi_points: Dict[str, np.ndarray], metadata: Optional[Dict[str, T]] = None):\n        \"\"\"Initializes the StructureSet class containing contour points\n        \n        Parameters\n        ----------\n        roi_points\n            Dictionary of {\"ROI\": [ndarray of shape n x 3 of contour points]}\n        \n        metadata\n            Dictionary of DICOM metadata\n        \"\"\"\n        self.roi_points = roi_points\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n\n    @classmethod\n    def from_dicom_rtstruct(cls, rtstruct_path: str) -> 'StructureSet':\n        rtstruct = dcmread(rtstruct_path, force=True)\n        roi_names = [roi.ROIName for roi in rtstruct.StructureSetROISequence]\n        roi_points = {}\n        for i, name in enumerate(roi_names):\n            try:\n                roi_points[name] = _get_roi_points(rtstruct, i)\n            except AttributeError:\n                warn(f\"Could not get points for ROI {name} (in {rtstruct_path}).\")\n\n        metadata = {}\n        \n        return cls(roi_points, metadata)\n\n    @property\n    def roi_names(self) -> List[str]:\n        return list(self.roi_points.keys())\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        \"\"\"\n        Parameters\n        ----\n        roi_select_first\n            Select the first matching ROI/regex for each OAR, no duplicate matches. \n\n        roi_separate\n            Process each matching ROI/regex as individual masks, instead of consolidating into one mask\n            Each mask will be named ROI_n, where n is the nth regex/name/string.\n        \"\"\"\n        labels = {}\n        cur_label = 0\n        if names == self.roi_names:\n            for i, name in enumerate(self.roi_names):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):  # checks if all ROIs have already been processed.\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:  # if multiple regex/names to match\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched:  # break if roi_select_first and we're matched\n                            break\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels\n\n    def get_mask(self, reference_image, mask, label, idx, continuous):\n        size = reference_image.GetSize()[::-1]\n        physical_points = self.roi_points.get(label, np.array([]))\n        mask_points = physical_points_to_idxs(reference_image, physical_points, continuous=continuous)\n        for contour in mask_points:\n            try:\n                z, slice_points = np.unique(contour[:, 0]), contour[:, 1:]\n                if len(z) == 1:  # assert len(z) == 1, f\"This contour ({name}) spreads across more than 1 slice.\"\n                    slice_mask = polygon2mask(size[1:], slice_points)\n                    mask[z[0], :, :, idx] += slice_mask\n            except:  # rounding errors for points on the boundary\n                if z == mask.shape[0]:\n                    z -= 1\n                elif z == -1: #?\n                    z += 1\n                elif z > mask.shape[0] or z < -1:\n                    raise IndexError(f\"{z} index is out of bounds for image sized {mask.shape}.\")\n                \n                # if the contour spans only 1 z-slice \n                if len(z) == 1:\n                    z_idx = int(np.floor(z[0]))\n                    slice_mask = polygon2mask(size[1:], slice_points)\n                    mask[z_idx, :, :, idx] += slice_mask\n                else:\n                    raise ValueError(\"This contour is corrupted and spans across 2 or more slices.\")\n\n    def to_segmentation(self, reference_image: sitk.Image,\n                        roi_names: Dict[str, str] = None,\n                        continuous: bool = True,\n                        existing_roi_indices: Dict[str, int] = None,\n                        ignore_missing_regex: bool = False,\n                        roi_select_first: bool = False,\n                        roi_separate: bool = False) -> Segmentation:\n        \"\"\"Convert the structure set to a Segmentation object.\n\n        Parameters\n        ----------\n        reference_image\n            Image used as reference geometry.\n        roi_names\n            List of ROI names to export. Both full names and\n            case-insensitive regular expressions are allowed.\n            All labels within one sublist will be assigned\n            the same label.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation object.\n\n        Notes\n        -----\n        If `roi_names` contains lists of strings, each matching\n        name within a sublist will be assigned the same label. This means\n        that `roi_names=['pat']` and `roi_names=[['pat']]` can lead\n        to different label assignments, depending on how many ROI names\n        match the pattern. E.g. if `self.roi_names = ['fooa', 'foob']`,\n        passing `roi_names=['foo(a|b)']` will result in a segmentation with \n        two labels, but passing `roi_names=[['foo(a|b)']]` will result in\n        one label for both `'fooa'` and `'foob'`.\n\n        In general, the exact ordering of the returned labels cannot be\n        guaranteed (unless all patterns in `roi_names` can only match\n        a single name or are lists of strings).\n        \"\"\"\n        labels = {}\n        if roi_names is None or roi_names == {}:\n            roi_names = self.roi_names  # all the contour names\n            labels = self._assign_labels(roi_names, roi_select_first, roi_separate)  # only the ones that match the regex\n        elif isinstance(roi_names, dict):\n            for name, pattern in roi_names.items():\n                if isinstance(pattern, str):\n                    matching_names = list(self._assign_labels([pattern], roi_select_first).keys())\n                    if matching_names:\n                        labels[name] = matching_names  # {\"GTV\": [\"GTV1\", \"GTV2\"]} is the result of _assign_labels()\n                elif isinstance(pattern, list):  # for inputs that have multiple patterns for the input, e.g. {\"GTV\": [\"GTV.*\", \"HTVI.*\"]}\n                    labels[name] = []\n                    for pattern_one in pattern:\n                        matching_names = list(self._assign_labels([pattern_one], roi_select_first).keys())\n                        if matching_names:\n                            labels[name].extend(matching_names)  # {\"GTV\": [\"GTV1\", \"GTV2\"]}\n        if isinstance(roi_names, str):\n            roi_names = [roi_names]\n        if isinstance(roi_names, list):  # won't this always trigger after the previous?\n            labels = self._assign_labels(roi_names, roi_select_first)\n        print(\"labels:\", labels)\n        all_empty = True\n        for v in labels.values():\n            if v != []:\n                all_empty = False\n        if all_empty:\n            if not ignore_missing_regex:\n                raise ValueError(f\"No ROIs matching {roi_names} found in {self.roi_names}.\")\n            else:\n                return None\n        labels = {k:v for (k,v) in labels.items() if v != [] }\n        size = reference_image.GetSize()[::-1] + (len(labels),)\n        mask = np.zeros(size, dtype=np.uint8)\n\n        seg_roi_indices = {}\n        if roi_names != {} and isinstance(roi_names, dict):\n            for i, (name, label_list) in enumerate(labels.items()):\n                for label in label_list:\n                    self.get_mask(reference_image, mask, label, i, continuous)\n                seg_roi_indices[name] = i\n\n        else:\n            for name, label in labels.items():\n                self.get_mask(reference_image, mask, name, label, continuous)\n            seg_roi_indices = {\"_\".join(k): v for v, k in groupby(labels, key=lambda x: labels[x])}\n\n        mask[mask > 1] = 1\n        mask = sitk.GetImageFromArray(mask, isVector=True)\n        mask.CopyInformation(reference_image)\n        mask = Segmentation(mask, roi_indices=seg_roi_indices, existing_roi_indices=existing_roi_indices, raw_roi_names=labels)  # in the segmentation, pass all the existing roi names and then process is in the segmentation class\n\n        return mask\n\n    def __repr__(self):\n        return f\"<StructureSet with ROIs: {self.roi_names!r}>\"\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `_assign_labels` method in the `StructureSet` class, and how does it handle different input scenarios?",
        "answer": "The `_assign_labels` method in the `StructureSet` class is responsible for assigning numeric labels to ROI (Region of Interest) names. It handles different input scenarios as follows:\n\n1. If the input `names` matches `self.roi_names`, it assigns sequential labels to each ROI.\n2. For string patterns, it uses regex matching to assign labels to matching ROI names.\n3. For lists of patterns, it can either select the first matching ROI (if `roi_select_first` is True) or process each matching ROI separately (if `roi_separate` is True).\n\nThe method is flexible and can handle various input formats, including single strings, lists of strings, and nested lists for grouping multiple patterns."
      },
      {
        "question": "Explain the functionality of the `get_mask` method in the `StructureSet` class. What are its key parameters and what does it do?",
        "answer": "The `get_mask` method in the `StructureSet` class is responsible for creating a binary mask for a specific ROI (Region of Interest) based on its contour points. Key aspects of this method include:\n\n1. Parameters:\n   - `reference_image`: The image used as a reference for spatial information.\n   - `mask`: The output mask array where the ROI will be drawn.\n   - `label`: The name of the ROI to be processed.\n   - `idx`: The index in the mask array where the ROI should be placed.\n   - `continuous`: A boolean flag for point interpolation.\n\n2. Functionality:\n   - It converts physical contour points to image indices.\n   - For each contour, it creates a 2D polygon mask using `polygon2mask`.\n   - It handles edge cases, such as rounding errors for points on the boundary.\n   - The method adds the created mask to the appropriate slice in the 3D mask array.\n\nThis method is crucial for converting contour data into a volumetric binary mask representation of the ROI."
      },
      {
        "question": "How does the `to_segmentation` method in the `StructureSet` class work, and what are its key features for handling different ROI selection scenarios?",
        "answer": "The `to_segmentation` method in the `StructureSet` class converts the structure set to a `Segmentation` object. Key features and functionality include:\n\n1. Flexible ROI selection:\n   - Accepts various input formats for `roi_names` (None, dict, list, or string).\n   - Supports regex patterns for matching ROI names.\n\n2. Label assignment:\n   - Uses `_assign_labels` to map ROI names to numeric labels.\n   - Handles grouping of multiple ROIs under a single label.\n\n3. Mask creation:\n   - Creates a 3D mask array based on the reference image dimensions.\n   - Calls `get_mask` for each selected ROI to populate the mask array.\n\n4. Special options:\n   - `roi_select_first`: Selects only the first matching ROI for each pattern.\n   - `roi_separate`: Processes each matching ROI as individual masks.\n   - `ignore_missing_regex`: Allows the method to return None if no matching ROIs are found.\n\n5. Output:\n   - Returns a `Segmentation` object containing the created mask and ROI indices.\n\nThis method provides a comprehensive way to convert contour-based structure sets into volumetric segmentations, with various options for ROI selection and processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class StructureSet:\n    def __init__(self, roi_points: Dict[str, np.ndarray], metadata: Optional[Dict[str, T]] = None):\n        self.roi_points = roi_points\n        self.metadata = metadata or {}\n\n    @classmethod\n    def from_dicom_rtstruct(cls, rtstruct_path: str) -> 'StructureSet':\n        rtstruct = dcmread(rtstruct_path, force=True)\n        roi_names = [roi.ROIName for roi in rtstruct.StructureSetROISequence]\n        roi_points = {}\n        for i, name in enumerate(roi_names):\n            try:\n                roi_points[name] = _get_roi_points(rtstruct, i)\n            except AttributeError:\n                warn(f\"Could not get points for ROI {name} (in {rtstruct_path}).\")\n        return cls(roi_points, {})\n\n    @property\n    def roi_names(self) -> List[str]:\n        # TODO: Implement this method",
        "complete": "class StructureSet:\n    def __init__(self, roi_points: Dict[str, np.ndarray], metadata: Optional[Dict[str, T]] = None):\n        self.roi_points = roi_points\n        self.metadata = metadata or {}\n\n    @classmethod\n    def from_dicom_rtstruct(cls, rtstruct_path: str) -> 'StructureSet':\n        rtstruct = dcmread(rtstruct_path, force=True)\n        roi_names = [roi.ROIName for roi in rtstruct.StructureSetROISequence]\n        roi_points = {}\n        for i, name in enumerate(roi_names):\n            try:\n                roi_points[name] = _get_roi_points(rtstruct, i)\n            except AttributeError:\n                warn(f\"Could not get points for ROI {name} (in {rtstruct_path}).\")\n        return cls(roi_points, {})\n\n    @property\n    def roi_names(self) -> List[str]:\n        return list(self.roi_points.keys())"
      },
      {
        "partial": "def get_mask(self, reference_image, mask, label, idx, continuous):\n    size = reference_image.GetSize()[::-1]\n    physical_points = self.roi_points.get(label, np.array([]))\n    mask_points = physical_points_to_idxs(reference_image, physical_points, continuous=continuous)\n    for contour in mask_points:\n        try:\n            z, slice_points = np.unique(contour[:, 0]), contour[:, 1:]\n            if len(z) == 1:\n                # TODO: Implement mask creation for single slice\n            else:\n                raise ValueError(\"This contour is corrupted and spans across 2 or more slices.\")\n        except:\n            # TODO: Implement error handling for boundary cases",
        "complete": "def get_mask(self, reference_image, mask, label, idx, continuous):\n    size = reference_image.GetSize()[::-1]\n    physical_points = self.roi_points.get(label, np.array([]))\n    mask_points = physical_points_to_idxs(reference_image, physical_points, continuous=continuous)\n    for contour in mask_points:\n        try:\n            z, slice_points = np.unique(contour[:, 0]), contour[:, 1:]\n            if len(z) == 1:\n                slice_mask = polygon2mask(size[1:], slice_points)\n                mask[z[0], :, :, idx] += slice_mask\n            else:\n                raise ValueError(\"This contour is corrupted and spans across 2 or more slices.\")\n        except:\n            z = int(np.floor(contour[0, 0]))\n            if z == mask.shape[0]:\n                z -= 1\n            elif z == -1:\n                z += 1\n            elif z > mask.shape[0] or z < -1:\n                raise IndexError(f\"{z} index is out of bounds for image sized {mask.shape}.\")\n            slice_mask = polygon2mask(size[1:], slice_points)\n            mask[z, :, :, idx] += slice_mask"
      }
    ],
    "dependencies": {
      "imports": [
        "re",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "warnings.warn",
        "typing.Dict",
        "pydicom.dcmread",
        "itertools.groupby",
        "skimage.draw.polygon2mask",
        "segmentation.Segmentation",
        "utils.physical_points_to_idxs"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/src/RcppExports.cpp",
    "language": "cpp",
    "content": "// Generated by using Rcpp::compileAttributes() -> do not edit by hand\n// Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393\n\n#include <Rcpp.h>\n\nusing namespace Rcpp;\n\n#ifdef RCPP_USE_GLOBAL_ROSTREAM\nRcpp::Rostream<true>&  Rcpp::Rcout = Rcpp::Rcpp_cout_get();\nRcpp::Rostream<false>& Rcpp::Rcerr = Rcpp::Rcpp_cerr_get();\n#endif\n\n// partialCorQUICKSTOP\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x, SEXP pin_y, SEXP pobsCor, SEXP pGroupFactor, SEXP pGroupSize, SEXP pnumGroup, SEXP pMaxIter, SEXP pn, SEXP preq_alpha, SEXP ptolerance_par, SEXP plog_decision_boundary, SEXP pseed);\nRcppExport SEXP _PharmacoGx_partialCorQUICKSTOP(SEXP pin_xSEXP, SEXP pin_ySEXP, SEXP pobsCorSEXP, SEXP pGroupFactorSEXP, SEXP pGroupSizeSEXP, SEXP pnumGroupSEXP, SEXP pMaxIterSEXP, SEXP pnSEXP, SEXP preq_alphaSEXP, SEXP ptolerance_parSEXP, SEXP plog_decision_boundarySEXP, SEXP pseedSEXP) {\nBEGIN_RCPP\n    Rcpp::RObject rcpp_result_gen;\n    Rcpp::RNGScope rcpp_rngScope_gen;\n    Rcpp::traits::input_parameter< SEXP >::type pin_x(pin_xSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pin_y(pin_ySEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pobsCor(pobsCorSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pGroupFactor(pGroupFactorSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pGroupSize(pGroupSizeSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pnumGroup(pnumGroupSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pMaxIter(pMaxIterSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pn(pnSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type preq_alpha(preq_alphaSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type ptolerance_par(ptolerance_parSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type plog_decision_boundary(plog_decision_boundarySEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pseed(pseedSEXP);\n    rcpp_result_gen = Rcpp::wrap(partialCorQUICKSTOP(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed));\n    return rcpp_result_gen;\nEND_RCPP\n}\n\nstatic const R_CallMethodDef CallEntries[] = {\n    {\"_PharmacoGx_partialCorQUICKSTOP\", (DL_FUNC) &_PharmacoGx_partialCorQUICKSTOP, 12},\n    {NULL, NULL, 0}\n};\n\nRcppExport void R_init_PharmacoGx(DllInfo *dll) {\n    R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n    R_useDynamicSymbols(dll, FALSE);\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `BEGIN_RCPP` and `END_RCPP` macros in this code snippet?",
        "answer": "The `BEGIN_RCPP` and `END_RCPP` macros are used to wrap the main body of the function. They set up a try-catch block to handle exceptions that might occur during the execution of the Rcpp code. This ensures that any C++ exceptions are properly translated into R errors, maintaining a consistent error handling mechanism between R and C++."
      },
      {
        "question": "How does this code handle the input parameters for the `partialCorQUICKSTOP` function?",
        "answer": "The code uses `Rcpp::traits::input_parameter< SEXP >::type` to handle input parameters. Each parameter is declared with this type and initialized with the corresponding SEXP (S-expression) argument. This approach allows the function to accept various R data types as input and automatically convert them to appropriate C++ types, facilitating seamless integration between R and C++."
      },
      {
        "question": "What is the purpose of the `R_init_PharmacoGx` function in this code?",
        "answer": "The `R_init_PharmacoGx` function is an initialization function for the R package. It registers the C++ routines (specifically `_PharmacoGx_partialCorQUICKSTOP`) with R's dynamic symbol mechanism. This registration process links the C++ function to its corresponding R interface, allowing R to call the C++ function. The `R_useDynamicSymbols(dll, FALSE)` call disables dynamic lookup, which can improve performance and security."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/GWC.R",
    "language": "R",
    "content": "#' GWC Score\n#' \n#' @inherit CoreGx::gwc\n#' \n#' @examples\n#' data(CCLEsmall)\n#' x <- molecularProfiles(CCLEsmall,\"rna\")[,1]\n#' y <- molecularProfiles(CCLEsmall,\"rna\")[,2]\n#' x_p <- rep(0.05, times=length(x))\n#' y_p <- rep(0.05, times=length(y))\n#' names(x_p) <- names(x)\n#' names(y_p) <- names(y)\n#' gwc(x,x_p,y,y_p, nperm=100)\n#' \n#' @export\ngwc <-\nfunction (x1, p1, x2, p2, method.cor=c(\"pearson\", \"spearman\"), nperm=1e4, \n          truncate.p=1e-16, ...) {\n  CoreGx::gwc(x1, p1, x2, p2, method.cor, nperm, truncate.p, ...)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `gwc` function in this code snippet, and how does it relate to the `CoreGx` package?",
        "answer": "The `gwc` function in this code snippet is a wrapper for the `CoreGx::gwc` function. It inherits documentation from the `CoreGx::gwc` function and passes all its arguments directly to it. This allows users to call the `gwc` function from the current package while using the implementation from the `CoreGx` package."
      },
      {
        "question": "Explain the significance of the `@inherit CoreGx::gwc` roxygen2 tag in the function documentation.",
        "answer": "The `@inherit CoreGx::gwc` roxygen2 tag is used to inherit the documentation from the `CoreGx::gwc` function. This means that all the parameter descriptions, return value information, and other documentation details from the original `CoreGx::gwc` function will be automatically included in the documentation for this wrapper function. This saves time and ensures consistency in documentation between the wrapper and the original function."
      },
      {
        "question": "What is the purpose of the example provided in the function documentation, and what does it demonstrate?",
        "answer": "The example in the function documentation demonstrates how to use the `gwc` function with sample data. It loads a dataset called 'CCLEsmall', extracts RNA molecular profiles, creates p-value vectors, and then calls the `gwc` function with these inputs. This example serves to illustrate the correct usage of the function, including the format of input data and how to set up the necessary parameters. It helps users understand how to apply the function to their own data."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/dicomutils.py",
    "language": "py",
    "content": "import pydicom\nfrom typing import Dict, TypeVar, Union\nimport copy\n\nT = TypeVar('T')\n\n\ndef get_modality_metadata(dicom_data, modality: str):\n    keys = {'ALL': {'BodyPartExamined': 'BodyPartExamined', \n                    'DataCollectionDiameter': 'DataCollectionDiameter', \n                    'NumberofSlices': 'NumberofSlices', \n                    'SliceThickness': 'SliceThickness', \n                    'ScanType': 'ScanType', \n                    'ScanProgressionDirection': 'ScanProgressionDirection', \n                    'PatientPosition': 'PatientPosition', \n                    'ContrastType': 'ContrastBolusAgent',\n                    'Manufacturer': 'Manufacturer',\n                    'ScanOptions': 'ScanOptions',\n                    'RescaleType': 'RescaleType',\n                    'RescaleSlope': 'RescaleSlope',\n                    'ManufacturerModelName': 'ManufacturerModelName'},\n            'CT': {'KVP': 'KVP',\n                   'XRayTubeCurrent': 'XRayTubeCurrent',\n                   'ScanOptions': 'ScanOptions',\n                   'ReconstructionAlgorithm': 'ReconstructionAlgorithm',\n                   'ContrastFlowRate': 'ContrastFlowRate',\n                   'ContrastFlowDuration': 'ContrastFlowDuration',\n                   'ContrastType': 'ContrastBolusAgent',\n                   'ReconstructionMethod': 'ReconstructionMethod',\n                   'ReconstructionDiameter': 'ReconstructionDiameter',\n                   'ConvolutionKernel': 'ConvolutionKernel'},\n            'MR': ['AcquisitionTime', 'AcquisitionContrast', 'AcquisitionType', 'RepetitionTime', 'EchoTime', 'ImagingFrequency', 'MagneticFieldStrength', 'SequenceName'],\n            'PT': ['RescaleType', 'RescaleSlope', 'RadionuclideTotalDose', 'RadionuclideHalfLife']\n            }\n    \n    # initialize metadata dictionary\n    if modality == 'ALL':\n        metadata = {}\n    else:\n        metadata = all_modalities_metadata(dicom_data)\n    \n    # populating metadata\n    if modality == 'RTSTRUCT':\n        if hasattr(dicom_data, 'StructureSetROISequence'):\n            metadata[\"numROIs\"] = str(len(dicom_data.StructureSetROISequence))\n    elif modality in keys:\n        keys_mod = keys[modality]\n        if isinstance(keys_mod, dict):\n            for k in keys_mod:\n                if hasattr(dicom_data, keys_mod[k]):\n                    metadata[k] = getattr(dicom_data, keys_mod[k])\n        elif isinstance(keys_mod, list):\n            for k in keys_mod:\n                if hasattr(dicom_data, k):\n                    metadata[k] = getattr(dicom_data, k)\n        else:\n            print('WAGUANWAGUANWAGUAN')\n\n    return metadata\n\n\ndef all_modalities_metadata(dicom_data: Union[pydicom.dataset.FileDataset, pydicom.dicomdir.DicomDir]) -> Dict[str, T]:\n    metadata = get_modality_metadata(dicom_data, 'ALL')\n    \n    if hasattr(dicom_data, 'PixelSpacing') and hasattr(dicom_data, 'SliceThickness'):\n        pixel_size = copy.copy(dicom_data.PixelSpacing)\n        pixel_size.append(dicom_data.SliceThickness)\n        metadata[\"PixelSize\"] = str(tuple(pixel_size))\n    \n    return metadata\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `get_modality_metadata` function and how does it handle different modalities?",
        "answer": "The `get_modality_metadata` function extracts metadata from DICOM data based on the specified modality. It uses a dictionary of keys for different modalities (ALL, CT, MR, PT) to determine which attributes to extract. For 'ALL' modality, it initializes an empty dictionary, while for other modalities, it first calls `all_modalities_metadata`. The function then populates the metadata dictionary based on the modality-specific keys, handling both dictionary and list type key definitions. For 'RTSTRUCT' modality, it specifically checks for the number of ROIs."
      },
      {
        "question": "How does the function handle the case when the modality is not found in the `keys` dictionary?",
        "answer": "The function doesn't explicitly handle the case when the modality is not found in the `keys` dictionary. If an unknown modality is passed, the function will return the metadata dictionary populated by `all_modalities_metadata` without adding any modality-specific information. This could potentially lead to unexpected behavior or incomplete metadata for unsupported modalities. A potential improvement would be to add error handling or a default case for unknown modalities."
      },
      {
        "question": "What is the purpose of the `all_modalities_metadata` function and how does it differ from `get_modality_metadata`?",
        "answer": "The `all_modalities_metadata` function is used to extract common metadata applicable to all modalities. It first calls `get_modality_metadata` with 'ALL' as the modality to get general metadata. Then, it specifically checks for 'PixelSpacing' and 'SliceThickness' attributes to calculate and add a 'PixelSize' entry to the metadata. This function is called by `get_modality_metadata` for all non-'ALL' modalities to ensure that common metadata is always included before adding modality-specific information."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def get_modality_metadata(dicom_data, modality: str):\n    keys = {\n        'ALL': {'BodyPartExamined': 'BodyPartExamined', 'DataCollectionDiameter': 'DataCollectionDiameter'},\n        'CT': {'KVP': 'KVP', 'XRayTubeCurrent': 'XRayTubeCurrent'},\n        'MR': ['AcquisitionTime', 'AcquisitionContrast'],\n        'PT': ['RescaleType', 'RescaleSlope']\n    }\n    \n    metadata = all_modalities_metadata(dicom_data) if modality != 'ALL' else {}\n    \n    if modality == 'RTSTRUCT':\n        if hasattr(dicom_data, 'StructureSetROISequence'):\n            metadata['numROIs'] = str(len(dicom_data.StructureSetROISequence))\n    elif modality in keys:\n        keys_mod = keys[modality]\n        if isinstance(keys_mod, dict):\n            for k, v in keys_mod.items():\n                if hasattr(dicom_data, v):\n                    metadata[k] = getattr(dicom_data, v)\n        elif isinstance(keys_mod, list):\n            for k in keys_mod:\n                if hasattr(dicom_data, k):\n                    metadata[k] = getattr(dicom_data, k)\n    \n    return metadata",
        "complete": "def get_modality_metadata(dicom_data, modality: str):\n    keys = {\n        'ALL': {'BodyPartExamined': 'BodyPartExamined', 'DataCollectionDiameter': 'DataCollectionDiameter', 'NumberofSlices': 'NumberofSlices', 'SliceThickness': 'SliceThickness', 'ScanType': 'ScanType', 'ScanProgressionDirection': 'ScanProgressionDirection', 'PatientPosition': 'PatientPosition', 'ContrastType': 'ContrastBolusAgent', 'Manufacturer': 'Manufacturer', 'ScanOptions': 'ScanOptions', 'RescaleType': 'RescaleType', 'RescaleSlope': 'RescaleSlope', 'ManufacturerModelName': 'ManufacturerModelName'},\n        'CT': {'KVP': 'KVP', 'XRayTubeCurrent': 'XRayTubeCurrent', 'ScanOptions': 'ScanOptions', 'ReconstructionAlgorithm': 'ReconstructionAlgorithm', 'ContrastFlowRate': 'ContrastFlowRate', 'ContrastFlowDuration': 'ContrastFlowDuration', 'ContrastType': 'ContrastBolusAgent', 'ReconstructionMethod': 'ReconstructionMethod', 'ReconstructionDiameter': 'ReconstructionDiameter', 'ConvolutionKernel': 'ConvolutionKernel'},\n        'MR': ['AcquisitionTime', 'AcquisitionContrast', 'AcquisitionType', 'RepetitionTime', 'EchoTime', 'ImagingFrequency', 'MagneticFieldStrength', 'SequenceName'],\n        'PT': ['RescaleType', 'RescaleSlope', 'RadionuclideTotalDose', 'RadionuclideHalfLife']\n    }\n    \n    metadata = all_modalities_metadata(dicom_data) if modality != 'ALL' else {}\n    \n    if modality == 'RTSTRUCT':\n        if hasattr(dicom_data, 'StructureSetROISequence'):\n            metadata['numROIs'] = str(len(dicom_data.StructureSetROISequence))\n    elif modality in keys:\n        keys_mod = keys[modality]\n        if isinstance(keys_mod, dict):\n            for k, v in keys_mod.items():\n                if hasattr(dicom_data, v):\n                    metadata[k] = getattr(dicom_data, v)\n        elif isinstance(keys_mod, list):\n            for k in keys_mod:\n                if hasattr(dicom_data, k):\n                    metadata[k] = getattr(dicom_data, k)\n    \n    return metadata"
      },
      {
        "partial": "def all_modalities_metadata(dicom_data: Union[pydicom.dataset.FileDataset, pydicom.dicomdir.DicomDir]) -> Dict[str, T]:\n    metadata = get_modality_metadata(dicom_data, 'ALL')\n    \n    # Add code here to handle PixelSpacing and SliceThickness\n    \n    return metadata",
        "complete": "def all_modalities_metadata(dicom_data: Union[pydicom.dataset.FileDataset, pydicom.dicomdir.DicomDir]) -> Dict[str, T]:\n    metadata = get_modality_metadata(dicom_data, 'ALL')\n    \n    if hasattr(dicom_data, 'PixelSpacing') and hasattr(dicom_data, 'SliceThickness'):\n        pixel_size = copy.copy(dicom_data.PixelSpacing)\n        pixel_size.append(dicom_data.SliceThickness)\n        metadata['PixelSize'] = str(tuple(pixel_size))\n    \n    return metadata"
      }
    ],
    "dependencies": {
      "imports": [
        "pydicom",
        "copy"
      ],
      "from_imports": [
        "typing.Dict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeDrugSensitivity.R",
    "language": "R",
    "content": "#' @importFrom BiocParallel bplapply\n.calculateSensitivitiesStar <- function (pSets=list(), exps=NULL, cap=NA,\n        na.rm=TRUE, area.type=c(\"Fitted\", \"Actual\"), nthread=1) {\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    }\n    if (is.null(exps)) {\n        stop(\"expriments is empty!\")\n    }\n    for (study in names(pSets)) {\n        sensitivityProfiles(pSets[[study]])$auc_recomputed_star <- NA\n    }\n    if (!is.na(cap)) {\n        trunc <- TRUE\n    }else{\n        trunc <- FALSE\n    }\n\n    for (i in seq_len(nrow(exps))) {\n        ranges <- list()\n        for (study in names(pSets)) {\n            ranges[[study]] <- as.numeric(sensitivityRaw(pSets[[study]])[\n                exps[i, study], , \"Dose\"\n            ])\n        }\n        ranges <- .getCommonConcentrationRange(ranges)\n        names(ranges) <- names(pSets)\n        for (study in names(pSets)) {\n            myx <- as.numeric(sensitivityRaw(pSets[[study]])[\n                exps[i, study],,\"Dose\"]) %in% ranges[[study]\n            ]\n            sensitivityRaw(pSets[[study]])[exps[i, study], !myx, ] <- NA\n        }\n    }\n\n    op <- options()\n    options(mc.cores=nthread)\n    on.exit(options(op))\n\n    for (study in names(pSets)) {\n        auc_recomputed_star <- unlist(\n            bplapply(rownames(sensitivityRaw(pSets[[study]])),\n                FUN=function(experiment, exps, study, dataset, area.type) {\n                    if (!experiment %in% exps[,study]) return(NA_real_)\n                    return(computeAUC(\n                        concentration=as.numeric(dataset[experiment, , 1]),\n                        viability=as.numeric(dataset[experiment, , 2]),\n                        trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE,\n                        area.type=area.type\n                        ) / 100\n                        )\n                    },\n                exps=exps, study=study, dataset=sensitivityRaw(pSets[[study]]),\n                area.type=area.type)\n            )\n        sensitivityProfiles(pSets[[study]])$auc_recomputed_star <-\n            auc_recomputed_star\n    }\n    return(pSets)\n}\n\n## This function computes AUC for the whole raw sensitivity data of a pset\n.calculateFromRaw <- function(raw.sensitivity, cap=NA, nthread=1,\n        family=c(\"normal\", \"Cauchy\"), scale=0.07, n=1) {\n    family <- match.arg(family)\n\n    AUC <- vector(length=dim(raw.sensitivity)[1])\n    names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n    IC50 <- vector(length=dim(raw.sensitivity)[1])\n    names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n    trunc <- !is.na(cap)\n\n    if (nthread == 1) {\n        pars <- lapply(names(AUC),\n            FUN=function(exp, raw.sensitivity, family, scale, n) {\n                if (length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 ||\n                        all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n                    NA\n                } else{\n                    logLogisticRegression(raw.sensitivity[exp, , \"Dose\"],\n                        raw.sensitivity[exp, , \"Viability\"], trunc=trunc,\n                        conc_as_log=FALSE, viability_as_pct=TRUE, family=family,\n                        scale=scale, median_n=n)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, family=family, scale=scale,\n            n=n\n        )\n        names(pars) <- dimnames(raw.sensitivity)[[1]]\n        AUC <- unlist(lapply(names(pars),\n            FUN=function(exp, raw.sensitivity, pars) {\n                if (any(is.na(pars[[exp]]))) {\n                    NA\n                } else{\n                    computeAUC(concentration=raw.sensitivity[exp, , \"Dose\"],\n                        Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE,\n                        viability_as_pct=TRUE)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, pars=pars\n        ))\n        IC50 <- unlist(lapply(names(pars), function(exp, pars) {\n            if (any(is.na(pars[[exp]]))) {\n                NA\n            } else{\n                computeIC50(Hill_fit=pars[[exp]], trunc=trunc,\n                    conc_as_log=FALSE, viability_as_pct=TRUE)\n            }\n        }, pars=pars))\n    } else {\n        pars <- parallel::mclapply(names(AUC),\n            FUN=function(exp, raw.sensitivity, family, scale, n, trunc) {\n                if (length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 ||\n                        all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n                    NA\n                } else {\n                    logLogisticRegression(\n                        raw.sensitivity[exp, , \"Dose\"],\n                        raw.sensitivity[exp, , \"Viability\"],\n                        trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE,\n                        family=family, scale=scale, median_n=n)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, family=family, scale=scale, n=n,\n            trunc=trunc, mc.cores=nthread\n        )\n        names(pars) <- dimnames(raw.sensitivity)[[1]]\n        AUC <- unlist(parallel::mclapply(names(pars),\n            FUN=function(exp, raw.sensitivity, pars, trunc) {\n                if (any(is.na(pars[[exp]]))) {\n                    NA\n                } else{\n                    computeAUC(\n                        concentration=raw.sensitivity[exp, , \"Dose\"],\n                        Hill_fit=pars[[exp]],\n                        trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, pars=pars, trunc=trunc,\n            mc.cores=nthread\n        ))\n        IC50 <- unlist(parallel::mclapply(names(pars),\n            FUN=function(exp, pars, trunc) {\n                if(any(is.na(pars[[exp]]))) {\n                    NA\n                } else{\n                    computeIC50(Hill_fit=pars[[exp]], trunc=trunc,\n                        conc_as_log=FALSE, viability_as_pct=TRUE)\n                }\n            }, pars=pars, trunc=trunc, mc.cores=nthread\n        ))\n    }\n    names(AUC) <- dimnames(raw.sensitivity)[[1]]\n    names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n    return(list(\"AUC\"=AUC, \"IC50\"=IC50, \"pars\"=pars))\n}\n\n\n## This function computes intersected concentration range between a list of\n## concentration ranges\n.getCommonConcentrationRange <- function(doses) {\n    min.dose <- 0\n    max.dose <- 10^100\n    for (i in seq_len(length(doses))) {\n        min.dose <- max(min.dose, min(as.numeric(doses[[i]]), na.rm=TRUE),\n            na.rm=TRUE)\n        max.dose <- min(max.dose, max(as.numeric(doses[[i]]), na.rm=TRUE),\n            na.rm=TRUE)\n    }\n    common.ranges <- list()\n    for (i in seq_len(length(doses))) {\n        common.ranges[[i]] <- doses[[i]][\n            seq(which.min(abs(as.numeric(doses[[i]]) - min.dose)), max(\n                which(abs(as.numeric(doses[[i]]) - max.dose) ==\n                    min(abs(as.numeric(doses[[i]]) - max.dose), na.rm=TRUE)\n                ))\n            )\n        ]\n    }\n    return(common.ranges)\n}\n\n## predict viability from concentration data and curve parameters\n.Hill <- function(x, pars) {\n    return(pars[2] + (1 - pars[2]) / (1 + (10 ^ x / 10 ^ pars[3]) ^ pars[1]))\n}\n\n## calculate residual of fit\n## FIXME:: Why is this different from CoreGx?\n#' @importFrom CoreGx .dmedncauchys .dmednnormals .edmednnormals .edmedncauchys\n.residual <- function(x, y, n, pars, scale=0.07, family=c(\"normal\", \"Cauchy\"),\n        trunc=FALSE) {\n    family <- match.arg(family)\n    Cauchy_flag=(family == \"Cauchy\")\n    if (Cauchy_flag == FALSE) {\n        # return(sum((.Hill(x, pars) - y) ^ 2))\n        diffs <- .Hill(x, pars)-y\n        if (trunc == FALSE) {\n            return(sum(-log(.dmednnormals(diffs, n, scale))))\n        } else {\n            down_truncated <- abs(y) >= 1\n            up_truncated <- abs(y) <= 0\n\n            # For up truncated, integrate the cauchy dist up until -\n            #>because anything less gets truncated to 0, and thus the residual\n            #>is -diff, and the prob function becomes discrete For\n            #>down_truncated, 1-cdf(diffs)=cdf(-diffs)\n            return(\n                sum(-log(.dmednnormals(diffs[!(down_truncated | up_truncated)],\n                    n, scale))) +\n                sum(-log(.edmednnormals(-diffs[up_truncated | down_truncated],\n                    n, scale)))\n            )\n\n        }\n    } else {\n        diffs <- .Hill(x, pars) - y\n        if (trunc == FALSE) {\n            return(sum(-log(.dmedncauchys(diffs, n, scale))))\n        } else {\n            down_truncated <- abs(y) >= 1\n            up_truncated <- abs(y) <= 0\n            # For up truncated, integrate the cauchy dist up until -diff because\n            #> anything less gets truncated to 0, and thus the residual is -diff,\n            #>and the prob function becomes discrete For down_truncated,\n            #>1 - cdf(diffs) = cdf(-diffs)\n            return(\n                sum(-log(.dmedncauchys(diffs[!(down_truncated | up_truncated)],\n                    n, scale))) +\n                sum(-log(.edmedncauchys(-diffs[up_truncated | down_truncated],\n                    n, scale))))\n        }\n    }\n}\n\n##FIXME:: Why is this different from CoreGx?\n.meshEval <- function(log_conc, viability, lower_bounds=c(0, 0, -6),\n        upper_bounds=c(4, 1, 6), density=c(2, 10, 2), scale=0.07, n=1,\n        family=c(\"normal\", \"Cauchy\"), trunc=FALSE) {\n    family <- match.arg(family)\n    guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n        pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n        pmin(pmax(log_conc[which.min(abs(viability - 1 / 2))], lower_bounds[3]),\n        upper_bounds[3]))\n    guess_residual <- .residual(log_conc, viability, pars=guess, n=n,\n        scale=scale, family=family, trunc=trunc)\n    for (i in seq(from=lower_bounds[1], to=upper_bounds[1],\n            by=1 / density[1])) {\n        for (j in seq(from=lower_bounds[2], to=upper_bounds[2],\n                by=1 / density[2])) {\n            for (k in seq(from=lower_bounds[3], to=upper_bounds[3],\n                    by=1 / density[3])) {\n                test_guess_residual <- .residual(log_conc, viability,\n                    pars=c(i, j, k), n=n, scale=scale, family=family,\n                    trunc=trunc)\n                if (!is.finite(test_guess_residual)) {\n                    warning(paste0(\" Test Guess Residual is: \",\n                        test_guess_residual, \"\\n Other Pars: log_conc: \",\n                        paste(log_conc, collapse=\", \"), \"\\n Viability: \",\n                        paste(viability, collapse=\", \"), \"\\n Scale: \", scale,\n                        \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \",\n                        i, \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n                }\n                if (!length(test_guess_residual)) {\n                    warning(paste0(\" Test Guess Residual is: \",\n                        test_guess_residual,  \"\\n Other Pars: log_conc: \",\n                        paste(log_conc, collapse=\", \"), \"\\n Viability: \",\n                        paste(viability, collapse=\", \"), \"\\n Scale: \", scale,\n                        \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \", i,\n                        \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n                }\n                if (test_guess_residual < guess_residual) {\n                    guess <- c(i, j, k)\n                    guess_residual <- test_guess_residual\n                }\n            }\n        }\n    }\n    return(guess)\n}\n\n## FIXME:: Documentation?\n#  Fits dose-response curves to data given by the user\n#  and returns the AUC of the fitted curve, normalized to the length of the concentration range.\n#\n#  @param concentration `numeric` is a vector of drug concentrations.\n#\n#  @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#  drug concentrations whose logarithms are in the corresponding entries of the log_conc, expressed as percentages\n#  of viability in the absence of any drug.\n#\n#  @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#  curve-fitting is performed.\n#' @importFrom CoreGx .getSupportVec\n#' @export\n#' @keywords internal\n.computeAUCUnderFittedCurve <- function(concentration, viability, trunc=TRUE,\n        verbose=FALSE) {\n    log_conc <- concentration\n    #FIT CURVE AND CALCULATE IC50\n    pars <- unlist(logLogisticRegression(log_conc, viability,\n        conc_as_log=TRUE, viability_as_pct=FALSE, trunc=trunc))\n    x <- .getSupportVec(log_conc)\n    return(1 - trapz(x, .Hill(x, pars)) /\n        (log_conc[length(log_conc)] - log_conc[1]))\n}\n\n#This function is being used in computeSlope\n.optimizeRegression <- function(x, y, x0=-3, y0=100) {\n    beta1 <- (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n    return(beta1)\n}\n\nupdateMaxConc <- function(pSet) {\n    sensitivityInfo(pSeto)$max.conc <- apply(sensitivityRaw(pSet)[, , \"Dose\"],\n        1, max, na.rm=TRUE)\n    return(pSet)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the '.calculateSensitivitiesStar' function and what are its main input parameters?",
        "answer": "The '.calculateSensitivitiesStar' function is designed to calculate sensitivities for multiple pharmacological datasets (pSets). Its main input parameters are: 'pSets' (a list of pharmacological datasets), 'exps' (experiments to analyze), 'cap' (for truncation), 'area.type' (either 'Fitted' or 'Actual'), and 'nthread' (for parallel processing). The function recomputes AUC (Area Under the Curve) values for specified experiments across multiple studies, considering common concentration ranges."
      },
      {
        "question": "How does the '.getCommonConcentrationRange' function work, and why is it important in the context of the '.calculateSensitivitiesStar' function?",
        "answer": "The '.getCommonConcentrationRange' function computes the intersected concentration range between a list of concentration ranges. It finds the maximum of the minimum doses and the minimum of the maximum doses across all input ranges, then selects the doses within this common range for each input. This function is important in '.calculateSensitivitiesStar' because it ensures that sensitivity calculations are performed on a consistent concentration range across different studies, allowing for fair comparisons and standardized analysis of drug responses."
      },
      {
        "question": "What is the purpose of the '.residual' function, and how does it handle different statistical families and truncation?",
        "answer": "The '.residual' function calculates the residual of the fit between observed and predicted values in a dose-response curve. It supports two statistical families: normal and Cauchy. For each family, it computes the negative log-likelihood of the differences between predicted and observed values. When truncation is enabled, it handles up-truncated (y <= 0) and down-truncated (y >= 1) cases separately, using the cumulative distribution function for these cases. This function is crucial for assessing the goodness of fit in the dose-response curve modeling process, adapting to different statistical assumptions and data constraints."
      }
    ],
    "completion_tasks": [
      {
        "partial": "updateMaxConc <- function(pSet) {\n    sensitivityInfo(pSeto)$max.conc <- apply(sensitivityRaw(pSet)[, , \"Dose\"],\n        1, max, na.rm=TRUE)\n    return(pSet)\n}",
        "complete": "updateMaxConc <- function(pSet) {\n    sensitivityInfo(pSet)$max.conc <- apply(sensitivityRaw(pSet)[, , \"Dose\"],\n        1, max, na.rm=TRUE)\n    return(pSet)\n}"
      },
      {
        "partial": ".optimizeRegression <- function(x, y, x0=-3, y0=100) {\n    beta1 <- (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n    return(beta1)\n}",
        "complete": ".optimizeRegression <- function(x, y, x0=-3, y0=100) {\n    beta1 <- (sum(x * y) - y0 * sum(x)) / (sum(x^2) - x0 * sum(x))\n    return(beta1)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test-utils.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.asDT` function in the given code snippet?",
        "answer": "The `.asDT` function is used to convert a list into a data.table object. In this test, it takes a list `l` with two elements (Name and Age) and converts it into a data.table, which is then verified using the `expect_data_table` function from the testthat package."
      },
      {
        "question": "Which R packages are being used in this code snippet and what are their purposes?",
        "answer": "The code snippet uses three R packages: 1) AnnotationGx: likely contains the `.asDT` function being tested, 2) testthat: provides functions for unit testing in R, such as `test_that` and `expect_data_table`, 3) checkmate: although imported, it's not explicitly used in this snippet but typically provides functions for argument checking in R functions."
      },
      {
        "question": "How is the test case structured in this code, and what is it verifying?",
        "answer": "The test case is structured using the `test_that` function from the testthat package. It creates a list `l`, converts it to a data.table using `.asDT`, and then uses `expect_data_table` to verify that the result is indeed a data.table object. This test is checking if the `.asDT` function correctly converts a list to a data.table."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  # Add expectation here\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})"
      },
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  # Create a list here\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/ops/__init__.py",
    "language": "py",
    "content": "from .ops import *\n",
    "qa_pairs": [
      {
        "question": "What does the line 'from .ops import *' do in Python, and what potential issues might it introduce?",
        "answer": "The line 'from .ops import *' imports all names from the 'ops' module in the current package. This is known as a wildcard import. While convenient, it can introduce several issues: 1) It can pollute the namespace with unexpected names, 2) It makes it unclear which names are being used from the imported module, 3) It can lead to name conflicts if multiple modules are imported this way, and 4) It goes against PEP 8 style guidelines. It's generally recommended to use explicit imports instead."
      },
      {
        "question": "How would you modify the import statement to follow Python best practices?",
        "answer": "To follow Python best practices, you should avoid wildcard imports and instead explicitly import the names you need. For example: 'from .ops import function1, function2, Class1'. If you need to import many names, you can use: 'import .ops as ops' and then use 'ops.function1()' syntax. This makes the code more readable and reduces the risk of namespace pollution."
      },
      {
        "question": "What does the dot (.) before 'ops' in the import statement signify?",
        "answer": "The dot (.) before 'ops' in the import statement indicates a relative import. It means that the 'ops' module is located in the same package as the current module. Relative imports are used to organize related modules within a package. In this case, it's importing from a sibling module named 'ops' within the same package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .ops import *\n\ndef calculate_result(a, b):\n    # Perform addition\n    result = add(a, b)\n    \n    # Perform multiplication\n    result = multiply(result, 2)\n    \n    # Perform subtraction\n    result = ",
        "complete": "from .ops import *\n\ndef calculate_result(a, b):\n    # Perform addition\n    result = add(a, b)\n    \n    # Perform multiplication\n    result = multiply(result, 2)\n    \n    # Perform subtraction\n    result = subtract(result, 5)\n    \n    return result"
      },
      {
        "partial": "from .ops import *\n\nclass Calculator:\n    def __init__(self, initial_value):\n        self.value = initial_value\n    \n    def add(self, x):\n        self.value = add(self.value, x)\n        return self\n    \n    def multiply(self, x):\n        ",
        "complete": "from .ops import *\n\nclass Calculator:\n    def __init__(self, initial_value):\n        self.value = initial_value\n    \n    def add(self, x):\n        self.value = add(self.value, x)\n        return self\n    \n    def multiply(self, x):\n        self.value = multiply(self.value, x)\n        return self\n    \n    def get_result(self):\n        return self.value"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "ops.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/PharmacoSet-utils.R",
    "language": "R",
    "content": "#' @include PharmacoSet-class.R PharmacoSet-accessors.R\nNULL\n\n.local_class <- 'PharmacoSet'\n.local_data <- 'CCLEsmall'\n.local_treatment <- 'drug'\n\n\n#### PharmacoGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown=TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n# ===================================\n# Utility Method Documentation Object\n# -----------------------------------\n\n\n#' @name PharmacoSet-utils\n#' @eval CoreGx:::.docs_CoreSet_utils(class_=.local_class)\n#' @eval .parseToRoxygen(\"@examples data({data_})\", data_=.local_data)\nNULL\n\n\n# ======================================\n# Subset Methods\n# --------------------------------------\n\n\n\n## ===================\n## ---- subsetBySample\n## -------------------\n\n\n#' @rdname PharmacoSet-utils\n#' @importMethodsFrom CoreGx subsetBySample\n#' @eval CoreGx:::.docs_CoreSet_subsetBySample(class_=.local_class,\n#' data_=.local_data)\nsetMethod('subsetBySample', signature(x='PharmacoSet'), function(x, samples) {\n    callNextMethod(x=x, samples=samples)\n})\n\n\n## ======================\n## ---- subsetByTreatment\n## ----------------------\n\n\n#' @rdname PharmacoSet-utils\n#' @importMethodsFrom CoreGx subsetByTreatment\n#' @eval CoreGx:::.docs_CoreSet_subsetByTreatment(class_=.local_class, \n#' data_=.local_data, treatment_=.local_treatment)\nsetMethod('subsetByTreatment', signature(x='PharmacoSet'),\n        function(x, treatments) {\n    callNextMethod(x=x, treatments=treatments)\n})\n\n\n## ====================\n## ---- subsetByFeature\n## --------------------\n\n\n#' @rdname PharmacoSet-utils\n#' @importFrom CoreGx subsetByFeature\n#' @eval CoreGx:::.docs_CoreSet_subsetByFeature(class_=.local_class, \n#' data_=.local_data)\nsetMethod('subsetByFeature', signature(x='PharmacoSet'), \n        function(x, features, mDataTypes) {\n    callNextMethod(x=x, features=features, mDataTypes)\n})\n\n## ===========\n## ---- subset\n## -----------\n\n#'\n#' \n#' \nsetMethod('subset', signature('PharmacoSet'),\n        function(x, samples, treatments, features, ..., mDataTypes) {\n    callNextMethod(x=x, samples=samples, treatments=treatments, \n        features=features, ..., mDataTypes=mDataTypes)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.local_class`, `.local_data`, and `.local_treatment` variables in this code snippet?",
        "answer": "These variables are used for dynamic documentation generation. `.local_class` is set to 'PharmacoSet', `.local_data` to 'CCLEsmall', and `.local_treatment` to 'drug'. They are likely used in the `@eval` tags to customize the documentation for specific classes, datasets, and treatment types."
      },
      {
        "question": "How does the `subsetBySample` method for the `PharmacoSet` class differ from its parent class implementation?",
        "answer": "The `subsetBySample` method for the `PharmacoSet` class doesn't introduce any new functionality. It uses `callNextMethod(x=x, samples=samples)` to call the parent class implementation, passing along the same arguments. This suggests that the `PharmacoSet` class inherits this method's behavior from its parent class, likely `CoreSet`."
      },
      {
        "question": "What is the purpose of the `@eval` tags used in the Roxygen documentation blocks?",
        "answer": "The `@eval` tags in the Roxygen documentation blocks are used for dynamic documentation generation. They evaluate R expressions to generate documentation content at compile-time. For example, `@eval CoreGx:::.docs_CoreSet_subsetBySample(class_=.local_class, data_=.local_data)` likely generates documentation specific to the `PharmacoSet` class and the 'CCLEsmall' dataset for the `subsetBySample` method."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('subsetBySample', signature(x='PharmacoSet'), function(x, samples) {\n    # Complete the function body\n})",
        "complete": "setMethod('subsetBySample', signature(x='PharmacoSet'), function(x, samples) {\n    callNextMethod(x=x, samples=samples)\n})"
      },
      {
        "partial": "setMethod('subset', signature('PharmacoSet'),\n        function(x, samples, treatments, features, ..., mDataTypes) {\n    # Complete the function body\n})",
        "complete": "setMethod('subset', signature('PharmacoSet'),\n        function(x, samples, treatments, features, ..., mDataTypes) {\n    callNextMethod(x=x, samples=samples, treatments=treatments, \n        features=features, ..., mDataTypes=mDataTypes)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_status.R",
    "language": "R",
    "content": "#' Retrieves the status of a PubChem request\n#'\n#' This function sends a request to PubChem to retrieve the status of a given URL.\n#' It returns the status code and, if specified, the parsed information from the response.\n#'\n#' @param returnMessage Logical indicating whether to return the parsed information from the response.\n#' @param printMessage Logical indicating whether to print the status message.\n#' @param url The URL to send the request to. Default is \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\".\n#'\n#' @return The status code of the response. If \\code{returnMessage} is \\code{TRUE}, the parsed information from the response is also returned.\n#'\n#' @examples\n#' getPubchemStatus()\n#' getPubchemStatus(returnMessage = TRUE)\n#' getPubchemStatus(printMessage = FALSE)\n#'\n#' @export\ngetPubchemStatus <- function(\n    returnMessage = FALSE, printMessage = TRUE,\n    url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\") {\n  funContext <- .funContext(\"getPubchemStatus\")\n\n  request <- .buildURL(url) |> .build_pubchem_request()\n\n  # need to do NULL while loop bc sometimes X-Throttling-Control is not in the response\n  message <- NULL\n\n  while(is.null(message)) {\n    response <- httr2::req_perform(request)\n\n    if (httr2::resp_status(response) == 200) {\n      message <- response$headers[[\"X-Throttling-Control\"]]\n    } else {\n      .warn(\"Request failed. Retrying...\")\n      Sys.sleep(1)\n    }\n  }\n  parsed_info <- .checkThrottlingStatus2(message, printMessage)\n  if (returnMessage) {\n    return(parsed_info)\n  }\n}\n\n\n\n#' names are: request_count, request_time and service\n#' each has status and percent\n#' main throttlers for user are request_count and request_time\n#' main statuses are:\n#'  Green - less than 50% of the permitted request limit has been used\n#'  Yellow - between 50% and 75% of the request limit has been used\n#'  Red - more than 75% of the request limit has been reached\n#'  Black - the limit has been exceeded and requests are being blocked\n#'\n#' @noRd\n#' @keywords internal\n.checkThrottlingStatus2 <- function(message, printMessage) {\n  parsed_info <- .parse_throttling_message(message)\n  if (printMessage) {\n    message(\"Throttling status:\\n\", paste0(strsplit(message, \", \")[[1]], collapse = \"\\n\"))\n  }\n  # Check if the request count or request time is\n  if (parsed_info$service$status == \"Black\") {\n    .warn(\"The request limit has been exceeded and requests are being blocked.\")\n  } else if (parsed_info$service$status %in% c(\"Red\", \"Yellow\")) {\n    .warn(\"The request limit has been reached or is close to being reached.\")\n  } else {\n    .debug(\"The request limit is not close to being reached.\")\n  }\n  return(parsed_info)\n}\n\n#' Parses the throttling message from the response\n#' @noRd\n#' @keywords internal\n.parse_throttling_message <- function(message) {\n  # Split the message into components\n  components <- strsplit(message, \", \")[[1]]\n\n  # Initialize an empty list to store the parsed information\n  parsed_info <- list()\n\n  # Loop through each component and extract the relevant information\n  for (comp in components) {\n    # Split each component into key-value pairs\n    kv <- strsplit(comp, \": \")[[1]]\n    key <- tolower(gsub(\" status\", \"\", kv[1]))\n    key <- gsub(\" \", \"_\", key)\n    value <- kv[2]\n\n    # Extract status and percent\n    status <- sub(\"\\\\s*\\\\(.*\\\\)\", \"\", value)\n    percent <- as.integer(sub(\".*\\\\((\\\\d+)%\\\\).*\", \"\\\\1\", value))\n\n    # Store the extracted information in the parsed_info list\n    parsed_info[[key]] <- list(status = status, percent = percent)\n  }\n\n  return(parsed_info)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemStatus` function and what are its main parameters?",
        "answer": "The `getPubchemStatus` function retrieves the status of a PubChem request. It sends a request to a specified URL (defaulting to a PubChem API endpoint for Aspirin) and returns the status code. Its main parameters are: `returnMessage` (boolean to determine if parsed information should be returned), `printMessage` (boolean to control whether status messages are printed), and `url` (the URL to send the request to)."
      },
      {
        "question": "How does the function handle potential request failures and throttling?",
        "answer": "The function uses a while loop to retry the request if it fails (status code != 200). It checks the 'X-Throttling-Control' header in the response to determine the throttling status. The `.checkThrottlingStatus2` function is then used to parse this information and provide warnings based on the throttling status (Green, Yellow, Red, or Black). If the status is Red or Yellow, it warns that the request limit is close to being reached, and if it's Black, it warns that requests are being blocked."
      },
      {
        "question": "Explain the purpose and functionality of the `.parse_throttling_message` function.",
        "answer": "The `.parse_throttling_message` function is an internal helper function that parses the throttling message received from the PubChem API. It splits the message into components, extracts key information such as request count, request time, and service status. For each component, it extracts the status (e.g., Green, Yellow, Red, Black) and the percentage of the limit used. The function returns a structured list containing this parsed information, which is then used by the `.checkThrottlingStatus2` function to determine appropriate warnings or messages to display."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getPubchemStatus <- function(returnMessage = FALSE, printMessage = TRUE, url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\") {\n  funContext <- .funContext(\"getPubchemStatus\")\n  request <- .buildURL(url) |> .build_pubchem_request()\n  message <- NULL\n\n  while(is.null(message)) {\n    response <- httr2::req_perform(request)\n    if (httr2::resp_status(response) == 200) {\n      message <- response$headers[[\"X-Throttling-Control\"]]\n    } else {\n      .warn(\"Request failed. Retrying...\")\n      Sys.sleep(1)\n    }\n  }\n  parsed_info <- .checkThrottlingStatus2(message, printMessage)\n  # Complete the function here\n}",
        "complete": "getPubchemStatus <- function(returnMessage = FALSE, printMessage = TRUE, url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\") {\n  funContext <- .funContext(\"getPubchemStatus\")\n  request <- .buildURL(url) |> .build_pubchem_request()\n  message <- NULL\n\n  while(is.null(message)) {\n    response <- httr2::req_perform(request)\n    if (httr2::resp_status(response) == 200) {\n      message <- response$headers[[\"X-Throttling-Control\"]]\n    } else {\n      .warn(\"Request failed. Retrying...\")\n      Sys.sleep(1)\n    }\n  }\n  parsed_info <- .checkThrottlingStatus2(message, printMessage)\n  if (returnMessage) {\n    return(parsed_info)\n  }\n}"
      },
      {
        "partial": ".parse_throttling_message <- function(message) {\n  components <- strsplit(message, \", \")[[1]]\n  parsed_info <- list()\n\n  for (comp in components) {\n    kv <- strsplit(comp, \": \")[[1]]\n    key <- tolower(gsub(\" status\", \"\", kv[1]))\n    key <- gsub(\" \", \"_\", key)\n    value <- kv[2]\n\n    # Extract status and percent\n    status <- sub(\"\\\\s*\\\\(.*\\\\)\", \"\", value)\n    percent <- as.integer(sub(\".*\\\\((\\\\d+)%\\\\).*\", \"\\\\1\", value))\n\n    # Complete the function here\n  }\n\n  return(parsed_info)\n}",
        "complete": ".parse_throttling_message <- function(message) {\n  components <- strsplit(message, \", \")[[1]]\n  parsed_info <- list()\n\n  for (comp in components) {\n    kv <- strsplit(comp, \": \")[[1]]\n    key <- tolower(gsub(\" status\", \"\", kv[1]))\n    key <- gsub(\" \", \"_\", key)\n    value <- kv[2]\n\n    status <- sub(\"\\\\s*\\\\(.*\\\\)\", \"\", value)\n    percent <- as.integer(sub(\".*\\\\((\\\\d+)%\\\\).*\", \"\\\\1\", value))\n\n    parsed_info[[key]] <- list(status = status, percent = percent)\n  }\n\n  return(parsed_info)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/chembl_helpers.R",
    "language": "R",
    "content": "#' Internal function that returns all possible chembl resources\n#' @keywords internal\n#' @noRd\n.chembl_resources <- function() {\n  c(\n    \"activity\", \"assay\", \"atc_class\", \"binding_site\", \"biotherapeutic\", \"cell_line\",\n    \"chembl_id_lookup\", \"compound_record\", \"compound_structural_alert\", \"document\",\n    \"document_similarity\", \"document_term\", \"drug\", \"drug_indication\", \"drug_warning\",\n    \"go_slim\", \"image\", \"mechanism\", \"metabolism\", \"molecule\", \"molecule_form\",\n    \"organism\", \"protein_classification\", \"similarity\", \"source\", \"status\", \"substructure\",\n    \"target\", \"target_component\", \"target_relation\", \"tissue\", \"xref_source\"\n  )\n}\n\n#' Internal function that returns all possible chembl filter types\n#' @keywords internal\n#' @noRd\n.chembl_filter_types <- function() {\n  c(\n    \"exact\", \"iexact\", \"contains\", \"icontains\", \"startswith\", \"istartswith\",\n    \"endswith\", \"iendswith\", \"regex\", \"iregex\", \"gt\", \"gte\", \"lt\", \"lte\",\n    \"range\", \"in\", \"isnull\", \"search\", \"only\"\n  )\n}\n\n#' Internal function that returns all possible chembl mechanism columns\n#' @keywords internal\n#' @noRd\n.chembl_mechanism_cols <- function() {\n  c(\n    \"action_type\", \"binding_site_comment\", \"direct_interaction\", \"disease_efficacy\",\n    \"max_phase\", \"mec_id\", \"mechanism_comment\", \"mechanism_of_action\",\n    \"mechanism_refs\", \"molecular_mechanism\", \"molecule_chembl_id\",\n    \"parent_molecule_chembl_id\", \"record_id\", \"selectivity_comment\",\n    \"site_id\", \"target_chembl_id\", \"variant_sequence\"\n  )\n}\n\n#' Internal function that returns the complete schema for a Chembl resource\n#' @keywords internal\n#' @noRd\n.chembl_resource_schema <- function(resource) {\n  .build_chembl_request(paste0(resource, \"/schema\")) |>\n    .perform_request() |>\n    .parse_resp_json()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.chembl_resources()` function and how is it implemented?",
        "answer": "The `.chembl_resources()` function is an internal function that returns all possible ChEMBL resources. It is implemented as a simple function that returns a character vector containing the names of various ChEMBL resources, such as 'activity', 'assay', 'molecule', 'target', etc. This function is likely used to provide a standardized list of available resources for other parts of the package or API."
      },
      {
        "question": "How does the `.chembl_filter_types()` function differ from `.chembl_resources()`, and what might it be used for?",
        "answer": "The `.chembl_filter_types()` function is similar to `.chembl_resources()` in that it returns a character vector, but it contains filter types instead of resources. It includes filter operations like 'exact', 'contains', 'startswith', 'gt' (greater than), 'lt' (less than), etc. This function is likely used in query construction or data filtering operations within the ChEMBL API, allowing users to specify how they want to filter or search for data."
      },
      {
        "question": "What is the purpose of the `.chembl_resource_schema()` function, and how does it differ from the other functions in this code snippet?",
        "answer": "The `.chembl_resource_schema()` function is more complex than the other functions in this snippet. It takes a 'resource' parameter and returns the complete schema for that ChEMBL resource. Unlike the other functions which simply return static vectors, this function makes an API request to fetch the schema. It uses helper functions like `.build_chembl_request()`, `.perform_request()`, and `.parse_resp_json()` to construct the request, send it, and parse the response. This function is likely used to dynamically retrieve and provide structure information about specific ChEMBL resources."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Internal function that returns all possible chembl resources\n#' @keywords internal\n#' @noRd\n.chembl_resources <- function() {\n  c(\n    \"activity\", \"assay\", \"atc_class\", \"binding_site\", \"biotherapeutic\", \"cell_line\",\n    # Add more resources here\n  )\n}",
        "complete": "#' Internal function that returns all possible chembl resources\n#' @keywords internal\n#' @noRd\n.chembl_resources <- function() {\n  c(\n    \"activity\", \"assay\", \"atc_class\", \"binding_site\", \"biotherapeutic\", \"cell_line\",\n    \"chembl_id_lookup\", \"compound_record\", \"compound_structural_alert\", \"document\",\n    \"document_similarity\", \"document_term\", \"drug\", \"drug_indication\", \"drug_warning\",\n    \"go_slim\", \"image\", \"mechanism\", \"metabolism\", \"molecule\", \"molecule_form\",\n    \"organism\", \"protein_classification\", \"similarity\", \"source\", \"status\", \"substructure\",\n    \"target\", \"target_component\", \"target_relation\", \"tissue\", \"xref_source\"\n  )\n}"
      },
      {
        "partial": "#' Internal function that returns the complete schema for a Chembl resource\n#' @keywords internal\n#' @noRd\n.chembl_resource_schema <- function(resource) {\n  # Complete the function body\n}",
        "complete": "#' Internal function that returns the complete schema for a Chembl resource\n#' @keywords internal\n#' @noRd\n.chembl_resource_schema <- function(resource) {\n  .build_chembl_request(paste0(resource, \"/schema\")) |>\n    .perform_request() |>\n    .parse_resp_json()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/utils-testing.R",
    "language": "R",
    "content": "#' Tests of table 1 is a subset of table2, in which case there will be no rows\n#' is the set difference.\n#'\n#' @param table1,table2 A `data.table` or a table-like object coercible to one \n#' via `as.data.table`.\n#'\n#' @return `logical(1)` TRUE if `table1` is a subset of `table2`, otherwise \n#' `FALSE`\n#'\n#' @importFrom data.table fsetdiff as.data.table\n#'\n#' @noRd\n#' @keywords internal\n.table_is_subset <- function(table1, table2) {\n    if (!is(table1, \"data.table\")) table1 <- as.data.table(table1)\n    if (!is(table2, \"data.table\")) table2 <- as.data.table(table2)\n    nrow(fsetdiff(\n        table1,\n        table2,\n    )) == 0\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.table_is_subset` function in this code snippet?",
        "answer": "The `.table_is_subset` function checks if `table1` is a subset of `table2`. It returns `TRUE` if `table1` is a subset of `table2`, and `FALSE` otherwise. It does this by performing a set difference operation using `fsetdiff` and checking if the result has zero rows."
      },
      {
        "question": "How does the function handle input tables that are not of class 'data.table'?",
        "answer": "The function uses the `is()` function to check if the input tables are of class 'data.table'. If they are not, it converts them to 'data.table' objects using the `as.data.table()` function. This ensures that the function can work with various table-like objects that can be coerced to 'data.table'."
      },
      {
        "question": "What is the significance of the `@noRd` and `@keywords internal` tags in the function documentation?",
        "answer": "The `@noRd` tag indicates that this function should not be included in the package's R documentation. The `@keywords internal` tag marks the function as internal, meaning it's not intended for direct use by package users. These tags suggest that `.table_is_subset` is a helper function meant for internal use within the package rather than a public-facing function."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Tests if table1 is a subset of table2\n#'\n#' @param table1,table2 A `data.table` or a table-like object\n#'\n#' @return `logical(1)` TRUE if `table1` is a subset of `table2`, otherwise `FALSE`\n#'\n#' @importFrom data.table fsetdiff as.data.table\n#'\n#' @noRd\n#' @keywords internal\n.table_is_subset <- function(table1, table2) {\n    # Convert inputs to data.table if necessary\n    if (!is(table1, \"data.table\")) table1 <- as.data.table(table1)\n    if (!is(table2, \"data.table\")) table2 <- as.data.table(table2)\n    \n    # Complete the function to check if table1 is a subset of table2\n    \n}",
        "complete": "#' Tests if table1 is a subset of table2\n#'\n#' @param table1,table2 A `data.table` or a table-like object\n#'\n#' @return `logical(1)` TRUE if `table1` is a subset of `table2`, otherwise `FALSE`\n#'\n#' @importFrom data.table fsetdiff as.data.table\n#'\n#' @noRd\n#' @keywords internal\n.table_is_subset <- function(table1, table2) {\n    # Convert inputs to data.table if necessary\n    if (!is(table1, \"data.table\")) table1 <- as.data.table(table1)\n    if (!is(table2, \"data.table\")) table2 <- as.data.table(table2)\n    \n    # Check if table1 is a subset of table2\n    nrow(fsetdiff(table1, table2)) == 0\n}"
      },
      {
        "partial": "#' Tests if table1 is a subset of table2\n#'\n#' @param table1,table2 A `data.table` or a table-like object\n#'\n#' @return `logical(1)` TRUE if `table1` is a subset of `table2`, otherwise `FALSE`\n#'\n#' @importFrom data.table fsetdiff as.data.table\n#'\n#' @noRd\n#' @keywords internal\n.table_is_subset <- function(table1, table2) {\n    # Complete the function to check if table1 is a subset of table2\n    # Ensure both inputs are converted to data.table if necessary\n    # Use fsetdiff to compare the tables\n    \n}",
        "complete": "#' Tests if table1 is a subset of table2\n#'\n#' @param table1,table2 A `data.table` or a table-like object\n#'\n#' @return `logical(1)` TRUE if `table1` is a subset of `table2`, otherwise `FALSE`\n#'\n#' @importFrom data.table fsetdiff as.data.table\n#'\n#' @noRd\n#' @keywords internal\n.table_is_subset <- function(table1, table2) {\n    table1 <- if (!is(table1, \"data.table\")) as.data.table(table1) else table1\n    table2 <- if (!is(table2, \"data.table\")) as.data.table(table2) else table2\n    nrow(fsetdiff(table1, table2)) == 0\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/tests/testthat/test-LongTable-accessors.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(CoreGx)\nlibrary(data.table)\n\ndata(nci_TRE_small)\ntre <- nci_TRE_small\n\n## tre = treatmentResponseExperiment\n## ntre = New treatmentResponseExperiment\n\n# see https://github.com/bhklab/CoreGx/wiki/CoreGx-Design-Documentation for\n# explanation\ntest_that(\"`rowData,LongTable-method` orders data correctly\", {\n    ## TODO::\n})\n\n# == @rowData slot\n\ntestthat::test_that(\"`rowData<-` rowData must be updated with data.table or data.frame\", {\n    ntre <- copy(tre)\n    testthat::expect_error({ rowData(ntre) <- NULL }, ## rowData slot\n        regexp = \".*Please pass a data.frame or data.table to update.the rowData slot.*\"\n    )\n})\n\n\ntestthat::test_that(\"`rowData<-` prevents intentionally breaking referential integrity\", {\n    ntre <- copy(tre)\n    rowData_bad <- rowData(ntre)\n    rowData_bad <- rbind(rowData_bad, rowData_bad[.N, ])\n    testthat::expect_warning({ rowData(ntre) <- rowData_bad },\n        regexp = \".*The ID columns are duplicated for rows [0-9]+! These rows will be dropped before assignment.\"\n    )\n})\n\ntestthat::test_that(\"`colData<-` prevents intentionally breaking referential integrity\", {\n    ntre <- copy(tre)\n    colData_bad <- colData(ntre)\n    colData_bad <- rbind(colData_bad, colData_bad[.N, ])\n    testthat::expect_warning({ colData(ntre) <- colData_bad },\n        regexp =  \".*The ID columns are duplicated for rows [0-9]+! These rows will be dropped before assignment.\"\n    )\n})\n\n# This warning doesn't trigger if we remove another ID column.\n# Instead, an error like in the NCI-ALMANAC script will occur. (Too many duplicate rows)\n\n## FIXME:: We should probably just throw an error for these cases! This test\n##   doesn't work for the full NCI_TRE object due to cartesian join\n# testthat::test_that(\"`rowData<-` ensures necessary row ID columns present in the replacement rowData\", {\n#     ntre <- copy(tre)\n#     rowData_missingID <- rowData(ntre)\n#     rowData_missingID[, (rowIDs(ntre)[4]) := NULL] # remove one ID column\n#     testthat::expect_warning({ rowData(ntre) <- rowData_missingID },\n#         regexp = \".*The function will attempt to join with existing rowIDs, but this may fail!.*\"\n#     )\n# })\n\n# == @colData slot\n\ntestthat::test_that(\"`colData<-` colData must be updated with data.table or data.frame\", {\n    ntre <- copy(tre)\n    testthat::expect_error({ colData(ntre) <- NULL }, ## colData slot\n        regexp = \".*Please pass a data\\\\.frame or data\\\\.table.*\"\n    )\n})\n\n# == @assay slot\n\ntestthat::test_that(\"`assay` invalid assay name and index\", {\n    testthat::expect_error({ assay(tre, c(1, 2)) },\n        regexp = \".*Please specifying a single string assay name or integer.*\"\n    )\n    testthat::expect_error({ assay(tre, paste(assayNames(tre), collapse = '')) },\n        regexp = \".*There is no assay.*\"\n    )\n})\n\ntestthat::test_that(\"`assay<-,LongTable-method` prevents invalid assay slot assignment\", {\n    ntre <- copy(tre)\n    testthat::expect_error({\n        assay(ntre, i = \"sensitivity\", withDimnames = FALSE) <- c(1, 2, 3)\n    },\n    regexp = \".*Only a\\ndata.frame or data.table can be assiged to the assay slot!.*\"\n    )\n})\n\ntestthat::test_that(\"`assay,LongTable-method` and `assays,LongTable-method` return equivalent data\", {\n    assay_list <- lapply(seq_along(assayNames(tre)), FUN=assay,\n        x=tre, withDimnames=TRUE, summarize=FALSE)\n    assays_ <- assays(tre)\n    for (i in seq_along(assay_list)) {\n        testthat::expect_true(all.equal(assay_list[[i]], assays_[[i]]))\n    }\n})\n\ntestthat::test_that(\"`assay<-,LongTable-method` assignment does not corrupt data relationships\", {\n    ntre <- copy(tre)\n    for (nm in assayNames(tre)) {\n        ntre[[nm]] <- ntre[[nm]]\n        testthat::expect_true(all.equal(ntre[[nm]], tre[[nm]]))\n        testthat::expect_true(all.equal(assays(ntre, raw=TRUE)[[nm]], assays(tre, raw=TRUE)[[nm]]))\n    }\n    testthat::expect_true(all.equal(getIntern(ntre)$assayIndex, getIntern(tre)$assayIndex))\n})\n\ntestthat::test_that(\"`assay<-,LongTable-method` allows non-id column updates\", {\n    ntre <- copy(tre)\n    assay_ <- ntre[[\"sensitivity\"]]\n    assay_[, viability := rnorm(.N)]\n    ntre[[\"sensitivity\"]] <- assay_\n    testthat::expect_true(all.equal(ntre[[\"sensitivity\"]], assay_))\n    testthat::expect_false(isTRUE(all.equal(ntre[[\"sensitivity\"]], tre[[\"sensitivity\"]])))\n})\n\ntestthat::test_that(\"`assay<-LongTable-method` prevents id column updates\", {\n    ntre <- copy(tre)\n    assay_ <- ntre[[\"sensitivity\"]]\n#    assay_[, treatment1dose := rnorm(.N)]\n#    testthat::expect_error({ ntre[[\"sensitivity\"]] <- assay_ },\n#        regexp=\".*Identifier columns cannot be modified via assay assignment!.*\"\n#    )\n    testthat::expect_true(all.equal(ntre$sensitivity, tre$sensitivity))\n})\n\ntestthat::test_that(\"`assay<-LongTable-method` allows simple summary assignments\", {\n    ntre <- copy(tre)\n    sens <- ntre$sensitivity\n    sens_sum <- sens[,\n        .(\n            mean_treatment1dose=mean(treatment1dose, na.rm=TRUE),\n            mean_treatment2dose=mean(treatment2dose, na.rm=TRUE),\n            mean_viability=mean(viability, na.rm=TRUE)\n        ),\n        by=.(treatment1id, treatment2id, sampleid)\n    ]\n    testthat::expect_silent(ntre$sens_sum <- sens_sum)\n    # ensure that the returned assay matches the assigned assay when\n    #   summarize=TRUE (the default)\n    sens_sum_accessed <- ntre$sens_sum[, colnames(sens_sum), with=FALSE]\n    setkeyv(sens_sum, key(sens_sum_accessed))\n    testthat::expect_true(all.equal(\n        sens_sum,\n        sens_sum_accessed,\n    ))\n    # test that summarzie=FALSE attaches all original data\n    testthat::expect_true(all.equal(\n        rowIDs(tre, data=TRUE),\n        unique(assay(ntre, \"sens_sum\", summarize=FALSE)[, rowIDs(ntre), with=FALSE]),\n        check.attributes=FALSE\n    ))\n    testthat::expect_true(all.equal(\n        colIDs(tre, data=TRUE),\n        unique(assay(ntre, \"sens_sum\", summarize=FALSE)[order(sampleid), colIDs(ntre), with=FALSE]),\n        check.attributes=FALSE\n    ))\n})\n\ntestthat::test_that(\"`assay<-,LongTable-method` summary assignment doesn't break referential integrity\", {\n    ntre <- copy(tre)\n    sens <- ntre$sensitivity\n    sens_sum <- sens[,\n        .(\n            mean_treatment1dose=mean(treatment1dose, na.rm=TRUE),\n            mean_treatment2dose=mean(treatment2dose, na.rm=TRUE),\n            mean_viability=mean(viability, na.rm=TRUE)\n        ),\n        by=.(treatment1id, treatment2id, sampleid)\n    ]\n    testthat::expect_silent(ntre$sens_sum <- sens_sum)\n    testthat::expect_true(all.equal(rowData(tre), rowData(ntre)))\n    testthat::expect_true(all.equal(colData(tre), colData(ntre)))\n    non_summary_assays <- setdiff(assayNames(ntre), \"sens_sum\")\n    for (aname in non_summary_assays) {\n        testthat::expect_true(all.equal(\n            tre[[aname]],\n            ntre[[aname]]\n        ))\n    }\n})\n\ntestthat::test_that(\"`assay<-,LongTable-method` prevents modified row ID in new assay from breaking referential integrity\", {\n    ntre <- copy(tre)\n    sens <- ntre$sensitivity\n    sens_sum <- sens[,\n        .(\n            mean_treatment1dose=mean(treatment1dose, na.rm=TRUE),\n            mean_treatment2dose=mean(treatment2dose, na.rm=TRUE),\n            mean_viability=mean(viability, na.rm=TRUE)\n        ),\n        by=.(treatment1id, treatment2id, sampleid)\n    ]\n    set(sens_sum,\n        i     = which(sens_sum[[\"treatment1id\"]] == sens_sum[1, treatment1id]),\n        j     = \"treatment1id\",           # rowID to modify\n        value = sens_sum[1, sampleid]) # Replace a treatment id with a sample id\n    testthat::expect_error({ ntre$sens_sum <- sens_sum },\n        regexp = paste(\n            \".*One or more rowIDs\\\\(x\\\\) columns have been modified.\",\n            \"Identifier columns cannot be modified via assay assignment!.*\"\n            )\n    )\n})\n\ntestthat::test_that(\"`assay<-,LongTable-method` prevents modified column ID in new assay from breaking referential integrity\", {\n    ntre <- copy(tre)\n    sens <- ntre$sensitivity\n    sens_sum <- sens[,\n        .(\n            mean_treatment1dose=mean(treatment1dose, na.rm=TRUE),\n            mean_treatment2dose=mean(treatment2dose, na.rm=TRUE),\n            mean_viability=mean(viability, na.rm=TRUE)\n        ),\n        by=.(treatment1id, treatment2id, sampleid)\n    ]\n    set(sens_sum,\n        i = which(sens_sum[[\"sampleid\"]] == sens_sum[1, sampleid]),\n        j = \"sampleid\",             # column ID to modify\n        value = sens_sum[1, treatment2id]) # Replace a sample ID with a treatment ID\n    testthat::expect_error({ ntre$sens_sum <- sens_sum },\n        regexp = paste(\n            \".*One or more colIDs\\\\(x\\\\) column have been modified.\",\n            \"Identifier columns cannot be modified via assay assignment!.*\"\n            )\n    )\n})\n\n# == @metadata slot\n\ntestthat::test_that(\"`metadata<-` invalid metadata slot assignment\", {\n    ntre <- copy(tre)\n    testthat::expect_error({ metadata(ntre) <- NULL },\n        regexp = \".*The `metadata` slot must be a list!.*\"\n    )\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rowData<-` method test in this code, and what does it check for?",
        "answer": "The `rowData<-` method test checks for referential integrity when updating the rowData slot of a TreatmentResponseExperiment object. It ensures that duplicate rows in the rowData are detected and warns the user. Specifically, it creates a new rowData with a duplicated last row and expects a warning about duplicate ID columns when trying to assign this to the object."
      },
      {
        "question": "How does the code test the `assay<-` method for preventing invalid assignments?",
        "answer": "The code tests the `assay<-` method by attempting to assign an invalid value (a vector of numbers) to an assay slot. It expects an error with a message indicating that only a data.frame or data.table can be assigned to the assay slot. This ensures that the method properly validates the type of data being assigned to maintain the integrity of the TreatmentResponseExperiment object."
      },
      {
        "question": "What is the purpose of the test checking if `assay` and `assays` methods return equivalent data?",
        "answer": "This test ensures consistency between the `assay` and `assays` methods of the TreatmentResponseExperiment object. It creates a list of assays using the `assay` method for each assay name, and then compares this list with the result of the `assays` method. The test verifies that both methods return the same data, which is crucial for maintaining consistent behavior across different ways of accessing assay data in the object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "testthat::test_that(\"`assay,LongTable-method` and `assays,LongTable-method` return equivalent data\", {\n    assay_list <- lapply(seq_along(assayNames(tre)), FUN=assay,\n        x=tre, withDimnames=TRUE, summarize=FALSE)\n    assays_ <- assays(tre)\n    for (i in seq_along(assay_list)) {\n        testthat::expect_true(all.equal(assay_list[[i]], assays_[[i]]))\n    }\n})",
        "complete": "testthat::test_that(\"`assay,LongTable-method` and `assays,LongTable-method` return equivalent data\", {\n    assay_list <- lapply(seq_along(assayNames(tre)), FUN=assay,\n        x=tre, withDimnames=TRUE, summarize=FALSE)\n    assays_ <- assays(tre)\n    for (i in seq_along(assay_list)) {\n        testthat::expect_true(all.equal(assay_list[[i]], assays_[[i]]))\n    }\n})"
      },
      {
        "partial": "testthat::test_that(\"`assay<-,LongTable-method` allows non-id column updates\", {\n    ntre <- copy(tre)\n    assay_ <- ntre[[\"sensitivity\"]]\n    assay_[, viability := rnorm(.N)]\n    ntre[[\"sensitivity\"]] <- assay_\n    testthat::expect_true(all.equal(ntre[[\"sensitivity\"]], assay_))\n    testthat::expect_false(isTRUE(all.equal(ntre[[\"sensitivity\"]], tre[[\"sensitivity\"]])))\n})",
        "complete": "testthat::test_that(\"`assay<-,LongTable-method` allows non-id column updates\", {\n    ntre <- copy(tre)\n    assay_ <- ntre[[\"sensitivity\"]]\n    assay_[, viability := rnorm(.N)]\n    ntre[[\"sensitivity\"]] <- assay_\n    testthat::expect_true(all.equal(ntre[[\"sensitivity\"]], assay_))\n    testthat::expect_false(isTRUE(all.equal(ntre[[\"sensitivity\"]], tre[[\"sensitivity\"]])))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/mergeAssays-method.R",
    "language": "R",
    "content": "#' @include LongTable-class.R\n#'\n#' @importFrom data.table key merge.data.table\n#' @import checkmate\nNULL\n\n#' Merge assays with an `S4` object.\n#'\n#' @param object `S4` An S4 object a list-like slot containing assays for the\n#'   object.\n#' @param ... Allow new arguments to be defined for this generic.\n#'\n#' @return A modified version of `object`.\n#'\n#' @examples\n#' \"This is a generic method!\"\n#'\n#' @exportMethod mergeAssays\nsetGeneric(\"mergeAssays\", function(object, ...) standardGeneric(\"mergeAssays\"))\n\n\n#' Endomorphically merge assays within a `LongTable` or inheriting class\n#'\n#' @param object A `LongTable` or inheriting class.\n#' @param x `character(1)` A valid assay name in `object`.\n#' @param y `character(1)` A valid assay name in `object`.\n#' @param target `character(1)` Name of the assay to assign the result to.\n#' Can be a new or existing assay. Defaults to `x`.\n#' @param ... Fallthrough arguments to merge.data.table to specify the join\n#'   type. Use this to specify which columns to merge on. If excluded, defaults\n#'   to by=assayKeys(objecty, y).\n#' @param metadata `logical` A logical vector indicating whether to attach\n#' metadata to either assay before the merge occurs. If only one value is\n#' passed that value is used for both assays. Defaults to `FALSE`.\n#'\n#' @return A copy of `object` with assays `x` and `y` merged and assigned to\n#' `target`.\n#'\n#' @seealso [`merge.data.table`]\n#'\n#' @author\n#' Christopher Eeles\n#'\n#' @export\nsetMethod(\"mergeAssays\", signature(\"LongTable\"),\n        function(object, x, y, target=x, ..., metadata=FALSE) {\n    checkmate::qassert(target, \"S1\")\n    z <- .merge_longtable_assays(object, x=x, y=y, ...,\n        metadata=metadata)\n    object[[target]] <- z\n    object\n})\n\n#' @noRd\n.merge_longtable_assays <- function(object, x, y, ..., metadata=FALSE) {\n    # -- input validation\n    checkmate::qassert(x, \"S1\")\n    checkmate::qassert(y, \"S1\")\n    checkmate::assertChoice(x, assayNames(object))\n    checkmate::assertChoice(x, assayNames(object))\n    checkmate::qassert(metadata, c(\"B1\", \"B2\"))\n    if (length(metadata) == 1) metadata <- c(metadata, metadata)\n\n    # -- extract assays to merge\n    x_ <- assay(object, x, summarize=TRUE, metadata=metadata[1])\n    y_ <- if (x == y) x_ else\n        assay(object, y, summarize=TRUE, metadata=metadata[2])\n\n    # -- handle by argument\n    by <- NA\n    by_args <- c(\"by\", \"by.x\", \"by.y\")\n    if (!(any(by_args %in% ...names())))  {\n        by <- assayKeys(object, y)\n    }\n\n    if (!is.character(by))\n        merge.data.table(x=x_, y=y_, ...)\n    else\n        merge.data.table(x=x_, y=y_, by=by, ...)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mergeAssays` generic function and how is it implemented for the `LongTable` class?",
        "answer": "The `mergeAssays` generic function is designed to merge assays within an S4 object. For the `LongTable` class, it's implemented as a method that merges two specified assays (`x` and `y`) and assigns the result to a target assay (defaulting to `x`). It uses the `.merge_longtable_assays` helper function to perform the actual merge operation and returns a modified copy of the input object with the merged assay."
      },
      {
        "question": "How does the `.merge_longtable_assays` function handle the 'by' argument for merging data tables?",
        "answer": "The `.merge_longtable_assays` function checks if any of the 'by', 'by.x', or 'by.y' arguments are provided in the ... (ellipsis) arguments. If none are provided, it defaults to using `assayKeys(object, y)` as the 'by' argument. If a 'by' argument is provided or defaulted, it's passed to `merge.data.table` along with other arguments. This allows flexibility in specifying merge conditions while providing a sensible default."
      },
      {
        "question": "What is the purpose of the `metadata` parameter in the `mergeAssays` method, and how is it used?",
        "answer": "The `metadata` parameter is a logical vector that determines whether to attach metadata to the assays before merging. If a single logical value is provided, it's used for both assays. The function uses this parameter when calling `assay(object, x, summarize=TRUE, metadata=metadata[1])` and `assay(object, y, summarize=TRUE, metadata=metadata[2])`. This allows users to optionally include metadata in the merge operation, which can be useful for preserving additional information during the merge process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"mergeAssays\", signature(\"LongTable\"),\n        function(object, x, y, target=x, ..., metadata=FALSE) {\n    checkmate::qassert(target, \"S1\")\n    z <- .merge_longtable_assays(object, x=x, y=y, ...,\n        metadata=metadata)\n    object[[target]] <- z\n    object\n})",
        "complete": "setMethod(\"mergeAssays\", signature(\"LongTable\"),\n        function(object, x, y, target=x, ..., metadata=FALSE) {\n    checkmate::qassert(target, \"S1\")\n    z <- .merge_longtable_assays(object, x=x, y=y, ...,\n        metadata=metadata)\n    object[[target]] <- z\n    object\n})"
      },
      {
        "partial": ".merge_longtable_assays <- function(object, x, y, ..., metadata=FALSE) {\n    checkmate::qassert(x, \"S1\")\n    checkmate::qassert(y, \"S1\")\n    checkmate::assertChoice(x, assayNames(object))\n    checkmate::assertChoice(x, assayNames(object))\n    checkmate::qassert(metadata, c(\"B1\", \"B2\"))\n    if (length(metadata) == 1) metadata <- c(metadata, metadata)\n\n    x_ <- assay(object, x, summarize=TRUE, metadata=metadata[1])\n    y_ <- if (x == y) x_ else\n        assay(object, y, summarize=TRUE, metadata=metadata[2])\n\n    by <- NA\n    by_args <- c(\"by\", \"by.x\", \"by.y\")\n    if (!(any(by_args %in% ...names())))  {\n        by <- assayKeys(object, y)\n    }\n\n    if (!is.character(by))\n        merge.data.table(x=x_, y=y_, ...)\n    else\n        merge.data.table(x=x_, y=y_, by=by, ...)\n}",
        "complete": ".merge_longtable_assays <- function(object, x, y, ..., metadata=FALSE) {\n    checkmate::qassert(x, \"S1\")\n    checkmate::qassert(y, \"S1\")\n    checkmate::assertChoice(x, assayNames(object))\n    checkmate::assertChoice(y, assayNames(object))\n    checkmate::qassert(metadata, c(\"B1\", \"B2\"))\n    if (length(metadata) == 1) metadata <- c(metadata, metadata)\n\n    x_ <- assay(object, x, summarize=TRUE, metadata=metadata[1])\n    y_ <- if (x == y) x_ else\n        assay(object, y, summarize=TRUE, metadata=metadata[2])\n\n    by <- NA\n    by_args <- c(\"by\", \"by.x\", \"by.y\")\n    if (!(any(by_args %in% ...names())))  {\n        by <- assayKeys(object, y)\n    }\n\n    if (!is.character(by))\n        merge.data.table(x=x_, y=y_, ...)\n    else\n        merge.data.table(x=x_, y=y_, by=by, ...)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-coerce.R",
    "language": "R",
    "content": "# ==== LongTable Class\n\n#' @include LongTableDataMapper-class.R\n#' @include DataMapper-class.R\n#' @include TreatmentResponseExperiment-class.R\nNULL\n\n#' @title LongTable to data.table conversion\n#' @name as\n#'\n#' @examples\n#' as(merckLongTable, 'data.table')\n#'\n#' @description Coerce a LongTable into a `data.table`.\n#'\n#' @param from `LongTable` Object to coerce.\n#'\n#' @return A `data.table` with the data from a LongTable.\n#'\n#' @import data.table\n#' @export\nsetAs('LongTable', 'data.table', def=function(from) {\n\n    # extract the assay data\n    longTableData <- assays(from, withDimnames=FALSE, key=TRUE)\n\n    # join assays into a single table\n    DT <- longTableData[[1]]\n    longTableData[[1]] <- NULL\n    for (i in seq_along(longTableData)) {\n        DT <- merge.data.table(DT, longTableData[[i]],\n            suffixes=c('', paste0('._', i)), by=.EACHI, all.x=TRUE, all.y=TRUE)\n    }\n\n    # extract assay columns\n    assayCols <- assayCols(from)\n\n    # fix assayCols if there are duplicate column names between assays\n    # the join will append '._n' where n is the assay index - 1\n    ## TODO:: Make this a helper since it is reused in multiple functions\n    .greplAny <- function(...) any(grepl(...))\n    .paste0IfElse <- function(vector, suffix, isIn=c('rowKey', 'colKey'))\n        ifelse(vector %in% isIn, vector, paste0(vector, suffix))\n    hasSuffixes <- unlist(lapply(paste0('._', seq_along(longTableData)),\n        FUN=.greplAny, x=colnames(DT)))\n    if (any(hasSuffixes)) {\n        whichHasSuffixes <- which(hasSuffixes) + 1\n        assayCols[whichHasSuffixes] <-\n            Map(FUN=.paste0IfElse,\n                vector=assayCols[whichHasSuffixes],\n                suffix=paste0('._', seq_along(longTableData))[hasSuffixes]\n            )\n    }\n\n    # join the row and column data\n    DT <- merge.data.table(DT, rowData(from, key=TRUE), by='rowKey')\n    DT <- merge.data.table(DT, colData(from, key=TRUE), by='colKey')\n    setkeyv(DT, c('rowKey', 'colKey'))\n\n    # drop interal key columns\n    DT[, c('rowKey', 'colKey') := NULL]\n\n    # organize the returned columns\n    colOrder <- unique(c(setdiff(\n        colnames(DT), unlist(assayCols)),\n        unlist(assayCols)\n    ))\n    setcolorder(DT, colOrder)\n\n    aMap <- lapply(assayCols, FUN=setdiff,\n        y=c(idCols(from), rowMeta(from), colMeta(from)))\n    aMap <- Map(list, mutable(getIntern(from, \"assayKeys\")), aMap)\n\n    DT <- cbind(DT, metadata(from)$experiment_metadata)\n\n    metaCols <- names(metadata(from)$experiment_metadata)\n    longTableMapper <- LongTableDataMapper(\n        rowDataMap=list(rowIDs(from), rowMeta(from)),\n        colDataMap=list(colIDs(from), colMeta(from)),\n        assayMap=aMap,\n        metadataMap=if (is.character(metaCols)) list(metaCols) else list()\n    )\n    metadata(longTableMapper) <- metadata(from)[names(metadata(from) != \"experiment_metadata\")]\n    attr(DT, 'longTableDataMapper') <- longTableMapper\n\n    # return the data.table\n    return(DT)\n})\n# #' @title Coerce a LongTable into a `data.table`\n# #' @name as\n# #'\n# #' @description S3 version of coerce method for convenience.\n# #'\n# #' @param from `LongTable` to coerce to a `data.table`\n# #'\n# #' @return A `data.table` containing the data from the LongTable, as well\n# #'   as the `longTableDataMapper' attribute which contains the data needed to\n# #'   reverse the coercion.\n# #' @export\n# as.data.table.long.table <- function(from, keep.rownames=FALSE,...) as(from, 'data.table')\n\n#' @title Coerce a LongTable into a `data.frame`\n#' @name as\n#'\n#' @description Currently only supports coercing to data.table or data.frame\n#'\n#' @param from `LongTable` Object to coerce.\n#'\n#' @return `data.table` containing the data from the LongTable, with the\n#'   `longTableDataMapper' attribute containg the metadata needed to reverse\n#'   the coercing operation.\n#'\n#' @importFrom data.table data.table setDF\n#' @export\nsetAs('LongTable', 'data.frame', def=function(from) {\n    DT <- as(from, 'data.table')\n    setDF(DT)\n    return(DT)\n})\n\n# #' @title Coerce a LongTable to a data.frame\n# #' @name as\n# #'\n# #' @examples\n# #' as(merckLongTable, 'data.frame')\n# #'\n# #' @description S3 version of coerce method fro convenience.\n# #'\n# #' @param x `LongTable` to coerce to `data.frame`.\n# #' @param row.names An optional `character` vector of rownames. We do not\n# #'   recommend using this parameter, it is included for S3 method consistency\n# #'   with `as.data.frame`.\n# #' @param optional `logical` Is it optional for row and column names to be\n# #'   valid R names? If FALSE will use the make.names function to ensure the\n# #'   row and column names are valid R names. Defaults to TRUE.\n# #' @param ... Does nothing.\n# #'\n# #' @param from `LongTable` Object to coerce.\n# #' \n# #' @return `data.frame` containing the data from the LongTable, with the\n# #'   `longTableDataMapper' attribute containg the metadata needed to reverse\n# #'   the coercion operation.\n# #'\n# #' @importFrom data.table data.table\n# #' @export\n# as.data.frame.long.table <- function(x, row.names, optional=TRUE, ...) {\n#     DF <- as(x, 'data.frame')\n#     if (!missing(row.names)) {\n#         if (!is.character(x) || length(row.names) != nrow(DF))\n#             stop(.errorMsg('[CoreGx::as.data.frame.LongTable] The row.names ',\n#                 'argument must be a character vector with length equal to ',\n#                 nrow(DF)))\n#         if (!optional) {\n#             row.names <- make.names(row.names)\n#             colnames(DF) <- make.names(colnames(DF))\n#         }\n#         rownames(DF) <- row.names\n#     }\n#     DF\n# }\n\n\n#' @title Coerce to data.table to LongTable\n#' @name as\n#'\n#' @examples\n#' dataTable <- as(merckLongTable, 'data.table')\n#' print(attr(dataTable, 'longTableDataMapper')) # Method doesn't work without this\n#' as(dataTable, 'LongTable')\n#'\n#' @description Coerce a data.table with the proper configuration attributes\n#'   back to a LongTable\n#'\n#' @param from A `data.table` with the 'longTableDataMapper' attribute, containing\n#'   three lists named assayCols, rowDataCols and colDataCols. This attribute is\n#'   automatically created when coercing from a `LongTable` to a `data.table`.\n#'\n#' @return `LongTable` object configured with the longTableDataMapper\n#'\n#' @export\nsetAs('data.table', 'LongTable', def=function(from) {\n\n    if (!('longTableDataMapper' %in% names(attributes(from))))\n        stop(.errorMsg('[CoreGx::as,data.table,LongTable] Coercing from ',\n            'data.table to LongTable only works if the longTableMapper ',\n            'attribute has been set!'))\n\n    longTableMapper <- attr(from, 'longTableDataMapper')\n\n    requiredConfig <- c('assayMap', 'rowDataMap', 'colDataMap')\n    hasRequiredConfig <- vapply(requiredConfig,\n        FUN=\\(x, f) length(do.call(f, list(x)))[[1]] > 0,\n        x=longTableMapper, FUN.VALUE=logical(1))\n    if (!all(hasRequiredConfig))\n        stop(.errorMsg('The longTableDataMapper object is missing data from',\n            'the ', paste0(requiredConfig[!hasRequiredConfig], collapse=', '),\n            ' slots! Check attributes(from).'))\n\n    rawdata(longTableMapper) <- from\n    return(metaConstruct(longTableMapper))\n})\n#' @name as.long.table\n#' @title Coerce from data.table to LongTable\n#'\n#' @examples\n#' dataTable <- as(merckLongTable, 'data.table')\n#' print(attr(dataTable, 'longTableDataMapper')) # Method doesn't work without this\n#' as.long.table(dataTable)\n#'\n#' @description Coerce a data.table with the proper configuration attributes\n#'   back to a LongTable\n#'\n#' @param x A `data.frame` with the 'longTableDataMapper' attribute, containing\n#'  three lists named assayCols, rowDataCols and colDataCols. This attribute\n#'  is automatically created when coercing from a LongTable to a data.table.\n#'\n#' @return `LongTable` object configured with the longTableDataMapper\n#' @export\nas.long.table <- function(x) as(x, 'LongTable')\n\n\n#' @name as\n#' @title Coerce a SummarizedExperiment to a data.table\n#'\n#' @examples\n#' SE <- molecularProfilesSlot(clevelandSmall_cSet)[[1]]\n#' as(SE, 'data.table')\n#'\n#' @param from `SummarizedExperiment` object.\n#'\n#' @return `data.table` with long format of data in `from`\n#'\n#' @importFrom data.table as.data.table melt.data.table merge.data.table\n#' @export\nsetAs(from='SummarizedExperiment', to='data.table', function(from) {\n    # -- extract sample metadata\n    colDT <- as.data.table(colData(from), keep.rownames='.sample')\n    # -- extract feature metadata\n    rowDT <- as.data.table(rowData(from), keep.rownames='.feature')\n    # -- extract and process assays\n    assayL <- assays(from)\n    assayDtL <- lapply(assayL, as.data.table, keep.rownames='.feature')\n    meltDtL <- lapply(assayDtL, melt, id.vars='.feature',\n        variable.name='.sample', variable.factor=FALSE)\n    assayDT <- meltDtL[[1]][, .(.sample, .feature)]\n    for (i in seq_along(meltDtL))\n        assayDT[[names(assayL)[[i]]]] <- meltDtL[[i]][['value']]\n    # -- merge into a single long format table\n    DT <- merge.data.table(assayDT, colDT, by='.sample')\n    DT <- merge.data.table(DT, rowDT, by='.feature')\n    # -- add metadata\n    metadata <- metadata(from)\n    notS4 <- !vapply(metadata, isS4, logical(1))\n    if (!all(notS4))\n        .warning('Dropped S4 metadata during coercion to data.table!')\n    for (name in names(metadata)[notS4]) assayDT[[name]] <- metadata[[name]]\n    return(DT)\n})\n\n#' @name as\n#' @title Coerce a SummarizedExperiment to a data.frame\n#'\n#' @examples\n#' SE <- molecularProfilesSlot(clevelandSmall_cSet)[[1]]\n#' as(SE, 'data.frame')\n#'\n#' @param from `SummarizedExperiment` object.\n#'\n#' @return `data.frame` with long format of data in `from`.\n#'\n#' @importFrom data.table as.data.table melt.data.table merge.data.table\n#' @export\nsetAs(from='SummarizedExperiment', to='data.frame', function(from) {\n    setDF(as(from, 'data.table'))\n})\n\n\n#' @title Coerce a `LongTable` into a `SummarizedExperiment`\n#' @name as\n#'\n#' @param from `LongTable` object coerce to a `SummarizedExperiment`. Assays\n#'   are converted to `BumpyMatrix`es to allow treatment combination support\n#'   and integration with the `gDR` package.\n#'\n#' @return `SummarizedExperiment` with each assay as a `BumpyMatrix`\n#'\n#' @seealso [`BumpyMatrix::BumpyMatrix`]\n#'\n#' @md\n#' @importFrom SummarizedExperiment SummarizedExperiment\n#' @export\nsetAs(\"LongTable\", \"SummarizedExperiment\", def=function(from) {\n    .longTableToSummarizedExperiment(from, assay_names=assayNames(from))\n})\n\n\n#' @title Convert a LongTable assay into a BumpyMatrix object\n#'\n#' @param LT `LongTable` with assay to convert into `BumpyMatrix`\n#' @param assay `character(1)` A valid assay name in `LT`, as returned by\n#'     `assayNames(LT)`.\n#' @param rows `character()` The rownames associated with the assay rowKey\n#' @param cols `character()` The names associated with the assay colKey\n#' @param sparse `logical(1)` Should the `BumpyMatrix` be sparse (i.e., is the\n#'   assay sparse).\n#'\n#' @return `BumpyMatrix` containing the data from `assay`.\n#'\n#' @md\n#' @importFrom data.table data.table\n#' @importFrom BumpyMatrix splitAsBumpyMatrix\n.assayToBumpyMatrix <- function(LT, assay, rows, cols, sparse=TRUE) {\n    assay_data <- assay(LT, assay, key=TRUE)\n    assay_data[, rownames := rows[rowKey]]\n    assay_data[, colnames := cols[colKey]]\n    assay_data[, c('rowKey', 'colKey') := NULL]\n    splitAsBumpyMatrix(assay_data[, -c('rownames', 'colnames')],\n        row=assay_data$rownames, column=assay_data$colnames, sparse)\n}\n\n#' Convert LongTable to gDR Style SummarizedExperiment\n#'\n#' @param LT `LongTable` to convert to gDR `SummarizedExperiment` format.\n#' @param assay_names `character()` Names to rename the assays to. These\n#'   are assumed to be in the same order as `assayNames(LT)`.\n#'\n#' @return `SummarizedExperiment` object with all assay from `LT` as\n#'   `BumpyMatrix`es.\n#'\n#' @md\n#' @importFrom data.table setnames\n#' @importFrom SummarizedExperiment SummarizedExperiment\n.longTableToSummarizedExperiment <- function(LT, assay_names) {\n    assay_list <- lapply(assayNames(LT), FUN=.assayToBumpyMatrix,\n        LT=LT, rows=rownames(LT), cols=colnames(LT))\n    if (!missing(assay_names) && length(assay_names) == length(assayNames(LT)))\n        names(assay_list) <- assay_names\n    SummarizedExperiment(\n        assays=assay_list, rowData=rowData(LT), colData=colData(LT),\n            metadata=c(metadata(LT), list(.intern=as.list(getIntern(LT))))\n    )\n}\n\n#' @name as\n#'\n#' @title\n#' Coerce a `LongTableDataMapper` to a `TREDataMapper`\n#'\n#' @param from A `LongTableDataMapper` to coerce.\n#'\n#' @return A `TREDataMapper` object.\n#'\n#' @md\n#' @export\nsetAs(\"LongTableDataMapper\", \"TREDataMapper\", def=function(from) {\n    TREDataMapper(from)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setAs('LongTable', 'data.table', ...)` method in this code?",
        "answer": "The `setAs('LongTable', 'data.table', ...)` method is used to coerce a LongTable object into a data.table. It extracts assay data, joins multiple assays into a single table, merges row and column data, and organizes the columns. The resulting data.table includes a 'longTableDataMapper' attribute for reverse coercion."
      },
      {
        "question": "How does the code handle duplicate column names between assays when coercing a LongTable to a data.table?",
        "answer": "The code handles duplicate column names by appending suffixes to the duplicate columns. It uses a helper function `.paste0IfElse` to add '._n' suffixes (where n is the assay index minus 1) to duplicate column names. This ensures that all column names in the resulting data.table are unique."
      },
      {
        "question": "What is the purpose of the `longTableDataMapper` attribute in the resulting data.table, and how is it created?",
        "answer": "The `longTableDataMapper` attribute is created to store metadata needed for reverse coercion from data.table back to LongTable. It is an instance of `LongTableDataMapper` class, containing mappings for row data, column data, assays, and metadata. This attribute is essential for preserving the structure and relationships of the original LongTable when converting back from data.table."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setAs('LongTable', 'data.table', def=function(from) {\n    longTableData <- assays(from, withDimnames=FALSE, key=TRUE)\n    DT <- longTableData[[1]]\n    longTableData[[1]] <- NULL\n    for (i in seq_along(longTableData)) {\n        DT <- merge.data.table(DT, longTableData[[i]],\n            suffixes=c('', paste0('._', i)), by=.EACHI, all.x=TRUE, all.y=TRUE)\n    }\n    assayCols <- assayCols(from)\n    # TODO: Complete the function\n})",
        "complete": "setAs('LongTable', 'data.table', def=function(from) {\n    longTableData <- assays(from, withDimnames=FALSE, key=TRUE)\n    DT <- longTableData[[1]]\n    longTableData[[1]] <- NULL\n    for (i in seq_along(longTableData)) {\n        DT <- merge.data.table(DT, longTableData[[i]],\n            suffixes=c('', paste0('._', i)), by=.EACHI, all.x=TRUE, all.y=TRUE)\n    }\n    assayCols <- assayCols(from)\n    .greplAny <- function(...) any(grepl(...))\n    .paste0IfElse <- function(vector, suffix, isIn=c('rowKey', 'colKey'))\n        ifelse(vector %in% isIn, vector, paste0(vector, suffix))\n    hasSuffixes <- unlist(lapply(paste0('._', seq_along(longTableData)),\n        FUN=.greplAny, x=colnames(DT)))\n    if (any(hasSuffixes)) {\n        whichHasSuffixes <- which(hasSuffixes) + 1\n        assayCols[whichHasSuffixes] <-\n            Map(FUN=.paste0IfElse,\n                vector=assayCols[whichHasSuffixes],\n                suffix=paste0('._', seq_along(longTableData))[hasSuffixes]\n            )\n    }\n    DT <- merge.data.table(DT, rowData(from, key=TRUE), by='rowKey')\n    DT <- merge.data.table(DT, colData(from, key=TRUE), by='colKey')\n    setkeyv(DT, c('rowKey', 'colKey'))\n    DT[, c('rowKey', 'colKey') := NULL]\n    colOrder <- unique(c(setdiff(\n        colnames(DT), unlist(assayCols)),\n        unlist(assayCols)\n    ))\n    setcolorder(DT, colOrder)\n    aMap <- lapply(assayCols, FUN=setdiff,\n        y=c(idCols(from), rowMeta(from), colMeta(from)))\n    aMap <- Map(list, mutable(getIntern(from, \"assayKeys\")), aMap)\n    DT <- cbind(DT, metadata(from)$experiment_metadata)\n    metaCols <- names(metadata(from)$experiment_metadata)\n    longTableMapper <- LongTableDataMapper(\n        rowDataMap=list(rowIDs(from), rowMeta(from)),\n        colDataMap=list(colIDs(from), colMeta(from)),\n        assayMap=aMap,\n        metadataMap=if (is.character(metaCols)) list(metaCols) else list()\n    )\n    metadata(longTableMapper) <- metadata(from)[names(metadata(from) != \"experiment_metadata\")]\n    attr(DT, 'longTableDataMapper') <- longTableMapper\n    return(DT)\n})"
      },
      {
        "partial": "setAs('data.table', 'LongTable', def=function(from) {\n    if (!('longTableDataMapper' %in% names(attributes(from))))\n        stop(.errorMsg('[CoreGx::as,data.table,LongTable] Coercing from ',\n            'data.table to LongTable only works if the longTableMapper ',\n            'attribute has been set!'))\n    longTableMapper <- attr(from, 'longTableDataMapper')\n    # TODO: Complete the function\n})",
        "complete": "setAs('data.table', 'LongTable', def=function(from) {\n    if (!('longTableDataMapper' %in% names(attributes(from))))\n        stop(.errorMsg('[CoreGx::as,data.table,LongTable] Coercing from ',\n            'data.table to LongTable only works if the longTableMapper ',\n            'attribute has been set!'))\n    longTableMapper <- attr(from, 'longTableDataMapper')\n    requiredConfig <- c('assayMap', 'rowDataMap', 'colDataMap')\n    hasRequiredConfig <- vapply(requiredConfig,\n        FUN=\\(x, f) length(do.call(f, list(x)))[[1]] > 0,\n        x=longTableMapper, FUN.VALUE=logical(1))\n    if (!all(hasRequiredConfig))\n        stop(.errorMsg('The longTableDataMapper object is missing data from',\n            'the ', paste0(requiredConfig[!hasRequiredConfig], collapse=', '),\n            ' slots! Check attributes(from).'))\n    rawdata(longTableMapper) <- from\n    return(metaConstruct(longTableMapper))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/rankGeneRadSensitivity.R",
    "language": "R",
    "content": "#' Rank genes based on radiation effect in the Connectivity Map\n#'\n#' @param data gene expression data matrix\n#' @param drugpheno sensititivity values fo thr drug of interest\n#' @param type cell or tissue type for each experiment\n#' @param batch experiment batches\n#' @param single.type Should the statitsics be computed for each cell/tissue\n#'   type separately?\n#' @param standardize How to standardize the data? Currently only supports \"SD\"\n#' @param nthread  number of parallel threads (bound to the maximum number of cores available)\n#' @param verbose Should details of function operation be printed to console?\n#'\n#' @return A \\code{list} of data.frames with the statistics for each gene, for\n#'   each type\n#'\n#' @importFrom stats complete.cases\n#' @importFrom stats p.adjust\n#'\n#' @noRd\nrankGeneRadSensitivity <- function(data,\n                                     drugpheno,\n                                     type, batch,\n                                     single.type=FALSE,\n                                     standardize = \"SD\",\n                                     nthread=1,\n                                     verbose=FALSE)\n{\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  if(is.null(dim(drugpheno))){\n    drugpheno <- data.frame(drugpheno)\n  } else if(!is(drugpheno, \"data.frame\")) {\n    drugpheno <- as.data.frame(drugpheno)\n  }\n\n  if (missing(type) || all(is.na(type))) {\n    type <- array(\"other\", dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (missing(batch) || all(is.na(batch))) {\n    batch <- array(1, dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, duration, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  res <- NULL\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n  res <- NULL\n  ccix <- complete.cases(data, type, batch, drugpheno)\n  nn <- sum(ccix)\n  if(!any(unlist(lapply(drugpheno,is.factor)))){\n     if(ncol(drugpheno)>1){\n      ##### FIX NAMES!!!\n      nc <- lapply(seq_len(ncol(drugpheno)), function(i){\n\n        est <- paste(\"estimate\", i, sep=\".\")\n        se <-  paste(\"se\", i, sep=\".\")\n        tstat <- paste(\"tstat\", i, sep=\".\")\n\n        nc <- c(est, se, tstat)\n        return(nc)\n\n      })\n      #nc <- do.call(c, rest)\n      nc  <- c(nc, n=nn, \"fstat\"=NA, \"pvalue\"=NA, \"fdr\")\n    } else {\n      nc  <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\")\n    }\n  } else {\n    nc  <- c(\"estimate\", \"se\", \"n\", \"pvalue\", \"fdr\")\n  }\n\n  for (ll in seq_along(ltype)) {\n    iix <- !is.na(type) & is.element(type, ltype[[ll]])\n\n    data.not.all.na <- apply(data[iix,,drop=FALSE], 1, function(x) {\n      any(!is.na(x))\n    })\n    drugpheno.not.all.na <- apply(drugpheno[iix,,drop=FALSE], 1, function(x) {\n      any(!is.na(x))\n    })\n    type.not.na <- !is.na(type[iix])\n    batch.not.na <- !is.na(batch[iix])\n\n    ccix <- data.not.all.na & drugpheno.not.all.na & type.not.na & batch.not.na\n\n    if (sum(ccix) < 3) {\n      ## not enough experiments\n      rest <- list(matrix(NA, nrow=ncol(data), ncol=length(nc), dimnames=list(colnames(data), nc)))\n      res <- c(res, rest)\n    } else {\n      splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n      splitix <- splitix[vapply(splitix, length, numeric(1)) > 0]\n      mcres <- BiocParallel::bplapply(splitix, function(x, data, type, batch, drugpheno, standardize) {\n        res <- t(apply(data[ , x, drop=FALSE], 2, geneRadSensitivity, type=type, batch=batch, drugpheno=drugpheno, verbose=verbose, standardize=standardize))\n        return(res)\n      }, data=data[iix, , drop=FALSE], type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix,,drop=FALSE], standardize=standardize)\n      rest <- do.call(rbind, mcres)\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      res <- c(res, list(rest))\n    }\n  }\n  names(res) <- names(ltype)\n  return(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rankGeneRadSensitivity` function, and what are its main input parameters?",
        "answer": "The `rankGeneRadSensitivity` function is designed to rank genes based on radiation effect in the Connectivity Map. Its main input parameters are:\n1. `data`: gene expression data matrix\n2. `drugpheno`: sensitivity values for the drug of interest\n3. `type`: cell or tissue type for each experiment\n4. `batch`: experiment batches\n5. `single.type`: boolean to determine if statistics should be computed for each cell/tissue type separately\n6. `standardize`: method to standardize the data (currently only supports 'SD')\n7. `nthread`: number of parallel threads for computation\n8. `verbose`: boolean to control printing of operation details to console"
      },
      {
        "question": "How does the function handle parallel processing, and what precautions are taken regarding the number of threads?",
        "answer": "The function handles parallel processing by:\n1. Checking if `nthread` is not 1\n2. Detecting available cores using `parallel::detectCores()`\n3. Setting `nthread` to the number of available cores if it's missing, less than 1, or greater than available cores\n4. Setting multicore options using `options(mc.cores=nthread)`\n5. Using `on.exit(options(op))` to restore original options when the function exits\n\nPrecautions:\n- The function ensures that the number of threads doesn't exceed the available cores\n- It temporarily modifies global options for multicore processing and restores them afterward"
      },
      {
        "question": "Explain the purpose of the `splitix` variable and how it's used in the parallel computation of gene radiation sensitivity.",
        "answer": "The `splitix` variable is used to divide the computation workload for parallel processing:\n\n1. It's created using `parallel::splitIndices(nx=ncol(data), ncl=nthread)`, which splits the column indices of the data matrix into chunks based on the number of threads.\n\n2. Empty chunks are removed with `splitix <- splitix[vapply(splitix, length, numeric(1)) > 0]`.\n\n3. It's then used in `BiocParallel::bplapply()` to apply the `geneRadSensitivity` function to subsets of the data columns in parallel:\n   ```R\n   mcres <- BiocParallel::bplapply(splitix, function(x, data, type, batch, drugpheno, standardize) {\n     res <- t(apply(data[ , x, drop=FALSE], 2, geneRadSensitivity, type=type, batch=batch, drugpheno=drugpheno, verbose=verbose, standardize=standardize))\n     return(res)\n   }, data=data[iix, , drop=FALSE], type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix,,drop=FALSE], standardize=standardize)\n   ```\n\nThis approach allows for efficient parallel computation of gene radiation sensitivity across the entire dataset."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rankGeneRadSensitivity <- function(data, drugpheno, type, batch, single.type=FALSE, standardize = \"SD\", nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  if(is.null(dim(drugpheno))){\n    drugpheno <- data.frame(drugpheno)\n  } else if(!is(drugpheno, \"data.frame\")) {\n    drugpheno <- as.data.frame(drugpheno)\n  }\n\n  if (missing(type) || all(is.na(type))) {\n    type <- array(\"other\", dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (missing(batch) || all(is.na(batch))) {\n    batch <- array(1, dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, duration, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  # Complete the function here\n}",
        "complete": "rankGeneRadSensitivity <- function(data, drugpheno, type, batch, single.type=FALSE, standardize = \"SD\", nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  if(is.null(dim(drugpheno))){\n    drugpheno <- data.frame(drugpheno)\n  } else if(!is(drugpheno, \"data.frame\")) {\n    drugpheno <- as.data.frame(drugpheno)\n  }\n\n  if (missing(type) || all(is.na(type))) {\n    type <- array(\"other\", dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (missing(batch) || all(is.na(batch))) {\n    batch <- array(1, dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, duration, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  res <- NULL\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  ccix <- complete.cases(data, type, batch, drugpheno)\n  nn <- sum(ccix)\n  nc <- if(!any(unlist(lapply(drugpheno,is.factor)))) {\n    if(ncol(drugpheno)>1) {\n      c(unlist(lapply(seq_len(ncol(drugpheno)), function(i) {\n        c(paste0(c(\"estimate\", \"se\", \"tstat\"), \".\", i))\n      })), \"n\", \"fstat\", \"pvalue\", \"fdr\")\n    } else {\n      c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\")\n    }\n  } else {\n    c(\"estimate\", \"se\", \"n\", \"pvalue\", \"fdr\")\n  }\n\n  res <- lapply(ltype, function(lt) {\n    iix <- !is.na(type) & is.element(type, lt)\n    ccix <- apply(data[iix,,drop=FALSE], 1, function(x) any(!is.na(x))) &\n            apply(drugpheno[iix,,drop=FALSE], 1, function(x) any(!is.na(x))) &\n            !is.na(type[iix]) & !is.na(batch[iix])\n    \n    if (sum(ccix) < 3) {\n      return(matrix(NA, nrow=ncol(data), ncol=length(nc), dimnames=list(colnames(data), nc)))\n    }\n    \n    splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n    mcres <- BiocParallel::bplapply(splitix[lengths(splitix) > 0], function(x) {\n      t(apply(data[iix, x, drop=FALSE], 2, geneRadSensitivity, type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix,,drop=FALSE], verbose=verbose, standardize=standardize))\n    })\n    rest <- do.call(rbind, mcres)\n    cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n  })\n\n  return(res)\n}"
      },
      {
        "partial": "geneRadSensitivity <- function(x, type, batch, drugpheno, verbose=FALSE, standardize=\"SD\") {\n  # Implement the function body here\n}",
        "complete": "geneRadSensitivity <- function(x, type, batch, drugpheno, verbose=FALSE, standardize=\"SD\") {\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  x <- x[ccix]\n  type <- type[ccix]\n  batch <- batch[ccix]\n  drugpheno <- drugpheno[ccix, , drop=FALSE]\n\n  if (length(x) < 3) return(rep(NA, 7))\n\n  if (standardize == \"SD\") {\n    x <- scale(x)\n  }\n\n  if (is.factor(drugpheno[,1])) {\n    fit <- try(summary(lm(x ~ drugpheno[,1] + type + batch)))\n  } else {\n    fit <- try(summary(lm(x ~ drugpheno + type + batch)))\n  }\n\n  if (inherits(fit, \"try-error\")) return(rep(NA, 7))\n\n  coef <- fit$coefficients[2, ]\n  c(coef[1:2], n=nrow(fit$model), coef[3:4], fit$fstatistic[1], fit$df[2])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/summarizeMolecularProfiles-methods.R",
    "language": "R",
    "content": "#' Takes molecular data from a RadioSet, and summarises them\n#' into one entry per drug\n#'\n#' Given a RadioSet with molecular data, this function will summarize\n#' the data into one profile per cell line, using the chosed summary.stat. Note\n#' that this does not really make sense with perturbation type data, and will\n#' combine experiments and controls when doing the summary if run on a\n#' perturbation dataset.\n#'\n#' @examples\n#' data(clevelandSmall)\n#' clevelandSmall <- summarizeMolecularProfiles(clevelandSmall,\n#'                     mDataType = \"rna\", cell.lines=sampleNames(clevelandSmall),\n#'                     summary.stat = 'median', fill.missing = TRUE, verbose=TRUE)\n#' clevelandSmall\n#'\n#' @param object \\code{RadioSet} The RadioSet to summarize\n#' @param mDataType \\code{character} which one of the molecular data types\n#' to use in the analysis, out of all the molecular data types available for the rSet\n#' for example: rna, rnaseq, snp\n#' @param cell.lines \\code{character} The cell lines to be summarized.\n#'   If any cell.line has no data, missing values will be created\n#' @param features \\code{caracter} A vector of the feature names to include in the summary\n#' @param summary.stat \\code{character} which summary method to use if there are repeated\n#'   cell.lines? Choices are \"mean\", \"median\", \"first\", or \"last\"\n#'   In case molecular data type is mutation or fusion \"and\" and \"or\" choices are available\n#' @param fill.missing \\code{boolean} should the missing cell lines not in the\n#'   molecular data object be filled in with missing values?\n#' @param summarize A flag which when set to FALSE (defaults to TRUE) disables\n#'   summarizing and returns the data unchanged as a ExpressionSet\n#' @param verbose \\code{boolean} should messages be printed\n#' @return \\code{matrix} An updated RadioSet with the molecular data summarized\n#'   per cell line.\n#'\n#' @importMethodsFrom CoreGx summarizeMolecularProfiles\n#' @export\nsetMethod('summarizeMolecularProfiles',\n          signature(object='RadioSet'),\n          function(object, mDataType, cell.lines, features, summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                   fill.missing=TRUE, summarize=TRUE, verbose=TRUE) {\n              .summarizeMolecularProfilesRadioSet(object=object, mDataType=mDataType, cell.lines=cell.lines,\n                                                   features=features, summary.stat=summary.stat,\n                                                   fill.missing=fill.missing, summarize=summarize, verbose=verbose)\n          })\n\n# Takes molecular data from a RadioSet, and summarises them\n# into one entry per drug\n#\n# Given a RadioSet with molecular data, this function will summarize\n# the data into one profile per cell line, using the chosed summary.stat. Note\n# that this does not really make sense with perturbation type data, and will\n# combine experiments and controls when doing the summary if run on a\n# perturbation dataset.\n#\n# @examples\n# data(clevelandSmall)\n# clevelandSmall <- summarizeMolecularProfiles(clevelandSmall,\n#                     mDataType = \"rna\", cell.lines=sampleNames(clevelandSmall),\n#                     summary.stat = 'median', fill.missing = TRUE, verbose=TRUE)\n# clevelandSmall\n#\n# @param object \\code{RadioSet} The RadioSet to summarize\n# @param mDataType \\code{character} which one of the molecular data types\n# to use in the analysis, out of all the molecular data types available for the rSet\n# for example: rna, rnaseq, snp\n# @param cell.lines \\code{character} The cell lines to be summarized.\n#   If any cell.line has no data, missing values will be created\n# @param features \\code{caracter} A vector of the feature names to include in the summary\n# @param summary.stat \\code{character} which summary method to use if there are repeated\n#   cell.lines? Choices are \"mean\", \"median\", \"first\", or \"last\"\n#   In case molecular data type is mutation or fusion \"and\" and \"or\" choices are available\n# @param fill.missing \\code{boolean} should the missing cell lines not in the\n#   molecular data object be filled in with missing values?\n# @param summarize A flag which when set to FALSE (defaults to TRUE) disables\n#   summarizing and returns the data unchanged as a ExpressionSet\n# @param verbose \\code{boolean} should messages be printed\n# @return \\code{matrix} An updated RadioSet with the molecular data summarized\n#   per cell line.\n#\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom SummarizedExperiment SummarizedExperiment rowData rowData<- colData colData<- assays assays<- assayNames assayNames<-\n#' @importFrom Biobase AnnotatedDataFrame\n#' @importFrom matrixStats rowMeans2 rowMedians\n#' @keywords internal\n.summarizeMolecularProfilesRadioSet <- function(object,\n                                       mDataType,\n                                       cell.lines,\n                                       features,\n                                       summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                                       fill.missing=TRUE,\n                                       summarize=TRUE,\n                                       verbose=TRUE) {\n\n\n  ### Placed here to make sure the object argument gets checked first by R.\n  mDataTypes <- names(molecularProfilesSlot(object))\n  if (!(mDataType %in% mDataTypes)) {\n    stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n  }\n\n  if(summarize==FALSE){\n    return(molecularProfilesSlot(object)[[mDataType]])\n  }\n\n  if (missing(features)) {\n    features <- rownames(featureInfo(object, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n    if (verbose && !all(fix)) {\n      warning (sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  summary.stat <- match.arg(summary.stat)\n  if((!S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) {\n    stop (\"Invalid summary.stat, choose among: mean, median, first, last\" )\n  }\n  if((S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"and\", \"or\"))) {\n    stop (\"Invalid summary.stat, choose among: and, or\" )\n  }\n\n  if (missing(cell.lines)) {\n    cell.lines <- sampleNames(object)\n  }\n\n  dd <- molecularProfiles(object, mDataType)\n  pp <- phenoInfo(object, mDataType)\n\n  if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation == \"mutation\") {\n    tt <- dd\n    tt[which(!is.na(dd) & dd ==\"wt\")] <- FALSE\n    tt[which(!is.na(dd) & dd !=\"wt\")] <- TRUE\n    tt <- apply(tt, 2, as.logical)\n    dimnames(tt) <- dimnames(dd)\n    dd <- tt\n  }\n  if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation == \"fusion\") {\n    tt <- dd\n    tt[which(!is.na(dd) & dd ==\"0\")] <- FALSE\n    tt[which(!is.na(dd) & dd !=\"0\")] <- TRUE\n    tt <- apply(tt, 2, as.logical)\n    dimnames(tt) <- dimnames(dd)\n    dd <- tt\n  }\n  if (any(colnames(dd) != rownames(pp))) {\n    warning (\"Samples in phenodata and expression matrices must be ordered the same way\")\n    dd <- dd[ , rownames(pp), drop=FALSE]\n  }\n  if (!fill.missing) {\n    cell.lines <- intersect(cell.lines, unique(pp[!is.na(pp[ , \"sampleid\"]), \"sampleid\"]))\n  }\n  if (length(cell.lines) == 0) {\n    stop (\"No cell lines in common\")\n  }\n\n  ## select profiles with no replicates\n  duplix <- unique(pp[!is.na(pp[ , \"sampleid\"]) & duplicated(pp[ , \"sampleid\"]), \"sampleid\"])\n  ucell <- setdiff(cell.lines, duplix)\n\n  ## keep the non ambiguous cases\n  dd2 <- dd[ , match(ucell, pp[ , \"sampleid\"]), drop=FALSE]\n  pp2 <- pp[match(ucell, pp[ , \"sampleid\"]), , drop=FALSE]\n  if (length(duplix) > 0) {\n    if (verbose) {\n      message(sprintf(\"Summarizing %s molecular data for:\\t%s\", mDataType, annotation(object)$name))\n      total <- length(duplix)\n      # create progress bar\n      pb <- utils::txtProgressBar(min=0, max=total, style=3)\n      i <- 1\n    }\n    ## replace factors by characters to allow for merging duplicated experiments\n    pp2 <- apply(pp2, 2, function (x) {\n      if (is.factor(x)) {\n        return (as.character(x))\n      } else {\n        return (x)\n      }\n    })\n    ## there are some replicates to collapse\n    for (x in duplix) {\n      myx <- which(!is.na(pp[ , \"sampleid\"]) & is.element(pp[ , \"sampleid\"], x))\n      switch(summary.stat,\n        \"mean\" = {\n          ddt <- rowMeans2(dd[ , myx, drop=FALSE])\n        },\n        \"median\"={\n          ddt <- rowMedians(dd[ , myx, drop=FALSE])\n        },\n        \"first\"={\n          ddt <- dd[ , myx[1], drop=FALSE]\n        },\n        \"last\" = {\n          ddt <- dd[ , myx[length(myx)], drop=FALSE]\n        },\n        \"and\" = {\n          ddt <- apply(dd[ , myx, drop=FALSE], 1, function(x) do.call(`&`, as.list(x)))\n        },\n        \"or\" = {\n          ddt <- apply(dd[ , myx, drop=FALSE], 1, function(x) do.call(`|`, as.list(x)))\n        }\n      )\n      ppt <- apply(pp[myx, , drop=FALSE], 2, function (x) {\n        x <- paste(unique(as.character(x[!is.na(x)])), collapse=\"///\")\n        return (x)\n      })\n      ppt[!is.na(ppt) & ppt == \"\"] <- NA\n      dd2 <- cbind(dd2, ddt)\n      pp2 <- rbind(pp2, ppt)\n      if (verbose){\n        utils::setTxtProgressBar(pb, i)\n        i <- i + 1\n      }\n    }\n    if (verbose) {\n      close(pb)\n    }\n  }\n  colnames(dd2) <- rownames(pp2) <- c(ucell, duplix)\n\n  ## reorder cell lines\n  dd2 <- dd2[ , cell.lines, drop=FALSE]\n  pp2 <- pp2[cell.lines, , drop=FALSE]\n  pp2[ , \"sampleid\"] <- cell.lines\n  res <- molecularProfilesSlot(object)[[mDataType]]\n  if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\", \"fusion\")) {\n    tt <- dd2\n    tt[which(!is.na(dd2) & dd2)] <- \"1\"\n    tt[which(!is.na(dd2) & !dd2)] <- \"0\"\n    dd2 <- tt\n  }\n  res <- SummarizedExperiment::SummarizedExperiment(dd2)\n  pp2 <- S4Vectors::DataFrame(pp2, row.names=rownames(pp2))\n  pp2$tissueid <- sampleInfo(object)[pp2$sampleid, \"tissueid\"]\n  SummarizedExperiment::colData(res) <- pp2\n  SummarizedExperiment::rowData(res) <- featureInfo(object, mDataType)\n  ##TODO:: Generalize this to multiple assay SummarizedExperiments!\n  if(!is.null(SummarizedExperiment::assay(res, 1))) {\n    SummarizedExperiment::assay(res, 2) <-\n      matrix(rep(NA, length(assay(res, 1))),\n             nrow=nrow(assay(res, 1)),\n             ncol=ncol(assay(res, 1)),\n             dimnames=dimnames(assay(res, 1))\n      )\n  }\n  assayNames(res) <- assayNames(molecularProfilesSlot(object)[[mDataType]])\n  res <- res[features,]\n  S4Vectors::metadata(res) <- S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])\n  return(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeMolecularProfiles` function in the given code snippet?",
        "answer": "The `summarizeMolecularProfiles` function is designed to summarize molecular data from a RadioSet object into one profile per cell line. It takes molecular data and condenses it using a specified summary statistic (e.g., mean, median, first, or last). This function is particularly useful for summarizing data when there are repeated cell lines, allowing for a more concise representation of the molecular profiles."
      },
      {
        "question": "How does the function handle different types of molecular data, particularly for mutation and fusion data?",
        "answer": "For mutation and fusion data, the function uses a different approach. Instead of using mean, median, first, or last as summary statistics, it offers 'and' and 'or' options. For mutation data, it converts 'wt' (wild type) to FALSE and any other value to TRUE. For fusion data, it converts '0' to FALSE and any other value to TRUE. This boolean representation allows for logical operations ('and' or 'or') to be applied when summarizing the data across replicates."
      },
      {
        "question": "What is the significance of the `fill.missing` parameter in the `summarizeMolecularProfiles` function?",
        "answer": "The `fill.missing` parameter is a boolean flag that determines how the function handles cell lines that are not present in the molecular data object. When set to TRUE (default), the function will create entries with missing values for these cell lines, ensuring that all specified cell lines are represented in the output. If set to FALSE, the function will only include cell lines that have data in the molecular profiles, potentially resulting in fewer cell lines in the output than initially specified."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('summarizeMolecularProfiles',\n          signature(object='RadioSet'),\n          function(object, mDataType, cell.lines, features, summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                   fill.missing=TRUE, summarize=TRUE, verbose=TRUE) {\n              # Complete the function body\n          })",
        "complete": "setMethod('summarizeMolecularProfiles',\n          signature(object='RadioSet'),\n          function(object, mDataType, cell.lines, features, summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                   fill.missing=TRUE, summarize=TRUE, verbose=TRUE) {\n              .summarizeMolecularProfilesRadioSet(object=object, mDataType=mDataType, cell.lines=cell.lines,\n                                                   features=features, summary.stat=summary.stat,\n                                                   fill.missing=fill.missing, summarize=summarize, verbose=verbose)\n          })"
      },
      {
        "partial": ".summarizeMolecularProfilesRadioSet <- function(object, mDataType, cell.lines, features,\n                                       summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                                       fill.missing=TRUE, summarize=TRUE, verbose=TRUE) {\n  # Implement the function body\n}",
        "complete": ".summarizeMolecularProfilesRadioSet <- function(object, mDataType, cell.lines, features,\n                                       summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                                       fill.missing=TRUE, summarize=TRUE, verbose=TRUE) {\n  mDataTypes <- names(molecularProfilesSlot(object))\n  if (!(mDataType %in% mDataTypes)) {\n    stop(sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n  }\n  if(summarize==FALSE) return(molecularProfilesSlot(object)[[mDataType]])\n  if (missing(features)) features <- rownames(featureInfo(object, mDataType))\n  else {\n    fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n    if (verbose && !all(fix)) warning(sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n    features <- features[fix]\n  }\n  summary.stat <- match.arg(summary.stat)\n  if ((!S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) && (!summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) {\n    stop(\"Invalid summary.stat, choose among: mean, median, first, last\")\n  }\n  if ((S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) && (!summary.stat %in% c(\"and\", \"or\"))) {\n    stop(\"Invalid summary.stat, choose among: and, or\")\n  }\n  if (missing(cell.lines)) cell.lines <- sampleNames(object)\n  dd <- molecularProfiles(object, mDataType)\n  pp <- phenoInfo(object, mDataType)\n  # ... (rest of the function implementation)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/gwc.R",
    "language": "R",
    "content": "## - http://en.wikipedia.org/wiki/Pearson_product-moment_correlation_coefficient#Calculating_a_weighted_correlation\n## http://www.mathworks.com/matlabcentral/fileexchange/20846-weighted-correlation-matrix F. Pozzi, T. Di Matteo, T. Aste, 'Exponential\n## smoothing weighted correlations', The European Physical Journal B, Vol. 85, No 6, 2012. DOI: 10.1140/epjb/e2012-20697-x TODO:: Give\n## this function a more descriptive name\n#' GWC Score\n#' \n#' Calculate the gwc score between two vectors, using either a weighted spearman \n#'  or pearson correlation\n#'\n#' @examples\n#' data(clevelandSmall_cSet)\n#' x <- molecularProfiles(clevelandSmall_cSet,'rna')[,1]\n#' y <- molecularProfiles(clevelandSmall_cSet,'rna')[,2]\n#' x_p <- rep(0.05, times=length(x))\n#' y_p <- rep(0.05, times=length(y))\n#' names(x_p) <- names(x)\n#' names(y_p) <- names(y)\n#' gwc(x,x_p,y,y_p, nperm=100)\n#'\n#'@param x1 \\code{numeric} vector of effect sizes (e.g., fold change or t statitsics) for the first experiment\n#'@param p1 \\code{numeric} vector of p-values for each corresponding effect size for the first experiment\n#'@param x2 \\code{numeric} effect size (e.g., fold change or t statitsics) for the second experiment\n#'@param p2 \\code{numeric} vector of p-values for each corresponding effect size for the second experiment\n#'@param method.cor \\code{character} string identifying if a \\code{pearson} or\n#'\\code{spearman} correlation should be used\n#'@param nperm \\code{numeric} how many permutations should be done to determine\n#'@param truncate.p \\code{numeric} Truncation value for extremely low p-values\n#'@param ... Other passed down to internal functions\n#'\n#'@return \\code{numeric} a vector of two values, the correlation and associated p-value.\n#'@export\ngwc <- function(x1, p1, x2, p2, method.cor = c(\"pearson\", \"spearman\"), nperm = 10000, truncate.p = 1e-16, ...) {\n    method.cor <- match.arg(method.cor)\n    ## intersection between x and y\n    ii <- .intersectList(names(x1), names(p1), names(x2), names(p2))\n    if (length(ii) < 10) {\n        stop(\"Less than 10 probes/genes in common between x and y\")\n    }\n    x1 <- x1[ii]\n    p1 <- p1[ii]\n    x2 <- x2[ii]\n    p2 <- p2[ii]\n    ## truncate extremely low p-values\n    p1[!is.na(p1) & p1 < truncate.p] <- truncate.p\n    p2[!is.na(p2) & p2 < truncate.p] <- truncate.p\n    ## scaled weights\n    p1 <- -log10(p1)\n    p1 <- p1/sum(p1, na.rm = TRUE)\n    p2 <- -log10(p2)\n    p2 <- p2/sum(p2, na.rm = TRUE)\n    w <- p1 + p2\n    ## compute genome-wide connectivity score\n    res <- .corWeighted(x = x1, y = x2, w = w, method = method.cor, nperm = nperm, ...)\n    return(res)\n}\n\n#### Compute a weighted correlation coefficient inspired from package boot - TODO:: Write function documentation?\n#' @importFrom stats cov.wt complete.cases\n.corWeighted <- function(x, y, w, method = c(\"pearson\", \"spearman\"), alternative = c(\"two.sided\", \"greater\", \"less\"), nperm = 0, nthread = 1, \n    na.rm = FALSE) {\n    \n    ###################### \n    wcor <- function(d, w, na.rm = TRUE) {\n        CovM <- cov.wt(d, wt = w)[[\"cov\"]]\n        res <- CovM[1, 2]/sqrt(CovM[1, 1] * CovM[2, 2])\n        return(res)\n    }\n    \n    ###################### \n    \n    if (missing(w)) {\n        w <- rep(1, length(x))/length(x)\n    }\n    if (length(x) != length(y) || length(x) != length(w)) {\n        stop(\"x, y, and w must have the same length\")\n    }\n    method <- match.arg(method)\n    if (method == \"spearman\") {\n        x <- rank(x)\n        y <- rank(y)\n    }\n    alternative <- match.arg(alternative)\n    \n    res <- c(rho = NA, p = NA)\n    \n    ## remove missing values\n    ccix <- complete.cases(x, y, w)\n    if (!all(ccix) && !na.rm) {\n        warning(\"Missing values are present\")\n    }\n    if (sum(ccix) < 3) {\n        return(res)\n    }\n    x <- x[ccix]\n    y <- y[ccix]\n    w <- w[ccix]\n    \n    wc <- wcor(d = cbind(x, y), w = w)\n    res[\"rho\"] <- wc\n    if (nperm > 1) {\n        splitix <- parallel::splitIndices(nx = nperm, ncl = nthread)\n        if (!is.list(splitix)) {\n            splitix <- list(splitix)\n        }\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- BiocParallel::bplapply(splitix, function(x, xx, yy, ww) {\n            pres <- vapply(x, function(x, xx, yy, ww) {\n                ## permute the data and the weights\n                d2 <- cbind(xx[sample(seq_len(length(xx)))], yy[sample(seq_len(length(yy)))])\n                w2 <- ww[sample(seq_len(length(ww)))]\n                return(wcor(d = d2, w = w2))\n            }, xx = xx, yy = yy, ww = ww, FUN.VALUE = double(1))\n            return(pres)\n        }, xx = x, yy = y, ww = w)\n        perms <- do.call(c, mcres)\n        \n        switch(alternative, two.sided = {\n            if (res[\"rho\"] < 0) {\n                p <- sum(perms <= res, na.rm = TRUE)\n            } else {\n                p <- sum(perms >= res, na.rm = TRUE)\n            }\n            if (p == 0) {\n                p <- 1/(nperm + 1)\n            } else {\n                p <- p/nperm\n            }\n            p <- p * 2\n        }, greater = {\n            p <- sum(perms >= res, na.rm = TRUE)\n            if (p == 0) {\n                p <- 1/(nperm + 1)\n            } else {\n                p <- p/nperm\n            }\n        }, less = {\n            p <- sum(perms <= res, na.rm = TRUE)\n            if (p == 0) {\n                p <- 1/(nperm + 1)\n            } else {\n                p <- p/nperm\n            }\n        })\n        res[\"p\"] <- p\n    }\n    return(res)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `gwc` function and what are its main input parameters?",
        "answer": "The `gwc` function calculates the genome-wide connectivity (GWC) score between two vectors using either a weighted Spearman or Pearson correlation. Its main input parameters are:\n- x1 and x2: numeric vectors of effect sizes for two experiments\n- p1 and p2: corresponding p-values for the effect sizes\n- method.cor: specifies whether to use Pearson or Spearman correlation\n- nperm: number of permutations for determining statistical significance\n- truncate.p: value for truncating extremely low p-values"
      },
      {
        "question": "How does the `.corWeighted` function handle missing values and what is the minimum number of non-missing values required for the function to proceed?",
        "answer": "The `.corWeighted` function handles missing values by using the `complete.cases` function to identify and remove any rows with missing values in x, y, or w. If the `na.rm` parameter is set to `FALSE` and missing values are present, a warning is issued. The function requires at least 3 non-missing values to proceed with the calculation. If there are fewer than 3 complete cases, the function returns a result vector with NA values for both the correlation coefficient (rho) and p-value."
      },
      {
        "question": "Explain the permutation process used in the `.corWeighted` function to calculate the p-value.",
        "answer": "The permutation process in `.corWeighted` function works as follows:\n1. It performs `nperm` number of permutations.\n2. For each permutation, it randomly shuffles the data (x and y) and weights.\n3. It calculates the weighted correlation for each permuted dataset.\n4. It then compares the original correlation to the distribution of permuted correlations.\n5. The p-value is calculated based on the proportion of permuted correlations that are as extreme as or more extreme than the original correlation.\n6. The calculation method depends on the specified alternative hypothesis (two.sided, greater, or less).\n7. If no permuted correlations are more extreme, it uses a conservative p-value of 1/(nperm+1).\nThis process allows for empirical estimation of the statistical significance of the weighted correlation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "gwc <- function(x1, p1, x2, p2, method.cor = c(\"pearson\", \"spearman\"), nperm = 10000, truncate.p = 1e-16, ...) {\n    method.cor <- match.arg(method.cor)\n    ii <- .intersectList(names(x1), names(p1), names(x2), names(p2))\n    if (length(ii) < 10) {\n        stop(\"Less than 10 probes/genes in common between x and y\")\n    }\n    x1 <- x1[ii]\n    p1 <- p1[ii]\n    x2 <- x2[ii]\n    p2 <- p2[ii]\n    p1[!is.na(p1) & p1 < truncate.p] <- truncate.p\n    p2[!is.na(p2) & p2 < truncate.p] <- truncate.p\n    # Complete the function by adding the remaining steps\n}",
        "complete": "gwc <- function(x1, p1, x2, p2, method.cor = c(\"pearson\", \"spearman\"), nperm = 10000, truncate.p = 1e-16, ...) {\n    method.cor <- match.arg(method.cor)\n    ii <- .intersectList(names(x1), names(p1), names(x2), names(p2))\n    if (length(ii) < 10) {\n        stop(\"Less than 10 probes/genes in common between x and y\")\n    }\n    x1 <- x1[ii]\n    p1 <- p1[ii]\n    x2 <- x2[ii]\n    p2 <- p2[ii]\n    p1[!is.na(p1) & p1 < truncate.p] <- truncate.p\n    p2[!is.na(p2) & p2 < truncate.p] <- truncate.p\n    p1 <- -log10(p1)\n    p1 <- p1/sum(p1, na.rm = TRUE)\n    p2 <- -log10(p2)\n    p2 <- p2/sum(p2, na.rm = TRUE)\n    w <- p1 + p2\n    res <- .corWeighted(x = x1, y = x2, w = w, method = method.cor, nperm = nperm, ...)\n    return(res)\n}"
      },
      {
        "partial": ".corWeighted <- function(x, y, w, method = c(\"pearson\", \"spearman\"), alternative = c(\"two.sided\", \"greater\", \"less\"), nperm = 0, nthread = 1, na.rm = FALSE) {\n    wcor <- function(d, w, na.rm = TRUE) {\n        CovM <- cov.wt(d, wt = w)[[\"cov\"]]\n        res <- CovM[1, 2]/sqrt(CovM[1, 1] * CovM[2, 2])\n        return(res)\n    }\n    # Complete the function by adding the remaining steps\n}",
        "complete": ".corWeighted <- function(x, y, w, method = c(\"pearson\", \"spearman\"), alternative = c(\"two.sided\", \"greater\", \"less\"), nperm = 0, nthread = 1, na.rm = FALSE) {\n    wcor <- function(d, w, na.rm = TRUE) {\n        CovM <- cov.wt(d, wt = w)[[\"cov\"]]\n        res <- CovM[1, 2]/sqrt(CovM[1, 1] * CovM[2, 2])\n        return(res)\n    }\n    if (missing(w)) w <- rep(1, length(x))/length(x)\n    if (length(x) != length(y) || length(x) != length(w)) stop(\"x, y, and w must have the same length\")\n    method <- match.arg(method)\n    if (method == \"spearman\") {\n        x <- rank(x)\n        y <- rank(y)\n    }\n    alternative <- match.arg(alternative)\n    res <- c(rho = NA, p = NA)\n    ccix <- complete.cases(x, y, w)\n    if (!all(ccix) && !na.rm) warning(\"Missing values are present\")\n    if (sum(ccix) < 3) return(res)\n    x <- x[ccix]; y <- y[ccix]; w <- w[ccix]\n    wc <- wcor(d = cbind(x, y), w = w)\n    res[\"rho\"] <- wc\n    if (nperm > 1) {\n        splitix <- parallel::splitIndices(nx = nperm, ncl = nthread)\n        if (!is.list(splitix)) splitix <- list(splitix)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- BiocParallel::bplapply(splitix, function(x, xx, yy, ww) {\n            vapply(x, function(x, xx, yy, ww) {\n                d2 <- cbind(xx[sample(seq_len(length(xx)))], yy[sample(seq_len(length(yy)))])\n                w2 <- ww[sample(seq_len(length(ww)))]\n                wcor(d = d2, w = w2)\n            }, xx = xx, yy = yy, ww = ww, FUN.VALUE = double(1))\n        }, xx = x, yy = y, ww = w)\n        perms <- do.call(c, mcres)\n        p <- switch(alternative,\n            two.sided = 2 * min(sum(perms <= wc), sum(perms >= wc)) / nperm,\n            greater = sum(perms >= wc) / nperm,\n            less = sum(perms <= wc) / nperm\n        )\n        res[\"p\"] <- max(p, 1 / (nperm + 1))\n    }\n    return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/LongTable-class.R",
    "language": "R",
    "content": "#' @include immutable-class.R\n#' @include allGenerics.R\nNULL\n\n#' @title LongTable class definition\n#'\n#' @description Define a private constructor method to be used to build a\n#'   `LongTable` object.\n#'\n#' @slot rowData See Slots section.\n#' @slot colData See Slots section.\n#' @slot assays See Slots section.\n#' @slot metadata See Slots section.\n#' @slot .intern See Slots section.\n#'\n#' @section Slots:\n#' - *rowData*: A `data.table` containing the metadata associated with the\n#'   row dimension of a `LongTable`.\n#' - *colData*: A `data.table` containing the metadata associated with the\n#'   column dimension of a `LongTable`.\n#' - *assays*: A `list` of `data.table`s, one for each assay in a\n#'   `LongTable`.\n#' - *metadata*: An optional `list` of additional metadata for a `LongTable`\n#'   which doesn't map to one of the dimensions.\n#' - *.intern*: An `immutable` `list` that holds internal structural metadata\n#'   about a LongTable object, such as which columns are required to key\n#'   the object.\n#'\n#' @return `LongTable` object containing the assay data from a treatment\n#'   response experiment\n#'\n#' @md\n#' @import data.table\n#' @keywords internal\n#' @rdname LongTable-class\n#' @aliases .LongTable\n#' @exportClass LongTable\n.LongTable <- setClass(\"LongTable\",\n    slots=list(\n        rowData='data.table',\n        colData='data.table',\n        assays='list',\n        metadata='list',\n        .intern='immutable_list')\n)\n\n\n#' @title LongTable constructor method\n#'\n#' @rdname LongTable\n#'\n#' @param rowData `data.frame` A rectangular object coercible to a `data.table`.\n#' @param rowIDs `character` A vector of `rowData` column names needed to\n#'   uniquely identify each row in a `LongTable`.\n#' @param colData `data.frame` A rectangular object coercible to a `data.table.`\n#' @param colIDs `chacter` A vector of `colData` column names needed to uniquely\n#'   identify each column in a `LongTable`.\n#' @param assays `list` A list of rectangular objects, each coercible to\n#'   a `data.table`. Must be named and item names must match the `assayIDs`\n#'   list.\n#' @param assayIDs `list` A list of `character` vectors specifying the columns\n#'   needed to uniquely identify each row in an `assay`. Names must match the\n#'   `assays` list.\n#' @param metadata `list` A list of one or more metadata items associated with\n#'   a LongTable experiment.\n#' @param keep.rownames `logical(1)` or `character(1)` Should rownames be\n#'   retained when coercing to `data.table` inside the constructor. Default\n#'   is FALSE. If TRUE, adds a 'rn' column to each rectangular object that\n#'   gets coerced from `data.frame` to `data.table`. If a string, that becomes\n#'   the name of the rownames column.\n#'\n#' @return A `LongTable` object containing the data for a treatment response\n#'   experiment and configured according to the rowIDs and colIDs arguments.\n#'\n#' @examples\n#' \"See vignette('The LongTable Class', package='CoreGx')\"\n#'\n#' @importFrom data.table key setkeyv\n#' @export\nLongTable <- function(rowData, rowIDs, colData, colIDs, assays, assayIDs,\n        metadata=list(), keep.rownames=FALSE) {\n\n    # handle missing parameters\n    isMissing <- c(rowData=missing(rowData), rowIDs=missing(rowIDs),\n        colIDs=missing(colIDs), colData=missing(colData), assays=missing(assays),\n        assayIDs=missing(assayIDs))\n\n    if (any(isMissing)) stop(.errorMsg('\\nRequired parameter(s) missing: ',\n        names(isMissing)[isMissing], collapse='\\n\\t'))\n\n    # check parameter types and coerce or error\n    if (!is(colData, \"data.table\")) {\n        tryCatch({\n            colData <- data.table(colData, keep.rownames=keep.rownames)\n        }, error=function(e)\n            stop(.errorMsg(\"colData must be coercible to a data.frame!\"))\n        )\n    } else {\n        colData <- copy(colData)\n    }\n\n    if (!is(rowData, \"data.table\")) {\n        tryCatch({\n            rowData <- data.table(rowData, keep.rownames=keep.rownames) },\n        error=function(e)\n            stop(.errorMsg(\"rowData must be coerceible to a data.frame!\"))\n        )\n    } else {\n        rowData <- copy(rowData)\n    }\n\n    isDT <- is.items(assays, FUN=is.data.table)\n    isDF <- is.items(assays, FUN=is.data.frame) & !isDT\n    if (!all(isDT)) {\n        tryCatch({\n            for (i in which(isDF))\n                assays[[i]] <- data.table(assays[[i]], keep.rownames)\n        }, error = function(e, assays) {\n            message(e)\n            types <- lapply(assays, typeof)\n            stop(.errorMsg(\n                '\\nList items are types: ',\n                types, '\\nPlease ensure all items in the assays list are ',\n                'coerceable to a data.frame!'), collapse=', ')\n        })\n    }\n    assays <- copy(assays)\n\n    ## FIXME:: Move all validity checks to top of the function to prevent wasted\n    ## computation or into class validity method\n\n    # capture row internal metadata\n    if (is.numeric(rowIDs) || is.logical(rowIDs))\n        rowIDs <- colnames(rowData)[rowIDs]\n    if (!all(rowIDs %in% colnames(rowData)))\n        stop(.errorMsg('\\nRow IDs not in rowData: ',\n            setdiff(rowIDs, colnames(rowData)), collapse=', '))\n\n    # Create the row and column keys for LongTable internal mappings\n    if (!('rowKey' %in% colnames(rowData)))\n        rowData[, c('rowKey') := .GRP, keyby=c(rowIDs)]\n    if (!('colKey' %in% colnames(colData)))\n        colData[, c('colKey') := .GRP, keyby=c(colIDs)]\n\n    # initialize the internals object to store private metadata for a LongTable\n    internals <- setNames(vector(\"list\", length=6),\n        c(\"rowIDs\", \"rowMeta\", \"colIDs\", \"colMeta\", \"assayKeys\", \"assayIndex\"))\n    internals$rowIDs <- rowIDs\n    internals$rowMeta <- setdiff(colnames(rowData[, -'rowKey']), rowIDs)\n\n    # capture column internal metadata\n    if (is.numeric(colIDs) || is.logical(colIDs))\n        colIDs <- colnames(colData)[colIDs]\n    if (!all(colIDs %in% colnames(colData)))\n        stop(.errorMsg('\\nColumn IDs not in colData: ',\n            setdiff(colIDs, colnames(colData)), collapse=', '),\n            call.=FALSE)\n    internals$colIDs <- colIDs\n    internals$colMeta <- setdiff(colnames(colData[, -'colKey']), colIDs)\n\n    # -- capture assays internal metadata\n    # sort such that rowIDs are first, then colIDs; ensures reindex returns\n    # the same order as construtor\n    for (i in seq_along(assayIDs)) {\n        rids <- intersect(rowIDs, assayIDs[[i]])\n        cids <- intersect(colIDs, assayIDs[[i]])\n        assayIDs[[i]] <- c(rids, cids)\n    }\n    internals$assayKeys <- assayIDs\n\n    # ensure names of assays and assayIDs match\n    hasMatchingAssayNames <- names(assays) == names(assayIDs)\n    if (!all(hasMatchingAssayNames)) stop(.errorMsg(\n        \"Mismatched names between assays and assayIDs for:\\n\\t\",\n        paste0(names(assays)[!hasMatchingAssayNames], collapse=\", \")),\n        call.=FALSE)\n    # set keys for join with metadata\n    for (nm in names(assays)) {\n        setkeyv(assays[[nm]], assayIDs[[nm]])\n        .nm <- paste0(\".\", nm)\n        assays[[nm]][, (.nm) := .I]\n    }\n    \n    # build the index mapping assay rows to rowKey and colKey\n    cat(.infoMsg(\"Building assay index...\\n\", time=TRUE))\n    assayIndex <- expand.grid(rowKey=rowData$rowKey, colKey=colData$colKey)\n    setDT(assayIndex)\n    # setkeyv(assayIndex, c(\"rowKey\", \"colKey\"))\n    \n    cat(.infoMsg(\"Joining rowData to assayIndex...\\n\", time=TRUE))\n    setkeyv(rowData, \"rowKey\")\n    setkeyv(assayIndex, \"rowKey\")\n    # assayIndex <- assayIndex[\n    #     rowData[, c(rowIDs, \"rowKey\"), with=FALSE], ,\n    #     on=\"rowKey\", allow.cartesian=FALSE\n    # ]\n    rd <- rowData[, c(rowIDs, \"rowKey\"), with=FALSE]\n    assayIndex <- merge(\n        assayIndex, rd,\n        by=\"rowKey\", all.x=TRUE, allow.cartesian=FALSE\n    )\n\n    # print if rowKey in rowData is not unique\n    if(nrow(rowData) != uniqueN(rowData$rowKey)) {\n        cat(.warnMsg(\"rowData rowKey is not unique!\"))\n        show(assayIndex)\n        show(rowData)\n    }\n    rm(rd)\n    gc()\n    cat(.infoMsg(\"Joining colData to assayIndex...\\n\", time=TRUE))\n    setkeyv(colData, \"colKey\")\n    # assayIndex <- assayIndex[\n    #     colData[, c(colIDs, \"colKey\"), with=FALSE], ,\n    #     on=\"colKey\", allow.cartesian=FALSE\n    # ]\n    cd <- colData[, c(colIDs, \"colKey\"), with=FALSE]\n    assayIndex <- merge(\n        assayIndex, cd,\n        by=\"colKey\", all.x=TRUE, allow.cartesian=FALSE\n    )\n    rm(cd)\n    gc()\n    cat(.infoMsg(\"Joining assays to assayIndex...\\n\", time=TRUE))\n\n    # Set the key variables for the assayIndex using rowIDs and colIDs\n    setkeyv(assayIndex, c(rowIDs, colIDs))\n\n\n    for (nm in names(assays)) {\n        .nm <- paste0(\".\", nm)\n        assayIndex[assays[[nm]], (.nm) := get(.nm)]\n    }\n    gc()\n    assayIndex[, (c(rowIDs, colIDs)) := NULL]\n    assayIndex <- assayIndex[\n        which(rowAnys(!is.na(assayIndex[, paste0(\".\", names(assays)), with=FALSE]))),\n    ]\n    gc()\n    cat(.infoMsg(\"Setting assayIndex key...\\n\", time=TRUE))\n    setkeyv(assayIndex, paste0(\".\", names(assays)))\n    internals$assayIndex <- assayIndex\n\n\n    # make internals immutable to prevent users from modifying structural metadata\n    internals <- immutable(internals)\n\n    gc()\n    cat(.infoMsg(\"Building LongTable...\\n\", time=TRUE))\n    # Drop extra assay columns and key by the assay key in the assay index\n    for (i in seq_along(assays)) {\n        assays[[i]][, (assayIDs[[i]]) := NULL]\n        setkeyv(assays[[i]], paste0(\".\", names(assays)[i]))\n    }\n\n    # Reorder columns to match the keys, this prevents issues in unit tests\n    # caused by different column orders\n    setkeyv(rowData, \"rowKey\")\n    setkeyv(colData, \"colKey\")\n    setcolorder(rowData, unlist(internals[c(\"rowIDs\", \"rowMeta\")]))\n    setcolorder(colData, unlist(internals[c('colIDs', 'colMeta')]))\n\n    ## Assemble  the pseudo row and column names for the LongTable\n    .pasteColons <- function(...) paste(..., collapse=':')\n    rowData[, `:=`(.rownames=mapply(.pasteColons, transpose(.SD))),\n        .SDcols=rowIDs]\n    colData[, `:=`(.colnames=mapply(.pasteColons, transpose(.SD))),\n        .SDcols=colIDs]\n    return(CoreGx:::.LongTable(rowData=rowData, colData=colData, assays=assays,\n        metadata=metadata, .intern=internals))\n}\n\n#' Function to combine two LongTables into a single LongTable\n#' @param x A `LongTable` object\n#' @param y A `LongTable` object\n#' \n\n# ---- Class unions for CoreSet slots\n#' A class union to allow multiple types in a CoreSet slot\n#'\n#' @include LongTable-class.R\nsetClassUnion('list_OR_LongTable', c('list', 'LongTable'))\n\n# #' Ensure that all rowID and colID keys are valid\n# #'\n# #' @param rowData A `data.table` containing row level annotations.\n# #' @param colData A `data.table` containing column level annotations for a\n# #'   `LongTable`.\n# #' @param assays A `list` of `data.table`s, one for each assay in an\n# #'   `LongTable`.\n# #'\n# #' @keywords internal\n### FIXME:: Finish this and implement class validity methods for LongTable!\n#.verifyKeyIntegrity <- function(rowData, colData, assays) {\n#    if (!('rowKey' %in% colnames(rowData)) || !is.numeric(rowData$rowID))\n#        message(blue('The rowKey column is missing from rowData! Please try\n#            rebuilding the LongTable object with the constructor.'))\n#    if (!('colKey' %in% colnames(colData)) || !is.numeric(colData$colID))\n#        stop()\n#}\n\n# ---- LongTable Class Methods\n\n#' Helper function to print slot information\n#' @param slotName `character` The name of the slot to print.\n#' @param slotData `data.table` The data to print.\n#' \n#' @keywords internal\nprintSlot <- function(slotName, slotData) {\n    slotCols <- ncol(slotData)\n    slotString <- paste0(slotName, '(', slotCols, '): ')\n    slotColnames <- colnames(slotData)\n    slotNamesString <-\n        if (length(slotColnames) > 6) {\n            paste0(.collapse(head(slotColnames, 3)), ' ... ', .collapse(tail(slotColnames, 3)))\n        } else {\n            .collapse(slotColnames)\n        }\n    cat(\"  \", yellow$bold(slotString) %+% green(slotNamesString), '\\n')\n}\n\n#' Show method for the LongTable class\n#'\n#' @examples\n#' show(merckLongTable)\n#'\n#' @param object A `LongTable` object to print the results for.\n#'\n#' @return `invisible` Prints to console.\n#'\n#' @importFrom crayon %+% yellow red green blue cyan magenta\n#' @import data.table\n#' @export\nsetMethod('show', signature(object='LongTable'), function(object) {\n\n    ## FIXME:: Function too long. Can I refactor to a helper that prints each slot?\n\n    # ---- class descriptions\n    cat(yellow$bold$italic(paste0(\"<\", class(object)[1], \">\"), '\\n'))\n    cat(\"  \", yellow$bold('dim: ', .collapse(dim(object)), '\\n'))\n\n    # --- assays slot\n    assayLength <- length(assayNames(object))\n    assaysString <- paste0('assays(', assayLength, '): ')\n    assayNames <- assayNames(object)\n    assayNamesString <- .collapse(assayNames(object))\n    if (nchar(assayNamesString) > options(\"width\")) {\n        assayNamesString <- paste0(strwrap(assayNamesString), collapse=\"\\n  \")\n    }\n    cat(\"  \", yellow$bold(assaysString) %+% red(assayNamesString), '\\n')\n\n    # --- rownames\n    rows <- nrow(rowData(object))\n    rowsString <- paste0('rownames(', rows, '): ')\n    rowNames <- rownames(object)\n    rownamesString <-\n        if (length(rowNames) > 6) {\n            paste0(.collapse(head(rowNames, 2)), ' ... ', .collapse(tail(rowNames, 2)))\n        } else {\n            .collapse(rowNames)\n        }\n    cat(\"  \", yellow$bold(rowsString) %+% green(rownamesString), '\\n')\n\n    # ---- rowData slot\n    printSlot('rowData', rowData(object))\n\n    # ---- colnames\n    cols <- nrow(colData(object))\n    colsString <- paste0('colnames(', cols, '): ')\n    colnames <- colnames(object)\n    colnamesString <-\n        if (length(colnames) > 6) {\n            paste0(.collapse(head(colnames, 3)), ' ... ', .collapse(tail(colnames, 3)))\n        } else {\n            .collapse(colnames)\n        }\n    cat(\"  \", yellow$bold(colsString) %+% green(colnamesString), '\\n')\n\n    # ---- colData slot\n    printSlot('colData', colData(object))\n\n    # --- metadata slot\n    metadataString <- paste0('metadata(', length(metadata(object)), '): ')\n    metadataNames <- names(metadata(object))\n    metadataNamesString <-\n        if (length(metadataNames) > 6) {\n            paste0(.collapse(head(metadataNames, 3), ' ... ', .collapse(tail(metadataNames, 3))))\n        } else if (length(metadataNames) >= 1) {\n            .collapse(metadataNames)\n        } else {\n            'none'\n        }\n    cat(\"  \", yellow$bold(metadataString) %+% green(metadataNamesString), '\\n')\n})\n\n\n# ==== LongTable Accessor Methods\n\n#' Get the id column names for the rowData slot of a LongTable\n#'\n#' @examples\n#' rowIDs(merckLongTable)\n#'\n#' @param object A `LongTable` to get the rowData id columns for.\n#' @param data `logical` Should the rowData for the id columns be returned\n#' instead of the column names? Default is FALSE.\n#' @param key `logical` Should the key column also be returned?\n#'\n#' @return A `character` vector of rowData column names if data is FALSE,\n#' otherwise a `data.table` with the data from the rowData id columns.\n#'\n#' @rdname LongTable-class\n#' @family LongTable-class\n#' @family LongTable-accessors\n#'\n#' @import data.table\n#' @export\nsetMethod('rowIDs', signature(object='LongTable'),\n        function(object, data=FALSE, key=FALSE) {\n    cols <- mutable(getIntern(object, 'rowIDs'))\n    if (key) cols <- c(cols, 'rowKey')\n    if (data) rowData(object, key=key)[, ..cols] else cols\n})\n\n#' Get the id column names for the rowData slot of a LongTable\n#'\n#' @examples\n#' rowMeta(merckLongTable)\n#'\n#' @describeIn LongTable Get the names of the non-id columns from rowData.\n#'\n#' @param object A `LongTable` to get the rowData metadata columns for.\n#' @param data `logical` Should the rowData for the metadata columns be returned\n#' instead of the column names? Default is FALSE.\n#' @param key `logical` Should the key column also be returned? Default is FALSE\n#'\n#' @return A `character` vector of rowData column names if data is FALSE,\n#' otherwise a `data.table` with the data from the rowData metadta columns.\n#'\n#' @import data.table\n#' @export\nsetMethod('rowMeta', signature(object='LongTable'),\n        function(object, data=FALSE, key=FALSE) {\n    cols <- mutable(getIntern(object, 'rowMeta'))\n    cols <- cols[!grepl('^\\\\.', cols)]\n    if (key) cols <- c(cols, 'rowKey')\n    if (data) rowData(object, key=key)[, ..cols] else cols\n})\n\n#' Get the id column names for the colData slot of a LongTable\n#'\n#' @examples\n#' colIDs(merckLongTable)\n#'\n#' @describeIn LongTable Get the names of the columns in colData required to\n#' uniquely identify each row.\n#'\n#' @param object A `LongTable` to get the colData id columns for.\n#' @param data `logical` Should the colData for the id columns be returned\n#' instead of the column names? Default is FALSE.\n#' @param key `logical` Should the key column also be returned? Default is FALSE.\n#'\n#' @return A `character` vector of colData column names if data is FALSE,\n#' otherwise a `data.table` with the data from the colData id columns.\n#'\n#' @import data.table\n#' @export\nsetMethod('colIDs', signature(object='LongTable'),\n        function(object, data=FALSE, key=FALSE) {\n\n    cols <- mutable(getIntern(object, 'colIDs'))\n    if (key) cols <- c(cols, 'colKey')\n    if (data) colData(object, key=TRUE)[, ..cols] else cols\n\n})\n\n#' Get the id column names for the colData slot of a LongTable\n#'\n#' @examples\n#' colMeta(merckLongTable)\n#'\n#' @describeIn LongTable Get the names of the non-id columns in the colData\n#'   `data.table`.\n#'\n#' @param object A `LongTable` to get the colData metadata columns for.\n#' @param data `logical` Should the colData for the metadata columns be returned\n#'   instead of the column names? Default is FALSE.\n#' @param key `logical` Should the key column also be returned?\n#'\n#' @return A `character` vector of colData column names if data is FALSE,\n#'   otherwise a `data.table` with the data from the colData metadta columns.\n#'\n#' @import data.table\n#' @export\nsetMethod('colMeta', signature(object='LongTable'),\n    function(object, data=FALSE, key=FALSE) {\n\n    cols <- mutable(getIntern(object, 'colMeta'))\n    cols <- cols[!grepl('^\\\\.', cols)]\n    if (key) cols <- c(cols, 'colKey')\n    if (data) colData(object, key=TRUE)[, ..cols] else cols\n})\n\n\n\n#' Retrieve the unique identifier columns used for primary keys in rowData and\n#'    colData.\n#'\n#' @describeIn LongTable Get the names of all id columns.\n#'\n#' @examples\n#' idCols(merckLongTable)\n#'\n#' @param object `LongTable`\n#'\n#' @return `character` A character vector containing the unique rowIDs and\n#'   colIDs in a LongTable object.\n#'\n#' @export\nsetMethod('idCols', signature('LongTable'),\n    function(object) {\n    return(unique(c(rowIDs(object), colIDs(object))))\n})\n\n#' Retrieve a copy of the assayIndex from the `@.intern` slot.\n#'\n#' @describeIn LongTable Get the assayIndex item from the objects internal metadata.\n#'\n#' @param `x` A `LongTable` or inheriting class.\n#'\n#' @return A `mutable` copy of the \"assayIndex\" for `x`\n#'\n#' @examples\n#' assayIndex(nci_TRE_small)\n#'\n#' @aliases assayIndex,LongTable-method\n#' @export\nsetMethod(\"assayIndex\", signature(\"LongTable\"), function(x) {\n    mutable(getIntern(x, \"assayIndex\"))\n})\n\n#' Retrieve a copy of the assayKeys from the `@.intern` slot.\n#'\n#' @describeIn LongTable Get the assayKeys item from the objects internal metadata.\n#'\n#' @param `x` A `LongTable` or inheriting class.\n#' @param `i` An optional valid assay name or index in `x`.\n#'\n#' @return A `mutable` copy of the \"assyKeys\" for `x`\n#'\n#' @examples\n#' assayKeys(nci_TRE_small)\n#' assayKeys(nci_TRE_small, \"sensitivity\")\n#' assayKeys(nci_TRE_small, 1)\n#'\n#' @aliases assayKeys,LongTable-method\n#' @export\nsetMethod(\"assayKeys\", signature(\"LongTable\"), function(x, i) {\n    keys <- mutable(getIntern(x, \"assayKeys\"))\n    # error handling occurs in `[[`\n    if (!missing(i)) keys[[i]] else keys\n})\n\n\n#' Retrieve the value columns for the assays in a LongTable\n#'\n#' @examples\n#' assayCols(merckLongTable)\n#'\n#' @describeIn LongTable Get a list of column names for each assay in the object.\n#'\n#' @param object `LongTable`\n#' @param i Optional parameter specifying the `character` name or `integer`\n#' index of the assay to get the column names for. If missing, returns a\n#' list of value column names for all the assays.\n#'\n#' @return A `list` of `character` vectors containing the value column names for\n#' each assay if i is missing, otherwise a `character` vector of value column\n#' names for the selected assay.\n#'\n#' @import data.table\n#' @export\nsetMethod('assayCols', signature(object='LongTable'),\n        function(object, i) {\n    if (!missing(i)) {\n        stopifnot(is.numeric(i) || is.character(i))\n        stopifnot(length(i) == 1)\n        stopifnot(i %in% assayNames(object) ||\n            i %in% seq_along(assayNames(object)))\n    }\n    keys <- assayKeys(object)\n    assayColnames <- Map(setdiff,\n        x=lapply(assays(object, raw=TRUE), FUN=colnames),\n        y=as.list(paste0(\".\", assayNames(object)))\n    )\n    assayCols <- Map(c, keys, assayColnames)\n    if (!missing(i)) assayCols[[i]] else assayCols\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.LongTable` function in this code snippet?",
        "answer": "The `.LongTable` function is used to define the `LongTable` class. It sets up the class structure with specific slots for rowData, colData, assays, metadata, and internal data. This function is marked as internal and is used as a private constructor for building `LongTable` objects."
      },
      {
        "question": "How does the `LongTable` constructor handle the case when required parameters are missing?",
        "answer": "The `LongTable` constructor checks for missing required parameters using the `missing()` function. It creates a logical vector `isMissing` for each required parameter. If any of these parameters are missing (i.e., `any(isMissing)` is true), it stops execution and throws an error message listing the missing parameters."
      },
      {
        "question": "What is the purpose of the `assayIndex` in the `LongTable` constructor, and how is it built?",
        "answer": "The `assayIndex` is a data structure that maps the relationships between rowKeys, colKeys, and assay data. It's built by first creating a grid of all possible rowKey and colKey combinations using `expand.grid()`. Then, it joins this grid with rowData and colData, and finally adds references to the actual assay data. This index allows for efficient querying and manipulation of the LongTable data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "LongTable <- function(rowData, rowIDs, colData, colIDs, assays, assayIDs,\n        metadata=list(), keep.rownames=FALSE) {\n\n    # handle missing parameters\n    isMissing <- c(rowData=missing(rowData), rowIDs=missing(rowIDs),\n        colIDs=missing(colIDs), colData=missing(colData), assays=missing(assays),\n        assayIDs=missing(assayIDs))\n\n    if (any(isMissing)) stop(.errorMsg('\\nRequired parameter(s) missing: ',\n        names(isMissing)[isMissing], collapse='\\n\\t'))\n\n    # check parameter types and coerce or error\n    if (!is(colData, \"data.table\")) {\n        tryCatch({\n            colData <- data.table(colData, keep.rownames=keep.rownames)\n        }, error=function(e)\n            stop(.errorMsg(\"colData must be coercible to a data.frame!\"))\n        )\n    } else {\n        colData <- copy(colData)\n    }\n\n    if (!is(rowData, \"data.table\")) {\n        tryCatch({\n            rowData <- data.table(rowData, keep.rownames=keep.rownames) },\n        error=function(e)\n            stop(.errorMsg(\"rowData must be coerceible to a data.frame!\"))\n        )\n    } else {\n        rowData <- copy(rowData)\n    }\n\n    # TODO: Complete the function\n}",
        "complete": "LongTable <- function(rowData, rowIDs, colData, colIDs, assays, assayIDs,\n        metadata=list(), keep.rownames=FALSE) {\n\n    # handle missing parameters\n    isMissing <- c(rowData=missing(rowData), rowIDs=missing(rowIDs),\n        colIDs=missing(colIDs), colData=missing(colData), assays=missing(assays),\n        assayIDs=missing(assayIDs))\n\n    if (any(isMissing)) stop(.errorMsg('\\nRequired parameter(s) missing: ',\n        names(isMissing)[isMissing], collapse='\\n\\t'))\n\n    # check parameter types and coerce or error\n    if (!is(colData, \"data.table\")) {\n        tryCatch({\n            colData <- data.table(colData, keep.rownames=keep.rownames)\n        }, error=function(e)\n            stop(.errorMsg(\"colData must be coercible to a data.frame!\"))\n        )\n    } else {\n        colData <- copy(colData)\n    }\n\n    if (!is(rowData, \"data.table\")) {\n        tryCatch({\n            rowData <- data.table(rowData, keep.rownames=keep.rownames) },\n        error=function(e)\n            stop(.errorMsg(\"rowData must be coerceible to a data.frame!\"))\n        )\n    } else {\n        rowData <- copy(rowData)\n    }\n\n    isDT <- is.items(assays, FUN=is.data.table)\n    isDF <- is.items(assays, FUN=is.data.frame) & !isDT\n    if (!all(isDT)) {\n        tryCatch({\n            for (i in which(isDF))\n                assays[[i]] <- data.table(assays[[i]], keep.rownames)\n        }, error = function(e, assays) {\n            message(e)\n            types <- lapply(assays, typeof)\n            stop(.errorMsg(\n                '\\nList items are types: ',\n                types, '\\nPlease ensure all items in the assays list are ',\n                'coerceable to a data.frame!'), collapse=', ')\n        })\n    }\n    assays <- copy(assays)\n\n    # capture row internal metadata\n    if (is.numeric(rowIDs) || is.logical(rowIDs))\n        rowIDs <- colnames(rowData)[rowIDs]\n    if (!all(rowIDs %in% colnames(rowData)))\n        stop(.errorMsg('\\nRow IDs not in rowData: ',\n            setdiff(rowIDs, colnames(rowData)), collapse=', '))\n\n    # Create the row and column keys for LongTable internal mappings\n    if (!('rowKey' %in% colnames(rowData)))\n        rowData[, c('rowKey') := .GRP, keyby=c(rowIDs)]\n    if (!('colKey' %in% colnames(colData)))\n        colData[, c('colKey') := .GRP, keyby=c(colIDs)]\n\n    # initialize the internals object to store private metadata for a LongTable\n    internals <- setNames(vector(\"list\", length=6),\n        c(\"rowIDs\", \"rowMeta\", \"colIDs\", \"colMeta\", \"assayKeys\", \"assayIndex\"))\n    internals$rowIDs <- rowIDs\n    internals$rowMeta <- setdiff(colnames(rowData[, -'rowKey']), rowIDs)\n\n    # capture column internal metadata\n    if (is.numeric(colIDs) || is.logical(colIDs))\n        colIDs <- colnames(colData)[colIDs]\n    if (!all(colIDs %in% colnames(colData)))\n        stop(.errorMsg('\\nColumn IDs not in colData: ',\n            setdiff(colIDs, colnames(colData)), collapse=', '),\n            call.=FALSE)\n    internals$colIDs <- colIDs\n    internals$colMeta <- setdiff(colnames(colData[, -'colKey']), colIDs)\n\n    # capture assays internal metadata\n    for (i in seq_along(assayIDs)) {\n        rids <- intersect(rowIDs, assayIDs[[i]])\n        cids <- intersect(colIDs, assayIDs[[i]])\n        assayIDs[[i]] <- c(rids, cids)\n    }\n    internals$assayKeys <- assayIDs\n\n    # ensure names of assays and assayIDs match\n    hasMatchingAssayNames <- names(assays) == names(assayIDs)\n    if (!all(hasMatchingAssayNames)) stop(.errorMsg(\n        \"Mismatched names between assays and assayIDs for:\\n\\t\",\n        paste0(names(assays)[!hasMatchingAssayNames], collapse=\", \")),\n        call.=FALSE)\n    # set keys for join with metadata\n    for (nm in names(assays)) {\n        setkeyv(assays[[nm]], assayIDs[[nm]])\n        .nm <- paste0(\".\", nm)\n        assays[[nm]][, (.nm) := .I]\n    }\n    \n    # build the index mapping assay rows to rowKey and colKey\n    cat(.infoMsg(\"Building assay index...\\n\", time=TRUE))\n    assayIndex <- expand.grid(rowKey=rowData$rowKey, colKey=colData$colKey)\n    setDT(assayIndex)\n    \n    cat(.infoMsg(\"Joining rowData to assayIndex...\\n\", time=TRUE))\n    setkeyv(rowData, \"rowKey\")\n    setkeyv(assayIndex, \"rowKey\")\n    rd <- rowData[, c(rowIDs, \"rowKey\"), with=FALSE]\n    assayIndex <- merge(\n        assayIndex, rd,\n        by=\"rowKey\", all.x=TRUE, allow.cartesian=FALSE\n    )\n\n    if(nrow(rowData) != uniqueN(rowData$rowKey)) {\n        cat(.warnMsg(\"rowData rowKey is not unique!\"))\n        show(assayIndex)\n        show(rowData)\n    }\n    rm(rd)\n    gc()\n    cat(.infoMsg(\"Joining colData to assayIndex...\\n\", time=TRUE))\n    setkeyv(colData, \"colKey\")\n    cd <- colData[, c(colIDs, \"colKey\"), with=FALSE]\n    assayIndex <- merge(\n        assayIndex, cd,\n        by=\"colKey\", all.x=TRUE, allow.cartesian=FALSE\n    )\n    rm(cd)\n    gc()\n    cat(.infoMsg(\"Joining assays to assayIndex...\\n\", time=TRUE))\n\n    # Set the key variables for the assayIndex using rowIDs and colIDs\n    setkeyv(assayIndex, c(rowIDs, colIDs))\n\n    for (nm in names(assays)) {\n        .nm <- paste0(\".\", nm)\n        assayIndex[assays[[nm]], (.nm) := get(.nm)]\n    }\n    gc()\n    assayIndex[, (c(rowIDs, colIDs)) := NULL]\n    assayIndex <- assayIndex[\n        which(rowAnys(!is.na(assayIndex[, paste0(\".\", names(assays)), with=FALSE]))),\n    ]\n    gc()\n    cat(.infoMsg(\"Setting assayIndex key...\\n\", time=TRUE))\n    setkeyv(assayIndex, paste0(\".\", names(assays)))\n    internals$assayIndex <- assayIndex\n\n    # make internals immutable to prevent users from modifying structural metadata\n    internals <- immutable(internals)\n\n    gc()\n    cat(.infoMsg(\"Building LongTable...\\n\", time=TRUE))\n    # Drop extra assay columns and key by the assay key in the assay index\n    for (i in seq_along(assays)) {\n        assays[[i]][, (assayIDs[[i]]) := NULL]\n        setkeyv(assays[[i]], paste0(\".\", names(assays)[i]))\n    }\n\n    # Reorder columns to match the keys, this prevents issues in unit tests\n    # caused by different column orders\n    setkeyv(rowData, \"rowKey\")\n    setkeyv(colData, \"colKey\")\n    setcolorder(rowData, unlist(internals[c(\"rowIDs\", \"rowMeta\")]))\n    setcolorder(colData, unlist(internals[c('colIDs', 'colMeta')]))\n\n    ## Assemble  the pseudo row and column names for the LongTable\n    .pasteColons <- function(...) paste(..., collapse=':')\n    rowData[, `:=`(.rownames=mapply(.pasteColons, transpose(.SD))),\n        .SDcols=rowIDs]\n    colData[, `:=`(.colnames=mapply(.pasteColons, transpose(.SD))),\n        .SDcols=colIDs]\n    return(CoreGx:::.LongTable(rowData=rowData, colData=colData, assays=assays,\n        metadata=metadata, .intern=internals))\n}"
      },
      {
        "partial": "setMethod('show', signature(object='LongTable'), function(object) {\n    cat(yellow$bold$italic(paste0(\"<\", class(object)[1], \">\", '\\n')))\n    cat(\"  \", yellow$bold('dim: ', .collapse(dim(object)), '\\n'))\n\n    # --- assays slot\n    assayLength <- length(assayNames(object))\n    assaysString <- paste0('assays(', assayLength, '): ')\n    assayNames <- assayNames(object)\n    assayNamesString <- .collapse(assayNames(object))\n    if (nchar(assayNamesString) > options(\"width\")) {\n        assayNamesString <- paste0(strwrap(assayNamesString), collapse=\"\\n  \")\n    }\n    cat(\"  \", yellow$bold(assaysString) %+% red(assayNamesString), '\\n')\n\n    # TODO: Complete the show method\n})",
        "complete": "setMethod('show', signature(object='LongTable'), function(object) {\n    cat(yellow$bold$italic(paste0(\"<\", class(object)[1], \">\", '\\n')))\n    cat(\"  \", yellow$bold('dim: ', .collapse(dim(object)), '\\n'))\n\n    # --- assays slot\n    assayLength <- length(assayNames(object))\n    assaysString <- paste0('assays(', assayLength, '): ')\n    assayNames <- assayNames(object)\n    assayNamesString <- .collapse(assayNames(object))\n    if (nchar(assayNamesString) > options(\"width\")) {\n        assayNamesString <- paste0(strwrap(assayNamesString), collapse=\"\\n  \")\n    }\n    cat(\"  \", yellow$bold(assaysString) %+% red(assayNamesString), '\\n')\n\n    # --- rownames\n    rows <- nrow(rowData(object))\n    rowsString <- paste0('rownames(', rows, '): ')\n    rowNames <- rownames(object)\n    rownamesString <-\n        if (length(rowNames) > 6) {\n            paste"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/CoreSet-class.R",
    "language": "R",
    "content": "#' @include CoreSet-class.R LongTable-class.R\n#' @importClassesFrom MultiAssayExperiment MultiAssayExperiment\n#' @importFrom MultiAssayExperiment MultiAssayExperiment\n#' @import checkmate\nNULL\n\nsetClassUnion('list_OR_MAE', c('list', 'MultiAssayExperiment'))\n\n.local_class <- 'CoreSet'\n.local_data <- \"clevelandSmall_cSet\"\n\n#' @title\n#' CoreSet - A generic data container for molecular profiles and\n#'   treatment response data\n#'\n#' @slot annotation See Slots section.\n#' @slot molecularProfiles See Slots section.\n#' @slot sample See Slots section.\n#' @slot treatment See Slots section.\n#' @slot treatmentResponse See Slots section.\n#' @slot perturbation See Slots section.\n#' @slot curation See Slots section.\n#' @slot datasetType See Slots section.\n#'\n#' @details\n#' The CoreSet (cSet) class was developed as a superclass for pSets in the\n#' PharmacoGx and RadioGx packages to contain the data generated in screens\n#' of cancer sample lines for their genetic profile and sensitivities to therapy\n#' (Pharmacological or Radiation). This class is meant to be a superclass which\n#' is contained within the PharmacoSet (pSet) and RadioSet (rSet) objects\n#' exported by PharmacoGx and RadioGx. The format of the data is similar for\n#' both pSets and rSets, allowing much of the code to be abstracted into\n#' the CoreSet super-class. However, the models involved with quantifying\n#' sampleular response to Pharmacological and Radiation therapy are widely\n#' different, and extension of the cSet class allows the\n#' packages to apply the correct model for the given data.\n#'\n#' @section Slots:\n#' * annotation: A `list` of annotation data about the ``r .local_class``,\n#'   including the `$name` and the session information for how the object\n#'   was created, detailing the exact versions of R and all the packages used.\n#' * molecularProfiles: A `list` or `MultiAssayExperiment` containing\n#    a set of `SummarizedExperiment`s with molecular profile data for a given\n#'   ``r .local_class`` object.\n#' * sample: A `data.frame` containg the annotations for all the samples\n#'   profiled in the data set, across all molecular data types and\n#'   treatment response experiments.\n#' * treatment: A `data.frame` containing the annotations for all treatments\n#'   in the dataset, including the mandatory 'treatmentid' column to uniquely\n#'   identify each treatment.\n#' * treatmentResponse: A `list` or `LongTable` containing all the data for the\n#'   treatment response experiment, including `$info`, a `data.frame`\n#'   containing the experimental info, `$raw` a 3D `array` containing raw data,\n#'   `$profiles`, a `data.frame` containing sensitivity profiles\n#'   statistics, and `$n`, a `data.frame` detailing the number of\n#'   experiments for each sample-drug/radiationInfo pair\n#' * perturbation: `list` containing `$n`, a `data.frame`\n#'   summarizing the available perturbation data. This slot is currently\n#'   being deprecated.\n#' * curation: `list` containing mappings for `treatment`,\n#'   `sample` and `tissue` names used in the data set to universal\n#'   identifiers used between different ``r .local_class`` objects.\n#' * datasetType: `character` string of 'sensitivity',\n#'   'perturbation', or both detailing what type of data can be found in the\n#'   ``r .local_class``, for proper processing of the data\n#'\n#' @seealso [`CoreSet-accessors`]\n#'\n#' @md\n#' @aliases CoreSet-class\n#' @exportClass CoreSet\n.CoreSet <- setClass(\"CoreSet\",\n    slots=list(\n        treatmentResponse=\"list_OR_LongTable\",\n        annotation=\"list\",\n        molecularProfiles=\"list_OR_MAE\",\n        sample=\"data.frame\",\n        treatment=\"data.frame\",\n        datasetType=\"character\",\n        perturbation=\"list\",\n        curation=\"list\"\n    )\n)\n\n# The default constructor above does a poor job of explaining the required structure of a CoreSet.\n# The constructor function defined below guides the user into providing the required components of the curation and senstivity lists\n# and hides the annotation slot which the user does not need to manually fill.\n# This also follows the design of the Expression Set class.\n\n## ==========================\n## CONSTRUCTOR\n## --------------------------\n\n#' CoreSet constructor\n#'\n#' A constructor that simplifies the process of creating CoreSets, as well\n#' as creates empty objects for data not provided to the constructor. Only\n#' objects returned by this constructor are expected to work with the CoreSet\n#' methods.\n#'\n#' ## __WARNING__:\n#' Parameters to this function have been renamed!\n#' * cell is now sample\n#' * drug is now treatment\n#'\n#' @param name A \\code{character} string detailing the name of the dataset\n#' @param molecularProfiles A \\code{list} of SummarizedExperiment objects containing\n#'   molecular profiles for each molecular data type.\n#' @param sample A \\code{data.frame} containing the annotations for all the sample\n#'   profiled in the data set, across all data types. Must contain the mandatory\n#'   `sampleid` column which uniquely identifies each sample in the object.\n#' @param treatment A `data.frame` containing annotations for all treatments\n#'   profiled in the dataset. Must contain the mandatory `treatmentid` column\n#'   which uniquely identifies each treatment in the object.\n#' @param sensitivityInfo A \\code{data.frame} containing the information for the\n#'   sensitivity experiments. Must contain a 'sampleid' column with unique\n#'   identifiers to each sample, matching the `sample` object and a 'treatmentid'\n#'   columns with unique indenifiers for each treatment, matching the `treatment`\n#'   object.\n#' @param sensitivityRaw A 3 Dimensional \\code{array} contaning the raw drug\n#'   dose response data for the sensitivity experiments\n#' @param sensitivityProfiles \\code{data.frame} containing drug sensitivity profile\n#'   statistics such as IC50 and AUC\n#' @param sensitivityN,perturbationN A \\code{data.frame} summarizing the\n#'   available sensitivity/perturbation data\n#' @param curationSample,curationTissue,curationTreatment A \\code{data.frame} mapping\n#'   the names for samples, tissues and treatments used in the data set to\n#'   universal identifiers used between different CoreSet objects\n#' @param datasetType A `character(1)` string of 'sensitivity',\n#'   'preturbation', or 'both' detailing what type of data can be found in the\n#'   `CoreSet`, for proper processing of the data\n#' @param verify `logical(1)`Should the function verify the CoreSet and\n#'   print out any errors it finds after construction?\n#' @param ... Catch and parse any renamed constructor arguments.\n#'\n#' @return An object of class `CoreSet`\n#'\n#' @examples\n#' data(clevelandSmall_cSet)\n#' clevelandSmall_cSet\n#'\n#' @export\n#'\n#' @include LongTable-class.R\n#' @import methods\n#' @importFrom utils sessionInfo\n#' @importFrom stats na.omit\n#' @importFrom SummarizedExperiment rowData colData assays\nCoreSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n    sensitivityInfo=data.frame(), sensitivityRaw=array(dim=c(0,0,0)),\n    sensitivityProfiles=matrix(), sensitivityN=matrix(nrow=0, ncol=0),\n    perturbationN=array(NA, dim=c(0,0,0)), curationSample=data.frame(),\n    curationTissue=data.frame(), curationTreatment=data.frame(),\n    treatment=data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n    verify=TRUE, ...\n) {\n\n    # .Deprecated(\"CoreSet2\", package=packageName(), msg=\"The CoreSet class is\n    #     being redesigned. Please use the new constructor to ensure forwards\n    #     compatibility with future releases! Old objects can be updated with\n    #     the updateObject method.\", old=\"CoreSet\")\n\n    # parse deprecated parameters to ensure changes don't break old code\n    dotnames <- ...names()\n    if (\"cell\" %in% dotnames) {\n        .warning(\"The cell parameter is deprecated, assigning to sample...\")\n        sample <- cell\n    }\n    if (\"drug\" %in% dotnames) {\n        .warning(\"The drug paramter is deprecated, assigning to treatment...\")\n        treatment <- drug\n    }\n\n    # ensure new sampleid and treatmentid identifiers are honoured\n    sample <- .checkForSampleId(sample)\n    treatment <- .checkForTreatmentId(treatment)\n    sensitivityInfo <- .checkForSampleId(sensitivityInfo)\n    sensitivityInfo <- .checkForTreatmentId(sensitivityInfo)\n    curationSample <- .checkForIdColumn(curationSample, c(\"sampleid\", \"unique.sampleid\"), \"cellid\")\n    curationTreatment <- .checkForIdColumn(curationTreatment, c(\"treatmentid\", \"unique.treatmentid\"), \"drugid\")\n    for (nm in names(molecularProfiles)) {\n        colData(molecularProfiles[[nm]]) <- .checkForSampleId(\n            colData(molecularProfiles[[nm]]))\n        # handle perturbation case\n        colData(molecularProfiles[[nm]]) <- .checkForIdColumn(\n            colData(molecularProfiles[[nm]]), \"treatmentid\", \"drugid\",\n            error=FALSE)\n    }\n\n    datasetType <- match.arg(datasetType)\n\n    annotation <- list()\n    annotation$name <- as.character(name)\n    annotation$dateCreated <- date()\n    annotation$sessionInfo <- sessionInfo()\n    annotation$call <- match.call()\n\n    for (i in seq_len(length(molecularProfiles))){\n        if (!is(molecularProfiles[[i]], \"SummarizedExperiment\")) {\n            stop(sprintf(\"Please provide the %s data as a SummarizedExperiment\",\n                names(molecularProfiles[i])))\n        } else {\n            rowData(molecularProfiles[[i]]) <-\n                rowData(molecularProfiles[[i]])[\n                    rownames(assays(molecularProfiles[[i]])[[1]]), , drop=FALSE\n            ]\n            colData(molecularProfiles[[i]]) <- colData(molecularProfiles[[i]])[\n                colnames(assays(molecularProfiles[[i]])[[1]]), , drop=FALSE\n            ]\n        }\n    }\n\n    sensitivity <- list()\n\n    if (!all(rownames(sensitivityInfo) == rownames(sensitivityProfiles) &\n        rownames(sensitivityInfo) == dimnames(sensitivityRaw)[[1]])) {\n        stop(\"Please ensure all the row names match between the sensitivity data.\")\n    }\n\n    sensitivity$info <- as.data.frame(sensitivityInfo, stringsAsFactors=FALSE)\n    sensitivity$raw <- sensitivityRaw\n    sensitivity$profiles <- as.data.frame(sensitivityProfiles,\n        stringsAsFactors=FALSE)\n    sensitivity$n <- sensitivityN\n\n    curation <- list()\n    curation$sample <- as.data.frame(curationSample, stringsAsFactors=FALSE)\n    curation$tissue <- as.data.frame(curationTissue, stringsAsFactors=FALSE)\n\n    perturbation <- list()\n    perturbation$n <- perturbationN\n    if (datasetType == \"perturbation\" || datasetType == \"both\") {\n        perturbation$info <- \"The metadata for the perturbation experiments is\n            available for each molecular type by calling the appropriate info\n            function. \\n For example, for RNA transcriptome perturbations, the\n            metadata can be accessed using rnaInfo(cSet).\"\n    } else {\n        perturbation$info <- \"Not a perturbation dataset.\"\n    }\n\n    object  <- .CoreSet(annotation=annotation,\n        molecularProfiles=molecularProfiles,\n        sample=as.data.frame(sample), datasetType=datasetType,\n        treatmentResponse=sensitivity, perturbation=perturbation,\n        curation=curation, treatment=treatment)\n    if (verify) { checkCsetStructure(object)}\n\n    ## TODO:: Are these functions identitical in inheriting packages?\n    if(length(sensitivityN) == 0 &&\n            datasetType %in% c(\"sensitivity\", \"both\")) {\n        sensNumber(object) <- .summarizeSensitivityNumbers(object)\n    }\n    if(length(perturbationN) == 0  &&\n            datasetType %in% c(\"perturbation\", \"both\")) {\n        pertNumber(object) <- .summarizePerturbationNumbers(object)\n    }\n    return(object)\n}\n\n\n#' Utility to help identify and fix deprecated identifiers\n#'\n#' @param new_col `character(1)` The new identifier.\n#' @param old_col `character(1)` A regex matching any old identifers to\n#' replace.\n#'\n#' @return `rectangular` object, with old_col updated to new_col if it exists.\n#'\n#' @noRd\n.checkForIdColumn <- function(df, new_col, old_col, error=TRUE) {\n    if (nrow(df) == 0 || ncol(df) == 0) return(df)\n    name <- as.character(substitute(df))\n    if (!any(colnames(df) %in% new_col)) {\n        if (old_col %in% colnames(df)) {\n            .warning(\"The \", old_col, \"identifier is deprecated, updating to\",\n                new_col, \" in \", name, \"!\")\n            colnames(df) <- gsub(old_col, new_col[1], colnames(df))\n        } else {\n            if (error)\n                .error(\"The \", new_col[1], \" identifier is mandatory in \", name, \"!\")\n        }\n        return(df)\n    }\n    return(df)\n}\n\n#' @noRd\n.checkForTreatmentId <- function(df)\n    .checkForIdColumn(df, new_col=\"treatmentid\", old_col=\"drugid\")\n\n#' @noRd\n.checkForSampleId <- function(df)\n    .checkForIdColumn(df, new_col=\"sampleid\", old_col=\"cellid\")\n\n\n#' @noRd\n.docs_CoreSet2_constructor <- function(...) .parseToRoxygen(\n    \"\n    @title Make a CoreSet with the updated class structure\n\n    @description\n    New implementation of the CoreSet constructor to support MAE and TRE. This\n    constructor will be swapped with the original `CoreSet` constructor as\n    part of an overhaul of the CoreSet class structure.\n\n    @param name A `character(1)` vector with the `{class_}` objects name.\n    @param treatment A `data.frame` with treatment level metadata. {tx_}\n    @param sample A `data.frame` with sample level metadata for the union\n        of samples in `treatmentResponse` and `molecularProfiles`. {sx_}\n    @param molecularProfiles A `MultiAssayExperiment` containing one\n        `SummarizedExperiment` object for each molecular data type.\n    @param treatmentResponse A `LongTable` or `LongTableDataMapper` object\n        containing all treatment response data associated with the `{class_}`\n        object.\n    @param curation {cx_}\n    @param perturbation A deprecated slot in a `{class_}` object included\n    for backwards compatibility. This may be removed in future releases.\n    @param datasetType A deprecated slot in a `{class_}` object included\n    for backwards compatibility. This may be removed in future releases.\n\n    @examples\n    data({data_})\n    {data_}\n\n    @return A `CoreSet` object storing standardized and curated treatment\n        response and multiomic profile data associated with a given publication.\n\n    @importFrom MultiAssayExperiment MultiAssayExperiment\n    @importFrom checkmate assertCharacter assertDataFrame assertClass assert\n    assertList assertSubset\n    \",\n    ...\n)\n\n#' @eval .docs_CoreSet2_constructor(class_=.local_class,\n#' tx_=\"\",\n#' sx_=\"\",\n#' cx_=\"A `list(2)` object with two items named `treatment` and `sample` with\n#' mappings from publication identifiers to standardized identifiers for\n#' both annotations, respectively.\",\n#' data_=.local_data)\n#' @md\n#' @export\nCoreSet2 <- function(name=\"emptySet\", treatment=data.frame(),\n    sample=data.frame(), molecularProfiles=MultiAssayExperiment(),\n    treatmentResponse=LongTable(), datasetType=\"sensitivity\",\n    perturbation=list(n=array(dim=3), info=\"No perturbation data!\"),\n    curation=list(sample=data.frame(), treatment=data.frame())\n) {\n\n    # -- update old curation names\n    names(curation) <- gsub(\"drug|radiation\", \"treatment\", names(curation))\n    names(curation) <- gsub(\"cell\", \"sample\", names(curation))\n\n    ## -- input validation\n    assertCharacter(name, len=1)\n    assertDataFrame(treatment)\n    assertDataFrame(sample)\n    assertClass(molecularProfiles, \"MultiAssayExperiment\")\n    assert(\n        checkClass(treatmentResponse, \"LongTable\"),\n        checkClass(treatmentResponse, \"LongTableDataMapper\")\n    )\n    assertList(curation, min.len=2)\n    assertSubset(c(\"sample\", \"treatment\"), choices=names(curation))\n\n    ## -- capture object creation environment\n    annotation <- list(name=name, dateCreated=date(),\n        sessionInfo=sessionInfo(), call=match.call())\n\n    ## -- conditionally materialize DataMapper\n    if (is(treatmentResponse, 'LongTableDataMapper'))\n        treatmentResponse <- metaConstruct(treatmentResponse)\n\n\n    ## -- handle missing rownames for sample\n    if (!all(sample$sampleid == rownames(sample)))\n        rownames(sample) <- sample$sampleid\n\n    object <- .CoreSet(\n        annotation=annotation,\n        sample=sample,\n        treatment=treatment,\n        molecularProfiles=molecularProfiles,\n        treatmentResponse=treatmentResponse,\n        datasetType=datasetType,\n        curation=curation,\n        perturbation=perturbation\n    )\n\n    ## -- data integrity checks\n    # molecularProfiles\n    validProfiles <- .checkMolecularProfiles(object)\n\n    # treatmentResponse\n    validTreatments <- .checkTreatmentResponse(object)\n\n    diagnosis <- c(!isTRUE(validProfiles), !isTRUE(validTreatments))\n    if (any(diagnosis)) {\n        .error(paste0(list(validProfiles, validTreatments)[diagnosis],\n            collapse=\"\\n\", sep=\"\\n\"))\n    }\n    return(object)\n}\n\n#' Show a CoreSet\n#'\n#' @param object `CoreSet` object to show via `cat`.\n#'\n#' @seealso [`cat`]\n#'\n#' @examples\n#' show(clevelandSmall_cSet)\n#'\n#' @return Prints the CoreSet object to the output stream, and returns\n#'   invisible NULL.\n#' \n#' @importFrom crayon %+% yellow red green blue cyan magenta\n#' \n#' @md\n#' @export\nsetMethod(\"show\", signature=signature(object=\"CoreSet\"), function(object) {\n\n    if (!.hasSlot(object, \"sample\") || !.hasSlot(object, \"treatment\"))\n        stop(.errorMsg(\"This \", class(object)[1], \" object appears to be out\",\n            \"of date! Please run object <- updateObject(object) to update \",\n            \"the object for compatibility with the current release.\"),\n            call.=FALSE)\n\n    cat(yellow$bold$italic(paste0(\"<\", class(object)[1], \">\\n\")))\n    space <- \"  \"\n    cat(yellow$bold$italic(\"Name: \") %+% green(name(object)), \"\\n\")\n\n    cat(yellow$bold$italic(\"Date Created: \") %+% green(dateCreated(object)), \"\\n\")\n\n    # cat(\"Number of samples: \", nrow(sampleInfo(object)), \"\\n\")\n    cat(yellow$bold$italic(\"Number of samples: \"), green(nrow(sampleInfo(object))), \"\\n\")\n\n    mProfiles <- molecularProfilesSlot(object)\n    mProfileNames <- names(mProfiles)\n    if (is(mProfiles, \"MultiAssayExperiment\")) {\n        cat(yellow$bold$italic(\"Molecular profiles: \"))\n        cat(yellow$bold$italic(paste0(\"<\", class(mProfiles)[1], \">\"), '\\n'))\n\n        # changing this to just experiments() as its cleaner\n        # showMAE <- capture.output(show(mProfiles))\n        # dropAfter <- which(grepl(\"Functionality\", showMAE)) - 1\n        # showCompactMAE <- showMAE[1:dropAfter]\n        # cat(space, paste0(showCompactMAE, collapse=\"\\n  \"), \"\\n\")\n\n        showExp <- capture.output(experiments(mProfiles))\n        cat(space, yellow$bold(showExp[1], '\\n'))\n\n        # iterate through rest of experiments \n        # split by \":\" and print the first element in yellow, the rest in cyan\n        for (i in 2:length(showExp)) {\n            splitExp <- strsplit(showExp[i], \":\")[[1]]\n            \n            # print the first 5 characters in splitExp[1] in yellow\n            cat(space, yellow$bold(substr(splitExp[1], 1, 5)))\n            # print after the first 5 characters in spllitExp[1] in cyan\n            cat(\n                red(substr(splitExp[1], 6, nchar(splitExp[1]))),\n                green(paste0(\":\", splitExp[2:length(splitExp)], collapse=\":\"),'\\n'))\n        }\n\n        # samplenames <- sort(sampleNames(object))\n        # samplenames <- \n        #     if (length(samplenames) > 6) {\n        #         paste0(.collapse(head(samplenames, 3)), ' ... ', .collapse(tail(samplenames, 3)))\n        #     } else {\n        #         .collapse(samplenames)\n        #     }\n        # x <- yellow$bold(paste0(\"colnames(\", length(sampleNames(object)), \"):\"))\n        # cat(space, x, green(samplenames), \"\\n\")\n    } else {\n        cat(yellow$bold$italic(\"Molecular profiles: \\n\"))\n        if (!length(mProfileNames)) cat(space, \"None\\n\")\n        for (item in mProfileNames) {\n            title <- switch(item,\n                \"dna\"=\"DNA\",\n                \"rna\"=\"RNA\",\n                \"rnaseq\"=\"RNAseq\",\n                \"snp\"=\"SNP\",\n                \"cnv\"=\"CNV\",\n                item\n            )\n            cat(title, \":\\n\")\n            cat(paste0(space, \"Dim: \",\n                paste0(dim(molecularProfiles(object, mDataType=item)), collapse=\", \")),\n                \"\\n\"\n            )\n        }\n    }\n    cat(yellow$bold$italic(\"Treatment response: \"))\n    if (is(treatmentResponse(object), \"LongTable\")) {\n        show(treatmentResponse(object))\n        # showLT <- capture.output(show(treatmentResponse(object)))\n        # cat(space, paste0(showLT, collapse=\"\\n  \"), \"\\n\")\n    } else {\n        cat(\"Drug pertubation:\\n\")\n        cat(space,\n            \"Please look at pertNumber(cSet) to determine number of experiments\",\n            \" for each drug-sample combination.\\n\")\n        cat(\"Drug sensitivity:\\n\")\n        cat(space, \"Number of Experiments: \", nrow(sensitivityInfo(object)), \"\\n\")\n        cat(space, \"Please look at sensNumber(cSet) to determine number of \",\n            \"experiments for each drug-sample combination.\\n\")\n    }\n})\n\n\n#' Update the sample ids in a cSet object\n#'\n#' @examples\n#' updateSampleId(clevelandSmall_cSet, sampleNames(clevelandSmall_cSet))\n#'\n#' @param object The object for which the sample ids will be updated\n#' @param new.ids The new ids to assign to the object\n#'\n#' @return \\code{CoreSet} The modified CoreSet object\n#'\n#' @keywords internal\n#' @importFrom S4Vectors endoapply\n#' @importFrom SummarizedExperiment colData rowData\n#' @export\nupdateSampleId <- function(object, new.ids=vector(\"character\")) {\n\n    if (length(new.ids) != nrow(sampleInfo(object))){\n        stop(\"Wrong number of sample identifiers\")\n    }\n\n    if (datasetType(object) == \"sensitivity\" || datasetType(object) == \"both\") {\n        myx <- match(sensitivityInfo(object)[, \"sampleid\"],\n            rownames(sampleInfo(object)))\n        if (is(treatmentResponse(object), 'LongTable')) {\n            LT <- treatmentResponse(object)\n            whichSampleIds <- which(colData(LT)$sampleid %in% sampleNames(object))\n            colData(LT)$sampleid <- new.ids[whichSampleIds]\n            treatmentResponse(object) <- LT\n        } else {\n            sensitivityInfo(object)[, \"sampleid\"] <- new.ids[myx]\n        }\n    }\n\n    molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object), function(SE) {\n        myx <- match(colData(SE)[[\"sampleid\"]],\n            rownames(sampleInfo(object)))\n        colData(SE)[[\"sampleid\"]]  <- new.ids[myx]\n        return(SE)\n    })\n\n    if (any(duplicated(new.ids))) {\n        warning(\"Duplicated ids passed to updateSampleId. Merging old ids into\",\n            \" the same identifier\")\n\n        if(ncol(sensNumber(object)) > 0) {\n            sensMatch <- match(rownames(sensNumber(object)),\n                rownames(sampleInfo(object)))\n        }\n        if(dim(pertNumber(object))[[2]] > 0) {\n            pertMatch <- match(dimnames(pertNumber(object))[[1]],\n                rownames(sampleInfo(object)))\n        }\n\n        curMatch <- match(rownames(curation(object)$sample),\n            rownames(sampleInfo(object)))\n        duplId <- unique(new.ids[duplicated(new.ids)])\n\n        for(id in duplId){\n            if (ncol(sensNumber(object)) > 0) {\n                myx <- which(new.ids[sensMatch] == id)\n                sensNumber(object)[myx[1],] <- apply(sensNumber(object)[myx, ],\n                    2, sum)\n                sensNumber(object) <- sensNumber(object)[-myx[-1], ]\n                # sensMatch <- sensMatch[-myx[-1]]\n        }\n        if (dim(pertNumber(object))[[1]] > 0) {\n            myx <- which(new.ids[pertMatch] == id)\n            pertNumber(object)[myx[1], , ] <- apply(pertNumber(object)[myx, , ],\n                c(1,3), sum)\n            pertNumber(object) <- pertNumber(object)[-myx[-1], , ]\n        }\n\n        myx <- which(new.ids[curMatch] == id)\n        curation(object)$sample[myx[1],] <- apply(curation(object)$sample[myx, ], 2,\n            FUN=paste, collapse=\"///\")\n        curation(object)$sample <- curation(object)$sample[-myx[-1], ]\n        curation(object)$tissue[myx[1],] <- apply(curation(object)$tissue[myx, ],\n            2, FUN=paste, collapse=\"///\")\n        curation(object)$tissue <- curation(object)$tissue[-myx[-1], ]\n\n        myx <- which(new.ids == id)\n        sampleInfo(object)[myx[1],] <- apply(sampleInfo(object)[myx,], 2,\n            FUN=paste, collapse=\"///\")\n        sampleInfo(object) <- sampleInfo(object)[-myx[-1], ]\n        new.ids <- new.ids[-myx[-1]]\n        if(ncol(sensNumber(object)) > 0){\n            sensMatch <- match(rownames(sensNumber(object)),\n                rownames(sampleInfo(object)))\n        }\n        if(dim(pertNumber(object))[[1]] > 0){\n            pertMatch <- match(dimnames(pertNumber(object))[[1]],\n                rownames(sampleInfo(object)))\n        }\n        curMatch <- match(rownames(curation(object)$sample),\n            rownames(sampleInfo(object)))\n        }\n    } else {\n        if (dim(pertNumber(object))[[1]] > 0) {\n            pertMatch <- match(dimnames(pertNumber(object))[[1]],\n                rownames(sampleInfo(object)))\n        }\n        if (ncol(sensNumber(object)) > 0) {\n            sensMatch <- match(rownames(sensNumber(object)),\n                rownames(sampleInfo(object)))\n        }\n        curMatch <- match(rownames(curation(object)$sample),\n            rownames(sampleInfo(object)))\n    }\n    if (dim(pertNumber(object))[[1]] > 0) {\n        dimnames(pertNumber(object))[[1]] <- new.ids[pertMatch]\n    }\n    if (ncol(sensNumber(object)) > 0) {\n        rownames(sensNumber(object)) <- new.ids[sensMatch]\n    }\n    rownames(curation(object)$sample) <- new.ids[curMatch]\n    rownames(curation(object)$tissue) <- new.ids[curMatch]\n    rownames(sampleInfo(object)) <- new.ids\n    return(object)\n}\n\n# updateFeatureNames <- function(object, new.ids=vector(\"character\")){\n#\n#   if (length(new.ids)!=nrow(sampleInfo(object))){\n#     stop(\"Wrong number of sample identifiers\")\n#   }\n#\n#   if(datasetType(object)==\"sensitivity\"|datasetType(object)==\"both\"){\n#     myx <- match(sensitivityInfo(object)[,\"sampleid\"],rownames(sampleInfo(object)))\n#     sensitivityInfo(object)[,\"sampleid\"] <- new.ids[myx]\n#\n#   }\n#\n#   molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object), function(eset){\n#\n#     myx <- match(colData(eset)[[\"sampleid\"]],rownames(sampleInfo(object)))\n#     colData(eset)[[\"sampleid\"]]  <- new.ids[myx]\n#     return(eset)\n#       })\n#   myx <- match(rownames(curation(object)$sample),rownames(sampleInfo(object)))\n#   rownames(curation(object)$sample) <- new.ids[myx]\n#   rownames(curation(object)$tissue) <- new.ids[myx]\n#   if (dim(pertNumber(object))[[1]]>0){\n#     myx <- match(dimnames(pertNumber(object))[[1]], rownames(sampleInfo(object)))\n#     dimnames(pertNumber(object))[[1]] <- new.ids[myx]\n#   }\n#   if (nrow(sensNumber(object))>0){\n#     myx <- match(rownames(sensNumber(object)), rownames(sampleInfo(object)))\n#     rownames(sensNumber(object)) <- new.ids[myx]\n#   }\n#   rownames(sampleInfo(object)) <- new.ids\n#   return(object)\n#\n# }\n\n\n### TODO:: Add updating of sensitivity Number tables\n#' Update the treatment ids in a cSet object\n#'\n#' @examples\n#' updateTreatmentId(clevelandSmall_cSet, treatmentNames(clevelandSmall_cSet))\n#'\n#' @param object The object for which the treatment ids will be updated\n#' @param new.ids The new ids to assign to the object\n#'\n#' @return `CoreSet` The modified CoreSet object\n#'\n#' @keywords internal\n#' @importFrom S4Vectors endoapply\n#' @importFrom SummarizedExperiment colData rowData\n#' @export\nupdateTreatmentId <- function(object, new.ids = vector('character')){\n\n    if (nrow(treatmentInfo(object)) < 1) {\n        message(\"No treatments in this object! Returning without modification.\")\n        return(object)\n    }\n\n    if (length(new.ids) != nrow(treatmentInfo(object))) {\n        stop('Wrong number of drug identifiers')\n    }\n    if (datasetType(object) == 'sensitivity' || datasetType(object) == 'both') {\n        myx <- match(sensitivityInfo(object)[, \"treatmentid\"], rownames(treatmentInfo(object)))\n        sensitivityInfo(object)[, \"treatmentid\"] <- new.ids[myx]\n    }\n    if (datasetType(object) == 'perturbation' || datasetType(object) == 'both') {\n        molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object),\n                function(SE) {\n            myx <- match(\n                SummarizedExperiment::colData(SE)[[\"treatmentid\"]],\n                rownames(treatmentInfo(object))\n            )\n            SummarizedExperiment::colData(SE)[[\"treatmentid\"]] <- new.ids[myx]\n            return(SE)\n        })\n    }\n    if (any(duplicated(new.ids))) {\n        warning('Duplicated ids passed to updateTreatmentId. Merging old ids ',\n            'into the same identifier')\n        if (ncol(sensNumber(object)) > 0){\n            sensMatch <- match(colnames(sensNumber(object)),\n                rownames(treatmentInfo(object)))\n        }\n        if (dim(pertNumber(object))[[2]] > 0) {\n            pertMatch <- match(dimnames(pertNumber(object))[[2]],\n                rownames(treatmentInfo(object)))\n        }\n        if (\"treatment\" %in% names(curation(object))) {\n            curMatch <- match(rownames(curation(object)$treatment),\n                rownames(treatmentInfo(object)))\n        }\n        duplId <- unique(new.ids[duplicated(new.ids)])\n        for(id in duplId) {\n            if (ncol(sensNumber(object))>0){\n                myx <- which(new.ids[sensMatch] == id)\n                sensNumber(object)[, myx[1]] <- apply(sensNumber(object)[, myx], 1, sum)\n                sensNumber(object) <- sensNumber(object)[, -myx[-1]]\n                # sensMatch <- sensMatch[-myx[-1]]\n            }\n            if (dim(pertNumber(object))[[2]] > 0) {\n                myx <- which(new.ids[pertMatch] == id)\n                pertNumber(object)[,myx[1],] <- apply(pertNumber(object)[,myx,],\n                    c(1,3), sum)\n                pertNumber(object) <- pertNumber(object)[,-myx[-1], ]\n                # pertMatch <- pertMatch[-myx[-1]]\n            }\n            if (\"treatment\" %in% names(curation(object))) {\n                myx <- which(new.ids[curMatch] == id)\n                curation(object)$treatment[myx[1], ] <-\n                    apply(curation(object)$treatment[myx, ], 2, paste,\n                        collapse='///')\n                curation(object)$treatment <- curation(object)$treatment[-myx[-1], ]\n                # curMatch <- curMatch[-myx[-1]]\n            }\n\n            myx <- which(new.ids == id)\n            treatmentInfo(object)[myx[1],] <- apply(treatmentInfo(object)[myx,],\n                2, paste, collapse='///')\n            treatmentInfo(object) <- treatmentInfo(object)[-myx[-1], ]\n            new.ids <- new.ids[-myx[-1]]\n            if (ncol(sensNumber(object)) > 0) {\n                sensMatch <- match(colnames(sensNumber(object)),\n                    rownames(treatmentInfo(object)))\n            }\n            if (dim(pertNumber(object))[[2]] > 0) {\n                pertMatch <- match(dimnames(pertNumber(object))[[2]],\n                    rownames(treatmentInfo(object)))\n            }\n            if (\"treatment\" %in% names(curation(object))) {\n                curMatch <- match(rownames(curation(object)$treatment),\n                    rownames(treatmentInfo(object)))\n            }\n        }\n    } else {\n        if (dim(pertNumber(object))[[2]]>0){\n            pertMatch <- match(dimnames(pertNumber(object))[[2]],\n                rownames(treatmentInfo(object)))\n        }\n        if (ncol(sensNumber(object))>0){\n            sensMatch <- match(colnames(sensNumber(object)),\n                rownames(treatmentInfo(object)))\n        }\n        if (\"treatment\" %in% names(curation(object))) {\n            curMatch <- match(rownames(curation(object)$treatment),\n                rownames(treatmentInfo(object)))\n        }\n    }\n    if (dim(pertNumber(object))[[2]]>0){\n        dimnames(pertNumber(object))[[2]] <- new.ids[pertMatch]\n    }\n    if (ncol(sensNumber(object))>0){\n        colnames(sensNumber(object)) <- new.ids[sensMatch]\n    }\n    if (\"treatment\" %in% names(curation(object))) {\n        rownames(curation(object)$treatment) <- new.ids[curMatch]\n    }\n    rownames(treatmentInfo(object)) <- new.ids\n    return(object)\n}\n\n\n.summarizeSensitivityNumbers <- function(object) {\n\n    if (datasetType(object) != \"sensitivity\" && datasetType(object) != \"both\") {\n        stop (\"Data type must be either sensitivity or both\")\n    }\n\n    ## unique drug identifiers\n    # drugn <- sort(unique(treatmentResponse(object)$info[ , \"treatmentid\"]))\n\n    ## consider all drugs\n    drugn <- rownames(treatmentInfo(object))\n\n    ## unique drug identifiers\n    # samplen <- sort(unique(treatmentResponse(object)$info[ , \"sampleid\"]))\n\n    ## consider all sample\n    samplen <- rownames(sampleInfo(object))\n\n    sensitivity.info <- matrix(0, nrow=length(samplen), ncol=length(drugn),\n        dimnames=list(samplen, drugn))\n    drugids <- sensitivityInfo(object)[, \"treatmentid\"]\n    sampleids <- sensitivityInfo(object)[, \"sampleid\"]\n    sampleids <- sampleids[grep(\"///\", drugids, invert=TRUE)]\n    drugids <- drugids[grep(\"///\", drugids, invert=TRUE)]\n\n    tt <- table(sampleids, drugids)\n    sensitivity.info[rownames(tt), colnames(tt)] <- tt\n\n    return(sensitivity.info)\n}\n\n#' @export\n#' @keywords internal\n.summarizeMolecularNumbers <- function(object) {\n\n    ## consider all molecular types\n    mDT <- mDataNames(object)\n\n    ## consider all sample lines\n    samplen <- rownames(sampleInfo(object))\n\n    molecular.info <- matrix(0, nrow=length(samplen), ncol=length(mDT),\n        dimnames=list(samplen, mDT))\n\n    for(mDataType in mDT) {\n        tt <- table(phenoInfo(object, mDataType)$sampleid)\n        molecular.info[names(tt), mDataType] <- tt\n    }\n    return(molecular.info)\n}\n\n#' @importFrom SummarizedExperiment colData rowData\n.summarizePerturbationNumbers <- function(object) {\n\n    if (datasetType(object) != \"perturbation\" && datasetType(object) != \"both\") {\n        stop (\"Data type must be either perturbation or both\")\n    }\n\n    ## consider all drugs\n    drugn <- rownames(treatmentInfo(object))\n\n    ## consider all sample lines\n    samplen <- rownames(sampleInfo(object))\n\n    perturbation.info <- array(0, dim=c(length(samplen), length(drugn),\n        length(molecularProfilesSlot(object))),\n        dimnames=list(samplen, drugn, names((molecularProfilesSlot(object)))))\n\n    for (i in seq_len(length(molecularProfilesSlot(object)))) {\n        if (nrow(colData(molecularProfilesSlot(object)[[i]])) > 0 &&\n                all(is.element(c(\"sampleid\", \"treatmentid\"),\n                    colnames(colData(molecularProfilesSlot(object)[[i]]))))) {\n            tt <- table(colData(molecularProfilesSlot(object)[[i]])[ , \"sampleid\"],\n                colData(molecularProfilesSlot(object)[[i]])[ , \"treatmentid\"])\n            perturbation.info[rownames(tt), colnames(tt),\n                names(molecularProfilesSlot(object))[i]] <- tt\n        }\n    }\n\n    return(perturbation.info)\n}\n\n#' A function to verify the structure of a CoreSet\n#'\n#' This function checks the structure of a PharamcoSet, ensuring that the\n#' correct annotations are in place and all the required slots are filled so\n#' that matching of samples and drugs can be properly done across different types\n#' of data and with other studies.\n#'\n#' @examples\n#' checkCsetStructure(clevelandSmall_cSet)\n#'\n#' @param object A `CoreSet` to be verified\n#' @param plotDist Should the function also plot the distribution of molecular\n#'   data?\n#' @param result.dir The path to the directory for saving the plots as a string.\n#'   Defaults to this R sessions `tempdir()`.\n#'\n#' @return Prints out messages whenever describing the errors found in the\n#'   structure of the cSet object passed in.\n#'\n#' @export\n#'\n#' @md\n#' @importFrom graphics hist\n#' @importFrom grDevices dev.off pdf\n#' @importFrom SummarizedExperiment assay rowData colData\n#' @importFrom S4Vectors metadata\ncheckCsetStructure <- function(object, plotDist=FALSE, result.dir=tempdir()) {\n\n    msg <- c()\n\n    # Make directory to store results if it doesn't exist\n    if (!file.exists(result.dir) && plotDist) {\n        dir.create(result.dir, showWarnings=FALSE, recursive=TRUE)\n    }\n\n    ####\n    ## Checking molecularProfiles\n    ####\n    for (i in seq_along(molecularProfilesSlot(object))) {\n        profile <- molecularProfilesSlot(object)[[i]]\n        nn <- names(molecularProfilesSlot(object))[i]\n\n        # Testing plot rendering for rna and rnaseq\n        if ((metadata(profile)$annotation == \"rna\" ||\n                metadata(profile)$annotation == \"rnaseq\") && plotDist) {\n            pdf(file=file.path(result.dir, sprintf(\"%s.pdf\", nn)))\n            hist(assay(profile, 'exprs'), breaks=100)\n            dev.off()\n        }\n\n        ## Test if sample and feature annotations dimensions match the assay\n        if (nrow(rowData(profile)) != nrow(assays(profile)$exprs)) {\n            msg <- c(msg, paste0(nn, \" number of features in rowData is \",\n                \"different from SummarizedExperiment slots\"))\n        }\n        if (nrow(colData(profile)) != ncol(assays(profile)$exprs)) {\n            msg <- c(msg, paste0(nn, \"number of samples in colData is \",\n                \"different from expression slots\", nn))\n        }\n\n        # Checking sample metadata for required columns\n        if (!(\"sampleid\" %in% colnames(colData(profile)))) {\n            msg <- c(msg, paste0(nn, \" sampleid does not exist in colData \",\n                \"(samples) columns\"))\n        }\n        if (!(\"batchid\" %in% colnames(colData(profile)))) {\n            msg <- c(msg, sprintf(nn, \" batchid does not exist in colData \",\n                \"(samples) columns\"))\n        }\n\n        # Checking mDataType of the SummarizedExperiment for required columns\n        if (metadata(profile)$annotation == \"rna\" ||\n                metadata(profile)$annotation == \"rnaseq\") {\n            if (!(\"BEST\" %in% colnames(rowData(profile)))) {\n                msg <- c(msg, paste0(nn, \" BEST does not exist in rowData \",\n                    \"(features) columns\"))\n            }\n            if (!(\"Symbol\" %in% colnames(rowData(profile)))) {\n                msg <- c(msg, paste0(nn, \" Symbol does not exist in rowData \",\n                    \"(features) columns\"))\n            }\n        }\n\n        # Check that all sampleids from the cSet are included in molecularProfiles\n        if (\"sampleid\" %in% colnames(rowData(profile))) {\n            if (!all(colData(profile)[, \"sampleid\"] %in% rownames(sampleInfo(object)))) {\n                msg <- c(msg, paste0(nn, \" not all the sample lines in this \",\n                    \"profile are in sample lines slot\"))\n            }\n        } else {\n            msg <- c(msg, paste0(nn, \" sampleid does not exist in colData \",\n                \"(samples)\"))\n        }\n    }\n\n    #####\n    # Checking sample\n    #####\n    if (\"tissueid\" %in% colnames(sampleInfo(object))) {\n        if (\"unique.tissueid\" %in% colnames(curation(object)$tissue)) {\n            if (length(intersect(rownames(curation(object)$tissue),\n                    rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n                msg <- c(msg, paste0(\"rownames of curation tissue slot should\",\n                    \" be the same as sample slot (curated sample ids)\"))\n            } else {\n                if (length(intersect(sampleInfo(object)$tissueid,\n                        curation(object)$tissue$unique.tissueid)) !=\n                            length(table(sampleInfo(object)$tissueid))) {\n                    msg <- c(msg, paste0(\"tissueid should be the same as unique\",\n                        \" tissue id from tissue curation slot\"))\n                }\n            }\n        } else {\n            msg <- c(msg, paste0(\"unique.tissueid which is curated tissue id\",\n                \" across data set should be a column of tissue curation slot\"))\n        }\n        if (any(is.na(sampleInfo(object)[,\"tissueid\"]) |\n                sampleInfo(object)[, \"tissueid\"] == \"\", na.rm=TRUE)) {\n            msg <- c(msg, paste0(\n                    \"There is no tissue type for these samples\",\n                    paste(\n                        rownames(sampleInfo(object))[\n                            which(is.na(sampleInfo(object)[,\"tissueid\"]) |\n                                sampleInfo(object)[,\"tissueid\"] == \"\")\n                            ],\n                        collapse=\" \")))\n        }\n    } else {\n        msg <- c(msg, \"tissueid does not exist in sample slot\")\n    }\n\n    if(\"unique.sampleid\" %in% colnames(curation(object)$sample)) {\n        if (length(intersect(curation(object)$sample$unique.sampleid,\n                rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n            msg <- c(msg, \"rownames of sample slot should be curated sample ids\")\n        }\n    } else {\n        msg <- c(msg, paste0(\"unique.sampleid which is curated sample id across\",\n            \" data set should be a column of sample curation slot\"))\n    }\n\n    if (length(intersect(rownames(curation(object)$sample),\n            rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n        msg <- c(msg, paste0(\"rownames of curation sample slot should be the\",\n            \" same as sample slot (curated sample ids)\"))\n    }\n\n    if (!is(sampleInfo(object), \"data.frame\")) {\n        msg <- c(msg, \"sample slot class type should be dataframe\")\n    }\n    if (length(msg)) return(paste0(msg, collapse=\"\\n\")) else TRUE\n}\n\n#' @importFrom MultiAssayExperiment MultiAssayExperiment experiments\n#' @importFrom S4Vectors List\n#' @importFrom BiocGenerics %in% match\n.checkMolecularProfiles <- function(object) {\n    msg <- character()\n    # ---- Make a MutliAssayExperiment, if it isn't one already\n    molecProf <- molecularProfilesSlot(object)\n    isSummarizedExperiment <- all(as(lapply(experiments(molecProf), is,\n        'SummarizedExperiment'), 'List'))\n    if (!all(isSummarizedExperiment)) {\n        nmsg <- .formatMessage('All molecular profiles must be stored as\n            SummarizedExperiment objects. The following are not ',\n            paste(names(which(!isSummarizedExperiment)), collapse=', '))\n        msg <- c(msg, nmsg)\n    }\n    tryCatch({\n        MAE <- if (is(molecProf, 'MultiAssayExperiment')) molecProf else\n            MultiAssayExperiment(molecProf)\n    }, error=function(e) msg <- c(msg, paste0('Failed coercing to\n        MultiAssayExperiment: ', as.character(e))))\n\n    # ---- Check for correct metadata columns\n    # -- sample identifiers\n    colDataL <- lapply(experiments(MAE), FUN=colData)\n    colColNameL <- as(lapply(colDataL, FUN=colnames), 'List')\n    hasSampleId <- any(colColNameL %in%  'sampleid')\n    if (!all(hasSampleId)) {\n        nmsg <- .formatMessage('All SummarizedExperiments must have a sampleid\n            column. This is not the case for ',\n            paste(names(which(!hasSampleId)), collapse=', '), '!')\n        msg <- c(msg, nmsg)\n    }\n    hasBatchId <- any(colColNameL %in% 'batchid')\n    if (!all(hasBatchId)) {\n        nmsg <- .formatMessage('All SummarizedExpeirments must have a batchid\n            column. This is not the case for ',\n            paste(names(which(!hasBatchId)), collapse=', '), '!')\n        msg <- c(msg, nmsg)\n    }\n    # -- feature identifiers\n    rowDataL <- lapply(experiments(MAE), FUN=rowData)\n    rowColNameL <- as(lapply(rowDataL, colnames), 'List')\n    # hasGeneId <- rowColNameL %in% 'geneid'\n    hasSymbol <- rowColNameL %in% 'Symbol'\n    hasBEST <- rowColNameL %in% 'BEST'\n    # hasEnsemblId <- rowColNamesL %in% 'ensemblid'\n\n    # ---- Check all samples are in the @sample slot\n    samples <- sampleNames(object)\n\n    \n    sampleIdL <- as(lapply(colDataL, `[[`, i='sampleid'), 'List')\n    hasValidSamples <- sampleIdL %in% samples\n    if (!all(all(hasValidSamples))) {\n        nmsg <- .formatMessage('All sampleids in the @molecularProfiles slot\n            must also be in the @sample slot. This is not the case\n            for ', paste(names(which(all(hasValidSamples))), collapse=', '))\n        msg <- c(msg, nmsg)\n    }\n\n    # ---- Return messages if something is wrong, or TRUE if everything is good\n    return(if (length(msg)) msg else TRUE)\n}\n\n.checkTreatmentResponse <- function(object) {\n    msg <- character()\n    # ---- Extract sensitivity data\n    samples <- sampleNames(object)\n    sensSlot <- treatmentResponse(object)\n    if (!is(sensSlot, \"TreatmentResponseExperiment\")) {\n        nmsg <- \"The treatmentReponse parameter must be a\n            TreatmentResponseExperiment!\"\n        msg <- c(msg, nmsg)\n        return(msg)\n    }\n    return(if (length(msg)) msg else TRUE)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the CoreSet class?",
        "answer": "The CoreSet (cSet) class is designed as a superclass for pSets in the PharmacoGx and RadioGx packages to contain data generated in screens of cancer sample lines for their genetic profile and sensitivities to therapy (Pharmacological or Radiation). It provides a standardized data structure for storing and analyzing molecular profiles and treatment response data."
      },
      {
        "question": "What are the main slots in the CoreSet class?",
        "answer": "The main slots in the CoreSet class are: annotation, molecularProfiles, sample, treatment, treatmentResponse, perturbation, curation, and datasetType. These slots store various types of data and metadata related to the molecular profiles, samples, treatments, and experimental results."
      },
      {
        "question": "How does the CoreSet constructor handle missing or empty data?",
        "answer": "The CoreSet constructor creates empty objects for data not provided. It also performs checks on the structure of the provided data, ensures proper formatting, and creates summary statistics for sensitivity and perturbation data if they are not provided but the datasetType indicates they should be present."
      }
    ],
    "completion_tasks": [
      {
        "partial": "CoreSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n    sensitivityInfo=data.frame(), sensitivityRaw=array(dim=c(0,0,0)),\n    sensitivityProfiles=matrix(), sensitivityN=matrix(nrow=0, ncol=0),\n    perturbationN=array(NA, dim=c(0,0,0)), curationSample=data.frame(),\n    curationTissue=data.frame(), curationTreatment=data.frame(),\n    treatment=data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n    verify=TRUE, ...\n) {\n    # Parse deprecated parameters\n    dotnames <- ...names()\n    if (\"cell\" %in% dotnames) {\n        .warning(\"The cell parameter is deprecated, assigning to sample...\")\n        sample <- cell\n    }\n    if (\"drug\" %in% dotnames) {\n        .warning(\"The drug paramter is deprecated, assigning to treatment...\")\n        treatment <- drug\n    }\n\n    # Ensure new sampleid and treatmentid identifiers are honoured\n    sample <- .checkForSampleId(sample)\n    treatment <- .checkForTreatmentId(treatment)\n    sensitivityInfo <- .checkForSampleId(sensitivityInfo)\n    sensitivityInfo <- .checkForTreatmentId(sensitivityInfo)\n    curationSample <- .checkForIdColumn(curationSample, c(\"sampleid\", \"unique.sampleid\"), \"cellid\")\n    curationTreatment <- .checkForIdColumn(curationTreatment, c(\"treatmentid\", \"unique.treatmentid\"), \"drugid\")\n    for (nm in names(molecularProfiles)) {\n        colData(molecularProfiles[[nm]]) <- .checkForSampleId(\n            colData(molecularProfiles[[nm]]))\n        colData(molecularProfiles[[nm]]) <- .checkForIdColumn(\n            colData(molecularProfiles[[nm]]), \"treatmentid\", \"drugid\",\n            error=FALSE)\n    }\n\n    datasetType <- match.arg(datasetType)\n\n    # Create annotation list\n    annotation <- list(\n        name = as.character(name),\n        dateCreated = date(),\n        sessionInfo = sessionInfo(),\n        call = match.call()\n    )\n\n    # TODO: Complete the rest of the function\n}",
        "complete": "CoreSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n    sensitivityInfo=data.frame(), sensitivityRaw=array(dim=c(0,0,0)),\n    sensitivityProfiles=matrix(), sensitivityN=matrix(nrow=0, ncol=0),\n    perturbationN=array(NA, dim=c(0,0,0)), curationSample=data.frame(),\n    curationTissue=data.frame(), curationTreatment=data.frame(),\n    treatment=data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n    verify=TRUE, ...\n) {\n    # Parse deprecated parameters\n    dotnames <- ...names()\n    if (\"cell\" %in% dotnames) {\n        .warning(\"The cell parameter is deprecated, assigning to sample...\")\n        sample <- cell\n    }\n    if (\"drug\" %in% dotnames) {\n        .warning(\"The drug paramter is deprecated, assigning to treatment...\")\n        treatment <- drug\n    }\n\n    # Ensure new sampleid and treatmentid identifiers are honoured\n    sample <- .checkForSampleId(sample)\n    treatment <- .checkForTreatmentId(treatment)\n    sensitivityInfo <- .checkForSampleId(sensitivityInfo)\n    sensitivityInfo <- .checkForTreatmentId(sensitivityInfo)\n    curationSample <- .checkForIdColumn(curationSample, c(\"sampleid\", \"unique.sampleid\"), \"cellid\")\n    curationTreatment <- .checkForIdColumn(curationTreatment, c(\"treatmentid\", \"unique.treatmentid\"), \"drugid\")\n    for (nm in names(molecularProfiles)) {\n        colData(molecularProfiles[[nm]]) <- .checkForSampleId(\n            colData(molecularProfiles[[nm]]))\n        colData(molecularProfiles[[nm]]) <- .checkForIdColumn(\n            colData(molecularProfiles[[nm]]), \"treatmentid\", \"drugid\",\n            error=FALSE)\n    }\n\n    datasetType <- match.arg(datasetType)\n\n    # Create annotation list\n    annotation <- list(\n        name = as.character(name),\n        dateCreated = date(),\n        sessionInfo = sessionInfo(),\n        call = match.call()\n    )\n\n    # Process molecular profiles\n    for (i in seq_len(length(molecularProfiles))){\n        if (!is(molecularProfiles[[i]], \"SummarizedExperiment\")) {\n            stop(sprintf(\"Please provide the %s data as a SummarizedExperiment\",\n                names(molecularProfiles[i])))\n        } else {\n            rowData(molecularProfiles[[i]]) <-\n                rowData(molecularProfiles[[i]])[\n                    rownames(assays(molecularProfiles[[i]])[[1]]), , drop=FALSE\n                ]\n            colData(molecularProfiles[[i]]) <- colData(molecularProfiles[[i]])[\n                colnames(assays(molecularProfiles[[i]])[[1]]), , drop=FALSE\n            ]\n        }\n    }\n\n    # Create sensitivity list\n    sensitivity <- list(\n        info = as.data.frame(sensitivityInfo, stringsAsFactors=FALSE),\n        raw = sensitivityRaw,\n        profiles = as.data.frame(sensitivityProfiles, stringsAsFactors=FALSE),\n        n = sensitivityN\n    )\n\n    # Create curation list\n    curation <- list(\n        sample = as.data.frame(curationSample, stringsAsFactors=FALSE),\n        tissue = as.data.frame(curationTissue, stringsAsFactors=FALSE)\n    )\n\n    # Create perturbation list\n    perturbation <- list(\n        n = perturbationN,\n        info = if (datasetType %in% c(\"perturbation\", \"both\")) {\n            \"The metadata for the perturbation experiments is available for each molecular type by calling the appropriate info function.\"\n        } else {\n            \"Not a perturbation dataset.\"\n        }\n    )\n\n    # Create CoreSet object\n    object <- .CoreSet(\n        annotation = annotation,\n        molecularProfiles = molecularProfiles,\n        sample = as.data.frame(sample),\n        datasetType = datasetType,\n        treatmentResponse = sensitivity,\n        perturbation = perturbation,\n        curation = curation,\n        treatment = treatment\n    )\n\n    # Verify object structure if requested\n    if (verify) { checkCsetStructure(object) }\n\n    # Update sensitivity and perturbation numbers if necessary\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n        sensNumber(object) <- .summarizeSensitivityNumbers(object)\n    }\n    if (length(perturbationN) == 0 && datasetType %in% c(\"perturbation\", \"both\")) {\n        pertNumber(object) <- .summarizePerturbationNumbers(object)\n    }\n\n    return(object)\n}"
      },
      {
        "partial": "CoreSet2 <- function(name=\"emptySet\", treatment=data.frame(),\n    sample=data.frame(), molecularProfiles=MultiAssayExperiment(),\n    treatmentResponse=LongTable(), datasetType=\"sensitivity\",\n    perturbation=list(n=array(dim=3), info=\"No perturbation data!\"),\n    curation=list(sample=data.frame(), treatment=data.frame())\n) {\n    # Update old curation names\n    names(curation) <- gsub(\"drug|radiation\", \"treatment\", names(curation))\n    names(curation) <- gsub(\"cell\", \"sample\", names(curation))\n\n    # Input validation\n    assertCharacter(name, len=1)\n    assertDataFrame(treatment)\n    assertDataFrame(sample)\n    assertClass(molecularProfiles, \"MultiAssayExperiment\")\n    assert(\n        checkClass(treatmentResponse, \"LongTable\"),\n        checkClass(treatmentResponse, \"LongTableDataMapper\")\n    )\n    assertList(curation, min.len=2)\n    assertSubset(c(\"sample\", \"treatment\"), choices=names(curation))\n\n    # Capture object creation environment\n    annotation <- list(name=name, dateCreated=date(),\n        sessionInfo=sessionInfo(), call=match.call())\n\n    # Conditionally materialize DataMapper\n    if (is(treatmentResponse, 'LongTableDataMapper'))\n        treatmentResponse <- metaConstruct(treatmentResponse)\n\n    # Handle missing rownames for sample\n    if (!all(sample$sampleid == rownames(sample)))\n        rownames(sample) <- sample$sampleid\n\n    # TODO: Complete the rest of the function\n}",
        "complete": "CoreSet2 <- function(name=\"emptySet\", treatment=data.frame(),\n    sample=data.frame(), molecularProfiles=MultiAssayExperiment(),\n    treatmentResponse=LongTable(), datasetType=\"sensitivity\",\n    perturbation=list(n=array(dim=3), info=\"No perturbation data!\"),\n    curation=list(sample=data.frame(), treatment=data.frame())\n) {\n    # Update old curation names\n    names(curation) <- gsub(\"drug|radiation\", \"treatment\", names(curation))\n    names(curation) <- gsub(\"cell\", \"sample\", names(curation))\n\n    # Input validation\n    assertCharacter(name, len=1)\n    assertDataFrame(treatment)\n    assertDataFrame(sample)\n    assertClass(molecularProfiles, \"MultiAssayExperiment\")\n    assert(\n        checkClass(treatmentResponse, \"LongTable\"),\n        checkClass(treatmentResponse, \"LongTableDataMapper\")\n    )\n    assertList(curation, min.len=2)\n    assertSubset(c(\"sample\", \"treatment\"), choices=names(curation))\n\n    # Capture object creation environment\n    annotation <- list(name=name, dateCreated=date(),\n        sessionInfo=sessionInfo(), call=match.call())\n\n    # Conditionally materialize DataMapper\n    if (is(treatmentResponse, 'LongTableDataMapper'))\n        treatmentResponse <- metaConstruct(treatmentResponse)\n\n    # Handle missing rownames for sample\n    if (!all(sample$sampleid == rownames(sample)))\n        rownames(sample) <- sample$sampleid\n\n    # Create CoreSet object\n    object <- .CoreSet(\n        annotation = annotation,\n        sample = sample,\n        treatment = treatment,\n        molecularProfiles = molecularProfiles,\n        treatmentResponse = treatmentResponse,\n        datasetType = datasetType,\n        curation = curation,\n        perturbation = perturbation\n    )\n\n    # Data integrity checks\n    validProfiles <- .checkMolecularProfiles(object)\n    validTreatments <- .checkTreatmentResponse(object)\n\n    diagnosis <- c(!isTRUE(validProfiles), !isTRUE(validTreatments))\n    if (any(diagnosis)) {\n        .error(paste0(list(validProfiles, validTreatments)[diagnosis],\n            collapse=\"\\n\", sep=\"\\n\"))\n    }\n\n    return(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/deprecated.R",
    "language": "R",
    "content": "#' @title List of `deprecated` or `defunct` methods in the `CoreGx` R package.\n#'\n#' @details\n#' ## deprecated\n#' `CoreSet`: The `CoreSet` constructor is being updated to have a new API. This\n#' API is currently available via the `CoreSet2` constructor. In Bioconductor\n#' 3.16, the old constructor will be renamed `CoreSet2` and the new constructor\n#' will be renamed `CoreSet`.\n#'\n#' ## defunct\n#' `buildLongTable`: This function no longer works as building a `LongTable` or\n#' `TreatmentResponseExperiment` now uses a `DataMapper` and the `metaConstruct`\n#' method. See `vignette(\"LongTable\")` for a detailed description of how\n#' to create a LongTable object.\n#'\n#' @name CoreGx-deprecated\n#' @aliases\n#' CoreGx-defunct\nNULL\n\n\n# ===== Deprecated\n\n\n# ===== Defunct\n\n# -- buildLongTableClass\n\n# ==== LongTable Class\n\n#' LongTable build method\n#'\n#' @description Create a LongTable object from a single data.table or\n#'   data.frame object.\n#'\n#' @param from `character` Path to the .csv file containing the data and\n#'   metadata from which to build the `LongTable`.\n#' @param colDataCols `list` List with two `character` vectors, the first\n#'   specifying one or more columns to be used as column identifiers (e.g.,\n#'   drug name columns) and the second containing any additional metadata\n#'   columns related to the column identifiers. If you wish to rename any of\n#'   these columns, assign the new names to their respective character vectors.\n#' @param rowDataCols `list` List with two `character` vectors, the first\n#'   specifying one or more columns to be used as cell identifiers (e.g.,\n#'   cell-line name columns) and the second containing any additional metadata\n#'   columns related to the cell identifiers. If you wish to rename any of\n#'   these columns, assign the new names to their respective character vectors.\n#' @param assayCols `list` A named list of character vectors specifying how to\n#'   parse assay columns into a list of `data.table`s. Each list data.table\n#'   will be named for the name of corresponding list item and contain the columns\n#'   specified in the character vector of column names in each list item. If\n#'   there are no names for assayCols, the assays will be numbered by instead.\n#'\n#' @return A `LongTable` object containing one or more assays, indexed by\n#'   rowID and colID.\n#'\n#' @import data.table\n#' @export\nsetMethod('buildLongTable', signature(from='data.frame'),\n        function(from, rowDataCols, colDataCols, assayCols) {\n\n    .Defunct(\"metaConstruct\", msg=\"This method has been deprecated\n        in favour of use of the LongTableDataMapper metadata object\n        with the metaConstruct method to build a LongTable object!\")\n\n    # -- local helpers\n    .unlist <- function(x) unlist(x, recursive=TRUE, use.names=FALSE)\n\n    # -- handle missing params\n    missingParams <- c(missing(rowDataCols), missing(colDataCols), missing(assayCols))\n    if (any(missingParams))\n        stop(.errorMsg('[CoreGx::buildLongTable,data.frame] The following',\n            ' parameters are required:',\n            .collapse(c('rowDataCols', 'colDataCols', 'assayCols')[missingParams])))\n\n    # -- validate input and return useful messages if invalid\n    ## TODO:: Check input parameters are valid\n\n    # -- convert to data.table by reference\n    if (!is.data.table(from))\n        from <- data.table(from)\n\n    # -- build drug and cell metadata tables and index by the appropriate ID\n    colData <- unique(from[, .unlist(colDataCols), with=FALSE])\n    setorderv(colData, colDataCols[[1]])  # order by id columns\n    colData[, colKey := seq_len(.N)]\n    rowData <- unique(from[, .unlist(rowDataCols), with=FALSE])\n    setorderv(rowData, rowDataCols[[1]])  # order by id columns\n    rowData[, rowKey := seq_len(.N)]\n\n    # -- add the row and column ids to the value data\n    assayData <- from[rowData, on=.unlist(rowDataCols)][colData, on=as.character(unlist(colDataCols))]\n    rm(from)\n    assayData[, as.character(unique(c(.unlist(rowDataCols), .unlist(colDataCols)))) := NULL]\n    # row reason to prevent sort in join because key sorts\n    setkey(assayData, rowKey, colKey)\n\n    setkey(rowData, rowKey)\n    setkey(colData, colKey)\n\n    # -- rename columns, if necessary\n    rowDataColnames <- lapply(rowDataCols, names)\n    notNullRownames <- !vapply(rowDataColnames, FUN=is.null, FUN.VALUE=logical(1))\n    if (any(notNullRownames))\n        for (i in which(notNullRownames)) {\n            setnames(rowData, rowDataCols[[i]], names(rowDataCols[[i]]))\n            rowDataCols[[i]] <- names(rowDataCols[[i]])\n        }\n\n    colDataColnames <- lapply(colDataCols, names)\n    notNullColnames <- !vapply(colDataColnames, FUN=is.null, FUN.VALUE=logical(1))\n    if (any(notNullColnames))\n        for (i in which(notNullColnames)) {\n            setnames(colData, colDataCols[[i]], names(colDataCols[[i]]))\n            colDataCols[[i]] <- names(colDataCols[[i]])\n        }\n\n    # -- drop colKey or rowKey from assayCols, since we are adding it back in the\n    # next step\n    ## TODO:: Add a check to see if the keys are there to avoid dropping/re-adding\n    .drop.in <- function(x, y) x[!(x %in% y)]\n    assayCols <- lapply(assayCols, .drop.in, y=c('colKey', 'rowKey'))\n\n    # -- add the index columns to the different assay column vectors\n    # this allows the .selectDataTable helper to be more general\n    .prependToVector <- function(vector, values) c(values, vector)\n    assayCols <- lapply(assayCols, FUN=.prependToVector, values=c('rowKey', 'colKey'))\n    if (is.null(names(assayCols))) names(assayCols) <- paste0('assay', seq_along(assayCols))\n    assays <- lapply(assayCols, .selectDataTable, DT=assayData)\n\n    # -- remove the colname suffixes by reference from assays which had the same\n    # colnames prior to joining into a single DT\n    for (assay in assays) {\n        setnames(assay, colnames(assay), gsub('\\\\._\\\\d+$', '', colnames(assay)))\n    }\n\n    return(LongTable(rowData=rowData, rowIDs=rowDataCols[[1]],\n                     colData=colData, colIDs=colDataCols[[1]],\n                     assays=assays))\n})\n\n#' LongTable build method from character\n#'\n#' @description LongTable Create a LongTable object from a single .csv file\n#'\n#' @param from `character` Path to the .csv file containing the data and\n#'   metadata from which to build the `LongTable`.\n#' @param colDataCols `list` List with two `character` vectors, the first\n#'   specifying one or more columns to be used as column identifiers (e.g.,\n#'   drug name columns) and the second containing any additional metadata\n#'   columns related to the column identifiers.\n#' @param rowDataCols `list` List with two `character` vectors, the first\n#'   specifying one or more columns to be used as cell identifiers (e.g.,\n#'   cell-line name columns) and the second containing any additional metadata\n#'   columns related to the cell identifiers.\n#' @param assayCols `list` A named list of character vectors specifying how to\n#'   parse assay columns into a list of `data.table`s. Each list data.table\n#'   will be named for the name of corresponding list item and contain the columns\n#'   specified in the character vector of column names in each list item.\n#'\n#' @return A `LongTable` object containing one or more assays, indexed by\n#'   rowID and colID.\n#'\n#' @import data.table\n#' @importFrom crayon magenta\n#' @export\nsetMethod('buildLongTable', signature(from='character'),\n          function(from, rowDataCols, colDataCols, assayCols)\n{\n    if (length(from) > 1)  # Call list subsetting method\n        buildLongTable(as.list(from), rowDataCols, colDataCols, assayCols)\n    if (!file.exists(from))\n        stop(magenta$bold(\"The is no file at path: \", from, '. Please double\n            check the location of the source file!'))\n\n    # read in data\n    tableData <- .freadNA(from)\n\n    return(buildLongTable(from=tableData, rowDataCols, colDataCols, assayCols))\n})\n\n#' LongTable build method from list\n#'\n#' @description Create a LongTable object from a list containing file paths,\n#'   data.frames and data.tables.\n#'\n#' @examples\n#' \\dontrun{\n#' assayList <- assays(merckLongTable, withDimnames=TRUE)\n#' rowDataCols <- list(rowIDs(merckLongTable), rowMeta(merckLongTable))\n#' colDataCols <- list(colIDs(merckLongTable), colMeta(merckLongTable))\n#' assayCols <- assayCols(merckLongTable)\n#' longTable <- buildLongTable(from=assayList, rowDataCols, colDataCols, assayCols)\n#' }\n#'\n#' @param from `list` A list containing any combination of character file paths,\n#'  data.tables and data.frames which will be used to construct the LongTable.\n#' @param colDataCols `list` List with two `character` vectors, the first\n#'   specifying one or more columns to be used as column identifiers (e.g.,\n#'   drug name columns) and the second containing any additional metadata\n#'   columns related to the column identifiers.\n#' @param rowDataCols `list` List with two `character` vectors, the first\n#'   specifying one or more columns to be used as cell identifiers (e.g.,\n#'   cell-line name columns) and the second containing any additional metadata\n#'   columns related to the cell identifiers.\n#' @param assayCols `list` A named list of character vectors specifying how to\n#'   parse assay columns into a list of `data.table`s. Each list data.table\n#'   will be named for the name of corresponding list item and contain the columns\n#'   specified in the character vector of column names in each list item.\n#'\n#' @return A `LongTable` object constructed with the data in `from`.\n#'\n#' @import data.table\n#' @importFrom crayon magenta cyan\n#' @export\nsetMethod('buildLongTable', signature(from='list'),\n        function(from, rowDataCols, colDataCols, assayCols) {\n\n    # Prevent modify by reference for data.tables in list\n    from <- copy(from)\n\n    # local helpers\n    ##FIXME:: This is exactly what the Map function is (an alias for mapply with\n    ##   SIMPLIFY=FALSE)\n    .mapply <- function(...) mapply(..., SIMPLIFY=FALSE)\n\n    # preprocess from list\n    isChar <- is.items(from, 'character')\n    isDT <- is.items(from, FUN=is.data.table)\n    isDF <- is.items(from, FUN=is.data.frame) & !isDT\n\n    if (!all(isChar | isDT | isDF))\n        stop(.errorMsg('\\n[CoreGx::buildLongTable,list-method] List items at',\n            ' indexes ', .collapse(which(!(isChar | isDT | isDF ))),\n            ' are not character, data.table or data.frame.', collapse=', '))\n\n    if (any(isChar)) from <- c(from[!isChar], lapply(from[isChar], FUN=.freadNA))\n    if (any(isDF)) for (i in which(isDF)) from[[i]] <- data.table(from[[i]])\n\n    # validate mappings\n    ## TODO:: Ensure there is no case where joining on rowMeta or colMeta gives\n    #  different results than just ids\n    joinCols <- unique(unlist(c(rowDataCols, colDataCols)))\n    dataColNames <- lapply(from, FUN=colnames)\n    joinColsIn <- lapply(dataColNames, `%in%`, x=joinCols)\n    hasAllIdCols <- unlist(lapply(joinColsIn, FUN=all))\n    if (!all(hasAllIdCols)) {\n        missingCols <- unique(unlist(.mapply(`[`, x=joinCols, i=joinColsIn)))\n        stop(.errorMsg('[CoreGx::buildLongTable,list] Assay(s) ',\n            .collapse(which(hasAllIdCols)), ' are missing one or more id ',\n            'columns: ', .collapse(missingCols), collapse=', '))\n    }\n\n    # Set keys for faster joins\n    for (i in seq_along(from)) setkeyv(from[[i]], cols=joinCols)\n\n    # join assays into a single table\n    DT <- from[[1]]\n    from[[1]] <- NULL\n    for (i in seq_along(from))\n        DT <- merge.data.table(DT, from[[i]], suffixes=c('', paste0('._', i)))\n\n    # fix assayCols if there are duplicate column names between assays\n    # the join will append '._n' where n is the assay index - 1\n    nonDataCols <- setdiff(colnames(DT), unique(c(unlist(rowDataCols), unlist(colDataCols))))\n    assaySuffixCols <- lapply(paste0('\\\\._', seq_along(from)), grep, x=nonDataCols, value=TRUE)\n    .length.gt.0 <- function(x) length(x) > 0\n    hasSuffixes <- unlist(lapply(assaySuffixCols, FUN=.length.gt.0))\n    duplicatedCols <- lapply(assaySuffixCols[hasSuffixes], gsub,\n        pattern='\\\\._\\\\d+', replacement='')\n\n    .which.in <- function(x, y) which(x %in% y)\n    whichHasSuffixes <- which(hasSuffixes) + 1\n    whichDuplicated <- .mapply(.which.in,\n        x=assayCols[whichHasSuffixes], y=duplicatedCols)\n    assayCols[whichHasSuffixes] <-\n        .mapply(replace, x=assayCols[whichHasSuffixes],\n            list=whichDuplicated, values=assaySuffixCols[hasSuffixes])\n\n    # construct new LongTable\n    buildLongTable(from=DT, rowDataCols, colDataCols, assayCols)\n})\n\n\n# ---- Helper Methods\n\n#' fread with more default na.strings\n#'\n#' @keywords internal\n#' @noRd\n.freadNA <- function(...) {\n    as.na <- unique(c(getOption('datatable.na.string'),\n        c('NA', 'NULL', 'NaN', 'missing', 'None',\n            'none', 'na', 'null', 'Null', 'Na')))\n    fread(..., na.strings=as.na)\n}\n\n\n#' Select a set of column names from a data.table, returning a copy of the\n#'   data.table with duplicate rows removed\n#'\n#' @param colNames `character` The column names to select from the data.table\n#' @param DT `data.table`, `data.frame`, `matrix` An object coercible to a `data.table`.\n#'   Please note rownames will be dropped by default.\n#' @param keep.rownames `logical` or `character` Passed through to the data.table coercing if DT is not a\n#'   `data.table`. If TRUE, rownames will be caputured in the `rn` column; if FALSE (default) rownames will\n#'   be dropped; if `character`, rownames will be captured in a column with the same name.\n#'\n#' @return `data.table` Copy of `DT` containing only the specified columns, with duplicate rows removed.\n#'\n#' @import data.table\n#' @keywords internal\n#' @noRd\n.selectDataTable <- function(colNames, DT, keep.rownames=FALSE) {\n    # validate input\n    if (!is.data.table(DT)) {\n        tryCatch({\n            DT <- data.table(DT, keep.rownames=keep.rownames)\n        }, warning=function(w) {\n            warning(w)\n        }, error=function(e) {\n            message(e)\n            stop(\"Argument to DT parameter must be coercible to a data.table!\")\n        })\n    }\n    if (!is.character(colnames(DT))) stop(\"Currently only character column ids are supported!\")\n    missingColumns <- setdiff(colNames, colnames(DT))\n    if (length(missingColumns) > 0)\n        warning(paste0(\"There are no columns named \", paste0(missingColumns, collapse=\", \"), 'in DT.\n            Continuing subset without these columns.'))\n\n    # perform subset and copy to prevent modify by refence issues\n    selectedDT <- copy(unique(DT[, .SD, .SDcols=colnames(DT) %in% colNames]))\n\n    return(selectedDT)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `buildLongTable` method in this code snippet?",
        "answer": "The `buildLongTable` method is used to create a LongTable object from various input types such as data frames, character file paths, or lists. It organizes data into a structured format with row data, column data, and assays, which is useful for storing and manipulating large datasets in a standardized way."
      },
      {
        "question": "How does the code handle different input types for the `buildLongTable` method?",
        "answer": "The code uses method dispatch with different signatures for `from` parameter: 'data.frame', 'character', and 'list'. For data frames, it directly processes the input. For character inputs (file paths), it reads the file using `.freadNA`. For list inputs, it handles a combination of file paths, data frames, and data tables, converting them as needed before processing."
      },
      {
        "question": "What is the purpose of the `.selectDataTable` helper function in this code?",
        "answer": "The `.selectDataTable` helper function is used to select a set of columns from a data table, remove duplicate rows, and return a copy of the resulting data table. It handles input validation, converts non-data.table objects to data tables, and performs the column selection and row deduplication operations. This function is likely used internally to process and prepare data for the LongTable object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('buildLongTable', signature(from='data.frame'),\n        function(from, rowDataCols, colDataCols, assayCols) {\n\n    .Defunct(\"metaConstruct\", msg=\"This method has been deprecated\n        in favour of use of the LongTableDataMapper metadata object\n        with the metaConstruct method to build a LongTable object!\")\n\n    # -- local helpers\n    .unlist <- function(x) unlist(x, recursive=TRUE, use.names=FALSE)\n\n    # -- handle missing params\n    missingParams <- c(missing(rowDataCols), missing(colDataCols), missing(assayCols))\n    if (any(missingParams))\n        stop(.errorMsg('[CoreGx::buildLongTable,data.frame] The following',\n            ' parameters are required:',\n            .collapse(c('rowDataCols', 'colDataCols', 'assayCols')[missingParams])))\n\n    # -- convert to data.table by reference\n    if (!is.data.table(from))\n        from <- data.table(from)\n\n    # -- build drug and cell metadata tables and index by the appropriate ID\n    colData <- unique(from[, .unlist(colDataCols), with=FALSE])\n    setorderv(colData, colDataCols[[1]])  # order by id columns\n    colData[, colKey := seq_len(.N)]\n    rowData <- unique(from[, .unlist(rowDataCols), with=FALSE])\n    setorderv(rowData, rowDataCols[[1]])  # order by id columns\n    rowData[, rowKey := seq_len(.N)]\n\n    # -- add the row and column ids to the value data\n    assayData <- from[rowData, on=.unlist(rowDataCols)][colData, on=as.character(unlist(colDataCols))]\n    rm(from)\n    assayData[, as.character(unique(c(.unlist(rowDataCols), .unlist(colDataCols)))) := NULL]\n    # row reason to prevent sort in join because key sorts\n    setkey(assayData, rowKey, colKey)\n\n    setkey(rowData, rowKey)\n    setkey(colData, colKey)\n\n    # -- rename columns, if necessary\n    rowDataColnames <- lapply(rowDataCols, names)\n    notNullRownames <- !vapply(rowDataColnames, FUN=is.null, FUN.VALUE=logical(1))\n    if (any(notNullRownames))\n        for (i in which(notNullRownames)) {\n            setnames(rowData, rowDataCols[[i]], names(rowDataCols[[i]]))\n            rowDataCols[[i]] <- names(rowDataCols[[i]])\n        }\n\n    colDataColnames <- lapply(colDataCols, names)\n    notNullColnames <- !vapply(colDataColnames, FUN=is.null, FUN.VALUE=logical(1))\n    if (any(notNullColnames))\n        for (i in which(notNullColnames)) {\n            setnames(colData, colDataCols[[i]], names(colDataCols[[i]]))\n            colDataCols[[i]] <- names(colDataCols[[i]])\n        }\n\n    # -- drop colKey or rowKey from assayCols, since we are adding it back in the\n    # next step\n    .drop.in <- function(x, y) x[!(x %in% y)]\n    assayCols <- lapply(assayCols, .drop.in, y=c('colKey', 'rowKey'))\n\n    # -- add the index columns to the different assay column vectors\n    # this allows the .selectDataTable helper to be more general\n    .prependToVector <- function(vector, values) c(values, vector)\n    assayCols <- lapply(assayCols, FUN=.prependToVector, values=c('rowKey', 'colKey'))\n    if (is.null(names(assayCols))) names(assayCols) <- paste0('assay', seq_along(assayCols))\n    assays <- lapply(assayCols, .selectDataTable, DT=assayData)\n\n    # -- remove the colname suffixes by reference from assays which had the same\n    # colnames prior to joining into a single DT\n    for (assay in assays) {\n        setnames(assay, colnames(assay), gsub('\\\\._\\\\d+$', '', colnames(assay)))\n    }\n\n    return(LongTable(rowData=rowData, rowIDs=rowDataCols[[1]],\n                     colData=colData, colIDs=colDataCols[[1]],\n                     assays=assays))\n})",
        "complete": "setMethod('buildLongTable', signature(from='data.frame'),\n        function(from, rowDataCols, colDataCols, assayCols) {\n\n    .Defunct(\"metaConstruct\", msg=\"This method has been deprecated\n        in favour of use of the LongTableDataMapper metadata object\n        with the metaConstruct method to build a LongTable object!\")\n\n    .unlist <- function(x) unlist(x, recursive=TRUE, use.names=FALSE)\n\n    missingParams <- c(missing(rowDataCols), missing(colDataCols), missing(assayCols))\n    if (any(missingParams))\n        stop(.errorMsg('[CoreGx::buildLongTable,data.frame] The following',\n            ' parameters are required:',\n            .collapse(c('rowDataCols', 'colDataCols', 'assayCols')[missingParams])))\n\n    if (!is.data.table(from)) from <- data.table(from)\n\n    colData <- unique(from[, .unlist(colDataCols), with=FALSE])\n    setorderv(colData, colDataCols[[1]])\n    colData[, colKey := seq_len(.N)]\n    rowData <- unique(from[, .unlist(rowDataCols), with=FALSE])\n    setorderv(rowData, rowDataCols[[1]])\n    rowData[, rowKey := seq_len(.N)]\n\n    assayData <- from[rowData, on=.unlist(rowDataCols)][colData, on=as.character(unlist(colDataCols))]\n    rm(from)\n    assayData[, as.character(unique(c(.unlist(rowDataCols), .unlist(colDataCols)))) := NULL]\n    setkey(assayData, rowKey, colKey)\n\n    setkey(rowData, rowKey)\n    setkey(colData, colKey)\n\n    for (i in which(!vapply(lapply(rowDataCols, names), is.null, logical(1)))) {\n        setnames(rowData, rowDataCols[[i]], names(rowDataCols[[i]]))\n        rowDataCols[[i]] <- names(rowDataCols[[i]])\n    }\n\n    for (i in which(!vapply(lapply(colDataCols, names), is.null, logical(1)))) {\n        setnames(colData, colDataCols[[i]], names(colDataCols[[i]]))\n        colDataCols[[i]] <- names(colDataCols[[i]])\n    }\n\n    assayCols <- lapply(assayCols, function(x) x[!(x %in% c('colKey', 'rowKey'))])\n    assayCols <- lapply(assayCols, function(vector) c('rowKey', 'colKey', vector))\n    if (is.null(names(assayCols))) names(assayCols) <- paste0('assay', seq_along(assayCols))\n    assays <- lapply(assayCols, .selectDataTable, DT=assayData)\n\n    for (assay in assays) {\n        setnames(assay, colnames(assay), gsub('\\\\._\\\\d+$', '', colnames(assay)))\n    }\n\n    return(LongTable(rowData=rowData, rowIDs=rowDataCols[[1]],\n                     colData=colData, colIDs=colDataCols[[1]],\n                     assays=assays))\n})"
      },
      {
        "partial": "setMethod('buildLongTable', signature(from='list'),\n        function(from, rowDataCols, colDataCols, assayCols) {\n\n    from <- copy(from)\n\n    isChar <- is.items(from, 'character')\n    isDT <- is.items(from, FUN=is.data.table)\n    isDF <- is.items(from, FUN=is.data.frame) & !isDT\n\n    if (!all(isChar | isDT | isDF))\n        stop(.errorMsg('\\n[CoreGx::buildLongTable,list-method] List items at',\n            ' indexes ', .collapse(which(!(isChar | isDT | isDF ))),\n            ' are not character, data.table or data.frame.', collapse=', '))\n\n    if (any(isChar)) from <- c(from[!isChar], lapply(from[isChar], FUN=.freadNA))\n    if (any(isDF)) for (i in which(isDF)) from[[i]] <- data.table(from[[i]])\n\n    joinCols <- unique(unlist(c(rowDataCols, colDataCols)))\n    dataColNames <- lapply(from, FUN=colnames)\n    joinColsIn <- lapply(dataColNames, `%in%`, x=joinCols)\n    hasAllIdCols <- unlist(lapply(joinColsIn, FUN=all))\n    if (!all(hasAllIdCols)) {\n        missingCols <- unique(unlist(mapply(`[`, x=joinCols, i=joinColsIn, SIMPLIFY=FALSE)))\n        stop(.errorMsg('[CoreGx::buildLongTable,list] Assay(s) ',\n            .collapse(which(hasAllIdCols)), ' are missing one or more id ',\n            'columns: ', .collapse(missingCols), collapse=', '))\n    }\n\n    for (i in seq_along(from)) setkeyv(from[[i]], cols=joinCols)\n\n    DT <- from[[1]]\n    from[[1]] <- NULL\n    for (i in seq_along(from))\n        DT <- merge.data.table(DT, from[[i]], suffixes=c('', paste0('._', i)))\n\n    nonDataCols <- setdiff(colnames(DT), unique(c(unlist(rowDataCols), unlist(colDataCols))))\n    assaySuffixCols <- lapply(paste0('\\\\._', seq_along(from)), grep, x=nonDataCols, value=TRUE)\n    hasSuffixes <- unlist(lapply(assaySuffixCols, function(x) length(x) > 0))\n    duplicatedCols <- lapply(assaySuffixCols[hasSuffixes], gsub,\n        pattern='\\\\._\\\\d+', replacement='')\n\n    whichHasSuffixes <- which(hasSuffixes) + 1\n    whichDuplicated <- mapply(function(x, y) which(x %in% y),\n        x=assayCols[whichHasSuffixes], y=duplicatedCols, SIMPLIFY=FALSE)\n    assayCols[whichHasSuffixes] <-\n        mapply(replace, x=assayCols[whichHasSuffixes],\n            list=whichDuplicated, values=assaySuffixCols[hasSuffixes], SIMPLIFY=FALSE)\n\n    buildLongTable(from=DT, rowDataCols, colDataCols, assayCols)\n})",
        "complete": "setMethod('buildLongTable', signature(from='list'),\n        function(from, rowDataCols, colDataCols, assayCols) {\n\n    from <- copy(from)\n\n    isChar <- is.items(from, 'character')\n    isDT <- is.items(from, FUN=is.data.table)\n    isDF <- is.items(from, FUN=is.data.frame) & !isDT\n\n    if (!all(isChar | isDT | isDF))\n        stop(.errorMsg('\\n[CoreGx::buildLongTable,list-method] List items at',\n            ' indexes ', .collapse(which(!(isChar | isDT | isDF ))),\n            ' are not character, data.table or data.frame.', collapse=', '))\n\n    if (any(isChar)) from <- c(from[!isChar], lapply(from[isChar], FUN=.freadNA))\n    if (any(isDF)) for (i in which(isDF)) from[[i]] <- data.table(from[[i]])\n\n    joinCols <- unique(unlist(c(rowDataCols, colDataCols)))\n    dataColNames <- lapply(from, FUN=colnames)\n    joinColsIn <- lapply(dataColNames, `%in%`, x=joinCols)\n    hasAllIdCols <- unlist(lapply(joinColsIn, FUN=all))\n    if (!all(hasAllIdCols)) {\n        missingCols <- unique(un"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/updateObject-methods.R",
    "language": "R",
    "content": "#' Update the `CoreSet` class after changes in it struture or API\n#'\n#' @param object A `CoreSet` object to update the class structure for.\n#' @param verify A `logical(1)` indicating is `validObject` should be called\n#' after updating the object. Defaults to `TRUE`, only set `FALSE` for debugging.\n#' @param verbose TRUE or FALSE, indicating whether information about the update\n#' should be reported\n#' @return `CoreSet` with update class structure.\n#'\n#' @md\n#'\n#' @importFrom MultiAssayExperiment MultiAssayExperiment\n#' @importMethodsFrom BiocGenerics updateObject\n#' @export\nsetMethod('updateObject', signature(object=\"CoreSet\"),\n        function(object, verify=FALSE, verbose = FALSE) {\n\n    if (verbose) {\n        message(\"updateObject object = 'CoreSet'\")\n    }\n    \n    if (!.hasSlot(object, \"sample\")) {\n        cell <- object@cell\n        sample_ <- cell\n    } else {\n        sample_ <- object@sample\n    }\n    colnames(sample_) <- gsub(\"cellid\", \"sampleid\", colnames(sample_))\n\n    if (!.hasSlot(object, \"treatment\")) {\n        if (.hasSlot(object, \"drug\")) {\n            treatment <- object@drug\n        } else if (.hasSlot(object, \"radiation\")) {\n            treatment <- object@radiation\n        } else  {\n            treatment <- data.frame()\n        }\n    } else {\n        treatment <- object@treatment\n    }\n    colnames(treatment) <- gsub(\"drugid\", \"treatmentid\", colnames(treatment))\n\n    if (!.hasSlot(object, \"treatmentResponse\")) {\n        treatmentResponse <- object@sensitivity\n    } else {\n        treatmentResponse <- object@treatmentResponse\n    }\n\n    if (is(treatmentResponse, \"LongTable\")) {\n        treatmentResponse <- updateObject(treatmentResponse)\n        mutableIntern <- mutable(getIntern(treatmentResponse))\n    } else {\n        colnames(treatmentResponse$info) <- gsub(\"cellid\", \"sampleid\",\n            colnames(treatmentResponse$info))\n        colnames(treatmentResponse$info) <- gsub(\"drugid\",\n            \"treatmentid\", colnames(treatmentResponse$info))\n    }\n\n    mProf <- object@molecularProfiles\n    for (i in seq_along(mProf)) {\n        colnames(colData(mProf[[i]])) <- gsub(\"cellid\", \"sampleid\",\n            colnames(colData(mProf[[i]])))\n        colnames(colData(mProf[[i]])) <- gsub(\"drugid\", \"treatmentid\",\n            colnames(colData(mProf[[i]])))\n    }\n    curation_ <- object@curation\n    names(curation_) <- gsub(\"cell\", \"sample\", names(curation_))\n    names(curation_) <- gsub(\"drug\", \"treatment\", names(curation_))\n    colnames(curation_$sample) <- gsub(\"cellid\", \"sampleid\",\n        colnames(curation_$sample))\n    if (\"treatment\" %in% names(curation_)) {\n        colnames(curation_$treatment) <- gsub(\"drugid\",\n            \"treatmentid\", colnames(curation_$treatment))\n    }\n\n    cSet <- .CoreSet(\n        sample=sample_,\n        treatment=treatment,\n        treatmentResponse=treatmentResponse,\n        molecularProfiles=mProf,\n        annotation=object@annotation,\n        curation=curation_,\n        perturbation=object@perturbation,\n        datasetType=object@datasetType\n    )\n\n    if (verify) isValid(cSet)\n\n    return(cSet)\n})\n\n#' Update the `LongTable` class after changes in it struture or API\n#'\n#' @param object A `LongTable` object to update the class structure for.\n#' @param verify A `logical(1)` indicating is `validObject` should be called\n#' after updating the object. Defaults to `TRUE`, only set `FALSE` for debugging.\n#' @param verbose TRUE or FALSE, indicating whether information about the update\n#' should be reported\n#' @return `LongTable` with update class structure.\n#'\n#' @md\n#'\n#' @importMethodsFrom BiocGenerics updateObject\n#' @export\nsetMethod(\"updateObject\", signature(object=\"LongTable\"),\n        function(object, verify=FALSE, verbose = FALSE) {\n\n     if (verbose) {\n        message(\"updateObject object = 'CoreSet'\")\n    }\n    if (is.environment(getIntern(object))) {\n        rData <- rowData(object, key=TRUE)\n        rIDs <- rowIDs(object)\n        cData <- colData(object, key=TRUE)\n        cIDs <- colIDs(object)\n        id_cols <- c(rIDs, cIDs)\n        assays_ <- assays(object, raw=TRUE)\n        setkeyv(rData, \"rowKey\")\n        assays_ <- lapply(assays_, merge.data.table,\n            y=rData[, c(\"rowKey\", rIDs), with=FALSE],\n            by=\"rowKey\"\n        )\n        setkeyv(cData, \"colKey\")\n        assays_ <- lapply(assays_, merge.data.table,\n            y=cData[, c(\"colKey\", cIDs), with=FALSE],\n            by=\"colKey\"\n        )\n        rData[, rowKey := NULL]\n        cData[, colKey := NULL]\n        for (a_ in assays_) a_[, c(\"rowKey\", \"colKey\") := NULL]\n        mdata <- metadata(object)\n        assayMap <- lapply(assays_, function(x, y) y, y=id_cols)\n        oclass <- class(object)[1]\n        object <- LongTable(\n            rowData=rData, rowIDs=rIDs,\n            colData=cData, colIDs=cIDs,\n            assays=assays_, assayIDs=assayMap,\n            metadata=mdata\n        )\n        object <- as(object, oclass)  # Coerce to inherting class if needed\n    }\n    if (verify) isValid(object)\n    return(object)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `updateObject` method for the `CoreSet` class, and how does it handle changes in the object's structure?",
        "answer": "The `updateObject` method for the `CoreSet` class is designed to update the class structure after changes in its structure or API. It handles changes by checking for the existence of certain slots (e.g., 'sample', 'treatment', 'treatmentResponse') and updating them accordingly. It also renames columns to maintain consistency (e.g., changing 'cellid' to 'sampleid' and 'drugid' to 'treatmentid'). The method creates a new `CoreSet` object with the updated structure and returns it."
      },
      {
        "question": "How does the `updateObject` method for the `LongTable` class differ from the one for `CoreSet`, and what specific operations does it perform?",
        "answer": "The `updateObject` method for the `LongTable` class focuses on updating the internal structure of the `LongTable` object. It performs the following operations: 1) Extracts row and column data, including IDs. 2) Merges assay data with row and column information. 3) Removes unnecessary keys. 4) Creates a new `LongTable` object with the updated structure. 5) Coerces the object back to its original class if it was inheriting from `LongTable`. This method is specifically designed to handle changes in the internal representation of `LongTable` objects, particularly when transitioning from an environment-based to a more structured data representation."
      },
      {
        "question": "What are the common parameters shared by both `updateObject` methods, and how do they affect the function's behavior?",
        "answer": "Both `updateObject` methods share three common parameters: 1) `object`: The object to be updated (either `CoreSet` or `LongTable`). 2) `verify`: A logical value indicating whether to call `validObject` (for `CoreSet`) or `isValid` (for `LongTable`) after updating. It defaults to `FALSE`. 3) `verbose`: A logical value indicating whether to report information about the update process. These parameters allow for flexibility in the update process, enabling verification of the updated object and providing optional logging for debugging purposes."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('updateObject', signature(object=\"CoreSet\"),\n        function(object, verify=FALSE, verbose = FALSE) {\n    if (verbose) {\n        message(\"updateObject object = 'CoreSet'\")\n    }\n    \n    if (!.hasSlot(object, \"sample\")) {\n        cell <- object@cell\n        sample_ <- cell\n    } else {\n        sample_ <- object@sample\n    }\n    colnames(sample_) <- gsub(\"cellid\", \"sampleid\", colnames(sample_))\n\n    # TODO: Update treatment and treatmentResponse\n\n    # TODO: Update molecularProfiles and curation\n\n    cSet <- .CoreSet(\n        sample=sample_,\n        # TODO: Add remaining parameters\n    )\n\n    if (verify) isValid(cSet)\n\n    return(cSet)\n})",
        "complete": "setMethod('updateObject', signature(object=\"CoreSet\"),\n        function(object, verify=FALSE, verbose = FALSE) {\n    if (verbose) message(\"updateObject object = 'CoreSet'\")\n    \n    sample_ <- if (.hasSlot(object, \"sample\")) object@sample else object@cell\n    colnames(sample_) <- gsub(\"cellid\", \"sampleid\", colnames(sample_))\n\n    treatment <- if (.hasSlot(object, \"treatment\")) object@treatment else\n                 if (.hasSlot(object, \"drug\")) object@drug else\n                 if (.hasSlot(object, \"radiation\")) object@radiation else data.frame()\n    colnames(treatment) <- gsub(\"drugid\", \"treatmentid\", colnames(treatment))\n\n    treatmentResponse <- if (.hasSlot(object, \"treatmentResponse\")) object@treatmentResponse else object@sensitivity\n    if (is(treatmentResponse, \"LongTable\")) {\n        treatmentResponse <- updateObject(treatmentResponse)\n    } else {\n        colnames(treatmentResponse$info) <- gsub(\"cellid|drugid\", c(\"sampleid\", \"treatmentid\"), colnames(treatmentResponse$info))\n    }\n\n    mProf <- object@molecularProfiles\n    for (i in seq_along(mProf)) {\n        colnames(colData(mProf[[i]])) <- gsub(\"cellid|drugid\", c(\"sampleid\", \"treatmentid\"), colnames(colData(mProf[[i]])))\n    }\n\n    curation_ <- object@curation\n    names(curation_) <- gsub(\"cell|drug\", c(\"sample\", \"treatment\"), names(curation_))\n    colnames(curation_$sample) <- gsub(\"cellid\", \"sampleid\", colnames(curation_$sample))\n    if (\"treatment\" %in% names(curation_)) {\n        colnames(curation_$treatment) <- gsub(\"drugid\", \"treatmentid\", colnames(curation_$treatment))\n    }\n\n    cSet <- .CoreSet(\n        sample=sample_,\n        treatment=treatment,\n        treatmentResponse=treatmentResponse,\n        molecularProfiles=mProf,\n        annotation=object@annotation,\n        curation=curation_,\n        perturbation=object@perturbation,\n        datasetType=object@datasetType\n    )\n\n    if (verify) isValid(cSet)\n    return(cSet)\n})"
      },
      {
        "partial": "setMethod(\"updateObject\", signature(object=\"LongTable\"),\n        function(object, verify=FALSE, verbose = FALSE) {\n    if (verbose) message(\"updateObject object = 'CoreSet'\")\n    \n    if (is.environment(getIntern(object))) {\n        rData <- rowData(object, key=TRUE)\n        rIDs <- rowIDs(object)\n        cData <- colData(object, key=TRUE)\n        cIDs <- colIDs(object)\n        id_cols <- c(rIDs, cIDs)\n        assays_ <- assays(object, raw=TRUE)\n        \n        # TODO: Update assays_, rData, and cData\n        \n        # TODO: Create new LongTable object\n        \n        # TODO: Coerce to inheriting class if needed\n    }\n    if (verify) isValid(object)\n    return(object)\n})",
        "complete": "setMethod(\"updateObject\", signature(object=\"LongTable\"),\n        function(object, verify=FALSE, verbose = FALSE) {\n    if (verbose) message(\"updateObject object = 'CoreSet'\")\n    \n    if (is.environment(getIntern(object))) {\n        rData <- rowData(object, key=TRUE)\n        rIDs <- rowIDs(object)\n        cData <- colData(object, key=TRUE)\n        cIDs <- colIDs(object)\n        id_cols <- c(rIDs, cIDs)\n        assays_ <- assays(object, raw=TRUE)\n        \n        setkeyv(rData, \"rowKey\")\n        assays_ <- lapply(assays_, merge.data.table,\n            y=rData[, c(\"rowKey\", rIDs), with=FALSE],\n            by=\"rowKey\"\n        )\n        setkeyv(cData, \"colKey\")\n        assays_ <- lapply(assays_, merge.data.table,\n            y=cData[, c(\"colKey\", cIDs), with=FALSE],\n            by=\"colKey\"\n        )\n        rData[, rowKey := NULL]\n        cData[, colKey := NULL]\n        for (a_ in assays_) a_[, c(\"rowKey\", \"colKey\") := NULL]\n        \n        mdata <- metadata(object)\n        assayMap <- lapply(assays_, function(x, y) y, y=id_cols)\n        oclass <- class(object)[1]\n        object <- LongTable(\n            rowData=rData, rowIDs=rIDs,\n            colData=cData, colIDs=cIDs,\n            assays=assays_, assayIDs=assayMap,\n            metadata=mdata\n        )\n        object <- as(object, oclass)\n    }\n    if (verify) isValid(object)\n    return(object)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-subsetTo.R",
    "language": "R",
    "content": "#' Subset a CoreSet object based on various parameters, such as cell lines, molecular features\n#'\n#' @param object An object inheriting from the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A subsetted version of the original `object`\n#'\n#' @examples\n#' \"Generics shouldn't need examples!\"\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"subsetTo\", function(object, ...) standardGeneric(\"subsetTo\"))\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setGeneric` function in this code snippet, and how does it relate to object-oriented programming in R?",
        "answer": "The `setGeneric` function is used to define a new generic function in R's S4 object-oriented programming system. It creates a function that can have multiple methods for different classes. In this case, it defines a generic function called 'subsetTo' that can be implemented differently for various classes inheriting from the CoreSet class. This allows for polymorphic behavior, where the same function name can have different implementations depending on the type of object it's called on."
      },
      {
        "question": "What is the significance of the `standardGeneric` function used within the generic function definition?",
        "answer": "The `standardGeneric` function is a crucial part of defining a generic function in R's S4 system. It serves as a placeholder for the actual method dispatch mechanism. When the generic function is called, `standardGeneric` ensures that the appropriate method is selected and executed based on the class of the arguments passed to the function. This allows for the implementation of polymorphism, where the behavior of the function can vary depending on the types of objects it's called with."
      },
      {
        "question": "Why does the Roxygen comment for this function include the line '@keywords internal', and what effect does this have on the documentation?",
        "answer": "The '@keywords internal' Roxygen tag is used to mark functions that are intended for internal use within a package and not part of the public API. When generating documentation, functions marked as internal are typically excluded from the main package documentation and are not prominently displayed to users. This helps to keep the public-facing documentation clean and focused on the functions that package users are expected to interact with directly. However, the function is still exported (as indicated by the '@export' tag), so it can be used by other packages or advanced users if necessary."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Subset a CoreSet object based on various parameters, such as cell lines, molecular features\n#'\n#' @param object An object inheriting from the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A subsetted version of the original `object`\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"subsetTo\", function(object, ...) {\n    # Complete the function body\n})",
        "complete": "#' Subset a CoreSet object based on various parameters, such as cell lines, molecular features\n#'\n#' @param object An object inheriting from the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A subsetted version of the original `object`\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"subsetTo\", function(object, ...) standardGeneric(\"subsetTo\"))"
      },
      {
        "partial": "#' Subset a CoreSet object based on various parameters, such as cell lines, molecular features\n#'\n#' @param object An object inheriting from the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A subsetted version of the original `object`\n#'\n# Complete the roxygen documentation and function definition\n",
        "complete": "#' Subset a CoreSet object based on various parameters, such as cell lines, molecular features\n#'\n#' @param object An object inheriting from the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A subsetted version of the original `object`\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"subsetTo\", function(object, ...) standardGeneric(\"subsetTo\"))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/src/Data.cpp",
    "language": "cpp",
    "content": "#include \"Data.h\"\n#include <sys/time.h>\n\nData::Data(double* const pData, Matrix const* const pPriorsMatrix, double const priorsWeight,\n        unsigned int const sampleCount, unsigned int const featureCount,\n        int const* const pSampleStrata, double const* const pSampleWeights,\n        int const* const pFeatureTypes, unsigned int const sampleStratumCount,\n        unsigned int const continuousEstimator, bool const outX, unsigned int const bootstrapCount) :\n        mpDataMatrix(new Matrix(pData, sampleCount, featureCount)), mpOrderMatrix(\n                continuousEstimator ? new Matrix(sampleCount, featureCount) : 0), mpPriorsMatrix(\n                pPriorsMatrix), mpHasOrderCached(new bool[mpDataMatrix->getColumnCount()]), mpSampleStrata(\n                pSampleStrata), mpSampleWeights(pSampleWeights), mpFeatureTypes(pFeatureTypes), mSampleStratumCount(\n                sampleStratumCount), mpSampleIndicesPerStratum(\n                new unsigned int*[sampleStratumCount]), mpMasterSampleIndicesPerStratum(\n                new unsigned int*[sampleStratumCount]), mpSampleCountPerStratum(\n                new unsigned int[sampleStratumCount]), mContinuousEstimator(continuousEstimator), mOutX(\n                outX), mBootstrapCount(bootstrapCount), mPriorsWeight(priorsWeight)\n{\n    for (unsigned int i = 0; i < mpDataMatrix->getColumnCount(); ++i)\n        mpHasOrderCached[i] = false;\n\n    Math::placeStratificationData(mpSampleStrata, mpSampleWeights, mpSampleIndicesPerStratum,\n            mpSampleCountPerStratum, mSampleStratumCount, sampleCount);\n\n    for (unsigned int i = 0; i < mSampleStratumCount; ++i)\n    {\n        mpMasterSampleIndicesPerStratum[i] = new unsigned int[mpSampleCountPerStratum[i]];\n        for (unsigned int j = 0; j < mpSampleCountPerStratum[i]; ++j)\n            mpMasterSampleIndicesPerStratum[i][j] = mpSampleIndicesPerStratum[i][j];\n    }\n}\n\nData::~Data()\n{\n    delete mpDataMatrix;\n    delete mpOrderMatrix;\n    delete[] mpHasOrderCached;\n    for (unsigned int i = 0; i < mSampleStratumCount; ++i)\n    {\n        delete[] mpSampleIndicesPerStratum[i];\n        delete[] mpMasterSampleIndicesPerStratum[i];\n    }\n    delete[] mpSampleIndicesPerStratum;\n    delete[] mpMasterSampleIndicesPerStratum;\n    delete[] mpSampleCountPerStratum;\n}\n\nvoid const\nData::bootstrap()\n{\n    // unsigned int seed = std::time(NULL); too long for small datasets\n    struct timeval start;\n    gettimeofday(&start, NULL);\n    unsigned int seed = start.tv_usec; //microseconds\n    \n    for (unsigned int i = 0; i < mSampleStratumCount; ++i)\n        for (unsigned int j = 0; j < mpSampleCountPerStratum[i]; ++j)\n        {\n            unsigned int index = Math::computeRandomNumber(&seed) % mpSampleCountPerStratum[i];\n            mpSampleIndicesPerStratum[i][j] = mpMasterSampleIndicesPerStratum[i][index];\n        }\n}\n\nvoid const\nData::computeMiBetweenFeatures(unsigned int const i, unsigned int const j, double* const mi_ij,\n        double* const mi_ji) const\n{\n    double val_ij = std::numeric_limits<double>::quiet_NaN();\n    double val_ji = std::numeric_limits<double>::quiet_NaN();\n\n    bool const A_is_continuous = mpFeatureTypes[i] == FEATURE_CONTINUOUS;\n    bool const A_is_discrete = mpFeatureTypes[i] == FEATURE_DISCRETE;\n    bool const A_is_survival_event = mpFeatureTypes[i] == FEATURE_SURVIVAL_EVENT;\n\n    bool const B_is_continuous = mpFeatureTypes[j] == FEATURE_CONTINUOUS;\n    bool const B_is_discrete = mpFeatureTypes[j] == FEATURE_DISCRETE;\n    bool const B_is_survival_event = mpFeatureTypes[j] == FEATURE_SURVIVAL_EVENT;\n\n    if (A_is_continuous && B_is_continuous)\n    {\n        switch (mContinuousEstimator)\n        {\n        case PEARSON_ESTIMATOR:\n        {\n            val_ij = val_ji = Math::computePearsonCorrelation(&(mpDataMatrix->at(0, i)),\n                    &(mpDataMatrix->at(0, j)), mpSampleWeights, mpSampleIndicesPerStratum,\n                    mpSampleCountPerStratum, mSampleStratumCount, mBootstrapCount);\n        }\n            break;\n\n        case SPEARMAN_ESTIMATOR:\n        {\n            if (!mpHasOrderCached[i])\n            {\n                Math::placeOrders(&(mpDataMatrix->at(0, i)), &(mpOrderMatrix->at(0, i)),\n                        mpSampleIndicesPerStratum, mpSampleCountPerStratum, mSampleStratumCount);\n                mpHasOrderCached[i] = true;\n            }\n\n            if (!mpHasOrderCached[j])\n            {\n                Math::placeOrders(&(mpDataMatrix->at(0, j)), &(mpOrderMatrix->at(0, j)),\n                        mpSampleIndicesPerStratum, mpSampleCountPerStratum, mSampleStratumCount);\n                mpHasOrderCached[j] = true;\n            }\n\n            double* const p_ranked_samples_x = new double[getSampleCount()];\n            double* const p_ranked_samples_y = new double[getSampleCount()];\n            Math::placeRanksFromOrders(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                    &(mpOrderMatrix->at(0, i)), &(mpOrderMatrix->at(0, j)), p_ranked_samples_x,\n                    p_ranked_samples_y, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                    mSampleStratumCount);\n            val_ij = val_ji = Math::computePearsonCorrelation(p_ranked_samples_x,\n                    p_ranked_samples_y, mpSampleWeights, mpSampleIndicesPerStratum,\n                    mpSampleCountPerStratum, mSampleStratumCount, mBootstrapCount);\n            delete[] p_ranked_samples_x;\n            delete[] p_ranked_samples_y;\n        }\n            break;\n\n        case KENDALL_ESTIMATOR:\n        {\n            val_ij =  Math::computeSomersD(\n              Math::computeConcordanceIndex(&(mpDataMatrix->at(0, i)),\n                                            &(mpDataMatrix->at(0, j)), mpSampleWeights, mpSampleIndicesPerStratum,\n                                            mpSampleCountPerStratum, mSampleStratumCount, mOutX));\n            val_ji = Math::computeSomersD(\n              Math::computeConcordanceIndex(&(mpDataMatrix->at(0, j)),\n                                            &(mpDataMatrix->at(0, i)), mpSampleWeights, mpSampleIndicesPerStratum,\n                                            mpSampleCountPerStratum, mSampleStratumCount, mOutX));\n        }\n            break;\n\n        case FREQUENCY_ESTIMATOR:\n        {\n            val_ij = Math::computeFrequency(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                    mpSampleWeights, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                    mSampleStratumCount, mBootstrapCount);\n            val_ji = 1 - val_ij;\n        }\n            break;\n        }\n    }\n    else if (A_is_discrete && B_is_continuous) // Not symmetrical\n        val_ij = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                        mpSampleWeights, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                        mSampleStratumCount, mOutX));\n    else if (A_is_continuous && B_is_discrete) // Not symmetrical\n        val_ij = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, j)), &(mpDataMatrix->at(0, i)),\n                        mpSampleWeights, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                        mSampleStratumCount, mOutX));\n    else if (A_is_discrete && B_is_discrete)\n        val_ij = val_ji = Math::computeCramersV(&(mpDataMatrix->at(0, i)),\n                &(mpDataMatrix->at(0, j)), mpSampleWeights, mpSampleIndicesPerStratum,\n                mpSampleCountPerStratum, mSampleStratumCount, mBootstrapCount);\n    else if (A_is_survival_event && B_is_continuous)\n        val_ij = val_ji = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                        &(mpDataMatrix->at(0, i + 1)), mpSampleWeights, mpSampleIndicesPerStratum,\n                        mpSampleCountPerStratum, mSampleStratumCount, mOutX));\n    else if (A_is_continuous && B_is_survival_event)\n        val_ij = val_ji = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, j)), &(mpDataMatrix->at(0, i)),\n                        &(mpDataMatrix->at(0, j + 1)), mpSampleWeights, mpSampleIndicesPerStratum,\n                        mpSampleCountPerStratum, mSampleStratumCount, mOutX));\n    else if (A_is_survival_event && B_is_discrete)\n        val_ij = val_ji = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                        &(mpDataMatrix->at(0, i + 1)), mpSampleWeights, mpSampleIndicesPerStratum,\n                        mpSampleCountPerStratum, mSampleStratumCount, mOutX));\n    else if (A_is_discrete && B_is_survival_event)\n        val_ij = val_ji = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, j)), &(mpDataMatrix->at(0, i)),\n                        &(mpDataMatrix->at(0, j + 1)), mpSampleWeights, mpSampleIndicesPerStratum,\n                        mpSampleCountPerStratum, mSampleStratumCount, mOutX));\n    else if (A_is_survival_event && B_is_survival_event) // Not symmetrical for some reason\n        val_ij = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                        &(mpDataMatrix->at(0, i + 1)), &(mpDataMatrix->at(0, j + 1)),\n                        mpSampleWeights, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                        mSampleStratumCount, mOutX));\n\n    if (mpPriorsMatrix != 0)\n    {\n        val_ij = (std::fabs(1.0 - mPriorsWeight) * val_ij)\n                \t+ (mPriorsWeight * mpPriorsMatrix->at(i, j));\n\n        val_ji = (std::fabs(1.0 - mPriorsWeight) * val_ji)\n                \t+ (mPriorsWeight * mpPriorsMatrix->at(j, i));\n    }\n\n    if (val_ij == val_ij)\n        *mi_ij = val_ij;\n\n    if (val_ji == val_ji)\n        *mi_ji = val_ji;\n}\n\nunsigned int const\nData::getSampleCount() const\n{\n    return mpDataMatrix->getRowCount();\n}\n\nunsigned int const\nData::getFeatureCount() const\n{\n    return mpDataMatrix->getColumnCount();\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `bootstrap()` method in the `Data` class, and how does it generate random numbers?",
        "answer": "The `bootstrap()` method is used to create a bootstrapped sample of the data. It generates random numbers using the microseconds from the current time as a seed. For each stratum, it randomly selects indices from the master sample indices and assigns them to the current sample indices. This creates a new random sample of the data while maintaining the original stratification."
      },
      {
        "question": "How does the `computeMiBetweenFeatures()` method handle different combinations of feature types (continuous, discrete, and survival event)?",
        "answer": "The `computeMiBetweenFeatures()` method uses different algorithms to compute mutual information (MI) based on the types of features being compared. For continuous-continuous pairs, it uses the specified continuous estimator (Pearson, Spearman, Kendall, or Frequency). For discrete-discrete pairs, it uses Cramer's V. For combinations involving survival events, it uses Somers' D with concordance index. The method also handles asymmetric cases, such as discrete-continuous pairs, differently in each direction."
      },
      {
        "question": "What is the purpose of the `mpHasOrderCached` array in the `Data` class, and how is it used in the `computeMiBetweenFeatures()` method?",
        "answer": "The `mpHasOrderCached` array is used to keep track of which features have their orders cached in the `mpOrderMatrix`. In the `computeMiBetweenFeatures()` method, when using the Spearman estimator for continuous-continuous pairs, it checks if the orders for each feature have been cached. If not, it computes and caches the orders. This optimization prevents redundant calculations of feature orders across multiple calls to the method."
      }
    ],
    "completion_tasks": [
      {
        "partial": "Data::Data(double* const pData, Matrix const* const pPriorsMatrix, double const priorsWeight,\n        unsigned int const sampleCount, unsigned int const featureCount,\n        int const* const pSampleStrata, double const* const pSampleWeights,\n        int const* const pFeatureTypes, unsigned int const sampleStratumCount,\n        unsigned int const continuousEstimator, bool const outX, unsigned int const bootstrapCount) :\n        mpDataMatrix(new Matrix(pData, sampleCount, featureCount)),\n        mpOrderMatrix(continuousEstimator ? new Matrix(sampleCount, featureCount) : 0),\n        mpPriorsMatrix(pPriorsMatrix),\n        mpHasOrderCached(new bool[mpDataMatrix->getColumnCount()]),\n        mpSampleStrata(pSampleStrata),\n        mpSampleWeights(pSampleWeights),\n        mpFeatureTypes(pFeatureTypes),\n        mSampleStratumCount(sampleStratumCount),\n        mpSampleIndicesPerStratum(new unsigned int*[sampleStratumCount]),\n        mpMasterSampleIndicesPerStratum(new unsigned int*[sampleStratumCount]),\n        mpSampleCountPerStratum(new unsigned int[sampleStratumCount]),\n        mContinuousEstimator(continuousEstimator),\n        mOutX(outX),\n        mBootstrapCount(bootstrapCount),\n        mPriorsWeight(priorsWeight)\n{\n    // Initialize mpHasOrderCached\n    // TODO: Complete the initialization of other data structures\n}",
        "complete": "Data::Data(double* const pData, Matrix const* const pPriorsMatrix, double const priorsWeight,\n        unsigned int const sampleCount, unsigned int const featureCount,\n        int const* const pSampleStrata, double const* const pSampleWeights,\n        int const* const pFeatureTypes, unsigned int const sampleStratumCount,\n        unsigned int const continuousEstimator, bool const outX, unsigned int const bootstrapCount) :\n        mpDataMatrix(new Matrix(pData, sampleCount, featureCount)),\n        mpOrderMatrix(continuousEstimator ? new Matrix(sampleCount, featureCount) : 0),\n        mpPriorsMatrix(pPriorsMatrix),\n        mpHasOrderCached(new bool[mpDataMatrix->getColumnCount()]),\n        mpSampleStrata(pSampleStrata),\n        mpSampleWeights(pSampleWeights),\n        mpFeatureTypes(pFeatureTypes),\n        mSampleStratumCount(sampleStratumCount),\n        mpSampleIndicesPerStratum(new unsigned int*[sampleStratumCount]),\n        mpMasterSampleIndicesPerStratum(new unsigned int*[sampleStratumCount]),\n        mpSampleCountPerStratum(new unsigned int[sampleStratumCount]),\n        mContinuousEstimator(continuousEstimator),\n        mOutX(outX),\n        mBootstrapCount(bootstrapCount),\n        mPriorsWeight(priorsWeight)\n{\n    std::fill(mpHasOrderCached, mpHasOrderCached + mpDataMatrix->getColumnCount(), false);\n\n    Math::placeStratificationData(mpSampleStrata, mpSampleWeights, mpSampleIndicesPerStratum,\n            mpSampleCountPerStratum, mSampleStratumCount, sampleCount);\n\n    for (unsigned int i = 0; i < mSampleStratumCount; ++i)\n    {\n        mpMasterSampleIndicesPerStratum[i] = new unsigned int[mpSampleCountPerStratum[i]];\n        std::copy(mpSampleIndicesPerStratum[i], mpSampleIndicesPerStratum[i] + mpSampleCountPerStratum[i],\n                  mpMasterSampleIndicesPerStratum[i]);\n    }\n}"
      },
      {
        "partial": "void const\nData::computeMiBetweenFeatures(unsigned int const i, unsigned int const j, double* const mi_ij,\n        double* const mi_ji) const\n{\n    double val_ij = std::numeric_limits<double>::quiet_NaN();\n    double val_ji = std::numeric_limits<double>::quiet_NaN();\n\n    bool const A_is_continuous = mpFeatureTypes[i] == FEATURE_CONTINUOUS;\n    bool const A_is_discrete = mpFeatureTypes[i] == FEATURE_DISCRETE;\n    bool const A_is_survival_event = mpFeatureTypes[i] == FEATURE_SURVIVAL_EVENT;\n\n    bool const B_is_continuous = mpFeatureTypes[j] == FEATURE_CONTINUOUS;\n    bool const B_is_discrete = mpFeatureTypes[j] == FEATURE_DISCRETE;\n    bool const B_is_survival_event = mpFeatureTypes[j] == FEATURE_SURVIVAL_EVENT;\n\n    // TODO: Implement the logic for different feature type combinations\n\n    // Apply priors if available\n    if (mpPriorsMatrix != 0)\n    {\n        // TODO: Apply priors to val_ij and val_ji\n    }\n\n    if (val_ij == val_ij)\n        *mi_ij = val_ij;\n\n    if (val_ji == val_ji)\n        *mi_ji = val_ji;\n}",
        "complete": "void const\nData::computeMiBetweenFeatures(unsigned int const i, unsigned int const j, double* const mi_ij,\n        double* const mi_ji) const\n{\n    double val_ij = std::numeric_limits<double>::quiet_NaN();\n    double val_ji = std::numeric_limits<double>::quiet_NaN();\n\n    bool const A_is_continuous = mpFeatureTypes[i] == FEATURE_CONTINUOUS;\n    bool const A_is_discrete = mpFeatureTypes[i] == FEATURE_DISCRETE;\n    bool const A_is_survival_event = mpFeatureTypes[i] == FEATURE_SURVIVAL_EVENT;\n\n    bool const B_is_continuous = mpFeatureTypes[j] == FEATURE_CONTINUOUS;\n    bool const B_is_discrete = mpFeatureTypes[j] == FEATURE_DISCRETE;\n    bool const B_is_survival_event = mpFeatureTypes[j] == FEATURE_SURVIVAL_EVENT;\n\n    if (A_is_continuous && B_is_continuous)\n    {\n        switch (mContinuousEstimator)\n        {\n        case PEARSON_ESTIMATOR:\n            val_ij = val_ji = Math::computePearsonCorrelation(&(mpDataMatrix->at(0, i)),\n                    &(mpDataMatrix->at(0, j)), mpSampleWeights, mpSampleIndicesPerStratum,\n                    mpSampleCountPerStratum, mSampleStratumCount, mBootstrapCount);\n            break;\n        case SPEARMAN_ESTIMATOR:\n            // Implement Spearman estimator\n            break;\n        case KENDALL_ESTIMATOR:\n            // Implement Kendall estimator\n            break;\n        case FREQUENCY_ESTIMATOR:\n            // Implement Frequency estimator\n            break;\n        }\n    }\n    else if (A_is_discrete && B_is_continuous)\n        val_ij = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, i)), &(mpDataMatrix->at(0, j)),\n                        mpSampleWeights, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                        mSampleStratumCount, mOutX));\n    else if (A_is_continuous && B_is_discrete)\n        val_ij = Math::computeSomersD(\n                Math::computeConcordanceIndex(&(mpDataMatrix->at(0, j)), &(mpDataMatrix->at(0, i)),\n                        mpSampleWeights, mpSampleIndicesPerStratum, mpSampleCountPerStratum,\n                        mSampleStratumCount, mOutX));\n    else if (A_is_discrete && B_is_discrete)\n        val_ij = val_ji = Math::computeCramersV(&(mpDataMatrix->at(0, i)),\n                &(mpDataMatrix->at(0, j)), mpSampleWeights, mpSampleIndicesPerStratum,\n                mpSampleCountPerStratum, mSampleStratumCount, mBootstrapCount);\n    // Implement other combinations (survival event)\n\n    if (mpPriorsMatrix != 0)\n    {\n        val_ij = (std::fabs(1.0 - mPriorsWeight) * val_ij) + (mPriorsWeight * mpPriorsMatrix->at(i, j));\n        val_ji = (std::fabs(1.0 - mPriorsWeight) * val_ji) + (mPriorsWeight * mpPriorsMatrix->at(j, i));\n    }\n\n    if (val_ij == val_ij)\n        *mi_ij = val_ij;\n\n    if (val_ji == val_ji)\n        *mi_ji = val_ji;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/R/generics.R",
    "language": "R",
    "content": "setGeneric(\"featureData\", function(object) standardGeneric(\"featureData\"))\n\nsetGeneric(\"subsetData\", function(object, ...) standardGeneric(\"subsetData\"))\n\nsetGeneric(\"sampleNames\", function(object) standardGeneric(\"sampleNames\"))\n\nsetGeneric(\"sampleCount\", function(object) standardGeneric(\"sampleCount\"))\n\nsetGeneric(\"featureCount\", function(object) standardGeneric(\"featureCount\"))\n\nsetGeneric(\"featureNames\", function(object) standardGeneric(\"featureNames\"))\n\nsetGeneric(\"sampleStrata\", function(object) standardGeneric(\"sampleStrata\"))\n\nsetGeneric(\"sampleStrata<-\", function(object, value) standardGeneric(\"sampleStrata<-\"))\n\nsetGeneric(\"sampleWeights\", function(object) standardGeneric(\"sampleWeights\"))\n\nsetGeneric(\"sampleWeights<-\", function(object, value) standardGeneric(\"sampleWeights<-\"))\n\nsetGeneric(\"priors\", function(object) standardGeneric(\"priors\"))\n\nsetGeneric(\"priors<-\", function(object, value) standardGeneric(\"priors<-\"))\n\nsetGeneric(\"mim\", function(object, method = c(\"mi\", \"cor\"), ...)\n{\n    method <- match.arg(method)\n    matrix <- standardGeneric(\"mim\")\n    \n    if (method == \"mi\")\n        matrix <- -.5 * log(1 - (matrix^2))\n    \n    return(matrix)\n})\n\nsetGeneric(\".expandFeatureMatrix\", function(object, ...) standardGeneric(\".expandFeatureMatrix\"))\n\nsetGeneric(\".compressFeatureMatrix\", function(object, ...) standardGeneric(\".compressFeatureMatrix\"))\n\nsetGeneric(\".expandFeatureIndices\", function(object, ...) standardGeneric(\".expandFeatureIndices\"))\n\nsetGeneric(\".compressFeatureIndices\", function(object, ...) standardGeneric(\".compressFeatureIndices\"))\n\nsetGeneric(\"solutions\", function(object, ...) standardGeneric(\"solutions\"))\n\nsetGeneric(\"scores\", function(object, ...) standardGeneric(\"scores\"))\n\nsetGeneric(\"causality\", function(object, ...) standardGeneric(\"causality\"))\n\nsetGeneric(\"target\", function(object) standardGeneric(\"target\"))\n\nsetGeneric(\"adjacencyMatrix\", function(object) standardGeneric(\"adjacencyMatrix\"))\n\nsetGeneric(\"adjacencyMatrixSum\", function(object) standardGeneric(\"adjacencyMatrixSum\"))\n\nsetGeneric(\"visualize\", function(object) standardGeneric(\"visualize\"))\n\n`.map.continuous.estimator` <- function(continuous_estimator)\n{\n    value <- switch(continuous_estimator, \"pearson\" = 0L, \"spearman\" = 1L, \"kendall\" = 2L, \"frequency\" = 3L, -1L)\n    \n    if (value < 0L || value > 4L || !is.character(continuous_estimator))\n        stop(\"estimator must be of the following: pearson, spearman, kendall, frequency\")\n    \n    return(value)\n}\n\n`correlate` <- function(X, Y, method = c(\"pearson\", \"spearman\", \"kendall\", \"frequency\", \"cramersv\", \"cindex\"), strata, weights, outX = TRUE, bootstrap_count = 0, alpha = 0.05, alternative=c(\"two.sided\", \"less\", \"greater\"))\n{\n    method <- match.arg(method)\n    alternative <- match.arg(alternative)\n    \n    if((is.Surv(X) || is.Surv(Y)) && method != \"cindex\") { stop(\"method should be cindex when dealing with survival data\") }\n    \n    if (method == \"pearson\" || method == \"spearman\" || method == \"kendall\" || method == \"frequency\")\n    {\n        X <- as.numeric(X)\n        Y <- as.numeric(Y)\n    }\n    else if (method == \"cramersv\")\n    {\n        X <- as.factor(X)\n        Y <- as.factor(Y)\n    }\n    else if (method != \"cindex\")\n        stop(\"estimator must be of the following: pearson, spearman, kendall, frequency, cramersv, cindex\")\n    \n    if(is.Surv(X)) { ll <- nrow(X) } else { ll <- length(X) }\n\n    if (missing(strata)) {\n      strata <- factor(rep(0, ll))\n      names(strata) <- names(X)\n    }\n    \n    if (missing(weights)) {\n      weights <- rep(1, ll)\n      names(weights) <- names(X)\n    } \n    \n    data <- mRMR.data(data = data.frame(X, Y), strata = strata, weights = weights)\n\n    if (method == \"cindex\")\n    {\n        empty <- vector(mode = \"numeric\", length = 0)\n        \n        if (length(data@feature_types) == 2)\n            input <- list(data@data[, 1], data@data[, 2], empty, empty)\n        else if (length(data@feature_types) == 3)\n        {\n            if (data@feature_types[[1]] == 2)\n                input <- list(data@data[, 1], data@data[, 3], data@data[, 2], empty)\n            else if (data@feature_types[[2]] == 2)\n                input <- list(data@data[, 2], data@data[, 1], data@data[, 3], empty)\n        }\n        else if (length(data@feature_types) == 4)\n            input <- list(data@data[, 1], data@data[, 3], data@data[, 2], data@data[, 4])\n        \n\t\t\t\tratio <- vector(mode = \"numeric\", length = 1)\n\t\t\t\tch <- vector(mode = \"numeric\", length = length(input[[1]]))\n\t\t\t\tdh <- vector(mode = \"numeric\", length = length(input[[1]]))\n\t\t\t\tuh <- vector(mode = \"numeric\", length = length(input[[1]]))\n\t\t\t\trh <- vector(mode = \"numeric\", length = length(input[[1]]))\n\n        .Call(.C_export_concordance_index, as.numeric(input[[1]]), as.numeric(input[[2]]),\n                as.numeric(input[[3]]), as.numeric(input[[4]]), as.integer(data@strata), as.numeric(data@weights),\n                as.integer(length(unique(data@strata))), outX, ratio, ch, dh, uh, rh)  \n        \n              cindex <- ratio\n              myx <- complete.cases(featureData(data), sampleStrata(data), sampleWeights(data))\n              N <- sum(weights[myx])\n              \n              cscount <- sum(ch + dh) ## comparable pairs\n              if (sum(ch) == 0 || sum (dh) ==0 || sum(ch * (ch - 1)) == 0 || sum(dh * (dh - 1)) == 0 || sum(ch * dh) == 0 || cscount < 10)\n                return(list(\"cindex\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p\"=NA, \"n\"=N))\n              \n              ## FIXME: N and subsequent calculation should be done withing strata\n              pc <- (1 / (N * (N - 1))) * sum(ch)\n              pd  <- (1 / (N * (N - 1))) * sum(dh)\n              pcc <- (1 / (N * (N - 1) * (N - 2))) * sum(ch * (ch - 1))\n              pdd <- (1 / (N * (N - 1) * (N - 2))) * sum(dh * (dh - 1))\n              pcd <- (1 / (N * (N - 1) * (N - 2))) * sum(ch * dh)\n              varp <- (4 / (pc + pd)^4) * (pd^2 * pcc - 2 * pc * pd * pcd + pc^2 * pdd)\n              if((varp / N) > 0) {\n                se <- sqrt(varp / N)\n                ci <- qnorm(p=alpha / 2, lower.tail=FALSE) * se\n                lower <- cindex - ci\n                upper <- cindex + ci\n                switch(alternative, \n                \"two.sided\"={ p <- pnorm((cindex - 0.5) / se, lower.tail=cindex < 0.5) * 2 }, \n                \"less\"={ p <- pnorm((cindex - 0.5) / se, lower.tail=TRUE) }, \n                \"greater\"={  p <- pnorm((cindex - 0.5) / se, lower.tail=FALSE) }\n                )\n              } else { se <- lower <- upper <- p <- NA } \n        \n        return(list(\"estimate\"=cindex, \"se\"=se, \"lower\"=lower, \"upper\"=upper, \"p\"=p, \"n\"=N))\n    }\n    else if (method == \"cramersv\")\n        return(list(statistic = mim(data, method = \"cor\", outX = outX, bootstrap_count = bootstrap_count)[1, 2]))\n    else\n        return(list(statistic = mim(data, method = \"cor\", continuous_estimator = method, outX = outX,\n                            bootstrap_count = bootstrap_count)[1, 2]))\n}\n\n`get.thread.count` <- function()\n{\n    thread_count <- vector(mode = \"integer\", length = 1)\n    \n    .Call(.C_get_thread_count, thread_count)\n    \n    return(thread_count)\n}\n\n`set.thread.count` <- function(thread_count)\n{\n    thread_count <- as.integer(thread_count)\n    \n    .Call(.C_set_thread_count, thread_count)\n    \n    return(thread_count)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setGeneric` function calls at the beginning of the code snippet?",
        "answer": "The `setGeneric` function calls are used to define generic functions in R's S4 object-oriented programming system. These calls create a standardized interface for methods that can be implemented differently for various classes. They allow for method dispatch based on the class of the arguments, enabling polymorphism in R."
      },
      {
        "question": "Explain the purpose and functionality of the `mim` generic function in this code.",
        "answer": "The `mim` generic function is designed to compute a matrix, likely a mutual information matrix or correlation matrix. It takes an object, a method ('mi' or 'cor'), and additional arguments. If the method is 'mi', it transforms the resulting matrix using the formula -.5 * log(1 - (matrix^2)), which converts correlation coefficients to mutual information values. This function allows for flexible computation of information-theoretic or correlation-based matrices."
      },
      {
        "question": "What is the purpose of the `correlate` function, and how does it handle different correlation methods?",
        "answer": "The `correlate` function is a versatile correlation calculator that supports multiple methods: Pearson, Spearman, Kendall, frequency, Cramer's V, and concordance index (c-index). It handles different data types (numeric, categorical, survival) and can incorporate stratification and weights. The function uses method dispatch to call the appropriate correlation calculation based on the specified method and data types, returning the correlation statistic along with additional information like standard error and p-value for some methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setGeneric(\"mim\", function(object, method = c(\"mi\", \"cor\"), ...)\n{\n    method <- match.arg(method)\n    matrix <- standardGeneric(\"mim\")\n    \n    if (method == \"mi\")\n        matrix <- -.5 * log(1 - (matrix^2))\n    \n    return(matrix)\n})",
        "complete": "setGeneric(\"mim\", function(object, method = c(\"mi\", \"cor\"), ...)\n{\n    method <- match.arg(method)\n    matrix <- standardGeneric(\"mim\")\n    \n    if (method == \"mi\")\n        matrix <- -.5 * log(1 - (matrix^2))\n    \n    return(matrix)\n})"
      },
      {
        "partial": "`.map.continuous.estimator` <- function(continuous_estimator)\n{\n    value <- switch(continuous_estimator,\n                    \"pearson\" = 0L,\n                    \"spearman\" = 1L,\n                    \"kendall\" = 2L,\n                    \"frequency\" = 3L,\n                    -1L)\n    \n    if (value < 0L || value > 4L || !is.character(continuous_estimator))\n        stop(\"estimator must be one of the following: pearson, spearman, kendall, frequency\")\n    \n    return(value)\n}",
        "complete": "`.map.continuous.estimator` <- function(continuous_estimator)\n{\n    value <- switch(continuous_estimator,\n                    \"pearson\" = 0L,\n                    \"spearman\" = 1L,\n                    \"kendall\" = 2L,\n                    \"frequency\" = 3L,\n                    -1L)\n    \n    if (value < 0L || value > 4L || !is.character(continuous_estimator))\n        stop(\"estimator must be one of the following: pearson, spearman, kendall, frequency\")\n    \n    return(value)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/RadioSet-class.R",
    "language": "R",
    "content": "#' A Class to Contain RadioGenomic datasets together with their curations\n#'\n#' The RadioSet (RSet) class was developed to contain and organise large\n#' RadioGenomic datasets, and aid in their metanalysis. It was designed\n#' primarily to allow bioinformaticians and biologists to work with data at the\n#' level of genes and cell lines, providing a more naturally intuitive\n#' interface and simplifying analyses between several datasets. As such, it was\n#' designed to be flexible enough to hold datasets of two different natures\n#' while providing a common interface. The class can accomidate datasets\n#' containing both radiation dose response data, as well as datasets contaning\n#' genetic profiles of cell lines pre and post treatement with compounds, known\n#' respecitively as sensitivity and perturbation datasets.\n#'\n#' @slot annotation A \\code{list} of annotation data about the RadioSet,\n#'    including the \\code{$name} and the session information for how the object\n#'    was creating, detailing the exact versions of R and all the packages used\n#' @slot molecularProfiles A \\code{list} containing 4 \\code{SummarizedExperiment}\n#'   type object for holding data for RNA, DNA, SNP and Copy Number Variation\n#'   measurements respectively, with associated \\code{fData} and \\code{pData}\n#'   containing the row and column metadata\n#' @slot sample A \\code{data.frame} containg the annotations for all the cell\n#'   lines profiled in the data set, across all data types\n#' @slot treatment A \\code{data.frame} containg the annotations for all the\n#'   radiation treatment types used in the in the dataset, across all data types\n#' @slot sensitivity A \\code{list} containing all the data for the sensitivity\n#'   experiments, including \\code{$info}, a \\code{data.frame} containing the\n#'   experimental info,\\code{$raw} a 3D \\code{array} containing raw data,\n#'   \\code{$profiles}, a \\code{data.frame} containing sensitivity profiles\n#'   statistics, and \\code{$n}, a \\code{data.frame} detailing the number of\n#'   experiments for each cell-radiation type pair\n#' @slot perturbation A \\code{list} containting \\code{$n}, a \\code{data.frame}\n#'   summarizing the available perturbation data,\n#' @slot curation A \\code{list} containing mappings for\n#'   \\code{cell} and \\code{tissue} names used in the data set to universal\n#'   identifiers used between different RadioSet objects\n#' @slot datasetType A \\code{character} string of 'sensitivity',\n#'   'perturbation', or both detailing what type of data can be found in the\n#'   RadioSet, for proper processing of the data\n#'\n#' @return An object of the RadioSet class\n#'\n#' @importClassesFrom CoreGx CoreSet\n.RadioSet <- setClass(\"RadioSet\", slots=list(radiation=\"data.frame\"),\n                      contains = \"CoreSet\")\n\n# The default constructor above does a poor job of explaining the required structure of a RadioSet.\n# The constructor function defined below guides the user into providing the required components of the curation and senstivity lists\n# and hides the annotation slot which the user does not need to manually fill.\n# This also follows the design of the Expression Set class.\n\n### -------------------------------------------------------------------------\n### Constructor -------------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' RadioSet constructor\n#'\n#' A constructor that simplifies the process of creating RadioSets, as well\n#' as creates empty objects for data not provided to the constructor. Only\n#' objects returned by this constructor are expected to work with the RadioSet\n#' methods. For a much more detailed instruction on creating RadioSets, please\n#' see the \"CreatingRadioSet\" vignette.\n#'\n#' @inheritParams CoreGx::CoreSet\n#'\n#' @return An object of class `RadioSet``\n#'\n#' @import methods\n#' @importFrom utils sessionInfo\n#' @importFrom stats na.omit\n#' @importFrom SummarizedExperiment rowData colData assay assays assayNames Assays\n#' @importFrom S4Vectors DataFrame SimpleList metadata\n#' @importFrom CoreGx CoreSet\n#'\n#' @export\nRadioSet <-  function(name,\n                      molecularProfiles=list(),\n                      sample=data.frame(),\n                      treatment=data.frame(),\n                      sensitivityInfo=data.frame(),\n                      sensitivityRaw=array(dim=c(0,0,0)),\n                      sensitivityProfiles=matrix(),\n                      sensitivityN=matrix(nrow=0, ncol=0),\n                      perturbationN=array(NA, dim=c(0,0,0)),\n                      curationSample = data.frame(),\n                      curationTissue = data.frame(),\n                      curationTreatment = data.frame(),\n                      datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n                      verify = TRUE)\n{\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        sample=sample,\n        treatment=treatment,\n        molecularProfiles=molecularProfiles,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify\n    )\n\n    rSet <- .RadioSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) { checkRSetStructure(rSet)}\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n      sensNumber(rSet) <- .summarizeSensitivityNumbers(rSet)\n    }\n      if (length(perturbationN) == 0  && datasetType %in% c(\"perturbation\", \"both\")) {\n        pertNumber(rSet) <- .summarizePerturbationNumbers(rSet)\n      }\n    return(rSet)\n}\n\n\n# Constructor Helper Functions ----------------------------------------------\n\n.summarizeSensitivityNumbers <- function(object) {\n\n  if (datasetType(object) != \"sensitivity\" && datasetType(object) != \"both\") {\n    stop (\"Data type must be either sensitivity or both\")\n  }\n\n  ## unique radiation identifiers\n  # radiationn <- sort(unique(sensitivityInfo(object)[ , \"treatmentid\"]))\n\n  ## consider all radiations\n  radiationn <- rownames(treatmentInfo(object))\n\n  ## unique radiation identifiers\n  # celln <- sort(unique(sensitivityInfo(object)[ , \"sampleid\"]))\n\n  ## consider all cell lines\n  celln <- rownames(sampleInfo(object))\n\n  sensitivity.info <- matrix(0, nrow=length(celln), ncol=length(radiationn), dimnames=list(celln, radiationn))\n  radiation.types <- sensitivityInfo(object)[ , \"treatmentid\"]\n  cellids <- sensitivityInfo(object)[ , \"sampleid\"]\n  cellids <- cellids[grep(\"///\", radiation.types, invert=TRUE)]\n  radiation.types <- radiation.types[grep(\"///\", radiation.types, invert=TRUE)]\n\n\n  tt <- table(cellids, radiation.types)\n  sensitivity.info[rownames(tt), colnames(tt)] <- tt\n\n  return(sensitivity.info)\n}\n\n\n.summarizeMolecularNumbers <- function(object) {\n\n  ## consider all molecular types\n  mDT <- mDataNames(object)\n\n  ## consider all cell lines\n  celln <- rownames(sampleInfo(object))\n\n  molecular.info <- matrix(0, nrow=length(celln), ncol=length(mDT), dimnames=list(celln, mDT))\n\n  for(mDataType in mDT) {\n    tt <- table(phenoInfo(object, mDataType)$sampleid)\n    molecular.info[names(tt), mDataType] <- tt\n\n  }\n  return(molecular.info)\n}\n\n\n.summarizePerturbationNumbers <- function(object) {\n\n  if (datasetType(object) != \"perturbation\" && datasetType(object) != \"both\") {\n    stop (\"Data type must be either perturbation or both\")\n  }\n\n  radiationn <- rownames(treatmentInfo(object))\n\n  celln <- rownames(sampleInfo(object))\n\n  perturbation.info <- array(0, dim=c(length(celln), length(radiationn), length(molecularProfilesSlot(object))), dimnames=list(celln, radiationn, names((molecularProfilesSlot(object)))))\n\n  for (i in seq_len(length(molecularProfilesSlot(object)))) {\n    if (nrow(SummarizedExperiment::colData(molecularProfilesSlot(object)[[i]])) > 0 && all(is.element(c(\"sampleid\", \"treatmentid\"), colnames(SummarizedExperiment::colData(molecularProfilesSlot(object)[[i]]))))) {\n      tt <- table(SummarizedExperiment::colData(molecularProfilesSlot(object)[[i]])[ , \"sampleid\"], SummarizedExperiment::colData(molecularProfilesSlot(object)[[i]])[ , \"treatmentid\"])\n      perturbation.info[rownames(tt), colnames(tt), names(molecularProfilesSlot(object))[i]] <- tt\n    }\n  }\n\n  return(perturbation.info)\n}\n\n### -------------------------------------------------------------------------\n### Class Validity ----------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' A function to verify the structure of a RadioSet\n#'\n#' This function checks the structure of a PharamcoSet, ensuring that the\n#' correct annotations are in place and all the required slots are filled so\n#' that matching of cells and radiations can be properly done across different\n#' types of data and with other studies.\n#'\n#' @examples\n#' checkRSetStructure(clevelandSmall)\n#'\n#' @param object A \\code{RadioSet} object\n#' @param plotDist Should the function also plot the distribution of molecular\n#'     data?\n#' @param result.dir The path to the directory for saving the plots as a string,\n#'     defaults to `tempdir()``\n#'\n#' @return Prints out messages whenever describing the errors found in the\n#'     structure of the pset object passed in.\n#'\n#' @importFrom graphics hist\n#' @importFrom grDevices dev.off pdf\n#' @export\ncheckRSetStructure <- function(object, plotDist=FALSE, result.dir=tempdir()) {\n    # Make directory to store results if it doesn't exist\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n\n    #####\n    # Checking molecularProfiles\n    #####\n    # Can this be parallelized or does it mess with the order of printing warnings?\n    for( i in seq_along(molecularProfilesSlot(object))) {\n      profile <- molecularProfilesSlot(object)[[i]]\n      nn <- names(molecularProfilesSlot(object))[i]\n\n      # Testing plot rendering for rna and rnaseq\n      if((S4Vectors::metadata(profile)$annotation == \"rna\" | S4Vectors::metadata(profile)$annotation == \"rnaseq\") & plotDist)\n      {\n        pdf(file=file.path(result.dir, sprintf(\"%s.pdf\", nn)))\n        hist(assays(profile)[[1]], breaks = 100)\n        dev.off()\n      }\n\n\n      ## Test if sample and feature annotations dimensions match the assay\n      warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                     sprintf(\"%s: number of features in fData is different from\n                             SummarizedExperiment slots\", nn),\n                     sprintf(\"%s: rowData dimension is OK\", nn)\n      )\n      )\n      warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                     sprintf(\"%s: number of cell lines in pData is different\n                             from expression slots\", nn),\n                     sprintf(\"%s: colData dimension is OK\", nn)\n      )\n      )\n\n\n      # Checking sample metadata for required columns\n      warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), \"\",\n                     sprintf(\"%s: sampleid does not exist in colData (samples)\n                             columns\", nn)))\n      warning(ifelse(\"batchid\" %in% colnames(colData(profile)), \"\",\n                     sprintf(\"%s: batchid does not exist in colData (samples)\n                             columns\", nn)))\n\n      # Checking mDataType of the SummarizedExperiment for required columns\n      if(S4Vectors::metadata(profile)$annotation == \"rna\" |\n         S4Vectors::metadata(profile)$annotation == \"rnaseq\")\n      {\n        warning(ifelse(\"BEST\" %in% colnames(rowData(profile)), \"BEST is OK\",\n                       sprintf(\"%s: BEST does not exist in rowData (features)\n                               columns\", nn)))\n        warning(ifelse(\"Symbol\" %in% colnames(rowData(profile)), \"Symbol is OK\",\n                       sprintf(\"%s: Symbol does not exist in rowData (features)\n                               columns\", nn)))\n      }\n\n      # Check that all cellids from the object are included in molecularProfiles\n      if(\"sampleid\" %in% colnames(rowData(profile))) {\n        if(!all(colData(profile)[,\"sampleid\"] %in% rownames(sampleInfo(object)))) {\n          warning(sprintf(\"%s: not all the cell lines in this profile are in\n                          cell lines slot\", nn))\n        }\n      }else {\n        warning(sprintf(\"%s: sampleid does not exist in colData (samples)\", nn))\n      }\n    }\n\n    ###\n    # CHECKING CELL\n    ###\n    if(\"tissueid\" %in% colnames(sampleInfo(object))) {\n      if(\"unique.tissueid\" %in% colnames(curation(object)$tissue))\n      {\n        if(length(intersect(rownames(curation(object)$tissue), rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n          message(\"rownames of curation tissue slot should be the same as cell slot (curated cell ids)\")\n        } else{\n          if(length(intersect(sampleInfo(object)$tissueid, curation(object)$tissue$unique.tissueid)) != length(table(sampleInfo(object)$tissueid))){\n            message(\"tissueid should be the same as unique tissue id from tissue curation slot\")\n          }\n        }\n      } else {\n        message(\"unique.tissueid which is curated tissue id across data set should be a column of tissue curation slot\")\n      }\n      if(any(is.na(sampleInfo(object)[,\"tissueid\"]) | sampleInfo(object)[,\"tissueid\"]==\"\", na.rm=TRUE)){\n        message(sprintf(\"There is no tissue type for this cell line(s): %s\", paste(rownames(sampleInfo(object))[which(is.na(sampleInfo(object)[,\"tissueid\"]) | sampleInfo(object)[,\"tissueid\"]==\"\")], collapse=\" \")))\n      }\n    } else {\n      warning(\"tissueid does not exist in cell slot\")\n    }\n\n    if(\"unique.sampleid\" %in% colnames(curation(object)$sample)) {\n      if(length(intersect(curation(object)$sample$unique.sampleid, rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n        message(\"rownames of cell slot should be curated cell ids\")\n      }\n    } else {\n      message(\"unique.sampleid which is curated cell id across data set should be a column of cell curation slot\")\n    }\n\n    if(length(intersect(rownames(curation(object)$sample), rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n      message(\"rownames of curation cell slot should be the same as cell slot (curated cell ids)\")\n    }\n\n    if(length(intersect(rownames(curation(object)$sample), rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n      message(\"rownames of curation radiation slot should be the same as radiation slot (curated radiation ids)\")\n    }\n\n    if(!is(sampleInfo(object), \"data.frame\")) {\n      warning(\"cell slot class type should be dataframe\")\n    }\n    if(!is(treatmentInfo(object), \"data.frame\")) {\n      warning(\"radiation slot class type should be dataframe\")\n    }\n    if(datasetType(object) %in% c(\"sensitivity\", \"both\"))\n    {\n      if(!is(sensitivityInfo(object), \"data.frame\")) {\n        warning(\"sensitivity info slot class type should be dataframe\")\n      }\n      if(\"sampleid\" %in% colnames(sensitivityInfo(object))) {\n        if(!all(sensitivityInfo(object)[,\"sampleid\"] %in% rownames(sampleInfo(object)))) {\n          warning(\"not all the cell lines in sensitivity data are in cell slot\")\n        }\n      }else {\n        warning(\"sampleid does not exist in sensitivity info\")\n      }\n\n      ###\n      # CHECKING RADIATION\n      ###\n      if(\"treatmentid\" %in% colnames(sensitivityInfo(object))) {\n        radiation.ids <- unique(sensitivityInfo(object)[,\"treatmentid\"])\n        radiation.ids <- radiation.ids[grep(\"///\",radiation.ids, invert=TRUE)]\n        if(!all(radiation.ids %in% rownames(treatmentInfo(object)))) {\n          message(\"not all the radiations in sensitivity data are in radiation slot\")\n        }\n      }else {\n        warning(\"treatmentid does not exist in sensitivity info\")\n      }\n\n      if(any(!is.na(sensitivityRaw(object)))) {\n        if(!all(dimnames(sensitivityRaw(object))[[1]] %in% rownames(sensitivityInfo(object)))) {\n          warning(\"For some experiments there is raw sensitivity data but no experimet information in sensitivity info\")\n        }\n      }\n      if(!all(rownames(sensitivityProfiles(object)) %in% rownames(sensitivityInfo(object)))) {\n        warning(\"For some experiments there is sensitivity profiles but no experimet information in sensitivity info\")\n      }\n    }\n  }\n\n### -------------------------------------------------------------------------\n### Method Definitions ------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' Show a RadioSet\n#'\n#' @examples\n#' data(clevelandSmall)\n#' clevelandSmall\n#'\n#' @param object A \\code{RadioSet} object\n#'\n#' @return Prints the RadioSet object to the output stream, and returns\n#'   invisible NULL.\n#'\n#' @export\nsetMethod(\"show\", signature=signature(object=\"RadioSet\"),\n      function(object) {\n    callNextMethod(object)\n})\n\n#' Get the dimensions of a RadioSet\n#'\n#' @examples\n#' data(clevelandSmall)\n#' dim(clevelandSmall)\n#'\n#' @param x RadioSet\n#' @return A named vector with the number of Cells and Drugs in the RadioSet\n#' @export\nsetMethod(\"dim\", signature=signature(x=\"RadioSet\"), function(x){\n  return(c(Cells=length(sampleNames(x)), Radiation=length(treatmentNames(x))))\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the RadioSet class and what types of data can it accommodate?",
        "answer": "The RadioSet class is designed to contain and organize large RadioGenomic datasets and aid in their metanalysis. It can accommodate two types of datasets: 1) radiation dose response data, and 2) genetic profiles of cell lines pre and post treatment with compounds, known as sensitivity and perturbation datasets respectively. It provides a common interface for bioinformaticians and biologists to work with data at the level of genes and cell lines."
      },
      {
        "question": "What are the key slots in the RadioSet class and what do they contain?",
        "answer": "The key slots in the RadioSet class include: 1) annotation: contains metadata about the RadioSet, 2) molecularProfiles: holds data for RNA, DNA, SNP, and Copy Number Variation measurements, 3) sample: contains annotations for cell lines, 4) treatment: contains annotations for radiation treatment types, 5) sensitivity: holds data for sensitivity experiments, 6) perturbation: contains perturbation data, 7) curation: contains mappings for cell and tissue names, and 8) datasetType: specifies the type of data in the RadioSet."
      },
      {
        "question": "How does the RadioSet constructor function simplify the process of creating RadioSets?",
        "answer": "The RadioSet constructor function simplifies the creation process by: 1) guiding users to provide required components of the curation and sensitivity lists, 2) hiding the annotation slot which users don't need to manually fill, 3) creating empty objects for data not provided to the constructor, 4) verifying the structure of the created RadioSet object, and 5) automatically summarizing sensitivity and perturbation numbers if not provided. This approach ensures that only properly structured RadioSet objects are created and work with the RadioSet methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "RadioSet <- function(name,\n                      molecularProfiles=list(),\n                      sample=data.frame(),\n                      treatment=data.frame(),\n                      sensitivityInfo=data.frame(),\n                      sensitivityRaw=array(dim=c(0,0,0)),\n                      sensitivityProfiles=matrix(),\n                      sensitivityN=matrix(nrow=0, ncol=0),\n                      perturbationN=array(NA, dim=c(0,0,0)),\n                      curationSample = data.frame(),\n                      curationTissue = data.frame(),\n                      curationTreatment = data.frame(),\n                      datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n                      verify = TRUE)\n{\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        sample=sample,\n        treatment=treatment,\n        molecularProfiles=molecularProfiles,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify\n    )\n\n    rSet <- .RadioSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) { checkRSetStructure(rSet)}\n    # Complete the function here",
        "complete": "RadioSet <- function(name,\n                      molecularProfiles=list(),\n                      sample=data.frame(),\n                      treatment=data.frame(),\n                      sensitivityInfo=data.frame(),\n                      sensitivityRaw=array(dim=c(0,0,0)),\n                      sensitivityProfiles=matrix(),\n                      sensitivityN=matrix(nrow=0, ncol=0),\n                      perturbationN=array(NA, dim=c(0,0,0)),\n                      curationSample = data.frame(),\n                      curationTissue = data.frame(),\n                      curationTreatment = data.frame(),\n                      datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n                      verify = TRUE)\n{\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        sample=sample,\n        treatment=treatment,\n        molecularProfiles=molecularProfiles,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify\n    )\n\n    rSet <- .RadioSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) { checkRSetStructure(rSet)}\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n      sensNumber(rSet) <- .summarizeSensitivityNumbers(rSet)\n    }\n    if (length(perturbationN) == 0  && datasetType %in% c(\"perturbation\", \"both\")) {\n      pertNumber(rSet) <- .summarizePerturbationNumbers(rSet)\n    }\n    return(rSet)\n}"
      },
      {
        "partial": "checkRSetStructure <- function(object, plotDist=FALSE, result.dir=tempdir()) {\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n\n    for( i in seq_along(molecularProfilesSlot(object))) {\n      profile <- molecularProfilesSlot(object)[[i]]\n      nn <- names(molecularProfilesSlot(object))[i]\n\n      if((S4Vectors::metadata(profile)$annotation == \"rna\" | S4Vectors::metadata(profile)$annotation == \"rnaseq\") & plotDist)\n      {\n        pdf(file=file.path(result.dir, sprintf(\"%s.pdf\", nn)))\n        hist(assays(profile)[[1]], breaks = 100)\n        dev.off()\n      }\n\n      warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                     sprintf(\"%s: number of features in fData is different from\n                             SummarizedExperiment slots\", nn),\n                     sprintf(\"%s: rowData dimension is OK\", nn)\n      ))\n      warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                     sprintf(\"%s: number of cell lines in pData is different\n                             from expression slots\", nn),\n                     sprintf(\"%s: colData dimension is OK\", nn)\n      ))\n\n      warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), \"\",\n                     sprintf(\"%s: sampleid does not exist in colData (samples)\n                             columns\", nn)))\n      warning(ifelse(\"batchid\" %in% colnames(colData(profile)), \"\",\n                     sprintf(\"%s: batchid does not exist in colData (samples)\n                             columns\", nn)))\n\n      if(S4Vectors::metadata(profile)$annotation == \"rna\" |\n         S4Vectors::metadata(profile)$annotation == \"rnaseq\")\n      {\n        warning(ifelse(\"BEST\" %in% colnames(rowData(profile)), \"BEST is OK\",\n                       sprintf(\"%s: BEST does not exist in rowData (features)\n                               columns\", nn)))\n        warning(ifelse(\"Symbol\" %in% colnames(rowData(profile)), \"Symbol is OK\",\n                       sprintf(\"%s: Symbol does not exist in rowData (features)\n                               columns\", nn)))\n      }\n\n      if(\"sampleid\" %in% colnames(rowData(profile))) {\n        if(!all(colData(profile)[,\"sampleid\"] %in% rownames(sampleInfo(object)))) {\n          warning(sprintf(\"%s: not all the cell lines in this profile are in\n                          cell lines slot\", nn))\n        }\n      }else {\n        warning(sprintf(\"%s: sampleid does not exist in colData (samples)\", nn))\n      }\n    }\n\n    # Complete the function here",
        "complete": "checkRSetStructure <- function(object, plotDist=FALSE, result.dir=tempdir()) {\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n\n    for( i in seq_along(molecularProfilesSlot(object))) {\n      profile <- molecularProfilesSlot(object)[[i]]\n      nn <- names(molecularProfilesSlot(object))[i]\n\n      if((S4Vectors::metadata(profile)$annotation == \"rna\" | S4Vectors::metadata(profile)$annotation == \"rnaseq\") & plotDist)\n      {\n        pdf(file=file.path(result.dir, sprintf(\"%s.pdf\", nn)))\n        hist(assays(profile)[[1]], breaks = 100)\n        dev.off()\n      }\n\n      warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                     sprintf(\"%s: number of features in fData is different from\n                             SummarizedExperiment slots\", nn),\n                     sprintf(\"%s: rowData dimension is OK\", nn)\n      ))\n      warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                     sprintf(\"%s: number of cell lines in pData is different\n                             from expression slots\", nn),\n                     sprintf(\"%s: colData dimension is OK\", nn)\n      ))\n\n      warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), \"\",\n                     sprintf(\"%s: sampleid does not exist in colData (samples)\n                             columns\", nn)))\n      warning(ifelse(\"batchid\" %in% colnames(colData(profile)), \"\",\n                     sprintf(\"%s: batchid does not exist in colData (samples)\n                             columns\", nn)))\n\n      if(S4Vectors::metadata(profile)$annotation == \"rna\" |\n         S4Vectors::metadata(profile)$annotation == \"rnaseq\")\n      {\n        warning(ifelse(\"BEST\" %in% colnames(rowData(profile)), \"BEST is OK\",\n                       sprintf(\"%s: BEST does not exist in rowData (features)\n                               columns\", nn)))\n        warning(ifelse(\"Symbol\" %in% colnames(rowData(profile)), \"Symbol is OK\",\n                       sprintf(\"%s: Symbol does not exist in rowData (features)\n                               columns\", nn)))\n      }\n\n      if(\"sampleid\" %in% colnames(rowData(profile))) {\n        if(!all(colData(profile)[,\"sampleid\"] %in% rownames(sampleInfo(object)))) {\n          warning(sprintf(\"%s: not all the cell lines in this profile are in\n                          cell lines slot\", nn))\n        }\n      }else {\n        warning(sprintf(\"%s: sampleid does not exist in colData (samples)\", nn))\n      }\n    }\n\n    if(\"tissueid\" %in% colnames(sampleInfo(object))) {\n      if(\"unique.tissueid\" %in% colnames(curation(object)$tissue))\n      {\n        if(length(intersect(rownames(curation(object)$tissue), rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n          message(\"rownames of curation tissue slot should be the same as cell slot (curated cell ids)\")\n        } else{\n          if(length(intersect(sampleInfo(object)$tissueid, curation(object)$tissue$unique.tissueid)) != length(table(sampleInfo(object)$tissueid))){\n            message(\"tissueid should be the same as unique tissue id from tissue curation slot\")\n          }\n        }\n      } else {\n        message(\"unique.tissueid which is curated tissue id across data set should be a column of tissue curation slot\")\n      }\n      if(any(is.na(sampleInfo(object)[,\"tissueid\"]) | sampleInfo(object)[,\"tissueid\"]==\"\", na.rm=TRUE)){\n        message(sprintf(\"There is no tissue type for this cell line(s): %s\", paste(rownames(sampleInfo(object))[which(is.na(sampleInfo(object)[,\"tissueid\"]) | sampleInfo(object)[,\"tissueid\"]==\"\")], collapse=\" \")))\n      }\n    } else {\n      warning(\"tissueid does not exist in cell slot\")\n    }\n\n    if(\"unique.sampleid\" %in% colnames(curation(object)$sample)) {\n      if(length(intersect(curation(object)$sample$unique.sampleid, rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n        message(\"rownames of cell slot should be curated cell ids\")\n      }\n    } else {\n      message(\"unique.sampleid which is curated cell id across data set should be a column of cell curation slot\")\n    }\n\n    if(length(intersect(rownames(curation(object)$sample), rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n      message(\"rownames of curation cell slot should be the same as cell slot (curated cell ids)\")\n    }\n\n    if(length(intersect(rownames(curation(object)$sample), rownames(sampleInfo(object)))) != nrow(sampleInfo(object))) {\n      message(\"rownames of curation radiation slot should be the same as radiation slot (curated radiation ids)\")\n    }\n\n    if(!is(sampleInfo(object), \"data"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/utilities.R",
    "language": "R",
    "content": "# rSet molecularProfiles from ESets to SEs\n#\n# Converts all ExpressionSet objects within the molecularProfiles slot of a\n#  RadioSet to SummarizedExperiments\n#\n# @param rSet \\code{S4} A RadioSet containing molecular data in ExpressionSets\n#\n# @return \\code{S4} A RadioSet containing molecular data in a SummarizedExperiments\n#\n#' @importFrom SummarizedExperiment assay assays assayNames\n#' @importClassesFrom SummarizedExperiment SummarizedExperiment Assays\n#' @importFrom Biobase exprs fData pData annotation protocolData assayData experimentData\n#' @importFrom S4Vectors SimpleList DataFrame\n#' @importFrom stats setNames\n#' @export\n#' @keywords internal\n.convertRsetMolecularProfilesToSE <- function(rSet) {\n\n  eSets <- molecularProfilesSlot(rSet) # Extract eSet data\n\n  molecularProfilesSlot(rSet) <-\n    lapply(eSets,\n           function(eSet){\n\n             # Change rownames from probes to EnsemblGeneId for rna data type\n             if (grepl(\"^rna$\", Biobase::annotation(eSet))) {\n               rownames(eSet) <- Biobase::fData(eSet)$EnsemblGeneId\n             }\n\n             # Build summarized experiment from eSet\n             SE <- SummarizedExperiment::SummarizedExperiment(\n               ## TODO:: Do we want to pass an environment for better memory efficiency?\n               assays=SimpleList(as.list(Biobase::assayData(eSet))\n               ),\n               # Switch rearrange columns so that IDs are first, probes second\n               rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                            rownames=rownames(Biobase::fData(eSet))\n               ),\n               colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                            rownames=rownames(Biobase::pData(eSet))\n               ),\n               metadata=list(\"experimentData\" = eSet@experimentData,\n                             \"annotation\" = Biobase::annotation(eSet),\n                             \"protocolData\" = Biobase::protocolData(eSet)\n               )\n             )\n             ## TODO:: Determine if this can be done in the SE constructor?\n             # Extract names from expression set\n             SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n             # Assign SE to rSet\n             mDataType <- Biobase::annotation(eSet)\n             molecularProfilesSlot(rSet)[[mDataType]] <- SE\n           })\n  setNames(molecularProfilesSlot(rSet), names(eSets))\n  rSet\n}\n\n## Validate rSet molecularProfiles Conversion\n##\n## Checks that all the information contained in an ExpressionSet molecularProfile\n##   was successfully tranferred to the SummarizedExperiment molecularProfile\n##\n## @param rSet_new \\code{S4} a rSet containing molecularProfiles as SummarizedExperiments\n## @param rSet_old \\code{S4} a rSet containing molecularProfiles as ExpressionSets\n##\n## @return \\code{message} Any slots which are not the same\n##\n#' @importFrom assertthat are_equal\n#' @importFrom SummarizedExperiment SummarizedExperiment Assays assay\n#'   assayNames assayNames<-\n#' @importFrom Biobase exprs fData pData annotation protocolData\n#'   assayDataElementNames experimentData assayData\n#' @keywords internal\n.validateRsetMolecularProfilesToSEConversion <- function(rSet_old, rSet_new) {\n\n  # Testing that rSets are in correct order\n  message(\"Checking is rSet structures are correct\")\n\n  if(!all(vapply(rSet_old@molecularProfiles,\n                 function(x) { is(x, \"ExpressionSet\") }, FUN.VALUE = logical(1)))\n  ) message(\"Old rSet doesn't contain ExpressionSet objects, maybe argument order is wrong?\")\n\n  if(\n    !all(vapply(molecularProfilesSlot(rSet_new),\n                function(x) { is(x, \"SummarizedExperiment\") }, FUN.VALUE = logical(1)))\n  ) message(\"New rSet doesn't contain SummarizedExperiment objects, maybe argument order is wrong?\")\n\n  # Comparing molecularProfiles slot data\n  message(\"Checking molecularProfiles slots hold equivalent data.\")\n\n  for (i in seq_len(length(rSet_old@molecularProfiles))) {\n    for (j in seq_along(assays(molecularProfilesSlot(rSet_new)[[i]]))) {\n      if(!all(\n          as.list(assayData(rSet_old@molecularProfiles[[i]]))[[j]],\n            assay(molecularProfilesSlot(rSet_new)[[i]], j),\n          na.rm = TRUE)) message(\"The assay data is not equivalent\")\n    }\n  }\n  ## TODO:: Rewrite this as an apply statement\n  for (i in seq_len(length(rSet_old@molecularProfiles))) { # Have to compare like this due to NAs in data\n    # Checking phenoData\n    if(\n      !(if (nrow(pData(rSet_old@molecularProfiles[[i]])) > 0) {\n        all(\n          as(rSet_old@molecularProfiles[[i]]@phenoData, \"data.frame\") ==\n            as.data.frame(molecularProfilesSlot(rSet_new)[[i]]@colData[\n              seq_len(length(molecularProfilesSlot(rSet_new)[[i]]@colData) -1)]),\n          na.rm = TRUE)\n      } else { FALSE })\n    ) message(\"The phenoData is not equivalent\")\n    # Checking featureData\n    if(\n      !(if (nrow(fData(rSet_old@molecularProfiles[[i]])) > 0) {\n        all(\n          as(rSet_old@molecularProfiles[[i]]@featureData, \"data.frame\") ==\n            as.data.frame(molecularProfilesSlot(rSet_new)[[i]]@elementMetadata[\n              seq_len(length(molecularProfilesSlot(rSet_new)[[i]]@elementMetadata) - 1)]),\n          na.rm = TRUE)\n      } else { FALSE })\n    ) message(\"The featureData is not equivalent\")\n    # Checking protocolData\n    if(\n      !all(\n        as(rSet_old@molecularProfiles[[i]]@protocolData, \"data.frame\") ==\n          as(molecularProfilesSlot(rSet_new)[[i]]@metadata$protocolData, \"data.frame\"),\n        na.rm = TRUE)\n      ) message(\"The protocolData is not equivalent\")\n  }\n\n  if(!assertthat::are_equal(\n    lapply(rSet_old@molecularProfiles, function(x) { annotation(x) }),\n    lapply(molecularProfilesSlot(rSet_new), function(x) { metadata(x)$annotation }))\n  )  message(\"The annotation is not equivalent\")\n\n  if(!assertthat::are_equal(\n    lapply(rSet_old@molecularProfiles, function(x) { experimentData(x) }),\n    lapply(molecularProfilesSlot(rSet_new), function(x) { metadata(x)$experimentData })\n    )\n  ) message(\"The experimentData is not equivalent\")\n\n  # Comparing remainder of rSet slots; should not be affect by conversion\n  message(\"Comparing remainder of rSet slots\")\n\n  if (!assertthat::are_equal(rSet_old@annotation, rSet_new@annotation))\n    message(\"annotation slots not equal!\")\n\n  if (!assertthat::are_equal(rSet_old@sample, rSet_new@sample))\n    message(\"cell slots are not equal!\")\n\n  if (!assertthat::are_equal(rSet_old@radiation, rSet_new@radiation))\n    message(\"radiation slots are not equal!\")\n\n  if (!assertthat::are_equal(rSet_old@treatmentResponse, treatmentResponse(rSet_new)))\n    message(\"sensitivty slots are not equal!\")\n\n  if (!assertthat::are_equal(rSet_old@datasetType, datasetType(rSet_new)))\n    message(\"datasetType slots are not equal!\")\n\n  if (!assertthat::are_equal(rSet_old@perturbation, rSet_new@perturbation))\n    message(\"perturbation slots are not equal!\")\n\n  if (!assertthat::are_equal(rSet_old@curation, rSet_new@curation))\n    message(\"curation slots are not equal\")\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `.convertRsetMolecularProfilesToSE` function?",
        "answer": "The main purpose of the `.convertRsetMolecularProfilesToSE` function is to convert all ExpressionSet objects within the molecularProfiles slot of a RadioSet to SummarizedExperiments. It takes a RadioSet containing molecular data in ExpressionSets as input and returns a RadioSet with molecular data in SummarizedExperiments."
      },
      {
        "question": "How does the function handle RNA data types when converting from ExpressionSet to SummarizedExperiment?",
        "answer": "For RNA data types (identified by the annotation starting with 'rna'), the function changes the rownames from probes to EnsemblGeneId. This is done by assigning `Biobase::fData(eSet)$EnsemblGeneId` to the rownames of the ExpressionSet before converting it to a SummarizedExperiment."
      },
      {
        "question": "What is the purpose of the `.validateRsetMolecularProfilesToSEConversion` function and how does it work?",
        "answer": "The `.validateRsetMolecularProfilesToSEConversion` function checks that all the information contained in an ExpressionSet molecularProfile was successfully transferred to the SummarizedExperiment molecularProfile. It compares various components of the old and new RadioSets, including assay data, phenoData, featureData, protocolData, annotation, and experimentData. If any discrepancies are found, it outputs messages indicating which parts are not equivalent."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Convert ExpressionSet to SummarizedExperiment\n.convertRsetMolecularProfilesToSE <- function(rSet) {\n  eSets <- molecularProfilesSlot(rSet)\n  molecularProfilesSlot(rSet) <- lapply(eSets, function(eSet) {\n    if (grepl(\"^rna$\", Biobase::annotation(eSet))) {\n      rownames(eSet) <- Biobase::fData(eSet)$EnsemblGeneId\n    }\n    SE <- SummarizedExperiment::SummarizedExperiment(\n      assays = SimpleList(as.list(Biobase::assayData(eSet))),\n      rowData = S4Vectors::DataFrame(Biobase::fData(eSet), rownames = rownames(Biobase::fData(eSet))),\n      colData = S4Vectors::DataFrame(Biobase::pData(eSet), rownames = rownames(Biobase::pData(eSet))),\n      metadata = list(\n        \"experimentData\" = eSet@experimentData,\n        \"annotation\" = Biobase::annotation(eSet),\n        \"protocolData\" = Biobase::protocolData(eSet)\n      )\n    )\n    SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n    # Complete the function by adding the missing line\n  })\n  setNames(molecularProfilesSlot(rSet), names(eSets))\n  rSet\n}",
        "complete": "# Convert ExpressionSet to SummarizedExperiment\n.convertRsetMolecularProfilesToSE <- function(rSet) {\n  eSets <- molecularProfilesSlot(rSet)\n  molecularProfilesSlot(rSet) <- lapply(eSets, function(eSet) {\n    if (grepl(\"^rna$\", Biobase::annotation(eSet))) {\n      rownames(eSet) <- Biobase::fData(eSet)$EnsemblGeneId\n    }\n    SE <- SummarizedExperiment::SummarizedExperiment(\n      assays = SimpleList(as.list(Biobase::assayData(eSet))),\n      rowData = S4Vectors::DataFrame(Biobase::fData(eSet), rownames = rownames(Biobase::fData(eSet))),\n      colData = S4Vectors::DataFrame(Biobase::pData(eSet), rownames = rownames(Biobase::pData(eSet))),\n      metadata = list(\n        \"experimentData\" = eSet@experimentData,\n        \"annotation\" = Biobase::annotation(eSet),\n        \"protocolData\" = Biobase::protocolData(eSet)\n      )\n    )\n    SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n    molecularProfilesSlot(rSet)[[Biobase::annotation(eSet)]] <- SE\n  })\n  setNames(molecularProfilesSlot(rSet), names(eSets))\n  rSet\n}"
      },
      {
        "partial": "# Validate rSet molecularProfiles Conversion\n.validateRsetMolecularProfilesToSEConversion <- function(rSet_old, rSet_new) {\n  message(\"Checking if rSet structures are correct\")\n  if (!all(vapply(rSet_old@molecularProfiles, function(x) is(x, \"ExpressionSet\"), logical(1)))) {\n    message(\"Old rSet doesn't contain ExpressionSet objects, maybe argument order is wrong?\")\n  }\n  if (!all(vapply(molecularProfilesSlot(rSet_new), function(x) is(x, \"SummarizedExperiment\"), logical(1)))) {\n    message(\"New rSet doesn't contain SummarizedExperiment objects, maybe argument order is wrong?\")\n  }\n  message(\"Checking molecularProfiles slots hold equivalent data.\")\n  # Add the missing validation checks here\n  \n  message(\"Comparing remainder of rSet slots\")\n  if (!assertthat::are_equal(rSet_old@annotation, rSet_new@annotation)) message(\"annotation slots not equal!\")\n  if (!assertthat::are_equal(rSet_old@sample, rSet_new@sample)) message(\"cell slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@radiation, rSet_new@radiation)) message(\"radiation slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@treatmentResponse, treatmentResponse(rSet_new))) message(\"sensitivity slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@datasetType, datasetType(rSet_new))) message(\"datasetType slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@perturbation, rSet_new@perturbation)) message(\"perturbation slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@curation, rSet_new@curation)) message(\"curation slots are not equal\")\n}",
        "complete": "# Validate rSet molecularProfiles Conversion\n.validateRsetMolecularProfilesToSEConversion <- function(rSet_old, rSet_new) {\n  message(\"Checking if rSet structures are correct\")\n  if (!all(vapply(rSet_old@molecularProfiles, function(x) is(x, \"ExpressionSet\"), logical(1)))) {\n    message(\"Old rSet doesn't contain ExpressionSet objects, maybe argument order is wrong?\")\n  }\n  if (!all(vapply(molecularProfilesSlot(rSet_new), function(x) is(x, \"SummarizedExperiment\"), logical(1)))) {\n    message(\"New rSet doesn't contain SummarizedExperiment objects, maybe argument order is wrong?\")\n  }\n  message(\"Checking molecularProfiles slots hold equivalent data.\")\n  for (i in seq_along(rSet_old@molecularProfiles)) {\n    for (j in seq_along(assays(molecularProfilesSlot(rSet_new)[[i]]))) {\n      if (!all(as.list(assayData(rSet_old@molecularProfiles[[i]]))[[j]] == assay(molecularProfilesSlot(rSet_new)[[i]], j), na.rm = TRUE)) {\n        message(\"The assay data is not equivalent\")\n      }\n    }\n    if (nrow(pData(rSet_old@molecularProfiles[[i]])) > 0 && !all(as(rSet_old@molecularProfiles[[i]]@phenoData, \"data.frame\") == as.data.frame(molecularProfilesSlot(rSet_new)[[i]]@colData[seq_len(length(molecularProfilesSlot(rSet_new)[[i]]@colData) - 1)]), na.rm = TRUE)) {\n      message(\"The phenoData is not equivalent\")\n    }\n    if (nrow(fData(rSet_old@molecularProfiles[[i]])) > 0 && !all(as(rSet_old@molecularProfiles[[i]]@featureData, \"data.frame\") == as.data.frame(molecularProfilesSlot(rSet_new)[[i]]@elementMetadata[seq_len(length(molecularProfilesSlot(rSet_new)[[i]]@elementMetadata) - 1)]), na.rm = TRUE)) {\n      message(\"The featureData is not equivalent\")\n    }\n    if (!all(as(rSet_old@molecularProfiles[[i]]@protocolData, \"data.frame\") == as(molecularProfilesSlot(rSet_new)[[i]]@metadata$protocolData, \"data.frame\"), na.rm = TRUE)) {\n      message(\"The protocolData is not equivalent\")\n    }\n  }\n  if (!assertthat::are_equal(lapply(rSet_old@molecularProfiles, annotation), lapply(molecularProfilesSlot(rSet_new), function(x) metadata(x)$annotation))) {\n    message(\"The annotation is not equivalent\")\n  }\n  if (!assertthat::are_equal(lapply(rSet_old@molecularProfiles, experimentData), lapply(molecularProfilesSlot(rSet_new), function(x) metadata(x)$experimentData))) {\n    message(\"The experimentData is not equivalent\")\n  }\n  message(\"Comparing remainder of rSet slots\")\n  if (!assertthat::are_equal(rSet_old@annotation, rSet_new@annotation)) message(\"annotation slots not equal!\")\n  if (!assertthat::are_equal(rSet_old@sample, rSet_new@sample)) message(\"cell slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@radiation, rSet_new@radiation)) message(\"radiation slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@treatmentResponse, treatmentResponse(rSet_new))) message(\"sensitivity slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@datasetType, datasetType(rSet_new))) message(\"datasetType slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@perturbation, rSet_new@perturbation)) message(\"perturbation slots are not equal!\")\n  if (!assertthat::are_equal(rSet_old@curation, rSet_new@curation)) message(\"curation slots are not equal\")\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/utils-optimization.R",
    "language": "R",
    "content": "#' Curve fitting via `stats::optim` L-BFGS-B with fall-back grid/pattern search\n#'   if convergence is not achieved.\n#'\n#' @description Function to fit curve via stats::optim\n#'\n#' @param par `numeric` Vector of intial guesses for the parameters. For each\n#'    index `i` of `par`, par\\[i\\] must be within the range (`lower\\[i\\]`,\n#'   `upper\\[i\\]`). If only a single `upper` or `lower` value is present,\n#'   that range is used for all parameters in `par`.\n#' @param x `numeric` Values to evaluate `fn` for.\n#' @param y `numeric` Target output values to optimze `fn` against.\n#' @param fn `function` A function to optimize. Any `fn` arguments passed via\n#'   `...` will be treated as constant and removed from the optimization. It\n#'   is assumed that the first argument is the x value to optimize over and\n#'   any subsequent arguments are free parameters to be optimized. Transformed\n#'   to be optim compatible via `make_optim_function` is the first arguement\n#'   isn't already `par`.\n#' @param loss `character(1)` or `function` Either the name of one of the bundled\n#'   loss functions (see details) or a custom loss function to compute for\n#'   the output of `fn` over `x`.\n#' @param lower `numeric(1)` Lower bound for parameters. Parallel to `par`.\n#' @param upper `numeric(1)` Upper bound for paramteres. Parallel to `par`.\n#' @param density `numeric` how many points in the dimension of each parameter\n#'   should be evaluated (density of the grid)\n#' @param step initial step size for pattern search.\n#' @param precision `numeric` smallest step size used in pattern search, once\n#'   step size drops below this value, the search terminates.\n#' @param span `numeric` Can be safely kept at 1, multiplicative ratio for\n#'   initial step size in pattern search. Must be larger than precision.\n#' @param ... `pairlist` Fall through arguments to `fn`.\n#' @param loss_args `list` Additional argument to the `loss` function.\n#'   These get passed to losss via `do.call` analagously to using `...`.\n#' @param optim_only `logical(1)` Should the fall back methods when optim fails\n#'   be skipped? Default is `FALSE`.\n#' @param control `list` List of control parameters to pass to `optim`. See\n#'   `?optim` for details.\n#'\n#' @examples\n#' \\dontrun{\n#'   # Four parameter hill curve equation\n#'   hillEqn <- function(x, Emin, Emax, EC50, lambda) {\n#'       (Emin + Emax * (x / EC50)^lambda) / (1 + (x / EC50)^lambda)\n#'   }\n#'   # Make some dummy data\n#'   doses <- rev(1000 / (2^(1:20)))\n#'   lambda <- 1\n#'   Emin <- 1\n#'   Emax <- 0.1\n#'   EC50 <- median(doses)\n#'   response <- hillEqn(doses, Emin=Emin, lambda=lambda, Emax=Emax, EC50=EC50)\n#'   nresponse <- response + rnorm(length(response), sd=sd(response)*0.1) # add noise\n#'   # 3-parameter optimization\n#'   3par <- .fitCurve2(\n#'       par=c(Emax, EC50, lambda),\n#'       x=doses,\n#'       y=nresponse,\n#'       fn=hillEqn,\n#'       Emin=Emin, # set this as constant in the function being optimized (via ...)\n#'       loss=.normal_loss,\n#'       loss_args=list(trunc=FALSE, n=1, scale=0.07),\n#'       upper=c(1, max(doses), 6),\n#'       lower=c(0, min(doses), 0)\n#'   )\n#'   # 2-parameter optimization\n#'   2par <- .fitCurve2(\n#'       par=c(Emax, EC50),\n#'       x=doses,\n#'       y=nresponse,\n#'       fn=hillEqn,\n#'       Emin=Emin, # set this as constant in the function being optimized (via ...)\n#'       lambda=1,\n#'       loss=.normal_loss,\n#'       loss_args=list(trunc=FALSE, n=1, scale=0.07),\n#'       upper=c(1, max(doses)),\n#'       lower=c(0, min(doses))\n#'   )\n#' }\n#'\n#' @details\n#' TODO\n#'\n#' @return `numeric` Vector of optimal parameters for `fn` fit against `y`\n#'   on the values of `x`.\n#'\n#' @importFrom stats optim\n#' @export\n.fitCurve2 <- function(par, x, y, fn, loss, lower=-Inf, upper=Inf,\n        precision=1e-4, density=c(2, 10, 5), step= 0.5 / density, ...,\n        loss_args=list(), span=1, optim_only=FALSE,\n        control=list(factr=1e-08, ndeps=rep(1e-4, times=length(par)), trace = 0)\n        ) {\n    stopifnot(is.function(fn))\n    stopifnot(is.function(loss) || is.character(loss))\n    stopifnot(c(\"par\", \"x\", \"y\", \"fn\") %in% formalArgs(loss))\n    stopifnot(\n        is.null(names(loss_args)) || all(names(loss_args) %in% formalArgs(loss))\n    )\n    stopifnot(\n        (length(par) == length(upper) && length(par) == length(lower)) ||\n        (length(upper) == 1 && length(lower) == 1)\n    )\n    stopifnot(span > precision)\n    optim_fn <- if (is_optim_compatible(fn)) {\n        fn\n    } else {\n        make_optim_function(fn, ...)\n    }\n    guess <- optim(\n        par=par,\n        fn=function(p)\n            do.call(loss,\n                args=c(\n                    list(par=p, x=x, y=y, fn=optim_fn), # mandatory loss args\n                    loss_args # additional args to loss\n                )\n            ),\n        upper=upper,\n        lower=lower,\n        control=control,\n        method=\"L-BFGS-B\"\n    )\n\n    failed <- guess[[\"convergence\"]] != 0\n    if (failed) guess <- list(par=par, convergence=(-1))\n    guess <- guess[[\"par\"]]\n\n    guess_residual <- do.call(loss,\n        args=c(list(par=guess, x=x, y=y, fn=optim_fn), loss_args))\n    gritty_guess_residual <- do.call(loss,\n        args=c(list(par=par, x=x, y=y, fn=optim_fn), loss_args))\n\n    if (\n        (failed || any(is.na(guess)) || guess_residual >= gritty_guess_residual)\n        && !optim_only\n    ) {\n        guess <- .meshEval2(density=density, par=guess, x=x, y=y, fn=optim_fn,\n            lower=lower, upper=upper, ..., loss=loss,loss_args=loss_args)\n        guess_residual <- do.call(loss,\n            args=c(list(par=guess, x=x, y=y, fn=optim_fn), loss_args))\n\n        guess <- .patternSearch2(span=span, precision=precision, step=step,\n            par=guess, par_residual=guess_residual, x=x, y=y, fn=optim_fn,\n            loss=loss, lower=lower, upper=upper, ..., loss_args=loss_args)\n    }\n\n    y_hat <- optim_fn(par=guess, x=x)\n\n    Rsqr <- 1 - (var(y - y_hat) / var(y))\n    attr(guess, \"Rsquare\") <- Rsqr\n\n    return(guess)\n}\n\n\n#' Compute the loss using the expectation of the likelihood of the median\n#'   for N samples from a probability density function.\n#'\n#' @param .pdf `function` Probability density function to use for computing loss.\n#' @param .edf `function` Expected likelihood function for the median of `n`\n#'   random samples from `.pdf`.\n#' @inheritParams .fitCurve2\n#' @param n `numeric(1)`\n#' @param scale `numeric(1)`\n#' @param trunc `logical(1)`\n#'\n#' @return `numeric(1)` Loss of `fn` on `x` relative to `y`.\n#'\n#' @keywords interal\n#' @noRd\n.sampling_loss <- function(.pdf, .edf, par, x, y, fn, ..., n=1, scale=0.07,\n        trunc=FALSE) {\n    diffs <- fn(par=par, x=x, ...) - y\n    if (trunc == FALSE) {\n        if (n == 1 && grepl(\"normals\", deparse(substitute(.pdf))))\n            return(sum(diffs^2))\n        return(sum(-log(.pdf(diffs, n, scale))))\n    } else {\n        down_truncated <- abs(y) >= 1\n        up_truncated <- abs(y) <= 0\n        return(\n            sum(-log(.pdf(diffs[!(down_truncated | up_truncated)], n, scale))) +\n            sum(-log(.edf(-diffs[up_truncated | down_truncated], n, scale)))\n        )\n    }\n}\n\n\n#' See docs for `.sampling_loss`\n#' @keywords interal\n#' @noRd\n.normal_loss <- function(par, x, y, fn, ..., n=1, scale=0.07,\n        trunc=FALSE) {\n    .sampling_loss(.pdf=.dmednnormals, .edf=.edmednnormals, par=par, x=x, y=y,\n        fn=fn, ..., n=n, scale=scale, trunc=trunc)\n}\n\n\n\n#' See docs for `.sampling_loss`\n#' @keywords internal\n#' @noRd\n.cauchy_loss <- function(par, x, y, fn, ..., n=1, scale=0.07,\n        trunc=FALSE) {\n    .sampling_loss(.pdf=.dmedncauchys, .edf=.edmedncauchys, par=par, x=x, y=y,\n        fn=fn, ..., n=n, scale=scale, trunc=trunc)\n}\n\n#' @export\n#' @keywords internal\n#' @noRd\n.meshEval2 <- function(density, par, x, y, fn,\n        loss=.normal_loss, lower, upper, ..., loss_args=list()) {\n    # input validation\n    stopifnot(is.function(fn))\n    stopifnot(is.function(loss) || is.character(loss))\n    stopifnot(c(\"par\", \"x\", \"y\", \"fn\") %in% formalArgs(loss))\n    stopifnot(\n        is.null(names(loss_args)) || all(names(loss_args) %in% formalArgs(loss))\n    )\n    # make function amenable to use via optim\n    optim_fn <- if (is_optim_compatible(fn)) {\n        fn\n    } else {\n        make_optim_function(fn, ...)\n    }\n    par_loss <- do.call(loss,\n        args=c(list(par=par, x=x, y=y, fn=optim_fn), loss_args))\n\n    periods <- matrix(NA, nrow = length(par), ncol = 1)\n    names(periods) <- names(par)\n    periods[1] <- 1\n\n    if (length(par) > 1) {\n        for (p in seq(2, length(par))) {\n            ## the par-1 is because we want 1 increment of par variable once\n            ##   all previous variables have their values tested once.\n            periods[p] <- periods[p - 1] * (density[p - 1] *\n                (upper[p - 1] - lower[p - 1]) + 1)\n        }\n    }\n\n    currentPars <- lower\n\n    ## The plus one is because we include endpoints.\n    for (point in seq_len(prod((upper - lower) * density + 1))) {\n\n        test_par_loss <- do.call(loss,\n            c(list(par=currentPars, x=x, y=y, fn=optim_fn), loss_args))\n\n        ## Check for something catastrophic going wrong\n        if (\n            !length(test_par_loss) ||\n            (!is.finite(test_par_loss) && test_par_loss != Inf)\n        ) {\n            stop(paste0(\" Test Guess Loss is: \", test_par_loss, \"\\n\",\n                \"Other Pars:\\n\", \"x: \", paste(x, collapse = \", \"), \"\\n\",\n                \"y: \", paste(y, collapse = \", \"), \"\\n\", \"n: \", n, \"\\n\",\n                \"pars: \", currentPars, \"\\n\", \"scale: \", scale, \"\\n\",\n                \"family : \", family, \"\\n\", \"Trunc \", trunc))\n        }\n        ## save the guess if its an improvement\n        if (test_par_loss < par_loss) {\n            par <- currentPars\n            par_loss <- test_par_loss\n        }\n        ## increment the variable(s) that should be incremented this loop\n        for (p in seq_along(par)) {\n            if (point %% periods[p] == 0) {\n                currentPars[p] <- currentPars[p] + 1 / density[p]\n                if (currentPars[p] > upper[p]) {\n                    currentPars[p] <- lower[p]\n                }\n            }\n        }\n    }\n\n    return(par)\n}\n\n\n#' @export\n#' @keywords internal\n#' @noRd\n.patternSearch2 <- function(span, precision, step, par, par_residual, x, y,\n        lower, upper, fn, loss, ..., loss_args=list()) {\n    # input validation\n    stopifnot(is.function(fn))\n    stopifnot(is.function(loss) || is.character(loss))\n    stopifnot(c(\"par\", \"x\", \"y\", \"fn\") %in% formalArgs(loss))\n    stopifnot(\n        is.null(names(loss_args)) || all(names(loss_args) %in% formalArgs(loss))\n    )\n    # make function amenable to use via optim\n    optim_fn <- if (is_optim_compatible(fn)) {\n        fn\n    } else {\n        make_optim_function(fn, ...)\n    }\n    # setup matrix for searching the parameter space\n    neighbours <- matrix(nrow = 2 * length(par), ncol = length(par))\n    neighbour_loss <- matrix(NA, nrow = 1, ncol = nrow(neighbours))\n\n    while (span > precision) {\n        for (neighbour in seq_len(nrow(neighbours))) {\n            neighbours[neighbour, ] <- par\n            dimension <- ceiling(neighbour / 2)\n            if (neighbour %% 2 == 1) {\n                neighbours[neighbour, dimension] <- pmin(\n                    par[dimension] + span * step[dimension],\n                    upper[dimension]\n                )\n            } else {\n                neighbours[neighbour, dimension] <- pmax(\n                    par[dimension] - span * step[dimension],\n                    lower[dimension]\n                )\n            }\n\n            neighbour_loss[neighbour] <- do.call(loss, args=c(\n                list(par=neighbours[neighbour, ], x=x, y=y, fn=optim_fn),\n                    loss_args))\n        }\n\n        if (min(neighbour_loss) < par_residual) {\n            par <- neighbours[which.min(neighbour_loss)[1], ]\n            par_residual <- min(neighbour_loss)\n        } else {\n            span <- span / 2\n        }\n    }\n    return(par)\n}\n\n\n\n#' Drop parameters from a function and replace them with constants\n#'   inside the function body.\n#'\n#' @param fn `function` A non-primitive function to remove parameters from\n#'   (via `base::formals(fn)`).\n#' @param args `list` A list where names are the function arguments (parameters)\n#'   to remove and the values are the appopriate value to replace the parameter\n#'   with in the function body.\n#'\n#' @return `function` A new non-primitize function with the parameters named in\n#'   `args` deleted and their values fixed with the values from `args` in the\n#'   function body.\n#'\n#' @importFrom compiler cmpfun\n#' @export\ndrop_fn_params <- function(fn, args) {\n    stopifnot(is.function(fn) && !is.primitive(fn))\n    if (length(args) == 0) return(fn)\n    stopifnot(all(names(args) %in% formalArgs(fn)))\n    stopifnot(is.list(args))\n    # Delete the arguments we are deparamterizing\n    formals(fn)[formalArgs(fn) %in% names(args)] <- NULL\n    # Replace the symbols with the new fixed value inside the fuction body\n    deparse_body <- deparse(body(fn))\n    for (i in seq_along(args)) {\n        deparse_body <- gsub(names(args)[i], args[[i]], deparse_body)\n    }\n    # Parse the new function body back to a call\n    body(fn, envir=parent.frame()) <- str2lang(paste0(deparse_body, collapse=\"\\n\"))\n    fn <- compiler::cmpfun(fn)\n    return(fn)\n}\n\n#' Collects all function arguments other than the first into a single list\n#'   parameter.\n#'\n#' @description\n#' Useful for converting a regular function into a function amenable to\n#' optimization via `stats::optim`, which requires all free parameters be\n#' passed as a single vector `par`.\n#'\n#' @details\n#' Takes a function of the form f(x, ...), where ... is any number of additional\n#'   function parameters (bot not literal `...`!) and parses it to a function of\n#'   the form f(par, x) where `par` is a vector of values for ... in\n#'   the same order as the arguments appear in `fn`.\n#'\n#' @param fn `function` A non-primitive function to refactor such that the first\n#'   argument becomes the second argument and all other parameters must be\n#'   passed as a vector to the first argument of the new function via the `par`\n#'   parameter.\n#'\n#' @return `function` A new non-primitive function where the first argument is\n#'   `par`, which takes a vector of parameters being optimized, and the\n#'   second argument is the old first argument to `fn` (usually `x` since this\n#'   is the independent variable to optimize the function over).\n#'\n#' @export\ncollect_fn_params <- function(fn) {\n    stopifnot(is.function(fn) && !is.primitive(fn))\n    # Capture the current formal args\n    formal_args <- formalArgs(fn)\n    if (\"...\" %in% formalArgs(fn))\n        stop(\"No support for fn with ... in signature!\")\n    # Replace args other than the first with a list\n    args <- paste0(\"par[[\", seq_along(formal_args[-1]), \"]]\") |>\n        as.list() |>\n        setNames(formal_args[-1])\n    fn <- drop_fn_params(fn, args=args)\n    # Add the `par` list to the formal args of the function\n    formals(fn) <- c(alist(par=), formals(fn))\n    return(fn)\n}\n\n\n#' Takes a non-primitive R function and refactors it to be compatible with\n#'   optimization via `stats::optim`.\n#'\n#' @description Takes a non-primitive R function and refactors it to be compatible with optimization via `stats::optim`.\n#'\n#'\n#' @param fn `function` A non-primitive function\n#' @param ... Arguments to `fn` to fix for before building the\n#'   function to be optimized. Useful for reducing the number of free parameters\n#'   in an optimization if there are insufficient degrees of freedom.\n#'\n#' @seealso [`drop_fn_params`], [`collect_fn_params`]\n#'\n#' @export\nmake_optim_function <- function(fn, ...) {\n    # NOTE: error handling done inside helper methods!\n    fn1 <- drop_fn_params(fn, args=list(...))\n    fn2 <- collect_fn_params(fn1)\n    return(fn2)\n}\n\n\n#' Check whether a function signature is amenable to optimization via `stats::optim`.\n#'\n#' @description\n#' Functions compatible with `optim` have the parameter named `par` as their\n#' first formal argument where each value is a respective free parameter to\n#' be optimized.\n#'\n#' @param fn `function` A non-primitive function.\n#'\n#' @return `logical(1)` `TRUE` if the first value of `formalArg(fn)` is \"par\",\n#'   otherwise `FALSE`.\n#'\n#' @export\nis_optim_compatible <- function(fn) formalArgs(fn)[1] == \"par\"\n\n\n\n\n\n\n# ======================\n# ==== Deprecating =====\n\n#' .fitCurve\n#'\n#' Curve optimization from 1 variable to 1 variable, using L-BFSG-B from optim,\n#' with fallback to pattern search if optimization fails to converge.\n#'\n#' @param x `numeric` input/x values for function\n#' @param y `numeric` output/y values for function\n#' @param f `function` function f, parameterized by parameters to optimize\n#' @param density `numeric` how many points in the dimension of each parameter\n#'   should be evaluated (density of the grid)\n#' @param step initial step size for pattern search.\n#' @param precision `numeric` smallest step size used in pattern search, once\n#'   step size drops below this value, the search terminates.\n#' @param lower_bounds `numeric` lower bounds for the paramater search space\n#' @param upper_bounds `numeric` upper bounds for the parameter search space\n#' @param median_n `integer` number of technical replicates per measured point\n#'   in x. Used to evaluate the proper median distribution for the normal and\n#'   cauchy error models\n#' @param scale `numeric` scale on which to measure probability for the error\n#'   model (roughly SD of error)\n#' @param family `character` which error family to use. Currently, \"normal\"\n#'   and \"Cauchy\" are implemented\n#' @param trunc `logical` Whether or not to truncate the values at 100% (1.0)\n#' @param verbose `logical` should diagnostic messages be printed?\n#' @param gritty_guess `numeric` intitial, uninformed guess on parameter\n#'   values (usually heuristic)\n#' @param span ['numeric'] can be safely kept at 1, multiplicative ratio for\n#'   initial step size in pattern search. Must be larger than precision.\n#'\n#' @keywords internal\n#' @noRd\n#'\n#' @importFrom stats optim var\n#' @export\n.fitCurve <- function(x, y, f, density, step, precision, lower_bounds,\n    upper_bounds, scale, family, median_n, trunc, verbose, gritty_guess,\n    span = 1) {\n\n    guess <- tryCatch(optim(par = gritty_guess, fn = function(t) {\n        .residual(\n            x = x, y = y, n = median_n, pars = t, f = f,\n            scale = scale, family = family, trunc = trunc\n        )\n    }, lower = lower_bounds, upper = upper_bounds, control = list(\n        factr = 1e-08,\n        ndeps = rep(1e-4, times = length(gritty_guess)),\n        trace = 0\n    ), method = \"L-BFGS-B\"), error = function(e) {\n        list(par = gritty_guess, convergence = -1)\n    })\n\n    failed <- guess[[\"convergence\"]] != 0\n    guess <- guess[[\"par\"]]\n\n    guess_residual <- .residual(x = x, y = y, n = median_n, pars = guess, f = f, scale = scale, family = family, trunc = trunc)\n    gritty_guess_residual <- .residual(x = x, y = y, n = median_n, pars = gritty_guess, f = f, scale = scale, family = family, trunc = trunc)\n\n    if (failed || any(is.na(guess)) || guess_residual >= gritty_guess_residual) {\n        guess <- .meshEval(x = x, y = y, f = f, guess = gritty_guess, lower_bounds = lower_bounds, upper_bounds = upper_bounds, density = density,\n            n = median_n, scale = scale, family = family, trunc = trunc)\n        guess_residual <- .residual(x = x, y = y, n = median_n, pars = guess, f = f, scale = scale, family = family, trunc = trunc)\n\n        guess <- .patternSearch(x = x, y = y, f = f, guess = guess, n = median_n, guess_residual = guess_residual, lower_bounds = lower_bounds,\n            upper_bounds = upper_bounds, span = span, precision = precision, step = step, scale = scale, family = family, trunc = trunc)\n    }\n\n    y_hat <- do.call(f, list(x, par=guess))\n\n    Rsqr <- 1 - (var(y - y_hat) / var(y))\n    attr(guess, \"Rsquare\") <- Rsqr\n\n    return(guess)\n}\n\n\n\n# meshEval ----------------------------------------------------------------\n#' meshEval\n#'\n#' Generate an initial guess for dose-response curve parameters by evaluating\n#' the residuals at different lattice points of the search space\n#'\n#' @export\n#' @keywords internal\n#' @noRd\n# ##FIXME:: Why is this different in PharmacoGx?\n.meshEval <- function(x, y, f, guess, lower_bounds, upper_bounds, density, n,\n        scale, family, trunc) {\n    pars <- NULL\n    guess_residual <- .residual(x = x, y = y, n = n, pars = guess, f = f,\n        scale = scale, family = family, trunc = trunc)\n\n    periods <- matrix(NA, nrow = length(guess), ncol = 1)\n    names(periods) <- names(guess)\n    periods[1] <- 1\n\n    if (length(guess) > 1) {\n        for (par in 2:length(guess)) {\n            ## the par-1 is because we want 1 increment of par variable once all previous variables have their values tested once.\n            periods[par] <- periods[par - 1] * (density[par - 1] * (upper_bounds[par - 1] - lower_bounds[par - 1]) + 1)\n        }\n    }\n\n    currentPars <- lower_bounds\n\n    ## The plus one is because we include endpoints.\n    for (point in seq_len(prod((upper_bounds - lower_bounds) * density + 1))) {\n\n        test_guess_residual <- .residual(x = x, y = y, n = n, pars = currentPars, f = f, scale = scale, family = family, trunc = trunc)\n\n        ## Check for something catastrophic going wrong\n        if (!length(test_guess_residual) || (!is.finite(test_guess_residual) && test_guess_residual != Inf)) {\n            stop(paste0(\" Test Guess Residual is: \", test_guess_residual, \"\\n\", \"Other Pars:\\n\", \"x: \", paste(x, collapse = \", \"), \"\\n\",\n                \"y: \", paste(y, collapse = \", \"), \"\\n\", \"n: \", n, \"\\n\", \"pars: \", pars, \"\\n\", \"scale: \", scale, \"\\n\", \"family : \", family,\n                \"\\n\", \"Trunc \", trunc))\n        }\n        ## save the guess if its an improvement\n        if (test_guess_residual < guess_residual) {\n            guess <- currentPars\n            guess_residual <- test_guess_residual\n        }\n        ## increment the variable(s) that should be incremented this loop\n        for (par in seq_along(guess)) {\n            if (point%%periods[par] == 0) {\n                currentPars[par] <- currentPars[par] + 1/density[par]\n\n                if (currentPars[par] > upper_bounds[par]) {\n                    currentPars[par] <- lower_bounds[par]\n                }\n            }\n        }\n    }\n\n    return(guess)\n}\n\n#' @export\n#' @keywords internal\n#' @noRd\n.patternSearch <- function(x, y, f, guess, n, guess_residual, lower_bounds,\n        upper_bounds, span, precision, step, scale, family, trunc) {\n    neighbours <- matrix(nrow = 2 * length(guess), ncol = length(guess))\n    neighbour_residuals <- matrix(NA, nrow = 1, ncol = nrow(neighbours))\n\n    while (span > precision) {\n        for (neighbour in seq_len(nrow(neighbours))) {\n            neighbours[neighbour, ] <- guess\n            dimension <- ceiling(neighbour / 2)\n            if (neighbour %% 2 == 1) {\n                neighbours[neighbour, dimension] <- pmin(\n                    guess[dimension] + span * step[dimension],\n                    upper_bounds[dimension]\n                )\n            } else {\n                neighbours[neighbour, dimension] <- pmax(\n                    guess[dimension] - span * step[dimension],\n                    lower_bounds[dimension]\n                )\n            }\n\n            neighbour_residuals[neighbour] <- .residual(x = x, y = y,\n                f = f, pars = neighbours[neighbour, ], n = n, scale = scale,\n                family = family,\n                trunc = trunc)\n        }\n\n        if (min(neighbour_residuals) < guess_residual) {\n            guess <- neighbours[which.min(neighbour_residuals)[1], ]\n            guess_residual <- min(neighbour_residuals)\n        } else {\n            span <- span / 2\n        }\n    }\n    return(guess)\n}\n\n## TODO:: Write documentation\n## FIXME:: Why is this different from PharmacoGx?\n#' @title Residual calculation\n#'\n#' @return A \\code{numeric} containing the estimated residuals for the model\n#'   fit\n#'\n#' @export\n#' @keywords internal\n#' @noRd\n.residual <- function(x, y, n, pars, f, scale = 0.07, family = c(\"normal\", \"Cauchy\"), trunc = FALSE) {\n    family <- match.arg(family)\n    diffs <- f(x, par=pars) - y\n\n    if (family != \"Cauchy\") {\n        if (trunc == FALSE) {\n            if (n == 1) {\n                return(sum(diffs^2))\n            }\n            return(sum(-log(.dmednnormals(diffs, n, scale))))\n        } else {\n            down_truncated <- abs(y) >= 1\n            up_truncated <- abs(y) <= 0\n            return(sum(-log(.dmednnormals(diffs[!(down_truncated | up_truncated)], n, scale))) + sum(-log(.edmednnormals(-diffs[up_truncated |\n                down_truncated], n, scale))))\n        }\n    } else {\n        if (trunc == FALSE) {\n            return(sum(-log(.dmedncauchys(diffs, n, scale))))\n        } else {\n            down_truncated <- abs(y) >= 1\n            up_truncated <- abs(y) <= 0\n            return(sum(-log(.dmedncauchys(diffs[!(down_truncated | up_truncated)], n, scale))) + sum(-log(.edmedncauchys(-diffs[up_truncated |\n                down_truncated], n, scale))))\n        }\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.fitCurve2` function and how does it handle optimization failures?",
        "answer": "The `.fitCurve2` function is designed to fit a curve using `stats::optim` with L-BFGS-B optimization. If convergence is not achieved, it falls back to grid/pattern search methods. It handles optimization failures by first attempting optimization with `optim`, and if that fails (convergence != 0), it uses `.meshEval2` for a grid search, followed by `.patternSearch2` for fine-tuning. This approach ensures robustness in curve fitting even when the initial optimization fails."
      },
      {
        "question": "How does the `make_optim_function` helper function transform a regular R function into one compatible with `stats::optim`?",
        "answer": "The `make_optim_function` helper transforms a regular R function into one compatible with `stats::optim` by performing two main steps: 1) It uses `drop_fn_params` to fix any constant parameters specified in the `...` argument. 2) It then applies `collect_fn_params` to restructure the function so that all remaining parameters are collected into a single `par` argument, which becomes the first argument of the new function. This transformation allows the function to be used with `optim`, which expects all parameters to be optimized to be passed as a single vector."
      },
      {
        "question": "What is the purpose of the `.sampling_loss` function and how does it relate to the `.normal_loss` and `.cauchy_loss` functions?",
        "answer": "The `.sampling_loss` function computes the loss using the expectation of the likelihood of the median for N samples from a probability density function. It's a generic function that can work with different probability distributions. The `.normal_loss` and `.cauchy_loss` functions are specific implementations of `.sampling_loss` for normal and Cauchy distributions respectively. They call `.sampling_loss` with the appropriate probability density function (`.dmednnormals` or `.dmedncauchys`) and expected likelihood function (`.edmednnormals` or `.edmedncauchys`) for their respective distributions. This design allows for flexible loss calculation with different error models."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Curve fitting via `stats::optim` L-BFGS-B with fall-back grid/pattern search\n#' if convergence is not achieved.\n#'\n#' @param par `numeric` Vector of intial guesses for the parameters.\n#' @param x `numeric` Values to evaluate `fn` for.\n#' @param y `numeric` Target output values to optimze `fn` against.\n#' @param fn `function` A function to optimize.\n#' @param loss `character(1)` or `function` Loss function to compute.\n#' @param lower `numeric(1)` Lower bound for parameters.\n#' @param upper `numeric(1)` Upper bound for paramteres.\n#' @param precision `numeric` smallest step size used in pattern search.\n#' @param density `numeric` how many points in the dimension of each parameter.\n#' @param step initial step size for pattern search.\n#' @param ... `pairlist` Fall through arguments to `fn`.\n#' @param loss_args `list` Additional argument to the `loss` function.\n#' @param span `numeric` Multiplicative ratio for initial step size.\n#' @param optim_only `logical(1)` Should fall back methods be skipped?\n#' @param control `list` List of control parameters to pass to `optim`.\n#'\n#' @return `numeric` Vector of optimal parameters for `fn` fit against `y`\n#'   on the values of `x`.\n#'\n#' @importFrom stats optim\n#' @export\n.fitCurve2 <- function(par, x, y, fn, loss, lower=-Inf, upper=Inf,\n        precision=1e-4, density=c(2, 10, 5), step= 0.5 / density, ...,\n        loss_args=list(), span=1, optim_only=FALSE,\n        control=list(factr=1e-08, ndeps=rep(1e-4, times=length(par)), trace = 0)\n        ) {\n    # Input validation\n    stopifnot(is.function(fn))\n    stopifnot(is.function(loss) || is.character(loss))\n    stopifnot(c(\"par\", \"x\", \"y\", \"fn\") %in% formalArgs(loss))\n    stopifnot(\n        is.null(names(loss_args)) || all(names(loss_args) %in% formalArgs(loss))\n    )\n    stopifnot(\n        (length(par) == length(upper) && length(par) == length(lower)) ||\n        (length(upper) == 1 && length(lower) == 1)\n    )\n    stopifnot(span > precision)\n\n    # Make function compatible with optim\n    optim_fn <- if (is_optim_compatible(fn)) {\n        fn\n    } else {\n        make_optim_function(fn, ...)\n    }\n\n    # Perform optimization\n    guess <- optim(\n        par=par,\n        fn=function(p)\n            do.call(loss,\n                args=c(\n                    list(par=p, x=x, y=y, fn=optim_fn),\n                    loss_args\n                )\n            ),\n        upper=upper,\n        lower=lower,\n        control=control,\n        method=\"L-BFGS-B\"\n    )\n\n    # Check for convergence\n    failed <- guess[\"convergence\"] != 0\n    if (failed) guess <- list(par=par, convergence=(-1))\n    guess <- guess[\"par\"]\n\n    # Calculate residuals\n    guess_residual <- do.call(loss,\n        args=c(list(par=guess, x=x, y=y, fn=optim_fn), loss_args))\n    gritty_guess_residual <- do.call(loss,\n        args=c(list(par=par, x=x, y=y, fn=optim_fn), loss_args))\n\n    # Fallback methods if optimization fails\n    if (\n        (failed || any(is.na(guess)) || guess_residual >= gritty_guess_residual)\n        && !optim_only\n    ) {\n        # Code for fallback methods goes here\n    }\n\n    # Calculate R-squared\n    y_hat <- optim_fn(par=guess, x=x)\n    Rsqr <- 1 - (var(y - y_hat) / var(y))\n    attr(guess, \"Rsquare\") <- Rsqr\n\n    return(guess)\n}",
        "complete": "# Curve fitting via `stats::optim` L-BFGS-B with fall-back grid/pattern search\n#' if convergence is not achieved.\n#'\n#' @param par `numeric` Vector of intial guesses for the parameters.\n#' @param x `numeric` Values to evaluate `fn` for.\n#' @param y `numeric` Target output values to optimze `fn` against.\n#' @param fn `function` A function to optimize.\n#' @param loss `character(1)` or `function` Loss function to compute.\n#' @param lower `numeric(1)` Lower bound for parameters.\n#' @param upper `numeric(1)` Upper bound for paramteres.\n#' @param precision `numeric` smallest step size used in pattern search.\n#' @param density `numeric` how many points in the dimension of each parameter.\n#' @param step initial step size for pattern search.\n#' @param ... `pairlist` Fall through arguments to `fn`.\n#' @param loss_args `list` Additional argument to the `loss` function.\n#' @param span `numeric` Multiplicative ratio for initial step size.\n#' @param optim_only `logical(1)` Should fall back methods be skipped?\n#' @param control `list` List of control parameters to pass to `optim`.\n#'\n#' @return `numeric` Vector of optimal parameters for `fn` fit against `y`\n#'   on the values of `x`.\n#'\n#' @importFrom stats optim\n#' @export\n.fitCurve2 <- function(par, x, y, fn, loss, lower=-Inf, upper=Inf,\n        precision=1e-4, density=c(2, 10, 5), step= 0.5 / density, ...,\n        loss_args=list(), span=1, optim_only=FALSE,\n        control=list(factr=1e-08, ndeps=rep(1e-4, times=length(par)), trace = 0)\n        ) {\n    # Input validation\n    stopifnot(is.function(fn))\n    stopifnot(is.function(loss) || is.character(loss))\n    stopifnot(c(\"par\", \"x\", \"y\", \"fn\") %in% formalArgs(loss))\n    stopifnot(\n        is.null(names(loss_args)) || all(names(loss_args) %in% formalArgs(loss))\n    )\n    stopifnot(\n        (length(par) == length(upper) && length(par) == length(lower)) ||\n        (length(upper) == 1 && length(lower) == 1)\n    )\n    stopifnot(span > precision)\n\n    # Make function compatible with optim\n    optim_fn <- if (is_optim_compatible(fn)) {\n        fn\n    } else {\n        make_optim_function(fn, ...)\n    }\n\n    # Perform optimization\n    guess <- optim(\n        par=par,\n        fn=function(p)\n            do.call(loss,\n                args=c(\n                    list(par=p, x=x, y=y, fn=optim_fn),\n                    loss_args\n                )\n            ),\n        upper=upper,\n        lower=lower,\n        control=control,\n        method=\"L-BFGS-B\"\n    )\n\n    # Check for convergence\n    failed <- guess[\"convergence\"] != 0\n    if (failed) guess <- list(par=par, convergence=(-1))\n    guess <- guess[\"par\"]\n\n    # Calculate residuals\n    guess_residual <- do.call(loss,\n        args=c(list(par=guess, x=x, y=y, fn=optim_fn), loss_args))\n    gritty_guess_residual <- do.call(loss,\n        args=c(list(par=par, x=x, y=y, fn=optim_fn), loss_args))\n\n    # Fallback methods if optimization fails\n    if (\n        (failed || any(is.na(guess)) || guess_residual >= gritty_guess_residual)\n        && !optim_only\n    ) {\n        guess <- .meshEval2(density=density, par=guess, x=x, y=y, fn=optim_fn,\n            lower=lower, upper=upper, ..., loss=loss,loss_args=loss_args)\n        guess_residual <- do.call(loss,\n            args=c(list(par=guess, x=x, y=y, fn=optim_fn), loss_args))\n\n        guess <- .patternSearch2(span=span, precision=precision, step=step,\n            par=guess, par_residual=guess_residual, x=x, y=y, fn=optim_fn,\n            loss=loss, lower=lower, upper=upper, ..., loss_args=loss_args)\n    }\n\n    # Calculate R-squared\n    y_hat <- optim_fn(par=guess, x=x)\n    Rsqr <- 1 - (var(y - y_hat) / var(y))\n    attr(guess, \"Rsquare\") <- Rsqr\n\n    return(guess)\n}"
      },
      {
        "partial": "#' @export\n#' @keywords internal\n#' @noRd\n.meshEval2 <- function(density, par, x, y, fn,\n        loss=.normal_loss, lower, upper, ..., loss_args=list()) {\n    # input validation\n    stopifnot(is.function(fn))\n    stopifnot(is.function(loss) || is.character(loss))\n    stopifnot(c(\"par\", \"x\", \"y\", \"fn\") %in% formalArgs(loss))\n    stopifnot(\n        is.null(names(loss_args)) || all(names(loss_args) %in% formalArgs(loss))\n    )\n    # make function amenable to use via optim\n    optim_fn <- if (is_optim_compatible(fn)) {\n        fn\n    } else {\n        make_optim_function(fn, ...)\n    }\n    par_loss <- do.call(loss,\n        args=c(list(par=par, x=x, y=y, fn=optim_fn), loss_args))\n\n    periods <- matrix(NA, nrow = length(par), ncol = 1)\n    names(periods) <- names(par)\n    periods[1] <- 1\n\n    if (length(par) > 1) {\n        for (p in seq(2, length(par))) {\n            periods[p] <- periods[p - 1] * (density[p - 1] *\n                (upper[p - 1] - lower[p - 1]) + 1)\n        }\n    }\n\n    currentPars <- lower\n\n    for (point in seq_len(prod((upper - lower) * density + 1))) {\n        test_par_loss <- do.call(loss,\n            c(list(par=currentPars, x=x, y=y, fn=optim_fn), loss_args))\n\n        if (\n            !length(test_par_loss) ||\n            (!is.finite(test_par_loss) && test_par_loss != Inf)\n        ) {\n            stop(paste0(\"Test Guess Loss is: \", test_par_loss, \"\\n\",\n                \"Other Pars:\\n\", \"x: \", paste(x, collapse = \", \"), \"\\n\",\n                \"y: \", paste(y, collapse = \", \"), \"\\n\", \"n: \", n, \"\\n\",\n                \"pars: \", currentPars, \"\\n\", \"scale: \", scale, \"\\n\",\n                \"family : \", family, \"\\n\", \"Trunc \", trunc))\n        }\n        if (test_par_loss < par_loss) {\n            par <- currentPars\n            par_loss <- test_par_loss\n        }\n        for (p in seq_along(par)) {\n            if (point %% periods[p] == 0) {\n                currentPars[p] <- currentPars[p] + 1 / density[p]\n                if (currentPars[p] > upper[p]) {\n                    currentPars[p] <- lower[p]\n                }\n            }\n        }\n    }\n\n    return(par)\n}",
        "complete": "#' @export\n#' @keywords internal\n#' @noRd\n.meshEval2 <- function(density, par, x, y, fn,\n        loss=.normal_loss, lower, upper, ..., loss_args=list())"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/utils-iteration.R",
    "language": "R",
    "content": "#' @title lapply\n#' lapply method for `MultiAssayExperiment`\n#'\n#' @param X A `MultiAssayExperiment` object.\n#' @param FUN A function to be applied to each `SummarizedExperiment` in a\n#' in `X`.\n#' @param ... Fall through parameters to `FUN`\n#'\n#' @return A `MultiAssayExperiment` object, modifed such that\n#' `experiments(X) <- endoapply(experiments(X), FUN, ...)`.s\n#'\n#' @importMethodsFrom BiocGenerics lapply\n#' @importFrom S4Vectors endoapply\n#' @importFrom MultiAssayExperiment experiments experiments<-\n#' @exportMethod lapply\nsetMethod('lapply', signature(X='MultiAssayExperiment'),\n        function(X, FUN, ...) {\n    experiments(X) <- endoapply(experiments(X), FUN, ...)\n    return(X)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `lapply` method for `MultiAssayExperiment` objects?",
        "answer": "The `lapply` method for `MultiAssayExperiment` objects applies a given function `FUN` to each `SummarizedExperiment` in the `MultiAssayExperiment` object. It modifies the original object by updating its experiments using the `endoapply` function and returns the modified object."
      },
      {
        "question": "How does this implementation handle additional parameters passed to the `FUN` function?",
        "answer": "The implementation uses the `...` (ellipsis) argument to pass any additional parameters to the `FUN` function. These additional parameters are forwarded to both the `endoapply` function and the `FUN` function, allowing for flexible function calls."
      },
      {
        "question": "What is the significance of the `@importMethodsFrom`, `@importFrom`, and `@exportMethod` annotations in this code?",
        "answer": "These annotations are used for package documentation and management. `@importMethodsFrom` imports specific methods from other packages, `@importFrom` imports functions from other packages, and `@exportMethod` indicates that this method should be exported and made available to users of the package. They help manage dependencies and control which functions are visible to users."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @title lapply\n#' lapply method for `MultiAssayExperiment`\n#'\n#' @param X A `MultiAssayExperiment` object.\n#' @param FUN A function to be applied to each `SummarizedExperiment` in a\n#' in `X`.\n#' @param ... Fall through parameters to `FUN`\n#'\n#' @return A `MultiAssayExperiment` object, modifed such that\n#' `experiments(X) <- endoapply(experiments(X), FUN, ...)`.\n#'\n#' @importMethodsFrom BiocGenerics lapply\n#' @importFrom S4Vectors endoapply\n#' @importFrom MultiAssayExperiment experiments experiments<-\n#' @exportMethod lapply\nsetMethod('lapply', signature(X='MultiAssayExperiment'),\n        function(X, FUN, ...) {\n    # Complete the function body\n})",
        "complete": "#' @title lapply\n#' lapply method for `MultiAssayExperiment`\n#'\n#' @param X A `MultiAssayExperiment` object.\n#' @param FUN A function to be applied to each `SummarizedExperiment` in a\n#' in `X`.\n#' @param ... Fall through parameters to `FUN`\n#'\n#' @return A `MultiAssayExperiment` object, modifed such that\n#' `experiments(X) <- endoapply(experiments(X), FUN, ...)`.\n#'\n#' @importMethodsFrom BiocGenerics lapply\n#' @importFrom S4Vectors endoapply\n#' @importFrom MultiAssayExperiment experiments experiments<-\n#' @exportMethod lapply\nsetMethod('lapply', signature(X='MultiAssayExperiment'),\n        function(X, FUN, ...) {\n    experiments(X) <- endoapply(experiments(X), FUN, ...)\n    X\n})"
      },
      {
        "partial": "#' @title lapply\n#' lapply method for `MultiAssayExperiment`\n#'\n#' @param X A `MultiAssayExperiment` object.\n#' @param FUN A function to be applied to each `SummarizedExperiment` in a\n#' in `X`.\n#' @param ... Fall through parameters to `FUN`\n#'\n#' @return A `MultiAssayExperiment` object, modifed such that\n#' `experiments(X) <- endoapply(experiments(X), FUN, ...)`.\n#'\n#' @importMethodsFrom BiocGenerics lapply\n#' @importFrom S4Vectors endoapply\n#' @importFrom MultiAssayExperiment experiments experiments<-\n#' @exportMethod lapply\nsetMethod('lapply', signature(X='MultiAssayExperiment'),\n        function(X, FUN, ...) {\n    # Complete the function body in a single line\n})",
        "complete": "#' @title lapply\n#' lapply method for `MultiAssayExperiment`\n#'\n#' @param X A `MultiAssayExperiment` object.\n#' @param FUN A function to be applied to each `SummarizedExperiment` in a\n#' in `X`.\n#' @param ... Fall through parameters to `FUN`\n#'\n#' @return A `MultiAssayExperiment` object, modifed such that\n#' `experiments(X) <- endoapply(experiments(X), FUN, ...)`.\n#'\n#' @importMethodsFrom BiocGenerics lapply\n#' @importFrom S4Vectors endoapply\n#' @importFrom MultiAssayExperiment experiments experiments<-\n#' @exportMethod lapply\nsetMethod('lapply', signature(X='MultiAssayExperiment'),\n        function(X, FUN, ...) {\n    `experiments<-`(X, endoapply(experiments(X), FUN, ...))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/computeSensitivity.R",
    "language": "R",
    "content": "## This function computes pars/AUC/SF2/D10 for the whole raw sensitivity data of a rset\n#' @importFrom BiocParallel bplapply\n.calculateFromRaw <- function(raw.sensitivity,\n                              trunc=TRUE,\n                              nthread=1,\n                              family=c(\"normal\", \"Cauchy\"),\n                              scale = 5,\n                              n = 1)\n  {\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n\n  family <- match.arg(family)\n\n  AUC <- vector(length=dim(raw.sensitivity)[1])\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n  SF2 <- vector(length=dim(raw.sensitivity)[1])\n  names(SF2) <- dimnames(raw.sensitivity)[[1]]\n\n  D10 <- vector(length=dim(raw.sensitivity)[1])\n  names(D10) <- dimnames(raw.sensitivity)[[1]]\n\n  if (nthread ==1){\n    pars <- lapply(names(AUC), function(exp, raw.sensitivity, family, scale, n) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else {\n        return(unlist(linearQuadraticModel(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Response\"], trunc=trunc, family=family, scale=scale, median_n=n)))\n      }\n    },raw.sensitivity=raw.sensitivity, family = family, scale = scale, n = n)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(lapply(names(pars), function(exp,raw.sensitivity, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else {\n        computeAUC(D=raw.sensitivity[exp, , \"Dose\"], pars=pars[[exp]], trunc=trunc)\n      }\n    },raw.sensitivity=raw.sensitivity, pars=pars))\n    SF2 <- unlist(lapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else {\n        computeSF2(pars=pars[[exp]])\n      }\n    }, pars=pars))\n    D10 <- unlist(lapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else {\n        computeD10(pars=pars[[exp]])\n      }\n    }, pars=pars))\n\n  } else {\n    pars <- BiocParallel::bplapply(names(AUC), function(exp, raw.sensitivity, family, scale, n, trunc) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else {\n        linearQuadraticModel(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Response\"], trunc=trunc, family=family, scale=scale, median_n=n)\n      }\n    }, raw.sensitivity=raw.sensitivity, family = family, scale = scale, n = n, trunc = trunc)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(BiocParallel::bplapply(names(pars), function(exp, raw.sensitivity, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else {\n        computeAUC(D=raw.sensitivity[exp, , \"Dose\"], pars=pars[[exp]], trunc=trunc)\n      }\n    },raw.sensitivity=raw.sensitivity, pars=pars, trunc = trunc))\n    SF2 <- unlist(BiocParallel::bplapply(names(pars), function(exp, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else {\n        computeSF2(pars=pars[[exp]])\n      }\n    }, pars=pars, trunc = trunc))\n    D10 <- unlist(BiocParallel::bplapply(names(pars), function(exp, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else {\n        computeD10(pars=pars[[exp]])\n      }\n    }, pars=pars, trunc = trunc))\n  }\n\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n  names(SF2) <- dimnames(raw.sensitivity)[[1]]\n  names(D10) <- dimnames(raw.sensitivity)[[1]]\n\n  alpha <- vapply(pars, function(x) return(x[1]), numeric(1))\n  beta <- vapply(pars, function(x) return(x[2]), numeric(1))\n\n\n  return(list(\"AUC\"=AUC, \"SF2\"=SF2, \"D10\"=D10 ,\"alpha\"=alpha, \"beta\"=beta))\n}\n\nupdateMaxConc <- function(rSet) {\n  sensitivityInfo(rSet)$max.conc <- apply(sensitivityRaw(rSet)[,,\"Dose\"], 1, max, na.rm=TRUE)\n  return(rSet)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.calculateFromRaw` function and what are its main outputs?",
        "answer": "The `.calculateFromRaw` function computes various radiobiological parameters from raw sensitivity data. Its main outputs are AUC (Area Under the Curve), SF2 (Survival Fraction at 2 Gy), D10 (Dose for 10% survival), alpha, and beta parameters. These are returned as a list containing vectors for each parameter."
      },
      {
        "question": "How does the function handle parallel processing, and what condition determines whether parallel processing is used?",
        "answer": "The function uses parallel processing when `nthread` is greater than 1. It utilizes the `BiocParallel::bplapply` function for parallel computation. When `nthread` is 1, it uses regular `lapply` for sequential processing. The parallel processing is applied to the calculation of pars, AUC, SF2, and D10 values."
      },
      {
        "question": "What is the purpose of the `updateMaxConc` function and how does it modify the input `rSet`?",
        "answer": "The `updateMaxConc` function updates the maximum concentration information in the sensitivity data of the input `rSet`. It calculates the maximum dose for each experiment in the raw sensitivity data and stores this information in the `sensitivityInfo` slot of the `rSet` object under the `max.conc` field. The function then returns the updated `rSet`."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".calculateFromRaw <- function(raw.sensitivity, trunc=TRUE, nthread=1, family=c(\"normal\", \"Cauchy\"), scale = 5, n = 1) {\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  family <- match.arg(family)\n\n  AUC <- vector(length=dim(raw.sensitivity)[1])\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n  SF2 <- vector(length=dim(raw.sensitivity)[1])\n  names(SF2) <- dimnames(raw.sensitivity)[[1]]\n\n  D10 <- vector(length=dim(raw.sensitivity)[1])\n  names(D10) <- dimnames(raw.sensitivity)[[1]]\n\n  if (nthread == 1) {\n    # Complete the single-threaded computation here\n  } else {\n    # Complete the multi-threaded computation here\n  }\n\n  # Complete the function\n}",
        "complete": ".calculateFromRaw <- function(raw.sensitivity, trunc=TRUE, nthread=1, family=c(\"normal\", \"Cauchy\"), scale = 5, n = 1) {\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  family <- match.arg(family)\n\n  AUC <- vector(length=dim(raw.sensitivity)[1])\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n  SF2 <- vector(length=dim(raw.sensitivity)[1])\n  names(SF2) <- dimnames(raw.sensitivity)[[1]]\n\n  D10 <- vector(length=dim(raw.sensitivity)[1])\n  names(D10) <- dimnames(raw.sensitivity)[[1]]\n\n  if (nthread == 1) {\n    pars <- lapply(names(AUC), function(exp, raw.sensitivity, family, scale, n) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else {\n        unlist(linearQuadraticModel(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Response\"], trunc=trunc, family=family, scale=scale, median_n=n))\n      }\n    }, raw.sensitivity=raw.sensitivity, family=family, scale=scale, n=n)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(lapply(names(pars), function(exp, raw.sensitivity, pars) {\n      if(any(is.na(pars[[exp]]))) NA else computeAUC(D=raw.sensitivity[exp, , \"Dose\"], pars=pars[[exp]], trunc=trunc)\n    }, raw.sensitivity=raw.sensitivity, pars=pars))\n    SF2 <- unlist(lapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) NA else computeSF2(pars=pars[[exp]])\n    }, pars=pars))\n    D10 <- unlist(lapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) NA else computeD10(pars=pars[[exp]])\n    }, pars=pars))\n  } else {\n    pars <- BiocParallel::bplapply(names(AUC), function(exp, raw.sensitivity, family, scale, n, trunc) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else {\n        linearQuadraticModel(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Response\"], trunc=trunc, family=family, scale=scale, median_n=n)\n      }\n    }, raw.sensitivity=raw.sensitivity, family=family, scale=scale, n=n, trunc=trunc)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(BiocParallel::bplapply(names(pars), function(exp, raw.sensitivity, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) NA else computeAUC(D=raw.sensitivity[exp, , \"Dose\"], pars=pars[[exp]], trunc=trunc)\n    }, raw.sensitivity=raw.sensitivity, pars=pars, trunc=trunc))\n    SF2 <- unlist(BiocParallel::bplapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) NA else computeSF2(pars=pars[[exp]])\n    }, pars=pars))\n    D10 <- unlist(BiocParallel::bplapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) NA else computeD10(pars=pars[[exp]])\n    }, pars=pars))\n  }\n\n  names(AUC) <- names(SF2) <- names(D10) <- dimnames(raw.sensitivity)[[1]]\n  alpha <- vapply(pars, function(x) x[1], numeric(1))\n  beta <- vapply(pars, function(x) x[2], numeric(1))\n\n  list(AUC=AUC, SF2=SF2, D10=D10, alpha=alpha, beta=beta)\n}"
      },
      {
        "partial": "updateMaxConc <- function(rSet) {\n  # Complete the function to update the maximum concentration\n}",
        "complete": "updateMaxConc <- function(rSet) {\n  sensitivityInfo(rSet)$max.conc <- apply(sensitivityRaw(rSet)[,,'Dose'], 1, max, na.rm=TRUE)\n  rSet\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/OERmodel.R",
    "language": "R",
    "content": "# OER function for a given hypoxic concentration by the user\n#\n# @param oxygenConc \\code{numeric} An oxygen concentratio between 0.1 and 10\n#\n# @return \\code{none} Generates a dose response curve for a given hypoxic\n#   concentration\n#\nOERmodel <- function(oxygenConc) {\n\n  pO2 <- oxygenConc\n  pO2 <- as.numeric(pO2)\n  if (is.na(pO2)) {\n    stop(\"Error: oxygenConc is NA!\")}\n\n  if (pO2<0.1 | pO2>10){\n    stop(\"Please enter an oxygenConc between 0.1 and 10!\")\n  }\n\n  OER_m = 3\n  K_m = 3\n  a = ((OER_m*pO2)+K_m)/(pO2+K_m)\n  OMF = (1/OER_m)*a\n\n  D <- seq(0, 10, 1)\n  SF1 = exp(-0.3*D*OMF-(0.03*D*D*OMF))\n  pdf(\"HyxpoxiaPlot.pdf\")\n  RadioGx::doseResponseCurve(Ds=list(\"Hypoxia\" = D),\n                              SFs=list(\"Hypoxia\" = SF1), plot.type=\"Actual\",\n                              legends.label = NULL,title = \"Effect of Hypoxia\",\n                              cex = 1.55,cex.main = 1.75,lwd = 2)\n  dev.off()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `OERmodel` function and what are its input constraints?",
        "answer": "The `OERmodel` function generates a dose response curve for a given hypoxic concentration. It takes a single input parameter `oxygenConc`, which must be a numeric value between 0.1 and 10. The function will stop execution with an error message if the input is outside this range or if it's NA."
      },
      {
        "question": "How does the function calculate the Oxygen Modification Factor (OMF) and what variables are used in this calculation?",
        "answer": "The function calculates the Oxygen Modification Factor (OMF) using the formula: OMF = (1/OER_m) * ((OER_m * pO2 + K_m) / (pO2 + K_m)). The variables used are: OER_m (set to 3), K_m (set to 3), and pO2 (the input oxygen concentration)."
      },
      {
        "question": "What does the function do after calculating the OMF, and what external package does it use?",
        "answer": "After calculating the OMF, the function generates a sequence of dose values (D) from 0 to 10, calculates the survival fraction (SF1) using the OMF, and then creates a dose response curve plot. It uses the `RadioGx` package to generate this plot, specifically the `doseResponseCurve` function. The plot is saved as a PDF file named 'HyxpoxiaPlot.pdf'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "OERmodel <- function(oxygenConc) {\n  pO2 <- as.numeric(oxygenConc)\n  if (is.na(pO2)) {\n    stop(\"Error: oxygenConc is NA!\")\n  }\n  if (pO2 < 0.1 | pO2 > 10) {\n    stop(\"Please enter an oxygenConc between 0.1 and 10!\")\n  }\n\n  OER_m <- 3\n  K_m <- 3\n  a <- ((OER_m * pO2) + K_m) / (pO2 + K_m)\n  OMF <- (1 / OER_m) * a\n\n  D <- seq(0, 10, 1)\n  SF1 <- exp(-0.3 * D * OMF - (0.03 * D * D * OMF))\n\n  # Complete the function to generate and save the plot\n}",
        "complete": "OERmodel <- function(oxygenConc) {\n  pO2 <- as.numeric(oxygenConc)\n  if (is.na(pO2)) {\n    stop(\"Error: oxygenConc is NA!\")\n  }\n  if (pO2 < 0.1 | pO2 > 10) {\n    stop(\"Please enter an oxygenConc between 0.1 and 10!\")\n  }\n\n  OER_m <- 3\n  K_m <- 3\n  a <- ((OER_m * pO2) + K_m) / (pO2 + K_m)\n  OMF <- (1 / OER_m) * a\n\n  D <- seq(0, 10, 1)\n  SF1 <- exp(-0.3 * D * OMF - (0.03 * D * D * OMF))\n\n  pdf(\"HyxpoxiaPlot.pdf\")\n  RadioGx::doseResponseCurve(Ds = list(\"Hypoxia\" = D),\n                            SFs = list(\"Hypoxia\" = SF1),\n                            plot.type = \"Actual\",\n                            legends.label = NULL,\n                            title = \"Effect of Hypoxia\",\n                            cex = 1.55, cex.main = 1.75, lwd = 2)\n  dev.off()\n}"
      },
      {
        "partial": "OERmodel <- function(oxygenConc) {\n  # Validate input\n  pO2 <- as.numeric(oxygenConc)\n  if (is.na(pO2) || pO2 < 0.1 || pO2 > 10) {\n    stop(\"Invalid oxygenConc. Please enter a value between 0.1 and 10.\")\n  }\n\n  # Calculate OER and OMF\n  OER_m <- 3\n  K_m <- 3\n  a <- ((OER_m * pO2) + K_m) / (pO2 + K_m)\n  OMF <- (1 / OER_m) * a\n\n  # Generate dose and survival fraction data\n  D <- seq(0, 10, 1)\n  SF1 <- exp(-0.3 * D * OMF - (0.03 * D * D * OMF))\n\n  # Complete the function to generate and save the plot\n}",
        "complete": "OERmodel <- function(oxygenConc) {\n  # Validate input\n  pO2 <- as.numeric(oxygenConc)\n  if (is.na(pO2) || pO2 < 0.1 || pO2 > 10) {\n    stop(\"Invalid oxygenConc. Please enter a value between 0.1 and 10.\")\n  }\n\n  # Calculate OER and OMF\n  OER_m <- 3\n  K_m <- 3\n  a <- ((OER_m * pO2) + K_m) / (pO2 + K_m)\n  OMF <- (1 / OER_m) * a\n\n  # Generate dose and survival fraction data\n  D <- seq(0, 10, 1)\n  SF1 <- exp(-0.3 * D * OMF - (0.03 * D * D * OMF))\n\n  # Generate and save the plot\n  pdf(\"HyxpoxiaPlot.pdf\")\n  RadioGx::doseResponseCurve(Ds = list(\"Hypoxia\" = D),\n                            SFs = list(\"Hypoxia\" = SF1),\n                            plot.type = \"Actual\",\n                            legends.label = NULL,\n                            title = \"Effect of Hypoxia\",\n                            cex = 1.55, cex.main = 1.75, lwd = 2)\n  dev.off()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/unitest/test.R",
    "language": "R",
    "content": "## Size\n\nlibrary(mRMRe)\ndata(cgps)\ndata <- mRMR.data(data = as.data.frame(cgps.ge))\nsystem.time(filter <- mRMR.ensemble(\"mRMRe.Filter\", data = data, target_indices = c(1), feature_count = 200, solution_count = 160))\nprint(object.size(filter), units = \"Mb\")\n\n\nlibrary(mRMRe)\nload('~/Downloads/irinotecan_cgp_ccle.RData')\ndata <- mRMR.data(data = data.frame(data_cgp[, 1:10000]))\nsystem.time(mim(data))\n\n## Simple Test\n\nlibrary(mRMRe)\nset.seed(0)\n\nx <- rnorm(100, 0)\ndd <- data.frame(\n        \"cont1\" = x,\n        \"cont2\" = x + rnorm(100, 0.1),\n\t\t\"cont3\" = x + rnorm(100, 0.1),\n\t\t\"cont4\" = x + rnorm(100, 0.1),\n\t\t\"cont5\" = x + rnorm(100, 0.1),\n\t\t\"cont6\" = x + rnorm(100, 0.01))\n\t\t\ndata <- mRMR.data(data = dd)\nfilter <- mRMR.classic(\"mRMRe.Filter\", data = data, target_indices = 3:5, feature_count = 2)\nscores(filter)\nsolutions(filter)\n\nmim(filter)\n## NETWORK TEST\n\nlibrary(mRMRe)\nset.seed(0)\ndd <- data.frame(\n        \"surv1\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont1\" = runif(100),\n        \"cat1\"  = factor(sample(1:5, 100, replace = TRUE), ordered = TRUE),\n        \"surv2\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont2\" = runif(100),\n        \"cont3\" = runif(100),\n        \"surv3\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cat2\"=factor(sample(1:5, 100, replace = TRUE), ordered = TRUE))\n\ndata <- mRMR.data(data = dd)\nnetwork <- new(\"mRMRe.Network\", data = data, target_indices = c(1, 2), levels = c(2, 1), layers = 1)\nnetwork@topologies\nadjacencyMatrix(network)\nvisualize(network)\n\n\ndata <- mRMR.data(data = dd,\n        strata = factor(sample(1:5, 100, replace=TRUE), ordered=TRUE),\n        weights = runif(100))\nmim(data) # Gives MI matrix\nmim(data, method = \"cor\") # Gives correlation matrix\n\n\n\nmim(filter_1)\n\nfilter_2 <- mRMR.ensemble(\"mRMRe.Filter\", data = data, target_index = 2, feature_count = 2, solution_count = 1)\nmim(filter_2)\n\nmim(data)\n\n# No wrapper just yet\nnetwork <- new(\"mRMRe.Network\", data = data, target_indices = c(1, 2), levels = c(1, 1, 1), layers = 1)\nmim(data)\nmim(network)\n\nadjacencyMatrix(network)\n\ncausality(network)[1, , ]\ncausality(filter_1)\n\ncausality(network)[2, , ]\ncausality(filter_2)\n\n# I have no idea how to get igraph to print out vertex names\n\nvisualize(network)\n\n## test for large network (1000 genes)\n## install new version of the package if needed on gen01\nlibrary(devtools)\ninstall_github(\"mRMRe\", username=\"bhaibeka\", ref=\"master\")\nsystem(\"chmod -R 775 /stockage/Laboratoires/HAI/Rlib\")\n\n\nlibrary(mRMRe)\n## set the number of threads\nset.thread.count(2)\n## run the network inference\ndata(cgps)\nge <- mRMR.data(data = data.frame(cgps.ge[ ,1:1000]))\n#Rprof(filename = \"Rprof.out\", append = FALSE, interval = 0.02, memory.profiling=TRUE)\nexect <- system.time(netw <- new(\"mRMRe.Network\", data = ge, target_indices = 1:10, levels = c(8, 1, 1, 1, 1), layers = 2))\nprint(exect)\n#summaryRprof(filename = \"Rprof.out\", chunksize = 5000, memory=c(\"both\"), index=2, diff=TRUE, exclude=NULL)\n\n\nprint(table(adjacencyMatrix(netw)))\n\npdf(\"temp.pdf\", width=14, height=14)\nvisualize(netw)\ndev.off()\n\n\n# Generate the basic stuff\ndata(cgps)\ndd <- mRMR.data(data=data.frame(cgps.ge))\nnetw <- new(\"mRMRe.Network\", data = dd, target_indices = c(1, 2), levels = c(1, 1, 1), layers = 1)\nfilter <- new(\"mRMRe.Filter\", data = dd, target_indices = c(1, 2), levels = c(1,1,1))\n\n# Create all 3 mim and remove NA's\nnetwork_mim <- mim(netw)\nindices <- !is.na(network_mim)\nnetwork_mim <- network_mim[indices]\nfilter_mim <- mim(filter)[indices]\ndata_mim <- mim(dd)[indices]\n\n# Compare\nquantile(network_mim)\nquantile(filter_mim)\nquantile(data_mim)\n\n# Causality\ncausality(filter) # a list of causality scores for each solution\ncausality(filter, merge=T) # a single vector of causality scores that is the min value between all solutions\ncausality(netw) # network-wide minimal causality scores\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mRMR.data()` function in the given code snippet, and how is it used?",
        "answer": "The `mRMR.data()` function is used to create a data object suitable for use with the mRMRe package. It converts the input data frame into a specialized format that can be used for feature selection and network analysis. In the code snippet, it's used twice: once with `as.data.frame(cgps.ge)` and once with `data.frame(data_cgp[, 1:10000])`, creating data objects for further analysis."
      },
      {
        "question": "Explain the purpose and parameters of the `mRMR.ensemble()` function call in the code snippet.",
        "answer": "The `mRMR.ensemble()` function is used to perform ensemble feature selection using the minimum redundancy maximum relevance (mRMR) algorithm. In this code, it's called with the following parameters:\n- 'mRMRe.Filter': specifies the type of filter to use\n- data: the input data object created by mRMR.data()\n- target_indices = c(1): indicates that the first column is the target variable\n- feature_count = 200: selects the top 200 features\n- solution_count = 160: generates 160 different solutions\nThe function returns a filter object containing the selected features and their scores."
      },
      {
        "question": "What is the purpose of the `system.time()` function in the code, and what information does it provide?",
        "answer": "The `system.time()` function is used to measure the execution time of the code enclosed within its parentheses. In this snippet, it's used twice:\n1. To measure the time taken by the `mRMR.ensemble()` function\n2. To measure the time taken by the `mim()` function\nIt provides information about the real time, user CPU time, and system CPU time taken by the enclosed expression. This is useful for performance analysis and optimization of computationally intensive operations in the mRMRe package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(mRMRe)\nset.seed(0)\n\nx <- rnorm(100, 0)\ndd <- data.frame(\n        \"cont1\" = x,\n        \"cont2\" = x + rnorm(100, 0.1),\n\t\t\"cont3\" = x + rnorm(100, 0.1),\n\t\t\"cont4\" = x + rnorm(100, 0.1),\n\t\t\"cont5\" = x + rnorm(100, 0.1),\n\t\t\"cont6\" = x + rnorm(100, 0.01))\n\t\t\ndata <- mRMR.data(data = dd)\nfilter <- mRMR.classic(\"mRMRe.Filter\", data = data, target_indices = 3:5, feature_count = 2)\n# Complete the code to print the scores and solutions of the filter",
        "complete": "library(mRMRe)\nset.seed(0)\n\nx <- rnorm(100, 0)\ndd <- data.frame(\n        \"cont1\" = x,\n        \"cont2\" = x + rnorm(100, 0.1),\n\t\t\"cont3\" = x + rnorm(100, 0.1),\n\t\t\"cont4\" = x + rnorm(100, 0.1),\n\t\t\"cont5\" = x + rnorm(100, 0.1),\n\t\t\"cont6\" = x + rnorm(100, 0.01))\n\t\t\ndata <- mRMR.data(data = dd)\nfilter <- mRMR.classic(\"mRMRe.Filter\", data = data, target_indices = 3:5, feature_count = 2)\nprint(scores(filter))\nprint(solutions(filter))"
      },
      {
        "partial": "library(mRMRe)\nset.seed(0)\ndd <- data.frame(\n        \"surv1\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont1\" = runif(100),\n        \"cat1\"  = factor(sample(1:5, 100, replace = TRUE), ordered = TRUE),\n        \"surv2\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont2\" = runif(100),\n        \"cont3\" = runif(100),\n        \"surv3\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cat2\"=factor(sample(1:5, 100, replace = TRUE), ordered = TRUE))\n\ndata <- mRMR.data(data = dd)\nnetwork <- new(\"mRMRe.Network\", data = data, target_indices = c(1, 2), levels = c(2, 1), layers = 1)\n# Complete the code to visualize the network and print its adjacency matrix",
        "complete": "library(mRMRe)\nset.seed(0)\ndd <- data.frame(\n        \"surv1\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont1\" = runif(100),\n        \"cat1\"  = factor(sample(1:5, 100, replace = TRUE), ordered = TRUE),\n        \"surv2\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont2\" = runif(100),\n        \"cont3\" = runif(100),\n        \"surv3\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cat2\"=factor(sample(1:5, 100, replace = TRUE), ordered = TRUE))\n\ndata <- mRMR.data(data = dd)\nnetwork <- new(\"mRMRe.Network\", data = data, target_indices = c(1, 2), levels = c(2, 1), layers = 1)\nvisualize(network)\nprint(adjacencyMatrix(network))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/src/Filter.cpp",
    "language": "cpp",
    "content": "#include \"Filter.h\"\n#include <iostream>\n\nFilter::Filter(int const* const pChildrenCountPerLevel, unsigned int const levelCount,\n        Matrix* const pFeatureInformationMatrix, unsigned int const targetFeatureIndex, \n        unsigned int const fixedFeatureCount) :\n        mpChildrenCountPerLevel(pChildrenCountPerLevel), mLevelCount(levelCount), mpFeatureInformationMatrix(\n                pFeatureInformationMatrix), mpStartingIndexPerLevel(\n                new unsigned int[mLevelCount + 2]), mFixedFeatureCount(fixedFeatureCount)\n{\n    unsigned int cumulative_element_count = 1;\n    unsigned int children_per_level = 1;\n\n    mpStartingIndexPerLevel[0] = 0;\n\n    for (unsigned int level = 0; level < mLevelCount; ++level)\n    {\n        mpStartingIndexPerLevel[level + 1] = cumulative_element_count;\n        children_per_level *= mpChildrenCountPerLevel[level];\n        cumulative_element_count += children_per_level;\n    }\n\n    mpStartingIndexPerLevel[mLevelCount + 1] = cumulative_element_count;\n    mTreeElementCount = cumulative_element_count;\n    mpIndexTree = new unsigned int[cumulative_element_count];\n    mpScoreTree = new double [cumulative_element_count];\n\n    for (unsigned int i = 0; i < mTreeElementCount; ++i)\n    {\n        mpIndexTree[i] = targetFeatureIndex;\n        mpScoreTree[i] = 0;\n    }\n}\n\nFilter::~Filter()\n{\n    delete[] mpStartingIndexPerLevel;\n    delete[] mpIndexTree;\n    delete[] mpScoreTree;\n}\n\nvoid const\nFilter::build()\n{\n    for (unsigned int level = 0; level < mLevelCount; ++level)\n    {\n        unsigned int const parent_count = mpStartingIndexPerLevel[level + 1]\n                - mpStartingIndexPerLevel[level];\n\n\n#ifdef _OPENMP\n#pragma omp parallel for schedule(dynamic)\n#endif \n        for (unsigned int parent = 0; parent < parent_count; ++parent)\n            placeElements(\n                    mpStartingIndexPerLevel[level + 1] + (parent * mpChildrenCountPerLevel[level]),\n                    mpChildrenCountPerLevel[level], level + 1);\n    }\n}\n\n/* inline */unsigned int const\nFilter::getParentAbsoluteIndex(unsigned int const absoluteIndex, unsigned int const level) const\n{\n    return (absoluteIndex - mpStartingIndexPerLevel[level]) / mpChildrenCountPerLevel[level - 1]\n            + mpStartingIndexPerLevel[level - 1];\n}\n\nvoid const\nFilter::getSolutions(int* const solutions) const\n{\n    unsigned int counter = 0;\n\n    for (unsigned int end_element_absolute_index = mTreeElementCount - 1;\n            end_element_absolute_index >= mpStartingIndexPerLevel[mLevelCount];\n            --end_element_absolute_index)\n    {\n        unsigned int element_absolute_index = end_element_absolute_index;\n\n        for (unsigned int level = mLevelCount; level > 0; --level)\n        {\n            solutions[counter++] = mpIndexTree[element_absolute_index];\n            element_absolute_index = getParentAbsoluteIndex(element_absolute_index, level);\n        }\n    }\n}\n\nvoid const\nFilter::getScores(double* const scores) const\n{\n    unsigned int counter = 0;\n\n    for (unsigned int end_element_absolute_index = mTreeElementCount - 1;\n            end_element_absolute_index >= mpStartingIndexPerLevel[mLevelCount];\n            --end_element_absolute_index)\n    {\n        unsigned int element_absolute_index = end_element_absolute_index;\n\n        for (unsigned int level = mLevelCount; level > 0; --level)\n        {\n            scores[counter++] = mpScoreTree[element_absolute_index];\n            element_absolute_index = getParentAbsoluteIndex(element_absolute_index, level);\n        }\n    }\n}\n\nbool const\nFilter::hasAncestorByFeatureIndex(unsigned int const absoluteIndex, unsigned int const featureIndex,\n        unsigned int level) const\n{\n    // This function only considers the ancestry of the putative absolute/featureIndex\n\n    unsigned int parent_absolute_index = absoluteIndex;\n\n    for (unsigned int i = level; i > 0; --i)\n    {\n        parent_absolute_index = getParentAbsoluteIndex(parent_absolute_index, i);\n\n        if (mpIndexTree[parent_absolute_index] == featureIndex)\n            return true;\n    }\n\n    return false;\n}\n\nbool const\nFilter::isRedundantPath(unsigned int const absoluteIndex, unsigned int const featureIndex,\n        unsigned int const level) const\n{\n    for (unsigned int i = mpStartingIndexPerLevel[level]; i < mpStartingIndexPerLevel[level + 1];\n            ++i)\n    {\n        if (mpIndexTree[i] == mpIndexTree[0])\n            continue;\n\n        unsigned int candidate_absolute_index = absoluteIndex;\n        unsigned int candidate_feature_index = featureIndex;\n\n        bool solution_is_redundant = true;\n\n        for (unsigned int j = level; j > 0; --j)\n        {\n            unsigned int parent_absolute_index = i;\n\n            bool feature_is_redundant = false;\n\n            for (unsigned int k = level; k > 0; --k)\n            {\n                if (mpIndexTree[parent_absolute_index] == candidate_feature_index)\n                {\n                    feature_is_redundant = true;\n                    break;\n                }\n\n                parent_absolute_index = getParentAbsoluteIndex(parent_absolute_index, k);\n            }\n\n            if (!feature_is_redundant)\n            {\n                solution_is_redundant = false;\n                break;\n            }\n\n            candidate_absolute_index = getParentAbsoluteIndex(candidate_absolute_index, j);\n            candidate_feature_index = mpIndexTree[candidate_absolute_index];\n        }\n\n        if (solution_is_redundant)\n            return true;\n    }\n\n    return false;\n}\n\nvoid const\nFilter::placeElements(unsigned int const startingIndex, unsigned int childrenCount,\n        unsigned int const level)\n{   \n    // This function calculates the solutions and scores\n    unsigned int counter = 0;\n    unsigned int const feature_count = mpFeatureInformationMatrix->getRowCount();\n    unsigned int* const p_candidate_feature_indices = new unsigned int[feature_count];\n    unsigned int* const p_order = new unsigned int[feature_count];\n    unsigned int* const p_adaptor = new unsigned int[feature_count];\n    double* const p_candidate_scores = new double[feature_count];\n\n    /*  The reason why the selection does not start from the 0, it is because the package request (temporarily) the input \n    *   dataframe has the fixed selected features in the first several columns\n    */ \n\n    for (unsigned int i = mFixedFeatureCount; i < feature_count; ++i)\n    {   \n        \n        if (hasAncestorByFeatureIndex(startingIndex, i, level))\n            continue;\n        \n        double ancestry_score = 0.;\n        \n        for (int j = 0; j < mFixedFeatureCount; ++j) \n        {\n            double ancestry_score_ij = Math::computeMi(\n                    mpFeatureInformationMatrix->at(i, j));\n            double ancestry_score_ji = Math::computeMi(\n                    mpFeatureInformationMatrix->at(j, i));\n            \n            ancestry_score += std::max(ancestry_score_ij, ancestry_score_ji);\n        }\n\n        \n        if (level > 1)\n        {\n            unsigned int ancestor_absolute_index = startingIndex;\n            \n            // The below for loop is used to compute the redundancy (loops from the top to bottom)\n\n            for (unsigned int j = level; j > 0; --j)\n            {\n                ancestor_absolute_index = getParentAbsoluteIndex(ancestor_absolute_index, j);\n\n                double ancestry_score_ij = Math::computeMi(\n                        mpFeatureInformationMatrix->at(i, mpIndexTree[ancestor_absolute_index]));\n                double ancestry_score_ji = Math::computeMi(\n                        mpFeatureInformationMatrix->at(mpIndexTree[ancestor_absolute_index], i));\n\n                ancestry_score += std::max(ancestry_score_ij, ancestry_score_ji);\n                \n            }\n        }\n\n        double const score = Math::computeMi(mpFeatureInformationMatrix->at(i, mpIndexTree[0]))\n                - (ancestry_score / (level + mFixedFeatureCount));\n\n        if (score == score)\n        {\n            p_order[counter] = counter;\n            p_adaptor[counter] = counter;\n            p_candidate_feature_indices[counter] = i;\n            p_candidate_scores[counter] = score;\n\n            ++counter;\n        }\n    }\n\n    std::sort(p_order, p_order + counter, Math::IndirectComparator(p_candidate_scores, p_adaptor));\n\n#pragma omp critical(filter_element_placement)\n    {\n        unsigned int children_counter = 0;\n        unsigned int i = counter - 1;\n\n        // i < counter depends on the fact that i-- causes an overflow on the unsigned int\n        while (i < counter && children_counter < childrenCount)\n        {\n            unsigned int const index = p_candidate_feature_indices[p_order[i--]];\n\n            if (!isRedundantPath(startingIndex + children_counter, index, level))\n            {\n                mpIndexTree[startingIndex + children_counter++] = index;\n                mpScoreTree[startingIndex + children_counter-1] = p_candidate_scores[p_order[i+1]];\n            }\n        }\n    }\n\n    delete[] p_order;\n    delete[] p_adaptor;\n    delete[] p_candidate_feature_indices;\n    delete[] p_candidate_scores;\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `Filter` class constructor, and how does it initialize the `mpStartingIndexPerLevel` array?",
        "answer": "The `Filter` class constructor initializes various member variables and allocates memory for data structures. It calculates the `mpStartingIndexPerLevel` array, which stores the starting indices for each level in the tree structure. The array is initialized by iterating through the levels, calculating cumulative element counts, and storing the starting index for each level. This array is used for efficient navigation and indexing within the tree structure."
      },
      {
        "question": "Explain the purpose and functionality of the `build()` method in the `Filter` class.",
        "answer": "The `build()` method constructs the filter's tree structure. It iterates through each level of the tree, calling the `placeElements()` method for each parent node to determine and place its child elements. The method uses OpenMP parallelization (`#pragma omp parallel for`) to improve performance on multi-core systems. The `placeElements()` function is responsible for calculating scores, selecting features, and populating the `mpIndexTree` and `mpScoreTree` arrays based on the filter's criteria."
      },
      {
        "question": "How does the `placeElements()` method handle feature selection and score calculation, and what measures does it take to avoid redundancy?",
        "answer": "The `placeElements()` method selects features and calculates scores for child nodes. It considers fixed features, avoids selecting features already present in ancestors, and calculates a score based on mutual information and ancestry. To avoid redundancy, it uses the `hasAncestorByFeatureIndex()` method to check if a feature is already present in the ancestry. It also calculates an ancestry score to penalize features that are similar to those already selected. The `isRedundantPath()` method is called to ensure that the selected features don't create redundant paths in the tree structure. Finally, it sorts the candidate features by score and selects the top-scoring non-redundant features for placement in the tree."
      }
    ],
    "completion_tasks": [
      {
        "partial": "void const\nFilter::getSolutions(int* const solutions) const\n{\n    unsigned int counter = 0;\n\n    for (unsigned int end_element_absolute_index = mTreeElementCount - 1;\n            end_element_absolute_index >= mpStartingIndexPerLevel[mLevelCount];\n            --end_element_absolute_index)\n    {\n        unsigned int element_absolute_index = end_element_absolute_index;\n\n        for (unsigned int level = mLevelCount; level > 0; --level)\n        {\n            // TODO: Fill in the missing code here\n        }\n    }\n}",
        "complete": "void const\nFilter::getSolutions(int* const solutions) const\n{\n    unsigned int counter = 0;\n\n    for (unsigned int end_element_absolute_index = mTreeElementCount - 1;\n            end_element_absolute_index >= mpStartingIndexPerLevel[mLevelCount];\n            --end_element_absolute_index)\n    {\n        unsigned int element_absolute_index = end_element_absolute_index;\n\n        for (unsigned int level = mLevelCount; level > 0; --level)\n        {\n            solutions[counter++] = mpIndexTree[element_absolute_index];\n            element_absolute_index = getParentAbsoluteIndex(element_absolute_index, level);\n        }\n    }\n}"
      },
      {
        "partial": "bool const\nFilter::hasAncestorByFeatureIndex(unsigned int const absoluteIndex, unsigned int const featureIndex,\n        unsigned int level) const\n{\n    unsigned int parent_absolute_index = absoluteIndex;\n\n    for (unsigned int i = level; i > 0; --i)\n    {\n        // TODO: Fill in the missing code here\n    }\n\n    return false;\n}",
        "complete": "bool const\nFilter::hasAncestorByFeatureIndex(unsigned int const absoluteIndex, unsigned int const featureIndex,\n        unsigned int level) const\n{\n    unsigned int parent_absolute_index = absoluteIndex;\n\n    for (unsigned int i = level; i > 0; --i)\n    {\n        parent_absolute_index = getParentAbsoluteIndex(parent_absolute_index, i);\n\n        if (mpIndexTree[parent_absolute_index] == featureIndex)\n            return true;\n    }\n\n    return false;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/radSensitivitySig.R",
    "language": "R",
    "content": "#' Creates a signature representing the association between gene expression (or\n#' other molecular profile) and radiation dose response, for use in radiation sensitivity\n#' analysis.\n#'\n#' Given a RadioSet of the sensitivity experiment type, and a list of radiation types,\n#' the function will compute a signature for the effect of gene expression on the\n#' molecular profile of a cell. The function returns the estimated coefficient,\n#' the t-stat, the p-value and the false discovery rate associated with that\n#' coefficient, in a 3 dimensional array, with genes in the first direction,\n#' drugs in the second, and the selected return values in the third.\n#'\n#' @examples\n#' data(clevelandSmall)\n#' rad.sensitivity <- radSensitivitySig(clevelandSmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(clevelandSmall, \"rna\")[1],\n#'              radiation.types=treatmentNames(clevelandSmall))\n#' print(rad.sensitivity)\n#'\n#' @param rSet A \\code{RadioSet} of the perturbation experiment type\n#' @param mDataType \\code{character} which one of the molecular data types to use\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv\n#' @param radiation.types \\code{character} a vector of radiation.types for which to compute the\n#'   signatures. Should match the names used in the PharmacoSet.\n#' @param features \\code{character} a vector of features for which to compute the\n#'   signatures. Should match the names used in correspondant molecular data in PharmacoSet.\n#' @param nthread \\code{numeric} if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#' @param returnValues \\code{character} Which of estimate, t-stat, p-value and fdr\n#'   should the function return for each gene?\n#' @param sensitivity.measure \\code{character} which measure of the radiation\n#'   sensitivity should the function use for its computations? Use the\n#'   sensitivityMeasures function to find out what measures are available for each PSet.\n#' @param molecular.summary.stat What summary statistic should be used to\n#'   summarize duplicates for cell line molecular profile measurements?\n#' @param sensitivity.summary.stat What summary statistic should be used to\n#'   summarize duplicates for cell line sensitivity measurements?\n#' @param sensitivity.cutoff Allows to provide upper and lower bounds to\n#'   sensitivity measures in the cases where the values exceed physical values\n#'   due to numerical or other errors.\n#' @param standardize \\code{character} One of \"SD\", \"rescale\", or \"none\", for the form of standardization of\n#'   the data to use. If \"SD\", the the data is scaled so that SD = 1. If rescale, then the data is scaled so that the 95%\n#'   interquantile range lies in [0,1]. If none no rescaling is done.\n#' @param verbose \\code{boolean} 'TRUE' if the warnings and other infomrative message shoud be displayed\n#' @param ... additional arguments not currently fully supported by the function\n#'\n#' @return \\code{list} a 3D array with genes in the first dimension, radiation.types in the\n#'   second, and return values in the third.\n#'\n#' @export\n#' @importFrom parallel detectCores splitIndices\nradSensitivitySig <- function(rSet,\n mDataType,\n radiation.types,\n features,\n sensitivity.measure = \"AUC_recomputed\",\n molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n sensitivity.cutoff=NA,\n standardize = c(\"SD\", \"rescale\", \"none\"),\n nthread = 1,\n verbose=TRUE, ...) {\n\n  ### This function needs to: Get a table of AUC values per cell line / drug\n  ### Be able to recompute those values on the fly from raw data if needed to change concentration\n  ### Be able to choose different summary methods on fly if needed (need to add annotation to table to tell what summary\n  #   method previously used)\n  ### Be able to extract genomic data\n  ### Run rankGeneDrugSens in parallel at the drug level\n  ### Return matrix as we had before\n\n  molecular.summary.stat <- match.arg(molecular.summary.stat)\n  sensitivity.summary.stat <- match.arg(sensitivity.summary.stat)\n  standardize <- match.arg(standardize)\n\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  dots <- list(...)\n  ndots <- length(dots)\n\n  if (!all(sensitivity.measure %in% colnames(sensitivityProfiles(rSet)))) {\n    stop (sprintf(\"Invalid sensitivity measure for %s, choose among: %s\",\n                  annotation(rSet)$name, paste(colnames(sensitivityProfiles(rSet)),\n                                              collapse=\", \")))\n  }\n\n  if (!(mDataType %in% names(molecularProfilesSlot(rSet)))) {\n    stop (sprintf(\"Invalid mDataType for %s, choose among: %s\",\n                  annotation(rSet)$name, paste(names(molecularProfilesSlot(rSet)),\n                                              collapse=\", \")))\n  }\n  switch(S4Vectors::metadata(molecularProfilesSlot(rSet)[[mDataType]])$annotation,\n    \"mutation\" = {\n      if (!is.element(molecular.summary.stat, c(\"or\", \"and\"))) {\n        stop(\"Molecular summary statistic for mutation must be either 'or' or 'and'\")\n      }\n    },\n    \"fusion\" = {\n      if (!is.element(molecular.summary.stat, c(\"or\", \"and\"))) {\n        stop(\"Molecular summary statistic for fusion must be either 'or' or 'and'\")\n      }\n    },\n    \"rna\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop(\"Molecular summary statistic for rna must be either 'mean', 'median', 'first' or 'last'\")\n      }\n    },\n    \"cnv\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop (\"Molecular summary statistic for cnv must be either 'mean', 'median', 'first' or 'last'\")\n      }\n    },\n    \"rnaseq\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop (\"Molecular summary statistic for rna must be either 'mean', 'median', 'first' or 'last'\")\n      }},\n      stop (sprintf(\"No summary statistic for %s has been implemented yet\", S4Vectors::metadata(molecularProfilesSlot(rSet)[[mDataType]])$annotation))\n      )\n\n  if (!is.element(sensitivity.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n    stop (\"Sensitivity summary statistic for sensitivity must be either 'mean', 'median', 'first' or 'last'\")\n  }\n\n  if (missing(radiation.types)){\n    radiation.types <- treatmentNames(rSet)\n  }\n\n  availcore <- parallel::detectCores()\n  if ( nthread > availcore) {\n    nthread <- availcore\n  }\n\n  if (missing(features)) {\n    features <- rownames(featureInfo(rSet, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(rSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning (sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  if(is.null(dots[[\"sProfiles\"]])){\n    drugpheno.all <- lapply(sensitivity.measure, function(sensitivity.measure) {\n\n      return(t(summarizeSensitivityProfiles(rSet,\n        sensitivity.measure = sensitivity.measure,\n        summary.stat = sensitivity.summary.stat,\n        verbose = verbose)))\n\n    })} else {\n      sProfiles <- dots[[\"sProfiles\"]]\n      drugpheno.all <- list(t(sProfiles))\n    }\n\n    dix <- is.element(radiation.types, do.call(colnames, drugpheno.all))\n    if (verbose && !all(dix)) {\n      warning (sprintf(\"Only %i/%i radiation types can be found\", sum(dix), length(radiation.types)))\n    }\n    if (!any(dix)) {\n      stop(\"None of the chosen radiation types were found in the dataset\")\n    }\n    radiation.types <- radiation.types[dix]\n\n    molecularProfilesSlot(rSet)[[mDataType]] <- summarizeMolecularProfiles(object=rSet,\n      mDataType = mDataType,\n      summary.stat = molecular.summary.stat,\n      verbose = verbose)[features, ]\n\n    if(!is.null(dots[[\"mProfiles\"]])) {\n      mProfiles <- dots[[\"mProfiles\"]]\n      SummarizedExperiment::assay(molecularProfilesSlot(rSet)[[mDataType]]) <- mProfiles[features, colnames(molecularProfilesSlot(rSet)[[mDataType]]), drop = FALSE]\n\n    }\n\n    drugpheno.all <- lapply(drugpheno.all, function(x) {x[phenoInfo(rSet, mDataType)[ ,\"sampleid\"], , drop = FALSE]})\n\n    type <- as.factor(sampleInfo(rSet)[phenoInfo(rSet, mDataType)[ ,\"sampleid\"], \"tissueid\"])\n    batch <- phenoInfo(rSet, mDataType)[, \"batchid\"]\n    batch[!is.na(batch) & batch == \"NA\"] <- NA\n    batch <- as.factor(batch)\n    names(batch) <- phenoInfo(rSet, mDataType)[ , \"sampleid\"]\n    batch <- batch[rownames(drugpheno.all[[1]])]\n    if (verbose) {\n      message(\"Computing radiation sensitivity signatures...\")\n    }\n\n    mcres <-  lapply(radiation.types, function(treatmentid, expr, drugpheno, type, batch, standardize, nthread) {\n     res <- NULL\n     for(i in treatmentid) {\n       ## using a linear model (x ~ concentration + cell + batch)\n       dd <- lapply(drugpheno, function(rad) rad[, i])\n       dd <- do.call(cbind, dd)\n       colnames(dd) <- seq_len(ncol(dd))\n       if(!is.na(sensitivity.cutoff)) {\n         dd <- factor(ifelse(dd > sensitivity.cutoff, 1, 0), levels=c(0, 1))\n       }\n       rr <- rankGeneRadSensitivity(data=expr, drugpheno=dd, type=type, batch=batch, single.type=FALSE, standardize=standardize, nthread=nthread, verbose=verbose)\n       res <- c(res, list(rr$all))\n     }\n     names(res) <- treatmentid\n     return(res)\n    }, expr=t(molecularProfiles(rSet, mDataType)[features, , drop=FALSE]), drugpheno=drugpheno.all, type=type, batch=batch, nthread=nthread, standardize=standardize)\n\n    res <- do.call(c, mcres)\n    res <- res[!vapply(res, is.null, logical(1))]\n    drug.sensitivity <- array(NA,\n      dim = c(nrow(featureInfo(rSet, mDataType)[features,, drop=FALSE]),\n        length(res), ncol(res[[1]])),\n      dimnames = list(rownames(featureInfo(rSet, mDataType)[features,]), names(res), colnames(res[[1]])))\n    for(j in seq_len(ncol(res[[1]]))) {\n      ttt <- unlist(lapply(res, function(x, j, k) {\n        xx <- array(NA, dim = length(k), dimnames = list(k))\n        xx[rownames(x)] <- x[ , j, drop=FALSE]\n        return (xx)\n      },\n      j = j,\n      k = rownames(featureInfo(rSet, mDataType)[features,, drop = FALSE])))\n      drug.sensitivity[rownames(featureInfo(rSet, mDataType)[features,, drop = FALSE]), names(res), j] <- ttt\n    }\n\n    drug.sensitivity <- RadioSig(drug.sensitivity, RSetName = name(rSet), Call =\"as.character(match.call())\", SigType='Sensitivity')\n\n    return(drug.sensitivity)\n  }\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `radSensitivitySig` function?",
        "answer": "The main purpose of the `radSensitivitySig` function is to create a signature representing the association between gene expression (or other molecular profile) and radiation dose response for use in radiation sensitivity analysis. It computes a signature for the effect of gene expression on the molecular profile of a cell, returning estimated coefficients, t-stats, p-values, and false discovery rates for genes across different radiation types."
      },
      {
        "question": "How does the function handle different types of molecular data?",
        "answer": "The function handles different types of molecular data through the `mDataType` parameter and uses appropriate summary statistics based on the data type. For example, it uses 'or' or 'and' for mutation and fusion data, while 'mean', 'median', 'first', or 'last' are used for RNA, CNV, and RNAseq data. The function checks the validity of the chosen summary statistic for each data type and throws an error if an incompatible statistic is selected."
      },
      {
        "question": "How does the function support parallel processing, and what precautions does it take?",
        "answer": "The function supports parallel processing through the `nthread` parameter, which determines how many cores should be used for computation. It uses the `parallel::detectCores()` function to check the available cores and adjusts the `nthread` value if it exceeds the available cores. The function also sets multicore options at the beginning and uses `on.exit()` to restore the original options when the function exits, ensuring that global settings are not permanently altered."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/src/Matrix.cpp",
    "language": "cpp",
    "content": "#include \"Matrix.h\"\n\nMatrix::Matrix(unsigned int const rowCount, unsigned int const columnCount) :\n        mpData(new double[rowCount * columnCount]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(\n                true)\n{\n\n}\n\n/* explicit */\nMatrix::Matrix(unsigned int const size, unsigned int const rowCount, unsigned int const columnCount) :\n        mpData(new double[size]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(\n                true)\n{\n\n}\n\n/* explicit */\nMatrix::Matrix(double* const pData, unsigned int const rowCount, unsigned int const columnCount) :\n        mpData(pData), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(false)\n{\n\n}\n\n/* virtual */\nMatrix::~Matrix()\n{\n    if (mHasAllocation)\n        delete[] mpData;\n}\n\n/* virtual */double&\nMatrix::at(unsigned int const i, unsigned int const j)\n{\n    return mpData[(j * mRowCount) + i];\n}\n\n/* virtual */double const&\nMatrix::at(unsigned int const i, unsigned int const j) const\n{\n    return mpData[(j * mRowCount) + i];\n}\n\nunsigned int const\nMatrix::getColumnCount() const\n{\n    return mColumnCount;\n}\n\nunsigned int const\nMatrix::getRowCount() const\n{\n    return mRowCount;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'mHasAllocation' member variable in the Matrix class, and how is it used?",
        "answer": "The 'mHasAllocation' member variable is a boolean flag that indicates whether the Matrix object owns the memory it's using. It's set to true when the Matrix allocates its own memory (in the first two constructors) and false when it's given a pointer to existing data (in the third constructor). This flag is used in the destructor to determine whether the object should delete the memory it's using. If mHasAllocation is true, the destructor calls delete[] on mpData; if it's false, it doesn't, preventing the deletion of memory that might be owned by another part of the program."
      },
      {
        "question": "How does the Matrix class handle element access, and why might the implementation be problematic for certain use cases?",
        "answer": "The Matrix class handles element access through the 'at' method, which takes row (i) and column (j) indices. The element is accessed using the formula mpData[(j * mRowCount) + i]. This implementation assumes column-major order, where elements in the same column are stored contiguously in memory. While this works, it might be problematic for certain use cases because: 1) It's not the most common convention (row-major order is more typical in C++), which could lead to confusion. 2) It may result in poor cache performance when iterating over the matrix in row-major order, which is a common operation. 3) The method doesn't include bounds checking, which could lead to undefined behavior if invalid indices are provided."
      },
      {
        "question": "What is the purpose of declaring some of the Matrix class methods as 'virtual', and what implications does this have for the class design?",
        "answer": "The 'virtual' keyword is used for the destructor and the 'at' methods in the Matrix class. This indicates that these methods can be overridden in derived classes. The implications of this design choice are: 1) It allows for polymorphic behavior, meaning derived classes can provide their own implementations of these methods. 2) It enables proper cleanup of derived class resources when deleting a Matrix pointer to a derived class object. 3) It suggests that the Matrix class is designed to be a base class in a hierarchy, potentially supporting different types of matrices or matrix operations in derived classes. 4) It introduces a small performance overhead due to the use of virtual function tables. 5) It requires that users of this class be aware of the potential for dynamic dispatch when calling these methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class Matrix {\nprivate:\n    double* mpData;\n    unsigned int mRowCount;\n    unsigned int mColumnCount;\n    bool mHasAllocation;\n\npublic:\n    Matrix(unsigned int rowCount, unsigned int columnCount);\n    Matrix(unsigned int size, unsigned int rowCount, unsigned int columnCount);\n    Matrix(double* pData, unsigned int rowCount, unsigned int columnCount);\n    virtual ~Matrix();\n\n    virtual double& at(unsigned int i, unsigned int j);\n    virtual double const& at(unsigned int i, unsigned int j) const;\n\n    unsigned int getColumnCount() const;\n    unsigned int getRowCount() const;\n};",
        "complete": "class Matrix {\nprivate:\n    double* mpData;\n    unsigned int mRowCount;\n    unsigned int mColumnCount;\n    bool mHasAllocation;\n\npublic:\n    Matrix(unsigned int rowCount, unsigned int columnCount)\n        : mpData(new double[rowCount * columnCount]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(true) {}\n\n    Matrix(unsigned int size, unsigned int rowCount, unsigned int columnCount)\n        : mpData(new double[size]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(true) {}\n\n    Matrix(double* pData, unsigned int rowCount, unsigned int columnCount)\n        : mpData(pData), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(false) {}\n\n    virtual ~Matrix() {\n        if (mHasAllocation)\n            delete[] mpData;\n    }\n\n    virtual double& at(unsigned int i, unsigned int j) {\n        return mpData[(j * mRowCount) + i];\n    }\n\n    virtual double const& at(unsigned int i, unsigned int j) const {\n        return mpData[(j * mRowCount) + i];\n    }\n\n    unsigned int getColumnCount() const {\n        return mColumnCount;\n    }\n\n    unsigned int getRowCount() const {\n        return mRowCount;\n    }\n};"
      },
      {
        "partial": "#include \"Matrix.h\"\n\nMatrix::Matrix(unsigned int const rowCount, unsigned int const columnCount)\n    : mpData(new double[rowCount * columnCount]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(true)\n{\n}\n\nMatrix::Matrix(unsigned int const size, unsigned int const rowCount, unsigned int const columnCount)\n    : mpData(new double[size]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(true)\n{\n}\n\nMatrix::Matrix(double* const pData, unsigned int const rowCount, unsigned int const columnCount)\n    : mpData(pData), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(false)\n{\n}\n\nMatrix::~Matrix()\n{\n    // Implement destructor\n}\n\ndouble& Matrix::at(unsigned int const i, unsigned int const j)\n{\n    // Implement at() method\n}\n\ndouble const& Matrix::at(unsigned int const i, unsigned int const j) const\n{\n    // Implement const at() method\n}\n\nunsigned int const Matrix::getColumnCount() const\n{\n    // Implement getColumnCount()\n}\n\nunsigned int const Matrix::getRowCount() const\n{\n    // Implement getRowCount()\n}",
        "complete": "#include \"Matrix.h\"\n\nMatrix::Matrix(unsigned int const rowCount, unsigned int const columnCount)\n    : mpData(new double[rowCount * columnCount]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(true)\n{\n}\n\nMatrix::Matrix(unsigned int const size, unsigned int const rowCount, unsigned int const columnCount)\n    : mpData(new double[size]), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(true)\n{\n}\n\nMatrix::Matrix(double* const pData, unsigned int const rowCount, unsigned int const columnCount)\n    : mpData(pData), mRowCount(rowCount), mColumnCount(columnCount), mHasAllocation(false)\n{\n}\n\nMatrix::~Matrix()\n{\n    if (mHasAllocation)\n        delete[] mpData;\n}\n\ndouble& Matrix::at(unsigned int const i, unsigned int const j)\n{\n    return mpData[(j * mRowCount) + i];\n}\n\ndouble const& Matrix::at(unsigned int const i, unsigned int const j) const\n{\n    return mpData[(j * mRowCount) + i];\n}\n\nunsigned int const Matrix::getColumnCount() const\n{\n    return mColumnCount;\n}\n\nunsigned int const Matrix::getRowCount() const\n{\n    return mRowCount;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/CoreSet-utils.R",
    "language": "R",
    "content": "#' @include CoreSet-class.R CoreSet-accessors.R\n#' @importFrom BiocGenerics match %in%\nNULL\n\n.local_class <- 'CoreSet'\n.local_data <- 'clevelandSmall_cSet'\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n# ===================================\n# Utility Method Documentation Object\n# -----------------------------------\n\n\n#' @noRd\n.docs_CoreSet_utils <- function(...) .parseToRoxygen(\n    \"\n    @title Utility methods for a `{class_}` object.\n\n    @description\n    Documentation for utility methods for a `{class_}` object, such as\n    set operations like subset and intersect. See @details for information\n    on different types of methods and their implementations.\n\n    @param x A `{class_}` object.\n    @param samples `character()` vector of sample names. Must be valid rownames\n    from `sampleInfo(x)`.\n    @param treatments `character()` vector of treatment names. Must be valid\n    rownames from `treatmentInfo(x)`. This method does not work with\n    `CoreSet` objects yet.\n    @param features `character()` vector of feature names. Must be valid feature\n    names for a given `mDataType`\n    @param mDataTypes `character()` One or more molecular data types to\n        to subset features by. Must be valid rownames for the selected\n        SummarizedExperiment mDataTypes.\n\n    @return See details.\n    \",\n    ...\n)\n\n\n#' @name CoreSet-utils\n#' @eval .docs_CoreSet_utils(class_=.local_class)\n#' @eval .parseToRoxygen(\"@examples data({data_})\", data_=.local_data)\nNULL\n\n\n# ======================================\n# Subset Methods\n# --------------------------------------\n\n\n## ===================\n## ---- subsetBySample\n## -------------------\n\n\n#' @export\nsetGeneric('subsetBySample', function(x, samples, ...)\n    standardGeneric('subsetBySample'))\n\n#' @noRd\n.docs_CoreSet_subsetBySample <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    ## subset methods\n    __subsetBySample__: Subset a `{class_}` object by sample identifier.\n    - value: a `{class_}` object containing only `samples`.\n\n    @examples\n\n    ## subset methods\n\n    ### subsetBySample\n    samples <- sampleInfo({data_})$sampleid[seq_len(10)]\n    {data_}_sub <- subsetBySample({data_}, samples)\n\n    @md\n    @aliases subsetBySample subsetBySample,CoreSet-method\n    @exportMethod subsetBySample\n    \",\n    ...\n)\n\n#' @rdname CoreSet-utils\n#' @eval .docs_CoreSet_subsetBySample(class_=.local_class, data_=.local_data)\nsetMethod('subsetBySample', signature('CoreSet'), function(x, samples) {\n\n    funContext <- .S4MethodContext('subsetBySample', 'CoreSet')\n\n    sampleNames <- rownames(sampleInfo(x))\n    if (!all(samples %in% sampleNames)) {\n        .warning(funContext, 'Samples missing from ', class(x)[1], ': ',\n            setdiff(samples, sampleNames), '! Please ensure all specified\n            samples are valid rownames of sampleInfo(x). Proceeding with\n            the valid samples only.')\n        samples <- union(samples, sampleNames)\n    }\n\n    # -- molecularProfiles slot\n    molecSlot <- molecularProfilesSlot(x)\n    molecularProfilesSlot(x) <-\n        .subsetMolecularProfilesBySample(molecSlot, samples)\n\n    # -- sensitivity slot\n    sensSlot <- treatmentResponse(x)\n    treatmentResponse(x) <- .subsetSensitivityBySample(sensSlot, samples)\n\n    # -- perturbatiion slot\n    ##TODO:: do we still need this?\n    \n    # -- curation slot\n    sampleCuration <- curation(x)$sample\n    curation(x)$sample <- sampleCuration[rownames(sampleCuration) %in% samples, ]\n\n    # -- sample slot\n    sampleInf <- sampleInfo(x)\n    sampleInfo(x) <- sampleInf[rownames(sampleInf) %in% samples, ]\n\n    # -- check object is still valid and return\n    tryCatch(checkCsetStructure(x), error = function(e) {})\n\n    return(x)\n})\n\n.subsetMolecularProfilesBySample <- function(slotData, samples) {\n    funContext <- .funContext(':::.subsetMolecularProfilesBySample')\n    if (is(slotData, 'MultiAssayExperiment')) {\n        hasSamples <- colData(slotData)$sampleid %in% samples\n        if (!all(hasSamples)) .warning(funContext, 'Some specified samples are\n            not present in `molecularProfilesSlot(x)`')\n        molecProfs <- slotData[, hasSamples]\n    } else {\n        SEcolData <- lapply(slotData, colData)\n        SEsamples <- lapply(SEcolData, FUN=`[[`, i='sampleid')\n        hasSEsamples <- lapply(SEsamples, FUN=`%in%`, samples)\n        molecProfs <- mapply(`[`, x=slotData, j=hasSEsamples)\n    }\n    return(molecProfs)\n}\n\n.subsetSensitivityBySample <- function(slotData, samples) {\n    funContext <- .funContext(':::.subsetSensitivityBySample')\n    if (is(slotData, 'LongTable')) {\n        slotData <- slotData[, samples]\n    } else {\n        keepSamples <- slotData$info$sampleid %in% samples\n        slotData$profiles <- slotData$profiles[keepSamples, ]\n        slotData$raw <- slotData$raw[keepSamples, , ]\n        slotData$n <- slotData$n[rownames(slotData$n) %in% samples, ]\n        slotData$info <- slotData$info[keepSamples, ]\n    }\n    return(slotData)\n}\n\n## ======================\n## ---- subsetByTreatment\n## ----------------------\n\n#' @export\nsetGeneric('subsetByTreatment', function(x, treatments, ...)\n    standardGeneric('subsetByTreatment'))\n\n#' @noRd\n.docs_CoreSet_subsetByTreatment <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    ## subset methods\n    __subsetByTreatment__: Subset a `{class_}` object by treatment identifier.\n    - value: a `{class_}` object containing only `treatments`.\n\n    @examples\n\n    ## subset methods\n\n    ### subsetByTreatment\n    #treatments <- {treatment_}Info({data_})${treatment_}id[seq_len(10)]\n    #{data_}_sub <- subsetByTreatment({data_}, treatments)\n\n    @md\n    @aliases subsetByTreatment subsetByTreatment,{class_}-method\n    @exportMethod subsetByTreatment\n    \",\n    ...\n)\n\n#' @rdname CoreSet-utils\n#' @eval CoreGx:::.docs_CoreSet_subsetByTreatment(class_=.local_class,\n#' data_=.local_data, treatment_='treatment')\nsetMethod('subsetByTreatment', signature('CoreSet'),\n        function(x, treatments) {\n    funContext <- .S4MethodContext('subsetByTreatment', 'PharmacoSet')\n    treatmentType <- switch(class(x)[1],\n        'PharmacoSet'='drug',\n        'ToxicoSet'='drug',\n        'RadioSet'='radiation',\n        'CoreSet'=return(data.frame())\n    )\n    treatmentNames <- rownames(treatmentInfo(x))\n    if (!all(treatments %in% treatmentNames)) {\n        .warning(funContext, 'Treatments missing from ', class(x)[1], ': ',\n            setdiff(treatments, treatmentNames), '! Please ensure all specified\n            treatments are valid rownames of treatmentInfo(x).\n            Proceeding with the valid treatments only.')\n        treatments <- union(treatments, treatmentNames)\n    }\n    # -- sensitivity slot\n    sensSlot <- treatmentResponse(x)\n    treatmentResponse(x) <- .subsetSensitivityByTreatment(sensSlot, treatments,\n        treatmentType=treatmentType)\n\n    # -- perturbation slot\n    ## TODO: do we still need this?\n\n    # -- curation slot\n    treatmentCuration <- curation(x)[[treatmentType]]\n    curation(x)[[treatmentType]] <- treatmentCuration[\n        rownames(treatmentCuration) %in% treatments, ]\n\n    # -- treatment slot\n    treatmentInf <- treatmentInfo(x)\n    treatmentInfo(x) <- treatmentInf[rownames(treatmentInf) %in% treatments, ]\n\n    # -- molecularProfiles\n    # deal with potential loss of samples when subsetting by treatment\n    keepSamples <- sampleNames(x)\n    molecSlot <- molecularProfilesSlot(x)\n    molecularProfilesSlot(x) <- .subsetMolecularProfilesBySample(molecSlot,\n        keepSamples)\n\n    # -- check object is still valid and return\n    tryCatch(checkCsetStructure(x), error = function(e) {})\n\n    return(x)\n})\n\n.subsetSensitivityByTreatment <- function(slotData, treatments,\n        treatmentType) {\n    funContext <- .funContext(':::.subsetSensitivityByTreatment')\n    treatmentId <- if (treatmentType == 'radiation')\n        paste0(treatmentType, '.type') else paste0(treatmentType, 'id')\n    if (is(slotData, 'LongTable')) {\n        slotData <- slotData[treatments, ]\n    } else {\n        keepTreatments <- slotData$info[[treatmentId]] %in% treatments\n        slotData$profiles <- slotData$profiles[keepTreatments, ]\n        slotData$raw <- slotData$raw[keepTreatments, , ]\n        slotData$info <- slotData$info[keepTreatments, ]\n        slotData$n <- slotData$n[, colnames(slotData$n) %in% treatments]\n    }\n    return(slotData)\n}\n\n\n## ====================\n## ---- subsetByFeature\n## --------------------\n\n\n#' @export\nsetGeneric('subsetByFeature', function(x, features, ...)\n    standardGeneric('subsetByFeature'))\n\n#' @noRd\n.docs_CoreSet_subsetByFeature <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    ## subset methods\n    __subsetByFeature__: Subset a `{class_}` object by molecular feature\n        identifier.\n    - value: a `{class_}` object containing only `features`.\n\n    @examples\n\n    ## subset methods\n\n    ### subsetByFeature\n    features <- fNames({data_}, 'rna')[seq_len(5)]\n    {data_}_sub <- subsetByFeature({data_}, features, 'rna')\n\n    @md\n    @aliases subsetByFeature subsetByFeature,{class_}-method\n    @importFrom MultiAssayExperiment MultiAssayExperiment\n    @exportMethod subsetByFeature\n    \",\n    ...\n)\n\n#' @rdname CoreSet-utils\n#' @eval .docs_CoreSet_subsetByFeature(class_=.local_class, data_=.local_data)\nsetMethod('subsetByFeature', signature(x='CoreSet'),\n        function(x, features, mDataTypes) {\n    slotData <- molecularProfilesSlot(x)\n    MAE <- if (!is(slotData, 'MultiAssayExperiment'))\n        MultiAssayExperiment(slotData) else slotData\n    if (missing(mDataTypes)) mDataTypes <- names(MAE)\n    suppressMessages({\n        suppressWarnings({\n            MAE_sub <- MAE[, , mDataTypes]\n        })\n    })\n    keepFeatures <- rownames(MAE_sub) %in% features\n    subsetMAE <- MAE[keepFeatures, drop=TRUE]\n    newSlotData <- if (is(slotData, 'MultiAssayExperiment')) subsetMAE else\n        as.list(experiments(subsetMAE))\n    molecularProfilesSlot(x) <- newSlotData\n    ## TODO:: What if this drops samples from the PSet?\n    return(x)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.docs_CoreSet_utils` function in this code snippet?",
        "answer": "The `.docs_CoreSet_utils` function is used to generate dynamic documentation for utility methods of a `CoreSet` object. It creates a Roxygen2 documentation string that describes the utility methods, their parameters, and return values. This function is used to maintain consistent and easily updatable documentation for the `CoreSet` class."
      },
      {
        "question": "How does the `subsetBySample` method handle invalid sample names?",
        "answer": "The `subsetBySample` method checks if all provided sample names are valid by comparing them to the rownames of `sampleInfo(x)`. If any invalid sample names are found, it issues a warning message and proceeds with only the valid samples. It uses the `union` function to ensure that only existing samples are used for subsetting, effectively ignoring any invalid sample names."
      },
      {
        "question": "What is the purpose of the `.subsetMolecularProfilesBySample` function, and how does it handle different types of input data?",
        "answer": "The `.subsetMolecularProfilesBySample` function is used to subset molecular profiles data by sample. It handles two types of input data: 1) If the input is a `MultiAssayExperiment` object, it subsets the data using column names. 2) If the input is not a `MultiAssayExperiment`, it assumes the data is a list of `SummarizedExperiment` objects and subsets each element individually. This function ensures that molecular profile data is correctly subset regardless of its structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('subsetBySample', signature('CoreSet'), function(x, samples) {\n    funContext <- .S4MethodContext('subsetBySample', 'CoreSet')\n\n    sampleNames <- rownames(sampleInfo(x))\n    if (!all(samples %in% sampleNames)) {\n        .warning(funContext, 'Samples missing from ', class(x)[1], ': ',\n            setdiff(samples, sampleNames), '! Please ensure all specified\n            samples are valid rownames of sampleInfo(x). Proceeding with\n            the valid samples only.')\n        samples <- union(samples, sampleNames)\n    }\n\n    # -- molecularProfiles slot\n    molecSlot <- molecularProfilesSlot(x)\n    molecularProfilesSlot(x) <-\n        .subsetMolecularProfilesBySample(molecSlot, samples)\n\n    # -- sensitivity slot\n    sensSlot <- treatmentResponse(x)\n    treatmentResponse(x) <- .subsetSensitivityBySample(sensSlot, samples)\n\n    # -- curation slot\n    sampleCuration <- curation(x)$sample\n    curation(x)$sample <- sampleCuration[rownames(sampleCuration) %in% samples, ]\n\n    # -- sample slot\n    sampleInf <- sampleInfo(x)\n    sampleInfo(x) <- sampleInf[rownames(sampleInf) %in% samples, ]\n\n    # -- check object is still valid and return\n    tryCatch(checkCsetStructure(x), error = function(e) {})\n\n    return(x)\n})",
        "complete": "setMethod('subsetBySample', signature('CoreSet'), function(x, samples) {\n    funContext <- .S4MethodContext('subsetBySample', 'CoreSet')\n\n    sampleNames <- rownames(sampleInfo(x))\n    if (!all(samples %in% sampleNames)) {\n        .warning(funContext, 'Samples missing from ', class(x)[1], ': ',\n            setdiff(samples, sampleNames), '! Please ensure all specified\n            samples are valid rownames of sampleInfo(x). Proceeding with\n            the valid samples only.')\n        samples <- intersect(samples, sampleNames)\n    }\n\n    # -- molecularProfiles slot\n    molecSlot <- molecularProfilesSlot(x)\n    molecularProfilesSlot(x) <-\n        .subsetMolecularProfilesBySample(molecSlot, samples)\n\n    # -- sensitivity slot\n    sensSlot <- treatmentResponse(x)\n    treatmentResponse(x) <- .subsetSensitivityBySample(sensSlot, samples)\n\n    # -- curation slot\n    sampleCuration <- curation(x)$sample\n    curation(x)$sample <- sampleCuration[rownames(sampleCuration) %in% samples, , drop=FALSE]\n\n    # -- sample slot\n    sampleInf <- sampleInfo(x)\n    sampleInfo(x) <- sampleInf[rownames(sampleInf) %in% samples, , drop=FALSE]\n\n    # -- check object is still valid and return\n    validObject(x)\n\n    return(x)\n})"
      },
      {
        "partial": "setMethod('subsetByFeature', signature(x='CoreSet'),\n        function(x, features, mDataTypes) {\n    slotData <- molecularProfilesSlot(x)\n    MAE <- if (!is(slotData, 'MultiAssayExperiment'))\n        MultiAssayExperiment(slotData) else slotData\n    if (missing(mDataTypes)) mDataTypes <- names(MAE)\n    suppressMessages({\n        suppressWarnings({\n            MAE_sub <- MAE[, , mDataTypes]\n        })\n    })\n    keepFeatures <- rownames(MAE_sub) %in% features\n    subsetMAE <- MAE[keepFeatures, drop=TRUE]\n    newSlotData <- if (is(slotData, 'MultiAssayExperiment')) subsetMAE else\n        as.list(experiments(subsetMAE))\n    molecularProfilesSlot(x) <- newSlotData\n    return(x)\n})",
        "complete": "setMethod('subsetByFeature', signature(x='CoreSet'),\n        function(x, features, mDataTypes) {\n    slotData <- molecularProfilesSlot(x)\n    MAE <- if (!is(slotData, 'MultiAssayExperiment'))\n        MultiAssayExperiment(slotData) else slotData\n    if (missing(mDataTypes)) mDataTypes <- names(MAE)\n    MAE_sub <- MAE[, , mDataTypes, drop=FALSE]\n    keepFeatures <- rownames(MAE_sub) %in% features\n    subsetMAE <- MAE_sub[keepFeatures, , drop=FALSE]\n    newSlotData <- if (is(slotData, 'MultiAssayExperiment')) subsetMAE else\n        as.list(experiments(subsetMAE))\n    molecularProfilesSlot(x) <- newSlotData\n    validObject(x)\n    return(x)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/linearQuadratic.R",
    "language": "R",
    "content": "# Linear quadratic\n#\n# @param D A vector of drug concentrations\n# @param pars Parameters (a, b) of the linear model\n# @param SF_as_log Boolen indicating whether survival fraction is logged\n#\n# @return \\code{numeric} The survival fraction for a linear quadratic model\n.linearQuadratic <- function(D, pars, SF_as_log = TRUE) {\n  SF <- -(pars[[1]] * D + pars[[2]] * D ^ 2)\n  if (!SF_as_log) {\n    SF <- exp(SF)\n  }\n  return(SF)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.linearQuadratic` function and what are its input parameters?",
        "answer": "The `.linearQuadratic` function calculates the survival fraction for a linear quadratic model. It takes three input parameters: 'D' (a vector of drug concentrations), 'pars' (a list of parameters 'a' and 'b' for the linear model), and 'SF_as_log' (a boolean indicating whether the survival fraction should be returned as a log value, defaulting to TRUE)."
      },
      {
        "question": "How does the function calculate the survival fraction (SF) and what does the equation represent?",
        "answer": "The function calculates the survival fraction using the equation: SF = -(pars[[1]] * D + pars[[2]] * D^2). This represents a linear quadratic model where pars[[1]] is the linear coefficient (a) and pars[[2]] is the quadratic coefficient (b). The negative sign indicates that higher drug concentrations (D) lead to lower survival fractions."
      },
      {
        "question": "What is the purpose of the `SF_as_log` parameter, and how does it affect the function's output?",
        "answer": "The `SF_as_log` parameter determines whether the survival fraction is returned as a log value or as a regular probability. If `SF_as_log` is TRUE (default), the function returns the log survival fraction. If it's FALSE, the function applies exp(SF) to convert the log survival fraction to a regular probability before returning it. This allows flexibility in how the results are used or interpreted."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Linear quadratic\n#\n# @param D A vector of drug concentrations\n# @param pars Parameters (a, b) of the linear model\n# @param SF_as_log Boolen indicating whether survival fraction is logged\n#\n# @return \\code{numeric} The survival fraction for a linear quadratic model\n.linearQuadratic <- function(D, pars, SF_as_log = TRUE) {\n  SF <- -(pars[[1]] * D + pars[[2]] * D ^ 2)\n  # Complete the function\n}",
        "complete": "# Linear quadratic\n#\n# @param D A vector of drug concentrations\n# @param pars Parameters (a, b) of the linear model\n# @param SF_as_log Boolen indicating whether survival fraction is logged\n#\n# @return \\code{numeric} The survival fraction for a linear quadratic model\n.linearQuadratic <- function(D, pars, SF_as_log = TRUE) {\n  SF <- -(pars[[1]] * D + pars[[2]] * D ^ 2)\n  if (!SF_as_log) SF <- exp(SF)\n  SF\n}"
      },
      {
        "partial": "# Linear quadratic\n#\n# @param D A vector of drug concentrations\n# @param pars Parameters (a, b) of the linear model\n# @param SF_as_log Boolen indicating whether survival fraction is logged\n#\n# @return \\code{numeric} The survival fraction for a linear quadratic model\n.linearQuadratic <- function(D, pars, SF_as_log = TRUE) {\n  # Complete the function body\n}",
        "complete": "# Linear quadratic\n#\n# @param D A vector of drug concentrations\n# @param pars Parameters (a, b) of the linear model\n# @param SF_as_log Boolen indicating whether survival fraction is logged\n#\n# @return \\code{numeric} The survival fraction for a linear quadratic model\n.linearQuadratic <- function(D, pars, SF_as_log = TRUE) {\n  SF <- -(pars[[1]] * D + pars[[2]] * D ^ 2)\n  if (!SF_as_log) SF <- exp(SF)\n  SF\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/matthewCor.R",
    "language": "R",
    "content": "## Matthews correlatipon coefficient\n## TODO:: Give this function a more descriptive name\n#' Compute a Mathews Correlation Coefficient\n#'\n#' The function computes a Matthews correlation coefficient for two factors\n#' provided to the function. It assumes each factor is a factor of class labels,\n#' and the enteries are paired in order of the vectors.\n#'\n#' Please note: we recommend you call set.seed() before using this function to\n#' ensure the reproducibility of your results. Write down the seed number or\n#' save it in a script if you intend to use the results in a publication.\n#'\n#' @examples\n#' x <- factor(c(1,2,1,2,3,1))\n#' y <- factor(c(2,1,1,1,2,2))\n#' mcc(x,y)\n#'\n#' @param x,y \\code{factor} of the same length with the same number of levels\n#' @param nperm \\code{numeric} number of permutations for significance\n#' estimation. If 0, no permutation testing is done\n#' @param nthread \\code{numeric} can parallelize permutation texting using\n#'   BiocParallels bplapply\n#' @param alternative indicates the alternative hypothesis and must be one of\n#' \u2018\"two.sided\"\u2019, \u2018\"greater\"\u2019 or \u2018\"less\"\u2019.  You can specify just\n#' the initial letter.  \u2018\"greater\"\u2019 corresponds to positive\n#' association, \u2018\"less\"\u2019 to negative association.\n#' @param ... \\code{list} Additional arguments\n#'\n#' @return A list with the MCC as the $estimate, and p value as $p.value\n#' @export\nmcc <- function(x, y, nperm = 1000, nthread = 1, alternative=c(\"two.sided\", \"less\", \"greater\"), ...) {\n    # PARAMETER CHANGE WARNING\n    if (!missing(...)) {\n        if (\"setseed\" %in% names(...)) {\n            warning(\"The setseed parameter has been removed in this release to conform\n              to Bioconductor coding standards. Please call set.seed in your\n              script before running this function.\")\n        }\n    }\n    alternative <- match.arg(alternative)\n    if ((length(x) != length(y)) || (!is.factor(x) || length(levels(x)) < 2) || (!is.factor(y) || length(levels(y)) < 2)) {\n        stop(\"x and y must be factors of the same length with at least two levels\")\n    }\n    if (length(levels(x)) != length(levels(y))) {\n\n        warning(\"The number of levels x and y was different. Taking the union of all class labels.\")\n        levels(x) <- union(levels(x), levels(y))\n        levels(y) <- union(levels(x), levels(y))\n\n    }\n    res <- list(estimate = NA, p.value = NA)\n    ## compute MCC\n    res[[\"estimate\"]] <- .mcc(ct = table(x, y))\n    ## compute significance of MCC using a permutation test\n    if (nperm > 0) {\n        splitix <- parallel::splitIndices(nx = nperm, ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- BiocParallel::bplapply(splitix, function(x, xx, yy) {\n            res <- vapply(x, function(x, xx, yy) {\n                xx <- sample(xx)\n                yy <- sample(yy)\n                return(.mcc(ct = table(xx, yy)))\n            }, xx = xx, yy = yy, FUN.VALUE = numeric(1))\n            return(res)\n        }, xx = x, yy = y)\n        mcres <- unlist(mcres)\n        #res[[\"p.value\"]] <- sum(mcres > res[\"estimate\"])/sum(!is.na(mcres))\n        switch(alternative, \"two.sided\" = {\n            if(res[[\"estimate\"]] > 0) {\n                res[[\"p.value\"]] <- 2*sum(res[[\"estimate\"]] <=  mcres )/sum(!is.na(mcres))\n            } else {\n                res[[\"p.value\"]] <- 2*sum(res[[\"estimate\"]] >=  mcres )/sum(!is.na(mcres))\n            }\n        }, \"less\" = {\n            res[[\"p.value\"]] <- sum(res[[\"estimate\"]] >=  mcres )/sum(!is.na(mcres)) \n        }, \"greater\" = {\n            res[[\"p.value\"]] <- sum(res[[\"estimate\"]] <=  mcres )/sum(!is.na(mcres))\n        })\n        if (res[\"p.value\"] == 0) {\n            res[\"p.value\"] <- 1/(nperm + 1)\n        }\n    }\n    return(res)\n}\n\n## Helper functions\n\n\n## Just a lot of math, multiclass MCC\n## https://en.wikipedia.org/wiki/Matthews_correlation_coefficient#Multiclass_case\n.mcc <-\n  function(ct, nbcat=nrow(ct)) {\n    if(nrow(ct) != ncol(ct)) { stop(\"the confusion table should be square!\") }\n\n    ## The following code checks if there would be a division by 0 error in the computation on the Matthew's\n    ## correlation coefficient. In practice, this occurs when there are multiple categories but all predictions end up\n    ## in only one category - in this case, the denominator is 0. This is dealt with by adding a psuedocount to each.\n    ## This is chosen because the mcc of a matrix of 1s is 0, and such should not affect the value in expectation.\n    ## Note this is not necessary when alll entries lie on the diagonal.\n    if( !(sum(ct)==sum(diag(ct))) && ## Check if all entries are on the diagonal. If they are, no adjustment necessary.\n        ((sum(rowSums(ct) == 0) == (nbcat-1)) || ## Otherwise, check if there is entries only in one column or only in one row.\n        (sum(colSums(ct) == 0) == (nbcat-1)))) {\n        ct <- ct + matrix(1, ncol=nbcat, nrow=nbcat)\n      } ### add element to categories if nbcat-1 predictive categories do not contain elements. Not in case where all are correct!\n\n    if (sum(ct, na.rm = TRUE) <= 0) {\n        return(NA)\n    }\n\n    myx <- matrix(TRUE, nrow = nrow(ct), ncol = ncol(ct))\n    diag(myx) <- FALSE\n    if (sum(ct[myx]) == 0) {\n        return(1)\n    }\n    myperf <- 0\n    for (k in seq_len(nbcat)) {\n        for (m in seq_len(nbcat)) {\n            for (l in seq_len(nbcat)) {\n                myperf <- myperf + ((ct[k, k] * ct[m, l]) - (ct[l, k] * ct[k, m]))\n            }\n        }\n    }\n    aa <- 0\n    for (k in seq_len(nbcat)) {\n        cc <- 0\n        for (l in seq_len(nbcat)) {\n            cc <- cc + ct[l, k]\n        }\n        dd <- 0\n        for (f in seq_len(nbcat)) {\n            for (g in seq_len(nbcat)) {\n                if (f != k) {\n                  dd <- dd + ct[g, f]\n                }\n            }\n        }\n        aa <- aa + (cc * dd)\n    }\n    bb <- 0\n    for (k in seq_len(nbcat)) {\n        cc <- 0\n        for (l in seq_len(nbcat)) {\n            cc <- cc + ct[k, l]\n        }\n        dd <- 0\n        for (f in seq_len(nbcat)) {\n            for (g in seq_len(nbcat)) {\n                if (f != k) {\n                  dd <- dd + ct[f, g]\n                }\n            }\n        }\n        bb <- bb + (cc * dd)\n    }\n\n    myperf <- myperf/(sqrt(aa) * sqrt(bb))\n    return(myperf)\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mcc` function and what does it return?",
        "answer": "The `mcc` function computes the Matthews Correlation Coefficient (MCC) for two factors. It takes two factors `x` and `y` as input, along with optional parameters for permutation testing. The function returns a list containing two elements: $estimate (the MCC value) and $p.value (the significance of the MCC if permutation testing is performed)."
      },
      {
        "question": "How does the function handle factors with different numbers of levels?",
        "answer": "If the input factors `x` and `y` have different numbers of levels, the function issues a warning and takes the union of all class labels. It then assigns this union of levels to both `x` and `y`, effectively expanding their level sets to include all unique levels from both factors."
      },
      {
        "question": "Explain the purpose of the `.mcc` helper function and how it handles potential division by zero errors.",
        "answer": "The `.mcc` helper function calculates the actual Matthews Correlation Coefficient for a confusion table. To handle potential division by zero errors, it checks if all predictions fall into a single category. If so, it adds a pseudocount of 1 to each cell in the confusion table. This adjustment prevents division by zero while maintaining the expected MCC value of 0 for a matrix of 1s."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mcc <- function(x, y, nperm = 1000, nthread = 1, alternative=c(\"two.sided\", \"less\", \"greater\"), ...) {\n    alternative <- match.arg(alternative)\n    if ((length(x) != length(y)) || (!is.factor(x) || length(levels(x)) < 2) || (!is.factor(y) || length(levels(y)) < 2)) {\n        stop(\"x and y must be factors of the same length with at least two levels\")\n    }\n    if (length(levels(x)) != length(levels(y))) {\n        warning(\"The number of levels x and y was different. Taking the union of all class labels.\")\n        levels(x) <- union(levels(x), levels(y))\n        levels(y) <- union(levels(x), levels(y))\n    }\n    res <- list(estimate = NA, p.value = NA)\n    res[\"estimate\"] <- .mcc(ct = table(x, y))\n    if (nperm > 0) {\n        # TODO: Implement permutation test\n    }\n    return(res)\n}",
        "complete": "mcc <- function(x, y, nperm = 1000, nthread = 1, alternative=c(\"two.sided\", \"less\", \"greater\"), ...) {\n    alternative <- match.arg(alternative)\n    if ((length(x) != length(y)) || (!is.factor(x) || length(levels(x)) < 2) || (!is.factor(y) || length(levels(y)) < 2)) {\n        stop(\"x and y must be factors of the same length with at least two levels\")\n    }\n    if (length(levels(x)) != length(levels(y))) {\n        warning(\"The number of levels x and y was different. Taking the union of all class labels.\")\n        levels(x) <- union(levels(x), levels(y))\n        levels(y) <- union(levels(x), levels(y))\n    }\n    res <- list(estimate = NA, p.value = NA)\n    res[\"estimate\"] <- .mcc(ct = table(x, y))\n    if (nperm > 0) {\n        splitix <- parallel::splitIndices(nx = nperm, ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- BiocParallel::bplapply(splitix, function(x, xx, yy) {\n            vapply(x, function(x, xx, yy) {\n                .mcc(ct = table(sample(xx), sample(yy)))\n            }, xx = xx, yy = yy, FUN.VALUE = numeric(1))\n        }, xx = x, yy = y)\n        mcres <- unlist(mcres)\n        res[\"p.value\"] <- switch(alternative,\n            \"two.sided\" = 2 * min(sum(res[\"estimate\"] <= mcres), sum(res[\"estimate\"] >= mcres)) / sum(!is.na(mcres)),\n            \"less\" = sum(res[\"estimate\"] >= mcres) / sum(!is.na(mcres)),\n            \"greater\" = sum(res[\"estimate\"] <= mcres) / sum(!is.na(mcres))\n        )\n        if (res[\"p.value\"] == 0) res[\"p.value\"] <- 1 / (nperm + 1)\n    }\n    return(res)\n}"
      },
      {
        "partial": ".mcc <- function(ct, nbcat=nrow(ct)) {\n    if(nrow(ct) != ncol(ct)) { stop(\"the confusion table should be square!\") }\n    if(sum(ct)==sum(diag(ct)) || \n       !((sum(rowSums(ct) == 0) == (nbcat-1)) || (sum(colSums(ct) == 0) == (nbcat-1)))) {\n        # No adjustment necessary\n    } else {\n        ct <- ct + matrix(1, ncol=nbcat, nrow=nbcat)\n    }\n    if (sum(ct, na.rm = TRUE) <= 0) return(NA)\n    myx <- matrix(TRUE, nrow = nrow(ct), ncol = ncol(ct))\n    diag(myx) <- FALSE\n    if (sum(ct[myx]) == 0) return(1)\n    # TODO: Implement MCC calculation\n}",
        "complete": ".mcc <- function(ct, nbcat=nrow(ct)) {\n    if(nrow(ct) != ncol(ct)) { stop(\"the confusion table should be square!\") }\n    if(sum(ct)==sum(diag(ct)) || \n       !((sum(rowSums(ct) == 0) == (nbcat-1)) || (sum(colSums(ct) == 0) == (nbcat-1)))) {\n        # No adjustment necessary\n    } else {\n        ct <- ct + matrix(1, ncol=nbcat, nrow=nbcat)\n    }\n    if (sum(ct, na.rm = TRUE) <= 0) return(NA)\n    myx <- matrix(TRUE, nrow = nrow(ct), ncol = ncol(ct))\n    diag(myx) <- FALSE\n    if (sum(ct[myx]) == 0) return(1)\n    myperf <- sum(sapply(seq_len(nbcat), function(k) {\n        sum(sapply(seq_len(nbcat), function(m) {\n            sum(sapply(seq_len(nbcat), function(l) {\n                (ct[k, k] * ct[m, l]) - (ct[l, k] * ct[k, m])\n            }))\n        }))\n    }))\n    aa <- sum(sapply(seq_len(nbcat), function(k) {\n        cc <- sum(ct[, k])\n        dd <- sum(ct[, -k])\n        cc * dd\n    }))\n    bb <- sum(sapply(seq_len(nbcat), function(k) {\n        cc <- sum(ct[k, ])\n        dd <- sum(ct[-k, ])\n        cc * dd\n    }))\n    myperf / (sqrt(aa) * sqrt(bb))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-guessMapping.R",
    "language": "R",
    "content": "#' Generic for Guessing the Mapping Between Some Raw Data and an S4 Object\n#'\n#' @param object An `S4` object containing so raw data to guess data to\n#'   object slot mappings for.\n#' @param ... Allow new arguments to be defined for this generic.\n#'\n#' @return A `list` with mapping guesses as items.\n#'\n#' @examples\n#' \"Generics shouldn't need examples!\"\n#'\n#' @md\n#' @export\nsetGeneric('guessMapping', function(object, ...) standardGeneric('guessMapping'))\n\n#' Guess which columns in raw experiment data map to which dimensions.\n#'\n#' Checks for columns which are uniquely identified by a group of identifiers.\n#' This should be used to help identify the columns required to uniquely\n#' identify the rows, columns, assays and metadata of a `DataMapper` class\n#' object.\n#'\n#' @details\n#' Any unmapped columns will be added to the end of the returned `list` in an\n#' item called unmapped.\n#'\n#' The function automatically guesses metadata by checking if any columns have\n#' only a single value. This is returned as an additional item in the list.\n#'\n#' @param object A `LongTableDataMapper` object.\n#' @param groups A `list` containing one or more vector of column names\n#'   to group-by. The function uses these to determine 1:1 mappings between\n#'   the combination of columns in each vector and unique values in the raw\n#'   data columns.\n#' @param subset A `logical` vector indicating whether to to subset out mapped\n#'   columns after each grouping. Must be a single `TRUE` or `FALSE` or have\n#'   the same length as groups, indicating whether to subset out mapped columns\n#'   after each grouping. This will prevent mapping a column to two different\n#'   groups.\n#' @param data A `logical` vector indicating whether you would like the data\n#'   for mapped columns to be returned instead of their column names. Defaults\n#'   to `FALSE` for easy use assigning mapped columns to a `DataMapper` object.\n#'\n#' @return A `list`, where each item is named for the associated `groups` item\n#' the guess is for. The character vector in each item are columns which are\n#' uniquely identified by the identifiers from that group.\n#'\n#' @examples\n#' guessMapping(exampleDataMapper, groups=list(rows='treatmentid', cols='sampleid'),\n#' subset=FALSE)\n#'\n#' @md\n#' @export\nsetMethod('guessMapping', signature(object='LongTableDataMapper'),\n        function(object, groups, subset, data=FALSE) {\n    funContext <- '[CoreGx::guessMapping,LongTableDataMapper-method]\\n\\t'\n\n    # Extract the raw data\n    mapData <- copy(rawdata(object))\n    if (!is.data.table(mapData)) setDT(mapData)\n\n    # Error handle for subset parameter\n    if (!(length(subset) == length(groups) || length(subset) == 1))\n        stop(.errorMsg(funContext, ' The subset parameter must be\n            either length 1 or length equal to the groups parameter!'))\n\n    if (length(subset) == 1) subset <- rep(subset, length(groups))\n\n    # Get the id columns to prevent subsetting them out\n    idCols <- unique(unlist(groups))\n\n    # Map unique columns in the data to the metadata slot\n    metadataColumns <- names(which(vapply(mapData, FUN=.length_unique,\n        numeric(1)) == 1))\n    metadataColumns <- setdiff(metadataColumns, idCols)\n    metadata <- mapData[, .SD, .SDcols=metadataColumns]\n    DT <- mapData[, .SD, .SDcols=!metadataColumns]\n\n    # Check the mappings for each group in groups\n    for (i in seq_along(groups)) {\n        message(funContext, paste0('Mapping for group ', names(groups)[i],\n            ': ', paste0(groups[[i]], collapse=', ')))\n        mappedCols <- checkColumnCardinality(DT, groups[[i]])\n        mappedCols <- setdiff(mappedCols, idCols)\n        assign(names(groups)[i], DT[, .SD, .SDcols=mappedCols])\n        if (subset[i]) DT <- DT[, .SD, .SDcols=!mappedCols]\n    }\n\n    # Merge the results\n    groups <- c(list(metadata=NA), groups)\n    mappings <- mget(names(groups))\n    unmapped <- setdiff(colnames(mapData),\n        unique(c(unlist(groups), unlist(lapply(mappings, colnames)))))\n    if (!data) mappings <- lapply(mappings, colnames)\n    mappings <- Map(f=list, groups, mappings)\n    mappings <- lapply(mappings, FUN=setNames,\n        nm=c('id_columns', 'mapped_columns'))\n\n    mappings[['unmapped']] <- unmapped\n\n    return(mappings)\n})\n\n#' Search a data.frame for 1:`cardinality` relationships between a group\n#'   of columns (your identifiers) and all other columns.\n#'\n#' @param df A `data.frame` to search for 1:`cardinality` mappings with\n#'   the columns in `group`.\n#' @param group A `character` vector of one or more column names to\n#'   check the cardinality of other columns against.\n#' @param cardinality The cardinality of to search for (i.e., 1:`cardinality`)\n#'   relationships with the combination of columns in group. Defaults to 1\n#'   (i.e., 1:1 mappings).\n#' @param ... Fall through arguments to data.table::`[`. For developer use.\n#'   One use case is setting verbose=TRUE to diagnose slow data.table\n#'   operations.\n#'\n#' @return A `character` vector with the names of the columns with\n#'    cardinality of 1:`cardinality` with the columns listed in `group`.\n#'\n#' @examples\n#' df <- rawdata(exampleDataMapper)\n#' checkColumnCardinality(df, group='treatmentid')\n#'\n#' @aliases cardinality\n#'\n#' @md\n#' @importFrom data.table setindexv\n#' @importFrom MatrixGenerics colAlls\n#' @export\ncheckColumnCardinality <- function(df, group, cardinality=1, ...) {\n\n    funContext <- '\\n[CoreGx::checkColumnCardinality]\\n\\t'\n\n    # Copy to prevent accidental modify by references\n    df <- copy(df)\n    if (!is.data.table(df)) setDT(df)\n\n    # Intercept slow data.table group by when nrow == .NGRP\n    setkeyv(df, cols=group)\n    nrowEqualsNGroup <- df[, .N, by=group, ...][, max(N), ...] == 1\n    if (nrowEqualsNGroup) {\n        if (cardinality != 1) stop(.errorMsg(funContext, 'The group argument\n            uniquely identifies each row, so the cardinality is 1:1!'))\n        columnsHaveCardinality <- setdiff(colnames(df), group)\n    } else {\n        dimDT <- df[, lapply(.SD, FUN=uniqueN), by=group, ...]\n        columnsHaveCardinality <- colnames(dimDT)[colAlls(dimDT == cardinality)]\n    }\n\n    return(columnsHaveCardinality)\n}\n#' @export\ncardinality <- checkColumnCardinality",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `guessMapping` generic function and its method for `LongTableDataMapper` objects?",
        "answer": "The `guessMapping` generic function and its method for `LongTableDataMapper` objects are designed to guess the mapping between raw data columns and dimensions of an S4 object. It helps identify columns required to uniquely identify rows, columns, assays, and metadata of a `DataMapper` class object. The method takes groups of column names, checks for unique identifiers, and returns a list of mapped columns for each group, along with metadata and unmapped columns."
      },
      {
        "question": "How does the `checkColumnCardinality` function work, and what is its primary use case?",
        "answer": "The `checkColumnCardinality` function searches a data.frame for 1:cardinality relationships between a group of columns (identifiers) and all other columns. It takes a data.frame, a group of column names, and a cardinality value (default 1). The function returns a character vector with names of columns that have the specified cardinality relationship with the group columns. Its primary use case is to find columns that are uniquely identified by a set of identifier columns, which is crucial for the `guessMapping` function to determine column mappings."
      },
      {
        "question": "What are the key steps in the `guessMapping` method for `LongTableDataMapper` objects?",
        "answer": "The key steps in the `guessMapping` method for `LongTableDataMapper` objects are: 1) Extract and copy the raw data. 2) Handle the subset parameter. 3) Identify metadata columns (columns with only one unique value). 4) Iterate through each group in the 'groups' parameter, using `checkColumnCardinality` to find mapped columns. 5) Optionally subset out mapped columns after each group. 6) Merge the results into a list, including metadata, mapped columns for each group, and unmapped columns. 7) Return the final list of mappings, with options to return column names or actual data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('guessMapping', signature(object='LongTableDataMapper'),\n        function(object, groups, subset, data=FALSE) {\n    funContext <- '[CoreGx::guessMapping,LongTableDataMapper-method]\\n\\t'\n\n    # Extract the raw data\n    mapData <- copy(rawdata(object))\n    if (!is.data.table(mapData)) setDT(mapData)\n\n    # Error handle for subset parameter\n    if (!(length(subset) == length(groups) || length(subset) == 1))\n        stop(.errorMsg(funContext, ' The subset parameter must be\n            either length 1 or length equal to the groups parameter!'))\n\n    if (length(subset) == 1) subset <- rep(subset, length(groups))\n\n    # Get the id columns to prevent subsetting them out\n    idCols <- unique(unlist(groups))\n\n    # Map unique columns in the data to the metadata slot\n    metadataColumns <- names(which(vapply(mapData, FUN=.length_unique,\n        numeric(1)) == 1))\n    metadataColumns <- setdiff(metadataColumns, idCols)\n    metadata <- mapData[, .SD, .SDcols=metadataColumns]\n    DT <- mapData[, .SD, .SDcols=!metadataColumns]\n\n    # TODO: Implement the rest of the function\n\n    return(mappings)\n})",
        "complete": "setMethod('guessMapping', signature(object='LongTableDataMapper'),\n        function(object, groups, subset, data=FALSE) {\n    funContext <- '[CoreGx::guessMapping,LongTableDataMapper-method]\\n\\t'\n\n    mapData <- copy(rawdata(object))\n    if (!is.data.table(mapData)) setDT(mapData)\n\n    if (!(length(subset) == length(groups) || length(subset) == 1))\n        stop(.errorMsg(funContext, ' The subset parameter must be\n            either length 1 or length equal to the groups parameter!'))\n\n    if (length(subset) == 1) subset <- rep(subset, length(groups))\n\n    idCols <- unique(unlist(groups))\n\n    metadataColumns <- names(which(vapply(mapData, FUN=.length_unique,\n        numeric(1)) == 1))\n    metadataColumns <- setdiff(metadataColumns, idCols)\n    metadata <- mapData[, .SD, .SDcols=metadataColumns]\n    DT <- mapData[, .SD, .SDcols=!metadataColumns]\n\n    for (i in seq_along(groups)) {\n        message(funContext, paste0('Mapping for group ', names(groups)[i],\n            ': ', paste0(groups[[i]], collapse=', ')))\n        mappedCols <- checkColumnCardinality(DT, groups[[i]])\n        mappedCols <- setdiff(mappedCols, idCols)\n        assign(names(groups)[i], DT[, .SD, .SDcols=mappedCols])\n        if (subset[i]) DT <- DT[, .SD, .SDcols=!mappedCols]\n    }\n\n    groups <- c(list(metadata=NA), groups)\n    mappings <- mget(names(groups))\n    unmapped <- setdiff(colnames(mapData),\n        unique(c(unlist(groups), unlist(lapply(mappings, colnames)))))\n    if (!data) mappings <- lapply(mappings, colnames)\n    mappings <- Map(f=list, groups, mappings)\n    mappings <- lapply(mappings, FUN=setNames,\n        nm=c('id_columns', 'mapped_columns'))\n\n    mappings[['unmapped']] <- unmapped\n\n    return(mappings)\n})"
      },
      {
        "partial": "checkColumnCardinality <- function(df, group, cardinality=1, ...) {\n    funContext <- '\\n[CoreGx::checkColumnCardinality]\\n\\t'\n\n    df <- copy(df)\n    if (!is.data.table(df)) setDT(df)\n\n    setkeyv(df, cols=group)\n    nrowEqualsNGroup <- df[, .N, by=group, ...][, max(N), ...] == 1\n    if (nrowEqualsNGroup) {\n        if (cardinality != 1) stop(.errorMsg(funContext, 'The group argument\n            uniquely identifies each row, so the cardinality is 1:1!'))\n        columnsHaveCardinality <- setdiff(colnames(df), group)\n    } else {\n        # TODO: Implement the else condition\n    }\n\n    return(columnsHaveCardinality)\n}",
        "complete": "checkColumnCardinality <- function(df, group, cardinality=1, ...) {\n    funContext <- '\\n[CoreGx::checkColumnCardinality]\\n\\t'\n\n    df <- copy(df)\n    if (!is.data.table(df)) setDT(df)\n\n    setkeyv(df, cols=group)\n    nrowEqualsNGroup <- df[, .N, by=group, ...][, max(N), ...] == 1\n    if (nrowEqualsNGroup) {\n        if (cardinality != 1) stop(.errorMsg(funContext, 'The group argument\n            uniquely identifies each row, so the cardinality is 1:1!'))\n        columnsHaveCardinality <- setdiff(colnames(df), group)\n    } else {\n        dimDT <- df[, lapply(.SD, FUN=uniqueN), by=group, ...]\n        columnsHaveCardinality <- colnames(dimDT)[colAlls(dimDT == cardinality)]\n    }\n\n    return(columnsHaveCardinality)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/updateObject-methods.R",
    "language": "R",
    "content": "#' @include RadioSet-accessors.R\nNULL\n\n#' Update the RadioSet class after changes in it struture or API\n#'\n#' @param object A `RadioSet` object to update the class structure for.\n#'\n#' @return `RadioSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"RadioSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    rSet <- as(cSet, \"RadioSet\")\n    # treatment slot\n    colnames(treatmentInfo(rSet)) <- gsub(\"X.radiation.|treatmentid|radiation\",\n        \"treatmentid\", colnames(treatmentInfo(rSet)))\n    # sensitivity slot\n    colnames(sensitivityInfo(rSet)) <- gsub(\"treatmentid\", \"treatmentid\",\n        colnames(sensitivityInfo(rSet)))\n    # curation slot\n    names(curation(rSet)) <- gsub(\"radiation\", \"treatment\", names(curation(rSet)))\n    if (\"radiation\" %in% names(curation(rSet))) {\n        colnames(curation(rSet)$radiation) <- gsub(\"treatmentid|radiation\",\n            \"treatment\", colnames(curation(rSet)$radiation))\n    }\n    # molecularProfiles slot\n    for (i in seq_along(molecularProfilesSlot(rSet))) {\n        colnames(colData(molecularProfilesSlot(rSet)[[i]])) <-\n            gsub(\"treatmentid|radiation\", \"treatmentid\",\n                colnames(colData(molecularProfilesSlot(rSet)[[i]]))\n            )\n    }\n    validObject(rSet)\n    return(rSet)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `updateObject` method for the `RadioSet` class?",
        "answer": "The `updateObject` method is designed to update the `RadioSet` class structure after changes in its structure or API. It performs several tasks:\n1. Calls the superclass method using `callNextMethod`\n2. Converts the result back to a `RadioSet` object\n3. Updates column names in various slots (treatmentInfo, sensitivityInfo, curation, molecularProfiles) to ensure consistency\n4. Validates the updated object\n5. Returns the updated `RadioSet` object"
      },
      {
        "question": "How does the method handle the 'treatment' terminology in different slots of the `RadioSet` object?",
        "answer": "The method standardizes the terminology related to 'treatment' across different slots:\n1. In the treatmentInfo slot, it replaces 'X.radiation.', 'treatmentid', and 'radiation' with 'treatmentid'\n2. In the sensitivityInfo slot, it ensures 'treatmentid' is used\n3. In the curation slot, it replaces 'radiation' with 'treatment' in the names\n4. If a 'radiation' element exists in the curation slot, it renames its columns from 'treatmentid' or 'radiation' to 'treatment'\n5. In the molecularProfiles slot, it replaces 'treatmentid' or 'radiation' with 'treatmentid' in column names\nThis standardization ensures consistency across the object's structure."
      },
      {
        "question": "What are the key steps in the `updateObject` method to ensure the updated `RadioSet` object is valid and properly structured?",
        "answer": "The key steps to ensure the updated `RadioSet` object is valid and properly structured are:\n1. Calling the superclass method with `callNextMethod(object)` to perform any necessary updates from the parent class\n2. Converting the result back to a `RadioSet` object using `as(cSet, \"RadioSet\")`\n3. Updating column names and slot names to ensure consistency in terminology\n4. Iterating through the molecularProfiles slot to update column names in each profile\n5. Calling `validObject(rSet)` to verify that the updated object meets all validity requirements\n6. Returning the validated and updated `RadioSet` object\nThese steps ensure that the object maintains its structure and validity after the update process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"updateObject\", signature(\"RadioSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    rSet <- as(cSet, \"RadioSet\")\n    # Update treatment slot\n    colnames(treatmentInfo(rSet)) <- gsub(\"X.radiation.|treatmentid|radiation\",\n        \"treatmentid\", colnames(treatmentInfo(rSet)))\n    # Update sensitivity slot\n    colnames(sensitivityInfo(rSet)) <- gsub(\"treatmentid\", \"treatmentid\",\n        colnames(sensitivityInfo(rSet)))\n    # Update curation slot\n    names(curation(rSet)) <- gsub(\"radiation\", \"treatment\", names(curation(rSet)))\n    if (\"radiation\" %in% names(curation(rSet))) {\n        colnames(curation(rSet)$radiation) <- gsub(\"treatmentid|radiation\",\n            \"treatment\", colnames(curation(rSet)$radiation))\n    }\n    # Update molecularProfiles slot\n    # TODO: Complete the code to update the molecularProfiles slot\n    \n    validObject(rSet)\n    return(rSet)\n})",
        "complete": "setMethod(\"updateObject\", signature(\"RadioSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    rSet <- as(cSet, \"RadioSet\")\n    # Update treatment slot\n    colnames(treatmentInfo(rSet)) <- gsub(\"X.radiation.|treatmentid|radiation\",\n        \"treatmentid\", colnames(treatmentInfo(rSet)))\n    # Update sensitivity slot\n    colnames(sensitivityInfo(rSet)) <- gsub(\"treatmentid\", \"treatmentid\",\n        colnames(sensitivityInfo(rSet)))\n    # Update curation slot\n    names(curation(rSet)) <- gsub(\"radiation\", \"treatment\", names(curation(rSet)))\n    if (\"radiation\" %in% names(curation(rSet))) {\n        colnames(curation(rSet)$radiation) <- gsub(\"treatmentid|radiation\",\n            \"treatment\", colnames(curation(rSet)$radiation))\n    }\n    # Update molecularProfiles slot\n    for (i in seq_along(molecularProfilesSlot(rSet))) {\n        colnames(colData(molecularProfilesSlot(rSet)[[i]])) <-\n            gsub(\"treatmentid|radiation\", \"treatmentid\",\n                colnames(colData(molecularProfilesSlot(rSet)[[i]]))\n            )\n    }\n    validObject(rSet)\n    return(rSet)\n})"
      },
      {
        "partial": "#' Update the RadioSet class after changes in it struture or API\n#'\n#' @param object A `RadioSet` object to update the class structure for.\n#'\n#' @return `RadioSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"RadioSet\"), function(object) {\n    # TODO: Implement the updateObject method for RadioSet\n    # Hint: Use callNextMethod, update various slots, and ensure the object is valid\n})",
        "complete": "#' Update the RadioSet class after changes in it struture or API\n#'\n#' @param object A `RadioSet` object to update the class structure for.\n#'\n#' @return `RadioSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"RadioSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    rSet <- as(cSet, \"RadioSet\")\n    colnames(treatmentInfo(rSet)) <- gsub(\"X.radiation.|treatmentid|radiation\", \"treatmentid\", colnames(treatmentInfo(rSet)))\n    colnames(sensitivityInfo(rSet)) <- gsub(\"treatmentid\", \"treatmentid\", colnames(sensitivityInfo(rSet)))\n    names(curation(rSet)) <- gsub(\"radiation\", \"treatment\", names(curation(rSet)))\n    if (\"radiation\" %in% names(curation(rSet))) {\n        colnames(curation(rSet)$radiation) <- gsub(\"treatmentid|radiation\", \"treatment\", colnames(curation(rSet)$radiation))\n    }\n    for (i in seq_along(molecularProfilesSlot(rSet))) {\n        colnames(colData(molecularProfilesSlot(rSet)[[i]])) <- gsub(\"treatmentid|radiation\", \"treatmentid\", colnames(colData(molecularProfilesSlot(rSet)[[i]])))\n    }\n    validObject(rSet)\n    return(rSet)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/src/Math.cpp",
    "language": "cpp",
    "content": "#include \"Math.h\"\n\nMath::IndirectComparator::IndirectComparator(double const* const pSamples,\n        unsigned int const* const pSampleIndices) :\n        mpSamples(pSamples), mpSampleIndices(pSampleIndices)\n{\n\n}\n\nbool const\nMath::IndirectComparator::operator()(unsigned int const i, unsigned int const j) const\n{\n    return mpSamples[mpSampleIndices[i]] < mpSamples[mpSampleIndices[j]];\n}\n\n/* static */void const\nMath::computeCausality(double* const pCausalityArray, Matrix const* const pMiMatrix,\n        int const* const pSolutions, unsigned int const solutionCount,\n        unsigned int const featureCountPerSolution, unsigned int const featureCount,\n        unsigned int const targetFeatureIndex)\n{\n    for (unsigned int s = 0; s < solutionCount; ++s)\n    {\n        for (unsigned int i = 0; i < featureCountPerSolution - 1; ++i)\n        {\n            for (unsigned int j = i + 1; j < featureCountPerSolution; ++j)\n            {\n                int const a = pSolutions[(featureCountPerSolution * s) + i];\n                int const b = pSolutions[(featureCountPerSolution * s) + j];\n\n                double const cor_ij =\n                        (std::fabs(pMiMatrix->at(a, b)) > std::fabs(pMiMatrix->at(b, a))) ?\n                                pMiMatrix->at(a, b) : pMiMatrix->at(b, a);\n\n                double const cor_ik = pMiMatrix->at(a, targetFeatureIndex);\n                double const cor_jk = pMiMatrix->at(b, targetFeatureIndex);\n\n                double const coefficient = Math::computeCoInformationLattice(cor_ij, cor_ik,\n                        cor_jk);\n\n                if (pCausalityArray[a] != pCausalityArray[a] || pCausalityArray[a] > coefficient)\n                    pCausalityArray[a] = coefficient;\n\n                if (pCausalityArray[b] != pCausalityArray[b] || pCausalityArray[b] > coefficient)\n                    pCausalityArray[b] = coefficient;\n            }\n        }\n    }\n}\n\n/* static */double const\nMath::computeCoInformationLattice(double const cor_ij, double const cor_ik, double const cor_jk)\n{\n    double const cor_ij_sq = cor_ij * cor_ij;\n    double const cor_jk_sq = cor_jk * cor_jk;\n    double const cor_ik_sq = cor_ik * cor_ik;\n\n    return -.5\n            * std::log(\n                    ((1 - cor_ij_sq) * (1 - cor_ik_sq) * (1 - cor_jk_sq))\n                            / (1 + 2 * cor_ij * cor_ik * cor_jk - cor_ij_sq - cor_ik_sq - cor_jk_sq));\n}\n\n/* static */double const\nMath::computeConcordanceIndex(double const* const pDiscreteSamples,\n        double const* const pContinuousSamples, double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        bool const outX, double* const pConcordantWeights, double* const pDiscordantWeights,\n        double* const pUninformativeWeights, double* const pRelevantWeights)\n{\n    double sum_concordant_weight = 0.;\n    double sum_relevant_weight = 0.;\n\n    for (unsigned int stratum = 0; stratum < sampleStratumCount; ++stratum)\n    {\n        for (unsigned int a = 0; a < pSampleCountPerStratum[stratum]; ++a)\n        {\n            unsigned int const i = pSampleIndicesPerStratum[stratum][a];\n\n            double concordant_weight = 0.;\n            double discordant_weight = 0.;\n            double uninformative_weight = 0.;\n            double relevant_weight = 0.; \n\n            if (pDiscreteSamples[i] != pDiscreteSamples[i]\n                    || pContinuousSamples[i] != pContinuousSamples[i])\n                continue;\n\n            for (unsigned int b = 0; b < pSampleCountPerStratum[stratum]; ++b)\n            {\n                unsigned int const j = pSampleIndicesPerStratum[stratum][b];\n\n                if (pDiscreteSamples[j] != pDiscreteSamples[j]\n                        || pContinuousSamples[j] != pContinuousSamples[j])\n                    continue;\n\n                double pair_weight = pSampleWeights[i] * pSampleWeights[j];\n\n                if (pDiscreteSamples[i] > pDiscreteSamples[j])\n                {\n                    relevant_weight += pair_weight;\n\n                    if (pContinuousSamples[i] > pContinuousSamples[j])\n                        concordant_weight += pair_weight;\n                    else if (pContinuousSamples[i] < pContinuousSamples[j])\n                        discordant_weight += pair_weight;\n                    else if (outX)\n                        uninformative_weight += pair_weight;\n                    else\n                        discordant_weight += pair_weight;\n                }\n                else if (pDiscreteSamples[i] < pDiscreteSamples[j])\n                {\n                    relevant_weight += pair_weight;\n\n                    if (pContinuousSamples[i] < pContinuousSamples[j])\n                        concordant_weight += pair_weight;\n                    else if (pContinuousSamples[i] > pContinuousSamples[j])\n                        discordant_weight += pair_weight;\n                    else if (outX)\n                        uninformative_weight += pair_weight;\n                    else\n                        discordant_weight += pair_weight;\n                }\n            }\n\n            sum_concordant_weight += concordant_weight;\n            sum_relevant_weight += relevant_weight;\n\n            if (pConcordantWeights != 0) // Implicity, the other similar vectors\n            {                            // should also match this condition.\n                pConcordantWeights[i] = concordant_weight;\n                pDiscordantWeights[i] = discordant_weight;\n                pUninformativeWeights[i] = uninformative_weight;\n                pRelevantWeights[i] = relevant_weight;\n            }\n        }\n    }\n\n    return sum_concordant_weight / sum_relevant_weight;\n}\n\n/*static*/double const\nMath::computeConcordanceIndex(double const* const pDiscreteSamples,\n        double const* const pContinuousSamples, double const* const pTimeSamples,\n        double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        bool const outX, double* const pConcordantWeights, double* const pDiscordantWeights,\n        double* const pUninformativeWeights, double* const pRelevantWeights)\n{\n    double sum_concordant_weight = 0.;\n    double sum_relevant_weight = 0.;\n\n    for (unsigned int stratum = 0; stratum < sampleStratumCount; ++stratum)\n    {\n        for (unsigned int a = 0; a < pSampleCountPerStratum[stratum]; ++a)\n        {\n            unsigned int const i = pSampleIndicesPerStratum[stratum][a];\n\n            double concordant_weight = 0.;\n            double discordant_weight = 0.;\n            double uninformative_weight = 0.;\n            double relevant_weight = 0.; \n\n            if (pDiscreteSamples[i] != pDiscreteSamples[i] || pTimeSamples[i] != pTimeSamples[i]\n                    || pContinuousSamples[i] != pContinuousSamples[i])\n                continue;\n\n            for (unsigned int b = 0; b < pSampleCountPerStratum[stratum]; ++b)\n            {\n                unsigned int const j = pSampleIndicesPerStratum[stratum][b];\n\n                if (pDiscreteSamples[j] != pDiscreteSamples[j] || pTimeSamples[j] != pTimeSamples[j]\n                        || pContinuousSamples[j] != pContinuousSamples[j])\n                    continue;\n\n                double pair_weight = pSampleWeights[i] * pSampleWeights[j];\n\n                if (pTimeSamples[i] < pTimeSamples[j] && pDiscreteSamples[i] == 1)\n                {\n                    relevant_weight += pair_weight;\n\n                    if (pContinuousSamples[i] > pContinuousSamples[j])\n                        concordant_weight += pair_weight;\n                    else if (pContinuousSamples[i] < pContinuousSamples[j])\n                        discordant_weight += pair_weight;\n                    else if (outX)\n                        uninformative_weight += pair_weight;\n                    else\n                        discordant_weight += pair_weight;\n                }\n                else if (pTimeSamples[i] > pTimeSamples[j] && pDiscreteSamples[j] == 1)\n                {\n                    relevant_weight += pair_weight;\n\n                    if (pContinuousSamples[i] < pContinuousSamples[j])\n                        concordant_weight += pair_weight;\n                    else if (pContinuousSamples[i] > pContinuousSamples[j])\n                        discordant_weight += pair_weight;\n                    else if (outX)\n                        uninformative_weight += pair_weight;\n                    else\n                        discordant_weight += pair_weight;\n                }\n            }\n\n            sum_concordant_weight += concordant_weight;\n            sum_relevant_weight += relevant_weight;\n\n            if (pConcordantWeights != 0) // Implicity, the other similar vectors\n            {                            // should also match this condition.\n                pConcordantWeights[i] = concordant_weight;\n                pDiscordantWeights[i] = discordant_weight;\n                pUninformativeWeights[i] = uninformative_weight;\n                pRelevantWeights[i] = relevant_weight;\n            }\n        }\n    }\n\n    return sum_concordant_weight / sum_relevant_weight;\n}\n\n/*static*/double const\nMath::computeConcordanceIndex(double const* const pDiscreteSamplesX,\n        double const* const pDiscreteSamplesY, double const* const pTimeSamplesX,\n        double const* const pTimeSamplesY, double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        bool const outX, double* const pConcordantWeights, double* const pDiscordantWeights,\n        double* const pUninformativeWeights, double* const pRelevantWeights)\n{\n    double sum_concordant_weight = 0.;\n    double sum_relevant_weight = 0.;\n\n    for (unsigned int stratum = 0; stratum < sampleStratumCount; ++stratum)\n    {\n        for (unsigned int a = 0; a < pSampleCountPerStratum[stratum]; ++a)\n        {\n            unsigned int const i = pSampleIndicesPerStratum[stratum][a];\n\n            double concordant_weight = 0.;\n            double discordant_weight = 0.;\n            double uninformative_weight = 0.;\n            double relevant_weight = 0.; \n\n            if (pDiscreteSamplesX[i] != pDiscreteSamplesX[i]\n                    || pDiscreteSamplesY[i] != pDiscreteSamplesY[i]\n                    || pTimeSamplesX[i] != pTimeSamplesX[i] || pTimeSamplesY[i] != pTimeSamplesY[i])\n                continue;\n\n            for (unsigned int b = 0; b < pSampleCountPerStratum[stratum]; ++b)\n            {\n                unsigned int const j = pSampleIndicesPerStratum[stratum][b];\n\n                if (pDiscreteSamplesX[j] != pDiscreteSamplesX[j]\n                        || pDiscreteSamplesY[j] != pDiscreteSamplesY[j]\n                        || pTimeSamplesX[j] != pTimeSamplesX[j]\n                        || pTimeSamplesY[j] != pTimeSamplesY[j])\n                    continue;\n\n                double pair_weight = pSampleWeights[i] * pSampleWeights[j];\n\n                if (pTimeSamplesX[i] < pTimeSamplesX[j] && pDiscreteSamplesX[i] == 1)\n                {\n                    relevant_weight += pair_weight;\n\n                    if (pTimeSamplesY[i] > pTimeSamplesY[j] && pDiscreteSamplesY[j] == 1)\n                        concordant_weight += pair_weight;\n                    else if (pTimeSamplesY[i] < pTimeSamplesY[j] && pDiscreteSamplesY[j] == 1)\n                        discordant_weight += pair_weight;\n                    else if (outX)\n                        uninformative_weight += pair_weight;\n                    else\n                        discordant_weight += pair_weight;\n                }\n                else if (pTimeSamplesX[i] > pTimeSamplesX[j] && pDiscreteSamplesX[j] == 1)\n                {\n                    relevant_weight += pair_weight;\n\n                    if (pTimeSamplesY[i] > pTimeSamplesY[j] && pDiscreteSamplesY[j] == 1)\n                        concordant_weight += pair_weight;\n                    else if (pTimeSamplesY[i] < pTimeSamplesY[j] && pDiscreteSamplesY[j] == 1)\n                        discordant_weight += pair_weight;\n                    else if (outX)\n                        uninformative_weight += pair_weight;\n                    else\n                        discordant_weight += pair_weight;\n                }\n            }\n\n           sum_concordant_weight += concordant_weight;\n            sum_relevant_weight += relevant_weight;\n\n            if (pConcordantWeights != 0) // Implicity, the other similar vectors\n            {                            // should also match this condition.\n                pConcordantWeights[i] = concordant_weight;\n                pDiscordantWeights[i] = discordant_weight;\n                pUninformativeWeights[i] = uninformative_weight;\n                pRelevantWeights[i] = relevant_weight;\n            }\n        }\n    }\n\n    return sum_concordant_weight / sum_relevant_weight;\n}\n\n/* static */double const\nMath::computeCramersV(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        unsigned int const bootstrapCount)\n{\n    bool const useBootstrap = bootstrapCount > 3 && sampleStratumCount > 0;\n    double* p_error_per_stratum = 0;\n\n    if (useBootstrap)\n    {\n        p_error_per_stratum = new double[sampleStratumCount];\n        unsigned int seed = std::time(NULL);\n        Matrix bootstraps(bootstrapCount, sampleStratumCount);\n\n#ifdef _OPENMP\n#pragma omp parallel for schedule(dynamic) firstprivate(seed)\n#endif\n        for (unsigned int i = 0; i < bootstrapCount; ++i)\n        {\n            for (unsigned int j = 0; j < sampleStratumCount; ++j)\n            {\n                unsigned int const sample_count = pSampleCountPerStratum[j];\n                unsigned int* const p_samples = new unsigned int[sample_count];\n\n                for (unsigned int k = 0; k < sample_count; ++k)\n                    p_samples[k] = pSampleIndicesPerStratum[j][Math::computeRandomNumber(&seed)\n                            % sample_count];\n\n                double const correlation = computeCramersV(pSamplesX, pSamplesY, pSampleWeights,\n                        p_samples, sample_count);\n                bootstraps.at(i, j) = correlation;\n\n                delete[] p_samples;\n            }\n        }\n\n        for (unsigned int i = 0; i < sampleStratumCount; ++i)\n            p_error_per_stratum[i] = 1.\n                    / Math::computeVariance(&(bootstraps.at(0, i)), bootstrapCount);\n    }\n\n    double r = 0.;\n    double total_weight = 0.;\n\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        double weight = 0.;\n        double const correlation = computeCramersV(pSamplesX, pSamplesY, pSampleWeights,\n                pSampleIndicesPerStratum[i], pSampleCountPerStratum[i], &weight);\n\n        if (useBootstrap)\n            weight = p_error_per_stratum[i];\n\n        r += weight * correlation;\n        total_weight += weight;\n    }\n\n    r /= total_weight;\n\n    delete[] p_error_per_stratum;\n\n    return r;\n}\n\n/* static */double const\nMath::computeCramersV(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    unsigned int x_class_count = 0;\n    unsigned int y_class_count = 0;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const index = pSampleIndices[i];\n\n        if (x_class_count <= pSamplesX[index])\n            x_class_count = pSamplesX[index] + 1;\n        if (y_class_count <= pSamplesY[index])\n            y_class_count = pSamplesY[index] + 1;\n    }\n\n    Matrix contingency_table(x_class_count + 1, y_class_count + 1);\n\n    for (unsigned int i = 0; i <= x_class_count; ++i)\n        for (unsigned int j = 0; j <= y_class_count; ++j)\n            contingency_table.at(i, j) = 0;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const index = pSampleIndices[i];\n\n        if (pSamplesX[index] != pSamplesX[index] || pSamplesY[index] != pSamplesY[index])\n            continue;\n\n        double const sample_weight = pSampleWeights[index];\n        contingency_table.at(pSamplesX[index], pSamplesY[index]) += sample_weight;\n        contingency_table.at(x_class_count, pSamplesY[index]) += sample_weight;\n        contingency_table.at(pSamplesX[index], y_class_count) += sample_weight;\n        contingency_table.at(x_class_count, y_class_count) += sample_weight;\n    }\n\n    double chi_square = 0.;\n\n    for (unsigned int i = 0; i < x_class_count; ++i)\n        for (unsigned int j = 0; j < y_class_count; ++j)\n        {\n            double expected_value = contingency_table.at(i, y_class_count)\n                    * contingency_table.at(x_class_count, j)\n                    / contingency_table.at(x_class_count, y_class_count);\n\n            chi_square += std::pow((contingency_table.at(i, j) - expected_value), 2)\n                    / expected_value;\n        }\n\n    unsigned int const min_classes =\n            (x_class_count < y_class_count) ? x_class_count : y_class_count;\n\n    *pTotalWeight = contingency_table.at(x_class_count, y_class_count);\n\n    double const v = std::sqrt(chi_square / ((*pTotalWeight) * (min_classes - 1)));\n\n    return v;\n}\n\n/* static */double const\nMath::computeFrequency(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        unsigned int const bootstrapCount)\n{\n    bool const useBootstrap = bootstrapCount > 3 && sampleStratumCount > 0;\n    double* p_error_per_stratum = 0;\n\n    if (useBootstrap)\n    {\n        p_error_per_stratum = new double[sampleStratumCount];\n        unsigned int seed = std::time(NULL);\n        Matrix bootstraps(bootstrapCount, sampleStratumCount);\n\n#ifdef _OPENMP\n#pragma omp parallel for schedule(dynamic) firstprivate(seed)\n#endif\n        for (unsigned int i = 0; i < bootstrapCount; ++i)\n        {\n            for (unsigned int j = 0; j < sampleStratumCount; ++j)\n            {\n                unsigned int const sample_count = pSampleCountPerStratum[j];\n                unsigned int* const p_samples = new unsigned int[sample_count];\n\n                for (unsigned int k = 0; k < sample_count; ++k)\n                    p_samples[k] = pSampleIndicesPerStratum[j][Math::computeRandomNumber(&seed)\n                            % sample_count];\n\n                double const correlation = computeFrequency(pSamplesX, pSamplesY, pSampleWeights,\n                        p_samples, sample_count);\n                bootstraps.at(i, j) = correlation;\n\n                delete[] p_samples;\n            }\n        }\n\n        for (unsigned int i = 0; i < sampleStratumCount; ++i)\n            p_error_per_stratum[i] = 1.\n                    / Math::computeVariance(&(bootstraps.at(0, i)), bootstrapCount);\n    }\n\n    double r = 0.;\n    double total_weight = 0.;\n\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        double weight = 0.;\n        double const correlation = computeFrequency(pSamplesX, pSamplesY, pSampleWeights,\n                pSampleIndicesPerStratum[i], pSampleCountPerStratum[i], &weight);\n\n        if (useBootstrap)\n            weight = p_error_per_stratum[i];\n\n        r += weight * correlation;\n        total_weight += weight;\n    }\n\n    r /= total_weight;\n\n    delete[] p_error_per_stratum;\n\n    return r;\n}\n\n/* static */double const\nMath::computeFrequency(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    double sum = 0.;\n    double total_weight = 0.;\n    double r = 0.;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const sample_index = pSampleIndices[i];\n        double const sample_weight = pSampleWeights[sample_index];\n\n        if (pSamplesX[sample_index] == pSamplesX[sample_index]\n                && pSamplesY[sample_index] == pSamplesY[sample_index])\n        {\n            total_weight += sample_weight;\n\n            if (pSamplesX[sample_index] > pSamplesY[sample_index])\n                sum += sample_weight;\n        }\n    }\n\n    if (pTotalWeight != 0)\n        *pTotalWeight = total_weight;\n    \n    r = sum / total_weight;\n    \n    return r;\n}\n\n/* static */double const\nMath::computeFisherTransformation(double const r)\n{\n    return 0.5 * std::log((1 + r) / (1 - r));\n}\n\n/* static */double const\nMath::computeFisherTransformationReverse(double const z)\n{\n    double const exp = std::exp(2 * z);\n    return (exp - 1) / (exp + 1);\n}\n\n/* static */double const\nMath::computeMi(double const r)\n{\n    return -0.5 * std::log(1 - (r * r));\n}\n\n/* static */double const\nMath::computePearsonCorrelation(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        unsigned int const bootstrapCount)\n{\n    bool const useBootstrap = bootstrapCount > 3 && sampleStratumCount > 0;\n    double* p_error_per_stratum = 0;\n\n    if (useBootstrap)\n    {\n        p_error_per_stratum = new double[sampleStratumCount];\n        unsigned int seed = std::time(NULL);\n        Matrix bootstraps(bootstrapCount, sampleStratumCount);\n\n#ifdef _OPENMP\n#pragma omp parallel for schedule(dynamic) firstprivate(seed)\n#endif\n        for (unsigned int i = 0; i < bootstrapCount; ++i)\n        {\n            for (unsigned int j = 0; j < sampleStratumCount; ++j)\n            {\n                unsigned int const sample_count = pSampleCountPerStratum[j];\n                unsigned int* const p_samples = new unsigned int[sample_count];\n\n                for (unsigned int k = 0; k < sample_count; ++k)\n                    p_samples[k] = pSampleIndicesPerStratum[j][Math::computeRandomNumber(&seed)\n                            % sample_count];\n\n                double const correlation = computeCramersV(pSamplesX, pSamplesY, pSampleWeights,\n                        p_samples, sample_count);\n                bootstraps.at(i, j) = correlation;\n\n                delete[] p_samples;\n            }\n        }\n\n        for (unsigned int i = 0; i < sampleStratumCount; ++i)\n            p_error_per_stratum[i] = 1.\n                    / Math::computeVariance(&(bootstraps.at(0, i)), bootstrapCount);\n    }\n\n    double r = 0.;\n    double total_weight = 0.;\n\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        double weight = 0.;\n        double const correlation = computePearsonCorrelation(pSamplesX, pSamplesY, pSampleWeights,\n                pSampleIndicesPerStratum[i], pSampleCountPerStratum[i], &weight);\n\n        if (useBootstrap)\n            weight = p_error_per_stratum[i];\n\n        r += weight * correlation;\n        total_weight += weight;\n    }\n\n    r /= total_weight;\n\n    delete[] p_error_per_stratum;\n\n    return r;\n}\n\n/* static */double const\nMath::computePearsonCorrelation(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    double sum_of_x = 0.;\n    double sum_of_x_x = 0.;\n    double sum_of_y = 0.;\n    double sum_of_y_y = 0.;\n    double sum_of_x_y = 0.;\n    double sum_of_weights = 0.;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        double const my_x = pSamplesX[pSampleIndices[i]];\n        double const my_y = pSamplesY[pSampleIndices[i]];\n\n        if (my_x == my_x && my_y == my_y)\n        {\n            double const my_weight = pSampleWeights[pSampleIndices[i]];\n            sum_of_x += my_x * my_weight;\n            sum_of_x_x += my_x * my_x * my_weight;\n            sum_of_y += my_y * my_weight;\n            sum_of_y_y += my_y * my_y * my_weight;\n            sum_of_x_y += my_x * my_y * my_weight;\n            sum_of_weights += my_weight;\n        }\n    }\n\n    double const r = (sum_of_x_y - ((sum_of_x * sum_of_y) / sum_of_weights))\n            / std::sqrt(\n                    (sum_of_x_x - ((sum_of_x * sum_of_x) / sum_of_weights))\n                            * (sum_of_y_y - ((sum_of_y * sum_of_y) / sum_of_weights)));\n\n    *pTotalWeight = sum_of_weights;\n\n    return r;\n}\n\n/* static */int const\nMath::computeRandomNumber(unsigned int* const seed)\n{\n    unsigned int next = *seed;\n    int result;\n\n    next *= 1103515245;\n    next += 12345;\n    result = static_cast<unsigned int>(next / 65536) % 2048;\n\n    next *= 1103515245;\n    next += 12345;\n    result <<= 10;\n    result ^= static_cast<unsigned int>(next / 65536) % 1024;\n\n    next *= 1103515245;\n    next += 12345;\n    result <<= 10;\n    result ^= static_cast<unsigned int>(next / 65536) % 1024;\n\n    *seed = next;\n    return result;\n}\n\n/* static */double const\nMath::computeSomersD(double const c)\n{\n    return (c - 0.5) * 2;\n}\n\n/* static */double const\nMath::computeSpearmanCorrelation(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount,\n        unsigned int const bootstrapCount, unsigned int const sampleCount)\n{\n    double* const p_ordered_samples_x = new double[sampleCount];\n    double* const p_ordered_samples_y = new double[sampleCount];\n\n    Math::placeOrders(&pSamplesX[0], p_ordered_samples_x, pSampleIndicesPerStratum,\n            pSampleCountPerStratum, sampleStratumCount);\n    Math::placeOrders(&pSamplesY[0], p_ordered_samples_y, pSampleIndicesPerStratum,\n            pSampleCountPerStratum, sampleStratumCount);\n\n    double* const p_ranked_samples_x = new double[sampleCount];\n    double* const p_ranked_samples_y = new double[sampleCount];\n\n    Math::placeRanksFromOrders(&pSamplesX[0], &pSamplesY[0], p_ordered_samples_x,\n            p_ordered_samples_y, p_ranked_samples_x, p_ranked_samples_y, pSampleIndicesPerStratum,\n            pSampleCountPerStratum, sampleStratumCount);\n\n    delete[] p_ordered_samples_x;\n    delete[] p_ordered_samples_y;\n\n    double const r = Math::computePearsonCorrelation(p_ranked_samples_x, p_ranked_samples_y,\n            &pSampleWeights[0], pSampleIndicesPerStratum, pSampleCountPerStratum,\n            sampleStratumCount, bootstrapCount);\n\n    delete[] p_ranked_samples_x;\n    delete[] p_ranked_samples_y;\n\n    return r;\n}\n\n/* static */double const\nMath::computeVariance(double const* const pSamples, unsigned int const sampleCount)\n{\n    if (sampleCount == 0)\n        return 0.;\n\n    double sum_for_mean = pSamples[0];\n    double sum_for_error = 0.;\n\n    for (unsigned int i = 1; i < sampleCount; ++i)\n    {\n        double const my_sum = pSamples[i] - sum_for_mean;\n        double const my_mean = ((i - 1) * my_sum) / i;\n        sum_for_mean += my_mean;\n        sum_for_error += my_mean * my_sum;\n    }\n\n    return sum_for_error / (sampleCount - 1);\n}\n\n/* static */void const\nMath::placeOrders(double const* const pSamples, double* const pOrders,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount)\n{\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        unsigned int const* const p_sample_indices = pSampleIndicesPerStratum[i];\n        unsigned int const sample_count = pSampleCountPerStratum[i];\n        unsigned int* const p_order = new unsigned int[sample_count];\n\n        unsigned int offset = 0;\n        for (unsigned int j = 0; j < sample_count; ++j)\n        {\n            if (pSamples[p_sample_indices[j]] == pSamples[p_sample_indices[j]])\n                p_order[j - offset] = j;\n            else\n                p_order[sample_count - 1 - offset++] = j;\n        }\n\n        std::sort(p_order, p_order + sample_count - offset,\n                Math::IndirectComparator(pSamples, p_sample_indices));\n\n        for (unsigned int j = 0; j < sample_count; ++j)\n            pOrders[p_sample_indices[j]] = p_order[j];\n\n        delete[] p_order;\n    }\n}\n\n/* static */void const\nMath::placeRanksFromOrders(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pOrdersX, double const* const pOrdersY, double* const pRanksX,\n        double* const pRanksY, unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount)\n{\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        unsigned int const* const p_sample_indices = pSampleIndicesPerStratum[i];\n        unsigned int const stratum_sample_count = pSampleCountPerStratum[i];\n\n        unsigned int offset_x = 0;\n        unsigned int offset_y = 0;\n\n        for (unsigned int j = 0; j < stratum_sample_count; ++j)\n        {\n            unsigned int const order_x =\n                    p_sample_indices[static_cast<unsigned int>(pOrdersX[p_sample_indices[j]])];\n            unsigned int const order_y =\n                    p_sample_indices[static_cast<unsigned int>(pOrdersY[p_sample_indices[j]])];\n\n            bool const NA_x = pSamplesX[order_x] != pSamplesX[order_x]\n                    || pSamplesY[order_x] != pSamplesY[order_x];\n            bool const NA_y = pSamplesY[order_y] != pSamplesY[order_y]\n                    || pSamplesX[order_y] != pSamplesX[order_y];\n\n            if (NA_x)\n                pRanksX[order_x] = std::numeric_limits<double>::quiet_NaN();\n            else\n                pRanksX[order_x] = offset_x++;\n\n            if (NA_y)\n                pRanksY[order_y] = std::numeric_limits<double>::quiet_NaN();\n            else\n                pRanksY[order_y] = offset_y++;\n        }\n    }\n}\n\n/* static */void const\nMath::placeRanksFromSamples(double const* const pSamples, double* const pRanks,\n        unsigned int const* const * const pSampleIndicesPerStratum,\n        unsigned int const* const pSampleCountPerStratum, unsigned int const sampleStratumCount)\n{\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        unsigned int const* const p_sample_indices = pSampleIndicesPerStratum[i];\n        unsigned int const sample_count = pSampleCountPerStratum[i];\n        unsigned int* const p_order = new unsigned int[sample_count];\n\n        unsigned int offset = 0;\n        for (unsigned int j = 0; j < sample_count; ++j)\n        {\n            unsigned int const my_index = p_sample_indices[j];\n\n            if (pSamples[my_index] == pSamples[my_index])\n                p_order[j - offset] = j;\n            else\n                ++offset;\n        }\n\n        std::sort(p_order, p_order + sample_count - offset,\n                Math::IndirectComparator(pSamples, p_sample_indices));\n\n        for (unsigned int j = 0; j < sample_count; ++j)\n            pRanks[j] = std::numeric_limits<double>::quiet_NaN();\n\n        for (unsigned int j = 0; j < sample_count - offset; ++j)\n            pRanks[p_sample_indices[p_order[j]]] = j;\n\n        delete[] p_order;\n    }\n}\n\n/* static */void const\nMath::placeStratificationData(int const* const pSampleStrata, double const* const pSampleWeights,\n        unsigned int** const pSampleIndicesPerStratum, unsigned int* const pSampleCountPerStratum,\n        unsigned int const sampleStratumCount, unsigned int const sampleCount)\n{\n    unsigned int* const p_iterator_per_stratum = new unsigned int[sampleStratumCount];\n\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n    {\n        p_iterator_per_stratum[i] = 0;\n        pSampleCountPerStratum[i] = 0;\n    }\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n        ++pSampleCountPerStratum[pSampleStrata[i]];\n\n    for (unsigned int i = 0; i < sampleStratumCount; ++i)\n        pSampleIndicesPerStratum[i] = new unsigned int[pSampleCountPerStratum[i]];\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const p_sample_stratum = pSampleStrata[i];\n        pSampleIndicesPerStratum[p_sample_stratum][p_iterator_per_stratum[p_sample_stratum]++] = i;\n    }\n\n    delete[] p_iterator_per_stratum;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `IndirectComparator` class in the `Math` namespace?",
        "answer": "The `IndirectComparator` class is used for indirect comparison of samples. It takes two pointers as constructor arguments: one to an array of samples and another to an array of sample indices. The class overloads the `operator()` to compare two indices based on their corresponding sample values. This allows for sorting or comparing elements indirectly, which is useful in various mathematical operations like ranking or ordering samples."
      },
      {
        "question": "Explain the functionality of the `computeConcordanceIndex` function that takes discrete and continuous samples as input.",
        "answer": "This `computeConcordanceIndex` function calculates the concordance index for paired samples. It iterates through sample pairs, comparing their discrete and continuous values. It accumulates weights for concordant, discordant, and uninformative pairs based on specific conditions. The function handles stratified data and can optionally compute weights for individual samples. The concordance index is calculated as the ratio of the sum of concordant weights to the sum of relevant weights, providing a measure of the agreement between the discrete and continuous samples."
      },
      {
        "question": "How does the `computeCramersV` function work, and what does it measure?",
        "answer": "The `computeCramersV` function calculates Cramer's V, a measure of association between two nominal variables. It first determines the number of classes for each variable, then constructs a contingency table. It computes the chi-square statistic from the observed and expected frequencies in the table. Finally, it calculates Cramer's V using the formula: V = sqrt(chi_square / (N * (min_classes - 1))), where N is the total sample weight. Cramer's V ranges from 0 to 1, with higher values indicating stronger association between the variables."
      }
    ],
    "completion_tasks": [
      {
        "partial": "/* static */double const\nMath::computeCramersV(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    unsigned int x_class_count = 0;\n    unsigned int y_class_count = 0;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const index = pSampleIndices[i];\n\n        if (x_class_count <= pSamplesX[index])\n            x_class_count = pSamplesX[index] + 1;\n        if (y_class_count <= pSamplesY[index])\n            y_class_count = pSamplesY[index] + 1;\n    }\n\n    Matrix contingency_table(x_class_count + 1, y_class_count + 1);\n\n    // TODO: Complete the function\n}",
        "complete": "/* static */double const\nMath::computeCramersV(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    unsigned int x_class_count = 0;\n    unsigned int y_class_count = 0;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const index = pSampleIndices[i];\n\n        if (x_class_count <= pSamplesX[index])\n            x_class_count = pSamplesX[index] + 1;\n        if (y_class_count <= pSamplesY[index])\n            y_class_count = pSamplesY[index] + 1;\n    }\n\n    Matrix contingency_table(x_class_count + 1, y_class_count + 1);\n\n    for (unsigned int i = 0; i <= x_class_count; ++i)\n        for (unsigned int j = 0; j <= y_class_count; ++j)\n            contingency_table.at(i, j) = 0;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        unsigned int const index = pSampleIndices[i];\n\n        if (pSamplesX[index] != pSamplesX[index] || pSamplesY[index] != pSamplesY[index])\n            continue;\n\n        double const sample_weight = pSampleWeights[index];\n        contingency_table.at(pSamplesX[index], pSamplesY[index]) += sample_weight;\n        contingency_table.at(x_class_count, pSamplesY[index]) += sample_weight;\n        contingency_table.at(pSamplesX[index], y_class_count) += sample_weight;\n        contingency_table.at(x_class_count, y_class_count) += sample_weight;\n    }\n\n    double chi_square = 0.;\n\n    for (unsigned int i = 0; i < x_class_count; ++i)\n        for (unsigned int j = 0; j < y_class_count; ++j)\n        {\n            double expected_value = contingency_table.at(i, y_class_count)\n                    * contingency_table.at(x_class_count, j)\n                    / contingency_table.at(x_class_count, y_class_count);\n\n            chi_square += std::pow((contingency_table.at(i, j) - expected_value), 2)\n                    / expected_value;\n        }\n\n    unsigned int const min_classes =\n            (x_class_count < y_class_count) ? x_class_count : y_class_count;\n\n    *pTotalWeight = contingency_table.at(x_class_count, y_class_count);\n\n    double const v = std::sqrt(chi_square / ((*pTotalWeight) * (min_classes - 1)));\n\n    return v;\n}"
      },
      {
        "partial": "/* static */double const\nMath::computePearsonCorrelation(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    double sum_of_x = 0.;\n    double sum_of_x_x = 0.;\n    double sum_of_y = 0.;\n    double sum_of_y_y = 0.;\n    double sum_of_x_y = 0.;\n    double sum_of_weights = 0.;\n\n    // TODO: Complete the function\n}",
        "complete": "/* static */double const\nMath::computePearsonCorrelation(double const* const pSamplesX, double const* const pSamplesY,\n        double const* const pSampleWeights, unsigned int const* const pSampleIndices,\n        unsigned int const sampleCount, double* const pTotalWeight)\n{\n    double sum_of_x = 0.;\n    double sum_of_x_x = 0.;\n    double sum_of_y = 0.;\n    double sum_of_y_y = 0.;\n    double sum_of_x_y = 0.;\n    double sum_of_weights = 0.;\n\n    for (unsigned int i = 0; i < sampleCount; ++i)\n    {\n        double const my_x = pSamplesX[pSampleIndices[i]];\n        double const my_y = pSamplesY[pSampleIndices[i]];\n\n        if (my_x == my_x && my_y == my_y)\n        {\n            double const my_weight = pSampleWeights[pSampleIndices[i]];\n            sum_of_x += my_x * my_weight;\n            sum_of_x_x += my_x * my_x * my_weight;\n            sum_of_y += my_y * my_weight;\n            sum_of_y_y += my_y * my_y * my_weight;\n            sum_of_x_y += my_x * my_y * my_weight;\n            sum_of_weights += my_weight;\n        }\n    }\n\n    double const r = (sum_of_x_y - ((sum_of_x * sum_of_y) / sum_of_weights))\n            / std::sqrt(\n                    (sum_of_x_x - ((sum_of_x * sum_of_x) / sum_of_weights))\n                            * (sum_of_y_y - ((sum_of_y * sum_of_y) / sum_of_weights)));\n\n    *pTotalWeight = sum_of_weights;\n\n    return r;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/optimizeCoreGx.R",
    "language": "R",
    "content": "#' @importFrom data.table setDTthreads as.data.table\n#' @importFrom bench press mark\n#' @importFrom parallel detectCores\n\n\n#' @title A helper method to find the best multithreading configuration for your\n#'   computer\n#'\n#' @param sample_data `TreatmentResponseExperiment`\n#' @param set `logical(1)` Should the function modify your R environment\n#'   with the predicted optimal settings? This changes the global state of your\n#'   R session!\n#' @param report `logical(1)` Should a `data.frame` of results be returned\n#'   by number of threads and operation be returned? Defaults to `!set`.\n#'\n#' @return\n#' If `set=TRUE`, modifies `data.table` threads via `setDTthreads()`, otherwise\n#' displays a message indicating the optimal number of threads.\n#' If `report=TRUE`, also returns a `data.frame` of the benchmark results.\n#'\n#' @examples\n#' \\donttest{\n#'   data(merckLongTable)\n#'   optimizeCoreGx(merckLongTable)\n#' }\n#'\n#' @md\n#' @export\noptimizeCoreGx <- function(sample_data, set=FALSE, report=!set) {\n    ncores <- max(1, parallel::detectCores() - 2)\n    nthread_range <- if (ncores == 1) 1 else c(1, seq(2, ncores, 2))\n    old_threads <- data.table::getDTthreads()\n    message(\"Benchmarking assay(sample_data, withDimnames=TRUE)...\")\n    assay_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ assay(sample_data, 1, withDimnames=TRUE); NA })\n    })\n    message(\"Benchmarking assays(sample_data)...\")\n    assays_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ assays(sample_data); NA })\n    })\n    message(\"Benchmarking reindex(sample_data)...\")\n    reindex_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ reindex(sample_data); NA })\n    })\n    report_table <- as.data.table(\n        rbind(assay_report, assays_report, reindex_report)\n    )[,\n        .(expression=as.character(expression), nthread,\n            min_sec=as.numeric(min), median_sec=as.numeric(median),\n            total_sec=as.numeric(total_time),\n            mem_alloc_mb=as.numeric(mem_alloc) / 1e6, `itr/sec`,  `gc/sec`)\n    ]\n    best_results <- report_table[,\n        .SD[which.min(median_sec), .(time=median_sec, nthread=nthread)],\n        by=expression\n    ]\n    optimal_cores <- best_results[,\n        as.integer(as.data.table(table(nthread))[which.max(N), nthread])\n    ]\n    message(\"Optimal cores for your machine are: \", optimal_cores)\n    if (set) {\n        message(\"Setting optimal cores\")\n        data.table::setDTthreads(optimal_cores)\n    } else {\n        data.table::setDTthreads(old_threads)\n    }\n    if (report) return(as.data.frame(report_table)) else return(invisible(NULL))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `optimizeCoreGx` function and what are its main parameters?",
        "answer": "The `optimizeCoreGx` function is designed to find the best multithreading configuration for a user's computer when working with `TreatmentResponseExperiment` data. It has three main parameters: `sample_data` (a `TreatmentResponseExperiment` object), `set` (a logical value indicating whether to modify the R environment with optimal settings), and `report` (a logical value determining if a data frame of benchmark results should be returned)."
      },
      {
        "question": "How does the function determine the optimal number of cores to use?",
        "answer": "The function determines the optimal number of cores by benchmarking three operations (`assay`, `assays`, and `reindex`) with different numbers of threads. It uses the `bench::press` function to run these benchmarks across a range of thread counts. The optimal core count is then determined by finding the thread count that yields the lowest median execution time across all three operations."
      },
      {
        "question": "What does the function return, and how does it handle the `set` and `report` parameters?",
        "answer": "The function's return value depends on the `set` and `report` parameters. If `set=TRUE`, it modifies the `data.table` threads using `setDTthreads()` and displays a message with the optimal core count. If `report=TRUE`, it returns a data frame containing the benchmark results. If both are FALSE, it only displays the optimal core count message and returns `invisible(NULL)`. The function also ensures that the original thread settings are restored if `set=FALSE`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "optimizeCoreGx <- function(sample_data, set=FALSE, report=!set) {\n    ncores <- max(1, parallel::detectCores() - 2)\n    nthread_range <- if (ncores == 1) 1 else c(1, seq(2, ncores, 2))\n    old_threads <- data.table::getDTthreads()\n    message(\"Benchmarking assay(sample_data, withDimnames=TRUE)...\")\n    assay_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ assay(sample_data, 1, withDimnames=TRUE); NA })\n    })\n    # Add code to benchmark assays and reindex operations\n    # Calculate report_table and best_results\n    # Determine optimal_cores\n    # Set or restore threads based on 'set' parameter\n    # Return report if requested\n}",
        "complete": "optimizeCoreGx <- function(sample_data, set=FALSE, report=!set) {\n    ncores <- max(1, parallel::detectCores() - 2)\n    nthread_range <- if (ncores == 1) 1 else c(1, seq(2, ncores, 2))\n    old_threads <- data.table::getDTthreads()\n    message(\"Benchmarking assay(sample_data, withDimnames=TRUE)...\")\n    assay_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ assay(sample_data, 1, withDimnames=TRUE); NA })\n    })\n    message(\"Benchmarking assays(sample_data)...\")\n    assays_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ assays(sample_data); NA })\n    })\n    message(\"Benchmarking reindex(sample_data)...\")\n    reindex_report <- bench::press(nthread=nthread_range, {\n        data.table::setDTthreads(nthread)\n        gc()\n        bench::mark({ reindex(sample_data); NA })\n    })\n    report_table <- as.data.table(rbind(assay_report, assays_report, reindex_report))[, .(expression=as.character(expression), nthread, min_sec=as.numeric(min), median_sec=as.numeric(median), total_sec=as.numeric(total_time), mem_alloc_mb=as.numeric(mem_alloc) / 1e6, `itr/sec`, `gc/sec`)]\n    best_results <- report_table[, .SD[which.min(median_sec), .(time=median_sec, nthread=nthread)], by=expression]\n    optimal_cores <- best_results[, as.integer(as.data.table(table(nthread))[which.max(N), nthread])]\n    message(\"Optimal cores for your machine are: \", optimal_cores)\n    if (set) {\n        message(\"Setting optimal cores\")\n        data.table::setDTthreads(optimal_cores)\n    } else {\n        data.table::setDTthreads(old_threads)\n    }\n    if (report) return(as.data.frame(report_table)) else return(invisible(NULL))\n}"
      },
      {
        "partial": "optimizeCoreGx <- function(sample_data, set=FALSE, report=!set) {\n    ncores <- max(1, parallel::detectCores() - 2)\n    nthread_range <- if (ncores == 1) 1 else c(1, seq(2, ncores, 2))\n    old_threads <- data.table::getDTthreads()\n    # Benchmark assay, assays, and reindex operations\n    # Calculate report_table\n    best_results <- report_table[,\n        .SD[which.min(median_sec), .(time=median_sec, nthread=nthread)],\n        by=expression\n    ]\n    optimal_cores <- best_results[,\n        as.integer(as.data.table(table(nthread))[which.max(N), nthread])\n    ]\n    message(\"Optimal cores for your machine are: \", optimal_cores)\n    # Set or restore threads based on 'set' parameter\n    # Return report if requested\n}",
        "complete": "optimizeCoreGx <- function(sample_data, set=FALSE, report=!set) {\n    ncores <- max(1, parallel::detectCores() - 2)\n    nthread_range <- if (ncores == 1) 1 else c(1, seq(2, ncores, 2))\n    old_threads <- data.table::getDTthreads()\n    benchmark_op <- function(op_name, op) {\n        message(paste(\"Benchmarking\", op_name, \"...\"))\n        bench::press(nthread=nthread_range, {\n            data.table::setDTthreads(nthread)\n            gc()\n            bench::mark({ op; NA })\n        })\n    }\n    reports <- list(\n        assay = benchmark_op(\"assay(sample_data, withDimnames=TRUE)\", assay(sample_data, 1, withDimnames=TRUE)),\n        assays = benchmark_op(\"assays(sample_data)\", assays(sample_data)),\n        reindex = benchmark_op(\"reindex(sample_data)\", reindex(sample_data))\n    )\n    report_table <- as.data.table(do.call(rbind, reports))[, .(expression=as.character(expression), nthread, min_sec=as.numeric(min), median_sec=as.numeric(median), total_sec=as.numeric(total_time), mem_alloc_mb=as.numeric(mem_alloc) / 1e6, `itr/sec`, `gc/sec`)]\n    best_results <- report_table[, .SD[which.min(median_sec), .(time=median_sec, nthread=nthread)], by=expression]\n    optimal_cores <- best_results[, as.integer(as.data.table(table(nthread))[which.max(N), nthread])]\n    message(\"Optimal cores for your machine are: \", optimal_cores)\n    if (set) {\n        message(\"Setting optimal cores\")\n        data.table::setDTthreads(optimal_cores)\n    } else {\n        data.table::setDTthreads(old_threads)\n    }\n    if (report) return(as.data.frame(report_table)) else return(invisible(NULL))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/R/mRMRe.Data.R",
    "language": "R",
    "content": "## Definition\n\nsetClass(\"mRMRe.Data\", representation(sample_names = \"character\", feature_names = \"character\", feature_types = \"numeric\", data = \"matrix\", strata = \"numeric\", weights = \"numeric\", priors = \"matrix\"))\n\n## Wrapper\n\n`mRMR.data` <- function(...)\n{\n    return(new(\"mRMRe.Data\", ...))\n}\n\n## initialize\n\nsetMethod(\"initialize\", signature(\"mRMRe.Data\"), function(.Object, data, strata, weights, priors)\n{\n    ## Data Processing\n    \n    if (!is.data.frame(data))\n        stop(\"data must be of type data frame\")\n        \n    if(ncol(data) > (sqrt((2^31) - 1)))\n        stop(\"Too many features, the number of features should be <= 46340!\")\n    \n    feature_types <- sapply(data, function(feature) paste(class(feature), collapse = \"_\"))\n    \n    if (any(!is.element(feature_types, c(\"numeric\", \"ordered_factor\", \"Surv\"))))\n        stop(\"data columns must be either of numeric, ordered factor or Surv type\")\n\n    .Object@sample_names <- rownames(data)\n    .Object@feature_names <- colnames(data)\n    .Object@feature_types <- unlist(lapply(feature_types, switch, \"Surv\" = c(2, 3), \"ordered_factor\" = 1, 0))\n    names(.Object@feature_types) <- NULL\n    # Optimize the case when all features are continuous\n    if(sum(.Object@feature_types) == 0)\n\t.Object@data <- as.matrix(data) \n    else\n        .Object@data <- do.call(cbind, lapply(seq(feature_types), function(i) switch(feature_types[[i]],\n                                \"Surv\" = cbind(event = data[, i][, \"status\"], time = data[, i][, \"time\"]),\n                                \"ordered_factor\" = as.numeric(as.integer(data[, i]) - 1),\n                                as.numeric(data[, i]))))\n    \n    \n    rownames(.Object@data) <- rownames(data)\n    colnames(.Object@data)[!.Object@feature_types %in% c(2, 3)] <- colnames(data)[feature_types != \"Surv\"]\n    colnames(.Object@data)[.Object@feature_types %in% c(2, 3)] <- paste(rep(colnames(data)[feature_types == \"Surv\"],\n                    each = 2), rep(c(\"event\", \"time\"), sum(feature_types == \"Surv\")), sep = \"_\")\n    \n    ## Sample Stratum Processing\n    \n    if (missing(strata)) \n        .Object@strata <- rep.int(0, nrow(data))\n    else\n        sampleStrata(.Object) <- strata\n    \n    ## Sample Weight Processing\n    \n    if (missing(weights)) \n        .Object@weights <- rep(1, nrow(data))\n    else\n        sampleWeights(.Object) <- weights\n\n    ## Prior Feature Matrix Processing\n    \n    if (!missing(priors) && !is.null(priors))\n        priors(.Object) <- priors\n    \n    return(.Object)\n})\n\n## show\n\nsetMethod(\"show\", signature(\"mRMRe.Data\"), function(object)\n{\n    str(object)\n})\n\n## featureData\n\nsetMethod(\"featureData\", signature(\"mRMRe.Data\"), function(object)\n{\n    data <- lapply(seq(object@feature_types), function(i) switch(as.character(object@feature_types[[i]]),\n                        \"3\" = Surv(time = object@data[, i], event = object@data[, i - 1]),\n                        \"2\" = NULL,\n                        \"1\" = object@data[, i] + 1,\n                        \"0\" = object@data[, i],\n                        NULL))\n    data <- data.frame(data[!sapply(data, is.null)])\n    colnames(data) <- object@feature_names\n    \n    return(data)\n})\n\n## subsetData\n\nsetMethod(\"subsetData\", signature(\"mRMRe.Data\"), function(object, row_indices, column_indices)\n{\n    if(missing(row_indices) && missing(column_indices))\n        return(object)\n        \n    if(missing(row_indices))\n        row_indices <- 1:sampleCount(object)\n    if(missing(column_indices))\n        column_indices <- 1:featureCount(object)\n    \n    data <- featureData(object)[row_indices, column_indices, drop=FALSE]\n    strata <- factor(sampleStrata(object)[row_indices])\n    weights <- sampleWeights(object)[row_indices]\n    priors <- priors(object)\n    if(length(priors) > 0)\n        priors <- priors[column_indices, column_indices, drop=FALSE]\n    else\n        priors <- NULL\n    \n    return(new(\"mRMRe.Data\", data = data, strata = strata, weights = weights, priors = priors))\n})\n\n## sampleCount\n\nsetMethod(\"sampleCount\", signature(\"mRMRe.Data\"), function(object)\n{\n    return(nrow(object@data))\n})\n\n## sampleNames\n\nsetMethod(\"sampleNames\", signature(\"mRMRe.Data\"), function(object)\n{\n    return(object@sample_names)\n})\n\n## featureCount\n\nsetMethod(\"featureCount\", signature(\"mRMRe.Data\"), function(object)\n{\n    return(length(object@feature_names))\n})\n\n## featureNames\n\nsetMethod(\"featureNames\", signature(\"mRMRe.Data\"), function(object)\n{\n    return(object@feature_names)\n})\n\n## sampleStrata\n\nsetMethod(\"sampleStrata\", signature(\"mRMRe.Data\"), function(object)\n{\n    strata <- object@strata\n    names(strata) <- rownames(object@data)\n    \n    return(strata)\n})\n\n## sampleStrata<-\n\nsetReplaceMethod(\"sampleStrata\", signature(\"mRMRe.Data\"), function(object, value)\n{\n    if (length(value) != nrow(object@data))\n        stop(\"data and strata must contain the same number of samples\")\n    else if (!is.factor(value))\n        stop(\"strata must be provided as factors\")\n    else if (sum(is.na(value)) > 0)\n        stop(\"cannot have missing values in strata\")\n    else\n        object@strata <- as.integer(value) - 1\n    \n    return(object)\n})\n\n## sampleWeights\n\nsetMethod(\"sampleWeights\", signature(\"mRMRe.Data\"), function(object)\n{\n    weights <- object@weights\n    names(weights) <- rownames(object@data)\n    \n    return(weights)\n})\n\n## sampleWeights<-\n\nsetReplaceMethod(\"sampleWeights\", signature(\"mRMRe.Data\"), function(object, value)\n{\n    if (length(value) != nrow(object@data))\n        stop(\"data and weight must contain the same number of samples\")\n    else if (sum(is.na(value)) > 0)\n        stop(\"cannot have missing values in weights\")\n    else\n        object@weights <- as.numeric(value)\n    \n    return(object)\n})\n\n## priors\n\nsetMethod(\"priors\", signature(\"mRMRe.Data\"), function(object)\n{\n    if (length(object@priors) == 0)\n        return(object@priors)\n    else\n        return(.compressFeatureMatrix(object, object@priors))\n})\n\n## priors<-\n\nsetReplaceMethod(\"priors\", signature(\"mRMRe.Data\"), function(object, value)\n{\n    if (ncol(value) != ncol(object@data) || nrow(value) != ncol(object@data))\n        stop(\"priors matrix must be a symmetric matrix containing as many features as data\")\n    else\n        object@priors <- .expandFeatureMatrix(object, value)\n    \n    return(object)\n})\n\n## mim\n\nsetMethod(\"mim\", signature(\"mRMRe.Data\"),\n        function(object, prior_weight = 0, continuous_estimator = c(\"pearson\", \"spearman\", \"kendall\", \"frequency\"), outX = TRUE, bootstrap_count = 0)\n{\n    continuous_estimator <- match.arg(continuous_estimator)\n    if (length(object@priors) != 0)\n    {\n        if (missing(prior_weight))\n            stop(\"prior weight must be provided if there are priors\")\n        else if  (prior_weight < 0 || prior_weight > 1)\n            stop(\"prior weight must be a value ranging from 0 to 1\")\n    }\n    else\n        prior_weight <- 0\n    \n    mi_matrix <- as.numeric(matrix(NA, ncol = ncol(object@data), nrow = ncol(object@data)))\n    \n    .Call(.C_export_mim, as.numeric(object@data), as.numeric(object@priors),\n            as.numeric(prior_weight), as.integer(object@strata), as.numeric(object@weights),\n            as.integer(object@feature_types), as.integer(nrow(object@data)), as.integer(ncol(object@data)),\n            as.integer(length(unique(object@strata))),\n            as.integer(.map.continuous.estimator(continuous_estimator)),\n            as.integer(outX), as.integer(bootstrap_count), mi_matrix)\n    \n    mi_matrix <- matrix(mi_matrix, ncol = ncol(object@data), nrow = ncol(object@data))\n    \n    mi_matrix <- .compressFeatureMatrix(object, mi_matrix)\n\n    # mi_matrix[i, j] contains the biased correlation between\n    # features i and j (i -> j directionality)\n    \n    return(mi_matrix)\n})\n\n## expandFeatureMatrix\n\nsetMethod(\".expandFeatureMatrix\", signature(\"mRMRe.Data\"), function(object, matrix)\n{\n    adaptor <- which(object@feature_types != 3)\n    matrix <- do.call(cbind, lapply(seq(adaptor), function(i)\n    {\n        column <- do.call(rbind, lapply(seq(adaptor), function(j)\n        {\n            item <- matrix[j, i]\n            \n            if (object@feature_types[[adaptor[[j]]]] == 2)\n                return(rbind(item, item, deparse.level = 0))\n            else\n                return(item)\n        }))\n        \n        if (object@feature_types[[adaptor[[i]]]] == 2)\n            return(cbind(column, column, deparse.level = 0))\n        else\n            return(column)\n    }))\n\n    return(matrix)\n})\n\n## compressFeatureMatrix\n\nsetMethod(\".compressFeatureMatrix\", signature(\"mRMRe.Data\"), function(object, matrix)\n{\n    adaptor <- which(object@feature_types != 3)\n    matrix <- matrix[adaptor, adaptor]\n    colnames(matrix) <- object@feature_names\n    rownames(matrix) <- object@feature_names\n    \n    return(matrix)\n})\n\n## expandFeatureIndices\n\nsetMethod(\".expandFeatureIndices\", signature(\"mRMRe.Data\"), function(object, indices)\n{\n    adaptor <- which(object@feature_types == 3)\n    if (length(adaptor) > 0 && any(indices >= adaptor))\n        indices <- sapply(indices, function(i) i + sum(sapply(1:length(adaptor), function(j) i >= (adaptor[[j]] - j + 1))))\n\n    return(as.integer(indices))\n})\n\n## compressFeatureIndices\n\nsetMethod(\".compressFeatureIndices\", signature(\"mRMRe.Data\"), function(object, indices)\n{\n    adaptor <- which(object@feature_types == 3)\n    \n    if (length(adaptor) > 0)\n        indices <- sapply(indices, function(i) i - sum(i >= adaptor))\n    \n    return(as.integer(indices))\n})\n\n\nsetMethod(\"scores\", signature(\"mRMRe.Data\"), function(object, solutions)\n{\n    mi_matrix <- mim(object)\n    targets <- names(solutions)\n    scores <- lapply(targets, function(target) {\n      apply(solutions[[target]], 2, function(solution) {\n\t \t \t \t sapply(1:length(solution), function(i) {\n\t \t \t \t \t \t \tfeature_i <- solution[i] \n\t\t\tif(i == 1)\n\t\t\t\treturn(mi_matrix[as.numeric(target), feature_i])\n\n\t\t\t ancestry_score <- mean(sapply((i-1):1, function(j) mi_matrix[feature_i, solution[j]]))\n\t \t \t \t \t \t \t return(mi_matrix[as.numeric(target), feature_i] - ancestry_score)\n\t \t \t \t \t })\n\t \t \t \t \n\t \t \t })\n\t })\n\t names(scores) <- targets\n\t return(scores)\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mRMRe.Data` class and its initialization method?",
        "answer": "The `mRMRe.Data` class is designed to store and manage data for feature selection. Its initialization method processes input data, validates it, and sets up various attributes such as sample names, feature names, feature types, and data matrix. It also handles optional inputs like strata, weights, and priors."
      },
      {
        "question": "How does the `mim` method handle different types of features and what is its primary output?",
        "answer": "The `mim` method calculates mutual information between features. It handles different feature types (numeric, ordered factor, and survival) by using the `feature_types` attribute. The method also considers sample strata and weights. Its primary output is a mutual information matrix, where each element [i, j] represents the biased correlation between features i and j."
      },
      {
        "question": "What is the purpose of the `expandFeatureMatrix` and `compressFeatureMatrix` methods in the `mRMRe.Data` class?",
        "answer": "These methods handle the conversion between the internal representation of the feature matrix and its user-facing form. `expandFeatureMatrix` expands the matrix to include separate columns for survival data (event and time), while `compressFeatureMatrix` does the opposite, combining survival data columns into a single feature. This allows for efficient internal processing while providing a more intuitive interface for the user."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"scores\", signature(\"mRMRe.Data\"), function(object, solutions)\n{\n    mi_matrix <- mim(object)\n    targets <- names(solutions)\n    scores <- lapply(targets, function(target) {\n      apply(solutions[[target]], 2, function(solution) {\n         sapply(1:length(solution), function(i) {\n              feature_i <- solution[i] \n            if(i == 1)\n                return(mi_matrix[as.numeric(target), feature_i])\n\n            # Complete the code here\n         })\n      })\n    })\n    names(scores) <- targets\n    return(scores)\n})",
        "complete": "setMethod(\"scores\", signature(\"mRMRe.Data\"), function(object, solutions)\n{\n    mi_matrix <- mim(object)\n    targets <- names(solutions)\n    scores <- lapply(targets, function(target) {\n      apply(solutions[[target]], 2, function(solution) {\n         sapply(1:length(solution), function(i) {\n              feature_i <- solution[i] \n            if(i == 1)\n                return(mi_matrix[as.numeric(target), feature_i])\n\n            ancestry_score <- mean(sapply((i-1):1, function(j) mi_matrix[feature_i, solution[j]]))\n            return(mi_matrix[as.numeric(target), feature_i] - ancestry_score)\n         })\n      })\n    })\n    names(scores) <- targets\n    return(scores)\n})"
      },
      {
        "partial": "setMethod(\".expandFeatureMatrix\", signature(\"mRMRe.Data\"), function(object, matrix)\n{\n    adaptor <- which(object@feature_types != 3)\n    matrix <- do.call(cbind, lapply(seq(adaptor), function(i)\n    {\n        column <- do.call(rbind, lapply(seq(adaptor), function(j)\n        {\n            item <- matrix[j, i]\n            \n            # Complete the code here\n        }))\n        \n        # Complete the code here\n    }))\n\n    return(matrix)\n})",
        "complete": "setMethod(\".expandFeatureMatrix\", signature(\"mRMRe.Data\"), function(object, matrix)\n{\n    adaptor <- which(object@feature_types != 3)\n    matrix <- do.call(cbind, lapply(seq(adaptor), function(i)\n    {\n        column <- do.call(rbind, lapply(seq(adaptor), function(j)\n        {\n            item <- matrix[j, i]\n            \n            if (object@feature_types[[adaptor[[j]]]] == 2)\n                return(rbind(item, item, deparse.level = 0))\n            else\n                return(item)\n        }))\n        \n        if (object@feature_types[[adaptor[[i]]]] == 2)\n            return(cbind(column, column, deparse.level = 0))\n        else\n            return(column)\n    }))\n\n    return(matrix)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/adaptiveMatthewCor.R",
    "language": "R",
    "content": "## adaptive Matthews correlation coefficient for binary classification\n#' Calculate an Adaptive Matthews Correlation Coefficient\n#' \n#' This function calculates an Adaptive Matthews Correlation Coefficient (AMCC) \n#' for two vectors of values of the same length. It assumes the entries in the \n#' two vectors are paired. The Adaptive Matthews Correlation Coefficient for two\n#' vectors of values is defined as the Maximum Matthews Coefficient over all \n#' possible binary splits of the ranks of the two vectors. In this way, it \n#' calculates the best possible agreement of a binary classifier on the two \n#' vectors of data. If the AMCC is low, then it is impossible to find any binary \n#' classification of the two vectors with a high degree of concordance.   \n#' \n#' @examples \n#' x <- c(1,2,3,4,5,6,7)\n#' y <- c(1,3,5,4,2,7,6)\n#' amcc(x,y, min.cat=2)\n#' \n#' @param x,y Two paired vectors of values. Could be replicates of observations \n#'   for the same experiments for example.  \n#' @param step.prct Instead of testing all possible splits of the data, it is \n#'   possible to test steps of a percentage size of the total number of ranks in \n#'   x/y. If this variable is 0, function defaults to testing all possible \n#'   splits.\n#' @param min.cat The minimum number of members per category. Classifications \n#'   with less members fitting into both categories will not be considered. \n#' @param nperm The number of perumatation to use for estimating significance. \n#'   If 0, then no p-value is calculated. \n#' @param nthread Number of threads to parallize over. Both the AMCC calculation \n#'   and the permutation testing is done in parallel. \n#' @param ... Additional arguments\n#' \n#' @return Returns a list with two elements. $amcc contains the highest 'mcc' \n#'   value over all the splits, the p value, as well as the rank at which the \n#'   split was done.\n#'\n#' @importFrom BiocParallel bplapply\n#' @importFrom stats quantile\n#'\n#' @export\n## FIXME:: We need a more descriptive name for this function\namcc <- function(x, y, step.prct = 0, min.cat = 3, nperm = 1000, nthread = 1, ...) {\n    # PARAMETER CHANGE WARNING\n    if (!missing(...)) {\n        if (\"setseed\" %in% names(...)) {\n            warning(\"The setseed parameter has been removed in this release to conform\n              to Bioconductor coding standards. Please call set.seed in your\n              script before running this function.\")\n        }\n    }\n    \n    if (!min.cat > 1) {\n        \n        stop(\"Min.cat should be at least 2\")\n        \n    }\n    ccix <- complete.cases(x, y)\n    if (sum(ccix) >= (2 * min.cat)) {\n        x2 <- rank(-x[ccix], ties.method = \"first\")\n        y2 <- rank(-y[ccix], ties.method = \"first\")\n        ## compute mcc for each rank\n        iix <- seq_len(min(max(x2), max(y2)) - 1)\n        if (step.prct > 0) {\n            iix <- round(quantile(iix, probs = seq(0, 1, by = step.prct)))\n        }\n        splitix <- parallel::splitIndices(nx = length(iix), ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- bplapply(splitix, function(x, iix, x2, y2) {\n            res <- t(vapply(iix[x], function(x, x2, y2) {\n                x3 <- factor(ifelse(x2 <= x, \"1\", \"0\"))\n                y3 <- factor(ifelse(y2 <= x, \"1\", \"0\"))\n                res <- mcc(x = x3, y = y3, nperm = 0, nthread = 1)\n                return(res)\n            }, x2 = x2, y2 = y2, FUN.VALUE = list(1, 1)))\n            ## TODO:: Why is return value a list of doubles when the class of res is matrix in debug ?\n            return(res)\n        }, iix = iix, x2 = x2, y2 = y2)\n        mm <- do.call(rbind, mcres)\n        mode(mm) <- \"numeric\"\n        ## remove extreme indices\n        rmix <- c(seq_len(min.cat - 1), (nrow(mm) - min.cat + 2):nrow(mm))\n        mccix <- max(which(mm[-rmix, \"estimate\", drop = FALSE] == max(mm[-rmix, \"estimate\", drop = FALSE], na.rm = TRUE))) + (min.cat - 1)\n        ## compute significance only for the AMCC\n        x3 <- factor(ifelse(x2 <= mccix, \"1\", \"0\"))\n        y3 <- factor(ifelse(y2 <= mccix, \"1\", \"0\"))\n        if (nperm > 0) {\n            mm[mccix, \"p.value\"] <- mcc(x = x3, y = y3, nperm = nperm, nthread = nthread)[[\"p.value\"]]\n            ## bonferronni correction mm[mccix, 'p'] <- mm[mccix, 'p'] * length(x3)\n        }\n        if (!is.na(mm[mccix, \"p.value\"]) && mm[mccix, \"p.value\"] > 1) {\n            mm[mccix, \"p.value\"] <- 1\n        }\n        res <- c(mcc = mm[mccix, ], n1 = mccix, n2 = nrow(mm) - mccix, n = nrow(mm))\n    } else {\n        res <- c(mcc = NA, p = NA, n1 = 0, n2 = 0, n = sum(ccix))\n        mm <- NA\n    }\n    names(res) <- c(\"mcc\", \"p\", \"n1\", \"n2\", \"n\")\n    return(list(amcc = res, mcc = mm))\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `amcc` function and what does AMCC stand for?",
        "answer": "The `amcc` function calculates an Adaptive Matthews Correlation Coefficient (AMCC) for two vectors of values. AMCC is defined as the Maximum Matthews Coefficient over all possible binary splits of the ranks of the two input vectors. It aims to find the best possible agreement of a binary classifier on the two vectors of data, which is useful for assessing the potential for binary classification between two sets of observations."
      },
      {
        "question": "How does the function handle parallel processing, and what parameters control this behavior?",
        "answer": "The function uses parallel processing for both the AMCC calculation and permutation testing. It utilizes the `bplapply` function from the BiocParallel package to parallelize computations. The `nthread` parameter controls the number of threads to use for parallelization. The function splits the work into chunks using `parallel::splitIndices` and processes these chunks in parallel."
      },
      {
        "question": "What is the significance of the `min.cat` parameter, and how does it affect the AMCC calculation?",
        "answer": "The `min.cat` parameter specifies the minimum number of members per category in the binary classification. It ensures that classifications with fewer members in both categories are not considered. This parameter affects the AMCC calculation by removing extreme indices from consideration. Specifically, it removes `min.cat - 1` indices from both the beginning and end of the sorted results before finding the maximum MCC value. This helps to avoid overfitting to very small subsets of the data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "amcc <- function(x, y, step.prct = 0, min.cat = 3, nperm = 1000, nthread = 1, ...) {\n    if (!min.cat > 1) {\n        stop(\"Min.cat should be at least 2\")\n    }\n    ccix <- complete.cases(x, y)\n    if (sum(ccix) >= (2 * min.cat)) {\n        x2 <- rank(-x[ccix], ties.method = \"first\")\n        y2 <- rank(-y[ccix], ties.method = \"first\")\n        iix <- seq_len(min(max(x2), max(y2)) - 1)\n        if (step.prct > 0) {\n            iix <- round(quantile(iix, probs = seq(0, 1, by = step.prct)))\n        }\n        splitix <- parallel::splitIndices(nx = length(iix), ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        # Complete the function from here\n    }\n}",
        "complete": "amcc <- function(x, y, step.prct = 0, min.cat = 3, nperm = 1000, nthread = 1, ...) {\n    if (!min.cat > 1) {\n        stop(\"Min.cat should be at least 2\")\n    }\n    ccix <- complete.cases(x, y)\n    if (sum(ccix) >= (2 * min.cat)) {\n        x2 <- rank(-x[ccix], ties.method = \"first\")\n        y2 <- rank(-y[ccix], ties.method = \"first\")\n        iix <- seq_len(min(max(x2), max(y2)) - 1)\n        if (step.prct > 0) {\n            iix <- round(quantile(iix, probs = seq(0, 1, by = step.prct)))\n        }\n        splitix <- parallel::splitIndices(nx = length(iix), ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- bplapply(splitix, function(x, iix, x2, y2) {\n            res <- t(vapply(iix[x], function(x, x2, y2) {\n                x3 <- factor(ifelse(x2 <= x, \"1\", \"0\"))\n                y3 <- factor(ifelse(y2 <= x, \"1\", \"0\"))\n                mcc(x = x3, y = y3, nperm = 0, nthread = 1)\n            }, x2 = x2, y2 = y2, FUN.VALUE = list(1, 1)))\n            return(res)\n        }, iix = iix, x2 = x2, y2 = y2)\n        mm <- do.call(rbind, mcres)\n        mode(mm) <- \"numeric\"\n        rmix <- c(seq_len(min.cat - 1), (nrow(mm) - min.cat + 2):nrow(mm))\n        mccix <- max(which(mm[-rmix, \"estimate\", drop = FALSE] == max(mm[-rmix, \"estimate\", drop = FALSE], na.rm = TRUE))) + (min.cat - 1)\n        x3 <- factor(ifelse(x2 <= mccix, \"1\", \"0\"))\n        y3 <- factor(ifelse(y2 <= mccix, \"1\", \"0\"))\n        if (nperm > 0) {\n            mm[mccix, \"p.value\"] <- mcc(x = x3, y = y3, nperm = nperm, nthread = nthread)[[\"p.value\"]]\n        }\n        if (!is.na(mm[mccix, \"p.value\"]) && mm[mccix, \"p.value\"] > 1) {\n            mm[mccix, \"p.value\"] <- 1\n        }\n        res <- c(mcc = mm[mccix, ], n1 = mccix, n2 = nrow(mm) - mccix, n = nrow(mm))\n    } else {\n        res <- c(mcc = NA, p = NA, n1 = 0, n2 = 0, n = sum(ccix))\n        mm <- NA\n    }\n    names(res) <- c(\"mcc\", \"p\", \"n1\", \"n2\", \"n\")\n    return(list(amcc = res, mcc = mm))\n}"
      },
      {
        "partial": "amcc <- function(x, y, step.prct = 0, min.cat = 3, nperm = 1000, nthread = 1, ...) {\n    if (!min.cat > 1) {\n        stop(\"Min.cat should be at least 2\")\n    }\n    ccix <- complete.cases(x, y)\n    if (sum(ccix) >= (2 * min.cat)) {\n        x2 <- rank(-x[ccix], ties.method = \"first\")\n        y2 <- rank(-y[ccix], ties.method = \"first\")\n        iix <- seq_len(min(max(x2), max(y2)) - 1)\n        if (step.prct > 0) {\n            iix <- round(quantile(iix, probs = seq(0, 1, by = step.prct)))\n        }\n        splitix <- parallel::splitIndices(nx = length(iix), ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- bplapply(splitix, function(x, iix, x2, y2) {\n            # Complete the function from here\n        }, iix = iix, x2 = x2, y2 = y2)\n    }\n}",
        "complete": "amcc <- function(x, y, step.prct = 0, min.cat = 3, nperm = 1000, nthread = 1, ...) {\n    if (!min.cat > 1) {\n        stop(\"Min.cat should be at least 2\")\n    }\n    ccix <- complete.cases(x, y)\n    if (sum(ccix) >= (2 * min.cat)) {\n        x2 <- rank(-x[ccix], ties.method = \"first\")\n        y2 <- rank(-y[ccix], ties.method = \"first\")\n        iix <- seq_len(min(max(x2), max(y2)) - 1)\n        if (step.prct > 0) {\n            iix <- round(quantile(iix, probs = seq(0, 1, by = step.prct)))\n        }\n        splitix <- parallel::splitIndices(nx = length(iix), ncl = nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE = numeric(1)) > 0]\n        mcres <- bplapply(splitix, function(x, iix, x2, y2) {\n            res <- t(vapply(iix[x], function(x, x2, y2) {\n                x3 <- factor(ifelse(x2 <= x, \"1\", \"0\"))\n                y3 <- factor(ifelse(y2 <= x, \"1\", \"0\"))\n                mcc(x = x3, y = y3, nperm = 0, nthread = 1)\n            }, x2 = x2, y2 = y2, FUN.VALUE = list(1, 1)))\n            return(res)\n        }, iix = iix, x2 = x2, y2 = y2)\n        mm <- do.call(rbind, mcres)\n        mode(mm) <- \"numeric\"\n        rmix <- c(seq_len(min.cat - 1), (nrow(mm) - min.cat + 2):nrow(mm))\n        mccix <- max(which(mm[-rmix, \"estimate\", drop = FALSE] == max(mm[-rmix, \"estimate\", drop = FALSE], na.rm = TRUE))) + (min.cat - 1)\n        x3 <- factor(ifelse(x2 <= mccix, \"1\", \"0\"))\n        y3 <- factor(ifelse(y2 <= mccix, \"1\", \"0\"))\n        if (nperm > 0) {\n            mm[mccix, \"p.value\"] <- mcc(x = x3, y = y3, nperm = nperm, nthread = nthread)[[\"p.value\"]]\n        }\n        if (!is.na(mm[mccix, \"p.value\"]) && mm[mccix, \"p.value\"] > 1) {\n            mm[mccix, \"p.value\"] <- 1\n        }\n        res <- c(mcc = mm[mccix, ], n1 = mccix, n2 = nrow(mm) - mccix, n = nrow(mm))\n    } else {\n        res <- c(mcc = NA, p = NA, n1 = 0, n2 = 0, n = sum(ccix))\n        mm <- NA\n    }\n    names(res) <- c(\"mcc\", \"p\", \"n1\", \"n2\", \"n\")\n    return(list(amcc = res, mcc = mm))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/LongTable-accessors.R",
    "language": "R",
    "content": "# Navigating this file:\n# - Slot section names start with ----\n# - Method section names start with ==\n#\n# As a result, you can use Ctrl + f to find the slot or method you are looking\n# for quickly, assuming you know its name.\n#\n# For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n# method, while Ctrl + f '---- molecularProfiles' would take you to the slot\n# section.\n\n#' @include LongTable-class.R allGenerics.R\nNULL\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n.local_class_4 <- 'LongTable'\n.local_data_4 <- 'merckLongTable'\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n\n#' @noRd\n.docs_LongTable_accessors <- function(...) .parseToRoxygen(\n    \"\n    @title Accessing and modifying information in a `{class_}`\n\n    @description\n    Documentation for the various setters and getters which allow manipulation\n    of data in the slots of a `{class_}` object.\n\n    @return Accessors: See details.\n    @return Setters: An updated `{class_}` object, returned invisibly.\n    \",\n    ...\n)\n\n#' @name LongTable-accessors\n#' @eval .docs_LongTable_accessors(class_=.local_class_4)\n#' @eval .parseToRoxygen(\"@examples data({data_})\", data_=.local_data_4)\nNULL\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ==================\n## ---- .intern Slot\n## ------------------\n\n\n##\n## == getIntern\n\n\n#' Get the symbol(s) x from the object@.intern slot of a LongTable\n#'\n#' This is used as an alternative to R attributes for storing structural\n#' metadata of an S4 objects.\n#'\n#' @examples\n#' getIntern(merckLongTable, 'rowIDs')\n#' getIntern(merckLongTable, c('colIDs', 'colMeta'))\n#'\n#' @describeIn LongTable Access structural metadata present within a\n#'   LongTable object. This is mostly for developmer use.\n#'\n#' @param object `LongTable`\n#' @param x `character` One or more symbol name strings to retrieve from\n#'     the object@.intern environment.\n#'\n#' @return `immutable` value of x if length(x) == 1 else named list of values\n#'     for all symbols in x.\n#'\n#' @include LongTable-class.R\n#' @export\nsetMethod('getIntern', signature(object='LongTable', x='character'),\n        function(object, x) {\n    return(if (length(x) == 1) object@.intern[[x]] else object@.intern[x])\n})\n#' @describeIn LongTable Access all structural metadata present within a\n#'   LongTable object. This is primarily for developmer use.\n#'\n#' @param object `LongTable`\n#' @param x `missing` This argument is excluded from from the function call.\n#'\n#' @return An `immutable` list.\n#'\n#' @examples\n#' getIntern(merckLongTable)\n#'\n#' @aliases getIntern,LongTable,missing-method\n#' @export\nsetMethod('getIntern', signature(object='LongTable', x='missing'),\n    function(object, x) object@.intern\n)\n\n#' Set the .intern slot of a LongTable\n#'\n#' @param object `LongTable`\n#' @param value An `immutable_list` object, being a class union between `list`\n#'   and `immutable` S3 classes.\n#'\n#' @return Updates the object and returns invisibly.\n#'\n#' @keywords internal\nsetReplaceMethod(\"getIntern\", signature(object=\"LongTable\",\n    value=\"immutable_list\"), function(object, value) {\n        object@.intern <- value\n        return(object)\n})\n\n## ==================\n## ---- rowData Slot\n## ------------------\n\n#' Retrieve the row metadata table from a LongTable object\n#'\n#' @examples\n#' rowData(merckLongTable)\n#'\n#' @describeIn LongTable Get the row level annotations for a `LongTable` object.\n#'\n#' @param x A `LongTable` object to retrieve the row metadata from.\n#' @param key `logical` Should the rowKey column also be returned? Defaults\n#'     to FALSE.\n#' @param use.names `logical` This parameter is just here to stop matching\n#'     the positional argument to use.names from the rowData generic. It\n#'     doesn't do anything at this time and can be ignored.\n#' @param ... For developer use only! Pass raw=TRUE to modify the slot\n#'   directly. This will corrupt your data if you don't know what you are\n#'   doing!\n#'\n#' @return A `data.table` containing rowID, row identifiers, and row metadata.\n#'\n#' @importFrom data.table data.table copy\n#' @export\nsetMethod('rowData', signature(x='LongTable'),\n        function(x, key=FALSE, use.names=FALSE, ...) {\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        return(x@rowData)\n    } else {\n        return(if (key) copy(x@rowData[, -'.rownames']) else\n            copy(x@rowData[, -c('.rownames', 'rowKey')]))\n    }\n})\n\n#' Helper method to share functionality between rowData and colData replace methods\n#'\n#' @param x `LongTable` or inheriting class to update dimData for.\n#' @param dim `character(1)` One of \"row\" or \"col\" indicating with dimension\n#'   to updated metadata for.\n#' @param value #' @param value A `data.table` or `data.frame` to update the\n#'   `rowData` or `colData` of `x` with.\n#'\n#' @return An updated version of `value` which meets all the requirements for\n#'   assignment to a `LongTable` or inheriting class.\n#'\n#' @noRd\n#' @keywords internal\n.update_dimData <- function(x, dim, value) {\n\n    titleDim <- paste0(toupper(substr(dim, 1, 1)), substr(dim, 2, nchar(dim)))\n    dimIDs <- get(paste0(dim, \"IDs\"))\n    dimKey <- paste(dim, \"Key\")\n    dimData <- paste0(dim, \"Data\")\n\n    # type check input\n    if (is(value, 'data.frame')) setDT(value)\n    if (!is(value, 'data.table'))\n        stop(.errorMsg('\\n[CoreGx::', dim, 'Data<-] Please pass a data.frame or ',\n            'data.table to update the ', dim, 'Data slot. We recommend modifying the',\n            ' object returned by ', dim, 'Data(x) then reassigning it with ',\n            dim, 'Data(x)',\n            ' <- new', titleDim, 'Data'),\n            call.=FALSE\n        )\n\n    # remove key column\n    if (dimKey %in% colnames(value)) {\n        value[, (dimKey) := NULL]\n        .message('\\n[CoreGx::', dim, ,'Data<-] Dropping ', dim, 'Key from replacement',\n            ' value, this function will deal with mapping the ', dim, 'Key',\n            ' automatically.')\n    }\n\n    # assemble information to select proper update method\n    dimIDCols <- dimIDs(x)\n    sharedDimIDCols <- intersect(dimIDCols, colnames(value))\n\n    # error if all the row/colID columns are not present in the new row/colData\n    equalDimIDs <- dimIDCols %in% sharedDimIDCols\n    if (!all(equalDimIDs)) warning(.warnMsg('\\n[CoreGx::', dim,\n        'Data<-] The ID columns ', dimIDCols[!equalDimIDs],\n        ' are not present in value. The function ',\n        'will attempt to join with existing ', dim, 'IDs, but this may fail!',\n        collapse=', '), call.=FALSE)\n\n    dimIDs_ <- dimIDs(x, data=TRUE, key=TRUE)\n\n    ## TODO:: Throw error if user tries to modify ID columns\n\n    duplicatedIDcols <- value[, .N, by=c(sharedDimIDCols)][, N > 1]\n    if (any(duplicatedIDcols))\n        warning(.warnMsg(\"\\n[CoreGx::\", dim, \"Data<-,\", class(x)[1], \"-method] The \",\n            \"ID columns are duplicated for rows \",\n            .collapse(which(duplicatedIDcols)),\n            \"! These rows will be dropped before assignment.\"),\n        call.=FALSE)\n\n    dimData <- dimIDs_[unique(value), on=.NATURAL, allow.cartesian=FALSE]\n    dimData[, (dimKey) := .I]\n    dimData <- dimData[!duplicated(get(dimKey)), ]\n    setkeyv(dimData, dimKey)\n    dimData[, .rownames := Reduce(.paste_colon, mget(dimIDCols))]\n\n    ## TODO:: Add some sanity checks before returing\n\n    return(dimData)\n}\n\n\n#' Updates the `rowData` slot as long as the ID columns are not changed.\n#'\n#' @examples\n#' rowData(merckLongTable) <- rowData(merckLongTable)\n#'\n#' @describeIn LongTable Update the row annotations for a `LongTable` object.\n#'   Currently requires that all columns in rowIDs(longTable) be present in\n#'   value.\n#'\n#' @param x A `LongTable` object to modify.\n#' @param value A `data.table` or `data.frame` to update the `rowData` of\n#'   `x` with.\n#' @param ... For developer use only! Pass raw=TRUE to modify the slot\n#'   directly. This will corrupt your data if you don't know what you are\n#'   doing!\n#'\n#' @return A copy of the `LongTable` object with the `rowData`\n#'   slot updated.\n#'\n#' @md\n#' @importFrom crayon cyan magenta\n#' @importFrom SummarizedExperiment `rowData<-`\n#' @importFrom data.table setDT\n#' @export\nsetReplaceMethod('rowData', signature(x='LongTable'), function(x, ..., value) {\n\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        x@rowData <- value\n        return(invisible(x))\n    }\n\n    x@rowData <- .update_dimData(x=x, dim=\"row\", value=value)\n    return(invisible(x))\n})\n\n\n## ==================\n## ---- colData Slot\n## ------------------\n\n\n#' Retrieve the column metadata table from a LongTable object\n#'\n#' @examples\n#' colData(merckLongTable)\n#'\n#' # Get the keys as well, mostly for internal use\n#' colData(merckLongTable, key=TRUE)\n#'\n#' @describeIn LongTable Get the column level annotations for a LongTable\n#'   object.\n#'\n#' @param x A `LongTable` to retrieve column metadata from.\n#' @param key `logical` Should the colKey column also be returned? Defaults to\n#'     FALSE.\n#' @param ... For developer use only! Pass raw=TRUE to return the slot for\n#'   modification by reference.\n#'\n#' @return A `data.table` containing row identifiers and metadata.\n#'\n#' @import data.table\n#' @export\nsetMethod('colData', signature(x='LongTable'),\n        function(x, key=FALSE, dimnames=FALSE, ...) {\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        return(x@colData)\n    }\n    return(if (key) copy(x@colData[, -'.colnames']) else\n        copy(x@colData[, -c('.colnames', 'colKey')]))\n})\n\n#' Updates the `colData` slot as long as the ID columns are not changed.\n#'\n#' @examples\n#' colData(merckLongTable) <- colData(merckLongTable)\n#'\n#' @describeIn LongTable Update the colData of a LongTable object. Currently\n#'   requires that all of the colIDs(longTable) be in the value object.\n#'\n#' @param x A `LongTable` object to modify.\n#' @param value A `data.table` or `data.frame` to update with. Must have\n#'   all of the colIDs currently in the `LongTable` object in order to ensure\n#'   assay key mappings are consistent.\n#' @param ... For developer use only! Pass raw=TRUE to modify the slot\n#'   directly. This will corrupt your data if you don't know what you are\n#'   doing!\n#'\n#' @return A copy of the `LongTable` object with the `colData`\n#'   slot updated.\n#'\n#' @importFrom crayon cyan magenta\n#' @importFrom SummarizedExperiment colData<-\n#' @importFrom data.table data.table setDT\n#' @export\nsetReplaceMethod('colData', signature(x='LongTable'),\n        function(x, ..., value) {\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        x@colData <- value\n        return(invisible(x))\n    }\n    x@colData <- .update_dimData(x=x, dim=\"col\", value=value)\n    return(invisible(x))\n})\n\n## ==================\n## ---- assaySlot\n## ------------------\n\n\n##\n## == assays\n\n\n#' Return a list of `data.table` objects with the assay measurements from a\n#'  `LongTable` object.\n#'\n#' @examples\n#' assays(merckLongTable)\n#'\n#' @describeIn LongTable Get a list containing all the assays in a `LongTable`.\n#'\n#' @param x `LongTable` What to extract the assay data from.\n#' @param withDimnames `logical` Should the returned assays be joined to\n#'   the row and column identifiers (i.e., the pseudo-dimnames of the object).\n#' @param metadata `logical` Should row and column metadata also be joined\n#'   to the returned assays. This is useful for modifying assays before\n#'   reconstructing a new LongTable.\n#' @param key `logical` Should the key columns also be returned? Defaults\n#'   to !`withDimnames`.\n#' @param ... For developer use only! Pass raw=TRUE to return the slot for\n#'   modification by reference.\n#'\n#' @return A `list` of `data.table` objects, one per assay in the object.\n#'\n#' @importMethodsFrom SummarizedExperiment assays\n#' @import data.table\n#' @export\nsetMethod('assays', signature(x='LongTable'), function(x, withDimnames=TRUE,\n        metadata=withDimnames, key=!withDimnames, ...) {\n    # secret arguments for internal use\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        return(x@assays)\n    }\n\n    # input validation\n    if (!withDimnames && metadata)\n        warning(.warnMsg('[CoreGx::assays] Cannot use metadata=TRUE when',\n            ' withDimnames=FALSE. Ignoring the metadata argument.'),\n            call.=FALSE)\n\n    # optionally join with rowData and colData\n    assayIndex <- assayIndex(x)\n    if (metadata) {\n        rData <- rowData(x, key=TRUE)\n        cData <- colData(x, key=TRUE)\n    } else {\n        rData <- rowIDs(x, data=TRUE, key=TRUE)\n        cData <- colIDs(x, data=TRUE, key=TRUE)\n    }\n    if (withDimnames) {\n        setkeyv(assayIndex, \"rowKey\")\n        assayIndex <- rData[assayIndex, , on=\"rowKey\"]\n        setkeyv(assayIndex, \"colKey\")\n        assayIndex <- cData[assayIndex, , on=\"colKey\"]\n    }\n\n    # honor row and column ordering guarantees from CoreGx design documentation\n    aList <- copy(x@assays)\n    aNames <- names(aList)\n    # prepend with . to match assay index naming convention\n    names(aList) <- paste0(\".\", aNames)\n    corder <- c(\n        if (withDimnames) idCols(x),\n        if (key) c(\"rowKey\", \"colKey\"),\n        if (withDimnames && metadata) c(sort(rowMeta(x)), sort(colMeta(x)))\n    )\n    for (i in seq_along(aList)) {\n        setkeyv(assayIndex, names(aList)[i])\n        aList[[i]] <- assayIndex[aList[[i]], ]\n        deleteCols <- setdiff(names(aList), names(aList)[i])\n        if (length(deleteCols) > 0) {\n            aList[[i]][, (deleteCols) := NULL]\n        }\n        if (withDimnames || key) aList[[i]][, (names(aList)[i]) := NULL]\n        if (!key) {\n            aList[[i]][, c(\"rowKey\", \"colKey\") := NULL]\n        }\n        if (withDimnames) setkeyv(aList[[i]], idCols(x))\n        else if (key) setkeyv(aList[[i]], c(\"rowKey\", \"colKey\"))\n        else setkeyv(aList[[i]], names(aList)[[i]])\n        if (!is.null(corder)) setcolorder(aList[[i]], corder) else\n            setcolorder(aList[[i]])\n    }\n    # reset names to non dot version\n    names(aList) <- aNames\n    return(aList)\n})\n\n\n#' Setter method for the assays slot in a LongTable object\n#'\n#' @examples\n#' assays(merckLongTable) <- assays(merckLongTable, withDimnames=TRUE)\n#'\n#' @describeIn LongTable Update the assays in a LongTable object. The rowIDs\n#'   and colIDs must be present in all assays to allow successfully remapping\n#'   the keys. We recommend modifying the list returned by\n#'   assays(longTable, withDimnames=TRUE) and the reassigning to the\n#'   `LongTable`.\n#'\n#' @param x A `LongTable` to modify the assays in.\n#' @param value A `list` of `data.frame` or `data.table` objects, all of which\n#'   contain the row and column identifiers and metadata.\n#' @param ... For developer use only! Pass raw=TRUE to modify the slot\n#'   directly. This will corrupt your data if you don't know what you are\n#'   doing!\n#'\n#' @return A copy of the `LongTable` with the assays modified.\n#'\n#' @importMethodsFrom SummarizedExperiment assays<-\n#' @import data.table\n#' @export\nsetReplaceMethod('assays', signature(x='LongTable', value='list'),\n        function(x, ..., value) {\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        x@assays <- value\n    } else {\n        assay_names <- names(value)\n        for (name in assay_names) {\n            x[[name]] <- value[[name]]\n        }\n    }\n    return(x)\n})\n\n\n##\n## == assay\n\n\n#' Get an assay from a LongTable object\n#'\n#' @describeIn LongTable Retrieve an assay `data.table` object from the\n#'   `assays` slot of a `LongTable` object.\n#'\n#' @examples\n#' # Default annotations, just the key columns\n#' assay(merckLongTable, 'sensitivity')\n#' assay(merckLongTable, 1)\n#'\n#' # With identifiers joined\n#' assay(merckLongTable, 'sensitivity', withDimnames=TRUE)\n#'\n#' # With identifiers and metadata\n#' assay(merckLongTable, 'profiles', withDimnames=TRUE, metadata=TRUE)\n#'\n#' @param x `LongTable` The `LongTable` object to get the assay from.\n#' @param i `integer` or `character` vector containing the index or name\n#'   of the assay, respectively.\n#' @param withDimnames `logical(1)` Should the dimension names be returned\n#'   joined to the assay. This retrieves both the row and column identifiers\n#'   and returns them joined to the assay. For\n#' @param summarize `logical(1)` If the assays is a summary where some of\n#'   `idCols(x)` are not in `assayKeys(x, i)`, then those missing columns\n#'   are dropped. Defaults to `FALSE`. When `metadata` is `TRUE`, only\n#'   metadata columns with 1:1 cardinality with the assay keys for `i`.\n#' @param metadata `logical(1)` Should all of the metadata also be joined to\n#'   the assay. This is useful when modifying assays as the resulting list\n#'   has all the information needed to recreated the LongTable object. Defaults\n#'   to `withDimnames`.\n#' @param key `logical` Should the key columns also be returned? Defaults to\n#'   !withDimnames. This is incompatible with `summarize=TRUE`, which will\n#'   drop the key columns regardless of the value of this argument.\n#' @param ... For developer use only! Pass raw=TRUE to return the slot for\n#'   modification by reference.\n#'\n#' @importMethodsFrom SummarizedExperiment assay\n#' @importFrom crayon magenta cyan\n#' @import data.table\n#' @export\nsetMethod('assay', signature(x='LongTable'), function(x, i, withDimnames=TRUE,\n        summarize=withDimnames, metadata=!summarize,\n        key=!(summarize || withDimnames), ...) {\n    # secret arguments for internal use\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        return(x@assays[[i]])\n    }\n\n    ## TODO:: Update input validation to use checkmate where possible\n    # validate input\n    if (length(i) > 1)\n        .error('\\n[CoreGx::assay] Please specifying a single string ',\n            'assay name or integer index. See assayNames(x) for available ',\n            'assays.')\n\n    keepAssay <- if (is.character(i)) which(assayNames(x) == i) else i\n    assayName <- assayNames(x)[keepAssay]\n    .assayName <- paste0(\".\", assayName)\n    if (length(keepAssay) < 1)\n        stop(.errorMsg('\\n[CoreGx::assay] There is no assay ', i,\n            ' in this LongTable. Use assayNames(longTable) for a list',\n            'of valid assay names.'),\n            call.=FALSE)\n\n    if (!withDimnames && metadata)\n        warning(.warnMsg('\\n[CoreGx::assay] Cannot use metadata=TRUE when',\n            ' withDimnames=FALSE. Ignoring the metadata argument.'),\n            call.=FALSE)\n\n    if (summarize && key)\n        warning(.warnMsg('\\n[CoreGx::assay] Cannot use key=TRUE when',\n            ' summarize=TRUE. Ignoring the key argument.'),\n            call.=FALSE)\n\n    # extract the specified assay\n    assayData <- copy(x@assays[[keepAssay]])\n\n    # extract the assay index\n    assayIndex <- na.omit(unique(assayIndex(x)[,\n        c(\"rowKey\", \"colKey\", .assayName),\n        with=FALSE\n    ]))\n\n    # handle summarized assays\n    aKeys <- assayKeys(x, assayName)\n    # only compute summaries for assays that are actually summarized\n    summarize <- summarize && !all(idCols(x) %in% aKeys)\n    if (summarize) {\n        assayIndex <- assayIndex[, first(.SD), by=.assayName]\n    }\n\n    setkeyv(assayIndex, .assayName)\n    assayData <- assayData[assayIndex, on=.assayName]\n    # # optinally join with row and column metadata\n    setkeyv(assayData, \"rowKey\")\n    if (withDimnames && !metadata) {\n        assayData <- rowIDs(x, data=TRUE, key=TRUE)[assayData, ]\n        setkeyv(assayData, \"colKey\")\n        assayData <- colIDs(x, data=TRUE, key=TRUE)[assayData, ]\n    } else if (withDimnames && metadata) {\n        assayData <- rowData(x, key=TRUE)[assayData, ]\n        setkeyv(assayData, \"colKey\")\n        assayData <- colData(x, key=TRUE)[assayData, ]\n    }\n    # honour row and column ordering guarantees\n    ## See: https://github.com/bhklab/CoreGx/wiki/CoreGx-Design-Documentation\n    if (withDimnames || key) {\n        assayData[, (.assayName) := NULL]\n        if (withDimnames) setkeyv(assayData, idCols(x)) else\n            setkeyv(assayData, c(\"rowKey\", \"colKey\"))\n    } else {\n        setkeyv(assayData, .assayName)\n    }\n    if (!key) assayData[, c(\"rowKey\", \"colKey\") := NULL]\n    corder <- c(\n        if (withDimnames) idCols(x),\n        if (key) c(\"rowKey\", \"colKey\"),\n        if (withDimnames && metadata) c(sort(rowMeta(x)), sort(colMeta(x)))\n    )\n    if (!is.null(corder)) setcolorder(assayData, corder) else setcolorder(assayData)\n\n    # Drop columns with wrong cardinality from summary assays\n    if (summarize) {\n        summaryCols <- assayCols(x, assayName)\n        if (metadata) {\n            rCols <- colnames(rowData(x))\n            cCols <- colnames(colData(x))\n            rBy <- intersect(rCols, aKeys)\n            cBy <- intersect(cCols, aKeys)\n            rKeep <- rCols[\n                rowData(x)[, lapply(.SD, uniqueN), by=c(rBy)][, lapply(.SD, max)] == 1\n            ]\n            cKeep <- cCols[\n                colData(x)[, lapply(.SD, uniqueN), by=c(cBy)][, lapply(.SD, max)] == 1\n            ]\n            summaryCols <- c(summaryCols, rKeep, cKeep)\n        }\n        # use of %in% should maintain column ordering gaurantees\n        assayData <- assayData[, colnames(assayData) %in% summaryCols, with=FALSE]\n        setkeyv(assayData, aKeys)\n    }\n\n    ## Add [] to ensure assay always prints, even after modify by reference\n    ## See: https://stackoverflow.com/questions/33195362/data-table-is-not-displayed-on-first-call-after-being-modified-in-a-function\n    return(assayData[])\n})\n\n\n#' Add or replace an assay in a LongTable by name or index\n#'\n#' @description Add or replace an assay in a LongTable by name. Currently\n#'    this function only works when the assay has all columns in row and column\n#'    data tables (i.e., when assays is retured withDimnames=TRUE).\n#'\n#' @examples\n#' assay(merckLongTable, 'sensitivity') <-\n#'      assay(merckLongTable, 'sensitivity', withDimnames=TRUE)\n#' assay(merckLongTable, 'sensitivity') <- merckLongTable$sensitivity\n#'\n#' @param x A `LongTable` to update.\n#' @param i `integer` or `character` vector containing the index or name\n#'   of the assay to update.\n#' @param value\n#' A `data.frame` or `data.table` to update the assay data\n#'   with. This must at minumum contain the row and column data identifier\n#'   columns to allow correctly mapping the assay keys. We recommend modifying\n#'   the results returned by assay(longTable, 'assayName', withDimnames=TRUE).\n#'   For convenience, both the `[[` and `$` LongTable accessors return an assay\n#'   with the dimnames.\n#'\n#' @return `LongTable` With updated assays slot.\n#'\n#' @describeIn LongTable\n#'\n#' @md\n#' @importMethodsFrom SummarizedExperiment assay<-\n#' @importFrom data.table data.table fsetdiff setcolorder set setDT\n#' @export\nsetReplaceMethod('assay', signature(x='LongTable'), function(x, i, value) {\n    stopifnot(is.character(i) || is.numeric(i))\n\n    funContext <- CoreGx:::.S4MethodContext('assay', class(x))\n    if (length(i) > 1) .error(funContext, ' Only a single assay ',\n        'name can be assiged with assay(x, i) <- value.')\n\n    # -- determine if the assay already exists\n    if (is.numeric(i)) i <- assayNames(x)[i]\n    .i <- paste0(\".\", i)\n    assayExists <- i %in% assayNames(x)\n\n    if (is.null(value)) {\n        keepAssays <- setdiff(assayNames(x), i)\n        return(x[, , keepAssays])\n    }\n\n    if (!is.data.frame(value)) .error(funContext, ' Only a data.frame or',\n        ' data.table can be assiged to the assay slot!')\n    value <- copy(value)  # prevent modify by reference\n    if (!is.data.table(value)) setDT(value)\n\n    # -- extract strucutral metadata from .intern slot\n    mutable_intern <- mutable(getIntern(x))\n    aIndex <- mutable_intern$assayIndex\n    aKeys <- mutable_intern$assayKeys\n\n    # -- determine the id columns if the assay doesn't already exits\n    if (!any(assayExists)) {\n        assayKey <- intersect(idCols(x), colnames(value))\n    } else {\n        if (sum(assayExists) > 1)\n            .error(funContext, \"Only one assay can be modified at a time.\",\n                \" Please set i to be a character(1) vector.\")\n        assayKey <- aKeys[[i]]\n    }\n    # -- add assayKey column to the value\n    setkeyv(value, assayKey)\n    value[, (.i) := .I]\n\n    # -- join assay with existing metadata\n    rKeys <- intersect(rowIDs(x), assayKey)\n    cKeys <- intersect(colIDs(x), assayKey)\n    rIndex <- rowIDs(x, data=TRUE, key=TRUE)[, .SD, .SDcols=c(\"rowKey\", rKeys)]\n    setkeyv(rIndex, rKeys)\n    cIndex <- colIDs(x, data=TRUE, key=TRUE)[, .SD, .SDcols=c(\"colKey\", cKeys)]\n    setkeyv(cIndex, cKeys)\n\n    # -- add an index to the assay\n    setkeyv(rIndex, \"rowKey\")\n    setkeyv(cIndex, \"colKey\")\n    setkeyv(aIndex, \"rowKey\")\n    annotatedIndex <- merge.data.table(aIndex, rIndex, all=TRUE)\n    setkeyv(annotatedIndex, \"colKey\")\n    annotatedIndex <- merge.data.table(annotatedIndex, cIndex, all=TRUE)\n\n    # -- update assayIndex with the new assay\n    setkeyv(annotatedIndex, assayKey)\n    if (.i %in% colnames(annotatedIndex)) annotatedIndex[, (.i) := NULL]\n    # FIXME:: This is really slow with by=.EACHI when the cardinality is high\n    annotatedIndex[value, (.i) := get(.i), on=assayKey, by=.EACHI]\n    annotatedIndex[, (assayKey) := NULL]\n    setkeyv(annotatedIndex, unique(c(paste0(\".\", assayNames(x)), .i)))\n\n    # -- detect and warn users if they have modified id columns\n    # rowIDs\n    presentRowIDs <- intersect(rowIDs(x), colnames(value))  # allow summary over some keys\n    if (!(length(presentRowIDs) > 0)) stop(.errorMsg(\"No rowIDs(x) present in\",\n        \"value! Cannot summarize over an entire dimension.\"), call.=FALSE)\n    ## set check.attributes=FALSE to allow unequal table keys\n    equalRowIDs <- .table_is_subset(\n        unique(value[, presentRowIDs, with=FALSE])[order(mget(presentRowIDs))],\n        unique(rowIDs(x, data=TRUE)[order(mget(presentRowIDs)), presentRowIDs,\n            with=FALSE])\n    )\n    if (!isTRUE(equalRowIDs))\n        stop(.errorMsg(\"One or more rowIDs(x) columns have been modified.\",\n                \" Identifier columns cannot be modified via assay assignment!\"),\n            call.=FALSE\n        )\n    # colIDs\n    presentColIDs <- intersect(colIDs(x), colnames(value))  # allow summary over some keys\n    if (!(length(presentColIDs) > 0)) stop(.errorMsg(\"No colIDs(x) present in\",\n        \"value! Cannot summarize over an entire dimension.\"), call.=FALSE)\n    equalColIDs <- .table_is_subset(\n        unique(value[, presentColIDs, with=FALSE])[order(mget(presentColIDs))],\n        unique(colIDs(x, data=TRUE)[order(mget(presentColIDs)), presentColIDs,\n            with=FALSE])\n    )\n    if (!isTRUE(equalColIDs))\n        stop(.errorMsg(\"One or more colIDs(x) column have been modified.\",\n                \" Identifier columns cannot be modified via assay assignment!\"),\n            call.=FALSE\n        )\n\n    # -- remove metadata columns for the assay\n    throwAwayCols <- c(idCols(x), rowMeta(x), colMeta(x))\n    keepCols <- setdiff(colnames(value), throwAwayCols)\n    assayValue <- unique(value[, keepCols, with=FALSE])\n    setkeyv(assayValue, .i)\n\n    # -- update the object\n    setcolorder(annotatedIndex, c(\"rowKey\", \"colKey\"))\n    mutable_intern$assayIndex <- annotatedIndex\n    mutable_intern$assayKeys[[i]] <- assayKey\n    x@.intern <- immutable(mutable_intern)\n    x@assays[[i]] <- assayValue\n\n    return(x)\n})\n\n##\n## == assayNames\n\n\n#' Retrieve the assay names from a `LongTable` object.\n#'\n#' @examples\n#' assayNames(merckLongTable)\n#' names(merckLongTable)\n#'\n#' @describeIn LongTable Return the names of the assays contained in a\n#'   `LongTable`\n#'\n#' @param x A `LongTable` object to retrieve the assay names from.\n#'\n#' @return `character` Names of the assays contained in the `LongTable`.\n#'\n#' @importMethodsFrom SummarizedExperiment assayNames\n#' @aliases names,LongTable-method names\n#' @export\nsetMethod('assayNames', signature(x='LongTable'), function(x) {\n    return(names(x@assays))\n})\n#' @export\nsetMethod(\"names\", signature(x=\"LongTable\"), function(x) {\n    return(assayNames(x))\n})\n\n\n## ==================\n## ---- metadata Slot\n## ------------------\n\n\n#' Getter method for the metadata slot of a `LongTable` object\n#'\n#' @param x The `LongTable` object from which to retrieve the metadata list.\n#'\n#' @return `list` The contents of the `metadata` slot of the `LongTable`\n#'   object.\n#'\n#' @importMethodsFrom S4Vectors metadata\n#' @export\nsetMethod('metadata', signature(x='LongTable'), function(x) {\n    return(x@metadata)\n})\n\n\n#' Setter method for the metadata slot of a `LongTable` object\n#'\n#' @param x `LongTable` The LongTable to update\n#' @param value `list` A list of new metadata associated with a `LongTable`\n#'   object.\n#'\n#' @return `LongTable` A copy of the `LongTable` object with the `value` in\n#'   the metadata slot.\n#'\n#' @importMethodsFrom S4Vectors metadata<-\n#' @importFrom crayon cyan magenta\n#' @export\nsetReplaceMethod('metadata', signature(x='LongTable'), function(x, value) {\n    if (!is(value, 'list'))\n        stop(magenta$bold('The `metadata` slot must be a list!'))\n    x@metadata <- value\n    return(x)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.local_class_4` and `.local_data_4` variables at the beginning of the code?",
        "answer": "These variables are used for dynamic documentation. `.local_class_4` is set to 'LongTable' and `.local_data_4` is set to 'merckLongTable'. They are likely used to generate documentation for the LongTable class and provide examples using the merckLongTable dataset."
      },
      {
        "question": "How does the `getIntern` method work for the LongTable class?",
        "answer": "The `getIntern` method retrieves structural metadata from the `.intern` slot of a LongTable object. It has two versions: one that takes a character vector of symbol names to retrieve specific metadata, and another that returns all metadata when no specific symbols are provided. The method returns either a single value or a named list of values, depending on the input."
      },
      {
        "question": "What is the purpose of the `.update_dimData` function in the context of the LongTable class?",
        "answer": "The `.update_dimData` function is a helper method used by both `rowData<-` and `colData<-` methods. It updates the row or column metadata of a LongTable object. It handles tasks such as removing key columns, joining with existing IDs, checking for duplicates, and ensuring the updated data meets the requirements for assignment to a LongTable or inheriting class."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Navigating this file:\n# - Slot section names start with ----\n# - Method section names start with ==\n#\n# As a result, you can use Ctrl + f to find the slot or method you are looking\n# for quickly, assuming you know its name.\n#\n# For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n# method, while Ctrl + f '---- molecularProfiles' would take you to the slot\n# section.\n\n#' @include LongTable-class.R allGenerics.R\nNULL\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n.local_class_4 <- 'LongTable'\n.local_data_4 <- 'merckLongTable'\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n\n#' @noRd\n.docs_LongTable_accessors <- function(...) .parseToRoxygen(\n    \"\n    @title Accessing and modifying information in a `{class_}`\n\n    @description\n    Documentation for the various setters and getters which allow manipulation\n    of data in the slots of a `{class_}` object.\n\n    @return Accessors: See details.\n    @return Setters: An updated `{class_}` object, returned invisibly.\n    \",\n    ...\n)\n\n#' @name LongTable-accessors\n#' @eval .docs_LongTable_accessors(class_=.local_class_4)\n#' @eval .parseToRoxygen(\"@examples data({data_})\", data_=.local_data_4)\nNULL\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ==================\n## ---- .intern Slot\n## ------------------\n\n\n##\n## == getIntern\n\n\n#' Get the symbol(s) x from the object@.intern slot of a LongTable\n#'\n#' This is used as an alternative to R attributes for storing structural\n#' metadata of an S4 objects.\n#'\n#' @examples\n#' getIntern(merckLongTable, 'rowIDs')\n#' getIntern(merckLongTable, c('colIDs', 'colMeta'))\n#'\n#' @describeIn LongTable Access structural metadata present within a\n#'   LongTable object. This is mostly for developmer use.\n#'\n#' @param object `LongTable`\n#' @param x `character` One or more symbol name strings to retrieve from\n#'     the object@.intern environment.\n#'\n#' @return `immutable` value of x if length(x) == 1 else named list of values\n#'     for all symbols in x.\n#'\n#' @include LongTable-class.R\n#' @export\nsetMethod('getIntern', signature(object='LongTable', x='character'),\n        function(object, x) {\n    return(if (length(x) == 1) object@.intern[[x]] else object@.intern[x])\n})\n#' @describeIn LongTable Access all structural metadata present within a\n#'   LongTable object. This is primarily for developmer use.\n#'\n#' @param object `LongTable`\n#' @param x `missing` This argument is excluded from from the function call.\n#'\n#' @return An `immutable` list.\n#'\n#' @examples\n#' getIntern(merckLongTable)\n#'\n#' @aliases getIntern,LongTable,missing-method\n#' @export\nsetMethod('getIntern', signature(object='LongTable', x='missing'),\n    function(object, x) object@.intern\n)\n\n#' Set the .intern slot of a LongTable\n#'\n#' @param object `LongTable`\n#' @param value An `immutable_list` object, being a class union between `list`\n#'   and `immutable` S3 classes.\n#'\n#' @return Updates the object and returns invisibly.\n#'\n#' @keywords internal\nsetReplaceMethod(\"getIntern\", signature(object=\"LongTable\",\n    value=\"immutable_list\"), function(object, value) {\n        object@.intern <- value\n        return(object)\n})\n\n## ==================\n## ---- rowData Slot\n## ------------------\n\n#' Retrieve the row metadata table from a LongTable object\n#'\n#' @examples\n#' rowData(merckLongTable)\n#'\n#' @describeIn LongTable Get the row level annotations for a `LongTable` object.\n#'\n#' @param x A `LongTable` object to retrieve the row metadata from.\n#' @param key `logical` Should the rowKey column also be returned? Defaults\n#'     to FALSE.\n#' @param use.names `logical` This parameter is just here to stop matching\n#'     the positional argument to use.names from the rowData generic. It\n#'     doesn't do anything at this time and can be ignored.\n#' @param ... For developer use only! Pass raw=TRUE to modify the slot\n#'   directly. This will corrupt your data if you don't know what you are\n#'   doing!\n#'\n#' @return A `data.table` containing rowID, row identifiers, and row metadata.\n#'\n#' @importFrom data.table data.table copy\n#' @export\nsetMethod('rowData', signature(x='LongTable'),\n        function(x, key=FALSE, use.names=FALSE, ...) {\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        return(x@rowData)\n    } else {\n        return(if (key) copy(x@rowData[, -'.rownames']) else\n            copy(x@rowData[, -c('.rownames', 'rowKey')]))\n    }\n})\n\n#' Helper method to share functionality between rowData and colData replace methods\n#'\n#' @param x `LongTable` or inheriting class to update dimData for.\n#' @param dim `character(1)` One of \"row\" or \"col\" indicating with dimension\n#'   to updated metadata for.\n#' @param value #' @param value A `data.table` or `data.frame` to update the\n#'   `rowData` or `colData` of `x` with.\n#'\n#' @return An updated version of `value` which meets all the requirements for\n#'   assignment to a `LongTable` or inheriting class.\n#'\n#' @noRd\n#' @keywords internal\n.update_dimData <- function(x, dim, value) {\n\n    titleDim <- paste0(toupper(substr(dim, 1, 1)), substr(dim, 2, nchar(dim)))\n    dimIDs <- get(paste0(dim, \"IDs\"))\n    dimKey <- paste(dim, \"Key\")\n    dimData <- paste0(dim, \"Data\")\n\n    # type check input\n    if (is(value, 'data.frame')) setDT(value)\n    if (!is(value, 'data.table'))\n        stop(.errorMsg('\\n[CoreGx::', dim, 'Data<-] Please pass a data.frame or ',\n            'data.table to update the ', dim, 'Data slot. We recommend modifying the',\n            ' object returned by ', dim, 'Data(x) then reassigning it with ',\n            dim, 'Data(x)',\n            ' <- new', titleDim, 'Data'),\n            call.=FALSE\n        )\n\n    # remove key column\n    if (dimKey %in% colnames(value)) {\n        value[, (dimKey) := NULL]\n        .message('\\n[CoreGx::', dim, ,'Data<-] Dropping ', dim, 'Key from replacement',\n            ' value, this function will deal with mapping the ', dim, 'Key',\n            ' automatically.')\n    }\n\n    # assemble information to select proper update method\n    dimIDCols <- dimIDs(x)\n    sharedDimIDCols <- intersect(dimIDCols, colnames(value))\n\n    # error if all the row/colID columns are not present in the new row/colData\n    equalDimIDs <- dimIDCols %in% sharedDimIDCols\n    if (!all(equalDimIDs)) warning(.warnMsg('\\n[CoreGx::', dim,\n        'Data<-] The ID columns ', dimIDCols[!equalDimIDs],\n        ' are not present in value. The function ',\n        'will attempt to join with existing ', dim, 'IDs, but this may fail!',\n        collapse=', '), call.=FALSE)\n\n    dimIDs_ <- dimIDs(x, data=TRUE, key=TRUE)\n\n    ## TODO:: Throw error if user tries to modify ID columns\n\n    duplicatedIDcols <- value[, .N, by=c(sharedDimIDCols)][, N > 1]\n    if (any(duplicatedIDcols))\n        warning(.warnMsg(\"\\n[CoreGx::\", dim, \"Data<-,\", class(x)[1], \"-method] The \",\n            \"ID columns are duplicated for rows \",\n            .collapse(which(duplicatedIDcols)),\n            \"! These rows will be dropped before assignment.\"),\n        call.=FALSE)\n\n    dimData <- dimIDs_[unique(value), on=.NATURAL, allow.cartesian=FALSE]\n    dimData[, (dimKey) := .I]\n    dimData <- dimData[!duplicated(get(dimKey)), ]\n    setkeyv(dimData, dimKey)\n    dimData[, .rownames := Reduce(.paste_colon, mget(dimIDCols))]\n\n    ## TODO:: Add some sanity checks before returing\n\n    return(dimData)\n}\n\n\n#' Updates the `rowData` slot as long as the ID columns are not changed.\n#'\n#' @examples\n#' rowData(merckLongTable) <- rowData(merckLongTable)\n#'\n#' @describeIn LongTable Update the row annotations for a `LongTable` object.\n#'   Currently requires that all columns in rowIDs(longTable) be present in\n#'   value.\n#'\n#' @param x A `LongTable` object to modify.\n#' @param value A `data.table` or `data.frame` to update the `rowData` of\n#'   `x` with.\n#' @param ... For developer use only! Pass raw=TRUE to modify the slot\n#'   directly. This will corrupt your data if you don't know what you are\n#'   doing!\n#'\n#' @return A copy of the `LongTable` object with the `rowData`\n#'   slot updated.\n#'\n#' @md\n#' @importFrom crayon cyan magenta\n#' @importFrom SummarizedExperiment `rowData<-`\n#' @importFrom data.table setDT\n#' @export\nsetReplaceMethod('rowData', signature(x='LongTable'), function(x, ..., value) {\n\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        x@rowData <- value\n        return(invisible(x))\n    }\n\n    x@rowData <- .update_dimData(x=x, dim=\"row\", value=value)\n    return(invisible(x))\n})\n\n\n## ==================\n## ---- colData Slot\n## ------------------\n\n\n#' Retrieve the column metadata table from a LongTable object\n#'\n#' @examples\n#' colData(merckLongTable)\n#'\n#' # Get the keys as well, mostly for internal use\n#' colData(merckLongTable, key=TRUE)\n#'\n#' @describeIn LongTable Get the column level annotations for a LongTable\n#'   object.\n#'\n#' @param x A `LongTable` to retrieve column metadata from.\n#' @param key `logical` Should the colKey column also be returned? Defaults to\n#'     FALSE.\n#' @param ... For developer use only! Pass raw=TRUE to return the slot for\n#'   modification by reference.\n#'\n#' @return A `data.table` containing row identifiers and metadata.\n#'\n#' @import data.table\n#' @export\nsetMethod('colData', signature(x='LongTable'),\n        function(x, key=FALSE, dimnames=FALSE, ...) {\n    if (any(...names() == \"raw\") && isTRUE(...elt(which(...names() == \"raw\")))) {\n        return(x@colData)\n    }\n    return(if (key) copy(x@colData[, -'.colnames']) else\n        copy(x@colData[, -c('.colnames', 'colKey')]))\n})\n\n#' Updates the `colData` slot as long as the ID columns are not changed.\n#'\n#' @examples\n#' colData(mer"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/zzz.R",
    "language": "R",
    "content": "# Package Start-up Functions\n\n.onAttach <- function(libname, pkgname) {\n\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\nRadioGx package brought to you by:\n\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\nFor more of our work visit bhklab.ca!\n        \"\n        )\n        # Prevent repeated messages when loading multiple lab packages\n        options(bhklab.startup_=FALSE)\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.onAttach` function in this code snippet?",
        "answer": "The `.onAttach` function is a special R function that is automatically called when a package is loaded. In this case, it's used to display a startup message with ASCII art when the RadioGx package is loaded interactively. It also sets an option to prevent the message from being displayed multiple times if other related packages are loaded."
      },
      {
        "question": "How does the code ensure that the startup message is only displayed once, even if multiple related packages are loaded?",
        "answer": "The code uses the `options()` function to set and check a custom option called 'bhklab.startup_'. Before displaying the message, it checks if this option is NULL. After displaying the message, it sets the option to FALSE. This prevents the message from being shown again if another package uses the same mechanism."
      },
      {
        "question": "What is the purpose of the `on.exit(options(oldOpts))` line in the code?",
        "answer": "The `on.exit(options(oldOpts))` line ensures that the original R options are restored when the function exits, even if an error occurs. This is a good practice to maintain the user's environment. In this case, it's specifically used to restore the warning level (`options(warn=-1)`) that was temporarily changed to suppress warnings during the function execution."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"RadioGx package brought to you by:\\n\\nBHK Lab\\n\\nFor more of our work visit bhklab.ca!\"\n        )\n        # Complete the function\n    }\n}",
        "complete": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"RadioGx package brought to you by:\\n\\nBHK Lab\\n\\nFor more of our work visit bhklab.ca!\"\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}"
      },
      {
        "partial": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        # Set up options and display message\n    }\n}",
        "complete": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n        packageStartupMessage(\"RadioGx package brought to you by:\\n\\nBHK Lab\\n\\nFor more of our work visit bhklab.ca!\")\n        options(bhklab.startup_=FALSE)\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-metaConstruct.R",
    "language": "R",
    "content": "#' Generic for preprocessing complex data before being used in the constructor\n#'   of an `S4` container object.\n#'\n#' This method is intended to abstract away complex constructor arguments\n#'   and data preprocessing steps needed to transform raw data, such as that\n#'   produced in a treatment-response or next-gen sequencing experiment, and\n#'   automate building of the appropriate `S4` container object. This is\n#'   is intended to allow mapping between different experimental designs,\n#'   in the form of an `S4` configuration object, and various S4 class\n#'   containers in the Bioconductor community and beyond.\n#'\n#' @param mapper An `S4` object abstracting arguments to an `S4` class\n#'   constructor into a well documented `Mapper` object.\n#' @param ... Allow new arguments to be defined for this generic.\n#'\n#' @return An `S4` object for which the class corresponds to the type of\n#'   the build configuration object passed to this method.\n#'\n#' @md\n#' @export\nsetGeneric('metaConstruct', function(mapper, ...) standardGeneric('metaConstruct'))\n\n#' @rdname metaConstruct\n#' @title metaConstruct\n#'\n#' @param mapper An `LongTableDataMapper` object abstracting arguments to an\n#'  the `LongTable` constructor.\n#'\n#' @return A `LongTable` object, as specified in the mapper.\n#'\n#' @examples\n#' data(exampleDataMapper)\n#' rowDataMap(exampleDataMapper) <- list(c('treatmentid'), c())\n#' colDataMap(exampleDataMapper) <- list(c('sampleid'), c())\n#' assayMap(exampleDataMapper) <- list(sensitivity=list(c(\"treatmentid\", \"sampleid\"), c('viability')))\n#' metadataMap(exampleDataMapper) <- list(experiment_metadata=c('metadata'))\n#' longTable <- metaConstruct(exampleDataMapper)\n#' longTable\n#'\n#' @md\n#' @importFrom data.table key\n#' @export\nsetMethod('metaConstruct', signature(mapper='LongTableDataMapper'),\n        function(mapper) {\n    .metaConstruct(mapper)\n})\n\n#' @rdname metaConstruct\n#' @title metaConstruct\n#'\n#' @param mapper An `TREDataMapper` object abstracting arguments to an\n#'  the `TreatmentResponseExperiment` constructor.\n#'\n#' @return A `TreatmentResponseExperiment` object, as specified in the mapper.\n#'\n#' @examples\n#' data(exampleDataMapper)\n#' exampleDataMapper <- as(exampleDataMapper, \"TREDataMapper\")\n#' rowDataMap(exampleDataMapper) <- list(c('treatmentid'), c())\n#' colDataMap(exampleDataMapper) <- list(c('sampleid'), c())\n#' assayMap(exampleDataMapper) <- list(sensitivity=list(c(\"treatmentid\", \"sampleid\"), c('viability')))\n#' metadataMap(exampleDataMapper) <- list(experiment_metadata=c('metadata'))\n#' tre <- metaConstruct(exampleDataMapper)\n#' tre\n#'\n#' @md\n#' @importFrom data.table key\n#' @export\nsetMethod('metaConstruct', signature(mapper='TREDataMapper'),\n        function(mapper) {\n    .metaConstruct(mapper, constructor=CoreGx::TreatmentResponseExperiment)\n})\n\n#' @keywords internal\n#' @noRd\n.metaConstruct <- function(mapper, constructor=CoreGx::LongTable) {\n    funContext <- paste0('[', .S4MethodContext('metaConstruct', class(mapper)[1]))\n\n    # subset the rawdata slot to build out each component of LongTable\n    rowDataDT <- rowData(mapper)\n    rowIDs <- rowDataMap(mapper)[[1]]\n    rid_names <- names(rowIDs)\n    has_rid_names <- !is.null(rid_names) & rid_names != \"\"\n    rowIDs[has_rid_names] <- rid_names[has_rid_names]\n    colDataDT <- colData(mapper)\n    colIDs <- colDataMap(mapper)[[1]]\n    cid_names <- names(colIDs)\n    has_cid_names <- !is.null(cid_names) & cid_names != \"\"\n    colIDs[has_cid_names] <- cid_names[has_cid_names]\n    assayDtL <- assays(mapper)\n    assayIDs <- lapply(assayMap(mapper), `[[`, i=1)\n    for (i in seq_along(assayIDs)) {\n        aid_names <- names(assayIDs[[i]])\n        notEmptyNames <- !is.null(aid_names) & aid_names != \"\"\n        assayIDs[[i]][notEmptyNames] <- aid_names[notEmptyNames]\n    }\n\n    # subset the metadata columns out of raw data and add any additional metadata\n    metadataL <- lapply(metadataMap(mapper),\n        function(j, x) as.list(unique(x[, j, with=FALSE])), x=rawdata(mapper))\n    metadataL <- c(metadataL, metadata(mapper))\n\n    ## FIXME:: Handle TREDataMapper class after updating constructor\n    object <- constructor(\n        rowData=rowDataDT, rowIDs=rowIDs,\n        colData=colDataDT, colIDs=colIDs,\n        assays=assayDtL, assayIDs=assayIDs,\n        metadata=metadataL)\n\n    return(object)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `metaConstruct` generic function in this code snippet?",
        "answer": "The `metaConstruct` generic function is designed to preprocess complex data before it's used in the constructor of an `S4` container object. It abstracts away complex constructor arguments and data preprocessing steps needed to transform raw data from experiments like treatment-response or next-gen sequencing. This function automates the building of appropriate `S4` container objects, allowing mapping between different experimental designs and various S4 class containers in the Bioconductor community."
      },
      {
        "question": "How does the `.metaConstruct` internal function handle the creation of rowIDs and colIDs?",
        "answer": "The `.metaConstruct` function creates rowIDs and colIDs by extracting them from the mapper object. It uses `rowDataMap(mapper)[[1]]` for rowIDs and `colDataMap(mapper)[[1]]` for colIDs. The function also handles naming of these IDs. If names are provided and are not empty, it assigns these names to the corresponding IDs. This is done using the logic: `rowIDs[has_rid_names] <- rid_names[has_rid_names]` for rowIDs, and similarly for colIDs."
      },
      {
        "question": "What is the difference between the `metaConstruct` method for `LongTableDataMapper` and `TREDataMapper`?",
        "answer": "The main difference lies in the constructor used in the `.metaConstruct` function call. For `LongTableDataMapper`, the default constructor `CoreGx::LongTable` is used. For `TREDataMapper`, the constructor is explicitly set to `CoreGx::TreatmentResponseExperiment`. This allows the `metaConstruct` generic to create different types of S4 objects based on the class of the mapper object passed to it, while using the same underlying `.metaConstruct` function for the actual object construction."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setGeneric('metaConstruct', function(mapper, ...) standardGeneric('metaConstruct'))\n\nsetMethod('metaConstruct', signature(mapper='LongTableDataMapper'),\n        function(mapper) {\n    # TODO: Implement the method\n})\n\nsetMethod('metaConstruct', signature(mapper='TREDataMapper'),\n        function(mapper) {\n    # TODO: Implement the method\n})",
        "complete": "setGeneric('metaConstruct', function(mapper, ...) standardGeneric('metaConstruct'))\n\nsetMethod('metaConstruct', signature(mapper='LongTableDataMapper'),\n        function(mapper) {\n    .metaConstruct(mapper)\n})\n\nsetMethod('metaConstruct', signature(mapper='TREDataMapper'),\n        function(mapper) {\n    .metaConstruct(mapper, constructor=CoreGx::TreatmentResponseExperiment)\n})"
      },
      {
        "partial": ".metaConstruct <- function(mapper, constructor=CoreGx::LongTable) {\n    # TODO: Implement the function body\n    # Hint: Extract data from mapper and create the object using the constructor\n}",
        "complete": ".metaConstruct <- function(mapper, constructor=CoreGx::LongTable) {\n    rowDataDT <- rowData(mapper)\n    rowIDs <- rowDataMap(mapper)[[1]]\n    rid_names <- names(rowIDs)\n    has_rid_names <- !is.null(rid_names) & rid_names != \"\"\n    rowIDs[has_rid_names] <- rid_names[has_rid_names]\n    colDataDT <- colData(mapper)\n    colIDs <- colDataMap(mapper)[[1]]\n    cid_names <- names(colIDs)\n    has_cid_names <- !is.null(cid_names) & cid_names != \"\"\n    colIDs[has_cid_names] <- cid_names[has_cid_names]\n    assayDtL <- assays(mapper)\n    assayIDs <- lapply(assayMap(mapper), `[[`, i=1)\n    for (i in seq_along(assayIDs)) {\n        aid_names <- names(assayIDs[[i]])\n        notEmptyNames <- !is.null(aid_names) & aid_names != \"\"\n        assayIDs[[i]][notEmptyNames] <- aid_names[notEmptyNames]\n    }\n    metadataL <- lapply(metadataMap(mapper),\n        function(j, x) as.list(unique(x[, j, with=FALSE])), x=rawdata(mapper))\n    metadataL <- c(metadataL, metadata(mapper))\n    object <- constructor(\n        rowData=rowDataDT, rowIDs=rowIDs,\n        colData=colDataDT, colIDs=colIDs,\n        assays=assayDtL, assayIDs=assayIDs,\n        metadata=metadataL)\n    return(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/endoaggregate-methods.R",
    "language": "R",
    "content": "#' @include allGenerics.R\n#' @include aggregate-methods.R\nNULL\n\n#' Functional API for endomorphic aggregation over a `LongTable` or\n#' inheriting class\n#'\n#' @description\n#' Compute a group-by operation over a `LongTable` object or its inhering\n#' classes.\n#'\n#' @param x `LongTable` or inheriting class to compute aggregation on.\n#' @param assay `character(1)` The assay to aggregate over.\n#' @param target `character(1)` The assay to assign the results to. Defaults\n#' to `assay`.\n#' @param subset `call` An R call to evaluate before perfoming an aggregate.\n#' This allows you to aggregate over a subset of columns in an assay but have\n#' it be assigned to the parent object. Default is TRUE, which includes all\n#' rows. Passed through as the `i` argument in `[.data.table`.\n#' @eval .docs_CoreGx_aggregate(curly=\"{\")\n#'\n#' @return Object with the same class as `x`, with the aggregation results\n#' assigned to `target`, using `strategy` if `target` is an existing assay in\n#' `x`.\n#'\n#' @seealso `data.table::[.data.table`, `BiocParallel::bplapply`\n#'\n#' @export\nsetMethod(\"endoaggregate\", signature(x=\"LongTable\"),\n        function(x, ..., assay, target=assay, by, subset=TRUE, nthread=1,\n        progress=TRUE, BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[[assay]][eval(i), ]\n    res <- aggregate2(\n        assay_,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n        moreArgs=moreArgs\n    )\n    if (target %in% assayNames(x)) {\n        res <- merge.data.table(x[[assay]], res, by=by)\n    }\n    x[[target]] <- res\n    x\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `endoaggregate` method in this code snippet?",
        "answer": "The `endoaggregate` method is designed to perform a group-by operation over a `LongTable` object or its inheriting classes. It computes an aggregation on a specified assay, potentially applies a subset operation, and assigns the results to a target assay within the same object. This method allows for endomorphic aggregation, meaning the input and output have the same class."
      },
      {
        "question": "How does the `subset` parameter work in the `endoaggregate` method?",
        "answer": "The `subset` parameter allows for filtering the data before aggregation. It's defined as a `call` object, which is evaluated before performing the aggregate operation. This parameter is passed as the `i` argument to `[.data.table`, enabling users to aggregate over a subset of columns in an assay while assigning the result to the parent object. By default, it's set to `TRUE`, which includes all rows in the aggregation."
      },
      {
        "question": "What happens if the `target` assay already exists in the input object `x`?",
        "answer": "If the `target` assay already exists in the input object `x`, the code merges the aggregation results with the existing assay data. This is done using the `merge.data.table` function, which combines the existing assay data (`x[[assay]]`) with the aggregation results (`res`) based on the `by` columns. This ensures that the existing data in the target assay is preserved and updated with the new aggregation results."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"endoaggregate\", signature(x=\"LongTable\"),\n        function(x, ..., assay, target=assay, by, subset=TRUE, nthread=1,\n        progress=TRUE, BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[[assay]][eval(i), ]\n    res <- aggregate2(\n        assay_,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n        moreArgs=moreArgs\n    )\n    # Complete the code to handle the case when target is in assayNames(x)\n    # and return the modified x object\n})",
        "complete": "setMethod(\"endoaggregate\", signature(x=\"LongTable\"),\n        function(x, ..., assay, target=assay, by, subset=TRUE, nthread=1,\n        progress=TRUE, BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[[assay]][eval(i), ]\n    res <- aggregate2(\n        assay_,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n        moreArgs=moreArgs\n    )\n    if (target %in% assayNames(x)) {\n        res <- merge.data.table(x[[assay]], res, by=by)\n    }\n    x[[target]] <- res\n    x\n})"
      },
      {
        "partial": "#' Functional API for endomorphic aggregation over a `LongTable` or\n#' inheriting class\n#'\n#' @param x `LongTable` or inheriting class to compute aggregation on.\n#' @param assay `character(1)` The assay to aggregate over.\n#' @param target `character(1)` The assay to assign the results to. Defaults\n#' to `assay`.\n#' @param subset `call` An R call to evaluate before perfoming an aggregate.\n#' This allows you to aggregate over a subset of columns in an assay but have\n#' it be assigned to the parent object. Default is TRUE, which includes all\n#' rows. Passed through as the `i` argument in `[.data.table`.\n#' @eval .docs_CoreGx_aggregate(curly=\"{\")\n#'\n#' @return Object with the same class as `x`, with the aggregation results\n#' assigned to `target`, using `strategy` if `target` is an existing assay in\n#' `x`.\n#'\n#' @seealso `data.table::[.data.table`, `BiocParallel::bplapply`\n#'\n#' @export\n# Complete the function signature and body",
        "complete": "#' Functional API for endomorphic aggregation over a `LongTable` or\n#' inheriting class\n#'\n#' @param x `LongTable` or inheriting class to compute aggregation on.\n#' @param assay `character(1)` The assay to aggregate over.\n#' @param target `character(1)` The assay to assign the results to. Defaults\n#' to `assay`.\n#' @param subset `call` An R call to evaluate before perfoming an aggregate.\n#' This allows you to aggregate over a subset of columns in an assay but have\n#' it be assigned to the parent object. Default is TRUE, which includes all\n#' rows. Passed through as the `i` argument in `[.data.table`.\n#' @eval .docs_CoreGx_aggregate(curly=\"{\")\n#'\n#' @return Object with the same class as `x`, with the aggregation results\n#' assigned to `target`, using `strategy` if `target` is an existing assay in\n#' `x`.\n#'\n#' @seealso `data.table::[.data.table`, `BiocParallel::bplapply`\n#'\n#' @export\nsetMethod(\"endoaggregate\", signature(x=\"LongTable\"),\n        function(x, ..., assay, target=assay, by, subset=TRUE, nthread=1,\n        progress=TRUE, BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[[assay]][eval(i), ]\n    res <- aggregate2(\n        assay_,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n        moreArgs=moreArgs\n    )\n    if (target %in% assayNames(x)) {\n        res <- merge.data.table(x[[assay]], res, by=by)\n    }\n    x[[target]] <- res\n    x\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-drugSensitivitySig.R",
    "language": "R",
    "content": "#' Compute the correlation between a molecular feature and treatment response\n#'\n#' @param object An object inheriting form the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A 3D array of genes x drugs x metric\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"drugSensitivitySig\", function(object, ...)\n    standardGeneric(\"drugSensitivitySig\"))",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drugSensitivitySig` function based on the given code snippet?",
        "answer": "The `drugSensitivitySig` function is designed to compute the correlation between a molecular feature and treatment response. It is defined as a generic function using `setGeneric`, which allows for method dispatch based on the class of the input object."
      },
      {
        "question": "What is the significance of the `@export` and `@keywords internal` annotations in the code?",
        "answer": "The `@export` annotation indicates that this function should be made available to users of the package, while `@keywords internal` suggests that it is primarily intended for internal use within the package. This combination might seem contradictory, but it could mean that the function is exported for advanced users or other packages to use, while not being part of the main public API."
      },
      {
        "question": "What does the `...` argument in the function definition represent, and why might it be useful?",
        "answer": "The `...` (ellipsis) argument in the function definition allows for the passing of additional arguments to the function. This is useful for creating a flexible interface that can accommodate future extensions or method-specific parameters without changing the generic function signature. It's particularly valuable in generic functions where different methods might require different parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Compute the correlation between a molecular feature and treatment response\n#'\n#' @param object An object inheriting form the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A 3D array of genes x drugs x metric\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"drugSensitivitySig\", function(object, ...)\n    # Complete the function body here\n)",
        "complete": "#' Compute the correlation between a molecular feature and treatment response\n#'\n#' @param object An object inheriting form the `CoreGx::CoreSet` class\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return A 3D array of genes x drugs x metric\n#'\n#' @export\n#' @keywords internal\nsetGeneric(\"drugSensitivitySig\", function(object, ...)\n    standardGeneric(\"drugSensitivitySig\"))"
      },
      {
        "partial": "# Create a generic function for drug sensitivity signature\n# Complete the function definition\nsetGeneric(\"drugSensitivitySig\", ...)",
        "complete": "# Create a generic function for drug sensitivity signature\nsetGeneric(\"drugSensitivitySig\", function(object, ...)\n    standardGeneric(\"drugSensitivitySig\"))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/connectivityScore.R",
    "language": "R",
    "content": "#' Function computing connectivity scores between two signatures\n#' \n#' A function for finding the connectivity between two signatures, using either\n#' the GSEA method based on the KS statistic, or the gwc method based on a \n#' weighted spearman statistic. The GSEA analysis is implemented in the piano package. \n#' \n#' @references \n#'    F. Pozzi, T. Di Matteo, T. Aste, 'Exponential smoothing weighted\n#'    correlations', The European Physical Journal B, Vol. 85, No 6, 2012. DOI:\n#'    10.1140/epjb/e2012-20697-x\n#' @references\n#'    Varemo, L., Nielsen, J. and Nookaew, I. (2013) Enriching the gene set\n#'    analysis of genome-wide data by incorporating directionality of gene\n#'    expression and combining statistical hypotheses and methods. Nucleic\n#'    Acids Research. 41 (8), 4378-4391. doi: 10.1093/nar/gkt111\n#'    \n#' @examples\n#' xValue <- c(1,5,23,4,8,9,2,19,11,12,13)\n#' xSig <- c(0.01, 0.001, .97, 0.01,0.01,0.28,0.7,0.01,0.01,0.01,0.01)\n#' yValue <- c(1,5,10,4,8,19,22,19,11,12,13)\n#' ySig <- c(0.01, 0.001, .97,0.01, 0.01,0.78,0.9,0.01,0.01,0.01,0.01)\n#' xx <- cbind(xValue, xSig)\n#' yy <- cbind(yValue, ySig)\n#' rownames(xx) <- rownames(yy) <- c('1','2','3','4','5','6','7','8','9','10','11')\n#' data.cor <- connectivityScore(xx,yy,method='gwc', gwc.method='spearman', nperm=300)\n#' \n#' @param x A \\code{matrix} with the first gene signature. In the case of GSEA the vector of\n#'   values per gene for GSEA in which we are looking for an enrichment. In the \n#'   case of gwc, this should be a matrix, with the per gene responses in the \n#'   first column, and the significance values in the second.\n#' @param y A \\code{matrix} with the second signature. In the case of GSEA, this is the\n#'   vector of up and down regulated genes we are looking for in our signature,\n#'   with the direction being determined from the sign. In the case of gwc, this\n#'   should be a matrix of identical size to x, once again with the per gene\n#'   responses in the first column, and their significance in the second.\n#' @param method \\code{character} string identifying which method to use, out of 'fgsea' and 'gwc'\n#' @param nperm \\code{numeric}, how many permutations should be done to determine\n#'   significance through permutation testing? The minimum is 100, default is\n#'   1e4.\n#' @param nthread \\code{numeric}, how many cores to run parallel processing on.\n#' @param gwc.method \\code{character}, should gwc use a weighted spearman or pearson\n#'   statistic?\n#' @param ... Additional arguments passed down to gsea and gwc functions\n#' \n#' @return \\code{numeric} a numeric vector with the score and the p-value associated\n#'   with it\n#' \n#' @export\n#' \n#' @importFrom piano runGSA loadGSC\n#' @importFrom stats complete.cases\n## TODO:: Implement 'gsea' method for this function\nconnectivityScore <- function(x, y, method = c(\"fgsea\", \"gwc\"), nperm = 10000, nthread = 1, gwc.method = c(\"spearman\", \"pearson\"), ...) {\n    \n    if (method == \"gsea\") {\n        stop(\"The gsea method is implemented using fgsea, please change your\n             method argument to 'fgsea'. Consult ?connectivityScore for\n             more information\")\n    }\n    \n    method <- match.arg(method)\n    gwc.method <- match.arg(gwc.method)\n    if (!is.matrix(x)) {\n        x <- as.matrix(x)\n    }\n    if (!is.matrix(y)) {\n        y <- as.matrix(y)\n    }\n    if ((ncol(x) != 2 || ncol(y) != 2) && method == \"gwc\") {\n        stop(\"x and y should have 2 columns: effect size and corresponding p-values\")\n    }\n    if (method == \"fgsea\" && nrow(y) >= nrow(x)) {\n        warning(\"GSEA method: query gene set (y) larger than signature (x)\")\n    }\n    \n    if (is.null(rownames(x)) || is.null(rownames(y)) || !length(intersect(rownames(x), rownames(y)))) {\n        stop(\"Row names of x and y are either missing or have no intersection\")\n    }\n    if (nperm < 100) {\n        stop(\"The minimum number of permutations for permutation testing is 100\")\n    }\n    switch(method, fgsea = {\n        ## remove missing values\n        y <- y[!is.na(y[, 1]), , drop = FALSE]\n        x <- x[!is.na(x[, 1]), , drop = FALSE]\n        ## create gene set\n        gset <- cbind(gene = rownames(y), set = ifelse(as.numeric(y[, 1]) >= 0, \"UP\", \"DOWN\"))\n        gset <- piano::loadGSC(gset)\n        \n        ## run enrichment analysis\n        ##FIXME:: Update runGSA to use fgseaMultilevel to stop warnings\n        suppressWarnings({ \n            nes <- piano::runGSA(geneLevelStats = x[, 1], geneSetStat = \"fgsea\", \n                                 gsc = gset, nPerm = nperm + (nperm%%nthread), \n                                 ncpus = nthread, \n            verbose = FALSE, adjMethod = \"none\", ...) \n        })\n        \n        ## merge p-values for negative and positive enrichment scores\n        nes$pDistinctDir <- nes$pDistinctDirUp\n        nes$pDistinctDir[is.na(nes$pDistinctDirUp), 1] <- nes$pDistinctDirDn[is.na(nes$pDistinctDirUp), 1]\n        nes.up <- c(nes$statDistinctDir[which(names(nes$gsc) == \"UP\"), 1], nes$pDistinctDir[which(names(nes$gsc) == \"UP\"), 1])\n        nes.down <- c(nes$statDistinctDir[which(names(nes$gsc) == \"DOWN\"), 1], nes$pDistinctDir[which(names(nes$gsc) == \"DOWN\"), 1])\n        ## combine UP and DOWN\n        if (length(nes.up) == 0) {\n            score = c(es = -nes.down[1], p = nes.down[2])\n        } else if (length(nes.down) == 0) {\n            score = c(es = nes.up[1], p = nes.up[2])\n        } else if (all(complete.cases(cbind(nes.up, nes.down))) && sign(nes.up[1]) != sign(nes.down[1])) {\n            score <- c(es = (nes.up[1] - nes.down[1])/2, p = .combineTest(p = c(nes.up[2], nes.down[2]), method = \"fisher\", na.rm = TRUE))\n        } else {\n            score <- c(score = 0, p = 1)\n        }\n    }, gwc = {\n        ## intersection between x and y\n        ii <- intersect(rownames(x), rownames(y))\n        if (length(ii) < 10) {\n            stop(\"Less than 10 probes/genes in common between x and y\")\n        }\n        score <- gwc(x1 = x[ii, 1], p1 = x[ii, 2], x2 = y[ii, 1], p2 = y[ii, 2], method.cor = gwc.method, nperm = nperm, ...)\n        names(score) <- c(\"score\", \"p\")\n    })\n    return(score)\n}\n\n#' @importFrom stats pchisq qnorm pnorm pt\n.combineTest <- function(p, weight, method = c(\"fisher\", \"z.transform\", \"logit\"), hetero = FALSE, na.rm = FALSE) {\n    if (hetero) {\n        stop(\"function to deal with heterogeneity is not implemented yet!\")\n    }\n    method <- match.arg(method)\n    na.ix <- is.na(p)\n    if (any(na.ix) && !na.rm) {\n        stop(\"missing values are present!\")\n    }\n    if (all(na.ix)) \n        {\n            return(NA)\n        }  ## all p-values are missing\n    p <- p[!na.ix]\n    k <- length(p)\n    if (k == 1) {\n        return(p)\n    }\n    if (missing(weight)) {\n        weight <- rep(1, k)\n    }\n    switch(method, fisher = {\n        cp <- pchisq(-2 * sum(log(p)), df = 2 * k, lower.tail = FALSE)\n    }, z.transform = {\n        z <- qnorm(p, lower.tail = FALSE)\n        cp <- pnorm(sum(weight * z)/sqrt(sum(weight^2)), lower.tail = FALSE)\n    }, logit = {\n        tt <- (-sum(log(p/(1 - p))))/sqrt(k * pi^2 * (5 * k + 2)/(3 * (5 * k + 4)))\n        cp <- pt(tt, df = 5 * k + 4, lower.tail = FALSE)\n    })\n    return(cp)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `connectivityScore` function and what are its main methods?",
        "answer": "The `connectivityScore` function computes connectivity scores between two gene signatures. It has two main methods: 'fgsea' (Fast Gene Set Enrichment Analysis) and 'gwc' (Gene-Wise Correlation). The 'fgsea' method is based on the GSEA approach using the KS statistic, while the 'gwc' method uses a weighted Spearman or Pearson correlation statistic."
      },
      {
        "question": "How does the function handle missing values and what is the minimum number of permutations allowed?",
        "answer": "The function removes missing values from the input matrices x and y before processing. For the 'fgsea' method, it explicitly removes NA values from both x and y. The function also checks if there are at least 10 common probes/genes between x and y for the 'gwc' method. The minimum number of permutations allowed is 100, as specified in the error check: `if (nperm < 100) { stop(\"The minimum number of permutations for permutation testing is 100\") }`."
      },
      {
        "question": "What is the purpose of the `.combineTest` helper function and what methods does it support for combining p-values?",
        "answer": "The `.combineTest` helper function is used to combine multiple p-values into a single p-value. It supports three methods for combining p-values: 'fisher' (Fisher's method), 'z.transform' (Z-transform method), and 'logit' (Logit method). The function is used in the 'fgsea' method of the `connectivityScore` function to combine p-values for negative and positive enrichment scores when necessary."
      }
    ],
    "completion_tasks": [
      {
        "partial": "connectivityScore <- function(x, y, method = c(\"fgsea\", \"gwc\"), nperm = 10000, nthread = 1, gwc.method = c(\"spearman\", \"pearson\"), ...) {\n    method <- match.arg(method)\n    gwc.method <- match.arg(gwc.method)\n    if (!is.matrix(x)) x <- as.matrix(x)\n    if (!is.matrix(y)) y <- as.matrix(y)\n    if ((ncol(x) != 2 || ncol(y) != 2) && method == \"gwc\") {\n        stop(\"x and y should have 2 columns: effect size and corresponding p-values\")\n    }\n    if (method == \"fgsea\" && nrow(y) >= nrow(x)) {\n        warning(\"GSEA method: query gene set (y) larger than signature (x)\")\n    }\n    if (is.null(rownames(x)) || is.null(rownames(y)) || !length(intersect(rownames(x), rownames(y)))) {\n        stop(\"Row names of x and y are either missing or have no intersection\")\n    }\n    if (nperm < 100) stop(\"The minimum number of permutations for permutation testing is 100\")\n    \n    # Complete the function implementation here\n}",
        "complete": "connectivityScore <- function(x, y, method = c(\"fgsea\", \"gwc\"), nperm = 10000, nthread = 1, gwc.method = c(\"spearman\", \"pearson\"), ...) {\n    method <- match.arg(method)\n    gwc.method <- match.arg(gwc.method)\n    if (!is.matrix(x)) x <- as.matrix(x)\n    if (!is.matrix(y)) y <- as.matrix(y)\n    if ((ncol(x) != 2 || ncol(y) != 2) && method == \"gwc\") {\n        stop(\"x and y should have 2 columns: effect size and corresponding p-values\")\n    }\n    if (method == \"fgsea\" && nrow(y) >= nrow(x)) {\n        warning(\"GSEA method: query gene set (y) larger than signature (x)\")\n    }\n    if (is.null(rownames(x)) || is.null(rownames(y)) || !length(intersect(rownames(x), rownames(y)))) {\n        stop(\"Row names of x and y are either missing or have no intersection\")\n    }\n    if (nperm < 100) stop(\"The minimum number of permutations for permutation testing is 100\")\n    \n    switch(method,\n        fgsea = {\n            y <- y[!is.na(y[, 1]), , drop = FALSE]\n            x <- x[!is.na(x[, 1]), , drop = FALSE]\n            gset <- cbind(gene = rownames(y), set = ifelse(as.numeric(y[, 1]) >= 0, \"UP\", \"DOWN\"))\n            gset <- piano::loadGSC(gset)\n            suppressWarnings({\n                nes <- piano::runGSA(geneLevelStats = x[, 1], geneSetStat = \"fgsea\", \n                                    gsc = gset, nPerm = nperm + (nperm%%nthread), \n                                    ncpus = nthread, verbose = FALSE, adjMethod = \"none\", ...)\n            })\n            nes$pDistinctDir <- nes$pDistinctDirUp\n            nes$pDistinctDir[is.na(nes$pDistinctDirUp), 1] <- nes$pDistinctDirDn[is.na(nes$pDistinctDirUp), 1]\n            nes.up <- c(nes$statDistinctDir[which(names(nes$gsc) == \"UP\"), 1], nes$pDistinctDir[which(names(nes$gsc) == \"UP\"), 1])\n            nes.down <- c(nes$statDistinctDir[which(names(nes$gsc) == \"DOWN\"), 1], nes$pDistinctDir[which(names(nes$gsc) == \"DOWN\"), 1])\n            if (length(nes.up) == 0) {\n                score = c(es = -nes.down[1], p = nes.down[2])\n            } else if (length(nes.down) == 0) {\n                score = c(es = nes.up[1], p = nes.up[2])\n            } else if (all(complete.cases(cbind(nes.up, nes.down))) && sign(nes.up[1]) != sign(nes.down[1])) {\n                score <- c(es = (nes.up[1] - nes.down[1])/2, p = .combineTest(p = c(nes.up[2], nes.down[2]), method = \"fisher\", na.rm = TRUE))\n            } else {\n                score <- c(score = 0, p = 1)\n            }\n        },\n        gwc = {\n            ii <- intersect(rownames(x), rownames(y))\n            if (length(ii) < 10) stop(\"Less than 10 probes/genes in common between x and y\")\n            score <- gwc(x1 = x[ii, 1], p1 = x[ii, 2], x2 = y[ii, 1], p2 = y[ii, 2], method.cor = gwc.method, nperm = nperm, ...)\n            names(score) <- c(\"score\", \"p\")\n        }\n    )\n    return(score)\n}"
      },
      {
        "partial": ".combineTest <- function(p, weight, method = c(\"fisher\", \"z.transform\", \"logit\"), hetero = FALSE, na.rm = FALSE) {\n    if (hetero) stop(\"function to deal with heterogeneity is not implemented yet!\")\n    method <- match.arg(method)\n    na.ix <- is.na(p)\n    if (any(na.ix) && !na.rm) stop(\"missing values are present!\")\n    if (all(na.ix)) return(NA)\n    p <- p[!na.ix]\n    k <- length(p)\n    if (k == 1) return(p)\n    if (missing(weight)) weight <- rep(1, k)\n    \n    # Complete the function implementation here\n}",
        "complete": ".combineTest <- function(p, weight, method = c(\"fisher\", \"z.transform\", \"logit\"), hetero = FALSE, na.rm = FALSE) {\n    if (hetero) stop(\"function to deal with heterogeneity is not implemented yet!\")\n    method <- match.arg(method)\n    na.ix <- is.na(p)\n    if (any(na.ix) && !na.rm) stop(\"missing values are present!\")\n    if (all(na.ix)) return(NA)\n    p <- p[!na.ix]\n    k <- length(p)\n    if (k == 1) return(p)\n    if (missing(weight)) weight <- rep(1, k)\n    \n    switch(method,\n        fisher = {\n            cp <- pchisq(-2 * sum(log(p)), df = 2 * k, lower.tail = FALSE)\n        },\n        z.transform = {\n            z <- qnorm(p, lower.tail = FALSE)\n            cp <- pnorm(sum(weight * z)/sqrt(sum(weight^2)), lower.tail = FALSE)\n        },\n        logit = {\n            tt <- (-sum(log(p/(1 - p))))/sqrt(k * pi^2 * (5 * k + 2)/(3 * (5 * k + 4)))\n            cp <- pt(tt, df = 5 * k + 4, lower.tail = FALSE)\n        }\n    )\n    return(cp)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/datasets.R",
    "language": "R",
    "content": "#' Cleaveland_mut RadioSet subsetted\n#'\n#' Documentation for this dataset will be added at a later date. For now I just\n#' need this package to pass the CRAN checks! This dataset powers the exampe\n#' usage in the roxygen2 documentation for CoreGx.\n#'\n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect\n#'   small molecules, genes, and disease. Science, 2006.\n#'\n#' @docType data\n#' @name clevelandSmall\n#' @usage data(clevelandSmall)\n#' @keywords datasets\n#' @format RadioSet object\n#'\nNULL\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'NULL' at the end of this R code snippet?",
        "answer": "The 'NULL' at the end of the code snippet is used to create a placeholder for the dataset documentation. In R, when documenting datasets using roxygen2, it's common to use 'NULL' to indicate that no actual R object is being created. This allows the documentation to be processed without defining the dataset in the same file."
      },
      {
        "question": "What class of object is 'clevelandSmall' based on the documentation provided?",
        "answer": "Based on the documentation, 'clevelandSmall' is a RadioSet object. This is indicated by the '@format RadioSet object' line in the roxygen2 documentation."
      },
      {
        "question": "What roxygen2 tag is used to specify how the dataset should be loaded, and what is the correct usage for the 'clevelandSmall' dataset?",
        "answer": "The roxygen2 tag used to specify how the dataset should be loaded is '@usage'. For the 'clevelandSmall' dataset, the correct usage as specified in the documentation is 'data(clevelandSmall)'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Cleaveland_mut RadioSet subsetted\n#'\n#' Documentation for this dataset will be added at a later date. For now I just\n#' need this package to pass the CRAN checks! This dataset powers the exampe\n#' usage in the roxygen2 documentation for CoreGx.\n#'\n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect\n#'   small molecules, genes, and disease. Science, 2006.\n#'\n#' @docType data\n#' @name clevelandSmall\n#' @usage data(clevelandSmall)\n#' @keywords datasets\n#' @format RadioSet object\n#'\n# Complete the code below\n",
        "complete": "#' Cleaveland_mut RadioSet subsetted\n#'\n#' Documentation for this dataset will be added at a later date. For now I just\n#' need this package to pass the CRAN checks! This dataset powers the exampe\n#' usage in the roxygen2 documentation for CoreGx.\n#'\n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect\n#'   small molecules, genes, and disease. Science, 2006.\n#'\n#' @docType data\n#' @name clevelandSmall\n#' @usage data(clevelandSmall)\n#' @keywords datasets\n#' @format RadioSet object\n#'\nNULL"
      },
      {
        "partial": "#' @name clevelandSmall\n#' @usage data(clevelandSmall)\n#' @keywords datasets\n#' @format RadioSet object\n#'\n# Complete the roxygen2 documentation\n",
        "complete": "#' Cleaveland_mut RadioSet subsetted\n#'\n#' Documentation for this dataset will be added at a later date. For now I just\n#' need this package to pass the CRAN checks! This dataset powers the exampe\n#' usage in the roxygen2 documentation for CoreGx.\n#'\n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect\n#'   small molecules, genes, and disease. Science, 2006.\n#'\n#' @docType data\n#' @name clevelandSmall\n#' @usage data(clevelandSmall)\n#' @keywords datasets\n#' @format RadioSet object\n#'\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/doseResponseCurve.R",
    "language": "R",
    "content": "#' Plot drug response curve of a given drug and a given cell for a list of\n#'   rSets (objects of the RadioSet class).\n#'\n#' Given a list of RadioSets, the function will plot the drug_response curve,\n#'   for a given drug/cell pair. The y axis of the plot is the viability\n#'   percentage and x axis is the log transformed Ds. If more than one rSet is\n#'   provided, a light gray area would show the common concentration range\n#'   between rSets. User can ask for type of sensitivity measurment to be shown\n#'   in the plot legend. The user can also provide a list of their own Ds and\n#'   viability values, as in the examples below, and it will be treated as\n#'   experiments equivalent to values coming from a pset. The names of the\n#'   concentration list determine the legend labels.\n#'\n#' @examples\n#' doseResponseCurve(Ds=list(\"Experiment 1\" = c(0, 2, 4, 6)),\n#'   SFs=list(\"Experiment 1\" = c(1,.6,.4,.2)), plot.type=\"Both\")\n#'\n#' @param rad.type `character(1)` The type(s) of radiation dosage to be\n#'   plotted. If the plot is desirable for more than one radioset, A unique drug\n#'   id should be provided.\n#' @param cellline `character(1)` A cell line name for which the radiation response\n#'   curve should be plotted. If the plot is desirable for more than one\n#'   radioset, a unique cell id should be provided.\n#' @param rSets `list` a list of RadioSet objects, for which the function\n#'   should plot the curves.\n#' @param Ds,SFs `list` A list of Doses and SFs to plot, the function assumes\n#'   that Ds[[i]] is plotted against SFs[[i]]. The names of the D list are used\n#'   to create the legend labels\n#' @param legends.label `numeric` A vector of sensitivity measurment types which\n#'   could be any combination of  ic50_published, auc_published, auc_recomputed\n#'   and auc_recomputed_star. A legend will be displayed on the top right of the\n#'   plot which each line of the legend is the values of requested sensitivity\n#'   measerments for one of the requested rSets. If this parameter is missed no\n#'   legend would be provided for the plot.\n#' @param ylim `numeric` A vector of two numerical values to be used as ylim of\n#'   the plot. If this parameter would be missed c(0,100) would be used as the\n#'   ylim of the plot.\n#' @param xlim `numeric` A vector of two numerical values to be used as xlim of\n#'   the plot. If this parameter would be missed the minimum and maximum\n#'   concentrations between all the rSets would be used as plot xlim.\n#' @param mycol `numeric` A vector with the same lenght of the rSets parameter\n#'   which will determine the color of the curve for the pharmaco sets. If this\n#'   parameter is missed default colors from Rcolorbrewer package will be used\n#'   as curves color.\n#' @param plot.type `character` Plot type which can be the actual one (\"Actual\")\n#'   or the one fitted by logl logistic regression (\"Fitted\") or both of them\n#'   (\"Both\"). If this parameter is missed by default actual curve is plotted.\n#' @param summarize.replicates `character` If this parameter is set to true\n#'   replicates are summarized and replicates are plotted individually otherwise\n#' @param title `character` The title of the graph. If no title is provided,\n#'   then it defaults to Drug':'Cell Line'.\n#' @param lwd `numeric` The line width to plot with\n#' @param cex `numeric` The cex parameter passed to plot\n#' @param cex.main `numeric` The cex.main parameter passed to plot, controls the\n#'   size of the titles\n#' @param legend.loc And argument passable to xy.coords for the position to\n#'   place the legend.\n#' @param trunc `logical(1)` Should the viability values be truncated to lie in\n#'   [0-100] before doing the fitting\n#' @param verbose `logical(1)` Should warning messages about the data passed in be\n#'   printed?\n#'\n#' @return Plots to the active graphics device and returns and invisible NULL.\n#'\n#' @import RColorBrewer\n#' @importFrom graphics plot rect axis points lines legend\n#' @importFrom grDevices rgb\n#' @importFrom magicaxis magaxis\n#' @importFrom matrixStats colMedians colMeans2\n#'\n#' @export\ndoseResponseCurve <-\nfunction(rad.type = \"radiation\",\n         cellline,\n         rSets=list(),\n         Ds=list(),\n         SFs=list(),\n         trunc=TRUE,\n         legends.label = c(\"alpha\", \"beta\",\"rsquared\"),\n         ylim=c(0,100),\n         xlim, mycol,\n         title,\n         plot.type=c(\"Fitted\",\"Actual\", \"Both\"),\n         summarize.replicates=TRUE,\n         lwd = 1,\n         cex = 0.7,\n         cex.main = 0.9,\n         legend.loc = \"topright\",\n         verbose=TRUE)\n{\n  if(!missing(rSets)){\n    if (!is(rSets, \"list\")) {\n      if (!is(rSets, \"RadioSet\")) {\n        temp <- name(rSets)\n        rSets <- list(rSets)\n        names(rSets) <- temp\n      } else {\n        stop(\"Type of rSets parameter should be either a rSet or a list of rSets.\")\n      }\n    }\n  }\n  if(!all(legends.label %in% c(\"alpha\", \"beta\",\"rsquared\"))){\n    stop(paste(\"Only\", paste(c(\"'alpha'\", \"'beta'\",\"'rsquared'\"), collapse = \", \"), \"implemented for legend labels.\", split = \" \"))\n  }\n  ##FIXME::\n  if(!missing(rSets) && (missing(cellline))){\n    stop(\"If you pass in a rSet then drug and cellline must be set\") }\n\n    if(!missing(Ds)){\n      if(missing(SFs)){\n\n        stop(\"Please pass in the Survival Fractions to Plot with the Doses.\")\n\n      }\n      if (!is(Ds, \"list\")) {\n        if (mode(Ds) == \"numeric\") {\n          if(mode(SFs) != \"numeric\"){\n            stop(\"Passed in 1 vector of Doses but the Survival Fractions are not numeric!\")\n          }\n          Ds <- list(Ds)\n          SFs <- list(SFs)\n          names(Ds) <- \"Exp1\"\n          names(SFs) <- \"Exp1\"\n        } else {\n          stop(\"Mode of Doses parameter should be either numeric or a list of numeric vectors\")\n        }\n      } else{\n        if(length(SFs)!= length(Ds)){\n          stop(\"The number of D and SF vectors passed in differs\")\n        }\n        if(is.null(names(Ds))){\n          names(Ds) <- paste(\"Exp\", seq_along(Ds))\n        }\n        for(i in seq_along(Ds)){\n\n          if (mode(Ds[[i]]) == \"numeric\") {\n            if(mode(SFs[[i]])!=\"numeric\"){\n              stop(sprintf(\"Ds[[%d]] are numeric but the SFs[[%d]] are not numeric!\",i,i))\n            }\n          } else {\n            stop(sprintf(\"Mode of Ds[[%d]] parameter should be numeric\",i))\n          }\n        }\n\n      }\n    }\n\n    common.range.star <- FALSE\n\n    if (missing(plot.type)) {\n      plot.type <- \"Actual\"\n    }\n\n    doses <- list(); responses <- list(); legend.values <- list(); j <- 0; rSetNames <- list()\n    if(!missing(rSets)){\n      for(i in seq_along(rSets)) {\n        exp_i <- which(sensitivityInfo(rSets[[i]])[ ,\"sampleid\"] == cellline & sensitivityInfo(rSets[[i]])[ ,\"treatmentid\"] == rad.type)\n        if(length(exp_i) > 0) {\n          if (summarize.replicates) {\n            rSetNames[[i]] <- name(rSets[[i]])\n            if (length(exp_i) == 1) {\n              drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(rSets[[i]])[exp_i, , \"Dose\"])),\n                \"Viability\" = as.numeric(as.vector(sensitivityRaw(rSets[[i]])[exp_i, , \"Viability\"])), stringsAsFactors=FALSE))\n              drug.responses <- drug.responses[complete.cases(drug.responses), ]\n            }else{\n              drug.responses <- as.data.frame(cbind(\"Dose\"=colMedians(sensitivityRaw(rSets[[i]])[exp_i, , \"Dose\"], na.rm=TRUE),\n                \"Viability\"=colMedians(sensitivityRaw(rSets[[i]])[exp_i, , \"Viability\"], na.rm=TRUE)))\n              drug.responses <- drug.responses[complete.cases(drug.responses), ]\n            }\n            doses[[i]] <- drug.responses$Dose\n            responses[[i]] <- drug.responses$Viability\n            names(doses[[i]]) <- names(responses[[i]]) <- seq_along(doses[[i]])\n            if (!missing(legends.label)) {\n              if(length(legends.label)>0) {\n                linQuad_params <- linearQuadraticModel(D = doses[[i]], SF = responses[[i]])\n                if(any(grepl(\"alpha\", x=legends.label))){\n                  legend.values[[i]] <- paste(legend.values[i][[1]],sprintf(\"%s = %s\", \"alpha\", round(linQuad_params[1], digits=2)), sep=\", \")\n                }\n                if(any(grepl(\"beta\", x=legends.label))){\n                  legend.values[[i]] <- paste(legend.values[i][[1]],sprintf(\"%s = %s\", \"beta\", round(linQuad_params[2], digits=2)), sep=\", \")\n                }\n                if(any(grepl(\"rsquared\", x=legends.label))){\n                  legend.values[[i]] <- paste(legend.values[i][[1]],sprintf(\"%s = %s\", \"R^2\", round(CoreGx::.examineGOF(linQuad_params)[1], digits=2)), sep=\", \")\n                }\n              } else {\n                legend.values[[i]] <- \"\"\n              }\n            }\n          } else {\n            for (exp in exp_i) {\n              j <- j + 1\n              rSetNames[[j]] <- name(rSets[[i]])\n\n              drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(rSets[[i]])[exp, , \"Dose\"])),\n                \"Viability\"=as.numeric(as.vector(sensitivityRaw(rSets[[i]])[exp, , \"Viability\"])), stringsAsFactors=FALSE))\n              drug.responses <- drug.responses[complete.cases(drug.responses), ]\n              doses[[j]] <- drug.responses$Dose\n              responses[[j]] <- drug.responses$Viability\n              names(doses[[j]]) <- names(responses[[j]]) <- seq_along(doses[[j]])\n              if (!missing(legends.label)) {\n                if(length(legends.label)>0){\n                  linQuad_params <- linearQuadraticModel(D = doses2[[i]], SF = responses2[[i]])\n                  if(any(grepl(\"alpha\", x=legends.label))){\n                    legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"alpha\", round(linQuad_params[1], digits=2)), sep=\", \")\n                  }\n                  if(any(grepl(\"beta\", x=legends.label))){\n                    legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"beta\", round(linQuad_params[2], digits=2)), sep=\", \")\n                  }\n                  if(any(grepl(\"rsquared\", x=legends.label))){\n                    legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"R^2\", round(CoreGx::.examineGOF(linQuad_params)[1], digits=2)), sep=\", \")\n                  }\n                }\n              } else {\n                tt <- unlist(strsplit(rownames(sensitivityInfo(rSets[[i]]))[exp], split=\"_\"))\n                if (tt[1] == \"treatmentid\") {\n                  legend.values[[j]] <- tt[2]\n                }else{\n                  legend.values[[j]] <- rownames(sensitivityInfo(rSets[[i]]))[exp]\n                }\n              }\n            }\n          }\n        } else {\n          warning(\"The cell line and drug combo were not tested together. Aborting function.\")\n          return()\n        }\n      }\n    }\n    if(!missing(Ds)){\n      doses2 <- list(); responses2 <- list(); legend.values2 <- list(); j <- 0; rSetNames2 <- list();\n      for (i in seq_along(Ds)){\n        doses2[[i]] <- Ds[[i]]\n        responses2[[i]] <- SFs[[i]]\n        if(length(legends.label)>0){\n          linQuad_params <- linearQuadraticModel(D = doses2[[i]], SF = responses2[[i]])\n          if(any(grepl(\"alpha\", x=legends.label))){\n            legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"alpha\", round(linQuad_params[1], digits=2)), sep=\", \")\n          }\n          if(any(grepl(\"beta\", x=legends.label))){\n            legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"beta\", round(linQuad_params[2], digits=2)), sep=\", \")\n          }\n          if(any(grepl(\"rsquared\", x=legends.label))){\n            legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"R^2\", round(CoreGx::.examineGOF(linQuad_params)[1], digits=2)), sep=\", \")\n          }\n        } else{ legend.values2[[i]] <- \"\"}\n\n        rSetNames2[[i]] <- names(Ds)[[i]]\n      }\n      doses <- c(doses, doses2)\n      responses <- c(responses, responses2)\n      legend.values <- c(legend.values, legend.values2)\n      rSetNames <- c(rSetNames, rSetNames2)\n    }\n\n    if (missing(mycol)) {\n      mycol <- RColorBrewer::brewer.pal(n=7, name=\"Set1\")\n    }\n\n    dose.range <- c(10^100 , 0)\n    viability.range <- c(1 , 1)\n    for(i in seq_along(doses)) {\n      dose.range <- c(0, max(dose.range[2], max(doses[[i]], na.rm=TRUE), na.rm=TRUE))\n      viability.range <- c(min(viability.range[1], min(responses[[i]], na.rm=TRUE), na.rm=TRUE), 1)\n    }\n    x1 <- 10 ^ 10; x2 <- 0\n\n    if (!missing(xlim)) {\n      dose.range <- xlim\n    }\n    if (!missing(ylim)) {\n      viability.range <- ylim\n    }\n    if(missing(title)){\n      if (length(rSets)){\n        title <- sprintf(\"Radiation Response Curve for: %s\", cellline)\n      } else {\n        title <- \"Radiation Response Curve\"\n      }\n    }\n    plot(NA, xlab=\"Dose (Gy)\", ylab=\"Survival Fraction\", axes =FALSE, main=title, log=\"y\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n    magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,5), minorn=c(5,3), label=c(TRUE,FALSE))\n    if(max(viability.range)/min(viability.range)<50){\n      ticks <- magicaxis::maglab(viability.range, exptext = TRUE)\n    } else {\n      ticks <- magicaxis::maglab(viability.range, exptext = TRUE, log=TRUE)\n    }\n    ticks$exp <- unlist(lapply(ticks$exp, function(x)\n      return(as.expression(bquote(10^ .(round(log10(eval(x)), 2)))))))\n    axis(2, at=ticks$labat,labels=ticks$exp)\n    legends <- NULL\n    legends.col <- NULL\n\n    for (i in seq_along(doses)) {\n      points(doses[[i]],responses[[i]],pch=20,col = mycol[i], cex=cex)\n\n      switch(plot.type , \"Actual\"={\n        lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i])\n      }, \"Fitted\"= {\n        linQuad_params <- linearQuadraticModel(D = doses[[i]], SF = responses[[i]])\n        x_vals <- CoreGx::.getSupportVec(c(0,doses[[i]]))\n        lines(x_vals, (.linearQuadratic(x_vals, pars=linQuad_params, SF_as_log=FALSE)),lty=1, lwd=lwd, col=mycol[i])\n      },\"Both\"={\n        linQuad_params <- linearQuadraticModel(D = doses[[i]], SF = responses[[i]])\n        x_vals <- CoreGx::.getSupportVec(c(0,doses[[i]]))\n        lines(x_vals, (.linearQuadratic(x_vals, pars=linQuad_params, SF_as_log=FALSE)),lty=1, lwd=lwd, col=mycol[i])\n      })\n      if (length(legend.values)){\n              legends<- c(legends, sprintf(\"%s%s\", rSetNames[[i]], legend.values[[i]]))\n      } else {\n                      legends<- c(legends, sprintf(\"%s\", rSetNames[[i]]))\n      }\n      legends.col <-  c(legends.col, mycol[i])\n    }\n\n    legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=c(15,15))\n    return(invisible(NULL))\n  }\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `doseResponseCurve` function and what are its main input parameters?",
        "answer": "The `doseResponseCurve` function is designed to plot drug response curves for given drug-cell line pairs across multiple RadioSet objects. Its main input parameters include `rad.type` (radiation type), `cellline` (cell line name), `rSets` (list of RadioSet objects), `Ds` and `SFs` (optional lists of doses and survival fractions), and various plotting options such as `legends.label`, `ylim`, `xlim`, and `plot.type`."
      },
      {
        "question": "How does the function handle replicates in the input data, and what option controls this behavior?",
        "answer": "The function can handle replicates in two ways, controlled by the `summarize.replicates` parameter. If set to TRUE (default), it summarizes replicates by taking the median of doses and viabilities. If FALSE, it plots each replicate individually. This is implemented in the code block that checks `if (summarize.replicates)` and processes the data accordingly."
      },
      {
        "question": "What types of plots can be generated by this function, and how are they specified?",
        "answer": "The function can generate three types of plots, specified by the `plot.type` parameter: 'Actual' (plots the actual data points and lines), 'Fitted' (plots a fitted curve using the linear quadratic model), or 'Both' (plots both actual and fitted curves). The plotting logic is implemented in a switch statement that handles each case separately, using the `linearQuadraticModel` function for fitting when required."
      }
    ],
    "completion_tasks": [
      {
        "partial": "doseResponseCurve <- function(rad.type = \"radiation\", cellline, rSets=list(), Ds=list(), SFs=list(), trunc=TRUE, legends.label = c(\"alpha\", \"beta\",\"rsquared\"), ylim=c(0,100), xlim, mycol, title, plot.type=c(\"Fitted\",\"Actual\", \"Both\"), summarize.replicates=TRUE, lwd = 1, cex = 0.7, cex.main = 0.9, legend.loc = \"topright\", verbose=TRUE) {\n  # Function implementation\n  # ...\n}",
        "complete": "doseResponseCurve <- function(rad.type = \"radiation\", cellline, rSets=list(), Ds=list(), SFs=list(), trunc=TRUE, legends.label = c(\"alpha\", \"beta\",\"rsquared\"), ylim=c(0,100), xlim, mycol, title, plot.type=c(\"Fitted\",\"Actual\", \"Both\"), summarize.replicates=TRUE, lwd = 1, cex = 0.7, cex.main = 0.9, legend.loc = \"topright\", verbose=TRUE) {\n  if(!missing(rSets)){\n    if (!is(rSets, \"list\")) {\n      if (!is(rSets, \"RadioSet\")) {\n        temp <- name(rSets)\n        rSets <- list(rSets)\n        names(rSets) <- temp\n      } else {\n        stop(\"Type of rSets parameter should be either a rSet or a list of rSets.\")\n      }\n    }\n  }\n  if(!all(legends.label %in% c(\"alpha\", \"beta\",\"rsquared\"))){\n    stop(paste(\"Only\", paste(c(\"'alpha'\", \"'beta'\",\"'rsquared'\"), collapse = \", \"), \"implemented for legend labels.\", split = \" \"))\n  }\n  if(!missing(rSets) && (missing(cellline))){\n    stop(\"If you pass in a rSet then drug and cellline must be set\")\n  }\n  if(!missing(Ds)){\n    if(missing(SFs)){\n      stop(\"Please pass in the Survival Fractions to Plot with the Doses.\")\n    }\n    if (!is(Ds, \"list\")) {\n      if (mode(Ds) == \"numeric\") {\n        if(mode(SFs) != \"numeric\"){\n          stop(\"Passed in 1 vector of Doses but the Survival Fractions are not numeric!\")\n        }\n        Ds <- list(Ds)\n        SFs <- list(SFs)\n        names(Ds) <- \"Exp1\"\n        names(SFs) <- \"Exp1\"\n      } else {\n        stop(\"Mode of Doses parameter should be either numeric or a list of numeric vectors\")\n      }\n    } else{\n      if(length(SFs)!= length(Ds)){\n        stop(\"The number of D and SF vectors passed in differs\")\n      }\n      if(is.null(names(Ds))){\n        names(Ds) <- paste(\"Exp\", seq_along(Ds))\n      }\n      for(i in seq_along(Ds)){\n        if (mode(Ds[[i]]) == \"numeric\") {\n          if(mode(SFs[[i]])!=\"numeric\"){\n            stop(sprintf(\"Ds[[%d]] are numeric but the SFs[[%d]] are not numeric!\",i,i))\n          }\n        } else {\n          stop(sprintf(\"Mode of Ds[[%d]] parameter should be numeric\",i))\n        }\n      }\n    }\n  }\n  # Rest of the function implementation\n  # ...\n}"
      },
      {
        "partial": "linearQuadraticModel <- function(D, SF) {\n  # Function implementation\n  # ...\n}",
        "complete": "linearQuadraticModel <- function(D, SF) {\n  if (length(D) != length(SF)) {\n    stop(\"Length of D and SF must be equal\")\n  }\n  if (any(SF <= 0)) {\n    warning(\"Some SF values are <= 0. These will be removed for fitting.\")\n    valid <- SF > 0\n    D <- D[valid]\n    SF <- SF[valid]\n  }\n  logSF <- log(SF)\n  model <- lm(logSF ~ D + I(D^2))\n  coef <- coef(model)\n  alpha <- -coef[2]\n  beta <- -coef[3]\n  return(c(alpha = alpha, beta = beta))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/TREDataMapper-class.R",
    "language": "R",
    "content": "#' @include DataMapper-class.R\n#' @include LongTableDataMapper-class.R\n#' @include TreatmentResponseExperiment-class.R\n#' @include LongTableDataMapper-accessors.R\nNULL\n\n#' A Class for Mapping Between Raw Data and an `TreatmentResponseExperiment`\n#'   Object\n#'\n#' @slot rawdata See Slots section.\n#' @slot rowDataMap See Slots section.\n#' @slot colDataMap See Slots section.\n#' @slot assayMap See Slots section.\n#' @slot metadataMap See Slots section.\n#'\n#' @inheritSection LongTableDataMapper-class Slots\n#'\n#' @md\n#' @aliases TREDataMapper-class\n.TREDataMapper <- setClass(\"TREDataMapper\", contains=c(\"LongTableDataMapper\"))\n\n#' Constructor for the `TREDataMapper` class, which maps from one or\n#'   more raw experimental data files to the slots of a `LongTable` object.\n#'\n#' @details\n#' The `guessMapping` method can be used to test hypotheses about the\n#' cardinality of one or more sets of identifier columns. This is helpful\n#' to determine the id columns for `rowDataMap` and `colDataMap`, as well\n#' as identify columns mapping to `assays` or `metadata`.\n#'\n#' To attach metadata not associated with `rawdata`, please use the `metadata`\n#' assignment method on your `TREDataMapper`. This metadata will be\n#' merge with any metadata from `metadataMap` and added to the `LongTable`\n#' which this object ultimately constructs.\n#'\n#' @param rawdata A `data.frame` of raw data from a treatment response\n#' experiment. This will be coerced to a `data.table` internally. We recommend\n#' using joins to aggregate your raw data if it is not present in a single file.\n#' @param rowDataMap A list-like object containing two `character` vectors.\n#' The first is column names in `rawdata` needed to uniquely identify each row,\n#' the second is additional columns which map to rows, but are not required to\n#' uniquely identify them. Rows should be treatments.\n#' @param colDataMap A list-like object containing two `character` vectors.\n#' The first is column names in `rawdata` needed to uniquely identify each\n#' column, the second is additional columns which map to rows, but are not\n#' required to uniquely identify them. Columns should be samples.\n#' @param assayMap A list-like where each item is a `list` with two `character`\n#' vectors defining an assay, the first containing the identifier columns in\n#' `rawdata` needed to uniquely identify each row an assay, and the second the\n#' `rawdata` columns to be mapped to that assay. The names of `assayMap`\n#' will be the names of the assays in the `TreatmentResponseExperiment` that\n#' is created when calling `metaConstruct` on this `DataMapper` object. If the\n#' character vectors have names, the value columns will be renamed accordingly.\n#' @param metadataMap A list-like where each item is a `character` vector of\n#' `rawdata` column names to assign to the `@metadata` of the `LongTable`,\n#' where the name of that assay is the name of the list item. If names are\n#' omitted, assays will be numbered by their index in the list.\n#'\n#' @return A `TREDataMapper` object, with columns mapped to it's slots according\n#' to the various maps in the `LongTableDataMapper` object.\n#'\n#' @seealso [`guessMapping`]\n#'\n#' @md\n#' @importFrom data.table setDT\n#' @export\nTREDataMapper <- function(rawdata=data.frame(),\n        rowDataMap=list(character(), character()),\n        colDataMap=list(character(), character()),\n        assayMap=list(list(character(), character())),\n        metadataMap=list(character())) {\n\n    if (is(rawdata, \"LongTableDataMapper\")) {\n        lt_dm <- rawdata\n    } else {\n        lt_dm <- LongTableDataMapper(rawdata=rawdata, rowDataMap=rowDataMap,\n            colDataMap=colDataMap, assayMap=assayMap, metadataMap=metadataMap)\n    }\n\n    .TREDataMapper(\n        rawdata=rawdata(lt_dm),\n        rowDataMap=rowDataMap(lt_dm),\n        colDataMap=colDataMap(lt_dm),\n        assayMap=assayMap(lt_dm),\n        metadataMap=metadata(lt_dm)\n    )\n}\n\n## ===========================================\n## TREDataMapper Accessors Documentation\n## -------------------------------------------\n\n.local_class_5 <- \"TREDataMapper\"\n.local_data_5 <- \"exampleDataMapper\"\n\n#' @name TREDataMapper-accessors\n#'\n#' @eval .docs_DataMapper_accessors(class_=.local_class_5)\n#' @eval .docs_DataMapper_get_rawdata(class_=.local_class_5)\n#'\n#' @param value See details.\nNULL\n\n\n## ---------------\n## -- rawdata slot\n\n\n#' @rdname TREDataMapper-accessors\n#' @eval .docs_DataMapper_set_rawdata(class_=.local_class_5,\n#' class1_='list')\nsetReplaceMethod(\"rawdata\", signature=c(object=\"TREDataMapper\",\n        value=\"list\"), function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n## --------------------\n## ---- rowDataMap slot\n\n\n##\n## -- rowDataMap\n\n#' @rdname TREDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_get_dimDataMap(dim_='row', class_=.local_class_5,\n#' data_=.local_data_5)\n#' @aliases rowDataMap\nsetMethod('rowDataMap', signature(object='TREDataMapper'), function(object) {\n    callNextMethod()\n})\n\n\n#' @rdname TREDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_set_dimDataMap(dim_='row', class_=.local_class_5,\n#' data_=.local_data_5, id_col_='treatmentid')\n#' @aliases rowDataMap<-\nsetReplaceMethod('rowDataMap', signature(object='TREDataMapper',\n        value='list_OR_List'), function(object, value) {\n    callNextMethod()\n})\n\n\n##\n## -- rowData\n\n\n#' Convenience method to subset the `rowData` out of the `rawdata` slot using\n#'   the assigned `rowDataMap` metadata.\n#'\n#' @param x `TREDataMapper` object with valid data in the `rawdata` and\n#'   `colDataMap` slots.\n#' @param key `logical(1)` Should the table be keyed according to the\n#'   `id_columns` of the `rowDataMap` slot? This will sort the table in memory.\n#'   Default is TRUE.\n#'\n#' @return `data.table` The `rowData` as specified in the `rowDataMap` slot.\n#'\n#' @export\nsetMethod(\"rowData\", signature(x=\"TREDataMapper\"), function(x, key=TRUE) {\n    callNextMethod()\n})\n\n\n## --------------------\n## ---- colDataMap slot\n\n\n##\n## -- colDataMap\n\n#' @rdname TREDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_get_dimDataMap(dim_='col', class_=.local_class_5,\n#' data_=.local_data_5)\n#' @aliases colDataMap\nsetMethod('colDataMap', signature(object='TREDataMapper'),\n        function(object) {\n    callNextMethod()\n})\n\n#' @rdname TREDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_set_dimDataMap(dim_='col', class_=.local_class_5,\n#' data_=.local_data_5, id_col_='sampleid')\n#' @aliases colDataMap<-\nsetReplaceMethod('colDataMap',\n        signature(object=\"TREDataMapper\", value=\"list_OR_List\"),\n        function(object, value) {\n    callNextMethod()\n})\n\n\n##\n## -- colData\n\n\n#' Convenience method to subset the `colData` out of the `rawdata` slot using\n#'   the assigned `colDataMap` metadata.\n#'\n#' @param x `TREDataMapper` object with valid data in the `rawdata` and\n#'   `colDataMap` slots.\n#' @param key `logical(1)` Should the table be keyed according to the\n#'   `id_columns` of the `colDataMap` slot? This will sort the table in memory.\n#'   Default is TRUE.\n#'\n#' @return `data.table` The `colData` as specified in the `colDataMap` slot.\n#'\n#' @export\nsetMethod(\"colData\", signature(x=\"TREDataMapper\"), function(x, key=TRUE) {\n    callNextMethod()\n})\n\n\n## ----------------\n## ---- assayMap slot\n\n#' @rdname TREDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_get_assayMap(class_=.local_class_5, data_=.local_data_5)\n#' @aliases assayMap\nsetMethod('assayMap', signature(object='TREDataMapper'),\n        function(object) {\n    callNextMethod()\n})\n\n\n#' @rdname TREDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_set_assayMap(class_=.local_class_5, data_=.local_data_5)\n#' @aliases assayMap<-\nsetReplaceMethod('assayMap', signature(object='TREDataMapper',\n        value='list_OR_List'), function(object, value) {\n    callNextMethod()\n})\n\n\n#' Extract the data for an assay from a `TREDataMapper`\n#'\n#' @param x `TREDataMapper` The object to retrive assay data form according\n#'   to the `assayMap` slot.\n#' @param i `character(1)` Name of an assay in the `assayMap` slot of `x`.\n#' @param withDimnames `logical(1)` For compatibility with\n#'   `SummarizedExperiment::assay` generic. Not used.\n#'\n#' @return `data.table` Data for the specified assay extracted from the\n#'   `rawdata` slot of `x`.\n#'\n#' @importFrom checkmate assertSubset assertCharacter\n#' @keywords internal\nsetMethod(\"assay\", signature(x=\"TREDataMapper\"),\n        function(x, i, withDimnames=TRUE) {\n    callNextMethod()\n})\n\n\n#' Extract the data for all assays from a `TREDataMapper`\n#'\n#' @param x `TREDataMapper` The object to retrive assay data form according\n#'   to the `assayMap` slot.\n#' @param withDimnames `logical(1)` For compatibility with\n#'   `SummarizedExperiment::assay` generic. Not used.\n#'\n#' @return `list` Data for all assays extracted from the\n#'   `rawdata` slot of `x` as a `list` of `data.tables`, where the `keys` for\n#'   each table are their `id_columns`.\n#'\n#' @importFrom checkmate assertSubset assertCharacter\n#' @keywords internal\nsetMethod(\"assays\", signature(x=\"TREDataMapper\"),\n        function(x, withDimnames=TRUE) {\n    callNextMethod()\n})\n\n# -- metadataMap\n\n#' @rdname TREDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_get_metadataMap(class_=.local_class_5, data_=.local_data_5)\n#' @aliases metadataMap\nsetMethod('metadataMap', signature(object='TREDataMapper'),\n        function(object) {\n    callNextMethod()\n})\n\n\n#' @rdname TREDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_set_metadataMap(class_=.local_class_5, data_=.local_data_5, col_='metadata')\n#' @aliases metadataMap<-\nsetReplaceMethod('metadataMap', signature(object='TREDataMapper',\n        value='list_OR_List'), function(object, value) {\n    callNextMethod()\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `TREDataMapper` class and how does it relate to the `LongTableDataMapper` class?",
        "answer": "The `TREDataMapper` class is designed for mapping between raw data and a `TreatmentResponseExperiment` object. It inherits from the `LongTableDataMapper` class, extending its functionality to specifically handle treatment response experiment data. The class provides a structured way to organize and map raw experimental data to the appropriate slots in a `LongTable` object, which can then be used to construct a `TreatmentResponseExperiment` object."
      },
      {
        "question": "How does the `TREDataMapper` constructor handle different types of input for the `rawdata` parameter?",
        "answer": "The `TREDataMapper` constructor can handle two types of input for the `rawdata` parameter: 1) If `rawdata` is a `data.frame`, it creates a new `LongTableDataMapper` object using the provided parameters. 2) If `rawdata` is already a `LongTableDataMapper` object, it uses that object directly. This flexibility allows users to either start with raw data or use a pre-existing `LongTableDataMapper` when creating a `TREDataMapper` object."
      },
      {
        "question": "What is the purpose of the `assay` and `assays` methods in the `TREDataMapper` class, and how do they differ?",
        "answer": "The `assay` method extracts data for a single specified assay from a `TREDataMapper` object, returning it as a `data.table`. It takes an argument `i` to specify which assay to extract. The `assays` method, on the other hand, extracts data for all assays defined in the `TREDataMapper` object, returning a list of `data.tables`. Both methods use the `assayMap` slot to determine how to extract the assay data from the `rawdata` slot. The `assay` method is useful for accessing a specific assay, while `assays` provides a comprehensive view of all assays in the object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "TREDataMapper <- function(rawdata=data.frame(),\n        rowDataMap=list(character(), character()),\n        colDataMap=list(character(), character()),\n        assayMap=list(list(character(), character())),\n        metadataMap=list(character())) {\n\n    if (is(rawdata, \"LongTableDataMapper\")) {\n        lt_dm <- rawdata\n    } else {\n        lt_dm <- LongTableDataMapper(rawdata=rawdata, rowDataMap=rowDataMap,\n            colDataMap=colDataMap, assayMap=assayMap, metadataMap=metadataMap)\n    }\n\n    # Complete the function by returning a new TREDataMapper object\n    # using the .TREDataMapper constructor\n}",
        "complete": "TREDataMapper <- function(rawdata=data.frame(),\n        rowDataMap=list(character(), character()),\n        colDataMap=list(character(), character()),\n        assayMap=list(list(character(), character())),\n        metadataMap=list(character())) {\n\n    if (is(rawdata, \"LongTableDataMapper\")) {\n        lt_dm <- rawdata\n    } else {\n        lt_dm <- LongTableDataMapper(rawdata=rawdata, rowDataMap=rowDataMap,\n            colDataMap=colDataMap, assayMap=assayMap, metadataMap=metadataMap)\n    }\n\n    .TREDataMapper(\n        rawdata=rawdata(lt_dm),\n        rowDataMap=rowDataMap(lt_dm),\n        colDataMap=colDataMap(lt_dm),\n        assayMap=assayMap(lt_dm),\n        metadataMap=metadata(lt_dm)\n    )"
      },
      {
        "partial": "setMethod(\"assay\", signature(x=\"TREDataMapper\"),\n        function(x, i, withDimnames=TRUE) {\n    # Implement the method to extract data for an assay\n    # from a TREDataMapper object\n})",
        "complete": "setMethod(\"assay\", signature(x=\"TREDataMapper\"),\n        function(x, i, withDimnames=TRUE) {\n    assertCharacter(i, len=1)\n    assertSubset(i, names(assayMap(x)))\n    am <- assayMap(x)[[i]]\n    dt <- rawdata(x)[, c(am$id_columns, am$value_columns), with=FALSE]\n    setkeyv(dt, am$id_columns)\n    dt\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-dim.R",
    "language": "R",
    "content": "# ==== LongTable\n\n#' Get the dimensions of a `LongTable` object.\n#'\n#' @examples\n#' dim(merckLongTable)\n#'\n#' @describeIn LongTable Get the number of row annotations by the number of\n#'   column annotations from a LongTable object. Please note that row x columns\n#'   does not necessarily equal the number of rows in an assay, since it is\n#'   not required for each assay to have every row or column present.\n#'\n#' @examples\n#' dim(merckLongTable)\n#'\n#' @param x A `LongTable` object to retrieve dimensions for.\n#'\n#' @return `numeric` Vector of object dimensions.\n#'\n#' @importMethodsFrom SummarizedExperiment dim\n#' @export\nsetMethod('dim', signature(x='LongTable'), function(x) {\n    return(c(nrow(rowData(x)), nrow(colData(x))))\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dim` method for a `LongTable` object?",
        "answer": "The `dim` method for a `LongTable` object returns the dimensions of the table, specifically the number of row annotations by the number of column annotations. It's important to note that this doesn't necessarily equal the number of rows in an assay, as not every assay is required to have every row or column present."
      },
      {
        "question": "How does the `dim` method calculate the dimensions of a `LongTable` object?",
        "answer": "The `dim` method calculates the dimensions by returning a vector with two elements: the number of rows in the `rowData` and the number of rows in the `colData` of the `LongTable` object. This is achieved using the `nrow` function on both `rowData(x)` and `colData(x)`."
      },
      {
        "question": "What is the significance of the `@importMethodsFrom SummarizedExperiment dim` annotation in this code?",
        "answer": "The `@importMethodsFrom SummarizedExperiment dim` annotation indicates that the `dim` method is being imported from the `SummarizedExperiment` package. This suggests that the `LongTable` class is likely extending or building upon functionality from the `SummarizedExperiment` package, which is commonly used in bioinformatics for representing and analyzing high-throughput genomic data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('dim', signature(x='LongTable'), function(x) {\n    # Complete the function to return the dimensions of a LongTable object\n})",
        "complete": "setMethod('dim', signature(x='LongTable'), function(x) {\n    return(c(nrow(rowData(x)), nrow(colData(x))))\n})"
      },
      {
        "partial": "setMethod('dim', signature(x='LongTable'), function(x) {\n    rows <- # Get the number of row annotations\n    cols <- # Get the number of column annotations\n    return(c(rows, cols))\n})",
        "complete": "setMethod('dim', signature(x='LongTable'), function(x) {\n    rows <- nrow(rowData(x))\n    cols <- nrow(colData(x))\n    return(c(rows, cols))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/linearQuadraticInv.R",
    "language": "R",
    "content": "# Calculate Linear Quadratic Inverse\n#\n# @return Estimated survival fraction for the model fit\n#\n.linearQuadraticInv <- function(SF, pars, SF_as_log = TRUE) {\n\n  if (!SF_as_log) {\n    if (SF < 0) {\n      stop(\"Survival fraction must be nonnegative.\")\n    } else {\n      SF <- log(SF)\n    }\n  }\n\n  if (SF > 0) {\n    stop(\"Positive log survival fraction \", SF,  \"cannot be reached at any dose\n         of radiation with linear quadratic paramaters alpha, beta > 0.\")\n  } else {\n    if (pars[[2]] == 0) {\n      if (pars[[1]] == 0) {\n        if (SF == 1) {\n          return(0)\n        } else {\n          stop(paste0(\"Survival fraction \", SF, \" cannot be reached at any dose\n                      of radiation with linear-quadratic parameters alpha = \",\n                      pars[[1]], \" and beta = \", pars[[2]], \".\"))\n        }\n      } else {\n        return(-SF / pars[[1]])\n      }\n    } else {\n      return((sqrt(pars[[1]] ^ 2 - 4 * pars[[2]] * SF) - pars[[1]]) / 2 / pars[[2]])\n    }\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `linearQuadraticInv` function and what are its input parameters?",
        "answer": "The `linearQuadraticInv` function calculates the estimated survival fraction for a linear quadratic model fit. It takes three input parameters: `SF` (survival fraction), `pars` (a list containing alpha and beta parameters), and `SF_as_log` (a boolean indicating whether SF is provided as a log value, defaulting to TRUE)."
      },
      {
        "question": "How does the function handle different cases of alpha and beta parameters?",
        "answer": "The function handles different cases as follows:\n1. If beta (pars[[2]]) is 0 and alpha (pars[[1]]) is 0, it returns 0 if SF is 1, otherwise it throws an error.\n2. If beta is 0 but alpha is not 0, it returns -SF / alpha.\n3. If both alpha and beta are non-zero, it uses the quadratic formula to calculate the result: (sqrt(alpha^2 - 4*beta*SF) - alpha) / (2*beta)."
      },
      {
        "question": "What error checks does the function perform on the input parameters?",
        "answer": "The function performs the following error checks:\n1. If SF is not provided as a log value (SF_as_log is FALSE), it checks if SF is negative and throws an error if so.\n2. It checks if the log survival fraction (SF) is positive and throws an error if it is, as this cannot be reached with positive alpha and beta.\n3. It checks for cases where the given survival fraction cannot be reached with the provided alpha and beta parameters and throws appropriate error messages."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".linearQuadraticInv <- function(SF, pars, SF_as_log = TRUE) {\n  if (!SF_as_log) {\n    if (SF < 0) {\n      stop(\"Survival fraction must be nonnegative.\")\n    } else {\n      SF <- log(SF)\n    }\n  }\n\n  if (SF > 0) {\n    stop(\"Positive log survival fraction \", SF,  \"cannot be reached at any dose\n         of radiation with linear quadratic paramaters alpha, beta > 0.\")\n  } else {\n    if (pars[[2]] == 0) {\n      if (pars[[1]] == 0) {\n        # Complete this section\n      } else {\n        return(-SF / pars[[1]])\n      }\n    } else {\n      return((sqrt(pars[[1]] ^ 2 - 4 * pars[[2]] * SF) - pars[[1]]) / 2 / pars[[2]])\n    }\n  }\n}",
        "complete": ".linearQuadraticInv <- function(SF, pars, SF_as_log = TRUE) {\n  if (!SF_as_log) {\n    if (SF < 0) {\n      stop(\"Survival fraction must be nonnegative.\")\n    } else {\n      SF <- log(SF)\n    }\n  }\n\n  if (SF > 0) {\n    stop(\"Positive log survival fraction \", SF,  \"cannot be reached at any dose\n         of radiation with linear quadratic paramaters alpha, beta > 0.\")\n  } else {\n    if (pars[[2]] == 0) {\n      if (pars[[1]] == 0) {\n        if (SF == 0) return(0)\n        stop(paste0(\"Survival fraction \", SF, \" cannot be reached at any dose\n                    of radiation with linear-quadratic parameters alpha = \",\n                    pars[[1]], \" and beta = \", pars[[2]], \".\"))\n      } else {\n        return(-SF / pars[[1]])\n      }\n    } else {\n      return((sqrt(pars[[1]] ^ 2 - 4 * pars[[2]] * SF) - pars[[1]]) / 2 / pars[[2]])\n    }\n  }\n}"
      },
      {
        "partial": ".linearQuadraticInv <- function(SF, pars, SF_as_log = TRUE) {\n  if (!SF_as_log) {\n    if (SF < 0) stop(\"Survival fraction must be nonnegative.\")\n    SF <- log(SF)\n  }\n\n  if (SF > 0) {\n    stop(\"Positive log survival fraction \", SF,  \"cannot be reached at any dose\n         of radiation with linear quadratic paramaters alpha, beta > 0.\")\n  } else {\n    # Complete the rest of the function\n  }\n}",
        "complete": ".linearQuadraticInv <- function(SF, pars, SF_as_log = TRUE) {\n  if (!SF_as_log) {\n    if (SF < 0) stop(\"Survival fraction must be nonnegative.\")\n    SF <- log(SF)\n  }\n\n  if (SF > 0) {\n    stop(\"Positive log survival fraction \", SF,  \"cannot be reached at any dose\n         of radiation with linear quadratic paramaters alpha, beta > 0.\")\n  } else {\n    if (pars[[2]] == 0) {\n      if (pars[[1]] == 0) {\n        if (SF == 0) return(0)\n        stop(paste0(\"Survival fraction \", SF, \" cannot be reached at any dose\n                    of radiation with linear-quadratic parameters alpha = \",\n                    pars[[1]], \" and beta = \", pars[[2]], \".\"))\n      }\n      return(-SF / pars[[1]])\n    }\n    return((sqrt(pars[[1]] ^ 2 - 4 * pars[[2]] * SF) - pars[[1]]) / 2 / pars[[2]])\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/computeAUC.R",
    "language": "R",
    "content": "#' computeAUC: computes AUC\n#'\n#' @description This function computes the area under a dose-response curve of\n#'   the form survival fraction SF = exp(-alpha * D - beta * D ^ 2).\n#'\n#' @examples\n#' computeAUC(D=c(0.1, 0.5, 0.7, 0.9), pars=c(0.2, 0.1), lower = 0,\n#'  upper = 1) # Returns 0.7039296\n#'\n#' @param D vector of dosages\n#' @param SF vector of survival fractions\n#' @param pars parameters (alpha, beta) in equation\n#'   y = exp(-alpha * x - beta * x ^ 2)\n#' @param lower lower bound of dose region to compute AUC over\n#' @param upper upper bound of dose region to compute AUC over\n#' @param trunc should survival fractions be truncated downward to 1 if they\n#'   exceed 1?\n#' @param SF_as_log A boolean indicating whether survival fraction is displayed\n#'   on a log axis. Defaults to FALSE\n#' @param area.type should the AUC of the raw (D, SF) points be returned, or\n#'   should the AUC of a curve fit to said points be returned instead?\n#' @param verbose how detailed should error and warning messages be?\n#'   See details.\n#'\n#' @return \\code{numeric} The area under the ROC curve\n#'\n#' @details If lower and/or upper are missing, the function assumes their values\n#'   to be the minimum and maximum D-values, respectively. For all warnings to\n#'   be silent, set trunc = FALSE. For warnings to be output, set trunc = TRUE.\n#'   For warnings to be output along with the arguments that triggered them,\n#'   set trunc = 2.\n#'\n#' @importFrom stats pnorm\n#' @importFrom caTools trapz\n#' @export\n# Added SF_as_log argument with default as false to match condition on line 93\ncomputeAUC <- function(D, SF, pars, lower, upper, trunc = TRUE,\n                       SF_as_log = FALSE,\n                       area.type = c(\"Fitted\", \"Actual\"),\n                       verbose = TRUE)\n  {\n  area.type <- match.arg(area.type)\n\n  if (!missing(SF)) {\n    CoreGx::.sanitizeInput(x = D,\n                            y = SF,\n                            x_as_log = FALSE,\n                            y_as_log = FALSE,\n                            y_as_pct = FALSE,\n                            trunc = trunc,\n                            verbose = FALSE)\n\n    DSF <- CoreGx::.reformatData(x = D,\n                                 y = SF,\n                                 x_to_log = FALSE,\n                                 y_to_log = FALSE,\n                                 y_to_frac = FALSE,\n                                 trunc = trunc)\n    D <- DSF[[\"x\"]]\n    SF <- DSF[[\"y\"]]\n  } else if (!missing(pars)) {\n    CoreGx::.sanitizeInput(pars = pars,\n                            x_as_log = FALSE,\n                            y_as_log = FALSE,\n                            y_as_pct = FALSE,\n                            trunc = trunc,\n                            verbose = FALSE)\n    Dpars <- CoreGx::.reformatData(x = D,\n                                    pars = pars,\n                                    x_to_log = FALSE,\n                                    y_to_log = FALSE,\n                                    y_to_frac = FALSE,\n                                    trunc = trunc)\n    D <- Dpars[[\"x\"]]\n    pars <- Dpars[[\"pars\"]]\n  } else {\n    stop(\"SF and pars can't both be missing.\")\n  }\n\n  if (!missing(lower) && !missing(upper)) {\n    ###TODO:: Check if this function still works correctly\n    CoreGx::.sanitizeInput(pars = pars, # Added this line to resolve error returned from CoreGx\n                           lower = lower,\n                           upper = upper,\n                           x_as_log = FALSE,\n                           y_as_log = FALSE,\n                           y_as_pct = FALSE,\n                           trunc = trunc,\n                           verbose = verbose)\n  }\n\n  if (area.type == \"Fitted\") {\n    if (missing(pars)) {\n      pars <- unlist(linearQuadraticModel(D = D,\n                                          SF = SF,\n                                          trunc = trunc,\n                                          verbose = verbose))\n    }\n    if (missing(lower)) {\n      lower <- min(D)\n    }\n    if (missing(upper)) {\n      upper <- max(D)\n    }\n\n    if (SF_as_log == TRUE) { # Modified condition to correct error\n      return(pars[[1]] / 2 * (lower ^ 2 - upper ^ 2) + pars[[2]] / 3 * (lower ^ 3 - upper ^ 3))\n    } else {\n      if (pars[[2]] == 0) {\n        if (pars[[1]] == 0) {\n          return(upper - lower)\n        } else {\n          return((exp(-pars[[1]] * lower) - exp(-pars[[1]] * upper)) / pars[[1]])\n        }\n      } else {\n        x <- CoreGx::.getSupportVec(x=D, output_length = 1000)\n        y <- .linearQuadratic(D=x, pars=pars, SF_as_log=FALSE)\n        return(caTools::trapz(x, y))\n\n      }\n    }\n\n  } else if (area.type == \"Actual\") {\n    if (missing(SF)) {\n      stop(\"Please pass in SF-values.\")\n    } else {\n      return(caTools::trapz(x = D, y = SF))\n    }\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC` function and what are its main input parameters?",
        "answer": "The `computeAUC` function computes the area under a dose-response curve. Its main input parameters are:\n- `D`: vector of dosages\n- `SF`: vector of survival fractions\n- `pars`: parameters (alpha, beta) for the equation y = exp(-alpha * x - beta * x ^ 2)\n- `lower` and `upper`: bounds of the dose region to compute AUC over\n- `trunc`: whether to truncate survival fractions to 1 if they exceed 1\n- `SF_as_log`: whether survival fraction is displayed on a log axis\n- `area.type`: whether to return AUC of raw points or fitted curve\n- `verbose`: level of detail for error and warning messages"
      },
      {
        "question": "How does the function handle the case when both `SF` and `pars` are missing?",
        "answer": "If both `SF` and `pars` are missing, the function will throw an error with the message \"SF and pars can't both be missing.\" This is because the function needs either the survival fractions (`SF`) or the parameters (`pars`) to compute the AUC. The check is performed in an if-else block, where if both `SF` and `pars` are missing, it reaches the else condition and calls `stop()` with the error message."
      },
      {
        "question": "What is the difference between 'Fitted' and 'Actual' area types in the `computeAUC` function?",
        "answer": "The `area.type` parameter in `computeAUC` function determines how the AUC is calculated:\n\n1. 'Fitted': This calculates the AUC using a fitted curve. If `pars` are not provided, it fits a linear quadratic model to the data. It then uses these parameters to compute the AUC either analytically (for simple cases) or numerically (using trapezoid rule).\n\n2. 'Actual': This calculates the AUC directly from the provided data points (D and SF) using the trapezoid rule, without fitting any curve. It requires the `SF` values to be provided.\n\nThe 'Fitted' option is more suitable for smoothing out noise in the data, while 'Actual' gives the AUC of the raw data points."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC <- function(D, SF, pars, lower, upper, trunc = TRUE,\n                       SF_as_log = FALSE,\n                       area.type = c(\"Fitted\", \"Actual\"),\n                       verbose = TRUE) {\n  area.type <- match.arg(area.type)\n\n  if (!missing(SF)) {\n    DSF <- CoreGx::.reformatData(x = D, y = SF, x_to_log = FALSE,\n                                 y_to_log = FALSE, y_to_frac = FALSE,\n                                 trunc = trunc)\n    D <- DSF[[\"x\"]]\n    SF <- DSF[[\"y\"]]\n  } else if (!missing(pars)) {\n    Dpars <- CoreGx::.reformatData(x = D, pars = pars, x_to_log = FALSE,\n                                    y_to_log = FALSE, y_to_frac = FALSE,\n                                    trunc = trunc)\n    D <- Dpars[[\"x\"]]\n    pars <- Dpars[[\"pars\"]]\n  } else {\n    stop(\"SF and pars can't both be missing.\")\n  }\n\n  if (area.type == \"Fitted\") {\n    # Complete the code for the \"Fitted\" area type\n  } else if (area.type == \"Actual\") {\n    # Complete the code for the \"Actual\" area type\n  }\n}",
        "complete": "computeAUC <- function(D, SF, pars, lower, upper, trunc = TRUE,\n                       SF_as_log = FALSE,\n                       area.type = c(\"Fitted\", \"Actual\"),\n                       verbose = TRUE) {\n  area.type <- match.arg(area.type)\n\n  if (!missing(SF)) {\n    DSF <- CoreGx::.reformatData(x = D, y = SF, x_to_log = FALSE,\n                                 y_to_log = FALSE, y_to_frac = FALSE,\n                                 trunc = trunc)\n    D <- DSF[[\"x\"]]\n    SF <- DSF[[\"y\"]]\n  } else if (!missing(pars)) {\n    Dpars <- CoreGx::.reformatData(x = D, pars = pars, x_to_log = FALSE,\n                                    y_to_log = FALSE, y_to_frac = FALSE,\n                                    trunc = trunc)\n    D <- Dpars[[\"x\"]]\n    pars <- Dpars[[\"pars\"]]\n  } else {\n    stop(\"SF and pars can't both be missing.\")\n  }\n\n  if (area.type == \"Fitted\") {\n    if (missing(pars)) {\n      pars <- unlist(linearQuadraticModel(D = D, SF = SF, trunc = trunc, verbose = verbose))\n    }\n    lower <- if (missing(lower)) min(D) else lower\n    upper <- if (missing(upper)) max(D) else upper\n\n    if (SF_as_log) {\n      return(pars[[1]] / 2 * (lower^2 - upper^2) + pars[[2]] / 3 * (lower^3 - upper^3))\n    } else {\n      if (pars[[2]] == 0) {\n        if (pars[[1]] == 0) return(upper - lower)\n        return((exp(-pars[[1]] * lower) - exp(-pars[[1]] * upper)) / pars[[1]])\n      } else {\n        x <- CoreGx::.getSupportVec(x = D, output_length = 1000)\n        y <- .linearQuadratic(D = x, pars = pars, SF_as_log = FALSE)\n        return(caTools::trapz(x, y))\n      }\n    }\n  } else if (area.type == \"Actual\") {\n    if (missing(SF)) stop(\"Please pass in SF-values.\")\n    return(caTools::trapz(x = D, y = SF))\n  }\n}"
      },
      {
        "partial": "linearQuadraticModel <- function(D, SF, trunc = TRUE, verbose = TRUE) {\n  # Implement the linear quadratic model fitting\n  # Return the fitted parameters\n}",
        "complete": "linearQuadraticModel <- function(D, SF, trunc = TRUE, verbose = TRUE) {\n  if (trunc) {\n    SF[SF > 1] <- 1\n    if (verbose) warning(\"Some SF values were > 1 and have been truncated.\")\n  }\n  \n  log_SF <- log(SF)\n  model <- lm(log_SF ~ D + I(D^2))\n  \n  alpha <- -coef(model)[2]\n  beta <- -coef(model)[3]\n  \n  return(list(alpha = alpha, beta = beta))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/LongTable-utils.R",
    "language": "R",
    "content": "#' @include LongTable-class.R LongTable-accessors.R\n#' @importFrom checkmate assertClass assertDataFrame\nNULL\n\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown=TRUE, r6=FALSE)\n#### in the DESCRIPTION file!\n\n\n# ===================================\n# Utility Method Documentation Object\n# -----------------------------------\n\n\n# ======================================\n# Subset Methods\n# --------------------------------------\n\n\n##\n## == subset\n\n\n#' Subset a `LongTable` using an \"assayIndex\" data.frame\n#'\n#' @param x `LongTable`\n#' @param index `data.frame` Table with columns \"rowKey\", \"colKey\" and\n#'   \".\\<assayName\\>\", where \\<assayName\\> is the value for each `assayNames(x)`.\n#'   Warning: rownames are dropped internally in coercion to `data.table`,\n#' @param reindex `logical(1)` Should index values be reset such that they\n#'   are the smallest possible set of consecutive integers. Modifies the\n#'   \"rowKey\", \"colKey\", and all assayKey columns. Initial benchmarks indicate\n#'   `reindex=FALSE` saves ~20% of both execution time and memory allocation. The\n#'   cost of reindexing decreases the smaller your subet gets.\n#'\n#' @return `LongTable` subset according to the provided index.\n#'\n#' @noRd\n.subsetByIndex <- function(x, index, reindex=FALSE) {\n\n    # -- validate input\n    assertClass(x, \"LongTable\")\n    assertDataFrame(index)\n    if (!is.data.table(index)) setDT(index)\n    x <- copy(x)\n\n    # -- subset slots\n    rData <- rowData(x, raw=TRUE)[sort(unique(index$rowKey)), ]\n    cData <- colData(x, raw=TRUE)[sort(unique(sort(index$colKey))), ]\n    assays <- assays(x, withDimnames=FALSE)\n    metaKeys <- c(\"rowKey\", \"colKey\")\n    setkeyv(index, metaKeys)\n    for (i in seq_along(assays)) {\n        setkeyv(assays[[i]], metaKeys)\n        aname <- paste0(\".\", names(assays)[i])\n        # join based subsets use binary-search, O(log(n)) vs O(n) for vector-scan\n        # see https://rdatatable.gitlab.io/data.table/articles/datatable-keys-fast-subset.html\n        assays[[i]] <- assays[[i]][\n            index[!is.na(get(aname)), c(metaKeys, aname), with=FALSE],\n        ]\n        setkeyv(assays[[i]], aname)\n    }\n    # -- update object\n    # delete row-/colKeys by reference\n    for (a in assays) a[, (metaKeys) := NULL]\n    # ensure uniqueness for summary assays, fixes #149\n    assays <- lapply(assays, FUN=unique)\n    # raw=TRUE allows direct modification of slots\n    setkeyv(rData, \"rowKey\")\n    rowData(x, raw=TRUE) <- rData\n    setkeyv(cData, \"colKey\")\n    colData(x, raw=TRUE) <- cData\n    assays(x, raw=TRUE) <- assays\n    mutableIntern <- mutable(getIntern(x))\n    setkeyv(index, paste0(\".\", names(assays)))\n    mutableIntern$assayIndex <- index\n    x@.intern <- immutable(mutableIntern)\n\n    # -- optionally reindex the table\n    if (reindex) {\n        x <- reindex(x)\n    }\n    return(x)\n}\n\n#' Subset method for a LongTable object.\n#'\n#' Allows use of the colData and rowData `data.table` objects to query based on\n#'  rowID and colID, which is then used to subset all assay `data.table`s stored\n#'  in the `assays` slot.\n#' This function is endomorphic, it always returns a LongTable object.\n#'\n#' @examples\n#' # Character\n#' subset(merckLongTable, 'ABT-888', 'CAOV3')\n#' # Numeric\n#' subset(merckLongTable, 1, c(1, 2))\n#' # Logical\n#' subset(merckLongTable, , colData(merckLongTable)$sampleid == 'A2058')\n#' # Call\n#' subset(merckLongTable, drug1id == 'Dasatinib' & drug2id != '5-FU',\n#'     sampleid == 'A2058')\n#'\n#' @param x `LongTable` The object to subset.\n#' @param i `character`, `numeric`, `logical` or `call`\n#'  Character: pass in a character vector of rownames for the `LongTable` object\n#'    or a valid regex query which will be evaluated against the rownames.\n#'  Numeric or Logical: vector of indices or a logical vector to subset\n#'    the rows of a `LongTable`.\n#'  Call: Accepts valid query statements to the `data.table` i parameter,\n#'    this can be used to make complex queries using the `data.table` API\n#'    for the `rowData` data.table.\n#' @param j `character`, `numeric`, `logical` or `call`\n#'  Character: pass in a character vector of colnames for the `LongTable` object\n#'    or a valid regex query which will be evaluated against the colnames.\n#'  Numeric or Logical: vector of indices or a logical vector to subset\n#'    the columns of a `LongTable`.\n#'  Call: Accepts valid query statements to the `data.table` i parameter,\n#'    this can be used to make complex queries using the `data.table` API\n#'    for the `colData` data.table.\n#' @param assays `character`, `numeric` or `logical` Optional list of assay\n#'   names to subset. Can be used to subset the assays list further,\n#'   returning only the selected items in the new LongTable.\n#' @param reindex `logical(1)` Should index values be reset such that they\n#'   are the smallest possible set of consecutive integers. Modifies the\n#'   \"rowKey\", \"colKey\", and all assayKey columns. Initial benchmarks indicate\n#'   `reindex=FALSE` saves ~20% of both execution time and memory allocation. The\n#'   cost of reindexing decreases the smaller your subset gets.\n#'\n#' @return `LongTable` A new `LongTable` object subset based on the specified\n#'      parameters.\n#'\n#' @importMethodsFrom BiocGenerics subset\n#' @importFrom crayon magenta cyan\n#' @importFrom MatrixGenerics rowAnys\n#' @import data.table\n#' @export\nsetMethod('subset', signature('LongTable'),\n        function(x, i, j, assays=assayNames(x),\n            reindex=TRUE) {\n\n    # prevent modify by reference\n    x <- copy(x)\n\n    # local helper functions\n    .rowData <- function(...) rowData(..., key=TRUE)\n    .colData <- function(...) colData(..., key=TRUE)\n    .tryCatchNoWarn <- function(...) suppressWarnings(tryCatch(...))\n    .strSplitLength <- function(...) length(unlist(strsplit(...)))\n\n    # subset rowData\n    ## FIXME:: Can I parameterize this into a helper that works for both row\n    ## and column data?\n    if (!missing(i)) {\n        ## TODO:: Clean up this if-else block\n        if (.tryCatchNoWarn(is.call(i), error=function(e) FALSE)) {\n            rowDataSubset <- .rowData(x)[eval(i), ]\n        } else if (.tryCatchNoWarn(is.character(i), error=function(e) FALSE)) {\n            ## TODO:: Implement diagnosis for failed regex queries\n            idCols <- rowIDs(x, key=TRUE)\n            if (max(unlist(lapply(i, .strSplitLength, split=':'))) > length(idCols))\n                stop(cyan$bold('Attempting to select more rowID columns than\n                    there are in the LongTable.\\n\\tPlease use query of the form ',\n                    paste0(idCols, collapse=':')))\n            imatch <- rownames(x) %in% i\n            if (!any(imatch))\n                imatch <- grepl(.preprocessRegexQuery(i), rownames(x),\n                    ignore.case=TRUE)\n            imatch <- str2lang(.variableToCodeString(imatch))\n            rowDataSubset <- .rowData(x)[eval(imatch), ]\n        } else {\n            isub <- substitute(i)\n            rowDataSubset <- .tryCatchNoWarn(.rowData(x)[i, ],\n                error=function(e) .rowData(x)[eval(isub), ])\n        }\n    } else {\n        rowDataSubset <- .rowData(x)\n    }\n\n    # subset colData\n    if (!missing(j)) {\n        ## TODO:: Clean up this if-else block\n        if (.tryCatchNoWarn(is.call(j), error=function(e) FALSE, silent=TRUE)) {\n            colDataSubset <- .colData(x)[eval(j), ]\n        } else if (.tryCatchNoWarn(is.character(j), error=function(e) FALSE, silent=TRUE)) {\n            ## TODO:: Implement diagnosis for failed regex queries\n            idCols <- colIDs(x, key=TRUE)\n            if (max(unlist(lapply(j, .strSplitLength, split=':'))) > length(idCols))\n                stop(cyan$bold('Attempting to select more ID columns than there\n                    are in the LongTable.\\n\\tPlease use query of the form ',\n                    paste0(idCols, collapse=':')))\n            jmatch <- colnames(x) %in% j\n            if (!any(jmatch))\n                jmatch <- grepl(.preprocessRegexQuery(j), colnames(x),\n                    ignore.case=TRUE)\n            jmatch <- str2lang(.variableToCodeString(jmatch))\n            colDataSubset <- .colData(x)[eval(jmatch), ]\n        } else {\n            jsub <- substitute(j)\n            colDataSubset <- .tryCatchNoWarn(.colData(x)[j, ],\n                error=function(e) .colData(x)[eval(jsub), ])\n        }\n    } else {\n        colDataSubset <- .colData(x)\n    }\n\n    # Subset assays to only keys in remaining in rowData/colData\n    rows <- rowDataSubset$rowKey\n    cols <- colDataSubset$colKey\n\n    # -- find matching assays\n    validAssays <- assays %in% assayNames(x)\n    if (any(!validAssays))\n        warning(.warnMsg(assays[!validAssays],\n            \" are not valid assay names, ignoring...\"), call.=FALSE)\n    keepAssays <- assayNames(x) %in% assays\n\n    # -- subset index, then use index to subset x\n    assayKeys <- paste0(\".\", assayNames(x)[keepAssays])\n    idx <- mutable(getIntern(x, \"assayIndex\"))[\n        rowKey %in% rows & colKey %in% cols,\n        .SD,\n        .SDcols=c(\"rowKey\", \"colKey\", assayKeys)\n    ]\n    # -- drop rowKeys or colKeys which no longer have any assay observation\n    #   after the initial subset, fixes #148\n    validKeys <- idx[\n        which(rowAnys(!is.na(idx[, assayKeys, with=FALSE]))),\n        .(rowKey, colKey)\n    ]\n    idx <- idx[\n        rowKey %in% unique(validKeys$rowKey) &\n            colKey %in% unique(validKeys$colKey),\n    ]\n    assays(x, raw=TRUE)[!keepAssays] <- NULL  # delete assays being dropped\n\n    return(.subsetByIndex(x, idx, reindex=reindex))\n})\n\n\n\n#' Convenience function for converting R code to a call\n#'\n#' This is used to pass through unevaluated R expressions into subset and\n#'   `[`, where they will be evaluated in the correct context.\n#'\n#' @examples\n#' .(sample_line1 == 'A2058')\n#'\n#' @param ... `pairlist` One or more R expressions to convert to calls.\n#'\n#' @return `call` An R call object containing the quoted expression.\n#'\n#' @export\n. <- function(...) substitute(...)\n\n# ---- subset LongTable helpers\n\n#' Collapse vector of regex queries with | and replace * with .*\n#'\n#' @param queryString `character` Raw regex queries.\n#'\n#' @return `character` Formatted regex query.\n#'\n#' @keywords internal\n#' @noRd\n.preprocessRegexQuery <- function(queryString) {\n    # Support vectors of regex queries\n    query <- paste0(unique(queryString), collapse='|')\n    # Swap all * with .*\n    query <- gsub('\\\\.\\\\*', '*', query)\n    return(gsub('\\\\*', '.*', query))\n}\n\n\n#' @keywords internal\n#' @noRd\n.validateRegexQuery <- function(regex, names) {\n    ## TODO:: return TRUE if reqex query is valid, otherwise return error message\n}\n\n#' Convert an R object in a variable into a string of the code necessary to\n#'   create that object\n#'\n#' @param variable `symbol` A symbol containing an R variable\n#'\n#' @return `character(1)` A string representation of the code necessary to\n#'   reconstruct the variable.\n#'\n#' @keywords internal\n#' @noRd\n.variableToCodeString <- function(variable) {\n    codeString <- paste0(capture.output(dput(variable)), collapse='')\n    codeString <- gsub('\\\"', \"'\", codeString)\n    return(codeString)\n}\n\n#' Filter a data.table object based on the rowID and colID columns\n#'\n#' @param DT `data.table` Object with the columns rowID and colID, preferably\n#'  as the key columns.\n#' @param indexList `list` Two integer vectors, one indicating the rowIDs and\n#'  one indicating the colIDs to filter the `data.table` on.\n#'\n#' @return `data.table` A copy of `DT` subset on the row and column IDs specified\n#'  in `indexList`.\n#'\n#' @import data.table\n#' @keywords internal\n#' @noRd\n.filterLongDataTable <- function(DT, indexList) {\n\n    # validate input\n    if (length(indexList) > 2)\n        stop(\"This object is 2D, please only pass in two ID vectors, one for\n             rows and one for columns!\")\n\n    if (!all(vapply(unlist(indexList), is.numeric, FUN.VALUE=logical(1))))\n        stop('Please ensure indexList only contains integer vectors!')\n\n    # extract indices\n    rowIndices <- indexList[[1]]\n    colIndices <- indexList[[2]]\n\n    # return filtered data.table\n    return(copy(DT[rowKey %in% rowIndices & colKey %in% colIndices, ]))\n}\n\n##\n## == [ method\n\n\n#' [ LongTable Method\n#'\n#' Single bracket subsetting for a LongTable object. See subset for more details.\n#'\n#' This function is endomorphic, it always returns a LongTable object.\n#'\n#' @examples\n#' # Character\n#' merckLongTable['ABT-888', 'CAOV3']\n#' # Numeric\n#' merckLongTable[1, c(1, 2)]\n#' # Logical\n#' merckLongTable[, colData(merckLongTable)$sampleid == 'A2058']\n#' # Call\n#' merckLongTable[\n#'      .(drug1id == 'Dasatinib' & drug2id != '5-FU'),\n#'      .(sampleid == 'A2058'),\n#'  ]\n#'\n#' @param x `LongTable` The object to subset.\n#' @param i `character`, `numeric`, `logical` or `call`\n#'  Character: pass in a character vector of drug names, which will subset the\n#'    object on all row id columns matching the vector. This parameter also\n#'    supports valid R regex query strings which will match on the colnames\n#'    of `x`. For convenience, * is converted to .* automatically. Colon\n#'    can be to denote a specific part of the colnames string to query.\n#'  Numeric or Logical: these select based on the rowKey from the `rowData`\n#'    method for the `LongTable`.\n#'  Call: Accepts valid query statements to the `data.table` i parameter as\n#'    a call object. We have provided the function .() to conveniently\n#'    convert raw R statements into a call for use in this function.\n#' @param j `character`, `numeric`, `logical` or `call`\n#'  Character: pass in a character vector of drug names, which will subset the\n#'      object on all drug id columns matching the vector. This parameter also\n#'      supports regex queries. Colon can be to denote a specific part of the\n#'      colnames string to query.\n#'  Numeric or Logical: these select based on the rowID from the `rowData`\n#'      method for the `LongTable`.\n#'  Call: Accepts valid query statements to the `data.table` i parameter as\n#'      a call object. We have provided the function .() to conveniently\n#'      convert raw R statements into a call for use in this function.\n#' @param assays `character` Names of assays which should be kept in the\n#'   `LongTable` after subsetting.\n#' @param ... Included to ensure drop can only be set by name.\n#' @param drop `logical` Included for compatibility with the '[' primitive,\n#'   it defaults to FALSE and changing it does nothing.\n#'\n#' @return A `LongTable` containing only the data specified in the function\n#'   parameters.\n#'\n#' @export\nsetMethod('[', signature('LongTable'),\n        function(x, i, j, assays=assayNames(x), ..., drop=FALSE) {\n    subset(x, i, j, assays=assays, ...)\n})\n\n\n##\n## == [[ method'\n\n\n#' [[ Method for LongTable Class\n#'\n#' Select an assay from within a LongTable object.\n#'\n#' @describeIn LongTable Get an assay from a LongTable object. This method\n#'   returns the row and column annotations by default to make assignment\n#'   and aggregate operations easiers.\n#'\n#' @examples\n#' merckLongTable[['sensitivity']]\n#'\n#' @param x `LongTable` object to retrieve assays from\n#' @param i `character(1)` name or `integer` index of the desired assay.\n#'\n#' @importFrom crayon cyan magenta\n#' @import data.table\n#' @export\nsetMethod('[[', signature('LongTable'), function(x, i) {\n    assay(x, i)\n})\n\n\n#' `[[<-` Method for LongTable Class\n#'\n#' Just a wrapper around assay<- for convenience. See\n#' `?'assay<-,LongTable,character-method'.`\n#'\n#' @param x A `LongTable` to update.\n#' @param i The name of the assay to update, must be in `assayNames(object)`.\n#' @param value A `data.frame`\n#'\n#' @examples\n#' merckLongTable[['sensitivity']] <- merckLongTable[['sensitivity']]\n#'\n#' @return A `LongTable` object with the assay `i` updated using `value`.\n#'\n#' @export\nsetReplaceMethod('[[', signature(x='LongTable'), function(x, i, value) {\n    assay(x, i) <- value\n    x\n})\n\n\n##\n## == $ method\n\n\n#' Select an assay from a LongTable object\n#'\n#' @examples\n#' merckLongTable$sensitivity\n#'\n#' @param x A `LongTable` object to retrieve an assay from\n#' @param name `character` The name of the assay to get.\n#'\n#' @return `data.frame` The assay object.\n#'\n#' @export\nsetMethod('$', signature('LongTable'), function(x, name) {\n    # error handling is done inside `[[`\n    x[[name]]\n})\n\n#' Update an assay from a LongTable object\n#'\n#' @examples\n#' merckLongTable$sensitivity <- merckLongTable$sensitivity\n#'\n#' @param x A `LongTable` to update an assay for.\n#' @param name `character(1)` The name of the assay to update\n#' @param value A `data.frame` or `data.table` to update the assay with.\n#'\n#' @return Updates the assay `name` in `x` with `value`, returning an invisible\n#' NULL.\n#'\n#' @export\nsetReplaceMethod('$', signature('LongTable'), function(x, name, value) {\n    # error handling done inside `assay<-`\n    x[[name]] <- value\n    x\n})\n\n\n# ======================================\n# Reindex Methods\n# --------------------------------------\n\n##\n## == reindex\n\n#' Redo indexing for a LongTable object to remove any gaps in integer indexes\n#'\n#' After subsetting a LongTable, it is possible that values of rowKey or colKey\n#'   could no longer be present in the object. As a result there the indexes\n#'   will no longer be contiguous integers. This method will calcualte a new\n#'   set of rowKey and colKey values such that integer indexes are the smallest\n#'   set of contiguous integers possible for the data.\n#'\n#' @param object The `LongTable` object to recalcualte indexes (rowKey and\n#'     colKey values) for.\n#'\n#' @return A copy of the `LongTable` with all keys as the smallest set of\n#'     contiguous integers possible given the current data.\n#'\n#' @export\nsetMethod('reindex', signature(object='LongTable'), function(object) {\n\n    # -- extract the requisite data\n    mutableIntern <- mutable(getIntern(object))\n    index <- mutableIntern$assayIndex\n    rData <- copy(rowData(object, raw=TRUE))\n    cData <- copy(colData(object, raw=TRUE))\n    aList <- copy(assays(object, raw=TRUE))\n\n    # -- sort metadata tables by their id columns and update the index\n    # Add row key to rData\n    rData[, .rowKey := .I, by=c(rowIDs(object))]\n\n    # Add column key to cData\n    cData[, .colKey := .I, by=c(colIDs(object))]\n\n    # -- update rowKey and colKey in the assayIndex, if they have changed\n    if (rData[, any(rowKey != .rowKey)]) {\n        index[rData, rowKey := .rowKey, on=\"rowKey\"]\n        rData[, rowKey := .rowKey]\n        setkeyv(rData, \"rowKey\")\n    }\n    if (cData[, any(colKey != .colKey)]) {\n        index[cData, colKey := .colKey, on=\"colKey\"]\n        cData[, colKey := .colKey]\n        setkeyv(cData, \"colKey\")\n    }\n    rData[, .rowKey := NULL]\n    cData[, .colKey := NULL]\n\n    # -- add new indices for assayKeys to index\n    setkeyv(index, c(\"rowKey\", \"colKey\"))\n    assays_ <- setdiff(colnames(index), c(\"rowKey\", \"colKey\"))\n    lapply(assays_, function(nm){\n        # drop the first \".\" from nm\n        name <- gsub(\"^\\\\.\", \"\", nm)\n\n        # if there are any more \".\" in the name, then raise an erro\n        if (grepl(\"\\\\.\", name))\n            stop(\"Assay names cannot contain '.' characters!\")\n\n    })\n\n    assayEqualKeys <- setNames(vector(\"logical\", length(assays_)), assays_)\n    # This loop iterates over each element in the 'assays_' vector.\n    # For each element, it performs the following operations:\n    # 1. It checks if the element is not missing in the current data.table.\n    # 2. If the element is not missing, it creates a new column with the name\n    #    \".<element>\" and assigns a unique group identifier (.GRP) to each row\n    #    that has a non-missing value for that element. The grouping is done by\n    #    the element itself.\n    # 3. It updates the 'assayEqualKeys' list with a logical value indicating\n    #    whether all the values in the newly created column are equal to the\n    #    corresponding values in the original column.\n    #    This is done to ensure that summary assays, with repeated keys, have\n    #    consistent values across all rows.\n    #\n    # The data.table syntax used in this code is as follows:\n    # - The 'get' function is used to retrieve the value of a variable by name.\n    # - The 'is.na' function is used to check if a value is missing.\n    # - The ':=' operator is used to create or update columns by reference.\n    # - The '.GRP' variable represents a unique group identifier.\n    # - The 'by' parameter is used to specify the grouping variable(s).\n    #\n    # Note: The line width of the code has been limited to 80 characters for better readability.\n    for (nm in assays_) {\n        ## Added by to maintain cardinality of the each assayKey\n        ## Required to fix #147 and ensure summary assays, with repeated keys\n        index[!is.na(get(nm)), paste0(\".\", nm) := .GRP, by=c(nm)]\n        assayEqualKeys[nm] <- index[!is.na(get(nm)), all(get(paste0(\".\", nm)) == get(nm))]\n    }\n\n    # -- check equality and update assayKeys in assays if they have changed\n    for (.nm in names(which(!assayEqualKeys))) {\n        nm <- gsub(\"\\\\.\", \"\", .nm)\n        setkeyv(index, .nm)\n        aList[[nm]][index, (.nm) := get(paste0(\".\", .nm))]\n        setkeyv(aList[[nm]], .nm)\n        index[, (.nm) := get(paste0(\".\", .nm))]\n    }\n    index[, paste0(\".\", assays_) := NULL]\n    setkeyv(index, paste0(\".\", assayNames(object)))\n\n    # -- update the object with the reindexed tables and return\n    rowData(object, raw=TRUE) <- rData\n    colData(object, raw=TRUE) <- cData\n    assays(object, raw=TRUE) <- aList\n    mutableIntern$assayIndex <- index\n    object@.intern <- immutable(mutableIntern)\n    return(object)\n})\n\n\n#' @keywords internal\n#' @noRd\n.extractIDData <- function(assayDataList, idCols, keyName) {\n    idDT <- data.table()\n    for (assay in assayDataList) {\n        idDT <- unique(rbindlist(list(idDT, assay[, ..idCols])))\n    }\n    rm(assayDataList)\n    idDT[, eval(substitute(keyName := seq_len(.N)))]\n    setkeyv(idDT, keyName)\n    return(idDT)\n}\n\n\n#' @keywords interal\n#' @noRd\n.joinDropOn <- function(DT1, DT2, on) {\n    DT1[DT2, on=on][, -get('on')]\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.subsetByIndex` function in this code?",
        "answer": "The `.subsetByIndex` function is used to subset a `LongTable` object using an 'assayIndex' data.frame. It takes the `LongTable` object, an index data.frame, and a reindex flag as inputs. The function subsets the rowData, colData, and assays of the `LongTable` based on the provided index, and optionally reindexes the resulting subset."
      },
      {
        "question": "How does the `subset` method handle different types of input for the `i` and `j` parameters?",
        "answer": "The `subset` method for `LongTable` objects handles different types of input for `i` and `j` parameters as follows:\n1. For character input, it matches against rownames/colnames or evaluates regex queries.\n2. For numeric or logical input, it uses them as indices or logical vectors for subsetting.\n3. For call objects, it evaluates them as queries against the rowData/colData data.tables.\nThis flexibility allows for various ways of subsetting the `LongTable` object."
      },
      {
        "question": "What is the purpose of the `reindex` method for `LongTable` objects?",
        "answer": "The `reindex` method for `LongTable` objects is used to recalculate the indexing (rowKey and colKey values) after subsetting. It ensures that the integer indexes are the smallest set of contiguous integers possible for the current data. This is useful because subsetting can create gaps in the index values, and reindexing helps maintain efficient and consistent indexing across the `LongTable` object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Subset a `LongTable` using an \"assayIndex\" data.frame\n.subsetByIndex <- function(x, index, reindex=FALSE) {\n    # -- validate input\n    assertClass(x, \"LongTable\")\n    assertDataFrame(index)\n    if (!is.data.table(index)) setDT(index)\n    x <- copy(x)\n\n    # -- subset slots\n    rData <- rowData(x, raw=TRUE)[sort(unique(index$rowKey)), ]\n    cData <- colData(x, raw=TRUE)[sort(unique(sort(index$colKey))), ]\n    assays <- assays(x, withDimnames=FALSE)\n    metaKeys <- c(\"rowKey\", \"colKey\")\n    setkeyv(index, metaKeys)\n    for (i in seq_along(assays)) {\n        setkeyv(assays[[i]], metaKeys)\n        aname <- paste0(\".\", names(assays)[i])\n        assays[[i]] <- assays[[i]][\n            index[!is.na(get(aname)), c(metaKeys, aname), with=FALSE],\n        ]\n        setkeyv(assays[[i]], aname)\n    }\n\n    # -- update object\n    for (a in assays) a[, (metaKeys) := NULL]\n    assays <- lapply(assays, FUN=unique)\n    setkeyv(rData, \"rowKey\")\n    rowData(x, raw=TRUE) <- rData\n    setkeyv(cData, \"colKey\")\n    colData(x, raw=TRUE) <- cData\n    assays(x, raw=TRUE) <- assays\n    mutableIntern <- mutable(getIntern(x))\n    setkeyv(index, paste0(\".\", names(assays)))\n    mutableIntern$assayIndex <- index\n    x@.intern <- immutable(mutableIntern)\n\n    # -- optionally reindex the table\n    if (reindex) {\n        x <- reindex(x)\n    }\n    return(x)\n}",
        "complete": "# Subset a `LongTable` using an \"assayIndex\" data.frame\n.subsetByIndex <- function(x, index, reindex=FALSE) {\n    assertClass(x, \"LongTable\")\n    assertDataFrame(index)\n    if (!is.data.table(index)) setDT(index)\n    x <- copy(x)\n\n    rData <- rowData(x, raw=TRUE)[sort(unique(index$rowKey)), ]\n    cData <- colData(x, raw=TRUE)[sort(unique(sort(index$colKey))), ]\n    assays <- assays(x, withDimnames=FALSE)\n    metaKeys <- c(\"rowKey\", \"colKey\")\n    setkeyv(index, metaKeys)\n    for (i in seq_along(assays)) {\n        setkeyv(assays[[i]], metaKeys)\n        aname <- paste0(\".\", names(assays)[i])\n        assays[[i]] <- assays[[i]][\n            index[!is.na(get(aname)), c(metaKeys, aname), with=FALSE],\n        ]\n        setkeyv(assays[[i]], aname)\n    }\n\n    for (a in assays) a[, (metaKeys) := NULL]\n    assays <- lapply(assays, FUN=unique)\n    setkeyv(rData, \"rowKey\")\n    rowData(x, raw=TRUE) <- rData\n    setkeyv(cData, \"colKey\")\n    colData(x, raw=TRUE) <- cData\n    assays(x, raw=TRUE) <- assays\n    mutableIntern <- mutable(getIntern(x))\n    setkeyv(index, paste0(\".\", names(assays)))\n    mutableIntern$assayIndex <- index\n    x@.intern <- immutable(mutableIntern)\n\n    if (reindex) x <- reindex(x)\n    return(x)\n}"
      },
      {
        "partial": "# Subset method for a LongTable object\nsetMethod('subset', signature('LongTable'),\n    function(x, i, j, assays=assayNames(x), reindex=TRUE) {\n        x <- copy(x)\n\n        .rowData <- function(...) rowData(..., key=TRUE)\n        .colData <- function(...) colData(..., key=TRUE)\n        .tryCatchNoWarn <- function(...) suppressWarnings(tryCatch(...))\n        .strSplitLength <- function(...) length(unlist(strsplit(...)))\n\n        # subset rowData\n        if (!missing(i)) {\n            if (.tryCatchNoWarn(is.call(i), error=function(e) FALSE)) {\n                rowDataSubset <- .rowData(x)[eval(i), ]\n            } else if (.tryCatchNoWarn(is.character(i), error=function(e) FALSE)) {\n                idCols <- rowIDs(x, key=TRUE)\n                if (max(unlist(lapply(i, .strSplitLength, split=':'))) > length(idCols))\n                    stop(cyan$bold('Attempting to select more rowID columns than there are in the LongTable.\\n\\tPlease use query of the form ', paste0(idCols, collapse=':')))\n                imatch <- rownames(x) %in% i\n                if (!any(imatch))\n                    imatch <- grepl(.preprocessRegexQuery(i), rownames(x), ignore.case=TRUE)\n                imatch <- str2lang(.variableToCodeString(imatch))\n                rowDataSubset <- .rowData(x)[eval(imatch), ]\n            } else {\n                isub <- substitute(i)\n                rowDataSubset <- .tryCatchNoWarn(.rowData(x)[i, ],\n                    error=function(e) .rowData(x)[eval(isub), ])\n            }\n        } else {\n            rowDataSubset <- .rowData(x)\n        }\n\n        # subset colData\n        if (!missing(j)) {\n            if (.tryCatchNoWarn(is.call(j), error=function(e) FALSE, silent=TRUE)) {\n                colDataSubset <- .colData(x)[eval(j), ]\n            } else if (.tryCatchNoWarn(is.character(j), error=function(e) FALSE, silent=TRUE)) {\n                idCols <- colIDs(x, key=TRUE)\n                if (max(unlist(lapply(j, .strSplitLength, split=':'))) > length(idCols))\n                    stop(cyan$bold('Attempting to select more ID columns than there are in the LongTable.\\n\\tPlease use query of the form ', paste0(idCols, collapse=':')))\n                jmatch <- colnames(x) %in% j\n                if (!any(jmatch))\n                    jmatch <- grepl(.preprocessRegexQuery(j), colnames(x), ignore.case=TRUE)\n                jmatch <- str2lang(.variableToCodeString(jmatch))\n                colDataSubset <- .colData(x)[eval(jmatch), ]\n            } else {\n                jsub <- substitute(j)\n                colDataSubset <- .tryCatchNoWarn(.colData(x)[j, ],\n                    error=function(e) .colData(x)[eval(jsub), ])\n            }\n        } else {\n            colDataSubset <- .colData(x)\n        }\n\n        # Subset assays to only keys in remaining in rowData/colData\n        rows <- rowDataSubset$rowKey\n        cols <- colDataSubset$colKey\n\n        # -- find matching assays\n        validAssays <- assays %in% assayNames(x)\n        if (any(!validAssays))\n            warning(.warnMsg(assays[!validAssays],\n                \" are not valid assay names, ignoring...\"), call.=FALSE)\n        keepAssays <- assayNames(x) %in% assays\n\n        # -- subset index, then use index to subset x\n        assayKeys <- paste0(\".\", assayNames(x)[keepAssays])\n        idx <- mutable(getIntern(x, \"assayIndex\"))[\n            rowKey %in% rows & colKey %in% cols,\n            .SD,\n            .SDcols=c(\"rowKey\", \"colKey\", assayKeys)\n        ]\n        # -- drop rowKeys or colKeys which no longer have any assay observation\n        #   after the initial subset, fixes #148\n        validKeys <- idx[\n            which(rowAnys(!is.na(idx[, assayKeys, with=FALSE]))),\n            .(rowKey, colKey)\n        ]\n        idx <- idx[\n            rowKey %in% unique(validKeys$rowKey) &\n                colKey %in% unique(validKeys$colKey),\n        ]\n        assays(x, raw=TRUE)[!keepAssays] <- NULL  # delete assays being dropped\n\n        return(.subsetByIndex(x, idx, reindex=reindex))\n    }\n)",
        "complete": "# Subset method for a LongTable object\nsetMethod('subset', signature('LongTable'),\n    function(x, i, j, assays=assayNames(x), reindex=TRUE) {\n        x <- copy(x)\n\n        .rowData <- function(...) rowData(..., key=TRUE)\n        .colData <- function(...) colData(..., key=TRUE)\n        .tryCatchNoWarn <- function(...) suppressWarnings(tryCatch(...))\n        .strSplitLength <- function(...) length(unlist(strsplit(...)))\n\n        if (!missing(i)) {\n            if (.tryCatchNoWarn(is.call(i), error=function(e) FALSE)) {\n                rowDataSubset <- .rowData(x)[eval(i), ]\n            } else if (.tryCatchNoWarn(is.character(i), error=function(e) FALSE)) {\n                idCols <- rowIDs(x, key=TRUE)\n                if (max(unlist(lapply(i, .strSplitLength, split=':'))) > length(idCols))\n                    stop(cyan$bold('Attempting to select more rowID columns than there are in the LongTable.\\n\\tPlease use query of the form ', paste0(idCols, collapse=':')))\n                imatch <- rownames(x) %in% i\n                if (!any(imatch))\n                    imatch <- grepl(.preprocessRegexQuery(i), rownames(x), ignore.case=TRUE)\n                imatch <- str2lang(.variableToCodeString(imatch))\n                rowDataSubset <- .rowData(x)[eval(imatch), ]\n            } else {\n                isub <- substitute(i)\n                rowDataSubset <- .tryCatchNoWarn(.rowData(x)[i, ],\n                    error=function(e) .rowData(x)[eval(isub), ])\n            }\n        } else {\n            rowDataSubset <- .rowData(x)\n        }\n\n        if (!missing(j)) {\n            if (.tryCatchNoWarn(is.call(j), error=function(e) FALSE, silent=TRUE)) {\n                colDataSubset <- .colData(x)[eval(j), ]\n            } else if (.tryCatchNoWarn(is.character(j), error=function(e) FALSE, silent=TRUE)) {\n                idCols <- colIDs(x, key=TRUE)\n                if (max(unlist(lapply(j, .strSplitLength, split=':'))) > length(idCols))\n                    stop(cyan$bold('Attempting to select more ID columns than there are in the LongTable.\\n\\tPlease use query of the form ', paste0(idCols, collapse=':')))\n                jmatch <- colnames(x) %in% j\n                if (!any(jmatch))\n                    jmatch <- grepl(.preprocessRegexQuery(j), colnames(x), ignore.case=TRUE)\n                jmatch <- str2lang(.variableToCodeString(jmatch))\n                colDataSubset <- .colData(x)[eval(jmatch), ]\n            } else {\n                jsub <- substitute(j)\n                colDataSubset <- .tryCatchNoWarn(.colData(x)[j, ],\n                    error=function(e) .colData(x)[eval(jsub), ])\n            }\n        } else {\n            colDataSubset <- .colData(x)\n        }\n\n        rows <- rowDataSubset$rowKey\n        cols <- colDataSubset$colKey\n\n        validAssays <- assays %in% assayNames(x)\n        if (any(!validAssays))\n            warning(.warnMsg(assays[!validAssays],\n                \" are not valid assay names, ignoring...\"), call.=FALSE)\n        keepAssays <- assayNames(x) %in% assays\n\n        assayKeys <- paste0(\".\", assayNames(x)[keepAssays])\n        idx <- mutable(getIntern(x, \"assayIndex\"))[\n            rowKey %in% rows & colKey %in% cols,\n            .SD,\n            .SDcols=c(\"rowKey\", \"colKey\", assayKeys)\n        "
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/CoreSet-accessors.R",
    "language": "R",
    "content": "# Navigating this file:\n# - Slot section names start with ----\n# - Method section names start with ==\n#\n# As a result, you can use Ctrl + f to find the slot or method you are looking\n# for quickly, assuming you know its name.\n#\n# For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n# method, while Ctrl + f '---- molecularProfiles' would take you to the slot\n# section.\n\n#' @include CoreSet-class.R allGenerics.R LongTable-class.R\nNULL\n\n.local_class <- 'CoreSet'\n.local_data <- 'clevelandSmall_cSet'\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n#' @noRd\n.docs_CoreSet_accessors <- function(...) .parseToRoxygen(\n    \"\n    @title Accessing and modifying information in a `{class_}`\n\n    @description\n    Documentation for the various setters and getters which allow manipulation\n    of data in the slots of a `{class_}` object.\n\n    @param object A `{class_}` object.\n    @param value See details.\n    @param mDataType `character(1)` The name of a molecular datatype to access\n    from the `molecularProfiles` of a `{class_}` object.\n    @param assay `character(1)` A valid assay name in the `SummarizedExperiment`\n    of `@molecularProfiles` of a {class_} object for data type `mDataType`.\n    @param dimension See details.\n    @param ... See details.\n\n    @return Accessors: See details.\n    @return Setters: An updated `{class_}` object, returned invisibly.\n    \",\n    ...\n)\n\n#' @name CoreSet-accessors\n#' @eval .docs_CoreSet_accessors(class_='CoreSet')\n#' @eval .parseToRoxygen(\"@examples data({data_})\", data_=.local_data)\nNULL\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ====================\n## ---- annotation slot\n## --------------------\n\n\n##\n## == annotation\n\n\n#' @noRd\n.docs_CoreSet_get_annotation <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    ## @annotation\n\n    __annotation__: A `list` of {class_} annotations with items: 'name',\n    the name of the object; 'dateCreated', date the object was created; 'sessionInfo',\n    the `sessionInfo()` when the object was created; 'call', the R constructor call;\n    and 'version', the object version.\n\n    @examples\n\n    ## @annotation\n\n    annotation({data_})\n\n    @md\n    @importMethodsFrom BiocGenerics annotation\n    @aliases annotation\n    @exportMethod annotation\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_annotation(class_=.local_class, data_=.local_data)\nsetMethod('annotation', signature(\"CoreSet\"), function(object) {\n    object@annotation\n})\n\n\n#' @noRd\n.docs_CoreSet_set_annotation <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __annotation<-__: Setter method for the annotation slot. Arguments:\n    - value: a `list` of annotations to update the {class_} with.\n\n    @examples\n    annotation({data_}) <- annotation({data_})\n\n    @md\n    @importMethodsFrom BiocGenerics annotation<-\n    @aliases annotation<-\n    @exportMethod annotation<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_annotation(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"annotation\", signature(\"CoreSet\", \"list\"),\n    function(object, value)\n{\n    object@annotation <- value\n    object\n})\n\n\n##\n## == dateCreated\n\n\n#' @export\nsetGeneric(\"dateCreated\", function(object, ...) standardGeneric(\"dateCreated\"))\n\n.docs_CoreSet_get_dateCreated <- function(...) .parseToRoxygen(\n    \"\n    @details\n    ## @dateCreated\n    __dateCreated__: `character(1)` The date the `{class_}` object was\n    created, as returned by the `date()` function.\n    @examples\n    dateCreated({data_})\n\n    @md\n    @aliases dateCreated,{class_}-method dateCreated\n    @exportMethod dateCreated\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_dateCreated(class_=.local_class, data_=.local_data)\nsetMethod('dateCreated', signature(\"CoreSet\"), function(object) {\n    object@annotation$dateCreated\n})\n\n\n#' @export\nsetGeneric(\"dateCreated<-\", function(object, ..., value)\n    standardGeneric(\"dateCreated<-\"))\n\n.docs_CoreSet_set_dateCreated <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __dateCreated<-__: Update the 'dateCreated' item in the `annotation` slot of\n    a `{class_}` object. Arguments:\n    - value: A `character(1)` vector, as returned by the `date()` function.\n    @examples\n    ## dateCreated\n    dateCreated({data_}) <- date()\n\n    @md\n    @aliases dateCreated<-,{class_}-method dateCreated<-\n    @exportMethod dateCreated<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_dateCreated(class_=.local_class, data_=.local_data)\nsetReplaceMethod('dateCreated', signature(object=\"CoreSet\", value=\"character\"),\n    function(object, value)\n{\n    ## TODO:: Error handling - do we ever want to allow a datetime object?\n    funContext <- .funContext('dateCreated')\n    if (length(value) > 1) .error(funContext, 'dateCreated must by a character\n        vector of length 1, as returned by the `date()` function.')\n    object@annotation$dateCreated <- value\n    return(object)\n})\n\n\n##\n## == name\n\n\n#' @export\nsetGeneric(\"name\", function(object, ...) standardGeneric(\"name\"))\n\n.docs_CoreSet_get_name <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __name__: `character(1)` The name of the `{class_}`, retreived from\n    the `@annotation` slot.\n\n    @examples\n    name({data_})\n\n    @md\n    @aliases name,{class_}-method name\n    @exportMethod name\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_name(class_=.local_class, data_=.local_data)\nsetMethod('name', signature(\"CoreSet\"), function(object) {\n    return(object@annotation$name)\n})\n\n#' @export\nsetGeneric(\"name<-\", function(object, ..., value) standardGeneric(\"name<-\"))\n\n#' @noRd\n.docs_CoreSet_set_name <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __name<-__: Update the `@annotation$name` value in a `{class_}`  object.\n    - value: `character(1)` The name of the `{class_}` object.\n\n    @examples\n    name({data_}) <- 'new_name'\n\n    @md\n    @aliases name<-,{class_},character-method name<-\n    @exportMethod name<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_name(class_=.local_class, data_=.local_data)\nsetReplaceMethod('name', signature(\"CoreSet\"), function(object, value) {\n    object@annotation$name <- value\n    return(object)\n})\n\n\n## ==============\n## ---- sample slot\n\n\n##\n## == sampleInfo\n\n\n#' @export\nsetGeneric(\"sampleInfo\", function(object, ...) standardGeneric(\"sampleInfo\"))\n\n#' @noRd\n.docs_CoreSet_get_sampleInfo <- function(...) .parseToRoxygen(\n    \"\n    ## @sample\n    @details\n    __{sample_}Info__: `data.frame` Metadata for all sample in a `{class_}` object.\n\n    @md\n    @aliases\n    sampleInfo,{class_}-method sampleInfo\n    {sample_}Info,{class_}-method\n    {sample_}Info\n    @exportMethod sampleInfo\n    \",\n    ...\n)\n\n\n.local_sample <- \"cell\"\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sampleInfo(class_=.local_class, sample_=.local_sample)\nsetMethod(\"sampleInfo\", \"CoreSet\", function(object) {\n    object@sample\n})\n#' @export\ncellInfo <- function(...) sampleInfo(...)\n\n#' @export\nsetGeneric(\"sampleInfo<-\", function(object, value) standardGeneric(\"sampleInfo<-\"))\n\n#' @noRd\n.docs_CoreSet_set_sampleInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sampleInfo<-__: assign updated sample annotations to the `{class_}`\n    object.\n    Arguments:\n    - value: a `data.frame` object.\n    @examples\n    sampleInfo({data_}) <- sampleInfo({data_})\n\n    @md\n    @aliases\n    sampleInfo<-,{class_},data.frame-method\n    sampleInfo<-\n    {sample_}Info<-,{class_},data.frame-method\n    {sample_}Info<-\n    @exportMethod sampleInfo<-\n    \",\n    ...\n)\n\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sampleInfo(class_=.local_class, data_=.local_data,\n#'  sample_=.local_sample)\nsetReplaceMethod(\"sampleInfo\", signature(object=\"CoreSet\", value=\"data.frame\"),\n        function(object, value) {\n    funContext <- .funContext('::sampleInfo')\n    if (is.null(rownames(value)))\n    .error(funContext, \"Please provide the sampleid as rownames for the sample\n        annotations\")\n    object@sample <- value\n    object\n})\n#' @export\n`cellInfo<-` <- function(object, value) `sampleInfo<-`(object, value=value)\n\n\n##\n## == sampleNames\n\n## TODO: Implement an actual @sample slot instead of using @sample  and aliases\n\n\n\n#' @noRd\n.docs_CoreSet_get_sampleNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sampleNames__: `character` Retrieve the rownames of the `data.frame` in\n    the `sample` slot from a {class_} object.\n    @examples\n    sampleNames({data_})\n\n    @md\n    @aliases\n    sampleName,{class_}-method\n    sampleNames\n    {sample_}Name,{class_}-method\n    {sample_}Names\n    @exportMethod sampleNames\n    \",\n    ...\n)\n\n#' @importMethodsFrom Biobase sampleNames\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sampleNames(class_=.local_class, data_=.local_data,\n#' sample_=.local_sample)\nsetMethod(\"sampleNames\", signature(\"CoreSet\"), function(object) {\n    rownames(sampleInfo(object))\n})\n#' @export\ncellNames <- function(object) sampleNames(object)\n\n\n#' @noRd\n.docs_CoreSet_set_sampleNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sampleNames<-__: assign new rownames to the sampleInfo `data.frame` for\n    a {class_} object.\n    Arguments:\n    - value: `character` vector of rownames for the `sampleInfo(object)` `data.frame`.\n    @examples\n    sampleNames({data_}) <- sampleNames({data_})\n\n    @md\n    @aliases\n    sampleNames<-,{class_},list-method\n    sampleNames<-\n    {sample_}Names<-,{class_},list-method\n    {sample_}Names<-\n    @exportMethod sampleNames<-\n    \",\n    ...\n)\n\n\n#' @importMethodsFrom Biobase sampleNames<-\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sampleNames(class_=.local_class, data_=.local_data,\n#' sample_=.local_sample)\nsetReplaceMethod(\"sampleNames\", signature(object=\"CoreSet\", value=\"character\"),\n        function(object, value) {\n    ## TODO: does updateSampleId also update slots other than sample?\n    object <- updateSampleId(object, value)\n    return(object)\n})\n#' @export\n`cellNames<-` <- function(object, value) `sampleNames<-`(object, value=value)\n\n\n## -------------------\n## ---- treatment slot\n\n## TODO: Implement an actual @treatment slot to replace @treatment and @radiation\n\n#\n# == treatmentInfo\n\n#' @export\nsetGeneric('treatmentInfo', function(object, ...)\n    standardGeneric('treatmentInfo'))\n\n#' @noRd\n.docs_CoreSet_get_treatmentInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __treatmentInfo__: `data.frame` Metadata for all treatments in a `{class_}`\n    object. Arguments:\n    - object: `{class_}` An object to retrieve treatment metadata from.\n\n    @examples\n    treatmentInfo({data_})\n\n    @md\n    @aliases treatmentInfo,{class_}-method treatmentInfo\n    @exportMethod treatmentInfo\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_treatmentInfo(class_=.local_class, data_=.local_data)\nsetMethod('treatmentInfo', signature('CoreSet'), function(object) {\n    treatmentType <- switch(class(object)[1],\n        'PharmacoSet'='treatment',\n        'ToxicoSet'='treatment',\n        'RadioSet'='radiation',\n        'CoreSet'='treatment'\n    )\n    package <- gsub('Set', 'Gx', class(object)[1])\n    if (\"treatment\" %in% slotNames(object)) return(object@treatment)\n    treatmentInfo <- get(paste0(treatmentType, 'Info'),\n        envir=asNamespace(package))\n    treatmentInfo(object)\n})\n\n#' @export\nsetGeneric('treatmentInfo<-', function(object, ..., value)\n    standardGeneric('treatmentInfo<-'))\n\n#' @noRd\n.docs_CoreSet_set_treatmentInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __treatmentInfo<-__: `{class_}` object with updated treatment metadata.\n    object. Arguments:\n    - object: `{class_}` An object to set treatment metadata for.\n    - value: `data.frame` A new table of treatment metadata for `object`.\n\n    @examples\n    treatmentInfo({data_}) <- treatmentInfo({data_})\n\n    @md\n    @aliases treatmentInfo<-,{class_},data.frame-method treatmentInfo<-\n    @exportMethod treatmentInfo<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_treatmentInfo(class_=.local_class, data_=.local_data)\nsetReplaceMethod('treatmentInfo', signature(object='CoreSet',\n        value='data.frame'), function(object, value) {\n    object@treatment <- value\n    return(invisible(object))\n})\n\n##\n## == treatmentNames\n\n\n#' @export\nsetGeneric('treatmentNames', function(object, ...)\n    standardGeneric('treatmentNames'))\n\n#' @noRd\n.docs_CoreSet_get_treatmentNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __treatmentNames__: `character` Names for all treatments in a `{class_}`\n    object. Arguments:\n    - object: `{class_}` An object to retrieve treatment names from.\n\n    @examples\n    treatmentNames({data_})\n\n    @md\n    @aliases treatmentNames,{class_}-method treatmentNames\n    @exportMethod treatmentNames\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_treatmentNames(class_=.local_class, data_=.local_data)\nsetMethod('treatmentNames', signature(object='CoreSet'), function(object) {\n    rownames(treatmentInfo(object))\n})\n\n\n#' @export\nsetGeneric('treatmentNames<-', function(object, ..., value)\n    standardGeneric('treatmentNames<-'))\n\n#' @noRd\n.docs_CoreSet_set_treatmentNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __treatmentNames<-__: `{class_}` Object with updates treatment names.\n    object. Arguments:\n    - object: `{class_}` An object to set treatment names from.\n    - value: `character` A character vector of updated treatment names.\n\n    @examples\n    treatmentNames({data_}) <- treatmentNames({data_})\n\n    @md\n    @aliases treatmentNames<-,{class_},character-method treatmentNames<-\n    @exportMethod treatmentNames<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_treatmentNames(class_=.local_class, data_=.local_data)\nsetReplaceMethod('treatmentNames',\n        signature(object='CoreSet', value='character'),\n        function(object, value) {\n    object <- updateTreatmentId(object, new.ids=value)\n    return(invisible(object))\n})\n\n## ------------------\n## ---- curation slot\n\n\n##\n## == curation\n\n\n#' @export\nsetGeneric(\"curation\", function(object, ...) standardGeneric(\"curation\"))\n\n#' @noRd\n.docs_CoreSet_get_curation <- function(...) .parseToRoxygen(\n    \"\n    @details\n    ## @curation\n    __curation__: A `list` of curated mappings between identifiers in the\n    {class_} object and the original data publication. {details_}\n    @examples\n    ## curation\n    curation({data_})\n\n    @md\n    @aliases curation,{class_}-method curation\n    @exportMethod curation\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_curation(class_=.local_class, data_=.local_data,\n#' details_=\"Contains two `data.frame`s, 'sample' with sample ids and\n#' 'tissue' with tissue ids.\")\nsetMethod('curation', signature(object=\"CoreSet\"), function(object) {\n    object@curation\n})\n\n#' @export\nsetGeneric(\"curation<-\", function(object, ..., value)\n    sstandardGeneric(\"curation<-\"))\n\n#' @noRd\n.docs_CoreSet_set_curation <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __curation<-__: Update the `curation` slot of a {class_} object. Arugments:\n    - value: A `list` of `data.frame`s, one for each type of curated\n    identifier. {details_}\n    @examples\n    curation({data_}) <- curation({data_})\n\n    @md\n    @aliases curation<-,{class_},list-method curation<-\n    @exportMethod curation<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_curation(class_=.local_class, data_=.local_data,\n#' details_=\"For a `CoreSet` object the slot should contain tissue and\n#' sample id `data.frame`s.\")\nsetReplaceMethod(\"curation\", signature(object=\"CoreSet\", value=\"list\"),\n    function(object, value)\n{\n    object@curation <- value\n    object\n})\n\n\n\n## ----------------------\n## ---- datasetType slot\n\n\n#\n# == datasetType\n\n\n#' @export\nsetGeneric(\"datasetType\", function(object, ...) standardGeneric(\"datasetType\"))\n\n#' @noRd\n.docs_CoreSet_get_datasetType <- function(...) .parseToRoxygen(\n    \"\n    @details\n    ## datasetType slot\n    __datasetType__: `character(1)` The type treatment response in the\n    `sensitivity` slot. Valid values are 'sensitivity', 'perturbation' or 'both'.\n    @examples\n    datasetType({data_})\n\n    @md\n    @aliases datasetType,{class_}-method datasetType\n    @exportMethod datasetType\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_datasetType(class_=.local_class, data_=.local_data)\nsetMethod(\"datasetType\", signature(\"CoreSet\"), function(object) {\n    object@datasetType\n})\n\n\n#' @export\nsetGeneric(\"datasetType<-\",  function(object, value)\n    standardGeneric(\"datasetType<-\"))\n\n#' @noRd\n.docs_CoreSet_set_datasetType <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __datasetType<-__: Update the datasetType slot of a {class_} object.\n    Arguments:\n    - value: A `character(1)` vector with one of 'sensitivity', 'perturbation'\n    or 'both'\n    @examples\n    datasetType({data_}) <- 'both'\n\n    @md\n    @aliases datasetType<-,{class_},character-method datasetType<-\n    @export\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_datasetType(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"datasetType\", signature(object=\"CoreSet\", value='character'),\n    function(object, value)\n{\n    funContext <- .funContext('::datasetType,CoreSet,character-method')\n    if (length(value) > 1) .error(funContext,\n        'datasetType must be a character vector of length 1.')\n    if (!is.element(value, c('sensitivity', 'perturbation', 'both')))\n        .error(funContext, 'datasetType must be one of \"sensitivity\",\n            \"perturbation\" or \"both\".')\n    object@datasetType <- value\n    object\n})\n\n\n\n## ---------------------------\n## ---- molecularProfiles slot\n\n\n##\n## == molecularProfiles\n\n\n#' @export\nsetGeneric(\"molecularProfiles\", function(object, mDataType, assay, ...)\n    standardGeneric(\"molecularProfiles\"))\n\n#' @noRd\n.docs_CoreSet_get_molecularProfiles <- function(...) .parseToRoxygen(\n    \"\n    @details\n    ## @molecularProfiles\n    __molecularProfiles__: `matrix()` Retrieve an assay in a\n    `SummarizedExperiment` from the `molecularProfiles` slot of a `{class_}`\n    object with the specified `mDataType`. Valid `mDataType` arguments can be\n    found with `mDataNames(object)`. Exclude `mDataType` and `assay` to\n    access the entire slot. Arguments:\n    - assay: Optional `character(1)` vector specifying an assay in the\n    `SummarizedExperiment` of the `molecularProfiles` slot of the\n    `{class_}` object for the specified `mDataType`. If excluded,\n    defaults to modifying the first assay in the `SummarizedExperiment` for\n    the given `mDataType`.\n\n    @md\n    @aliases molecularProfiles,{class_}-method molecularProfiles\n    @importClassesFrom S4Vectors DataFrame List\n    @importFrom S4Vectors DataFrame\n    @importFrom SummarizedExperiment colData assay assayNames\n    @exportMethod molecularProfiles\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_molecularProfiles(class_=.local_class, data_=.local_data)\nsetMethod(molecularProfiles, \"CoreSet\", function(object, mDataType, assay) {\n    funContext <- .funContext(paste0('::molecularProlfiles,', class(object), '-method'))\n    if (missing(mDataType) && missing(assay)) return(object@molecularProfiles)\n    if (mDataType %in% names(object@molecularProfiles)) {\n        if (!missing(assay)) {\n            if (assay %in% assayNames(object@molecularProfiles[[mDataType]])) {\n                return(SummarizedExperiment::assay(object@molecularProfiles[[mDataType]], assay))\n            } else {\n                .error(funContext, (paste('Assay', assay, 'not found in the SummarizedExperiment object!')))\n            }\n        } else {\n            return(SummarizedExperiment::assay(object@molecularProfiles[[mDataType]], 1))\n        }\n    } else {\n        stop(paste0('mDataType ', mDataType, ' not found the object!'))\n    }\n})\n\n#' @export\nsetGeneric(\"molecularProfiles<-\", function(object, mDataType, assay, value)\n    standardGeneric(\"molecularProfiles<-\"))\n\n#' @noRd\n.docs_CoreSet_set_molecularProfiles <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __molecularProfiles<-__: Update an assay in a `SummarizedExperiment` from\n    the `molecularProfiles` slot of a {class_} object with the specified\n    `mDataType`. Valid `mDataType` arguments can be found with\n    `mDataNames(object)`. Omit `mDataType` and `assay` to update the slot.\n    - assay: Optional `character(1)` vector specifying an assay in the\n    `SummarizedExperiment` of the `molecularProfiles` slot of the\n    `{class_}` object for the specified `mDataType`. If excluded,\n    defaults to modifying the first assay in the `SummarizedExperiment` for\n    the given `mDataType`.\n    - value: A `matrix` of values to assign to the `assay` slot of the\n    `SummarizedExperiment` for the selected `mDataType`. The rownames and\n    column names must match the associated `SummarizedExperiment`.\n    @examples\n    # No assay specified\n    molecularProfiles({data_}, 'rna') <- molecularProfiles({data_}, 'rna')\n\n    # Specific assay\n    molecularProfiles({data_}, 'rna', 'exprs') <-\n        molecularProfiles({data_}, 'rna', 'exprs')\n\n    # Replace the whole slot\n    molecularProfiles({data_}) <- molecularProfiles({data_})\n\n    @md\n    @aliases molecularProfiles<-,{class_},character,character,matrix-method\n    molecularProfiles<-,{class_},character,missing,matrix-method\n    molecularProfiles<-,{class_},missing,missing,list-method\n    molecularProfiles<-,{class_},missing,missing,MutliAssayExperiment-method\n    molecularProfiles<-\n    @importFrom SummarizedExperiment assay\n    @exportMethod molecularProfiles<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_molecularProfiles(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"molecularProfiles\", signature(object=\"CoreSet\",\n    mDataType =\"character\", assay=\"character\", value=\"matrix\"),\n    function(object, mDataType, assay, value)\n{\n    if (mDataType %in% names(object@molecularProfiles)) {\n        assay(object@molecularProfiles[[mDataType]], assay) <- value\n    }\n    object\n})\n#' @rdname CoreSet-accessors\nsetReplaceMethod(\"molecularProfiles\",\n    signature(object=\"CoreSet\", mDataType =\"character\", assay=\"missing\",\n        value=\"matrix\"), function(object, mDataType, assay, value)\n{\n    if (mDataType %in% names(object@molecularProfiles)) {\n        assay(object@molecularProfiles[[mDataType]], 1) <- value\n    }\n    object\n})\n#' @rdname CoreSet-accessors\nsetReplaceMethod(\"molecularProfiles\", signature(object=\"CoreSet\",\n        mDataType=\"missing\", assay=\"missing\", value=\"list_OR_MAE\"),\n        function(object, mDataType, assay, value) {\n    object@molecularProfiles <- value\n    object\n})\n\n\n##\n## == featureInfo\n\n\n#' @export\nsetGeneric(\"featureInfo\", function(object, mDataType, ...)\n    standardGeneric(\"featureInfo\"))\n\n#' @noRd\n.docs_CoreSet_get_featureInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __featureInfo__: Retrieve a `DataFrame` of feature metadata for the specified\n    `mDataType` from the `molecularProfiles` slot of a `{class_}` object. More\n    specifically, retrieve the `@rowData` slot from the `SummarizedExperiment`\n    from the `@molecularProfiles` of a `{class_}` object with the name\n    `mDataType`.\n    @examples\n    featureInfo({data_}, 'rna')\n\n    @md\n    @aliases featureInfo,{class_}-method featureInfo\n    @importFrom SummarizedExperiment rowData rowData<-\n    @exportMethod featureInfo\n    \",\n    ...\n)\n\n\n## FIXME: Why return NULL and not throw and error instead? Or at least a warning.\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_featureInfo(class_=.local_class, data_=.local_data)\nsetMethod(featureInfo, \"CoreSet\", function(object, mDataType) {\n    if (mDataType %in% names(object@molecularProfiles)) {\n        return(rowData(object@molecularProfiles[[mDataType]]))\n    } else{\n        return(NULL)\n    }\n})\n\n#' @export\nsetGeneric(\"featureInfo<-\", function(object, mDataType, value)\n    standardGeneric(\"featureInfo<-\"))\n\n#' @noRd\n.docs_CoreSet_set_featureInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __featureInfo<-__: Update the `featureInfo(object, mDataType)` `DataFrame`\n    with new feature metadata. Arguments:\n    - value: A `data.frame` or `DataFrame` with updated feature metadata for\n    the specified molecular profile in the `molecularProfiles` slot of a\n    `{class_}` object.\n    @examples\n    featureInfo({data_}, '{mDataType_}') <- featureInfo({data_}, '{mDataType_}')\n\n    @aliases featureInfo<-,{class_},character,data.frame-method\n    featureInfo<-,{class_},character,DataFrame-method featureInfo<-\n    @importFrom SummarizedExperiment rowData rowData<-\n    @importFrom S4Vectors DataFrame\n    @exportMethod featureInfo<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_featureInfo(class_=.local_class, data_=.local_data,\n#'   mDataType_='rna')\nsetReplaceMethod(\"featureInfo\", signature(object=\"CoreSet\",\n    mDataType =\"character\",value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    if (mDataType %in% names(object@molecularProfiles)) {\n        rowData(object@molecularProfiles[[mDataType]]) <-\n            DataFrame(value, rownames = rownames(value))\n    }\n    object\n})\nsetReplaceMethod(\"featureInfo\", signature(object=\"CoreSet\",\n    mDataType =\"character\",value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    if (mDataType %in% names(object@molecularProfiles)) {\n        rowData(object@molecularProfiles[[mDataType]]) <-\n            DataFrame(value, rownames = rownames(value))\n    }\n    object\n})\n\n\n##\n## == phenoInfo\n\n\n#' @export\nsetGeneric(\"phenoInfo\", function(object, mDataType, ...)\n    standardGeneric(\"phenoInfo\"))\n\n#' @noRd\n.docs_CoreSet_get_phenoInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __phenoInfo__: Return the `@colData` slot from the `SummarizedExperiment` of\n    `mDataType`, containing sample-level metadata, from a `{class_}` object.\n\n    @examples\n    phenoInfo({data_}, '{mDataType_}')\n\n    @md\n    @importFrom SummarizedExperiment colData\n    @aliases phenoInfo\n    @exportMethod phenoInfo\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_phenoInfo(class_=.local_class, data_=.local_data, mDataType_='rna')\nsetMethod(phenoInfo, signature(object='CoreSet', mDataType='character'),\n    function(object, mDataType)\n{\n    if (mDataType %in% mDataNames(object)) { # Columns = Samples\n        return(colData(object@molecularProfiles[[mDataType]]))\n    }else{\n        ## FIXME:: Is there a reason we throw a NULL instead of an error?\n        return(NULL)\n    }\n})\n\n#' @export\nsetGeneric(\"phenoInfo<-\", function(object, mDataType, value)\n    standardGeneric(\"phenoInfo<-\"))\n\n#' @noRd\n.docs_CoreSet_set_phenoInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __phenoInfo<-__: Update the `@colData` slot of the `SummarizedExperiment`\n    of `mDataType` in the `@molecularProfiles` slot of a `{class_}` object.\n    This updates the sample-level metadata in-place.\n    - value: A `data.frame` or `DataFrame` object where rows are samples\n    and columns are sample metadata.\n\n    @examples\n    phenoInfo({data_}, '{mDataType_}') <- phenoInfo({data_}, '{mDataType_}')\n\n    @md\n    @importFrom SummarizedExperiment colData colData<-\n    @importFrom S4Vectors DataFrame\n    @aliases phenoInfo<-,{class_},character,data.frame-method\n    phenoInfo<-,{class_},character,DataFrame-method phenoInfo<-\n    @exportMethod phenoInfo<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_phenoInfo(class_=.local_class, data_=.local_data, mDataType_='rna')\nsetReplaceMethod(\"phenoInfo\", signature(object=\"CoreSet\", mDataType =\"character\",\n    value=\"data.frame\"), function(object, mDataType, value)\n{\n    if(mDataType %in% mDataNames(object)) {\n        colData(object@molecularProfiles[[mDataType]]) <-\n            DataFrame(value, rownames = rownames(value))\n    }\n    object\n})\nsetReplaceMethod(\"phenoInfo\", signature(object=\"CoreSet\",\n    mDataType =\"character\", value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    if (mDataType %in% mDataNames(object)) {\n        colData(object@molecularProfiles[[mDataType]]) <- value\n    }\n    object\n})\n\n\n##\n## == fNames\n\n#' @export\nsetGeneric('fNames', function(object, mDataType, ...) standardGeneric('fNames'))\n\n#' @noRd\n.docs_CoreSet_get_fNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __fNames__: `character()` The features names from the `rowData` slot of a\n    `SummarizedExperiment` of `mDataType` within a `{class_}` object.\n\n    @examples\n    fNames({data_}, '{mDataType_}')\n\n    @md\n    @aliases fNames,{class_},character-method fNames\n    @exportMethod fNames\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_fNames(class_=.local_class, data_=.local_data,\n#' mDataType_='rna')\nsetMethod('fNames', signature(object='CoreSet', mDataType='character'),\n    function(object, mDataType)\n{\n    rownames(featureInfo(object, mDataType))\n})\n\n\n#' @export\nsetGeneric('fNames<-', function(object, mDataType, ..., value)\n    standardGeneric('fNames<-'))\n\n#' @noRd\n.docs_CoreSet_set_fNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __fNames__: Updates the rownames of the feature metadata (i.e., `rowData`)\n    for a `SummarizedExperiment` of `mDataType` within a `{class_}` object.\n    - value: `character()` A character vector of new features names for the\n    `rowData` of the `SummarizedExperiment` of `mDataType` in the\n    `@molecularProfiles` slot of a `{class_}` object. Must be the same\n    length as `nrow(featureInfo(object, mDataType))`,\n    the number of rows in the feature metadata.\n\n    @examples\n    fNames({data_}, '{mDataType_}') <- fNames({data_}, '{mDataType_}')\n\n    @md\n    @aliases fNames<-,{class_},character,character-method fNames<-\n    @exportMethod fNames<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_fNames(class_=.local_class, data_=.local_data,\n#' mDataType_='rna')\nsetReplaceMethod('fNames', signature(object='CoreSet', mDataType='character',\n    value='character'), function(object, mDataType, value)\n{\n    rownames(featureInfo(object, mDataType)) <- value\n    object\n})\n\n\n##\n## == mDataNames\n\n\n#' @export\nsetGeneric(\"mDataNames\", function(object, ...) standardGeneric(\"mDataNames\"))\n\n#' @noRd\n.docs_CoreSet_get_mDataNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __mDataNames__: `character` Retrieve the names of the molecular data types\n    available in the `molecularProfiles` slot of a `{class_}` object. These\n    are the options which can be used in the `mDataType` parameter of various\n    `molecularProfiles` slot accessors methods.\n    @examples\n    mDataNames({data_})\n\n    @md\n    @aliases mDataNames,{class_}-method mDataNames\n    @exportMethod mDataNames\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_mDataNames(class_=.local_class, data_=.local_data)\nsetMethod(\"mDataNames\", \"CoreSet\", function(object) {\n    return(names(object@molecularProfiles))\n})\n\n\n#' @export\nsetGeneric(\"mDataNames<-\", function(object, ..., value) standardGeneric(\"mDataNames<-\"))\n\n#' @noRd\n.docs_CoreSet_set_mDataNames <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __mDataNames__: Update the molecular data type names of the\n    `molecularProfiles` slot of a {class_} object. Arguments:\n    - value: `character` vector of molecular datatype names, with length\n    equal to `length(molecularProfilesSlot(object))`.\n    @examples\n    mDataNames({data_}) <- mDataNames({data_})\n\n    @md\n    @aliases mDataNames<-,{class_},ANY-method mDataNames<-\n    @exportMethod mDataNames<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_mDataNames(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"mDataNames\", \"CoreSet\", function(object, value) {\n    names(object@molecularProfiles) <- value\n    return(object)\n})\n\n##\n## == molecularProfilesSlot\n\n\n#' @export\nsetGeneric(\"molecularProfilesSlot\", function(object, ...)\n    standardGeneric(\"molecularProfilesSlot\"))\n\n#' @noRd\n.docs_CoreSet_get_molecularProfilesSlot <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __molecularProfilesSlot__: Return the contents of the `@molecularProfiles`\n    slot of a `{class_}` object. This will either be a `list` or\n    `MultiAssayExperiment` of `SummarizedExperiment`s.\n\n    @examples\n    molecularProfilesSlot({data_})\n\n    @md\n    @aliases moleculerProfilesSlot,{class_}-method molecularProfilesSlot\n    @exportMethod molecularProfilesSlot\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_molecularProfilesSlot(class_=.local_class, data_=.local_data)\nsetMethod(\"molecularProfilesSlot\", signature(\"CoreSet\"), function(object) {\n    object@molecularProfiles\n})\n\n\n#' @export\nsetGeneric(\"molecularProfilesSlot<-\",\n    function(object, value) standardGeneric(\"molecularProfilesSlot<-\"))\n\n#' @noRd\n.docs_CoreSet_set_molecularProfilesSlot <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __molecularProfilesSlot<-__: Update the contents of the `@molecularProfiles`\n    slot of a `{class_}` object. Arguemnts:\n    - value: A `list` or `MultiAssayExperiment` of `SummarizedExperiment`s. The\n    `list` and `assays` should be named for the molecular datatype in each\n    `SummarizedExperiment`.\n\n    @examples\n    molecularProfilesSlot({data_}) <- molecularProfilesSlot({data_})\n\n    @md\n    @aliases molecularProfilesSlot<-,{class_},list-method\n    molecularProfilesSlot<-{class_},MultiAssayExperiment-method\n    molecularProfilesSlot<-\n    @exportMethod molecularProfilesSlot<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_molecularProfilesSlot(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"molecularProfilesSlot\", signature(\"CoreSet\", \"list_OR_MAE\"),\n    function(object, value) {\n    # funContext <- .S4MethodContext('molecularProfilesSlot<-', class(object),\n    #     class(value))\n    # if (!is(value, class(object@molecularProfiles)[1])) .error(funContext,\n    #     'The class of value must be the same as the current @molecularProfiles!')\n    object@molecularProfiles <- value\n    object\n})\n\n\n## ---------------------\n## ---- sensitivity slot\n\n\n#\n# == sensitivityInfo\n\n#' @noRd\n.docs_CoreSet_get_sensitivityInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    ## @treatmentResponse\n\n    ### Arguments:\n    - `dimension`: Optional `character(1)` One of 'treatment', 'sample' or\n    'assay' to retrieve `rowData`, `colData` or the 'assay_metadata' assay from\n    the `{class_}` `@sensitvity` `LongTable` object, respectively. Ignored with\n    warning if `@treatmentResponse` is not a `LongTable` object.\n    -  `...`: Additional arguments to the `rowData` or `colData`.\n    `LongTable` methods. Only used if the sensitivity slot contains a\n    `LongTable` object instead of a `list` and the `dimension` argument is\n    specified.\n\n    ### Methods:\n\n    __sensitivityInfo__: `DataFrame` or `data.frame` of sensitivity treatment combo\n    by sample metadata for the `{class_}` object. When the `dimension`\n    parameter is used, it allows retrieval of the dimension specific metadata\n    from the `LongTable` object in `@treatmentResponse` of a {class_} object.\n\n    @examples\n    sensitivityInfo({data_})\n\n    @md\n    @aliases sensitivityInfo,{class_},missing-method\n    sensitivityInfo,{class_},character-method\n    @exportMethod sensitivityInfo\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\nsetMethod(sensitivityInfo, signature(\"CoreSet\"),\n        function(object, dimension, ...) {\n    funContext <- .funContext('::sensitivityInfo')\n    # case where sensitivity slot is a LongTable\n    if (is(treatmentResponse(object), 'LongTable')) {\n        if (!missing(dimension)) {\n            switch(dimension,\n                sample={ return(colData(treatmentResponse(object), ...)) },\n                treatment={ return(rowData(treatmentResponse(object), ...)) },\n                assay={ return(assay(treatmentResponse(object), 'assay_metadata')) },\n                .error(funContext, 'Invalid value for the dimension argument.\n                    Please select on of \"sample\", \"treatment\" or \"assay'))\n        } else {\n            return(.rebuildInfo(treatmentResponse(object)))\n        }\n    # sensitivity is a list\n    } else {\n        if (!missing(dimension))\n            .warning(funContext,' The dimension argument is only valid if the\n                sensitivity slot contains a LongTable object. Ignoring the\n                dimension and ... parameters.')\n        return(treatmentResponse(object)$info)\n    }\n})\n\n\n#' Replicate the $info slot in the old sensitivity list from the new LongTable\n#'   object\n#'\n#' @param longTable `LongTable`\n#'\n#' @keywords internal\n#' @importFrom MatrixGenerics colAlls\n#' @importFrom data.table setkeyv merge.data.table `:=` setDF\n#' @noRd\n.rebuildInfo <- function(longTable) {\n\n    # Extract the information needed to reconstruct the sensitivityInfo\n    #   data.frame\n    aidx <- which(assayNames(longTable) %in% \"assay_metadata\")\n    if (!length(aidx)) aidx <- 1\n    assayIndexDT <- assay(longTable, aidx, key=TRUE)\n    if (aidx == 1) assayIndexDT <- assayIndexDT[, .(rowKey, colKey)]\n    setkeyv(assayIndexDT, c('rowKey', 'colKey'))\n    rowDataDT <- rowData(longTable, key=TRUE)\n    setkeyv(rowDataDT, 'rowKey')\n    colDataDT <- colData(longTable, key=TRUE)\n    setkeyv(colDataDT, 'colKey')\n\n    rowIDcols <- rowIDs(longTable)[!grepl('dose$', rowIDs(longTable))]\n    colIDcols <- colIDs(longTable)\n    rownameCols <- c(rowIDcols, colIDcols)\n\n    # join the tables into the original data\n    infoDT <- merge.data.table(assayIndexDT, rowDataDT, all.x=TRUE)\n    setkeyv(infoDT, 'colKey')\n    infoDT <- merge.data.table(infoDT, colDataDT, all.x=TRUE)[,\n        -c('rowKey', 'colKey')\n    ]\n    infoDT <- tryCatch({\n        infoDT[, .SD, .SDcols=!patterns('treatment.*dose$')]\n    }, error=function(e) infoDT)\n\n    # determine which columns map 1:1 with new identifiers and subset to those\n    infoDT_first <- infoDT[, head(.SD, 1), by=rownameCols]\n    infoDT_last <- infoDT[, tail(.SD, 1), by=rownameCols]\n    keepCols <- colnames(infoDT_first)[\n        colAlls(infoDT_first == infoDT_last, na.rm=TRUE)\n        ]\n    infoDT_sub <- unique(infoDT[, ..keepCols])\n\n    # pad the dropped NA values, if they exists\n    if (\"sensitiivtyInfo_NA\" %in% names(metadata(longTable))) {\n            na_info <- copy(metadata(longTable)$sensitivityInfo_NA)\n        setnames(na_info, \"treatmentid\", \"treatment1id\")\n        na_info <- cbind(\n            na_info,\n            unique(infoDT_sub[, .SD, .SDcols=!patterns(\"^treatment1id$|^sampleid$|^replicate_id$|^rn$\")])\n        )\n        na_info[, replicate_id := seq_len(.N), by=.(treatment1id, sampleid)]\n        infoDT_sub <- rbind(infoDT_sub, na_info)\n    }\n    if (\"experiment_metadata\" %in% names(metadata(longTable))) {\n        infoDT_sub <- cbind(\n            infoDT_sub,\n            as.data.table(metadata(longTable)$experiment_metadata)\n        )\n    }\n\n    # rebuild the rownames\n    idCols <- grep(\"^treatment[0-9]*id\", colnames(infoDT_sub), value=TRUE)\n    infoDT_sub[, treatmentid := Reduce(.paste_slashes, mget(..idCols))]\n    infoDT_sub[, treatment_uid := Reduce(.paste_colon, mget(..rowIDcols))]\n    infoDT_sub[, sample_uid := Reduce(.paste_colon, mget(..colIDcols))]\n    infoDT_sub[, exp_id := Reduce(.paste_, .(treatment_uid, sample_uid))]\n\n    # convert to data.frame\n    setDF(infoDT_sub, rownames=infoDT_sub$exp_id)\n    return(infoDT_sub)\n}\n\n#' @noRd\n.docs_CoreSet_set_sensitivityInfo <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    __sensitivityInfo__<-: Update the `@treatmentResponse` slot metadata for a\n    `{class_}` object. When used without the `dimension` argument is behaves\n    similar to the old {class_} implementation, where the `@treatmentResponse` slot\n    contained a list with a `$info` `data.frame` item. When the `dimension`\n    arugment is used, more complicated assignments can occur where 'sample'\n    modifies the `@sensitvity` `LongTable` colData, 'treatment' the rowData and\n    'assay' the 'assay_metadata' assay.\n    Arguments:\n    - value: A `data.frame` of treatment response experiment metadata,\n    documenting experiment level metadata (mapping to treatments and samples). If\n    the `@treatmentResponse` slot doesn't contain a `LongTable` and `dimension` is\n    not specified, you can only modify existing columns as returned by\n    `sensitivityInfo(object)`.\n    @examples\n    sensitivityInfo({data_}) <- sensitivityInfo({data_})\n\n    @md\n    @aliases sensitivityInfo<-,{class_},missing,data.frame-method\n    sensitvityInfo<-,{class_},character,data.frame-method\n    @import data.table\n    @exportMethod sensitivityInfo<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sensitivityInfo(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"sensitivityInfo\", signature(object=\"CoreSet\", value=\"data.frame\"),\n                function(object, dimension, ..., value) {\n\n    funContext <- .funContext('::sensitivityInfo<-')\n    if (is(treatmentResponse(object), 'LongTable')) {\n        # coerce to data.table\n        if (!is.data.table(value)) value <- data.table(value)\n        if (missing(dimension)) {\n            valueCols <- colnames(value)\n            # get existing column names\n            rowDataCols <- colnames(rowData(object@treatmentResponse))\n            colDataCols <- colnames(colData(object@treatmentResponse))\n            # drop any value columns that don't already exist\n            hasValueCols <- valueCols %in% c(rowDataCols, colDataCols)\n            if (!all(hasValueCols))\n                .message(funContext, 'Dropping columns ',\n                    .collapse(valueCols[!hasValueCols]), ' from value. Currently\n                    this setter only allows modifying existing columns when\n                    @treatmentResponse is a LongTable. For more fine grained updates\n                    please use the dimension argument.')\n            # update the object\n            rowData(object@treatmentResponse, ...) <-\n                unique(value[, .SD, .SDcols=valueCols %in% rowDataCols])\n            colData(object@treatmentResponse, ...) <-\n                unique(value[, .SD, .SDcols=valueCols %in% colDataCols])\n        } else {\n            switch(dimension,\n                treatment={ rowData(object@treatmentResponse, ...) <- value },\n                sample={ colData(object@treatmentResponse, ...) <- value },\n                assay={ assay(object@treatmentResponse, 'assay_metadata') <- value },\n                .error(funContext, 'Invalid argument to dimension parameter.\n                    Please choose one of \"sample\", \"treatment\" or \"assay\"'))\n        }\n    } else {\n        if (!missing(dimension))\n            .warning(funContext, 'The dimension argument is only valid if the\n                sensitivity slot contains a LongTable object. Ignoring dimension\n                and ... parameters.')\n        object@treatmentResponse$info <- value\n    }\n    return(object)\n})\n\n\n#\n# == sensitvityMeasures\n\n\n#' @noRd\n.docs_CoreSet_get_sensitivityMeasures <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensitivityMeaures__: Get the 'sensitivityMeasures' available in a `{class_}`\n    object. Each measure reprents some summary of sample sensitivity to a given\n    treatment, such as ic50, ec50, AUC, AAC, etc. The results are returned as a\n    `character` vector with all available metrics for the PSet object.\n    @examples\n    sensitivityMeasures({data_}) <- sensitivityMeasures({data_})\n\n    @md\n    @exportMethod sensitivityMeasures\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sensitivityMeasures(class_=.local_class, data_=.local_data)\nsetMethod(sensitivityMeasures, \"CoreSet\", function(object) {\n    return(colnames(sensitivityProfiles(object)))\n})\n\n#' @noRd\n.docs_CoreSet_set_sensitityMeasures <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensitivityMeaures__: Update the sensitivity meaure in a `{class_}`\n    object. Thesee values are the column names of the 'profiles' assay and\n    represent various compued sensitviity metrics such as ic50, ec50, AUC, AAC,\n    etc.\n    - value: A `character` vector of new sensitivity measure names, the\n    then length of the character vector must matcht he number of columns of the\n    'profiles' assay, excluding metadata and key columns.\n    @examples\n    sensitivityMeasures({data_}) <- sensitivityMeasures({data_})\n\n    @md\n    @exportMethod sensitivityMeasures\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sensitityMeasures(class_=.local_class, data_=.local_data)\nsetReplaceMethod('sensitivityMeasures',\n    signature(object='CoreSet', value='character'),\n    function(object, value)\n{\n    colnames(sensitivityProfiles(object)) <- value\n    object\n})\n\n\n#\n# == sensitivityProfiles\n\n\n#' @noRd\n.docs_CoreSet_get_sensitivityProfiles <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensitivityProfiles__: Return the sensitivity profile summaries from the\n    sensitivity slot. This data.frame cotanins vaarious sensitivity summary\n    metrics, such as ic50, amax, EC50, aac, HS, etc as columns, with rows as\n    treatment by sample experiments.\n    @examples\n    sensitivityProfiles({data_})\n\n    @md\n    @exportMethod sensitivityProfiles\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sensitivityProfiles(class_=.local_class, data_=.local_data)\nsetMethod(sensitivityProfiles, \"CoreSet\", function(object) {\n    funContext <- .funContext('::sensitivityProfiles')\n    if (is(treatmentResponse(object), 'LongTable')) {\n        if (!('profiles' %in% assayNames(treatmentResponse(object)))) {\n            .error(funContext, 'The LongTable object in the sensivitiy slot\n                is not formatted correctly: it must contain an assay\n                named \"profiles\"!')\n        } else {\n            .rebuildProfiles(treatmentResponse(object))\n        }\n    } else {\n        return(object@treatmentResponse$profiles)\n    }\n})\n\n#' @keywords internal\n.rebuildProfiles <- function(object) {\n    profDT <- object$profiles\n    rowCols <- lapply(rowIDs(object)[\n        !grepl(\"treatment[0-9]*dose|drug[0-9]*dose\", rowIDs(object))\n    ], as.name)\n    colCols <- lapply(colIDs(object), as.name)\n    trt <- bquote(paste(..(rowCols), sep=\":\"), splice=TRUE)\n    smp <- bquote(paste(..(colCols), sep=\":\"), splice=TRUE)\n    profDT[, treatment_uid := eval(trt), by=.I]\n    profDT[, sample_uid := eval(smp), by=.I]\n    profDT[, exp_id := paste0(treatment_uid, \"_\", sample_uid), by=.I]\n    assayCols <- setdiff(colnames(assay(object, \"profiles\", raw=TRUE)), \".profiles\")\n    sensProf <- unique(profDT[, .SD, .SDcols=c(assayCols, \"exp_id\")])\n    obsPerExpId <- sensProf[, .N, by=\"exp_id\"][, max(N)]\n    if (obsPerExpId > 1) warning(.warnMsg(\"Multiple profile values per\",\n        \" experiment id, summarizing with mean!\"), call.=FALSE)\n    sensProf <- sensProf[, lapply(.SD, mean, na.rm=TRUE), by=\"exp_id\"]\n    return(sensProf)\n}\n\n#' @noRd\n.docs_CoreSet_set_sensitivityProfiles <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensitivityProfiles<-__: Update the sensitivity profile summaries the\n    sensitivity slot. Arguments:\n    -value: A `data.frame` the the same number of rows as as returned by\n    `sensitivityProfiles(object)`, but potentially modified columns, such as the\n    computation of additional summary metrics.\n    @examples\n    sensitivityProfiles({data_}) <- sensitivityProfiles({data_})\n\n    @md\n    @exportMethod sensitivityProfiles<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sensitivityProfiles(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"sensitivityProfiles\",\n    signature(object=\"CoreSet\", value=\"data.frame\"),\n    function(object, value)\n{\n    if (is(treatmentResponse(object), 'LongTable'))\n        warning(.warnMsg(\"The \", class(object)[1], \" class structure has been\",\n            \" updated! Assignment via sensitivityProfiles no long works, please\",\n            \" see vignette('The LongTable Class', package='CoreGx') for more\",\n            \" information.\"))\n    else\n        object@treatmentResponse$profiles <- value\n    return(object)\n})\n\n\n#\n# == sensitivityRaw\n\n\n#' @noRd\n.docs_CoreSet_get_sensitivityRaw <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensitivityRaw__: Access the raw sensitiity measurents for a {class_}\n    object. A 3D `array` where rows are experiment_ids, columns are doses\n    and the third dimension is metric, either 'Dose' for the doses used or\n    'Viability' for the sample viability at that dose.\n    @examples\n    head(sensitivityRaw({data_}))\n\n    @md\n    @exportMethod sensitivityRaw\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sensitivityRaw(class_=.local_class, data_=.local_data)\nsetMethod(\"sensitivityRaw\", signature(\"CoreSet\"), function(object) {\n    if (is(treatmentResponse(object), 'LongTable'))\n        return(.rebuildRaw(treatmentResponse(object)))\n    else\n        return(object@treatmentResponse$raw)\n})\n\n#' Replicate the $raw slot in the old @treatmentResponse list from a LongTable\n#'\n#' @param longTable `LongTable`\n#'\n#' @return A 3D `array` where rows are experiment_ids, columns are doses\n#' and the third dimension is metric, either 'Dose' for the doses used or\n#' 'Viability' for the sample viability at that dose.\n#'\n#' @keywords internal\n#' @importFrom data.table merge.data.table dcast\n#' @noRd\n.rebuildRaw <- function(longTable) {\n\n    ## TODO:: This function currently assumes there will only be one valid\n    ## dose per treatment combination, which may not be true.\n\n    funContext <- .funContext(':::.rebuildRaw')\n    if (!('sensitivity' %in% assayNames(longTable)))\n        .error(funContext, 'There is no assay named sensitivity. Not sure\n            how to retrieve sensitivityRaw without a sensitivity assay. Try\n            renaming your assays in the @treatmentResponse LongTable object?')\n\n    # Extract the information needed to reconstruct the sensitivityRaw array\n    viability <- longTable$sensitivity\n\n    # Early return for single treatment sensitivity experimentss\n    ## TODO:: refactor this into a helper?\n    if ('assay_metadata' %in% assayNames(longTable) &&\n        'old_column' %in% colnames(longTable$assay_metadata))\n    {\n        metadataDT <- copy(longTable$assay_metadata)\n        sensitivityDT <- copy(longTable$sensitivity)\n        # .NATURAL joins on all identical columns\n        assayDT <- metadataDT[sensitivityDT, on=.NATURAL]\n        if (length(colIDs(longTable)) > 1) {\n            assayDT[, sampleid := Reduce(.paste_colon, mget(colIDs(longTable)))]\n        }\n        assayDT[, exp_id := paste0(treatment1id, '_', sampleid)]\n        .mean <- function(x) mean(as.numeric(x), na.rm=TRUE)\n        doseDT <- dcast(assayDT, exp_id ~ old_column, value.var='treatment1dose',\n            fun.aggregate=.mean)\n        viabDT <- dcast(assayDT, exp_id ~ old_column, value.var='viability',\n            fun.aggregate=.mean)\n        sensRaw <- array(dim=list(nrow(doseDT), ncol(doseDT) -1, 2),\n            dimnames=list(doseDT$exp_id, colnames(doseDT)[-1],\n                c('Dose', 'Viability')))\n        sensRaw[, , 'Dose'] <- as.matrix(doseDT[, !'exp_id'])\n        sensRaw[, , 'Viability'] <- as.matrix(viabDT[, !'exp_id'])\n        return(sensRaw)\n    }\n\n    # Build the rownames\n    .paste_colons <- function(...) paste(..., sep=':')\n    # viability[, row_ids := Reduce(.paste_colons, mget(rowIDs(longTable)))]\n    # viability[, col_ids := Reduce(.paste_colons, mget(colIDs(longTable)))]\n    # viability[, rownames := paste0(row_ids, '_', col_ids)]\n    # viability[, c('row_ids', 'col_ids') := NULL]\n\n    viability[, rownames := {\n        row_ids <- Reduce(.paste_colons, mget(rowIDs(longTable)))\n        col_ids <- Reduce(.paste_colons, mget(colIDs(longTable)))\n        paste0(row_ids, '_', col_ids)\n    }]\n\n    # Merge the doses into vectors in a list column\n    viability[, dose := Reduce(.paste_slashes, mget(colnames(.SD))),\n        .SDcols=patterns('^.*[d|D]ose$')]\n\n    # Repeat the dose values if there are more viabilities\n    numReplicates <- viability[, ncol(.SD), .SDcols=patterns('^[V|v]iability.*')]\n    if (numReplicates > 1) {\n        viability[, paste0('dose', seq_len(numReplicates)) := dose]\n        viability[, dose := NULL]\n    }\n\n    # Build the array\n    sensRaw <- array(dim=list(nrow(viability), numReplicates, 2),\n        dimnames=list(viability$rownames, paste0('dose', seq_len(numReplicates)),\n            c('Dose', 'Viability')))\n    sensRaw[, , 'Dose'] <- as.matrix(viability[, .SD,\n        .SDcols=patterns('^dose.*')])\n    sensRaw[, , 'Viability'] <- as.matrix(viability[, .SD,\n        .SDcols=patterns('^[V|v]iability.*')])\n\n    return(sensRaw)\n}\n\n#' @noRd\n.docs_CoreSet_set_sensitivityRaw <- function(...) .parseToRoxygen(\n    \"\n    @details\n\n    __sensitvityRaw<-__: Update the raw dose and viability data in a `{class_}`.\n    - value: A 3D `array` object where rows are experiment_ids, columns are\n    replicates and pages are c('Dose', 'Viability'), with the corresponding\n    dose or viability measurement for that experiment_id and replicate.\n\n    @examples\n    sensitivityRaw({data_}) <- sensitivityRaw({data_})\n\n    @md\n    @importFrom data.table data.table as.data.table := merge.data.table tstrsplit\n    @exportMethod sensitivityRaw<-\n    \"\n    ,\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sensitivityRaw(class_=.local_class, data_=.local_data)\nsetReplaceMethod('sensitivityRaw', signature(\"CoreSet\", \"array\"),\n    function(object, value)\n{\n    funContext <- .funContext(\"::sensitivityRaw<-\")\n    if (is(treatmentResponse(object), 'LongTable')) {\n\n        ## TODO:: validate value\n        tre <- treatmentResponse(object)\n\n        viabilityCols <- assayCols(tre, \"sensitivity\")\n        # Handle the non-treatment combo case\n        if (length(viabilityCols) != ncol(value)) {\n            valueDT <- as.data.table(value)\n            valueDT <- dcast(valueDT, V1 + V2 ~ V3, value.var='value')\n            setnames(valueDT, old=c('Dose', 'Viability'),\n                new=c('treatment1dose', 'viability'))\n            valueDT[, V2 := NULL]  # delete the array column names\n            valueDT[, (idCols(tre)) := tstrsplit(V1, ':|_', type.convert=TRUE)]\n            valueDT[, V1 := NULL]\n            assay(tre, i='sensitivity') <- valueDT\n        } else {\n            # Process into a the proper format for the sensitivity assay\n            # NOTE: as.matrix deals with the case where there is only a single\n            #   viability column in the sensitivityRaw array object,\n            #   in which case the drop=TRUE argument causes a vector to be\n            #   returned\n            raw <- as.data.table(as.matrix(value[, , 'Viability']),\n                keep.rownames='rn', na.rm=FALSE)\n            coerceCols <- colnames(raw)[-1]\n            raw[, (coerceCols) := lapply(.SD, as.numeric), .SDcols=!'rn']\n            raw[, (idCols(tre)) := tstrsplit(rn, ':|_', type.convert=TRUE)]\n            raw[, c('rn') := NULL]\n            colnames(raw) <- gsub('^dose\\\\d*|^V\\\\d*', 'viability', colnames(raw))\n            # Update the assay\n            assay(tre, i='sensitivity') <- raw\n        }\n\n        # Update the object\n        treatmentResponse(object) <- tre\n    } else {\n        object@treatmentResponse$raw <- value\n        object\n    }\n    return(object)\n})\n\n\n#\n# == sensitivitySlot\n\n\n#' @export\nsetGeneric(\"treatmentResponse\", function(object, ...) standardGeneric(\"treatmentResponse\"))\n\n#' @noRd\n.docs_CoreSet_get_treatmentResponse <- function(...) .parseToRoxygen(\n    \"\n    __treatmentResponse__: Retrive the contents of `@treatmentResponse` from a `{class_}`\n    object.\n\n    @examples\n    treatmentResponse({data_})\n\n    @md\n    @aliases treatmentResponse,{class_}-method treatmentResponse\n    @aliases sensitivitySlot\n    @exportMethod treatmentResponse\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_treatmentResponse(class_=.local_class,\n#'   data_=.local_data)\nsetMethod(\"treatmentResponse\", signature(\"CoreSet\"), function(object) {\n    object@treatmentResponse\n})\n#' @export\nsensitivitySlot <- function(...) treatmentResponse(...)\n\n\n#' @export\nsetGeneric(\"treatmentResponse<-\", function(object, ..., value)\n    standardGeneric(\"treatmentResponse<-\"))\n\n.docs_CoreSet_set_treatmentResponse <- function(...) .parseToRoxygen(\n    \"\n    __treatmentResponse<-__: Assign data to the `@treatmentResponse` slot of a\n    `{class_}` object.\n    - value: Either a `TreatmentResponseExperiment` class object, or a list with\n    an 'info' `data.frame` of experiment metadata, 'profiles' `data.frame` with\n    summary statistics from the sensitivity experiment and a 'raw' 3D array\n    where rows are experiments, columns are replicates and pages are 'Dose'\n    or 'Viability' containing their respective values for that treatment by sample\n    experiment. The type of `value` must match type of the current `@treatmentResponse`\n    slot of the `{class_}` object.\n\n    @examples\n    treatmentResponse({data_}) <- treatmentResponse({data_})\n\n    @md\n    @aliases treatmentResponse<- treamentResponse<-,{class_},list-method\n    treatmentResponse<-,{class_},LongTable-method\n    @aliases sensitivitySlot<-\n    @exportMethod treatmentResponse<-\n    \",\n    ...\n)\n\n\n#' @rdname CoreSet-accessors\n#' @include LongTable-class.R\n#' @eval .docs_CoreSet_set_treatmentResponse(class_=.local_class, data_=.local_data)\nsetReplaceMethod(\"treatmentResponse\", signature(object=\"CoreSet\", value=\"list_OR_LongTable\"),\n    function(object, value)\n{\n    # funContext <- .S4MethodContext('sensitivitySlot<-', class(object), class(value))\n    # ## TODO:: Maybe try coercing the list to a LongTable and vice versa?\n    if (!is(object@treatmentResponse, class(value)[1])) .error(funContext, 'The types\n        of the current and @treatmentResponse slot and the value parameter must be\n        the same!')\n    object@treatmentResponse <- value\n    return(object)\n})\n#' @export\n`sensitivitySlot<-` <- function(..., value) `treatmentResponse<-`(..., value=value)\n\n\n##\n## == sensNumber\n\n\n#' @export\nsetGeneric(\"sensNumber\", function(object, ...) standardGeneric(\"sensNumber\"))\n\n#' @noRd\n.docs_CoreSet_get_sensNumber <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensNumber__: Return a count of viability observations in a `{class_}`\n    object for each treatment-combo by sample combination.\n\n    @examples\n    sensNumber({data_})\n\n    @md\n    @aliases sensNumber\n    @exportMethod sensNumber\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_sensNumber(class_=.local_class, data_=.local_data)\nsetMethod(sensNumber, \"CoreSet\", function(object){\n    return(\n        if (is(object@treatmentResponse, 'LongTable'))\n            .rebuildSensNumber(object@treatmentResponse)\n        else\n            object@treatmentResponse$n\n    )\n})\n\n.rebuildSensNumber <- function(object) {\n    sensitivityDT <- object$sensitivity\n    # Melt replicates so they get counted\n    sensitivityDT_melted <- melt(sensitivityDT,\n        measure=patterns('^viability'), variable.name='replicate',\n        value.name='viability')\n\n    # Determine the treatment and sample combos, ignoring other identifiers\n    .paste_colon <- function(x, y) paste(x, y, sep=':')\n    treatmentidCols <- sensitivityDT[, colnames(.SD), .SDcols=patterns('treatment.*id')]\n    sampleidCols <- sensitivityDT[, colnames(.SD), .SDcols=patterns('sample.*id')]\n\n    # Parse the columns to dcast by to get the counts\n    sensitivityDT_melted[, .treatmentCombo := Reduce(.paste_colon, mget(treatmentidCols))]\n    sensitivityDT_melted[, .sampleCombo := Reduce(.paste_colon, mget(sampleidCols))]\n\n    # Count existing sensitivity measurements\n    .count_not_NA <- function(x) sum(!is.na(x))\n    sensNumbDT <- dcast(sensitivityDT_melted, .treatmentCombo ~ .sampleCombo,\n        value.var='viability', fun.aggregate=.count_not_NA)\n    sensNumberM <- as.matrix(sensNumbDT[, !'.treatmentCombo'])\n    rownames(sensNumberM) <- sensNumbDT[['.treatmentCombo']]\n\n    return(sensNumberM)\n\n    ## TODO:: Pad for any missing treatments or samples\n    allDrugCombos <- rowData(object)[, Reduce(.paste_colon, mget(treatmentidCols))]\n    allSampleCombos <- colData(object)[, Reduce(.paste_colon, mget(sampleidCols))]\n\n}\n\n#' @export\nsetGeneric(\"sensNumber<-\", function(object, value) standardGeneric(\"sensNumber<-\"))\n\n#' @noRd\n.docs_CoreSet_set_sensNumber <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __sensNumber<-__: Update the 'n' item, which holds a matrix with a count\n    of treatment by sample-line experiment counts, in the `list` in `@treatmentResponse`\n    slot of a `{class_}` object. Will error when `@sensitviity` contains\n    a `LongTable` object, since the counts are computed on the fly. Arguments:\n    - value: A `matrix` where rows are samples and columns are treatments, with a\n    count of the number of experiments for each combination as the values.\n\n    @examples\n    sensNumber({data_}) <- sensNumber({data_})\n\n    @md\n    @aliases sensNumber<-\n    @exportMethod sensNumber<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_sensNumber(class_=.local_class, data_=.local_data)\nsetReplaceMethod('sensNumber', signature(object=\"CoreSet\", value=\"matrix\"),\n    function(object, value)\n{\n    if (is(treatmentResponse(object), 'LongTable')) {\n        object\n    } else {\n        object@treatmentResponse$n <- value\n        object\n    }\n})\n\n\n## ======================\n## ---- perturbation slot\n\n##\n## == pertNumber\n\n\n#' @export\nsetGeneric(\"pertNumber\", function(object, ...) standardGeneric(\"pertNumber\"))\n\n#' @noRd\n.docs_CoreSet_get_pertNumber <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __pertNumber__: `array` Summary of available perturbation experiments\n    from in a `{class_}` object. Returns a 3D `array` with the number of\n    perturbation experiments per treatment and sample, and data type.\n\n    @examples\n    pertNumber({data_})\n\n    @md\n    @aliases pertNumber\n    @exportMethod pertNumber\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_get_pertNumber(class_=.local_class, data_=.local_data)\nsetMethod(pertNumber, \"CoreSet\", function(object){\n    return(object@perturbation$n)\n})\n\n#' @export\nsetGeneric(\"pertNumber<-\", function(object, value) standardGeneric(\"pertNumber<-\"))\n\n.docs_CoreSet_set_pertNumber <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __pertNumber<-__: Update the `@perturbation$n` value in a `{class_}` object,\n    which stores a summary of the available perturbation experiments. Arguments:\n    - value: A new 3D `array` with the number of perturbation experiments per\n    treatment and sample, and data type\n\n    @examples\n    pertNumber({data_}) <- pertNumber({data_})\n\n    @md\n    @aliases pertNumber<-\n    @exportMethod pertNumber<-\n    \",\n    ...\n)\n\n#' @rdname CoreSet-accessors\n#' @eval .docs_CoreSet_set_pertNumber(class_=.local_class, data_=.local_data)\nsetReplaceMethod('pertNumber', signature(object=\"CoreSet\", value=\"array\"),\n    function(object, value)\n{\n  object@perturbation$n <- value\n  object\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .docs_CoreSet_accessors function?",
        "answer": "The .docs_CoreSet_accessors function generates documentation for the accessor methods of the CoreSet class. It creates a template for the documentation that includes details about accessing and modifying information in the slots of a CoreSet object."
      },
      {
        "question": "How does the annotation method work for the CoreSet class?",
        "answer": "The annotation method for the CoreSet class retrieves the contents of the @annotation slot from the CoreSet object. It returns a list containing metadata about the object, such as its name, creation date, session info, and version."
      },
      {
        "question": "What is the purpose of the sensitivityRaw method in the CoreSet class?",
        "answer": "The sensitivityRaw method accesses the raw sensitivity measurements for a CoreSet object. It returns a 3D array where rows are experiment_ids, columns are doses, and the third dimension contains 'Dose' and 'Viability' data for each experiment and dose combination."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('annotation', signature(\"CoreSet\"), function(object) {\n    object@annotation\n})",
        "complete": "setMethod('annotation', signature(\"CoreSet\"), function(object) {\n    object@annotation\n})\n\nsetReplaceMethod(\"annotation\", signature(\"CoreSet\", \"list\"),\n    function(object, value)\n{\n    object@annotation <- value\n    object\n})"
      },
      {
        "partial": "setMethod('dateCreated', signature(\"CoreSet\"), function(object) {\n    object@annotation$dateCreated\n})",
        "complete": "setMethod('dateCreated', signature(\"CoreSet\"), function(object) {\n    object@annotation$dateCreated\n})\n\nsetReplaceMethod('dateCreated', signature(object=\"CoreSet\", value=\"character\"),\n    function(object, value)\n{\n    funContext <- .funContext('dateCreated')\n    if (length(value) > 1) .error(funContext, 'dateCreated must by a character\n        vector of length 1, as returned by the `date()` function.')\n    object@annotation$dateCreated <- value\n    return(object)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/methods-dimnames.R",
    "language": "R",
    "content": "# ==== LongTable Class\n\n#' Get the column names from a `LongTable` object.\n#'\n#' @examples\n#' head(colnames(merckLongTable))\n#'\n#' @describeIn LongTable Retrieve the pseudo-colnames of a LongTable object,\n#'   these are constructed by pasting together the colIDs(longTable) and\n#'   can be used in the subset method for regex based queries.\n#'\n#' @param x A `LongTable` object to get the column names from\n#'\n#' @return `character` Vector of column names.\n#'\n#' @export\nsetMethod('colnames', signature(x='LongTable'), function(x) {\n    return(x@colData$.colnames)\n})\n\n#' Get the row names from a `LongTable` object.\n#'\n#' @examples\n#' head(rownames(merckLongTable))\n#'\n#' @describeIn LongTable Retrieve the pseudo-rownames of a LongTable object,\n#'   these are constructed by pasting together the rowIDs(longTable) and\n#'   can be used in the subset method for regex based queries.\n#'\n#' @param x A `LongTable` object to get the row names from\n#'\n#' @return `character` Vector of row names.\n#'\n#' @export\nsetMethod('rownames', signature(x='LongTable'), function(x) {\n    return(x@rowData$.rownames)\n})\n\n#' Getter for the dimnames of a `LongTable` object\n#'\n#' @examples\n#' lapply(dimnames(merckLongTable), head)\n#'\n#' @describeIn LongTable Get the pseudo-dimnames for a LongTable object. See\n#'   colnames and rownames for more information.\n#'\n#' @param x The `LongTable` object to retrieve the dimnames for.\n#'\n#' @return `list` List with two character vectors, one for row and one for\n#'     column names.\n#'\n#' @importMethodsFrom Biobase dimnames\n#' @export\nsetMethod('dimnames', signature(x='LongTable'), function(x) {\n    return(list(x@rowData$.rownames, x@colData$.colnames))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `colnames` method for the `LongTable` class, and how does it differ from traditional column names?",
        "answer": "The `colnames` method for the `LongTable` class retrieves pseudo-colnames of a LongTable object. These are constructed by pasting together the colIDs(longTable) and can be used in the subset method for regex-based queries. Unlike traditional column names, these pseudo-colnames are stored in the `x@colData$.colnames` slot of the LongTable object, allowing for more flexible and efficient handling of large datasets."
      },
      {
        "question": "How are the `rownames` and `colnames` methods related to the `dimnames` method in the `LongTable` class?",
        "answer": "The `dimnames` method in the `LongTable` class combines the functionality of `rownames` and `colnames`. It returns a list containing two character vectors: the first vector contains the row names (retrieved using `x@rowData$.rownames`), and the second vector contains the column names (retrieved using `x@colData$.colnames`). This method provides a convenient way to access both row and column names simultaneously, which is consistent with R's standard dimnames functionality for matrices and data frames."
      },
      {
        "question": "What is the significance of using `setMethod` with `signature` in these code snippets, and how does it relate to object-oriented programming in R?",
        "answer": "The use of `setMethod` with `signature` in these code snippets is part of R's S4 object-oriented programming system. It allows for method dispatch based on the class of the arguments. In this case, `setMethod` is used to define specialized versions of `colnames`, `rownames`, and `dimnames` methods for objects of the `LongTable` class. The `signature(x='LongTable')` specifies that these methods should be called when the first argument (x) is of class `LongTable`. This approach enables polymorphism, allowing different implementations of these methods for different classes while maintaining a consistent interface."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('colnames', signature(x='LongTable'), function(x) {\n    # Complete the function body\n})",
        "complete": "setMethod('colnames', signature(x='LongTable'), function(x) {\n    return(x@colData$.colnames)\n})"
      },
      {
        "partial": "setMethod('dimnames', signature(x='LongTable'), function(x) {\n    # Complete the function body\n})",
        "complete": "setMethod('dimnames', signature(x='LongTable'), function(x) {\n    return(list(x@rowData$.rownames, x@colData$.colnames))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/unitest/likelihood_test.R",
    "language": "R",
    "content": "# Given a data_set and network, computes barcode for each edge\n# Returns a matrix of 3 columns where col 1 is the higher value\n# col 2 is the lower value, and col 3 is the frequency of such a\n# relation.\nload('~/Testbed/irinotecan_cgp_ccle.RData')\n`compute.barcode` <- function(data_set, network)\n{\n\tedges <- which(network == 1)\n\tbarcode <- do.call(rbind, lapply(edges, function(edge) {\n\t\t\t\t\t\tj <- edge%%ncol(network)\n\t\t\t\t\t\tif(j == 0)\n\t\t\t\t\t\t\tj <- ncol(network)           \n\t\t\t\t\t\ti <- ceiling(edge/ncol(network))\n\t\t\t\t\t\tc(i, j, mean(data_set[, i] > data_set[, j], use=\"complete.obs\"))\n\t\t\t\t\t}))\n\treturn(barcode)\n}\n\n# Returns the product of likelihoods for a given barcode and sample\n`compute.likelihood` <- function(barcode, sample)\n{\n\tlikelihood <- apply(barcode, 1, function(feature) {\n\t\t\t\tif (sample[feature[1]] > sample[feature[2]])\n\t\t\t\t\treturn(feature[3])\n\t\t\t\telse\n\t\t\t\t\treturn(1 - feature[3])\n\t\t\t})\n\treturn(prod(likelihood))\n}\n\n`discretize.labels` <- function(labels, probabilities)\n{\n\tquantiles <- quantile(labels, probs=probabilities, na.rm=TRUE)\n\tclasses <- Hmisc::cut2(x=labels, cuts=quantiles)\n\tlevels(classes) <- c(0, NA, 1)\n\tnames(classes) <- names(labels)\n\tas.integer(classes) - 1\n}\n\n\n\ntraining_set <- data_cgp\ntraining_labels <- ic50_cgp\nvalidating_set <- data_cgp\n\n\n# Select the top 10 genes most correlated with phenotype\ngenes <- order(apply(training_set, 2, cor, training_labels, use=\"complete.obs\"))[1:10]\n\n# Create empty network\nnetwork <- matrix(0, ncol(training_set), ncol(training_set))\n\n# Connect the 10 selected genes (every pair of nodes)\nsapply(1:(length(genes) - 1), function(i) {\n\t\t\tsapply((i + 1):length(genes), function(j) {\n\t\t\t\t\t\tnetwork[genes[i], genes[j]] <<- 1\n\t\t\t\t\t})\n\t\t})\n\n# Discretize CGP into senstive (0) and resistant (1)\ndiscrete_labels <- discretize.labels(training_labels, c(0.25, 0.75))\n\n# Create barcodes for resistant and senstive cgp samples\nresistant_barcode <- compute.barcode(training_set[which(discrete_labels == 1), ], network)\nsensitive_barcode <- compute.barcode(training_set[which(discrete_labels == 0), ], network)\n\n# Compute the likelihoods\nlikelihoods <- t(apply(validating_set, 1, function(sample) {\n\t\t\tresistant_like <- compute.likelihood(resistant_barcode, sample)\n\t\t\tsensitive_like <- compute.likelihood(sensitive_barcode, sample)\n\t\t\treturn(c(resistant_like, sensitive_like))\n\t\t}))\n\nhist(likelihoods[, 1] / ( likelihoods[, 1] + likelihoods[, 2]))\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `compute.barcode` function in the given code snippet?",
        "answer": "The `compute.barcode` function computes a barcode for each edge in a given network. It takes a data_set and network as input, and returns a matrix with 3 columns. The first column represents the higher value node, the second column represents the lower value node, and the third column represents the frequency of the relation where the value of the first node is greater than the value of the second node across all samples in the data_set."
      },
      {
        "question": "How does the `discretize.labels` function work, and what is its purpose in the context of this code?",
        "answer": "The `discretize.labels` function is used to categorize continuous labels into discrete classes. It takes the labels and a vector of probabilities as input. It then calculates quantiles based on these probabilities and uses them to cut the labels into classes. In this code, it's used to discretize the CGP (Cancer Genome Project) data into 'sensitive' (0) and 'resistant' (1) categories, using the 25th and 75th percentiles as cutoff points. This discretization is crucial for creating separate barcodes for resistant and sensitive samples in the subsequent analysis."
      },
      {
        "question": "What is the significance of the final histogram plotted in this code?",
        "answer": "The final histogram plots the distribution of the ratio of resistant likelihood to total likelihood (resistant + sensitive) for each sample in the validating set. This ratio represents the probability of a sample being resistant. A ratio close to 1 indicates a high probability of resistance, while a ratio close to 0 indicates a high probability of sensitivity. The histogram provides a visual representation of how well the model distinguishes between resistant and sensitive samples in the validation set, which can be used to assess the performance of the barcode-based classification method."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Function to compute barcode for each edge\ncompute.barcode <- function(data_set, network) {\n  edges <- which(network == 1)\n  barcode <- do.call(rbind, lapply(edges, function(edge) {\n    j <- edge %% ncol(network)\n    if (j == 0) j <- ncol(network)\n    i <- ceiling(edge / ncol(network))\n    # Complete the function body\n  }))\n  return(barcode)\n}",
        "complete": "# Function to compute barcode for each edge\ncompute.barcode <- function(data_set, network) {\n  edges <- which(network == 1)\n  barcode <- do.call(rbind, lapply(edges, function(edge) {\n    j <- edge %% ncol(network)\n    if (j == 0) j <- ncol(network)\n    i <- ceiling(edge / ncol(network))\n    c(i, j, mean(data_set[, i] > data_set[, j], use = \"complete.obs\"))\n  }))\n  return(barcode)\n}"
      },
      {
        "partial": "# Function to compute likelihood for a given barcode and sample\ncompute.likelihood <- function(barcode, sample) {\n  likelihood <- apply(barcode, 1, function(feature) {\n    # Complete the function body\n  })\n  return(prod(likelihood))\n}",
        "complete": "# Function to compute likelihood for a given barcode and sample\ncompute.likelihood <- function(barcode, sample) {\n  likelihood <- apply(barcode, 1, function(feature) {\n    if (sample[feature[1]] > sample[feature[2]]) feature[3] else 1 - feature[3]\n  })\n  return(prod(likelihood))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/tests/testthat/test-LongTable-utils.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(CoreGx)\nlibrary(data.table)\n\ndata(nci_TRE_small)\ntre <- nci_TRE_small\n\n# == subset\n\ntestthat::test_that(\"`subset,LongTable-method` works with call queries\", {\n    ntre <- subset(tre,\n        treatment1id %in% unique(treatment1id)[1:5],\n        sampleid %in% unique(sampleid)[1:5]\n    )\n    testthat::expect_s4_class(ntre, \"LongTable\")\n    ## These tests need to be updated to use expect_true with .table_is_subset\n    ## instead of expect_equal due the fact that the subset,LongTable-method\n    ## will drop additional rowKey or colKey values than those in the initial\n    ## subset statement if there are no assay observations using those keys.\n    ## This change fixed #148, but now makes it impossible to store metadata\n    ## when there are no observations, which may not be ideal?\n    ## Alternative would be to rework the assayIndex to be free of NA values\n    testthat::expect_true(\n        CoreGx:::.table_is_subset(\n            rowData(ntre),\n            rowData(tre)[treatment1id %in% unique(treatment1id)[1:5]]\n        )\n    )\n    testthat::expect_true(\n        CoreGx:::.table_is_subset(\n            colData(ntre),\n            colData(tre)[sampleid %in% unique(sampleid)[1:5]]\n        )\n    )\n    # check for NA values in the key column of the assay\n    testthat::expect_true(\n        !anyNA(assays(ntre, raw=TRUE)[[\"sensitivity\"]]$sensitivity)\n    )\n    ntre2 <- tre[\n        .(treatment1id %in% unique(treatment1id)[1:5]),\n        .(sampleid %in% unique(sampleid)[1:5])\n    ]\n    testthat::expect_s4_class(ntre2, \"LongTable\")\n    testthat::expect_true(\n        CoreGx:::.table_is_subset(\n            rowData(ntre2),\n            rowData(tre)[treatment1id %in% unique(treatment1id)[1:5]]\n        )\n    )\n    testthat::expect_true(\n        CoreGx:::.table_is_subset(\n            colData(ntre2),\n            colData(tre)[sampleid %in% unique(sampleid)[1:5]]\n        )\n    )\n    # check for NA values in the key column of the assay\n    testthat::expect_true(\n        !anyNA(assays(ntre2, raw=TRUE)[[\"sensitivity\"]]$sensitivity)\n    )\n    testthat::expect_equal(ntre, ntre2)\n})\n\ntestthat::test_that(\"`subset,LongTable-method` works with regex queries\", {\n    ntre <- subset(tre,\n        c(\"Vinblastine\", \"Temozolomide\"),\n        c(\"HT*\", \"MOLT*\")\n    )\n    testthat::expect_s4_class(ntre, \"LongTable\")\n    testthat::expect_true(\n        CoreGx:::.table_is_subset(\n            rowData(ntre),\n            rowData(tre)[grepl(\"Vinblastine|Temozolomide\", rownames(tre)), ]\n        )\n    )\n    testthat::expect_true(\n        CoreGx:::.table_is_subset(\n            colData(ntre),\n            colData(tre)[grepl(\"HT*|MOLT-*\", colnames(tre)), ]\n        )\n    )\n})\n\ntestthat::test_that(\"`CoreGx:::.subsetByIndex` is equivalent to subsetting the raw data\", {\n    keepRows <- rowData(tre, key=TRUE)[treatment1id %in% treatment1id[1:5], ]\n    fullAssay <- tre$sensitivity\n    rawSubset <- fullAssay[treatment1id %in% keepRows$treatment1id, ]\n    aindex <- mutable(getIntern(tre, \"assayIndex\"))\n    subindex <- aindex[rowKey %in% keepRows$rowKey, ]\n    ntre <- CoreGx:::.subsetByIndex(tre, subindex)\n    testthat::expect_true(\n        !anyNA(assays(ntre, raw=TRUE)[[\"sensitivity\"]]$sensitivity)\n    )\n    assayByIndex <- ntre$sensitivity\n    testthat::expect_true(all.equal(rawSubset, assayByIndex))\n})\n\ntestthat::test_that(\"`subset,LongTable-method` works with row and column names\", {\n    ntre <- subset(tre, rownames(tre)[1:5], colnames(tre)[1:5])\n    testthat::expect_equal(rownames(ntre), rownames(tre)[1:5])\n    testthat::expect_true(all.equal(rowData(ntre), rowData(tre)[1:5, ]))\n    testthat::expect_equal(colnames(ntre), colnames(tre)[1:5])\n    testthat::expect_true(all.equal(colData(ntre), colData(tre)[1:5, ]))\n})\n\ntestthat::test_that(\"`subset,LongTable-method` doesn't produce non-existing assay observations from joining\", {\n    all_assays <- assays(tre, key = FALSE, withDimnames = TRUE)\n    select_row_idx <- seq.int(1, dim(tre)[1], by = 2)\n    select_col_idx <- seq.int(1, dim(tre)[2], by = 2)\n    sub_tre <- subset(tre, i = select_row_idx, j = select_col_idx)\n    for (a in seq_along(all_assays)) {\n        assay_sub <- assay(sub_tre, a, key = FALSE, withDimnames = TRUE)\n        obs_exists <- dim(\n            assay_sub[!all_assays[[a]], on = names(assay_sub)]\n            )[1] == 0 ## anti-join to check elements not a subset\n        testthat::expect_true({ obs_exists })\n    }\n})\n\ntestthat::test_that(\"`subset,LongTable-method` doesn't miss assay observations for either selected row/columns\", {\n    all_assays <- assays(tre, key = FALSE, withDimnames = TRUE)\n    select_row_idx <- sample.int(n = dim(tre)[1], size = 1, replace = FALSE)\n    sub_tre <- subset(tre, i = select_row_idx)\n    select_row <- rowData(tre)[select_row_idx, rowIDs(tre), with = FALSE]\n    for (a in seq_along(all_assays)) {\n        assay_sub1 <- assay(sub_tre, a, key = FALSE, withDimnames = TRUE)\n        assay_sub2 <- all_assays[[a]][select_row, ]\n        testthat::expect_equal(assay_sub1, assay_sub2)\n    }\n    select_col_idx <- sample.int(n = dim(tre)[2], size = 1, replace = FALSE)\n    sub_tre <- subset(tre, j = select_col_idx)\n    select_col <- colData(tre)[select_col_idx, colIDs(tre), with = FALSE]\n    assay_names <- assayNames(sub_tre)\n    for (a in seq_along(assay_names)) {\n        assay_sub1 <- assay(sub_tre, a, key = FALSE, withDimnames = TRUE)\n        assay_sub2 <- all_assays[[a]][sampleid == select_col, ]\n        testthat::expect_equal(assay_sub1, assay_sub2)\n    }\n    sub_tre <- tre[select_row_idx, select_col_idx]\n    select_both <- cbind(select_row, select_col)\n    for (a in seq_along(assay_names)) {\n        assay_sub1 <- assay(sub_tre, a, key = FALSE, withDimnames = TRUE)\n        assay_sub2 <- all_assays[[a]][select_both, ]\n        testthat::expect_equal(assay_sub1, assay_sub2)\n    }\n})\n\ntestthat::test_that(\"`subset,LongTable-method` subset indexing behaves the same as data.table\", {\n    ## controled by .subsetByIndex\n    sub_tre <- subset(tre, i = NULL) ## subset with row index by NULL\n    testthat::expect_equal(dim(sub_tre), c(0, 0))\n    sub_tre <- subset(tre, j = NULL) ## subset with column index by NULL\n    testthat::expect_equal(dim(sub_tre), c(0, 0))\n    sub_tre <- subset(tre, i = \"\") ## subset with row by empty rowname string\n    testthat::expect_equal(sub_tre, tre)\n    sub_tre <- subset(tre, j = \"\") ## subset with column by empty column name\n    testthat::expect_equal(sub_tre, tre)\n    sub_tre <- subset(tre, i = \"\", j = \"\") ## subset by empty row+column names\n    testthat::expect_equal(sub_tre, tre)\n    ## Get a subset with 2-Fluoro Ara-A of dose 6e-06 as second treatment in combination therapies\n    sub_tre <- subset(tre, i = \"*:2-Fluoro Ara-A:*:6e-06\")\n    regex <- \"(?=.*\\\\:2-Fluoro Ara-A)(?=.*6e-06\\\\:*)^\" ## rowData regex\n    testthat::expect_equal(\n        rowData(tre)[grepl(regex, rownames(tre), perl = TRUE), ],\n        rowData(sub_tre)\n    ) ## much nicer to query at the TRE level\n    ## Subset containing ovarian cancer cell line\n    sub_tre <- subset(tre, j = \".*OVCAR.*\")\n    testthat::expect_equal(\n        colData(tre)[grepl(\".*OVCAR.*\", colnames(tre), perl = TRUE), ],\n        colData(sub_tre)\n    )\n    ## Subset by negative index: TRE behaves the same as data.table\n    i <- sample.int(n = dim(tre)[1], size = 1, replace = FALSE)\n    sub_tre_1 <- tre[-i, ] ## Drop the i-th row\n    sub_tre_2 <- tre[i, ] ## Extract the i-th row\n    testthat::expect_equal(\n        rowData(tre)[!rowData(sub_tre_1), on = names(rowData(tre))],\n        rowData(sub_tre_2) # rowData(tre)\\rowData(sub_tre_1)=rowData(sub_tre_2)\n    )\n    j <- sample.int(n = dim(tre)[2], size = 1, replace = FALSE)\n    sub_tre_3 <- tre[, -j] ## Drop the j-th column\n    sub_tre_4 <- tre[, j] ## Extract the j-th column\n    testthat::expect_equal(\n        colData(tre)[!colData(sub_tre_3), on = names(colData(tre))],\n        colData(sub_tre_4) # colData(tre)\\colData(sub_tre_3)=colData(sub_tre_4)\n    )\n    sub_tre_5 <- tre[-i, -j] ## Drop data containing i-th row OR j-th column\n    sub_tre_6 <- tre[i, j] ## Extract i-th row AND j-th column\n    all_assays <- assays(tre, key = FALSE, withDimnames = TRUE)\n    for (a in seq_along(all_assays)) {\n        testthat::expect_equal(\n            all_assays[[a]][\n                !assay(sub_tre_1, a, key = FALSE, withDimnames = TRUE),\n            ],\n            assay(sub_tre_2, i = a, key = FALSE, withDimnames = TRUE)\n         )\n        testthat::expect_equal(\n            all_assays[[a]][\n                !assay(sub_tre_3, a, key = FALSE, withDimnames = TRUE),\n            ],\n            assay(sub_tre_4, i = a, key = FALSE, withDimnames = TRUE)\n        )\n        ## tre[i, ] UNION tre[, j] + (tre[i, ] INTERSECT tre[, j])\n        union_assay <- rbind(\n            assay(sub_tre_2, i = a, key = FALSE, withDimnames = TRUE),\n            assay(sub_tre_4, i = a, key = FALSE, withDimnames = TRUE)\n        ) ## contains double count, not a union yet\n        double_count_idx <- which(duplicated(union_assay))\n        # Show that the double counted element is the intersect\n        testthat::expect_equal(\n            union_assay[double_count_idx, ],\n            ## tre[i, ] INTERSECT tre[, j]\n            assay(sub_tre_6, i = a, key = FALSE, withDimnames = TRUE),\n            ignore_attr = TRUE\n        )\n        ## Show that these two produce equivalent union sets\n #       union_assay <- setorderv(union_assay[-double_count_idx, ],\n #                                cols = idCols(tre))\n #       testthat::expect_equal(\n #           union_assay,\n #           ## This indirect approach for tre[i, ] UNION tre[, j] is faster\n #           all_assays[[a]][\n #               !assay(sub_tre_5, a, key = FALSE, withDimnames = TRUE),\n #               on = idCols(tre)\n #           ], ## reordering done by internal reindexing\n #           ignore_attr = TRUE\n #       )\n    }\n})\n\n# == reindex\n\ntestthat::test_that(\"`reindex,LongTale-method` does not mutate by reference\", {\n    .tre <- copy(tre)\n    ntre <- reindex(tre)\n    testthat::expect_true(all.equal(.tre, tre))\n})\n\ntestthat::test_that(\"`reindex,LongTable-method` has same index as LongTable constructor\", {\n    ntre <- reindex(tre)\n    testthat::expect_true(all.equal(getIntern(ntre, \"assayIndex\"), getIntern(tre, \"assayIndex\")))\n    testthat::expect_true(all.equal(assays(ntre, raw=TRUE), assays(tre, raw=TRUE)))\n})\n\ntestthat::test_that(\"`reindex,LongTable-method` does not corrupt data relationships\", {\n    ntre <- reindex(tre)\n    for (i in seq_along(assayNames(tre))) {\n        assay1 <- assay(tre, i, withDimnames=TRUE)\n        setkeyv(assay1, idCols(tre))\n        assay2 <- assay(ntre, i, withDimnames=TRUE)\n        setkeyv(assay2, idCols(ntre))\n        testthat::expect_true(all.equal(assay1, assay2))\n    }\n    assayL1 <- assays(tre)\n    assayL2 <- assays(ntre)\n    for (i in seq_along(assayL1)) {\n        testthat::expect_true(all.equal(assayL1[[i]], assayL2[[i]]))\n    }\n})\n\ntestthat::test_that(\"`reindex,LongTable-method` removes gaps in keys in subset LongTable\", {\n    select_row <- seq.int(1, dim(tre)[1], by = 2)\n    stre <- tre[select_row, ] ## subset data\n    stre <- reindex(stre)\n    ## check if rowData and colData keys have gaps\n    row_keys <- rowData(stre, key = TRUE)$rowKey\n    col_keys <- colData(stre, key = TRUE)$colKey\n    has_no_gaps_in_row <- rle(diff(row_keys))$value == 1\n    has_no_gaps_in_col <- rle(diff(col_keys))$value == 1\n    testthat::expect_true(has_no_gaps_in_row)\n    testthat::expect_true(has_no_gaps_in_col)\n    ## check if assays' keys have gaps\n    for (i in seq_along(assayNames(stre))) {\n        assay_name <- assayNames(stre)[i]\n        assay_keys <- assay(stre, i, key = FALSE, withDimnames = FALSE, metadata=FALSE)[[paste0(\".\", assay_name)]]\n        has_no_gaps_in_assay <- rle(diff(assay_keys))$value == 1\n        if (length(has_no_gaps_in_assay) > 1) print(i)\n        testthat::expect_true(has_no_gaps_in_assay)\n    } # Leave summary assay out for now\n})\n\n# == [[\n\n#testthat::test_that(\"`[[,LongTable-method` returns assay metadata always with dimnames\",{\n#    testthat::expect_warning({ tre[[1, withDimnames = FALSE, metadata = TRUE]] },\n#        regexp = \".*Unable.to.return.metadata.without.dimnames,.proceeding.as.if.withDimnames=TRUE.*\"\n#    )\n#})\n#\n#testthat::test_that(\"`[[,LongTable-method` when keys = TRUE, ignore withDimnames and metadata\",{\n#    testthat::expect_warning({ tre[[1, keys = TRUE]] },\n#        regexp = \".*Ignoring withDimnames and metadata arguments when keys=TRUE.*\"\n#    )\n#})\n\n#testthat::test_that(\"`[[,LongTable-method` allows only one assay selection at a time\",{\n#    testthat::expect_error({ tre[[1:2]] },\n#        regexp = \".*Please specifying a single string assay name or integer index.*\"\n#    )\n#    testthat::expect_error({ tre[[c(\"sensitivity\", \"profiles\")]] },\n#        regexp = \".*Please specifying a single string assay name or integer index.*\"\n#    )\n#})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `subset` method for the `LongTable` class in this code, and how does it handle different types of queries?",
        "answer": "The `subset` method for the `LongTable` class is used to create a subset of the original data based on specified criteria. It can handle different types of queries:\n1. Call queries: Using conditions like `treatment1id %in% unique(treatment1id)[1:5]`\n2. Regex queries: Using patterns like `c(\"Vinblastine\", \"Temozolomide\")`\n3. Row and column names: Directly specifying row and column names\n4. Index-based subsetting: Using numerical indices\n\nThe method ensures that the resulting subset maintains the correct structure and relationships between rowData, colData, and assays, while also handling edge cases like empty subsets or NA values in key columns."
      },
      {
        "question": "How does the `reindex` method for the `LongTable` class work, and what are its key features?",
        "answer": "The `reindex` method for the `LongTable` class is used to rebuild the internal index of the object. Its key features include:\n1. It does not mutate the original object by reference.\n2. It produces the same index as the LongTable constructor.\n3. It maintains data relationships between assays, rowData, and colData.\n4. It removes gaps in keys for subset LongTable objects, ensuring continuous key values.\n5. It works on all assays in the LongTable, including the main assays and potentially summary assays.\n\nThe method is useful for optimizing the internal structure of a LongTable object, especially after subsetting operations that might leave gaps in the key values."
      },
      {
        "question": "What are the main test cases covered in this code for the `subset` and `reindex` methods of the `LongTable` class?",
        "answer": "The main test cases covered for the `subset` and `reindex` methods include:\n\nFor `subset`:\n1. Subsetting with call queries\n2. Subsetting with regex queries\n3. Comparing subsetting by index to raw data subsetting\n4. Subsetting with row and column names\n5. Ensuring no non-existing assay observations are produced\n6. Checking that no assay observations are missed for selected rows/columns\n7. Verifying that subsetting behavior matches data.table for various edge cases (e.g., NULL, empty string, negative indices)\n\nFor `reindex`:\n1. Verifying that reindexing doesn't mutate by reference\n2. Checking that reindexed object has the same index as a newly constructed LongTable\n3. Ensuring that reindexing doesn't corrupt data relationships\n4. Confirming that reindexing removes gaps in keys for subset LongTables\n\nThese tests cover a wide range of scenarios to ensure the robustness and correctness of the `subset` and `reindex` methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "testthat::test_that(\"reindex,LongTable-method removes gaps in keys in subset LongTable\", {\n    select_row <- seq.int(1, dim(tre)[1], by = 2)\n    stre <- tre[select_row, ] ## subset data\n    stre <- reindex(stre)\n    ## check if rowData and colData keys have gaps\n    row_keys <- rowData(stre, key = TRUE)$rowKey\n    col_keys <- colData(stre, key = TRUE)$colKey\n    has_no_gaps_in_row <- rle(diff(row_keys))$value == 1\n    has_no_gaps_in_col <- rle(diff(col_keys))$value == 1\n    testthat::expect_true(has_no_gaps_in_row)\n    testthat::expect_true(has_no_gaps_in_col)\n    ## check if assays' keys have gaps\n    for (i in seq_along(assayNames(stre))) {\n        assay_name <- assayNames(stre)[i]\n        assay_keys <- assay(stre, i, key = FALSE, withDimnames = FALSE, metadata=FALSE)[[paste0(\".\", assay_name)]]\n        has_no_gaps_in_assay <- rle(diff(assay_keys))$value == 1\n        testthat::expect_true(has_no_gaps_in_assay)\n    }\n})",
        "complete": "testthat::test_that(\"reindex,LongTable-method removes gaps in keys in subset LongTable\", {\n    select_row <- seq.int(1, dim(tre)[1], by = 2)\n    stre <- tre[select_row, ] ## subset data\n    stre <- reindex(stre)\n    ## check if rowData and colData keys have gaps\n    row_keys <- rowData(stre, key = TRUE)$rowKey\n    col_keys <- colData(stre, key = TRUE)$colKey\n    has_no_gaps_in_row <- all(rle(diff(row_keys))$values == 1)\n    has_no_gaps_in_col <- all(rle(diff(col_keys))$values == 1)\n    testthat::expect_true(has_no_gaps_in_row)\n    testthat::expect_true(has_no_gaps_in_col)\n    ## check if assays' keys have gaps\n    for (i in seq_along(assayNames(stre))) {\n        assay_name <- assayNames(stre)[i]\n        assay_keys <- assay(stre, i, key = FALSE, withDimnames = FALSE, metadata=FALSE)[[paste0(\".\", assay_name)]]\n        has_no_gaps_in_assay <- all(rle(diff(assay_keys))$values == 1)\n        testthat::expect_true(has_no_gaps_in_assay)\n    }\n})"
      },
      {
        "partial": "testthat::test_that(\"subset,LongTable-method works with row and column names\", {\n    ntre <- subset(tre, rownames(tre)[1:5], colnames(tre)[1:5])\n    testthat::expect_equal(rownames(ntre), rownames(tre)[1:5])\n    testthat::expect_true(all.equal(rowData(ntre), rowData(tre)[1:5, ]))\n    testthat::expect_equal(colnames(ntre), colnames(tre)[1:5])\n    testthat::expect_true(all.equal(colData(ntre), colData(tre)[1:5, ]))\n})",
        "complete": "testthat::test_that(\"subset,LongTable-method works with row and column names\", {\n    ntre <- subset(tre, rownames(tre)[1:5], colnames(tre)[1:5])\n    testthat::expect_equal(rownames(ntre), rownames(tre)[1:5])\n    testthat::expect_true(all.equal(rowData(ntre), rowData(tre)[1:5, ]))\n    testthat::expect_equal(colnames(ntre), colnames(tre)[1:5])\n    testthat::expect_true(all.equal(colData(ntre), colData(tre)[1:5, ]))\n    # Check if assays are correctly subsetted\n    for (assay_name in assayNames(tre)) {\n        original_assay <- assay(tre, assay_name)\n        subsetted_assay <- assay(ntre, assay_name)\n        testthat::expect_true(all(subsetted_assay$rowKey %in% rowData(ntre)$rowKey))\n        testthat::expect_true(all(subsetted_assay$colKey %in% colData(ntre)$colKey))\n    }\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/allGenerics.R",
    "language": "R",
    "content": "# ==== CoreSet\n\n#' Summarize across replicates for a sensitivity dose-response experiment\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to summarize sensitivity profiles for.\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"summarizeSensitivityProfiles\",\n    function(object, ...) standardGeneric(\"summarizeSensitivityProfiles\"))\n\n\n#' Summarize molecular profile data such that there is a single entry for each\n#'   sample line/treatment combination\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to summarize the molecular profiles for.\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"summarizeMolecularProfiles\",\n    function(object, ...) standardGeneric(\"summarizeMolecularProfiles\"))\n\n\n#' Get the annotations for a `Signature` class object, as returned by\n#'   `drugSensitivitysig` or `radSensitivtySig` functions available in\n#'   `PharmacoGx` and `RadioGx`, respectively.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object A `Signature` class object\n#' @param ... Allow definition of new arguments to this generic\n#'\n#' @return NULL Prints the signature annotations to console\n#'\n#' @export\nsetGeneric(\"showSigAnnot\",\n    function(object, ...) standardGeneric(\"showSigAnnot\"))\n\n\n#' Generic function to get the annotations for a treatment response experiment\n#'   from an S4 class\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to get treatment response experiment\n#'    annotations from.\n#' @param ... Allow new arguments to be defined for this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"sensitivityInfo\",\n    function(object, ...) standardGeneric(\"sensitivityInfo\"))\n\n\n#' sensitivityInfo<- Generic Method\n#'\n#' Generic function to get the annotations for a treatment response experiment\n#'   from an S4 class.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to set treatment response experiment\n#'    annotations for.\n#' @param ... Allow new arguments to be defined for this generic.\n#' @param value The new treatment response experiment annotations.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"sensitivityInfo<-\",\n    function(object, ..., value) standardGeneric(\"sensitivityInfo<-\"))\n\n\n#' sensitivityRaw Generic Method\n#'\n#' Generic function to get the raw data array for a treatment response experiment\n#'   from an S4 class.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to extract the raw sensitivity experiment\n#'     data from.\n#' @param ... `pairlist`  Allow new parameters to be defined for this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"sensitivityRaw\",\n    function(object, ...) standardGeneric(\"sensitivityRaw\"))\n\n#' sensitivityRaw<- Generic\n#'\n#' Generic function to set the raw data array for a treatment response experiment\n#'   in an S4 class.\n#'\n#' @param object An `S4` object to extract the raw sensitivity data from.\n#' @param ... `pairlist` Allow new parameters to be defined for this generic.\n#' @param value An object containing dose and viability metrics to update\n#'   the object with.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"sensitivityRaw<-\",\n    function(object, ..., value) standardGeneric(\"sensitivityRaw<-\"))\n\n#' sensitivityProfiles Generic\n#'\n#' A generic for sensitivityProfiles getter method\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object The `S4` object to retrieve sensitivity profile summaries\n#'   from.\n#' @param ... `pairlist` Allow defining new arguments for this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"sensitivityProfiles\", function(object, ...) standardGeneric(\"sensitivityProfiles\"))\n\n#' sensitivityProfiles<- Generic\n#'\n#' A generic for the sensitivityProfiles replacement method\n#'\n#' @param object An `S4` object to update the sensitivity profile summaries\n#'    for.\n#' @param ... Fallthrough arguments for defining new methods\n#' @param value An object with the new sensitivity profiles. If a\n#'   matrix object is passed in, converted to data.frame before assignment\n#'\n#' @return Updated \\code{CoreSet}\n#'\n#' @export\nsetGeneric(\"sensitivityProfiles<-\",\n    function(object, ..., value) standardGeneric(\"sensitivityProfiles<-\"))\n\n#' sensitivityMeasures Generic\n#'\n#' Get the names of the sensitivity summary metrics available in an S4\n#'   object.\n#'\n#' @examples\n#' sensitivityMeasures(clevelandSmall_cSet)\n#'\n#' @param object An `S4` object to retrieve the names of sensitivty summary\n#'    measurements for.\n#' @param ... Fallthrough arguements for defining new methods\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric(\"sensitivityMeasures\",\n    function(object, ...) standardGeneric(\"sensitivityMeasures\"))\n\n#' sensitivityMeasures<- Generic\n#'\n#' Set the names of the sensitivity summary metrics available in an S4\n#'   object.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to update.\n#' @param ... Allow new methods to be defined for this generic.\n#' @param value A set of names for sensitivity measures to use to\n#'   update the object with.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric('sensitivityMeasures<-',\n    function(object, ..., value) standardGeneric('sensitivityMeasures<-'))\n\n#' sensitivitySlotToLongTable Generic\n#'\n#' Convert the sensitivity slot in an object inheriting from a CoreSet from a\n#'   list to a LongTable.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `CoreSet` Object inheriting from CoreSet.\n#' @param ... Allow new arguments to be defined on this generic.\n#'\n#' @return A `LongTable` object containing the data in the sensitivity slot.\n#'\n#' @export\nsetGeneric('sensitivitySlotToLongTable',\n    function(object, ...) standardGeneric('sensitivitySlotToLongTable'))\n\n# ==== LongTable Class\n\n#' Generic method for resetting indexing in an S4 object\n#'\n#' This method allows integer indexes used to maintain referential integrity\n#'   internal to an S4 object to be reset. This is useful particularly after\n#'   subsetting an object, as certain indexes may no longer be present in the\n#'   object data. Reindexing removes gaps integer indexes and ensures that the\n#'   smallest contiguous integer values are used in an objects indexes.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to redo indexing for\n#' @param ... `pairlist` Allow definition of new parameters to this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric('reindex', function(object, ...) standardGeneric('reindex'))\n\n#' Build a LongTable object\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param from What to build the LongTable from?\n#' @param ... `pairlist` Allow definition of new parameters for\n#'     implementations of this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric('buildLongTable',\n    function(from, ...) standardGeneric('buildLongTable'))\n\n\n#' Perform aggregation over an S4 object, but return an object of the same\n#' class.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param x An `S4` object to endomorphically aggregate over.\n#' @param ... `pairlist` Allow definition of new parameters for\n#'     implementations of this generic.\n#'\n#' @return An object with the same class as `x`.\n#'\n#' @export\nsetGeneric(\"endoaggregate\", function(x, ...) standardGeneric(\"endoaggregate\"))\n\n#' Retrieve a set of assayKeys\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param x An `S4` object.\n#' @param ... `pairlist` Allow definition of new parameters for\n#'     implementations of this generic.\n#'\n#' @return An object representing the \"assayKeys\" of an `S4` object.\n#'\n#' @export\nsetGeneric(\"assayKeys\", function(x, ...) standardGeneric(\"assayKeys\"))\n\n\n#' Retrieve and assayIndex\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param x An `S4` object.\n#' @param ... `pairlist` Allow definition of new parameters for\n#'     implementations of this generic.\n#'\n#' @return An object representing the \"assayIndex\" of an `S4` object.\n#'\n#' @export\nsetGeneric(\"assayIndex\", function(x, ...) standardGeneric(\"assayIndex\"))\n\n# ===== Other Generics\n\n\n#' Retrieve the specified item from object internal metadata.\n#'\n#' Internal slot for storing metadata relevant to the internal operation of an\n#'     S4 object.\n#'\n#' Warning: This method is intended for developer use and can be ignored by\n#'   users.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object with an @.itern slot containing an environment.\n#' @param x `character` One or more symbol names to retrieve from the\n#'    object@.intern environment.\n#' @param ... Allow new parmeters to be defined for this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export getIntern\nsetGeneric('getIntern',\n    function(object, x, ...) standardGeneric('getIntern'))\n\n\n#' Set the internal structural metadata for an S4 class\n#'\n#' @param object An R object to update internal structural metadata for.\n#' @param value An `immutable_list` object, being a class union between `list`\n#'   and `immutable` S3 classes.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @return Updates the object and returns invisibly.\n#'\n#' @keywords internal\nsetGeneric(\"getIntern<-\",\n    function(object, ..., value) standardGeneric(\"getIntern<-\"))\n\n\n#' Generic to access the row identifiers from\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to get row id columns from.\n#' @param ... Allow new arguments to this generic.\n#'\n#' @return Depends on the implemented method.\n#'\n#' @export\nsetGeneric('rowIDs', function(object, ...) standardGeneric('rowIDs'))\n\n\n#' Generic to access the row identifiers from\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to get row metadata columns from.\n#' @param ... Allow new arguments to this generic.\n#'\n#' @return Depends on the implemented method.\n#'\n#' @export\nsetGeneric('rowMeta', function(object, ...) standardGeneric('rowMeta'))\n\n\n#' Generic to access the row identifiers for an object.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to get column id columns from.\n#' @param ... ALlow new arguments to this generic\n#'\n#' @return Depends on the implemented method.\n#'\n#' @export\nsetGeneric('colIDs', function(object, ...) standardGeneric('colIDs'))\n\n\n#' Generic to access the column identifiers for a rectangular object.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to get column metadata columns from.\n#' @param ... ALlow new arguments to this generic\n#'\n#' @return Depends on impemented method.\n#'\n#' @export\nsetGeneric('colMeta', function(object, ...) standardGeneric('colMeta'))\n\n\n#' Generic to access the assay columns of a rectangular object.\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to get assay ids from.\n#' @param ... Allow new arguments to this generic.\n#'\n#' @return Depends on the implemented method.\n#'\n#' @export\nsetGeneric('assayCols',\n    function(object, ...) standardGeneric('assayCols'))\n\n\n#' Generic to access the unique id columns in an S4 object used to\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object An `S4` object to get id columns from.\n#' @param ... Allow new arguments to this generic.\n#'\n#' @return Depends on the implemented method\n#'\n#' @export\nsetGeneric('idCols',\n    function(object, ...) standardGeneric('idCols'))\n\n##' Generic to access the build configuration for an S4 object.\n##'\n##' @param object `S4` The object to retireve the configuration from.\n##'\n##'\n#setGeneric('getConfig', function(object, ...) standardGeneric(''))",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setGeneric` function in this code snippet?",
        "answer": "The `setGeneric` function is used to define generic methods in R's S4 object-oriented system. It creates a new generic function or adds a method to an existing generic. In this code, it's used to define several generic methods like `summarizeSensitivityProfiles`, `sensitivityInfo`, and `sensitivityRaw`, allowing for method dispatch based on the class of the object passed to these functions."
      },
      {
        "question": "How does the `standardGeneric` function work within the `setGeneric` calls?",
        "answer": "The `standardGeneric` function is used within the body of each generic function defined by `setGeneric`. It serves as a placeholder for method dispatch, indicating that the actual implementation of the function will be provided by specific methods defined for different classes. When the generic function is called, `standardGeneric` ensures that the appropriate method is selected and executed based on the class of the object passed to the function."
      },
      {
        "question": "What is the significance of the `...` parameter in the generic function definitions?",
        "answer": "The `...` (ellipsis) parameter in the generic function definitions allows for additional arguments to be passed to the function. This provides flexibility for different implementations of the generic to accept and use additional parameters as needed. It's particularly useful when creating a generic interface that can be extended with new functionality in specific method implementations without modifying the generic definition itself."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setGeneric(\"summarizeSensitivityProfiles\",\n    function(object, ...) standardGeneric(\"summarizeSensitivityProfiles\"))\n\nsetGeneric(\"summarizeMolecularProfiles\",\n    function(object, ...) standardGeneric(\"summarizeMolecularProfiles\"))\n\nsetGeneric(\"showSigAnnot\",\n    function(object, ...) standardGeneric(\"showSigAnnot\"))\n\nsetGeneric(\"sensitivityInfo\",\n    function(object, ...) standardGeneric(\"sensitivityInfo\"))\n\nsetGeneric(\"sensitivityInfo<-\",\n    function(object, ..., value) standardGeneric(\"sensitivityInfo<-\"))\n\nsetGeneric(\"sensitivityRaw\",\n    function(object, ...) standardGeneric(\"sensitivityRaw\"))\n\nsetGeneric(\"sensitivityRaw<-\",\n    function(object, ..., value) standardGeneric(\"sensitivityRaw<-\"))\n\nsetGeneric(\"sensitivityProfiles\", function(object, ...) standardGeneric(\"sensitivityProfiles\"))\n\nsetGeneric(\"sensitivityProfiles<-\",\n    function(object, ..., value) standardGeneric(\"sensitivityProfiles<-\"))\n\nsetGeneric(\"sensitivityMeasures\",\n    function(object, ...) standardGeneric(\"sensitivityMeasures\"))\n\nsetGeneric('sensitivityMeasures<-',\n    function(object, ..., value) standardGeneric('sensitivityMeasures<-'))\n\nsetGeneric('sensitivitySlotToLongTable',\n    function(object, ...) standardGeneric('sensitivitySlotToLongTable'))\n\nsetGeneric('reindex', function(object, ...) standardGeneric('reindex'))\n\nsetGeneric('buildLongTable',\n    function(from, ...) standardGeneric('buildLongTable'))\n\nsetGeneric(\"endoaggregate\", function(x, ...) standardGeneric(\"endoaggregate\"))\n\nsetGeneric(\"assayKeys\", function(x, ...) standardGeneric(\"assayKeys\"))\n\nsetGeneric(\"assayIndex\", function(x, ...) standardGeneric(\"assayIndex\"))\n\nsetGeneric('getIntern',\n    function(object, x, ...) standardGeneric('getIntern'))\n\nsetGeneric(\"getIntern<-\",\n    function(object, ..., value) standardGeneric(\"getIntern<-\"))\n\nsetGeneric('rowIDs', function(object, ...) standardGeneric('rowIDs'))\n\nsetGeneric('rowMeta', function(object, ...) standardGeneric('rowMeta'))\n\nsetGeneric('colIDs', function(object, ...) standardGeneric('colIDs'))\n\nsetGeneric('colMeta', function(object, ...) standardGeneric('colMeta'))\n\nsetGeneric('assayCols',\n    function(object, ...) standardGeneric('assayCols'))\n\nsetGeneric('idCols',\n    function(object, ...) standardGeneric('idCols'))",
        "complete": "setGeneric(\"summarizeSensitivityProfiles\", function(object, ...) standardGeneric(\"summarizeSensitivityProfiles\"))\nsetGeneric(\"summarizeMolecularProfiles\", function(object, ...) standardGeneric(\"summarizeMolecularProfiles\"))\nsetGeneric(\"showSigAnnot\", function(object, ...) standardGeneric(\"showSigAnnot\"))\nsetGeneric(\"sensitivityInfo\", function(object, ...) standardGeneric(\"sensitivityInfo\"))\nsetGeneric(\"sensitivityInfo<-\", function(object, ..., value) standardGeneric(\"sensitivityInfo<-\"))\nsetGeneric(\"sensitivityRaw\", function(object, ...) standardGeneric(\"sensitivityRaw\"))\nsetGeneric(\"sensitivityRaw<-\", function(object, ..., value) standardGeneric(\"sensitivityRaw<-\"))\nsetGeneric(\"sensitivityProfiles\", function(object, ...) standardGeneric(\"sensitivityProfiles\"))\nsetGeneric(\"sensitivityProfiles<-\", function(object, ..., value) standardGeneric(\"sensitivityProfiles<-\"))\nsetGeneric(\"sensitivityMeasures\", function(object, ...) standardGeneric(\"sensitivityMeasures\"))\nsetGeneric('sensitivityMeasures<-', function(object, ..., value) standardGeneric('sensitivityMeasures<-'))\nsetGeneric('sensitivitySlotToLongTable', function(object, ...) standardGeneric('sensitivitySlotToLongTable'))\nsetGeneric('reindex', function(object, ...) standardGeneric('reindex'))\nsetGeneric('buildLongTable', function(from, ...) standardGeneric('buildLongTable'))\nsetGeneric(\"endoaggregate\", function(x, ...) standardGeneric(\"endoaggregate\"))\nsetGeneric(\"assayKeys\", function(x, ...) standardGeneric(\"assayKeys\"))\nsetGeneric(\"assayIndex\", function(x, ...) standardGeneric(\"assayIndex\"))\nsetGeneric('getIntern', function(object, x, ...) standardGeneric('getIntern'))\nsetGeneric(\"getIntern<-\", function(object, ..., value) standardGeneric(\"getIntern<-\"))\nsetGeneric('rowIDs', function(object, ...) standardGeneric('rowIDs'))\nsetGeneric('rowMeta', function(object, ...) standardGeneric('rowMeta'))\nsetGeneric('colIDs', function(object, ...) standardGeneric('colIDs'))\nsetGeneric('colMeta', function(object, ...) standardGeneric('colMeta'))\nsetGeneric('assayCols', function(object, ...) standardGeneric('assayCols'))\nsetGeneric('idCols', function(object, ...) standardGeneric('idCols'))"
      },
      {
        "partial": "setGeneric(\"summarizeSensitivityProfiles\", function(object, ...) standardGeneric(\"summarizeSensitivityProfiles\"))\n\nsetGeneric(\"summarizeMolecularProfiles\", function(object, ...) standardGeneric(\"summarizeMolecularProfiles\"))\n\nsetGeneric(\"showSigAnnot\", function(object, ...) standardGeneric(\"showSigAnnot\"))\n\nsetGeneric(\"sensitivityInfo\", function(object, ...) standardGeneric(\"sensitivityInfo\"))\n\nsetGeneric(\"sensitivityInfo<-\", function(object, ..., value) standardGeneric(\"sensitivityInfo<-\"))\n\nsetGeneric(\"sensitivityRaw\", function(object, ...) standardGeneric(\"sensitivityRaw\"))\n\nsetGeneric(\"sensitivityRaw<-\", function(object, ..., value) standardGeneric(\"sensitivityRaw<-\"))\n\nsetGeneric(\"sensitivityProfiles\", function(object, ...) standardGeneric(\"sensitivityProfiles\"))\n\nsetGeneric(\"sensitivityProfiles<-\", function(object, ..., value) standardGeneric(\"sensitivityProfiles<-\"))\n\nsetGeneric(\"sensitivityMeasures\", function(object, ...) standardGeneric(\"sensitivityMeasures\"))\n\nsetGeneric('sensitivityMeasures<-', function(object, ..., value) standardGeneric('sensitivityMeasures<-'))\n\nsetGeneric('sensitivitySlotToLongTable', function(object, ...) standardGeneric('sensitivitySlotToLongTable'))\n\nsetGeneric('reindex', function(object, ...) standardGeneric('reindex'))\n\nsetGeneric('buildLongTable', function(from, ...) standardGeneric('buildLongTable'))\n\nsetGeneric(\"endoaggregate\", function(x, ...) standardGeneric(\"endoaggregate\"))\n\nsetGeneric(\"assayKeys\", function(x, ...) standardGeneric(\"assayKeys\"))\n\nsetGeneric(\"assayIndex\", function(x, ...) standardGeneric(\"assayIndex\"))\n\nsetGeneric('getIntern', function(object, x, ...) standardGeneric('getIntern'))\n\nsetGeneric(\"getIntern<-\", function(object, ..., value) standardGeneric(\"getIntern<-\"))\n\nsetGeneric('rowIDs', function(object, ...) standardGeneric('rowIDs'))\n\nsetGeneric('rowMeta', function(object, ...) standardGeneric('rowMeta'))\n\nsetGeneric('colIDs', function(object, ...) standardGeneric('colIDs'))\n\nsetGeneric('colMeta', function(object, ...) standardGeneric('colMeta'))\n\nsetGeneric('assayCols', function(object, ...) standardGeneric('assayCols'))\n\nsetGeneric('idCols', function(object, ...) standardGeneric('idCols'))",
        "complete": "setGeneric(\"summarizeSensitivityProfiles\", function(object, ...) standardGeneric(\"summarizeSensitivityProfiles\"))\nsetGeneric(\"summarizeMolecularProfiles\", function(object, ...) standardGeneric(\"summarizeMolecularProfiles\"))\nsetGeneric(\"showSigAnnot\", function(object, ...) standardGeneric(\"showSigAnnot\"))\nsetGeneric(\"sensitivityInfo\", function(object, ...) standardGeneric(\"sensitivityInfo\"))\nsetGeneric(\"sensitivityInfo<-\", function(object, ..., value) standardGeneric(\"sensitivityInfo<-\"))\nsetGeneric(\"sensitivityRaw\", function(object, ...) standardGeneric(\"sensitivityRaw\"))\nsetGeneric(\"sensitivityRaw<-\", function(object, ..., value) standardGeneric(\"sensitivityRaw<-\"))\nsetGeneric(\"sensitivityProfiles\", function(object, ...) standardGeneric(\"sensitivityProfiles\"))\nsetGeneric(\"sensitivityProfiles<-\", function(object, ..., value) standardGeneric(\"sensitivityProfiles<-\"))\nsetGeneric(\"sensitivityMeasures\", function(object, ...) standardGeneric(\"sensitivityMeasures\"))\nsetGeneric('sensitivityMeasures<-', function(object, ..., value) standardGeneric('sensitivityMeasures<-'))\nsetGeneric('sensitivitySlotToLongTable', function(object, ...) standardGeneric('sensitivitySlotToLongTable'))\nsetGeneric('reindex', function(object, ...) standardGeneric('reindex'))\nsetGeneric('buildLongTable', function(from, ...) standardGeneric('buildLongTable'))\nsetGeneric(\"endoaggregate\", function(x, ...) standardGeneric(\"endoaggregate\"))\nsetGeneric(\"assayKeys\", function(x, ...) standardGeneric(\"assayKeys\"))\nsetGeneric(\"assayIndex\", function(x, ...) standardGeneric(\"assayIndex\"))\nsetGeneric('getIntern', function(object, x, ...) standardGeneric('getIntern'))\nsetGeneric(\"getIntern<-\", function(object, ..., value) standardGeneric(\"getIntern<-\"))\nsetGeneric('rowIDs', function(object, ...) standardGeneric('rowIDs'))\nsetGeneric('rowMeta', function(object, ...) standardGeneric('rowMeta'))\nsetGeneric('colIDs', function(object, ...) standardGeneric('colIDs'))\nsetGeneric('colMeta', function(object, ...) standardGeneric('colMeta'))\nsetGeneric('assayCols', function(object, ...) standardGeneric('assayCols'))\nsetGeneric('idCols', function(object, ...) standardGeneric('idCols'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/callingWaterfall.R",
    "language": "R",
    "content": "#' Drug sensitivity calling using waterfall plots\n#'\n#' 1. Sensitivity calls were made using one of IC50, ActArea or Amax\n#'\n#' 2. Sort log IC50s (or ActArea or Amax) of the samples to generate a\n#'   \u201cwaterfall distribution\u201d\n#'\n#' 3. Identify cutoff:\n#'\n#'  3.1 If the waterfall distribution is non-linear (pearson cc to the linear\n#'    fit <=0.95), estimate the major inflection point of the log IC50 curve as\n#'    the point on the curve with the maximal distance to a line drawn between\n#'    the start and end points of the distribution.\n#'\n#'  3.2 If the waterfall distribution appears linear (pearson cc to the linear\n#'    fit > 0.95), then use the median IC50 instead.\n#'\n#' 4. Samples within a 4-fold IC50 (or within a 1.2-fold ActArea or 20% Amax\n#'   difference) difference centered around this inflection point are classified\n#'   as being \u201cintermediate\u201d,  samples with lower IC50s (or ActArea/Amax\n#'   values) than this range are defined as sensitive, and those with IC50s (or\n#'   ActArea/Amax) higher than this range are called \u201cinsensitive\u201d.\n#'\n#' 5. Require at least x sensitive and x insensitive samples after applying\n#'   these criteria (x=5 in our case).\n#'\n## FIXME:: Write a real example\n#' @examples\n#' # Dummy example\n#' 1 + 1\n#'\n## FIXME:: Clarify the parameters of this function\n#' @param x What type of object does this take in?\n#' @param type\n#'   ic50: IC50 values in micro molar (positive values)\n#'   actarea: Activity Area, that is area under the drug activity curve (positive values)\n#'   amax: Activity at max concentration (positive values)\n#'\n#' @param intermediate.fold vector of fold changes used to define the intermediate sensitivities for ic50, actarea and amax respectively\n#' @param cor.min.linear \\code{numeric} The minimum linear correlation to\n#'   require?\n#' @param name \\code{character} The name of the output to use in plot\n#' @param plot \\code{boolean} Whether to plot the results\n#'\n#' @return \\code{factor} Containing the drug sensitivity status of each\n#'   sample.\n#'\n#' @importFrom stats complete.cases  cor.test lm median\n#' @importFrom graphics par points abline lines legend\n#' @importFrom grDevices rainbow\n#'\n#' @export\n#' @keywords internal\ncallingWaterfall <- function(x, type = c(\"IC50\", \"AUC\", \"AMAX\"), intermediate.fold = c(4, 1.2, 1.2), cor.min.linear = 0.95, name = \"Drug\",\n    plot = FALSE) {\n\n    type <- match.arg(type)\n    if (any(!is.na(intermediate.fold) & intermediate.fold < 0)) {\n        intermediate.fold <- intermediate.fold[!is.na(intermediate.fold) & intermediate.fold < 0] <- 0\n    }\n    if (is.null(names(x))) {\n        names(x) <- paste(\"X\", seq_along(x), sep = \".\")\n    }\n\n    xx <- x[complete.cases(x)]\n    switch(type, IC50 = {\n        xx <- -log10(xx)\n        ylabel <- \"-log10(IC50)\"\n        ## 4 fold difference around IC50 cutoff\n        if (length(intermediate.fold) == 3) {\n            intermediate.fold <- intermediate.fold[1]\n        }\n        if (intermediate.fold != 0) {\n            interfold <- log10(intermediate.fold)\n        } else {\n            interfold <- 0\n        }\n    }, AUC = {\n        ylabel <- \"AUC\"\n        ## 1.2 fold difference around Activity Area cutoff\n        if (length(intermediate.fold) == 3) {\n            intermediate.fold <- intermediate.fold[2]\n        }\n        interfold <- intermediate.fold\n    }, AMAX = {\n        ylabel <- \"Amax\"\n        ## 1.2 fold difference around Amax\n        if (length(intermediate.fold) == 3) {\n            intermediate.fold <- intermediate.fold[3]\n        }\n        interfold <- intermediate.fold\n    })\n\n    if (length(xx) < 3) {\n        tt <- array(NA, dim = length(x), dimnames = list(names(x)))\n        if (interfold == 0) {\n            tt <- factor(tt, levels = c(\"resistant\", \"sensitive\"))\n        } else {\n            tt <- factor(tt, levels = c(\"resistant\", \"intermediate\", \"sensitive\"))\n        }\n        return(tt)\n    }\n\n    oo <- order(xx, decreasing = TRUE)\n    ## test linearity with Pearson correlation\n    cc <- stats::cor.test(-xx[oo], seq_along(oo), method = \"pearson\")\n    ## line between the two extreme sensitivity values\n    dd <- cbind(y = xx[oo][c(1, length(oo))], x = c(1, length(oo)))\n    rr <- lm(y ~ x, data = data.frame(dd))\n    ## compute distance from sensitivity values and the line between the two extreme sensitivity values\n    ddi <- apply(cbind(seq_along(oo), xx[oo]), 1, function(x, slope, intercept) {\n        return(.distancePointLine(x = x[1], y = x[2], a = slope, b = intercept))\n    }, slope = rr$coefficients[2], intercept = rr$coefficients[1])\n    if (cc$estimate > cor.min.linear) {\n        ## approximately linear waterfall\n        cutoff <- which.min(abs(xx[oo] - median(xx[oo])))\n        cutoffn <- names(cutoff)[1]\n    } else {\n        ## non linear waterfall identify cutoff as the maximum distance\n        cutoff <- which.max(abs(ddi))\n        cutoffn <- names(ddi)[cutoff]\n    }\n    ## identify intermediate sensitivities\n    switch(type, IC50 = {\n        if (interfold == 0) {\n            rang <- c(xx[oo][cutoff], xx[oo][cutoff])\n        } else {\n            rang <- c(xx[oo][cutoff] - interfold, xx[oo][cutoff] + interfold)\n        }\n    }, AUC = {\n        if (interfold == 0) {\n            rang <- c(xx[oo][cutoff], xx[oo][cutoff])\n        } else {\n            rang <- c(xx[oo][cutoff]/interfold, xx[oo][cutoff] * interfold)\n        }\n    }, AMAX = {\n        if (interfold == 0) {\n            rang <- c(xx[oo][cutoff], xx[oo][cutoff])\n        } else {\n            rang <- c(xx[oo][cutoff]/interfold, xx[oo][cutoff] * interfold)\n        }\n    })\n\n\n    ## check whether range is either min or max\n    if (rang[2] >= max(xx)) {\n        rang[2] <- sort(unique(xx), decreasing = TRUE)[2]\n    }\n    if (rang[2] <= min(xx)) {\n        rang[2] <- sort(unique(xx), decreasing = FALSE)[2]\n    }\n    if (rang[1] <= min(xx)) {\n        rang[1] <- sort(unique(xx), decreasing = FALSE)[2]\n    }\n    if (rang[1] >= max(xx)) {\n        rang[1] <- sort(unique(xx), decreasing = TRUE)[2]\n    }\n\n    ## compute calls\n    calls <- rep(NA, length(xx))\n    names(calls) <- names(xx)\n    calls[xx < rang[1]] <- \"resistant\"\n    calls[xx >= rang[2]] <- \"sensitive\"\n    calls[xx >= rang[1] & xx < rang[2]] <- \"intermediate\"\n\n    if (plot) {\n        par(mfrow = c(2, 1))\n        ccols <- rainbow(4)\n        mycol <- rep(\"grey\", length(xx))\n        names(mycol) <- names(xx)\n        mycol[calls == \"sensitive\"] <- ccols[2]\n        mycol[calls == \"intermediate\"] <- ccols[3]\n        mycol[calls == \"resistant\"] <- ccols[4]\n        mycol[cutoffn] <- ccols[1]\n        mypch <- rep(16, length(xx))\n        names(mypch) <- names(xx)\n        mypch[cutoffn] <- 19\n        plot(xx[oo], col = mycol[oo], pch = mypch[oo], ylab = ylabel, main = sprintf(\"%s\\nWaterfall\", name))\n        points(x = cutoff, y = xx[cutoffn], pch = mypch[cutoffn], col = mycol[cutoffn])\n        graphics::abline(a = rr$coefficients[1], b = rr$coefficients[2], lwd = 2, col = \"darkgrey\")\n        lines(x = c(cutoff, cutoff), y = c(par(\"usr\")[3], xx[cutoffn]), col = \"red\")\n        lines(x = c(par(\"usr\")[1], cutoff), y = c(xx[cutoffn], xx[cutoffn]), col = \"red\")\n        legend(\"topright\", legend = c(sprintf(\"resistant (n=%i)\", sum(!is.na(calls) & calls == \"resistant\")), sprintf(\"intermediate (n=%i)\",\n            sum(!is.na(calls) & calls == \"intermediate\")), sprintf(\"sensitive (n=%i)\", sum(!is.na(calls) & calls == \"sensitive\")), \"cutoff\",\n            sprintf(\"R=%.3g\", cc$estimate)), col = c(rev(ccols), NA), pch = c(16, 16, 16, 19, NA), bty = \"n\")\n\n        plot(ddi, pch = mypch[oo], col = mycol[oo], ylab = \"Distance\", main = sprintf(\"%s\\n%s\", name, \"Distance from min--max line\"))\n        points(x = cutoff, y = ddi[cutoffn], pch = mypch[cutoffn], col = mycol[cutoffn])\n        legend(\"topright\", legend = c(\"resistant\", \"intermediate\", \"sensitive\", \"cutoff\"), col = rev(ccols), pch = c(16, 16, 16, 19), bty = \"n\")\n    }\n\n    tt <- rep(NA, length(x))\n    names(tt) <- names(x)\n    tt[names(calls)] <- calls\n    if (interfold == 0) {\n        tt <- factor(tt, levels = c(\"resistant\", \"sensitive\"))\n    } else {\n        tt <- factor(tt, levels = c(\"resistant\", \"intermediate\", \"sensitive\"))\n    }\n    return(tt)\n}\n\n\n# Helper Functions --------------------------------------------------------\n\n#' Calculate shortest distance between point and line\n#'\n#' @examples .distancePointLine(0, 0, 1, -1, 1)\n#'\n#' @description This function calculates the shortest distance between a point\n#'   and a line in 2D space.\n#'\n#' @param x x-coordinate of point\n#' @param y y-coordinate of point\n#' @param a `numeric(1)` The coefficient in line equation a * x + b * y + c = 0.\n#'   Defaults to 1.\n#' @param b `numeric(1)` The coefficient in line equation a * x + b * y + c = 0.\n#'   Defaults to 1.\n#' @param c `numeric(1)` The intercept in line equation a * x + b * y + c = 0.\n#'   Defaults to 0.\n#'\n#' @return `numeric` The shortest distance between a point and a line.\n#'\n#' @export\n#' @keywords internal\n.distancePointLine <- function(x, y, a=1, b=1, c=0) {\n\n    if (!(all(is.finite(c(x, y, a, b, c))))) {\n        stop(\"All inputs to .distancePointLine must be real numbers.\")\n    }\n\n    return(abs(a * x + b * y + c) / sqrt(a^2 + b^2))\n}\n\n#' @export\n#' @keywords internal\n.magnitude <- function(p1, p2) {\n    return(sqrt(sum((p2 - p1)^2)))\n}\n\n#' Calculate shortest distance between point and line segment\n#'\n#' @description This function calculates the shortest distance between a point\n#'   and a line segment in 2D space.\n#'\n#' @param x x-coordinate of point\n#' @param y y-coordinate of point\n#' @param x1 x-coordinate of one endpoint of the line segment\n#' @param y1 y-coordinate of line segment endpoint with x-coordinate x1\n#' @param x2 x-coordinate of other endpoint of line segment\n#' @param y2 y-coordinate of line segment endpoint with x-coordinate x2\n#'\n#' @return \\code{numeric} The shortest distance between a point and a line\n#'   segment\n#'\n#' @examples .distancePointSegment(0, 0, -1, 1, 1, -1)\n#'\n#' @export\n#' @keywords internal\n.distancePointSegment <- function(x, y, x1, y1, x2, y2) {\n    if (!(all(is.finite(c(x, y, x1, x2, y1, y2))))) {\n        stop(\"All inputs to linePtDist must be real numbers.\")\n    }\n\n    bestEndpointDistance <- min(c(.magnitude(c(x, y), c(x1, y1)), .magnitude(c(x, y), c(x2, y2))))\n\n    if (.magnitude(c(x, y), c((x1 + x2)/2, (y1 + y2)/2)) < bestEndpointDistance) {\n        # length to point has only one local minimum which is the global minimum; iff this condition is true then the shortest distance is to a\n        # point on the line segment vertical line segment\n        if (x1 == x2) {\n            a <- 1\n            b <- 0\n            c <- -x1\n        } else {\n            a <- (y2 - y1)/(x2 - x1)\n            b <- -1\n            c <- y1 - a * x1\n        }\n        return(.distancePointLine(x, y, a, b, c))\n    } else {\n        return(bestEndpointDistance)\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `callingWaterfall` function in this code snippet?",
        "answer": "The `callingWaterfall` function is designed to perform drug sensitivity calling using waterfall plots. It analyzes drug sensitivity data (IC50, AUC, or AMAX) to classify samples as resistant, intermediate, or sensitive based on their response to a drug. The function uses a waterfall distribution approach to determine cutoffs for these classifications."
      },
      {
        "question": "How does the function determine if the waterfall distribution is linear or non-linear, and why is this important?",
        "answer": "The function uses Pearson correlation to test the linearity of the waterfall distribution. If the correlation coefficient is greater than `cor.min.linear` (default 0.95), it's considered linear. For linear distributions, the median value is used as the cutoff. For non-linear distributions, the point of maximum distance from a line between the start and end points is used. This distinction is important because it affects how the sensitivity cutoff is determined, which in turn impacts the classification of samples."
      },
      {
        "question": "What are the three helper functions included in this code snippet, and what do they do?",
        "answer": "The three helper functions are:\n1. `.distancePointLine`: Calculates the shortest distance between a point and a line in 2D space.\n2. `.magnitude`: Computes the magnitude (distance) between two points.\n3. `.distancePointSegment`: Calculates the shortest distance between a point and a line segment in 2D space.\nThese functions are used to support the main `callingWaterfall` function in its calculations, particularly when determining cutoffs in non-linear distributions."
      }
    ],
    "completion_tasks": [
      {
        "partial": "callingWaterfall <- function(x, type = c(\"IC50\", \"AUC\", \"AMAX\"), intermediate.fold = c(4, 1.2, 1.2), cor.min.linear = 0.95, name = \"Drug\", plot = FALSE) {\n    type <- match.arg(type)\n    if (any(!is.na(intermediate.fold) & intermediate.fold < 0)) {\n        intermediate.fold <- intermediate.fold[!is.na(intermediate.fold) & intermediate.fold < 0] <- 0\n    }\n    if (is.null(names(x))) {\n        names(x) <- paste(\"X\", seq_along(x), sep = \".\")\n    }\n\n    xx <- x[complete.cases(x)]\n    switch(type,\n        IC50 = {\n            xx <- -log10(xx)\n            ylabel <- \"-log10(IC50)\"\n            if (length(intermediate.fold) == 3) {\n                intermediate.fold <- intermediate.fold[1]\n            }\n            interfold <- if (intermediate.fold != 0) log10(intermediate.fold) else 0\n        },\n        AUC = {\n            ylabel <- \"AUC\"\n            if (length(intermediate.fold) == 3) {\n                intermediate.fold <- intermediate.fold[2]\n            }\n            interfold <- intermediate.fold\n        },\n        AMAX = {\n            ylabel <- \"Amax\"\n            if (length(intermediate.fold) == 3) {\n                intermediate.fold <- intermediate.fold[3]\n            }\n            interfold <- intermediate.fold\n        }\n    )\n\n    # Complete the function here\n}",
        "complete": "callingWaterfall <- function(x, type = c(\"IC50\", \"AUC\", \"AMAX\"), intermediate.fold = c(4, 1.2, 1.2), cor.min.linear = 0.95, name = \"Drug\", plot = FALSE) {\n    type <- match.arg(type)\n    if (any(!is.na(intermediate.fold) & intermediate.fold < 0)) {\n        intermediate.fold <- intermediate.fold[!is.na(intermediate.fold) & intermediate.fold < 0] <- 0\n    }\n    if (is.null(names(x))) {\n        names(x) <- paste(\"X\", seq_along(x), sep = \".\")\n    }\n\n    xx <- x[complete.cases(x)]\n    switch(type,\n        IC50 = {\n            xx <- -log10(xx)\n            ylabel <- \"-log10(IC50)\"\n            if (length(intermediate.fold) == 3) {\n                intermediate.fold <- intermediate.fold[1]\n            }\n            interfold <- if (intermediate.fold != 0) log10(intermediate.fold) else 0\n        },\n        AUC = {\n            ylabel <- \"AUC\"\n            if (length(intermediate.fold) == 3) {\n                intermediate.fold <- intermediate.fold[2]\n            }\n            interfold <- intermediate.fold\n        },\n        AMAX = {\n            ylabel <- \"Amax\"\n            if (length(intermediate.fold) == 3) {\n                intermediate.fold <- intermediate.fold[3]\n            }\n            interfold <- intermediate.fold\n        }\n    )\n\n    if (length(xx) < 3) {\n        tt <- array(NA, dim = length(x), dimnames = list(names(x)))\n        return(factor(tt, levels = if (interfold == 0) c(\"resistant\", \"sensitive\") else c(\"resistant\", \"intermediate\", \"sensitive\")))\n    }\n\n    oo <- order(xx, decreasing = TRUE)\n    cc <- stats::cor.test(-xx[oo], seq_along(oo), method = \"pearson\")\n    dd <- cbind(y = xx[oo][c(1, length(oo))], x = c(1, length(oo)))\n    rr <- lm(y ~ x, data = data.frame(dd))\n    ddi <- apply(cbind(seq_along(oo), xx[oo]), 1, function(x, slope, intercept) {\n        return(.distancePointLine(x = x[1], y = x[2], a = slope, b = intercept))\n    }, slope = rr$coefficients[2], intercept = rr$coefficients[1])\n\n    if (cc$estimate > cor.min.linear) {\n        cutoff <- which.min(abs(xx[oo] - median(xx[oo])))\n        cutoffn <- names(cutoff)[1]\n    } else {\n        cutoff <- which.max(abs(ddi))\n        cutoffn <- names(ddi)[cutoff]\n    }\n\n    rang <- switch(type,\n        IC50 = if (interfold == 0) c(xx[oo][cutoff], xx[oo][cutoff]) else c(xx[oo][cutoff] - interfold, xx[oo][cutoff] + interfold),\n        AUC = if (interfold == 0) c(xx[oo][cutoff], xx[oo][cutoff]) else c(xx[oo][cutoff]/interfold, xx[oo][cutoff] * interfold),\n        AMAX = if (interfold == 0) c(xx[oo][cutoff], xx[oo][cutoff]) else c(xx[oo][cutoff]/interfold, xx[oo][cutoff] * interfold)\n    )\n\n    rang[2] <- min(max(rang[2], sort(unique(xx), decreasing = FALSE)[2]), sort(unique(xx), decreasing = TRUE)[2])\n    rang[1] <- max(min(rang[1], sort(unique(xx), decreasing = TRUE)[2]), sort(unique(xx), decreasing = FALSE)[2])\n\n    calls <- cut(xx, breaks = c(-Inf, rang[1], rang[2], Inf), labels = c(\"resistant\", \"intermediate\", \"sensitive\"), include.lowest = TRUE)\n\n    if (plot) {\n        # Plotting code here (omitted for brevity)\n    }\n\n    tt <- rep(NA, length(x))\n    names(tt) <- names(x)\n    tt[names(calls)] <- as.character(calls)\n    return(factor(tt, levels = if (interfold == 0) c(\"resistant\", \"sensitive\") else c(\"resistant\", \"intermediate\", \"sensitive\")))\n}"
      },
      {
        "partial": ".distancePointLine <- function(x, y, a=1, b=1, c=0) {\n    if (!(all(is.finite(c(x, y, a, b, c))))) {\n        stop(\"All inputs to .distancePointLine must be real numbers.\")\n    }\n\n    # Complete the function here\n}",
        "complete": ".distancePointLine <- function(x, y, a=1, b=1, c=0) {\n    if (!(all(is.finite(c(x, y, a, b, c))))) {\n        stop(\"All inputs to .distancePointLine must be real numbers.\")\n    }\n\n    return(abs(a * x + b * y + c) / sqrt(a^2 + b^2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/LongTableDataMapper-accessors.R",
    "language": "R",
    "content": "#' @include LongTableDataMapper-class.R\nNULL\n\n## =====================================\n## LongTableDataMapper Accessors Methods\n## -------------------------------------\n\n\n## ---------------------\n## ---- all slot helpers\n\n\n#' Method to subset the rawdata with the corresponding dimensions \"dimDataMap\"\n#'   method.\n#'\n#' @param x `LongTableDataMapper` or inheriting class.\n#' @param key `logical(1)` Should the returned value be keyed by the 'id_columns'\n#'   item of the dimDataMap? Ignored when dim is \"meta\".\n#' @param dim `character(1)` Which dimension should rawdata to subset for?\n#'   Options are \"row\", \"col\" and \"meta\", corresponding to the associated\n#'   slots of the `LongTableDataMapper`.\n#'\n#' @importFrom checkmate assertClass assertLogical\n#' @importFrom methods getPackageName\n#' @noRd\n#' @keywords internal\n.get_dimData <- function(x, key, dim=c(\"row\", \"col\", \"meta\")) {\n\n    # Input validation\n    assertClass(x, \"LongTableDataMapper\")\n    assertLogical(key)\n\n    # Determine which slot and accessor function to use\n    dim <- match.arg(dim)\n    dimSlot <- paste0(dim, \"DataMap\")\n    dimFun <- get(dimSlot)\n\n    # Get method name to simplify debugging from S4 classes\n    funContext <- paste0(\"\\n[\", getPackageName(), \"::\", dimSlot, \",\",\n        class(x)[1], \"-method\\n\\t\")\n\n    .dimDataMap <- dimFun(x)\n\n    # Ensure required data is present\n    if (length(unlist(.dimDataMap)) < 1) stop(.errorMsg(funContext,\n        \"The \", dimSlot, \" slot must contain valid data!\"))\n\n    return(.get_dimDataFromMap(x=x, key=(key && dim != \"meta\"),\n        dataMap=.dimDataMap, funContext=funContext))\n}\n\n#' Method to subset the rawdata with the corresponding dimensions \"dimDataMap\"\n#'   method.\n#'\n#' @param x `LongTableDataMapper` or inheriting class.\n#' @param dataMap `list` The map of a `LongTableDataMapper` dimension, as\n#'   returned by the '*DataMap' methods. Can also be used with a single assay\n#'   from the `assayMap` method, but not the entire list of assays.\n#' @param key `logical(1)` Should the returned value be keyed by the 'id_columns'\n#'   item of the dimDataMap? Default is `TRUE`.\n#' @param rename `logical(1)` Should columns be renamed from their value to\n#'   their name, if the item has a name in the `dataMap`. Default is `TRUE`.\n#' @param funContext `character(1)` Contextual information about the calling\n#'   function, for debugging. Users don't need to worry about this.\n#'\n#' @importFrom checkmate assertClass assertLogical assertList\n#' @importFrom methods getPackageName\n#' @importFrom data.table setkeyv\n#' @noRd\n#' @keywords internal\n.get_dimDataFromMap <- function(x, dataMap, key=TRUE, rename=TRUE, funContext) {\n\n    if (missing(funContext))\n        funContext <- paste0(\"\\n[\", getPackageName(), \"::.get_dimDataMap]\\n\\t\")\n\n    # Input validation\n    checkmate::assertClass(x, \"LongTableDataMapper\")\n    checkmate::assertLogical(key)\n    checkmate::assertList(dataMap, types=c(\"character\", \"NULL\"), max.len=2)\n\n    # Extract relevant data\n    .rawdata <- rawdata(x)\n\n    # Ensure required data is present\n    if (length(.rawdata) < 1) .error(funContext,\n        \"The rawdata slot must contain valid data!\")\n    hasDimDataCols <- unlist(dataMap) %in% colnames(.rawdata)\n    if (!all(hasDimDataCols)) .error(funContext, \"Columns \",\n        .collapse(unlist(dataMap)[!hasDimDataCols]),\n        \" are missing from rawdata!\")\n\n    # Subset rawdata, optionally keying table and/or renaming columns\n    .dimData <- .rawdata[, .SD, .SDcols=unlist(dataMap)]\n    if (key) setkeyv(.dimData, dataMap$id_columns)\n    if (rename) {\n        old <- unlist(unname(dataMap))\n        new <- names(old)\n        if (!is.null(new)) {\n            names_idx <- new != \"\" & !is.na(new)\n            data.table::setnames(.dimData, old[names_idx], new[names_idx])\n        }\n    }\n\n    return(unique(.dimData))\n}\n\n\n## ---------------\n## -- rawdata slot\n\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_DataMapper_set_rawdata(class_=.local_class_3,\n#' class1_='list')\nsetReplaceMethod('rawdata', signature=c(object='LongTableDataMapper',\n        value='list'), function(object, value) {\n    funContext <- .S4MethodContext('rawdata<-', class(object)[1],\n        class(value)[1])\n\n    rows <- unlist(rowDataMap(object))\n    cols <- unlist(colDataMap(object))\n    assays <- unlist(assayMap(object))\n    meta <- unlist(metadataMap(object))\n\n    ## TODO:: Improve parsing here such that it only throws warnings if meta-\n    ##>data columns are missing\n\n    mandatory <- c(rows, cols, assays, meta)\n    if (!length(mandatory) || !length(value)) {\n        object@rawdata <- value\n    } else if (length(mandatory) && !length(rawdata(object))) {\n        hasMandatory <- mandatory %in% colnames(value)\n        if (!all(hasMandatory)) {\n            stop(.errorMsg(funContext, \"One or more map column is missing from value\",\n                \": \", paste0(mandatory[!hasMandatory], collapse=', '), '!'))\n        }\n        object@rawdata <- value\n    } else {\n        stop(.errorMsg(funContext, \"In order to assign to the rawdata slot of \",\n            \"a LongTableDataMapper, either all the map slots must be \",\n            \"empty or the rawdata slot must be an empty list!\"))\n    }\n    return(object)\n})\n\n\n## --------------------\n## ---- rowDataMap slot\n\n\n##\n## -- rowDataMap\n\n.docs_LongTableDataMapper_get_dimDataMap <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __{dim_}DataMap__: `list` of two `character` vectors, the first are the\n    columns required to uniquely identify each row of a `{class_}` and the\n    second any additional {dim_}-level metadata. If the character vectors\n    have names, the resulting columns are automatically renamed to the\n    item name of the specified column.\n\n    @examples\n    {dim_}DataMap({data_})\n\n    @md\n    @aliases {dim_}DataMap,{class_}-method\n    @exportMethod {dim_}DataMap\n    \",\n    ...\n)\n\n#' @export\nsetGeneric('rowDataMap', function(object, ...) standardGeneric('rowDataMap'))\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_get_dimDataMap(dim_='row', class_=.local_class_3,\n#' data_=.local_data_3)\nsetMethod('rowDataMap', signature(object='LongTableDataMapper'),\n        function(object) {\n    object@rowDataMap\n})\n\n#' @export\nsetGeneric('rowDataMap<-', function(object, ..., value)\n    standardGeneric('rowDataMap<-'))\n\n.docs_LongTableDataMapper_set_dimDataMap <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __{dim_}DataMap<-__: Update the `@{dim_}DataMap` slot of a `{class_}` object,\n    returning an invisible NULL. Arguments:\n    - value: A `list` or `List` where the first item is the names of the\n    identifier columns -- columns needed to uniquely identify each row in\n    {dim_}Data -- and the second item is the metadata associated with those\n    the identifier columns, but not required to uniquely identify rows in\n    the object rowData.\n\n    @examples\n    {dim_}DataMap({data_}) <- list(c('{id_col_}'), c())\n\n    @md\n    @aliases rowDataMap<-,{class_},list-method {dim_}DataMap<-{class_},List-method\n    @exportMethod {dim_}DataMap<-\n    \",\n    ...\n)\n\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_set_dimDataMap(dim_='row', class_=.local_class_3,\n#' data_=.local_data_3, id_col_='treatmentid')\nsetReplaceMethod('rowDataMap', signature(object='LongTableDataMapper',\n        value='list_OR_List'), function(object, value) {\n    funContext <- '[CoreGx::`rowDataMap<-`,LongTableDataMapper-method]\\n\\t'\n    rawdataCols <- colnames(rawdata(object))\n\n    # -- Handle error conditions\n    if (length(value) > 2) {\n        stop(.errorMsg(funContext, 'Assignments to rowDataMap should be a list ',\n            'of length 2, where the first item is the name of the id columns ',\n            'and the second item is the name of the metadata columns which ',\n            'map to those id columns.'))\n    }\n\n    hasIDcols <- value[[1]] %in% rawdataCols\n    if (!all(hasIDcols) && length(rawdata(object))) {\n        stop(.errorMsg(funContext, 'One or more of the id columns specified ',\n            'in value[[1]] are not valid column names in the rawdata slot of ',\n            'this ', class(object)[1], ' object!'))\n    }\n\n    if (length(value) > 1 && length(value[[2]]) != 0 && length(rawdata(object))) {\n        hasMetaCols <- value[[2]] %in% rawdataCols\n        if (!all(hasMetaCols)) {\n            stop(.errorMsg(funContext, 'The follow metadata columns in value[[2]] ',\n                'are not present in rawdata(object): ',\n                .collapse(value[[2]][!hasMetaCols]), '!'))\n        }\n        hasOneToOneRelationship <-\n            value[[2]] %in% cardinality(rawdata(object), group=value[[1]])\n        if (!all(hasOneToOneRelationship)) {\n            stop(.errorMsg(funContext, 'The columns ',\n                .collapse(value[[2]][!hasOneToOneRelationship], ' do not have a ',\n                '1:1 relationship with the specified ID columns!')))\n        }\n    }\n\n    # -- Function body\n    object@rowDataMap <- value\n    return(object)\n})\n\n\n##\n## -- rowData\n\n\n#' Convenience method to subset the `rowData` out of the `rawdata` slot using\n#'   the assigned `rowDataMap` metadata.\n#'\n#' @param x `LongTableDataMapper` object with valid data in the `rawdata` and\n#'   `colDataMap` slots.\n#' @param key `logical(1)` Should the table be keyed according to the\n#'   `id_columns` of the `rowDataMap` slot? This will sort the table in memory.\n#'   Default is TRUE.\n#'\n#' @return `data.table` The `rowData` as specified in the `rowDataMap` slot.\n#'\n#' @export\nsetMethod(\"rowData\", signature(\"LongTableDataMapper\"), function(x, key=TRUE) {\n    .get_dimData(x, key, dim=\"row\")\n})\n\n\n## --------------------\n## ---- colDataMap slot\n\n\n##\n## -- colDataMap\n\n#' @export\nsetGeneric('colDataMap', function(object, ...) standardGeneric('colDataMap'))\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_get_dimDataMap(dim_='col', class_=.local_class_3,\n#' data_=.local_data_3)\nsetMethod('colDataMap', signature(object='LongTableDataMapper'),\n        function(object) {\n    object@colDataMap\n})\n\n#' @export\nsetGeneric('colDataMap<-', function(object, ..., value) standardGeneric('colDataMap<-'))\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval\n#' .docs_LongTableDataMapper_set_dimDataMap(dim_='col', class_=.local_class_3,\n#' data_=.local_data_3, id_col_='sampleid')\nsetReplaceMethod('colDataMap',\n        signature(object='LongTableDataMapper', value='list_OR_List'),\n        function(object, value) {\n    funContext <- '[CoreGx::`colDataMap<-`,LongTableDataMapper-method]\\n\\t'\n    rawdataCols <- colnames(rawdata(object))\n\n    # -- Handle error conditions\n    if (length(value) > 2 || !is.list(value)) {\n        .error(funContext, 'Assignments to colDataMap should be a list ',\n            'of length 2, where the first item is the name of the id columns ',\n            'and the second item is the name of the metadata columns which ',\n            'map to those id columns.')\n    }\n\n    hasIDcols <- value[[1]] %in% rawdataCols\n    if (!all(hasIDcols) && length(rawdata(object))) {\n        .error(funContext, 'One or more of the id columns specified ',\n            'in value[[1]] are not valid column names in the rawdata slot of ',\n            'this ', class(object)[1], ' object!')\n    }\n\n    if (length(value) > 1 && length(value[[2]]) != 0 &&\n            length(rawdata(object))) {\n        hasMetaCols <- value[[2]] %in% rawdataCols\n        if (!all(hasMetaCols)) {\n            .error(funContext,\n                'The follow metadata columns in value[[2]] ',\n                'are not present in rawdata(object): ',\n                .collapse(value[[2]][!hasMetaCols]), '!')\n        }\n        hasOneToOneRelationship <-\n            value[[2]] %in% cardinality(rawdata(object), group=value[[1]])\n        if (!all(hasOneToOneRelationship)) {\n            .error(funContext, 'The columns ',\n                .collapse(value[[2]][!hasOneToOneRelationship]),\n                ' do not have a 1:1 relationship with the specified ID ',\n                'columns!')\n        }\n    }\n\n    # -- Function body\n    object@colDataMap <- value\n    return(object)\n})\n\n\n##\n## -- colData\n\n\n#' Convenience method to subset the `colData` out of the `rawdata` slot using\n#'   the assigned `colDataMap` metadata.\n#'\n#' @param x `LongTableDataMapper` object with valid data in the `rawdata` and\n#'   `colDataMap` slots.\n#' @param key `logical(1)` Should the table be keyed according to the\n#'   `id_columns` of the `colDataMap` slot? This will sort the table in memory.\n#'   Default is TRUE.\n#'\n#' @return `data.table` The `colData` as specified in the `colDataMap` slot.\n#'\n#' @export\nsetMethod(\"colData\", signature(\"LongTableDataMapper\"), function(x, key=TRUE) {\n    .get_dimData(x, key, dim=\"col\")\n})\n\n\n## ----------------\n## ---- assayMap slot\n\n\n#' @export\nsetGeneric('assayMap', function(object, ...) standardGeneric('assayMap'))\n\n.docs_LongTableDataMapper_get_assayMap <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __assayMap__:  A `list` of character vectors. The name of each list item\n    will be the assay in a `LongTableDataMapper` object that the columns in the\n    `character` vector will be assigned to. Column renaming occurs automatically\n    when the character vectors have names (from the value to the name).\n\n    @examples\n    assayMap({data_})\n\n    @md\n    @aliases assayMap,{class_},list-method assayMap,{class_},List-method\n    @exportMethod assayMap\n    \",\n    ...\n)\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_get_assayMap(class_=.local_class_3, data_=.local_data_3)\nsetMethod('assayMap', signature(object='LongTableDataMapper'),\n        function(object) {\n    object@assayMap\n})\n\n\n#' @export\nsetGeneric('assayMap<-', function(object, ..., value) standardGeneric('assayMap<-'))\n\n#' @noRd\n.docs_LongTableDataMapper_set_assayMap <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __assayMap<-__: Updates the `@assayMap` slot of a `{class_}` object,\n    returning an invisible NULL. Arguments:\n    - value:  A `list` of character vectors, where the name of each list\n    item is the name of an assay and the values of each character vector\n    specify the columns mapping to the assay in the `S4` object the\n    `{class_}` constructs.\n\n    @examples\n    assayMap({data_}) <- list(sensitivity=c(viability1='viability'))\n\n    @md\n    @aliases assayMap<-,{class_},list-method assayMap<-,{class_},List-methhod\n    @exportMethod assayMap<-\n    \",\n    ...\n)\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_set_assayMap(class_=.local_class_3, data_=.local_data_3)\nsetReplaceMethod('assayMap', signature(object='LongTableDataMapper',\n        value='list_OR_List'), function(object, value) {\n    funContext <- '[CoreGx::`assayMap<-,LongTableDataMapper-method`]\\n\\t'\n    rawdataCols <- colnames(rawdata(object))\n    if (length(names(value)) == 0) stop(.errorMsg('The value argument must\n        be a named list-like of column name character vectors!'))\n\n    for (i in seq_along(value)) {\n        hasRawdataCols <- unlist(value[[i]]) %in% rawdataCols\n        if (!all(hasRawdataCols) && length(rawdata(object))) {\n            stop(.errorMsg(funContext, 'There are no columns named ',\n                .collapse(unlist(value[[i]])[!hasRawdataCols]),\n                ' in the rawdata of this ', class(object)[1],\n                ' object. Please ensure item ',\n                names(value)[i], ' of value has valid column names.'))\n        }\n    }\n\n    object@assayMap <- value\n    return(object)\n})\n\n\n#' Extract the data for an assay from a `LongTableDataMapper`\n#'\n#' @param x `LongTableDataMapper` The object to retrive assay data form according\n#'   to the `assayMap` slot.\n#' @param i `character(1)` Name of an assay in the `assayMap` slot of `x`.\n#' @param withDimnames `logical(1)` For compatibility with\n#'   `SummarizedExperiment::assay` generic. Not used.\n#'\n#' @return `data.table` Data for the specified assay extracted from the\n#'   `rawdata` slot of `x`.\n#'\n#' @importFrom checkmate assertSubset assertCharacter\n#' @keywords internal\nsetMethod(\"assay\", signature(x=\"LongTableDataMapper\"),\n        function(x, i, withDimnames=TRUE) {\n\n    # Input validation\n    .assayMap <- assayMap(x)\n    assertCharacter(i, max.len=1)\n    assertSubset(i, names(.assayMap))\n\n    # Execution context\n    funContext <- paste0(\"\\n[\", getPackageName(), \"::assay,\", class(x)[1],\n        \"-method]\")\n\n    return(.get_dimDataFromMap(x, key=TRUE, .assayMap[[i]],\n        funContext=funContext))\n})\n\n\n#' Extract the data for all assays from a `LongTableDataMapper`\n#'\n#' @param x `LongTableDataMapper` The object to retrive assay data form according\n#'   to the `assayMap` slot.\n#' @param withDimNames `logical(1)` For compatibility with\n#'   `SummarizedExperiment::assay` generic. Not used.\n#'\n#' @return `list` Data for all assays extracted from the\n#'   `rawdata` slot of `x` as a `list` of `data.tables`, where the `keys` for\n#'   each table are their `id_columns`.\n#'\n#' @importFrom checkmate assertSubset assertCharacter\n#' @keywords internal\nsetMethod(\"assays\", signature(x=\"LongTableDataMapper\"),\n        function(x, withDimnames=TRUE) {\n    lapply(names(assayMap(x)), FUN=assay, x=x) |>\n        setNames(names(assayMap(x)))\n})\n\n# -- metadataMap\n\n\n#' @export\nsetGeneric('metadataMap', function(object, ...) standardGeneric('metadataMap'))\n\n#' @noRd\n.docs_LongTableDataMapper_get_metadataMap <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __metadataMap__:  A `list` of `character` vectors. Each item is an element\n    of the constructed objects `@metadata` slot.\n\n    @examples\n    metadataMap({data_})\n\n    @md\n    @aliases metadataMap,{class_}-method\n    @exportMethod metadataMap\n    \",\n    ...\n)\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_get_metadataMap(class_=.local_class_3, data_=.local_data_3)\nsetMethod('metadataMap', signature(object='LongTableDataMapper'),\n        function(object) {\n    object@metadataMap\n})\n\n\n#' @export\nsetGeneric('metadataMap<-', function(object, ..., value)\n    standardGeneric('metadataMap<-'))\n\n#' @noRd\n.docs_LongTableDataMapper_set_metadataMap <- function(...) .parseToRoxygen(\n    \"\n    @details\n    __metadataMap<-__: Updates `{class_}` object in-place, then returns an\n    `invisible(NULL)`. Arguments:\n    - value:  A `list` of `character` vectors. The name of each list item\n    is the name of the item in the `@metadata` slot of the `{class_}` object\n    created when `metaConstruct` is called on the `DataMapper`, and a\n    character vector specifies the columns of `@rawdata` to assign to each item.\n\n    @examples\n    metadataMap({data_}) <- list(object_metadata=c('metadata'))\n\n    @md\n    @aliases metadataMap<-,{class_}-method\n    @exportMethod metadataMap<-\n    \",\n    ...\n)\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_LongTableDataMapper_set_metadataMap(class_=.local_class_3, data_=.local_data_3, col_='metadata')\nsetReplaceMethod('metadataMap', signature(object='LongTableDataMapper',\n    value='list_OR_List'), function(object, value) {\n    funContext <- '[CoreGx::`metadataMap<-,LongTableDataMapper-method`]\\n\\t'\n    rawdataCols <- colnames(rawdata(object))\n    if (length(names(value)) == 0) stop(.errorMsg('The value argument must\n        be a named list-like of column name character vectors!'))\n\n    for (i in seq_along(value)) {\n        hasRawdataCols <- value[[i]] %in% rawdataCols\n        if (!all(hasRawdataCols) && length(rawdata(object))) {\n            stop(.errorMsg(funContext, 'There are no columns named ',\n                .collapse(value[[i]][!hasRawdataCols]), ' in the rawdata ',\n                'of this ', class(object)[1], ' object. Please ensure item ',\n                names(value)[i], ' of value has valid column names.'))\n        }\n    }\n\n    object@metadataMap <- value\n    return(object)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.get_dimData` function in this code snippet?",
        "answer": "The `.get_dimData` function is an internal helper method used to subset the rawdata of a LongTableDataMapper object based on a specified dimension (row, column, or metadata). It retrieves the appropriate data map, validates inputs, and calls `.get_dimDataFromMap` to perform the actual subsetting."
      },
      {
        "question": "How does the `rowDataMap<-` method handle error conditions when assigning new values to the rowDataMap slot?",
        "answer": "The `rowDataMap<-` method checks for several error conditions: 1) It ensures the input value is a list of length 2 or less. 2) It verifies that all specified ID columns exist in the rawdata. 3) If metadata columns are provided, it checks that they exist in the rawdata and have a one-to-one relationship with the ID columns. If any of these conditions are not met, it throws an error with a descriptive message."
      },
      {
        "question": "What is the purpose of the `assay` method for LongTableDataMapper objects, and what does it return?",
        "answer": "The `assay` method for LongTableDataMapper objects is used to extract data for a specific assay from the rawdata slot according to the assayMap. It takes the object, an assay name, and an unused withDimnames parameter. The method validates inputs, retrieves the appropriate assay map, and returns a data.table containing the extracted assay data, with keys set to the ID columns."
      }
    ],
    "completion_tasks": [
      {
        "partial": "## ---------------\n## -- rawdata slot\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_DataMapper_set_rawdata(class_=.local_class_3,\n#' class1_='list')\nsetReplaceMethod('rawdata', signature=c(object='LongTableDataMapper',\n        value='list'), function(object, value) {\n    funContext <- .S4MethodContext('rawdata<-', class(object)[1],\n        class(value)[1])\n\n    rows <- unlist(rowDataMap(object))\n    cols <- unlist(colDataMap(object))\n    assays <- unlist(assayMap(object))\n    meta <- unlist(metadataMap(object))\n\n    mandatory <- c(rows, cols, assays, meta)\n    if (!length(mandatory) || !length(value)) {\n        object@rawdata <- value\n    } else if (length(mandatory) && !length(rawdata(object))) {\n        hasMandatory <- mandatory %in% colnames(value)\n        if (!all(hasMandatory)) {\n            stop(.errorMsg(funContext, \"One or more map column is missing from value\",\n                \": \", paste0(mandatory[!hasMandatory], collapse=', '), '!'))\n        }\n        object@rawdata <- value\n    } else {\n        stop(.errorMsg(funContext, \"In order to assign to the rawdata slot of \",\n            \"a LongTableDataMapper, either all the map slots must be \",\n            \"empty or the rawdata slot must be an empty list!\"))\n    }\n    return(object)\n})",
        "complete": "## ---------------\n## -- rawdata slot\n\n#' @rdname LongTableDataMapper-accessors\n#' @eval .docs_DataMapper_set_rawdata(class_=.local_class_3,\n#' class1_='list')\nsetReplaceMethod('rawdata', signature=c(object='LongTableDataMapper',\n        value='list'), function(object, value) {\n    funContext <- .S4MethodContext('rawdata<-', class(object)[1],\n        class(value)[1])\n\n    rows <- unlist(rowDataMap(object))\n    cols <- unlist(colDataMap(object))\n    assays <- unlist(assayMap(object))\n    meta <- unlist(metadataMap(object))\n\n    mandatory <- c(rows, cols, assays, meta)\n    if (!length(mandatory) || !length(value)) {\n        object@rawdata <- value\n    } else if (length(mandatory) && !length(rawdata(object))) {\n        hasMandatory <- mandatory %in% colnames(value)\n        if (!all(hasMandatory)) {\n            stop(.errorMsg(funContext, \"One or more map column is missing from value\",\n                \": \", paste0(mandatory[!hasMandatory], collapse=', '), '!'))\n        }\n        object@rawdata <- value\n    } else {\n        stop(.errorMsg(funContext, \"In order to assign to the rawdata slot of \",\n            \"a LongTableDataMapper, either all the map slots must be \",\n            \"empty or the rawdata slot must be an empty list!\"))\n    }\n    return(object)\n})"
      },
      {
        "partial": "#' Extract the data for an assay from a `LongTableDataMapper`\n#'\n#' @param x `LongTableDataMapper` The object to retrive assay data form according\n#'   to the `assayMap` slot.\n#' @param i `character(1)` Name of an assay in the `assayMap` slot of `x`.\n#' @param withDimnames `logical(1)` For compatibility with\n#'   `SummarizedExperiment::assay` generic. Not used.\n#'\n#' @return `data.table` Data for the specified assay extracted from the\n#'   `rawdata` slot of `x`.\n#'\n#' @importFrom checkmate assertSubset assertCharacter\n#' @keywords internal\nsetMethod(\"assay\", signature(x=\"LongTableDataMapper\"),\n        function(x, i, withDimnames=TRUE) {\n\n    # Input validation\n    .assayMap <- assayMap(x)\n    assertCharacter(i, max.len=1)\n    assertSubset(i, names(.assayMap))\n\n    # Execution context\n    funContext <- paste0(\"\\n[\", getPackageName(), \"::assay,\", class(x)[1],\n        \"-method]\")\n\n    return(.get_dimDataFromMap(x, key=TRUE, .assayMap[[i]],\n        funContext=funContext))\n})",
        "complete": "#' Extract the data for an assay from a `LongTableDataMapper`\n#'\n#' @param x `LongTableDataMapper` The object to retrive assay data form according\n#'   to the `assayMap` slot.\n#' @param i `character(1)` Name of an assay in the `assayMap` slot of `x`.\n#' @param withDimnames `logical(1)` For compatibility with\n#'   `SummarizedExperiment::assay` generic. Not used.\n#'\n#' @return `data.table` Data for the specified assay extracted from the\n#'   `rawdata` slot of `x`.\n#'\n#' @importFrom checkmate assertSubset assertCharacter\n#' @keywords internal\nsetMethod(\"assay\", signature(x=\"LongTableDataMapper\"),\n        function(x, i, withDimnames=TRUE) {\n\n    # Input validation\n    .assayMap <- assayMap(x)\n    assertCharacter(i, max.len=1)\n    assertSubset(i, names(.assayMap))\n\n    # Execution context\n    funContext <- paste0(\"\\n[\", getPackageName(), \"::assay,\", class(x)[1],\n        \"-method]\")\n\n    return(.get_dimDataFromMap(x, key=TRUE, .assayMap[[i]],\n        funContext=funContext))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/plotCurve.R",
    "language": "R",
    "content": "#' Plot radiation dose-response curve\n#'\n#' This function plots doses of radiation against the cancer cell survival fractions thereby observed.\n#'\n#' @examples plotCurve(c(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10),\n#'   c(1.1, 0.8, 0.7, 0.45, 0.15, -0.1, -0.1, -0.4, -0.65, -0.75, -1.1),\n#'   filename = NULL)\n#'\n#' @param D vector of radiation doses\n#' @param SF vector of survival fractions corresponding to the doses\n#' @param pars parameters (alpha, beta) in the equation SF = exp(-alpha * D - beta * D ^ 2)\n#' @param filename name of PDF which will be created by the function\n#' @param fit_curve should the graph include a linear-quadratic curve of best fit? Defaults to TRUE\n#' @param SF_as_log should SF be expressed in log10 on the graph? Defaults to TRUE\n#'\n#' @return \\code{nothing} Function works by side effects only\n#'\n#' @importFrom graphics lines plot points axis\n#' @importFrom grDevices dev.off pdf\n#'\n#' @export\nplotCurve <- function(D, SF, pars, filename = \"dose_response_plot.pdf\", fit_curve = TRUE, SF_as_log = TRUE) {\n  CoreGx::.sanitizeInput(x = D,\n                          y = SF,\n                          x_as_log = FALSE,\n                          y_as_log = FALSE,\n                          y_as_pct = FALSE,\n                          trunc = FALSE,\n                          verbose = FALSE)\n\n  padding <- 1.1 # whitespace on graph around function range\n\n  if (fit_curve) {\n    if (missing(pars)) {\n      pars <- unlist(linearQuadraticModel(D, SF))\n    } else {\n      CoreGx::.sanitizeInput(pars = pars,\n                              x_as_log = FALSE,\n                              y_as_log = FALSE,\n                              y_as_pct = FALSE,\n                              trunc = FALSE,\n                              verbose = FALSE)\n    }\n    message(paste0(\"A linear-quadratic curve was fit to the data with parameters alpha = \", pars[[1]], \" and beta = \", pars[[2]], \".\"))\n    trendlineDs <- CoreGx::.getSupportVec(D)\n    trendlineSFs <- .linearQuadratic(trendlineDs, pars = pars, SF_as_log = TRUE)\n  }\n\n  xlim <- range(D)\n  xlim <- mean(xlim) + padding * c((xlim[1] - mean(xlim)), xlim[2] - mean(xlim))\n  if (!missing(SF)) {\n    DSF <- CoreGx::.reformatData(x = D,\n                                 y = SF,\n                                 x_to_log = FALSE,\n                                 y_to_log = TRUE,\n                                 y_to_frac = FALSE,\n                                 trunc = FALSE)\n    D <- DSF[[\"x\"]]\n    SF <- DSF[[\"y\"]]\n  }\n\n\n  if (TRUE) {\n    if (!missing(SF)) {\n      if (fit_curve) {\n        ylim <- padding * c(min(c(SF, trendlineSFs[length(trendlineSFs)])), 0)\n      } else {\n        ylim <- padding * c(min(SF), 0)\n      }\n    } else {\n      ylim <- padding * c(trendlineSFs[length(trendlineSFs)], 0)\n    }\n  } else {\n    ylim <- c(0, 1)\n  }\n\n  pdf(file = filename)\n\n  plot(NULL,\n       xlim = xlim,\n       ylim = ylim,\n       xlab = \"Dose (Gy)\",\n       ylab = \"Survival Fraction\",\n       col = \"red\",\n       yaxt=\"n\")\n\n  if (!missing(SF)) {\n    points(D, SF, col = \"red\", pch = 19)\n  }\n\n  if (missing(SF) || fit_curve) {\n    lines(trendlineDs, trendlineSFs, col = \"blue\", pch = 19)\n  }\n\n  ticks <- CoreGx::.getSupportVec(x=signif(ylim,1), 10)\n  labels <- unlist(lapply(ticks, function(i) as.expression(bquote(10^ .(round(i, 2))))))\n  axis(2, at=ticks, labels=labels)\n  dev.off()\n\n  return(invisible(0))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `plotCurve` function and what are its main input parameters?",
        "answer": "The `plotCurve` function is designed to plot radiation dose-response curves. It takes vectors of radiation doses (D) and corresponding survival fractions (SF) as main inputs. Optional parameters include `pars` for curve fitting, `filename` for output, `fit_curve` to toggle curve fitting, and `SF_as_log` to control y-axis scaling."
      },
      {
        "question": "How does the function handle the y-axis (Survival Fraction) representation, and what method is used for curve fitting?",
        "answer": "The function represents the Survival Fraction (SF) on a logarithmic scale by default (SF_as_log = TRUE). For curve fitting, it uses a linear-quadratic model, defined by the equation SF = exp(-alpha * D - beta * D^2). The `linearQuadraticModel` function is used to calculate the parameters (alpha and beta) if not provided."
      },
      {
        "question": "What are the side effects of the `plotCurve` function, and how does it handle input sanitization?",
        "answer": "The main side effect of `plotCurve` is creating a PDF file with the plot. It doesn't return any value (returns invisible(0)). For input sanitization, it uses the `CoreGx::.sanitizeInput` function to check and process the input data (D and SF vectors, and pars if provided) before plotting. This ensures the input is in the correct format and range for plotting."
      }
    ],
    "completion_tasks": [
      {
        "partial": "plotCurve <- function(D, SF, pars, filename = \"dose_response_plot.pdf\", fit_curve = TRUE, SF_as_log = TRUE) {\n  CoreGx::.sanitizeInput(x = D,\n                          y = SF,\n                          x_as_log = FALSE,\n                          y_as_log = FALSE,\n                          y_as_pct = FALSE,\n                          trunc = FALSE,\n                          verbose = FALSE)\n\n  padding <- 1.1\n\n  if (fit_curve) {\n    if (missing(pars)) {\n      pars <- unlist(linearQuadraticModel(D, SF))\n    } else {\n      CoreGx::.sanitizeInput(pars = pars,\n                              x_as_log = FALSE,\n                              y_as_log = FALSE,\n                              y_as_pct = FALSE,\n                              trunc = FALSE,\n                              verbose = FALSE)\n    }\n    message(paste0(\"A linear-quadratic curve was fit to the data with parameters alpha = \", pars[[1]], \" and beta = \", pars[[2]], \".\"))\n    trendlineDs <- CoreGx::.getSupportVec(D)\n    trendlineSFs <- .linearQuadratic(trendlineDs, pars = pars, SF_as_log = TRUE)\n  }\n\n  # Complete the function by adding code to plot the data and save it as a PDF\n}",
        "complete": "plotCurve <- function(D, SF, pars, filename = \"dose_response_plot.pdf\", fit_curve = TRUE, SF_as_log = TRUE) {\n  CoreGx::.sanitizeInput(x = D,\n                          y = SF,\n                          x_as_log = FALSE,\n                          y_as_log = FALSE,\n                          y_as_pct = FALSE,\n                          trunc = FALSE,\n                          verbose = FALSE)\n\n  padding <- 1.1\n\n  if (fit_curve) {\n    if (missing(pars)) {\n      pars <- unlist(linearQuadraticModel(D, SF))\n    } else {\n      CoreGx::.sanitizeInput(pars = pars,\n                              x_as_log = FALSE,\n                              y_as_log = FALSE,\n                              y_as_pct = FALSE,\n                              trunc = FALSE,\n                              verbose = FALSE)\n    }\n    message(paste0(\"A linear-quadratic curve was fit to the data with parameters alpha = \", pars[[1]], \" and beta = \", pars[[2]], \".\"))\n    trendlineDs <- CoreGx::.getSupportVec(D)\n    trendlineSFs <- .linearQuadratic(trendlineDs, pars = pars, SF_as_log = TRUE)\n  }\n\n  xlim <- range(D)\n  xlim <- mean(xlim) + padding * c((xlim[1] - mean(xlim)), xlim[2] - mean(xlim))\n  if (!missing(SF)) {\n    DSF <- CoreGx::.reformatData(x = D, y = SF, x_to_log = FALSE, y_to_log = TRUE, y_to_frac = FALSE, trunc = FALSE)\n    D <- DSF[\"x\"]\n    SF <- DSF[\"y\"]\n  }\n\n  ylim <- if (SF_as_log) {\n    if (!missing(SF)) {\n      if (fit_curve) padding * c(min(c(SF, trendlineSFs[length(trendlineSFs)])), 0) else padding * c(min(SF), 0)\n    } else {\n      padding * c(trendlineSFs[length(trendlineSFs)], 0)\n    }\n  } else {\n    c(0, 1)\n  }\n\n  pdf(file = filename)\n  plot(NULL, xlim = xlim, ylim = ylim, xlab = \"Dose (Gy)\", ylab = \"Survival Fraction\", col = \"red\", yaxt = \"n\")\n  if (!missing(SF)) points(D, SF, col = \"red\", pch = 19)\n  if (missing(SF) || fit_curve) lines(trendlineDs, trendlineSFs, col = \"blue\", pch = 19)\n  ticks <- CoreGx::.getSupportVec(x = signif(ylim, 1), 10)\n  labels <- lapply(ticks, function(i) as.expression(bquote(10^.(round(i, 2)))))\n  axis(2, at = ticks, labels = labels)\n  dev.off()\n\n  invisible(0)\n}"
      },
      {
        "partial": "plotCurve <- function(D, SF, pars, filename = \"dose_response_plot.pdf\", fit_curve = TRUE, SF_as_log = TRUE) {\n  # Add input sanitization and parameter initialization here\n\n  # Calculate xlim and ylim\n  xlim <- range(D)\n  xlim <- mean(xlim) + padding * c((xlim[1] - mean(xlim)), xlim[2] - mean(xlim))\n  \n  if (!missing(SF)) {\n    DSF <- CoreGx::.reformatData(x = D, y = SF, x_to_log = FALSE, y_to_log = TRUE, y_to_frac = FALSE, trunc = FALSE)\n    D <- DSF[\"x\"]\n    SF <- DSF[\"y\"]\n  }\n\n  ylim <- if (SF_as_log) {\n    # Calculate ylim based on SF_as_log condition\n  } else {\n    c(0, 1)\n  }\n\n  # Complete the function by adding code to create the plot and save it as a PDF\n}",
        "complete": "plotCurve <- function(D, SF, pars, filename = \"dose_response_plot.pdf\", fit_curve = TRUE, SF_as_log = TRUE) {\n  CoreGx::.sanitizeInput(x = D, y = SF, x_as_log = FALSE, y_as_log = FALSE, y_as_pct = FALSE, trunc = FALSE, verbose = FALSE)\n  padding <- 1.1\n\n  if (fit_curve) {\n    if (missing(pars)) {\n      pars <- unlist(linearQuadraticModel(D, SF))\n    } else {\n      CoreGx::.sanitizeInput(pars = pars, x_as_log = FALSE, y_as_log = FALSE, y_as_pct = FALSE, trunc = FALSE, verbose = FALSE)\n    }\n    message(paste0(\"A linear-quadratic curve was fit to the data with parameters alpha = \", pars[[1]], \" and beta = \", pars[[2]], \".\"))\n    trendlineDs <- CoreGx::.getSupportVec(D)\n    trendlineSFs <- .linearQuadratic(trendlineDs, pars = pars, SF_as_log = TRUE)\n  }\n\n  xlim <- range(D)\n  xlim <- mean(xlim) + padding * c((xlim[1] - mean(xlim)), xlim[2] - mean(xlim))\n  \n  if (!missing(SF)) {\n    DSF <- CoreGx::.reformatData(x = D, y = SF, x_to_log = FALSE, y_to_log = TRUE, y_to_frac = FALSE, trunc = FALSE)\n    D <- DSF[\"x\"]\n    SF <- DSF[\"y\"]\n  }\n\n  ylim <- if (SF_as_log) {\n    if (!missing(SF)) {\n      if (fit_curve) padding * c(min(c(SF, trendlineSFs[length(trendlineSFs)])), 0) else padding * c(min(SF), 0)\n    } else {\n      padding * c(trendlineSFs[length(trendlineSFs)], 0)\n    }\n  } else {\n    c(0, 1)\n  }\n\n  pdf(file = filename)\n  plot(NULL, xlim = xlim, ylim = ylim, xlab = \"Dose (Gy)\", ylab = \"Survival Fraction\", col = \"red\", yaxt = \"n\")\n  if (!missing(SF)) points(D, SF, col = \"red\", pch = 19)\n  if (missing(SF) || fit_curve) lines(trendlineDs, trendlineSFs, col = \"blue\", pch = 19)\n  ticks <- CoreGx::.getSupportVec(x = signif(ylim, 1), 10)\n  labels <- lapply(ticks, function(i) as.expression(bquote(10^.(round(i, 2)))))\n  axis(2, at = ticks, labels = labels)\n  dev.off()\n\n  invisible(0)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/summarizeSensitivityProfiles-methods.R",
    "language": "R",
    "content": "#' Takes the sensitivity data from a RadioSet, and summarises them into a\n#' drug vs cell line table\n#'\n#' This function creates a table with cell lines as rows and radiation types as columns,\n#' summarising the drug senstitivity data of a RadioSet into drug-cell line\n#' pairs\n#'\n#' @examples\n#' data(clevelandSmall)\n#' GDSCauc <- summarizeSensitivityProfiles(clevelandSmall, sensitivity.measure='AUC_published')\n#'\n#' @param object `RadioSet` The RadioSet from which to extract the data\n#' @param sensitivity.measure `character` which sensitivity sensitivity.measure to use? Use the\n#'   sensitivityMeasures function to find out what measures are available for each PSet.\n#' @param cell.lines `character` The cell lines to be summarized.\n#'    If any cell lines has no data, it will be filled with\n#'   missing values\n#' @param radiation.types `character` The radiation types to be summarized.\n#'   If any radiation type has no data, it will be filled with\n#'   missing values\n#' @param summary.stat `character` which summary method to use if there are repeated\n#'   cell line-drug experiments? Choices are \"mean\", \"median\", \"first\", or \"last\"\n#' @param fill.missing `logical(1)` should the missing cell lines not in the\n#'   molecular data object be filled in with missing values?\n#' @param verbose Should the function print progress messages?\n#'\n#' @return [matrix] A matrix with cell lines going down the rows, radiation types across\n#'   the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @importMethodsFrom CoreGx summarizeSensitivityProfiles\n#' @export\nsetMethod('summarizeSensitivityProfiles',\n          signature(object=\"RadioSet\"),\n          function(object, sensitivity.measure=\"AUC_recomputed\", cell.lines, radiation.types,\n                   summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"), fill.missing=TRUE,\n                   verbose=TRUE) {\n            .summarizeSensitivityProfilesRadioSet(\n              object, sensitivity.measure, cell.lines, radiation.types, summary.stat, fill.missing, verbose\n            )\n          })\n\n# Takes the sensitivity data from a RadioSet, and summarises them into a\n# drug vs cell line table\n#\n# This function creates a table with cell lines as rows and radiation types as columns,\n# summarising the drug senstitivity data of a RadioSet into drug-cell line\n# pairs\n#\n# @examples\n# data(clevelandSmall)\n# GDSCauc <- summarizeSensitivityProfiles(clevelandSmall, sensitivity.measure='AUC_published')\n#\n# @param object [RadioSet] The RadioSet from which to extract the data\n# @param sensitivity.measure `character` which sensitivity sensitivity.measure to use? Use the\n#   sensitivityMeasures function to find out what measures are available for each PSet.\n# @param cell.lines \\code{character} The cell lines to be summarized.\n#    If any cell lines has no data, it will be filled with\n#   missing values\n# @param radiation.types \\code{character} The radiation types to be summarized.\n#   If any radiation type has no data, it will be filled with\n#   missing values\n# @param summary.stat \\code{character} which summary method to use if there are repeated\n#   cell line-drug experiments? Choices are \"mean\", \"median\", \"first\", or \"last\"\n# @param fill.missing \\code{boolean} should the missing cell lines not in the\n#   molecular data object be filled in with missing values?\n# @param verbose Should the function print progress messages?\n#\n# @return [matrix] A matrix with cell lines going down the rows, radiation types across\n#   the columns, with the selected sensitivity statistic for each pair.\n#\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom stats median\n#' @importFrom reshape2 acast\n#' @keywords internal\n.summarizeSensitivityProfilesRadioSet <- function(\n  object,\n  sensitivity.measure=\"AUC_recomputed\",\n  cell.lines,\n  radiation.types,\n  summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n  fill.missing=TRUE,\n  verbose=TRUE)\n{\n\tsummary.stat <- match.arg(summary.stat)\n  #sensitivity.measure <- match.arg(sensitivity.measure)\n  if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(object)),\"max.conc\"))) {\n    stop (sprintf(\"Invalid sensitivity measure for %s, choose among: %s\",\n                  annotation(object)$name,\n                  paste(colnames(sensitivityProfiles(object)),\n                        collapse=\", \")))\n  }\n  if (missing(cell.lines)) {\n    cell.lines <- sampleNames(object)\n  }\n  if (missing(radiation.types)) {\n    if (sensitivity.measure != \"Synergy_score\")\n    {\n      radTypes <- treatmentNames(object)\n    }else{\n      radTypes <- sensitivityInfo(object)[grep(\"///\",\n                                             sensitivityInfo(object)$treatmentid),\n                                        \"treatmentid\"]\n    }\n  }\n\n  pp <- sensitivityInfo(object)\n  ##FIXME: deal with duplicated rownames!\n  ppRows <- which(pp$sampleid %in% cell.lines & pp$treatmentid %in% radTypes)\n  if(sensitivity.measure != \"max.conc\") {\n    dd <- sensitivityProfiles(object)\n  } else {\n\n    if(!\"max.conc\"%in% colnames(sensitivityInfo(object))){\n\n      object <- updateMaxConc(object)\n\n    }\n    dd <- sensitivityInfo(object)\n\n  }\n\n  result <- matrix(NA_real_, nrow=length(radTypes), ncol=length(cell.lines))\n  rownames(result) <- radTypes\n  colnames(result) <- cell.lines\n\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\")],\n                 \"sensitivity.measure\"=dd[, sensitivity.measure])\n\n  summary.function <- function(x) {\n    if(all(is.na(x))){\n      return(NA_real_)\n    }\n    switch(summary.stat,\n        \"mean\" = {\n          return(mean(as.numeric(x), na.rm=TRUE))\n        },\n        \"median\" = {\n          return(median(as.numeric(x), na.rm=TRUE))\n        },\n        \"first\" = {\n          return(as.numeric(x)[[1]])\n        },\n        \"last\" = {\n          return(as.numeric(x)[[length(x)]])\n        },\n        \"max\"= {\n          return(max(as.numeric(x), na.rm=TRUE))\n        },\n        \"min\" = {\n          return(min(as.numeric(x), na.rm=TRUE))\n        })\n  }\n\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"]%in%cell.lines &\n                   pp_dd[,\"treatmentid\"]%in%radTypes,]\n\n  tt <- reshape2::acast(pp_dd, treatmentid~sampleid,\n                        fun.aggregate=summary.function,\n                        value.var=\"sensitivity.measure\")\n\n  result[rownames(tt), colnames(tt)] <- tt\n\n\tif (!fill.missing) {\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n\t}\n  return(result)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeSensitivityProfiles` function in the given code snippet?",
        "answer": "The `summarizeSensitivityProfiles` function is designed to create a summary table of drug sensitivity data from a RadioSet object. It generates a matrix with cell lines as rows and radiation types as columns, summarizing the drug sensitivity data into drug-cell line pairs. The function allows users to specify various parameters such as the sensitivity measure, cell lines, radiation types, and summary statistics to customize the output."
      },
      {
        "question": "How does the function handle missing data in the sensitivity profiles?",
        "answer": "The function handles missing data in several ways: 1) If any specified cell lines or radiation types have no data, they are filled with missing values (NA). 2) The `fill.missing` parameter determines whether missing cell lines not in the molecular data object should be filled with missing values. 3) When summarizing repeated cell line-drug experiments, the function uses the specified `summary.stat` (e.g., mean, median, first, last) to aggregate the data, ignoring NA values when possible."
      },
      {
        "question": "What is the significance of the `@importMethodsFrom CoreGx summarizeSensitivityProfiles` line in the code?",
        "answer": "The `@importMethodsFrom CoreGx summarizeSensitivityProfiles` line is an Roxygen2 documentation tag that indicates the function is importing the `summarizeSensitivityProfiles` method from the `CoreGx` package. This suggests that the current implementation is extending or overriding a method from the `CoreGx` package, specifically for objects of class 'RadioSet'. It's part of the S4 object-oriented system in R, where methods can be defined for specific classes."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('summarizeSensitivityProfiles',\n          signature(object=\"RadioSet\"),\n          function(object, sensitivity.measure=\"AUC_recomputed\", cell.lines, radiation.types,\n                   summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"), fill.missing=TRUE,\n                   verbose=TRUE) {\n            # Complete the function body\n          })",
        "complete": "setMethod('summarizeSensitivityProfiles',\n          signature(object=\"RadioSet\"),\n          function(object, sensitivity.measure=\"AUC_recomputed\", cell.lines, radiation.types,\n                   summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"), fill.missing=TRUE,\n                   verbose=TRUE) {\n            .summarizeSensitivityProfilesRadioSet(\n              object, sensitivity.measure, cell.lines, radiation.types, summary.stat, fill.missing, verbose\n            )\n          })"
      },
      {
        "partial": ".summarizeSensitivityProfilesRadioSet <- function(\n  object,\n  sensitivity.measure=\"AUC_recomputed\",\n  cell.lines,\n  radiation.types,\n  summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n  fill.missing=TRUE,\n  verbose=TRUE\n) {\n  summary.stat <- match.arg(summary.stat)\n  if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(object)),\"max.conc\"))) {\n    stop(sprintf(\"Invalid sensitivity measure for %s, choose among: %s\",\n                 annotation(object)$name,\n                 paste(colnames(sensitivityProfiles(object)), collapse=\", \")))\n  }\n  # Complete the rest of the function\n}",
        "complete": ".summarizeSensitivityProfilesRadioSet <- function(\n  object,\n  sensitivity.measure=\"AUC_recomputed\",\n  cell.lines,\n  radiation.types,\n  summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n  fill.missing=TRUE,\n  verbose=TRUE\n) {\n  summary.stat <- match.arg(summary.stat)\n  if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(object)),\"max.conc\"))) {\n    stop(sprintf(\"Invalid sensitivity measure for %s, choose among: %s\",\n                 annotation(object)$name,\n                 paste(colnames(sensitivityProfiles(object)), collapse=\", \")))\n  }\n  if (missing(cell.lines)) cell.lines <- sampleNames(object)\n  if (missing(radiation.types)) {\n    radTypes <- if(sensitivity.measure != \"Synergy_score\") treatmentNames(object) else sensitivityInfo(object)[grep(\"///\", sensitivityInfo(object)$treatmentid), \"treatmentid\"]\n  }\n  pp <- sensitivityInfo(object)\n  ppRows <- which(pp$sampleid %in% cell.lines & pp$treatmentid %in% radTypes)\n  dd <- if(sensitivity.measure != \"max.conc\") sensitivityProfiles(object) else {\n    if(!\"max.conc\" %in% colnames(sensitivityInfo(object))) object <- updateMaxConc(object)\n    sensitivityInfo(object)\n  }\n  result <- matrix(NA_real_, nrow=length(radTypes), ncol=length(cell.lines), dimnames=list(radTypes, cell.lines))\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\")], \"sensitivity.measure\"=dd[, sensitivity.measure])\n  summary.function <- function(x) {\n    if(all(is.na(x))) return(NA_real_)\n    switch(summary.stat,\n           mean = mean(as.numeric(x), na.rm=TRUE),\n           median = median(as.numeric(x), na.rm=TRUE),\n           first = as.numeric(x)[[1]],\n           last = as.numeric(x)[[length(x)]],\n           max = max(as.numeric(x), na.rm=TRUE),\n           min = min(as.numeric(x), na.rm=TRUE))\n  }\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell.lines & pp_dd[,\"treatmentid\"] %in% radTypes,]\n  tt <- reshape2::acast(pp_dd, treatmentid~sampleid, fun.aggregate=summary.function, value.var=\"sensitivity.measure\")\n  result[rownames(tt), colnames(tt)] <- tt\n  if (!fill.missing) {\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n  }\n  return(result)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/TreatmentResponseExperiment-class.R",
    "language": "R",
    "content": "#' @include LongTable-class.R\n#' @noRd\nNULL\n\n#' @title TreatmentResponseExperiment class definition\n#'\n#' @description Define a private constructor method to be used to build a\n#'   `TreatmentResponseExperiment` object.\n#'\n#' @slot rowData See Slots section.\n#' @slot colData See Slots section.\n#' @slot assays See Slots section.\n#' @slot metadata See Slots section.\n#' @slot .intern See Slots section.\n#'\n#' @section Slots:\n#' - *rowData*: A `data.table` containing the metadata associated with the\n#'   row dimension of a `TreatmentResponseExperiment`.\n#' - *colData*: A `data.table` containing the metadata associated with the\n#'   column dimension of a `TreatmentResponseExperiment`.\n#' - *assays*: A `list` of `data.table`s, one for each assay in a\n#'   `TreatmentResponseExperiment`.\n#' - *metadata*: An optional `list` of additional metadata for a\n#'   `TreatmentResponseExperiment` which doesn't map to one of the dimensions.\n#' - *.intern*: An `environment` that holds internal structural metadata\n#'   about a `TreatmentResponseExperiment` object, such as which columns are\n#'   required to key the object. An environment has been used to allow locking\n#'   items, which can prevent accidental modification of a property required\n#'   for the class to work.\n#'\n#' @return `TreatmentResponseExperiment` object containing the assay data from\n#'   a treatment response experiment\n#'\n#' @md\n#' @import data.table\n#' @keywords internal\n#' @rdname TreatmentResponseExperiment-class\n#' @aliases .TreatmentResponseExperiment\n#' @exportClass TreatmentResponseExperiment\n.TreatmentResponseExperiment <- setClass(\"TreatmentResponseExperiment\",\n    contains=\"LongTable\")\n\n\n#' @title TreatmentResponseExperiment constructor method\n#'\n#' @rdname TreatmentResponseExperiment\n#'\n#' @description Builds a `TreatmentResponseExperiment` object from rectangular\n#' objects. The `rowData` argument should contain row level metadata, while\n#' the `colData` argument should contain column level metadata, for the\n#' experimental assays\n#' in the `assays` list. The `rowIDs` and `colIDs` lists are used to configure\n#' the internal keys mapping rows or columns to rows in the assays. Each list\n#' should contain at minimum one character vector, specifying which columns\n#' in `rowData` or `colData` are required to uniquely identify each row. An\n#' optional second character vector can be included, specifying any metadata\n#' columns for either dimension. These should contain information about each\n#' row but NOT be required to uniquely identify a row in the `colData` or\n#' `rowData` objects. Additional metadata can be attached to a\n#' `TreatmentResponseExperiment` by passing a list to the metadata argument.\n#'\n#' @details\n#' For now this class is simply a wrapper around a `LongTable` class. In the\n#' future we plan to refactor CoreGx such that the `LongTable` class is in a\n#' separate pacakge. We can then specialize the implementation of\n#' `TreatmentResponseExperiment` to better capture the biomedical nature of\n#' this object.\n#'\n#' @param rowData `data.table`, `data.frame`, `matrix` A table like object\n#'   coercible to a `data.table` containing the a unique `rowID` column which\n#'   is used to key assays, as well as additional row metadata to subset on.\n#' @param rowIDs `character`, `integer` A vector specifying\n#'   the names or integer indexes of the row data identifier columns. These\n#'   columns will be pasted together to make up the rownames of the\n#'   `TreatmentResponseExperiment` object.\n#' @param colData `data.table`, `data.frame`, `matrix` A table like object\n#'   coercible to a `data.table` containing the a unique `colID` column which\n#'   is used to key assays, as well as additional column metadata to subset on.\n#' @param colIDs `character`, `integer` A vector specifying\n#'   the names or integer indexes of the column data identifier columns. These\n#'   columns will be pasted together to make up the colnames of the\n#'   `TreatmentResponseExperiment` object.\n#' @param assayIDs `list` A list of `character` vectors specifying the columns\n#'   needed to uniquely identify each row in an `assay`. Names must match the\n#'   `assays` list.\n#' @param assays A `list` containing one or more objects coercible to a\n#'   `data.table`, and keyed by rowIDs and colIDs corresponding to the rowID and\n#'   colID columns in colData and rowData.\n#' @param metadata A `list` of metadata associated with the\n#'   `TreatmentResponseExperiment` object being constructed\n#' @param keep.rownames `logical`, `character`\n#'   Logical: whether rownames should be added as a column if coercing to a\n#'   `data.table`, default is FALSE. If TRUE, rownames are added to the column\n#'   'rn'.\n#'   Character: specify a custom column name to store the rownames in.\n#'\n#' @return A `TreatmentResponseExperiment` object containing the data for a\n#'   treatment response experiment configured according to the rowIDs and\n#'   colIDs arguments.\n#'\n#'\n#' @import data.table\n#' @export\nTreatmentResponseExperiment <- function(rowData, rowIDs, colData, colIDs,\n        assays, assayIDs, metadata=list(), keep.rownames=FALSE) {\n    if (!missing(rowData) && is(rowData, \"LongTable\")) {\n        LT <- rowData\n    } else {\n        LT <- LongTable(rowData=rowData, rowIDs=rowIDs, colData=colData,\n            colIDs=colIDs, assays=assays, assayIDs=assayIDs, metadata=metadata,\n            keep.rownames=keep.rownames)\n    }\n    .TreatmentResponseExperiment(\n        rowData=LT@rowData,\n        colData=LT@colData,\n        assays=LT@assays,\n        .intern=LT@.intern,\n        metadata=LT@metadata\n    )\n}\n\n#' @name as\n#'\n#' @title\n#' Coerce a `LongTable` to a `TreatmentResponseExperiment`\n#'\n#' @param from `LongTable` object to coerce to a `TreatmentResponseExperiment`.\n#'\n#' @return The data in `object`, as the child-class\n#'   `TreatmentResponseExperiment`.\n#'\n#' @seealso [`TreatmentResponseExperiment`]\n#'\n#' @examples\n#' data(clevelandSmall_cSet)\n#' TRE <- as(treatmentResponse(clevelandSmall_cSet),\n#'     \"TreatmentResponseExperiment\")\n#' TRE\n#'\n#' @md\n#' @export\nsetAs(\"LongTable\", \"TreatmentResponseExperiment\", function(from) {\n    TreatmentResponseExperiment(from)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `TreatmentResponseExperiment` class and how does it relate to the `LongTable` class?",
        "answer": "The `TreatmentResponseExperiment` class is designed to represent data from treatment response experiments. It is currently implemented as a wrapper around the `LongTable` class, with plans to refactor it in the future to better capture the biomedical nature of the data. The class contains slots for rowData, colData, assays, metadata, and internal structural metadata."
      },
      {
        "question": "Explain the purpose of the `rowIDs` and `colIDs` parameters in the `TreatmentResponseExperiment` constructor function.",
        "answer": "The `rowIDs` and `colIDs` parameters in the `TreatmentResponseExperiment` constructor function are used to configure the internal keys mapping rows or columns to rows in the assays. They should be character vectors specifying which columns in `rowData` or `colData` are required to uniquely identify each row. These columns will be pasted together to make up the rownames and colnames of the `TreatmentResponseExperiment` object, respectively."
      },
      {
        "question": "How can a `LongTable` object be coerced into a `TreatmentResponseExperiment` object?",
        "answer": "A `LongTable` object can be coerced into a `TreatmentResponseExperiment` object using the `as()` function. The code defines a method for this coercion using `setAs()`. To perform the coercion, you would use: `as(longTableObject, \"TreatmentResponseExperiment\")`. This will create a new `TreatmentResponseExperiment` object containing the data from the `LongTable` object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @title TreatmentResponseExperiment class definition\n#'\n#' @description Define a private constructor method to be used to build a\n#'   `TreatmentResponseExperiment` object.\n#'\n#' @slot rowData See Slots section.\n#' @slot colData See Slots section.\n#' @slot assays See Slots section.\n#' @slot metadata See Slots section.\n#' @slot .intern See Slots section.\n#'\n#' @section Slots:\n#' - *rowData*: A `data.table` containing the metadata associated with the\n#'   row dimension of a `TreatmentResponseExperiment`.\n#' - *colData*: A `data.table` containing the metadata associated with the\n#'   column dimension of a `TreatmentResponseExperiment`.\n#' - *assays*: A `list` of `data.table`s, one for each assay in a\n#'   `TreatmentResponseExperiment`.\n#' - *metadata*: An optional `list` of additional metadata for a\n#'   `TreatmentResponseExperiment` which doesn't map to one of the dimensions.\n#' - *.intern*: An `environment` that holds internal structural metadata\n#'   about a `TreatmentResponseExperiment` object, such as which columns are\n#'   required to key the object. An environment has been used to allow locking\n#'   items, which can prevent accidental modification of a property required\n#'   for the class to work.\n#'\n#' @return `TreatmentResponseExperiment` object containing the assay data from\n#'   a treatment response experiment\n#'\n#' @md\n#' @import data.table\n#' @keywords internal\n#' @rdname TreatmentResponseExperiment-class\n#' @aliases .TreatmentResponseExperiment\n#' @exportClass TreatmentResponseExperiment\n.TreatmentResponseExperiment <- setClass(\"TreatmentResponseExperiment\",\n    contains=\"LongTable\")\n\n# Complete the constructor method",
        "complete": "#' @title TreatmentResponseExperiment class definition\n#'\n#' @description Define a private constructor method to be used to build a\n#'   `TreatmentResponseExperiment` object.\n#'\n#' @slot rowData See Slots section.\n#' @slot colData See Slots section.\n#' @slot assays See Slots section.\n#' @slot metadata See Slots section.\n#' @slot .intern See Slots section.\n#'\n#' @section Slots:\n#' - *rowData*: A `data.table` containing the metadata associated with the\n#'   row dimension of a `TreatmentResponseExperiment`.\n#' - *colData*: A `data.table` containing the metadata associated with the\n#'   column dimension of a `TreatmentResponseExperiment`.\n#' - *assays*: A `list` of `data.table`s, one for each assay in a\n#'   `TreatmentResponseExperiment`.\n#' - *metadata*: An optional `list` of additional metadata for a\n#'   `TreatmentResponseExperiment` which doesn't map to one of the dimensions.\n#' - *.intern*: An `environment` that holds internal structural metadata\n#'   about a `TreatmentResponseExperiment` object, such as which columns are\n#'   required to key the object. An environment has been used to allow locking\n#'   items, which can prevent accidental modification of a property required\n#'   for the class to work.\n#'\n#' @return `TreatmentResponseExperiment` object containing the assay data from\n#'   a treatment response experiment\n#'\n#' @md\n#' @import data.table\n#' @keywords internal\n#' @rdname TreatmentResponseExperiment-class\n#' @aliases .TreatmentResponseExperiment\n#' @exportClass TreatmentResponseExperiment\n.TreatmentResponseExperiment <- setClass(\"TreatmentResponseExperiment\",\n    contains=\"LongTable\")\n\n#' @title TreatmentResponseExperiment constructor method\n#'\n#' @rdname TreatmentResponseExperiment\n#'\n#' @description Builds a `TreatmentResponseExperiment` object from rectangular\n#' objects. The `rowData` argument should contain row level metadata, while\n#' the `colData` argument should contain column level metadata, for the\n#' experimental assays in the `assays` list.\n#'\n#' @param rowData `data.table`, `data.frame`, `matrix` A table like object\n#'   coercible to a `data.table` containing the a unique `rowID` column which\n#'   is used to key assays, as well as additional row metadata to subset on.\n#' @param rowIDs `character`, `integer` A vector specifying\n#'   the names or integer indexes of the row data identifier columns.\n#' @param colData `data.table`, `data.frame`, `matrix` A table like object\n#'   coercible to a `data.table` containing the a unique `colID` column which\n#'   is used to key assays, as well as additional column metadata to subset on.\n#' @param colIDs `character`, `integer` A vector specifying\n#'   the names or integer indexes of the column data identifier columns.\n#' @param assayIDs `list` A list of `character` vectors specifying the columns\n#'   needed to uniquely identify each row in an `assay`. Names must match the\n#'   `assays` list.\n#' @param assays A `list` containing one or more objects coercible to a\n#'   `data.table`, and keyed by rowIDs and colIDs corresponding to the rowID and\n#'   colID columns in colData and rowData.\n#' @param metadata A `list` of metadata associated with the\n#'   `TreatmentResponseExperiment` object being constructed\n#' @param keep.rownames `logical`, `character`\n#'   Logical: whether rownames should be added as a column if coercing to a\n#'   `data.table`, default is FALSE. If TRUE, rownames are added to the column\n#'   'rn'.\n#'   Character: specify a custom column name to store the rownames in.\n#'\n#' @return A `TreatmentResponseExperiment` object containing the data for a\n#'   treatment response experiment configured according to the rowIDs and\n#'   colIDs arguments.\n#'\n#' @import data.table\n#' @export\nTreatmentResponseExperiment <- function(rowData, rowIDs, colData, colIDs,\n        assays, assayIDs, metadata=list(), keep.rownames=FALSE) {\n    if (!missing(rowData) && is(rowData, \"LongTable\")) {\n        LT <- rowData\n    } else {\n        LT <- LongTable(rowData=rowData, rowIDs=rowIDs, colData=colData,\n            colIDs=colIDs, assays=assays, assayIDs=assayIDs, metadata=metadata,\n            keep.rownames=keep.rownames)\n    }\n    .TreatmentResponseExperiment(\n        rowData=LT@rowData,\n        colData=LT@colData,\n        assays=LT@assays,\n        .intern=LT@.intern,\n        metadata=LT@metadata\n    )\n}"
      },
      {
        "partial": "#' @name as\n#'\n#' @title\n#' Coerce a `LongTable` to a `TreatmentResponseExperiment`\n#'\n#' @param from `LongTable` object to coerce to a `TreatmentResponseExperiment`.\n#'\n#' @return The data in `object`, as the child-class\n#'   `TreatmentResponseExperiment`.\n#'\n#' @seealso [`TreatmentResponseExperiment`]\n#'\n#' @examples\n#' data(clevelandSmall_cSet)\n#' TRE <- as(treatmentResponse(clevelandSmall_cSet),\n#'     \"TreatmentResponseExperiment\")\n#' TRE\n#'\n#' @md\n#' @export\nsetAs(\"LongTable\", \"TreatmentResponseExperiment\", function(from) {\n    # Complete the coercion function\n})",
        "complete": "#' @name as\n#'\n#' @title\n#' Coerce a `LongTable` to a `TreatmentResponseExperiment`\n#'\n#' @param from `LongTable` object to coerce to a `TreatmentResponseExperiment`.\n#'\n#' @return The data in `object`, as the child-class\n#'   `TreatmentResponseExperiment`.\n#'\n#' @seealso [`TreatmentResponseExperiment`]\n#'\n#' @examples\n#' data(clevelandSmall_cSet)\n#' TRE <- as(treatmentResponse(clevelandSmall_cSet),\n#'     \"TreatmentResponseExperiment\")\n#' TRE\n#'\n#' @md\n#' @export\nsetAs(\"LongTable\", \"TreatmentResponseExperiment\", function(from) {\n    TreatmentResponseExperiment(from)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/signatureClass.R",
    "language": "R",
    "content": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.RadioSig <- setClass('RadioSig', slots=list(\n\n            RSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\n#' Radiation Signature Class Constructor\n#'\n#' A documented constructor to provide user friendly interface to .RadioSig\n#'\n#' @param Data The data\n#' @param RSetName The name of the pSet\n#' @param DateCreated The date the object was created\n#' @param SigType The type of sensitivyt signature\n#' @param SessionInfo The package version used to generate the object\n#' @param Call The calls for sensitivity vs not\n#'\n#' @return A \\code{RadioSig} object\n#'\n#' @export\nRadioSig <- function(Data=array(NA, dim=c(0,0,0)),\n                     RSetName='',\n                     DateCreated=date(),\n                     SigType='sensitivity',\n                     SessionInfo=sessionInfo(),\n                     Call='No Call Recorded')\n{\nreturn(.RadioSig(Data,\n                 RSetName=RSetName,\n                 DateCreated=DateCreated,\n                 SigType=SigType,\n                 SessionInfo=SessionInfo,\n                 Call=Call))\n}\n\n#' Show RadioGx Signatures\n#'\n#' @examples\n#' data(clevelandSmall)\n#' rad.sensitivity <- radSensitivitySig(clevelandSmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(clevelandSmall, \"rna\")[1])\n#' rad.sensitivity\n#'\n#' @param object \\code{RadioSig}\n#'\n#' @return Prints the RadioGx Signatures object to the output stream, and returns invisible NULL.\n#'\n#' @export\nsetMethod(\"show\", signature=signature(object='RadioSig'),\n        function(object) {\n        cat('RadioSet Name: ', attr(object, 'RSetName'), \"\\n\")\n        cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n        cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n        cat(\"Number of Radiation Types: \", dim(object)[[2]], \"\\n\")\n        cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n           })\n\n#' Show the Annotations of a signature object\n#'\n#' This funtion prints out the information about the call used to compute the rad signatures, and the session info\n#' for the session in which the computation was done. Useful for determining the exact conditions used to generate signatures.\n#'\n#' @examples\n#' data(clevelandSmall)\n#' rad.sensitivity <- radSensitivitySig(clevelandSmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(clevelandSmall, \"rna\")[1])\n#' showSigAnnot(rad.sensitivity)\n#'\n#' @param object An object of the \\code{RadioSig} Class, as\n#' returned by \\code{radPerturbationSig} or \\code{radSensitivitySig}\n#'\n#' @return Prints the RadioGx Signatures annotations to the output stream, and returns invisible NULL.\n#'\n#' @importMethodsFrom CoreGx showSigAnnot\n#' @export\nsetMethod(\"showSigAnnot\", signature(object='RadioSig'), function(object) {\n  .showSigAnnotRadioSig(object)\n})\n\n\n#' @keywords internal\n.showSigAnnotRadioSig <- function(object) {\n  print(attr(object, 'Call'))\n  print(attr(object, 'SessionInfo'))\n  return(invisible(NULL))\n}\n\n\n\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'RadioSig' class and its constructor function in this code snippet?",
        "answer": "The 'RadioSig' class is defined to represent radiation signatures. Its constructor function provides a user-friendly interface to create 'RadioSig' objects. It takes parameters such as Data, RSetName, DateCreated, SigType, SessionInfo, and Call to initialize the object. The constructor ensures proper encapsulation and standardization of radiation signature data."
      },
      {
        "question": "How does the 'show' method for the 'RadioSig' class work, and what information does it display?",
        "answer": "The 'show' method is defined for the 'RadioSig' class using setMethod(). When called on a 'RadioSig' object, it displays key information about the radiation signature, including: the RadioSet Name, Signature Type, Date Created, Number of Radiation Types, and Number of Genes/Probes. This method provides a quick summary of the object's contents without revealing all the underlying data."
      },
      {
        "question": "What is the purpose of the 'showSigAnnot' method, and how does it differ from the 'show' method?",
        "answer": "The 'showSigAnnot' method is designed to display detailed annotations of a 'RadioSig' object. Unlike the 'show' method, which provides a summary, 'showSigAnnot' prints out information about the call used to compute the radiation signatures and the session info in which the computation was done. This method is useful for determining the exact conditions used to generate signatures, providing more in-depth information for reproducibility and debugging purposes."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.RadioSig <- setClass('RadioSig', slots=list(\n            RSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\n#' Radiation Signature Class Constructor\n#'\n#' A documented constructor to provide user friendly interface to .RadioSig\n#'\n#' @param Data The data\n#' @param RSetName The name of the pSet\n#' @param DateCreated The date the object was created\n#' @param SigType The type of sensitivyt signature\n#' @param SessionInfo The package version used to generate the object\n#' @param Call The calls for sensitivity vs not\n#'\n#' @return A \\code{RadioSig} object\n#'\n#' @export\nRadioSig <- function(Data=array(NA, dim=c(0,0,0)),\n                     RSetName='',\n                     DateCreated=date(),\n                     SigType='sensitivity',\n                     SessionInfo=sessionInfo(),\n                     Call='No Call Recorded')\n{\n# Complete the function body\n}",
        "complete": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.RadioSig <- setClass('RadioSig', slots=list(\n            RSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\n#' Radiation Signature Class Constructor\n#'\n#' A documented constructor to provide user friendly interface to .RadioSig\n#'\n#' @param Data The data\n#' @param RSetName The name of the pSet\n#' @param DateCreated The date the object was created\n#' @param SigType The type of sensitivyt signature\n#' @param SessionInfo The package version used to generate the object\n#' @param Call The calls for sensitivity vs not\n#'\n#' @return A \\code{RadioSig} object\n#'\n#' @export\nRadioSig <- function(Data=array(NA, dim=c(0,0,0)),\n                     RSetName='',\n                     DateCreated=date(),\n                     SigType='sensitivity',\n                     SessionInfo=sessionInfo(),\n                     Call='No Call Recorded')\n{\nreturn(.RadioSig(Data,\n                 RSetName=RSetName,\n                 DateCreated=DateCreated,\n                 SigType=SigType,\n                 SessionInfo=SessionInfo,\n                 Call=Call))\n}"
      },
      {
        "partial": "#' Show RadioGx Signatures\n#'\n#' @examples\n#' data(clevelandSmall)\n#' rad.sensitivity <- radSensitivitySig(clevelandSmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(clevelandSmall, \"rna\")[1])\n#' rad.sensitivity\n#'\n#' @param object \\code{RadioSig}\n#'\n#' @return Prints the RadioGx Signatures object to the output stream, and returns invisible NULL.\n#'\n#' @export\nsetMethod(\"show\", signature=signature(object='RadioSig'),\n        function(object) {\n        # Complete the function body\n        })",
        "complete": "#' Show RadioGx Signatures\n#'\n#' @examples\n#' data(clevelandSmall)\n#' rad.sensitivity <- radSensitivitySig(clevelandSmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(clevelandSmall, \"rna\")[1])\n#' rad.sensitivity\n#'\n#' @param object \\code{RadioSig}\n#'\n#' @return Prints the RadioGx Signatures object to the output stream, and returns invisible NULL.\n#'\n#' @export\nsetMethod(\"show\", signature=signature(object='RadioSig'),\n        function(object) {\n        cat('RadioSet Name: ', attr(object, 'RSetName'), \"\\n\")\n        cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n        cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n        cat(\"Number of Radiation Types: \", dim(object)[[2]], \"\\n\")\n        cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n        })"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/datasets.R",
    "language": "R",
    "content": "#' Cleaveland_mut RadioSet subsetted and cast as CoreSet\n#'\n#' This dataset is just a dummy object derived from the Cleveland_mut RadioSet\n#'   in the RadioGx R package. It's contents should not be interpreted and it\n#'   is only present to test the functions in this package and provide\n#'   examples\n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect \n#' small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name clevelandSmall_cSet\n#' @usage data(clevelandSmall_cSet)\n#' @keywords datasets\n#' @format CoreSet object\nNULL\n\n#' Merck Drug Combination Data LongTable\n#'\n#' This is a LongTable object created from some drug combination data provided\n#'   to our lab by Merck.\n#'\n#' @references\n#' TODO:: Include a reference\n#'\n#' @docType data\n#' @name merckLongTable\n#' @usage data(merckLongTable)\n#' @keywords datasets\n#' @format LongTable object\nNULL\n\n#' Example LongTableDataMapper\n#'\n#' A dummy LongTableDataMapper object to be used in package examples.\n#'\n#' @docType data\n#' @name exampleDataMapper\n#' @usage data(exampleDataMapper)\n#' @keywords datasets\n#' @format LongTableDataMapper object\nNULL\n\n#' NCI-ALMANAC Drug Combination Data TreatmentResponseExperiment Subset\n#'\n#' This is a `TreatmentResponseExperiment` object containing a subset of\n#'   NCI-ALMANAC drug combination screening data,\n#'   with 2347 unique treatment combinations on 10 cancer cell lines selected.\n#'\n#' @references\n#' Susan L. Holbeck, Richard Camalier, James A. Crowell, Jeevan Prasaad Govindharajulu, Melinda Hollingshead, Lawrence W. Anderson, Eric Polley, Larry Rubinstein, Apurva Srivastava, Deborah Wilsker, Jerry M. Collins, James H. Doroshow; The National Cancer Institute ALMANAC: A Comprehensive Screening Resource for the Detection of Anticancer Drug Pairs with Enhanced Therapeutic Activity. Cancer Res 1 July 2017; 77 (13): 3564\u20133576. https://doi.org/10.1158/0008-5472.CAN-17-0489\n#'\n#' @docType data\n#' @name nci_TRE_small\n#' @usage data(nci_TRE_small)\n#' @keywords datasets\n#' @format TreatmentResponseExperiment object\nNULL",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'NULL' statement at the end of each data object declaration in this R code snippet?",
        "answer": "The 'NULL' statement at the end of each data object declaration is used to create a placeholder for the data object without actually defining its contents. This is a common practice in R package development when documenting datasets. It allows the package to include documentation for the data without including the actual data in the source code."
      },
      {
        "question": "How are the dataset objects in this code snippet documented, and what key information is provided for each?",
        "answer": "The dataset objects are documented using Roxygen2 syntax, which starts with #'. Each dataset documentation includes: 1) A brief description of the dataset, 2) @references for citing the data source (when applicable), 3) @docType data to specify it's a dataset, 4) @name to provide the object name, 5) @usage to show how to load the data, 6) @keywords datasets for categorization, and 7) @format to specify the object type."
      },
      {
        "question": "What is the significance of the @format tag in the documentation of these dataset objects?",
        "answer": "The @format tag in the documentation specifies the type of R object that each dataset represents. This is important for users to understand how to interact with the data. For example, 'clevelandSmall_cSet' is a CoreSet object, 'merckLongTable' is a LongTable object, 'exampleDataMapper' is a LongTableDataMapper object, and 'nci_TRE_small' is a TreatmentResponseExperiment object. Each of these object types likely has specific methods and properties associated with it."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/tests/testthat/test-LongTable-class.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(CoreGx)\nlibrary(data.table)\n\ndata(nci_TRE_small)\ndata(merckLongTable)\n\nlt <- merckLongTable\ntre <- nci_TRE_small\n\n# == LongTable constructor\n\ntestthat::test_that(\"`LongTable` is coercible to TRE\", {\n    tre <- as(lt, \"TreatmentResponseExperiment\")\n    testthat::expect_s4_class(tre, \"TreatmentResponseExperiment\")\n})\n\ntestthat::test_that(\"`LongTable` constructor method works with valid inputs\", {\n    ## Extract required parameters to create an TRE object\n    parameters <- formalArgs(LongTable)\n    parameters <- parameters[\n        !(parameters %in% c(\"metadata\", \"keep.rownames\"))\n    ]\n    row_data <- rowData(tre)\n    row_ids <- rowIDs(tre)\n    col_data <- colData(tre)\n    col_ids <- colIDs(tre)\n    assays_ <- assays(tre)\n    assay_ids <- replicate(3, idCols(tre), simplify = FALSE)\n    names(assay_ids) <- assayNames(tre)\n    ## regex lookaheads to check for ALL missing parameters\n    regex <- paste0(sprintf(\"(?=.*%s)\", parameters), collapse = \"\")\n    regex <- paste0(\"(?s)^\", regex) ## handle line breaks in error messages\n    ## Line 87: Report all missing parameters in error message\n    testthat::expect_error({ ntre <- LongTable() },\n        regexp = regex, perl = TRUE\n    )\n    ## Check for wrong input rowData class (those not coercible to data.frame)\n    ## FIX-ME:: We might need extra check for rowData, colData, assays: even NULL is coercible to data.table\n    #testthat::expect_error({\n    #    ntre <- LongTable(rowData  = NULL,\n    #                      rowIDs   = row_ids,\n    #                      colData  = col_data,\n    #                      colIDs   = col_ids,\n    #                      assays   = assays_,\n    #                      assayIDs = array_ids)\n    #},\n    #    regexp = \".*rowData must be coerceible to a data\\\\.frame\"\n    #)\n    ## Question: should we handle the case where assays' IDs are mislabeled?\n    ## Question: should we check for unequal length of names(assays) and names(assayIDs)? (refer to line 171)\n    ## Line 172\n    testthat::expect_error({\n        ntre <- LongTable(rowData = row_data[, -row_ids[1:2], with = FALSE],\n                          rowIDs = row_ids,\n                          colData = col_data,\n                          colIDs = col_ids,\n                          assays = assays_,\n                          assayIDs = array_ids)\n    },\n        regexp = paste0(\".*Row IDs not in rowData: \",\n                        row_ids[1:2],\n                        collapse = \",\")\n    )\n    ## Question: should we handle the case where assays' IDs are mislabeled?\n    ## Question: should we check for unequal length of names(assays) and names(assayIDs)? (refer to line 171)\n    ## Line 172\n    testthat::expect_error({\n        names(assay_ids)[1] <- \"not sensitivity\"\n        ntre <- LongTable(rowData = row_data,\n                          rowIDs = row_ids,\n                          colData = col_data,\n                          colIDs = col_ids,\n                          assays = assays_,\n                          assayIDs = assay_ids)\n    },\n        regexp = paste0(\".*Mismatched names between \",\n                        \"assays and assayIDs for\\\\:\\n\\t\",\n                        paste0(names(assays_)[\n                                    names(assays_) != names(assay_ids)\n                               ],\n                               collapse = \", \"),\n                        \".*\", collapse = \"\")\n    )\n})\n\n# == assayCols\n\ntestthat::test_that(\"`assayCols,LongTable-method` retrieves specified assay's column names\",{\n    testthat::expect_error({ assayCols(tre, i = 1:2) })\n    testthat::expect_error({ assayCols(tre, i = (length(assayNames(tre))) + 1) })\n    testthat::expect_error({ assayCols(tre, i = paste0(assayNames(tre), collapse = \"\")) })\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `LongTable` constructor in this code, and what error checking does it perform?",
        "answer": "The `LongTable` constructor is used to create a TreatmentResponseExperiment (TRE) object from a long-format data table. It performs several error checks, including:\n1. Ensuring all required parameters are provided (using regex lookaheads).\n2. Verifying that the specified row IDs exist in the rowData.\n3. Checking for mismatched names between assays and assayIDs.\nThese checks help ensure the integrity and consistency of the data being used to create the TRE object."
      },
      {
        "question": "How does the code test the coercion of a `LongTable` object to a `TreatmentResponseExperiment` object?",
        "answer": "The code tests the coercion of a `LongTable` object to a `TreatmentResponseExperiment` object using the `testthat` framework. Specifically, it uses the `test_that` function with the description '`LongTable` is coercible to TRE'. Inside this test, it performs the following steps:\n1. Coerces the `lt` object (a LongTable) to a TreatmentResponseExperiment using `as(lt, \"TreatmentResponseExperiment\")`.\n2. Uses `expect_s4_class` to verify that the resulting object (`tre`) is indeed of class \"TreatmentResponseExperiment\".\nThis test ensures that the coercion method is working correctly and produces the expected output."
      },
      {
        "question": "What is the purpose of the `assayCols` method test in this code, and what specific cases does it check?",
        "answer": "The `assayCols` method test checks the behavior of the `assayCols` function when applied to a LongTable object. The test verifies that the function correctly handles various input scenarios:\n1. It checks that an error is raised when trying to retrieve columns for multiple assays simultaneously (using `i = 1:2`).\n2. It ensures an error occurs when requesting an assay index that is out of bounds (using `i = (length(assayNames(tre))) + 1`).\n3. It verifies that an error is thrown when providing an invalid assay name (by concatenating all assay names into a single string).\nThese tests help ensure that the `assayCols` method behaves correctly and provides appropriate error messages for invalid inputs."
      }
    ],
    "completion_tasks": [
      {
        "partial": "testthat::test_that(\"`LongTable` constructor method works with valid inputs\", {\n    parameters <- formalArgs(LongTable)\n    parameters <- parameters[!(parameters %in% c(\"metadata\", \"keep.rownames\"))]\n    row_data <- rowData(tre)\n    row_ids <- rowIDs(tre)\n    col_data <- colData(tre)\n    col_ids <- colIDs(tre)\n    assays_ <- assays(tre)\n    assay_ids <- replicate(3, idCols(tre), simplify = FALSE)\n    names(assay_ids) <- assayNames(tre)\n    regex <- paste0(sprintf(\"(?=.*%s)\", parameters), collapse = \"\")\n    regex <- paste0(\"(?s)^\", regex)\n    \n    testthat::expect_error({ ntre <- LongTable() },\n        regexp = regex, perl = TRUE\n    )\n    \n    testthat::expect_error({\n        ntre <- LongTable(rowData = row_data[, -row_ids[1:2], with = FALSE],\n                          rowIDs = row_ids,\n                          colData = col_data,\n                          colIDs = col_ids,\n                          assays = assays_,\n                          assayIDs = array_ids)\n    },\n        regexp = paste0(\".*Row IDs not in rowData: \",\n                        row_ids[1:2],\n                        collapse = \",\")\n    )\n    \n    testthat::expect_error({\n        names(assay_ids)[1] <- \"not sensitivity\"\n        ntre <- LongTable(rowData = row_data,\n                          rowIDs = row_ids,\n                          colData = col_data,\n                          colIDs = col_ids,\n                          assays = assays_,\n                          assayIDs = assay_ids)\n    },\n        regexp = paste0(\".*Mismatched names between \",\n                        \"assays and assayIDs for\\:\\n\\t\",\n                        paste0(names(assays_)[names(assays_) != names(assay_ids)],\n                               collapse = \", \"),\n                        \".*\", collapse = \"\")\n    )\n})",
        "complete": "testthat::test_that(\"`LongTable` constructor method works with valid inputs\", {\n    parameters <- formalArgs(LongTable)\n    parameters <- parameters[!(parameters %in% c(\"metadata\", \"keep.rownames\"))]\n    row_data <- rowData(tre)\n    row_ids <- rowIDs(tre)\n    col_data <- colData(tre)\n    col_ids <- colIDs(tre)\n    assays_ <- assays(tre)\n    assay_ids <- replicate(3, idCols(tre), simplify = FALSE)\n    names(assay_ids) <- assayNames(tre)\n    regex <- paste0(sprintf(\"(?=.*%s)\", parameters), collapse = \"\")\n    regex <- paste0(\"(?s)^\", regex)\n    \n    testthat::expect_error({ ntre <- LongTable() },\n        regexp = regex, perl = TRUE\n    )\n    \n    testthat::expect_error({\n        ntre <- LongTable(rowData = row_data[, -row_ids[1:2], with = FALSE],\n                          rowIDs = row_ids,\n                          colData = col_data,\n                          colIDs = col_ids,\n                          assays = assays_,\n                          assayIDs = assay_ids)\n    },\n        regexp = paste0(\".*Row IDs not in rowData: \",\n                        row_ids[1:2],\n                        collapse = \",\")\n    )\n    \n    testthat::expect_error({\n        names(assay_ids)[1] <- \"not sensitivity\"\n        ntre <- LongTable(rowData = row_data,\n                          rowIDs = row_ids,\n                          colData = col_data,\n                          colIDs = col_ids,\n                          assays = assays_,\n                          assayIDs = assay_ids)\n    },\n        regexp = paste0(\".*Mismatched names between \",\n                        \"assays and assayIDs for\\:\\n\\t\",\n                        paste0(names(assays_)[names(assays_) != names(assay_ids)],\n                               collapse = \", \"),\n                        \".*\", collapse = \"\")\n    )\n})"
      },
      {
        "partial": "testthat::test_that(\"`assayCols,LongTable-method` retrieves specified assay's column names\",{\n    testthat::expect_error({ assayCols(tre, i = 1:2) })\n    testthat::expect_error({ assayCols(tre, i = (length(assayNames(tre))) + 1) })\n    testthat::expect_error({ assayCols(tre, i = paste0(assayNames(tre), collapse = \"\")) })\n    # Add expectations for successful retrieval of column names\n})",
        "complete": "testthat::test_that(\"`assayCols,LongTable-method` retrieves specified assay's column names\",{\n    testthat::expect_error({ assayCols(tre, i = 1:2) })\n    testthat::expect_error({ assayCols(tre, i = (length(assayNames(tre))) + 1) })\n    testthat::expect_error({ assayCols(tre, i = paste0(assayNames(tre), collapse = \"\")) })\n    testthat::expect_type(assayCols(tre, i = 1), \"character\")\n    testthat::expect_equal(assayCols(tre, i = assayNames(tre)[1]), colnames(assays(tre)[[1]]))\n    testthat::expect_true(all(assayCols(tre, i = assayNames(tre)[1]) %in% colnames(assays(tre)[[1]])))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/utils-updateS4.R",
    "language": "R",
    "content": "#' Convert the old sensitivity slot format into a `LongTable` and update the\n#' `CoreSet` object.\n#'\n#' @param object Inheriting from `CoreSet`.\n#' @param mapper Should the `LongTableDataMapper` object be early returned,\n#' instead of the `LongTable` object. This can be useful if the conversion\n#' fails or corrupts your data. You can then modify the `DataMapper` as\n#' necessary to fix the sensititivity data.\n#'\n#' @return A `LongTable` constructed from `object@treatmentResponse`, or a\n#' `LongTableDataMapper` if `mapper`=TRUE.\n#'\n#' @keywords internal\n#' @noRd\n#' @importFrom data.table data.table as.data.table merge.data.table\n#' melt.data.table\n.sensitivityToTRE <- function(object, mapper=FALSE) {\n\n    # -- validate input\n    funContext <- .funContext(':::.sensitivitySlotToLongTable')\n    if (!is(object, 'CoreSet')) .error(funContext, ' object must inherit from\n        the CoreSet class.')\n    oldSensitivity <- treatmentResponse(object)\n\n    if (!is(oldSensitivity, 'list')) .error(funContext, ' @sensitivty slot\n        is not a `list`?')\n\n    # -- extract the old data as data.tables\n\n    # sensitivityInfo\n    infoDT <- as.data.table(oldSensitivity$info, keep.rownames=TRUE)\n    rowCols <- c(treatment1id=\"treatmentid\", treatment1dose='dose')\n    colCols <- c(sampleid=\"sampleid\")\n\n    # sensitivityProfiles\n    profDT <- as.data.table(oldSensitivity$profiles, keep.rownames=TRUE)\n\n    # sensitivityRaw\n    doseDT <- as.data.table(oldSensitivity$raw[, , 1], keep.rownames=TRUE)\n    meltedDoseDT <- na.omit(melt.data.table(doseDT, id.vars='rn',\n        variable.name='old_column', value.name='dose'))\n    meltedDoseDT[, dose := as.numeric(dose)]\n    viabDT <- as.data.table(oldSensitivity$raw[, , 2], keep.rownames=TRUE)\n    meltedViabDT <- na.omit(melt.data.table(viabDT, id.vars='rn',\n        variable.name='old_column', value.name='viability'))\n    meltedViabDT[, viability := as.numeric(viability)]\n\n    # -- merge into a single long format data.table\n    assayDT <- merge.data.table(meltedDoseDT, meltedViabDT,\n        by=c('rn', 'old_column'))\n    assayMap <- list(sensitivity=c('viability'),\n        profiles=setdiff(colnames(profDT), 'rn'))\n\n    rawdataDT <- merge.data.table(assayDT, profDT, by='rn')\n    rawdataDT <- merge.data.table(rawdataDT, infoDT, by='rn')\n    rawdataDT[, replicate_id := seq_len(.N), by=c(rowCols, colCols)]\n\n    if (max(rawdataDT$replicate_id) > 1) {\n        # Handle case where there is only 1 drug (i.e., radiation in RadioGx)\n        if (length(unique(rawdataDT[[rowCols[1]]])) == 1) {\n            rowCols <- c(rowCols, 'replicate_id')\n        } else {\n            colCols <- c(colCols, 'replicate_id')\n        }\n    } else {\n        rawdataDT[, replicate_id := NULL]\n    }\n\n    groups <- list(\n        rowDataMap=rowCols,\n        colDataMap=colCols,\n        assayMap=c(rowCols, colCols)\n    )\n\n    # -- capute the na rownames to make recreation easier in .rebuildInfo\n    missing_rows <- setdiff(infoDT$rn, rawdataDT$rn)\n    na_index <- infoDT[rn %in% missing_rows, .(rn, treatmentid, sampleid)]\n\n    # -- build a LongTableDataMapper object\n    TREdataMapper <- TREDataMapper(rawdata=rawdataDT)\n    guess <- guessMapping(TREdataMapper, groups, subset=TRUE)\n\n    assayCols <- unlist(assayMap)\n\n    # do not steal any assay columns for the row or column data\n    guess$rowDataMap[[2]] <- setdiff(guess$rowDataMap[[2]], assayCols)\n    guess$colDataMap[[2]] <- setdiff(guess$colDataMap[[2]], assayCols)\n    guess$metadata[[2]] <- setdiff(guess$metadata[[2]],\n        c(assayCols, guess$rowDataMap[[2]], guess$colDataMap[[2]]))\n    assayMap$assay_metadata <- setdiff(guess$assayMap$mapped_columns, assayCols)\n    assayMap <- lapply(assayMap, FUN=function(x, y) list(y, x),  # add id columns\n        y=guess$assayMap[[1]])\n\n    # update the data mapper\n    rowDataMap(TREdataMapper) <- guess$rowDataMap\n    colDataMap(TREdataMapper) <- guess$colDataMap\n    assayMap(TREdataMapper) <- assayMap\n    metadataMap(TREdataMapper) <-\n        list(experiment_metadata=guess$metadata$mapped_columns)\n    metadata(TREdataMapper) <- list(sensitivityInfo_NA=na_index)\n\n    # build the object\n    return(if (!mapper) metaConstruct(TREdataMapper) else TREdataMapper)\n}\n\n\n#' Compare the values of sensitivityInfo before and after use of\n#' .sensitivityToTRE\n#'\n#' @param object `CoreSet` to be updated to the new\n#' `TreatmentResponseExperiment` sensitivity format.\n#' @param FUN `function` The function to compare results from.\n#'\n#' @return None, displays results of `all.equal` on the sensitivityInfo for\n#'   the columns which should be conserved.\n#'\n#' @keywords internal\n#' @noRd\n#' @importFrom data.table data.table as.data.table merge.data.table\n#' melt.data.table\n.compareTreatmentResponse <- function(object, FUN) {\n\n    new_object <- copy(object)\n    tre <- CoreGx:::.sensitivityToTRE(object)\n    new_object@treatmentResponse <- tre\n\n    old_res <- copy(FUN(object))\n    new_res <- copy(FUN(new_object))\n\n    setDT(old_res, keep.rownames=\"rownames\")\n    setDT(new_res, keep.rownames=\"rownames\")\n\n    equal_columns <- colnames(old_res)\n    all.equal(\n        old_res[order(rownames), .SD, .SDcols=equal_columns],\n        new_res[order(rownames), .SD, .SDcols=equal_columns]\n    )\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.sensitivityToTRE` function and what are its main steps?",
        "answer": "The `.sensitivityToTRE` function converts an old sensitivity slot format into a `LongTable` format and updates a `CoreSet` object. Its main steps are:\n1. Validate input\n2. Extract old data as data.tables (sensitivityInfo, sensitivityProfiles, sensitivityRaw)\n3. Merge data into a single long format data.table\n4. Handle replicates\n5. Build a LongTableDataMapper object\n6. Update the data mapper with row, column, assay, and metadata mappings\n7. Return either the constructed LongTable or the DataMapper object"
      },
      {
        "question": "How does the function handle replicates in the data, and what conditions determine how replicates are treated?",
        "answer": "The function handles replicates by:\n1. Creating a `replicate_id` column\n2. If max(replicate_id) > 1:\n   a. If there's only one unique drug (e.g., radiation in RadioGx), add `replicate_id` to `rowCols`\n   b. Otherwise, add `replicate_id` to `colCols`\n3. If max(replicate_id) == 1, remove the `replicate_id` column\n\nThis approach ensures proper grouping of replicates based on the data structure, accommodating different experimental designs."
      },
      {
        "question": "What is the purpose of the `.compareTreatmentResponse` function and how does it work?",
        "answer": "The `.compareTreatmentResponse` function compares the values of sensitivityInfo before and after using `.sensitivityToTRE`. It works by:\n1. Creating a copy of the input object\n2. Applying `.sensitivityToTRE` to create a new TreatmentResponseExperiment\n3. Updating the copy with the new TreatmentResponseExperiment\n4. Applying a user-provided function (FUN) to both the original and updated objects\n5. Converting results to data.tables and ordering them\n6. Using `all.equal` to compare the results for columns that should be conserved\n\nThis function helps validate that the conversion process preserves important information."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Partial code for .sensitivityToTRE function\n.sensitivityToTRE <- function(object, mapper=FALSE) {\n    funContext <- .funContext(':::.sensitivitySlotToLongTable')\n    if (!is(object, 'CoreSet')) .error(funContext, ' object must inherit from the CoreSet class.')\n    oldSensitivity <- treatmentResponse(object)\n\n    if (!is(oldSensitivity, 'list')) .error(funContext, ' @sensitivty slot is not a `list`?')\n\n    infoDT <- as.data.table(oldSensitivity$info, keep.rownames=TRUE)\n    profDT <- as.data.table(oldSensitivity$profiles, keep.rownames=TRUE)\n\n    # TODO: Complete the function by extracting dose and viability data,\n    # merging into a single long format data.table, and creating a LongTableDataMapper object\n\n    # Return the result based on the mapper parameter\n    return(if (!mapper) metaConstruct(TREdataMapper) else TREdataMapper)\n}",
        "complete": "#' Convert the old sensitivity slot format into a `LongTable` and update the `CoreSet` object.\n#'\n#' @param object Inheriting from `CoreSet`.\n#' @param mapper Should the `LongTableDataMapper` object be early returned,\n#' instead of the `LongTable` object.\n#'\n#' @return A `LongTable` constructed from `object@treatmentResponse`, or a\n#' `LongTableDataMapper` if `mapper`=TRUE.\n#'\n#' @keywords internal\n#' @noRd\n#' @importFrom data.table data.table as.data.table merge.data.table melt.data.table\n.sensitivityToTRE <- function(object, mapper=FALSE) {\n    funContext <- .funContext(':::.sensitivitySlotToLongTable')\n    if (!is(object, 'CoreSet')) .error(funContext, ' object must inherit from the CoreSet class.')\n    oldSensitivity <- treatmentResponse(object)\n\n    if (!is(oldSensitivity, 'list')) .error(funContext, ' @sensitivty slot is not a `list`?')\n\n    infoDT <- as.data.table(oldSensitivity$info, keep.rownames=TRUE)\n    rowCols <- c(treatment1id=\"treatmentid\", treatment1dose='dose')\n    colCols <- c(sampleid=\"sampleid\")\n\n    profDT <- as.data.table(oldSensitivity$profiles, keep.rownames=TRUE)\n\n    doseDT <- as.data.table(oldSensitivity$raw[, , 1], keep.rownames=TRUE)\n    meltedDoseDT <- na.omit(melt.data.table(doseDT, id.vars='rn', variable.name='old_column', value.name='dose'))\n    meltedDoseDT[, dose := as.numeric(dose)]\n    viabDT <- as.data.table(oldSensitivity$raw[, , 2], keep.rownames=TRUE)\n    meltedViabDT <- na.omit(melt.data.table(viabDT, id.vars='rn', variable.name='old_column', value.name='viability'))\n    meltedViabDT[, viability := as.numeric(viability)]\n\n    assayDT <- merge.data.table(meltedDoseDT, meltedViabDT, by=c('rn', 'old_column'))\n    assayMap <- list(sensitivity=c('viability'), profiles=setdiff(colnames(profDT), 'rn'))\n\n    rawdataDT <- merge.data.table(assayDT, profDT, by='rn')\n    rawdataDT <- merge.data.table(rawdataDT, infoDT, by='rn')\n    rawdataDT[, replicate_id := seq_len(.N), by=c(rowCols, colCols)]\n\n    if (max(rawdataDT$replicate_id) > 1) {\n        if (length(unique(rawdataDT[[rowCols[1]]])) == 1) {\n            rowCols <- c(rowCols, 'replicate_id')\n        } else {\n            colCols <- c(colCols, 'replicate_id')\n        }\n    } else {\n        rawdataDT[, replicate_id := NULL]\n    }\n\n    groups <- list(rowDataMap=rowCols, colDataMap=colCols, assayMap=c(rowCols, colCols))\n\n    missing_rows <- setdiff(infoDT$rn, rawdataDT$rn)\n    na_index <- infoDT[rn %in% missing_rows, .(rn, treatmentid, sampleid)]\n\n    TREdataMapper <- TREDataMapper(rawdata=rawdataDT)\n    guess <- guessMapping(TREdataMapper, groups, subset=TRUE)\n\n    assayCols <- unlist(assayMap)\n    guess$rowDataMap[[2]] <- setdiff(guess$rowDataMap[[2]], assayCols)\n    guess$colDataMap[[2]] <- setdiff(guess$colDataMap[[2]], assayCols)\n    guess$metadata[[2]] <- setdiff(guess$metadata[[2]], c(assayCols, guess$rowDataMap[[2]], guess$colDataMap[[2]]))\n    assayMap$assay_metadata <- setdiff(guess$assayMap$mapped_columns, assayCols)\n    assayMap <- lapply(assayMap, FUN=function(x, y) list(y, x), y=guess$assayMap[[1]])\n\n    rowDataMap(TREdataMapper) <- guess$rowDataMap\n    colDataMap(TREdataMapper) <- guess$colDataMap\n    assayMap(TREdataMapper) <- assayMap\n    metadataMap(TREdataMapper) <- list(experiment_metadata=guess$metadata$mapped_columns)\n    metadata(TREdataMapper) <- list(sensitivityInfo_NA=na_index)\n\n    return(if (!mapper) metaConstruct(TREdataMapper) else TREdataMapper)\n}"
      },
      {
        "partial": "# Partial code for .compareTreatmentResponse function\n.compareTreatmentResponse <- function(object, FUN) {\n    new_object <- copy(object)\n    tre <- CoreGx:::.sensitivityToTRE(object)\n    new_object@treatmentResponse <- tre\n\n    old_res <- copy(FUN(object))\n    new_res <- copy(FUN(new_object))\n\n    setDT(old_res, keep.rownames=\"rownames\")\n    setDT(new_res, keep.rownames=\"rownames\")\n\n    # TODO: Complete the function by comparing the old and new results\n}",
        "complete": "#' Compare the values of sensitivityInfo before and after use of .sensitivityToTRE\n#'\n#' @param object `CoreSet` to be updated to the new `TreatmentResponseExperiment` sensitivity format.\n#' @param FUN `function` The function to compare results from.\n#'\n#' @return None, displays results of `all.equal` on the sensitivityInfo for the columns which should be conserved.\n#'\n#' @keywords internal\n#' @noRd\n#' @importFrom data.table data.table as.data.table merge.data.table melt.data.table\n.compareTreatmentResponse <- function(object, FUN) {\n    new_object <- copy(object)\n    tre <- CoreGx:::.sensitivityToTRE(object)\n    new_object@treatmentResponse <- tre\n\n    old_res <- copy(FUN(object))\n    new_res <- copy(FUN(new_object))\n\n    setDT(old_res, keep.rownames=\"rownames\")\n    setDT(new_res, keep.rownames=\"rownames\")\n\n    equal_columns <- colnames(old_res)\n    all.equal(\n        old_res[order(rownames), .SD, .SDcols=equal_columns],\n        new_res[order(rownames), .SD, .SDcols=equal_columns]\n    )\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/RadioGx.git",
    "file": "../../../../repos/RadioGx/R/subsetTo-methods.R",
    "language": "R",
    "content": "#' @include RadioSet-accessors.R\nNULL\n\n#'`[`\n#'\n#' @examples\n#' data(clevelandSmall)\n#' clevelandSmall[sampleNames(clevelandSmall)[1], treatmentNames(clevelandSmall)[1]]\n#'\n#' @param x object\n#' @param i Cell lines to keep in object\n#' @param j Drugs to keep in object\n#' @param ... further arguments\n#' @param drop A boolean flag of whether to drop single dimensions or not\n#'\n#' @return Returns the subsetted object\n#'\n#' @export\nsetMethod(`[`, \"RadioSet\", function(x, i, j, ..., drop = FALSE) {\n    if (is.character(i)&&is.character(j)) {\n        return(subsetTo(x, cells=i, radiations=j,  molecular.data.cells=i))\n    }\n    else if (is.numeric(i) && is.numeric(j) && (as.integer(i)==i) &&\n            (as.integer(j)==j)){\n        return(subsetTo(x, cells=sampleNames(x)[i], radiations=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n})\n\n## FIXED? TODO:: Subset function breaks if it doesnt find cell line in sensitivity info\n#' A function to subset a RadioSet to data containing only specified radiations,\n#'   cells and genes\n#'\n#' This is the prefered method of subsetting a RadioSet. This function allows\n#' abstraction of the data to the level of biologically relevant objects:\n#'   radiations and cells. The function will automatically go through all of the\n#' combined data in the RadioSet and ensure only the requested radiations\n#' and cell lines are found in any of the slots. This allows quickly picking out\n#' all the experiments for a radiation or cell of interest, as well removes the\n#' need to keep track of all the metadata conventions between different\n#' datasets.\n#'\n#' @examples\n#' clevelandRadiationTypes  <- treatmentNames(clevelandSmall)\n#' clevelandCells <- sampleNames(clevelandSmall)\n#' RSet <- subsetTo(clevelandSmall, radiationTypes = clevelandRadiationTypes[1],\n#'   cells = clevelandCells[1])\n#' RSet\n#'\n#' @param object A \\code{RadioSet} to be subsetted\n#' @param cells A list or vector of cell names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all cells will be left in\n#'   the dataset.\n#' @param radiationTypes A list or vector of radiation names as used in the\n#'   dataset to which the object will be subsetted. If left blank, then all\n#'   radiationTypes will be left in the dataset.\n#' @param molecular.data.cells A list or vector of cell names to keep in the\n#'   molecular data\n#' @param keep.controls If the dataset has perturbation type experiments, should\n#'   the controls be kept in the dataset? Defaults to true.\n#' @param ... Other arguments passed by other function within the package\n#'\n#' @return A RadioSet with only the selected radiation types and cells\n#'\n#' @importMethodsFrom CoreGx subsetTo\n#' @export\nsetMethod(\"subsetTo\",\n          signature(object=\"RadioSet\"),\n          function(object , cells=NULL, radiationTypes=NULL, molecular.data.cells=NULL, keep.controls=TRUE, ...){\n              .subsetToRadioSet(object, cells, radiationTypes,\n              molecular.data.cells, keep.controls, ...)\n          })\n\n# @param object A `RadioSet` to be subsetted\n# @param cells A list or vector of cell names as used in the dataset to which\n#   the object will be subsetted. If left blank, then all cells will be left in\n#   the dataset.\n# @param radiationTypes A list or vector of radiation names as used in the\n#   dataset to which the object will be subsetted. If left blank, then all\n#   radiationTypes will be left in the dataset.\n# @param molecular.data.cells A list or vector of cell names to keep in the\n#   molecular data\n# @param keep.controls If the dataset has perturbation type experiments, should\n#   the controls be kept in the dataset? Defaults to true.\n# @param ... Other arguments passed by other function within the package\n# @return A RadioSet with only the selected radiation types and cells\n#' @importFrom CoreGx .unionList .message .warning .error\n#' @keywords internals\n.subsetToRadioSet <- function(object,\n                     cells=NULL,\n                     radiationTypes=NULL,\n                     molecular.data.cells=NULL,\n                     keep.controls=TRUE,\n                     ...)\n{\n    drop=FALSE\n\n    adArgs = list(...)\n    if (\"exps\" %in% names(adArgs)) {\n        exps <- adArgs[[\"exps\"]]\n        if(is(exps,\"data.frame\")){\n            exps2 <- exps[[name(object)]]\n            names(exps2) <- rownames(exps)\n            exps <- exps2\n        } else{\n            exps <- exps[[name(object)]]\n        }\n    }else {\n        exps <- NULL\n    }\n    if(!missing(cells)){\n        cells <- unique(cells)\n    }\n\n    if(!missing(radiationTypes)){\n        radiationTypes <- unique(radiationTypes)\n    }\n\n    if(!missing(molecular.data.cells)){\n        molecular.data.cells <- unique(molecular.data.cells)\n    }\n\n    ### TODO:: implement strict subsetting at this level!!!!\n\n    ### the function missing does not work as expected in the context below,\n    ### because the arguments are passed to the anonymous\n    ### function in lapply, so it does not recognize them as missing\n\n    molecularProfilesSlot(object) <-\n        lapply(molecularProfilesSlot(object),\n            function(SE, cells, radiationTypes, molecular.data.cells) {\n                molecular.data.type <-\n                    if (length(grep(\"rna\", S4Vectors::metadata(SE)$annotation) > 0))\n                        \"rna\"\n                    else\n                        S4Vectors::metadata(SE)$annotation\n                if (length(grep(molecular.data.type, names(molecular.data.cells))) > 0)\n                    cells <- molecular.data.cells[[molecular.data.type]]\n\n                column_indices <- NULL\n\n                if (length(cells)==0 && length(radiationTypes) == 0) {\n                    column_indices <- seq_len(ncol(SE)) # This still returns the number of samples in an SE, but without a label\n                }\n                if (length(cells) == 0 && datasetType(object) == \"sensitivity\") {\n                    column_indices <- seq_len(ncol(SE))\n                }\n\n                cell_line_index <- NULL\n                if(length(cells)!=0) {\n                    if (!all(cells %in% sampleNames(object))) {\n                        stop(\"Some of the cell names passed to function did not match to names in the RadoSet. Please ensure you are using cell names as returned by the cellNames function\")\n                    }\n                    cell_line_index <- which(SummarizedExperiment::colData(SE)[[\"sampleid\"]] %in% cells)\n                }\n                radiationTypes_index <- NULL\n                if(datasetType(object)==\"perturbation\" || datasetType(object)==\"both\"){\n                    if(length(radiationTypes) != 0) {\n                        if (!all(radiationTypes %in% treatmentNames(object))) {\n                            stop(\"Some of the radiation types passed to function did not match\n               to names in the RadioSet. Please ensure you are using radiation\n               names as returned by the radiations function\")\n                                                        }\n                                                        radiationTypes_index <- which(SummarizedExperiment::colData(SE)[[\"treatmentid\"]] %in% radiationTypes)\n                                                        if(keep.controls) {\n                                                            control_indices <- which(SummarizedExperiment::colData(SE)[[\"xptype\"]]==\"control\")\n                                                            radiationTypes_index <- c(radiationTypes_index, control_indices)\n                                                        }\n                                                    }\n                                                }\n\n                                                if(length(radiationTypes_index) != 0 && length(cell_line_index) != 0) {\n                                                    if(length(intersect(radiationTypes_index, cell_line_index)) == 0) {\n                                                        stop(\"This Drug - Cell Line combination was not tested together.\")\n                                                    }\n                                                    column_indices <- intersect(radiationTypes_index, cell_line_index)\n                                                } else {\n                                                    if(length(radiationTypes_index) !=0) {\n                                                        column_indices <- radiationTypes_index\n                                                    }\n                                                    if(length(cell_line_index) !=0) {\n                                                        column_indices <- cell_line_index\n                                                    }\n                                                }\n\n                                                row_indices <- seq_len(nrow(SummarizedExperiment::assay(SE, 1)))\n\n                                                SE <- SE[row_indices, column_indices]\n                                                return(SE)\n\n                                            }, cells=cells, radiationTypes=radiationTypes, molecular.data.cells=molecular.data.cells)\n\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") & length(exps) != 0) {\n        sensitivityInfo(object) <- sensitivityInfo(object)[exps, , drop=drop]\n        rownames(sensitivityInfo(object)) <- names(exps)\n        if(length(sensitivityRaw(object)) > 0) {\n            sensitivityRaw(object) <- sensitivityRaw(object)[exps, , , drop=drop]\n            dimnames(sensitivityRaw(object))[[1]] <- names(exps)\n        }\n        sensitivityProfiles(object) <- sensitivityProfiles(object)[exps, , drop=drop]\n        rownames(sensitivityProfiles(object)) <- names(exps)\n\n        sensNumber(object) <- .summarizeSensitivityNumbers(object)\n    }\n    else if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") & (length(radiationTypes) != 0 | length(cells) != 0)) {\n\n        radiationTypes_index <- which (sensitivityInfo(object)[, \"treatmentid\"] %in% radiationTypes)\n        cell_line_index <- which (sensitivityInfo(object)[,\"sampleid\"] %in% cells)\n        if (length(radiationTypes_index) !=0 & length(cell_line_index) !=0 ) {\n            if (length(intersect(radiationTypes_index, cell_line_index)) == 0) {\n                stop(\"This Drug - Cell Line combination was not tested together.\")\n            }\n            row_indices <- intersect(radiationTypes_index, cell_line_index)\n        } else {\n            if(length(radiationTypes_index)!=0 & length(cells)==0) {\n                row_indices <- radiationTypes_index\n            } else {\n                if(length(cell_line_index)!=0 & length(radiationTypes)==0){\n                    row_indices <- cell_line_index\n                } else {\n                    row_indices <- vector()\n                }\n            }\n        }\n        treatmentResponse(object)[names(treatmentResponse(object))[names(treatmentResponse(object))!=\"n\"]] <-\n            lapply(treatmentResponse(object)[names(treatmentResponse(object))[names(treatmentResponse(object))!=\"n\"]],\n                   function(x,i, drop){\n\n                       if (length(dim(x))==2){\n                           return(x[i,,drop=drop])\n                       }\n                       if (length(dim(x))==3){\n                           return(x[i,,,drop=drop])\n                       }\n                   }, i=row_indices, drop=drop)\n    }\n\n    if (length(radiationTypes)==0) {\n        if(datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\"){\n            radiationTypes <- unique(sensitivityInfo(object)[[\"treatmentid\"]])\n        }\n        if(datasetType(object) == \"perturbation\" | datasetType(object) == \"both\"){\n            radiationTypes <- union(radiationTypes, na.omit(.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(colData(SE)[[\"treatmentid\"]])}))))\n        }\n    }\n    if (length(cells)==0) {\n        cells <- union(cells, na.omit(.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(colData(SE)[[\"sampleid\"]])}))))\n        if (datasetType(object) ==\"sensitivity\" | datasetType(object) == \"both\"){\n            cells <- union(cells, sensitivityInfo(object)[[\"sampleid\"]])\n        }\n    }\n    treatmentInfo(object) <- treatmentInfo(object)[radiationTypes , , drop=drop]\n    sampleInfo(object) <- sampleInfo(object)[cells , , drop=drop]\n    curation(object)$radiation <- curation(object)$radiation[radiationTypes , , drop=drop]\n    curation(object)$sample <- curation(object)$sample[cells , , drop=drop]\n    curation(object)$tissue <- curation(object)$tissue[cells , , drop=drop]\n    if (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\"  & length(exps) == 0) {\n        sensNumber(object) <- sensNumber(object)[cells, radiationTypes , drop=drop]\n    }\n    if (datasetType(object) == \"perturbation\" | datasetType(object) == \"both\") {\n        object@perturbation$n <- object@perturbation$n[cells,radiationTypes, , drop=drop]\n    }\n    return(object)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `subsetTo` method for the `RadioSet` class?",
        "answer": "The `subsetTo` method for the `RadioSet` class is used to subset a RadioSet object to contain only specified radiations, cells, and genes. It allows for abstraction of data to biologically relevant objects (radiations and cells) and automatically ensures that only the requested radiations and cell lines are present in all slots of the RadioSet. This method is the preferred way to subset a RadioSet as it simplifies the process of selecting experiments for specific radiations or cells of interest."
      },
      {
        "question": "How does the `.subsetToRadioSet` function handle subsetting of molecular profiles?",
        "answer": "The `.subsetToRadioSet` function handles subsetting of molecular profiles by applying a lambda function to each element in the `molecularProfilesSlot`. This function determines the molecular data type (RNA or other), selects the appropriate cells based on the input parameters, and then subsets the SummarizedExperiment object using column indices. It considers both cell lines and radiation types when subsetting, ensuring that only the requested data is retained in the molecular profiles."
      },
      {
        "question": "What happens if the `cells` or `radiationTypes` parameters are not provided when calling the `subsetTo` method?",
        "answer": "If the `cells` or `radiationTypes` parameters are not provided (i.e., they are NULL or empty), the function behaves as follows: 1) For `cells`, it will use all unique sample IDs from the molecular profiles and sensitivity information (if available). 2) For `radiationTypes`, it will use all unique treatment IDs from the sensitivity information (if available) and molecular profiles. This ensures that the RadioSet object retains all relevant data when no specific subset is requested."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(`[`, \"RadioSet\", function(x, i, j, ..., drop = FALSE) {\n    if (is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, radiations=j, molecular.data.cells=i))\n    }\n    else if (is.numeric(i) && is.numeric(j) && (as.integer(i)==i) &&\n            (as.integer(j)==j)){\n        # Complete the code here\n    }\n})",
        "complete": "setMethod(`[`, \"RadioSet\", function(x, i, j, ..., drop = FALSE) {\n    if (is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, radiations=j, molecular.data.cells=i))\n    }\n    else if (is.numeric(i) && is.numeric(j) && (as.integer(i)==i) &&\n            (as.integer(j)==j)){\n        return(subsetTo(x, cells=sampleNames(x)[i], radiations=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n})"
      },
      {
        "partial": "setMethod(\"subsetTo\",\n          signature(object=\"RadioSet\"),\n          function(object, cells=NULL, radiationTypes=NULL, molecular.data.cells=NULL, keep.controls=TRUE, ...) {\n              # Complete the function body here\n          })",
        "complete": "setMethod(\"subsetTo\",\n          signature(object=\"RadioSet\"),\n          function(object, cells=NULL, radiationTypes=NULL, molecular.data.cells=NULL, keep.controls=TRUE, ...) {\n              .subsetToRadioSet(object, cells, radiationTypes,\n              molecular.data.cells, keep.controls, ...)\n          })"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/src/exports.cpp",
    "language": "cpp",
    "content": "#include \"exports.h\"\n#include <R.h>\n// borrowed from Matrix/rcpp\n#define CALLDEF(name, n)  {#name, (DL_FUNC) &name, n}\n\nstatic const R_CallMethodDef callEntries[] = {\n    CALLDEF(export_concordance_index, 13),\n    CALLDEF(export_filters, 16),\n    CALLDEF(export_filters_bootstrap, 17),\n    CALLDEF(export_mim, 13),\n    CALLDEF(set_thread_count, 1),\n    CALLDEF(get_thread_count, 1),\n    {NULL, NULL, 0}\n};\n\nextern \"C\" void\nR_init_mRMRe(DllInfo* info) {\n  R_registerRoutines(info, NULL, callEntries, NULL, NULL);\n  R_useDynamicSymbols(info, FALSE);\n}\n\nextern \"C\" SEXP\nexport_concordance_index(SEXP samplesA, SEXP samplesB, SEXP samplesC, SEXP samplesD,\n        SEXP sampleStrata, SEXP sampleWeights, SEXP sampleStratumCount, SEXP outX, SEXP ratio,\n        SEXP concordantWeights, SEXP discordantWeights, SEXP uninformativeWeights,\n        SEXP relevantWeights)\n{\n    unsigned int const sample_count = LENGTH(samplesA);\n    unsigned int** p_sample_indices_per_stratum = new unsigned int*[INTEGER(sampleStratumCount)[0]];\n    unsigned int* const p_sample_count_per_stratum =\n            new unsigned int[INTEGER(sampleStratumCount)[0]];\n    Math::placeStratificationData(INTEGER(sampleStrata), REAL(sampleWeights),\n            p_sample_indices_per_stratum, p_sample_count_per_stratum,\n            INTEGER(sampleStratumCount)[0], sample_count);\n\n    if (LENGTH(samplesD) != 0 && LENGTH(samplesC) != 0)\n        REAL(ratio)[0] = Math::computeConcordanceIndex(REAL(samplesA), REAL(samplesB),\n                REAL(samplesC), REAL(samplesD), REAL(sampleWeights), p_sample_indices_per_stratum,\n                p_sample_count_per_stratum, INTEGER(sampleStratumCount)[0], INTEGER(outX)[0] != 0,\n                REAL(concordantWeights), REAL(discordantWeights), REAL(uninformativeWeights),\n                REAL(relevantWeights));\n    else if (LENGTH(samplesC) != 0)\n        REAL(ratio)[0] = Math::computeConcordanceIndex(REAL(samplesA), REAL(samplesB),\n                REAL(samplesC), REAL(sampleWeights), p_sample_indices_per_stratum,\n                p_sample_count_per_stratum, INTEGER(sampleStratumCount)[0], INTEGER(outX)[0] != 0,\n                REAL(concordantWeights), REAL(discordantWeights), REAL(uninformativeWeights),\n                REAL(relevantWeights));\n    else\n        REAL(ratio)[0] = Math::computeConcordanceIndex(REAL(samplesA), REAL(samplesB),\n                REAL(sampleWeights), p_sample_indices_per_stratum, p_sample_count_per_stratum,\n                INTEGER(sampleStratumCount)[0], INTEGER(outX)[0] != 0, REAL(concordantWeights),\n                REAL(discordantWeights), REAL(uninformativeWeights), REAL(relevantWeights));\n\n    delete[] p_sample_count_per_stratum;\n    for (unsigned int i = 0; i < INTEGER(sampleStratumCount)[0]; ++i)\n        delete[] p_sample_indices_per_stratum[i];\n    delete[] p_sample_indices_per_stratum;\n\n    return R_NilValue;\n}\n\nextern \"C\" SEXP\nexport_filters(SEXP childrenCountPerLevel, SEXP dataMatrix, SEXP priorsMatrix, SEXP priorsWeight,\n        SEXP sampleStrata, SEXP sampleWeights, SEXP featureTypes, SEXP sampleCount,\n        SEXP featureCount, SEXP sampleStratumCount, SEXP targetFeatureIndices, SEXP fixedFeatureCount,\n        SEXP continuousEstimator, SEXP outX, SEXP bootstrapCount, SEXP miMatrix)\n{\n    Matrix const priors_matrix(REAL(priorsMatrix), INTEGER(featureCount)[0],\n            INTEGER(featureCount)[0]);\n    Matrix const* const p_priors_matrix =\n            LENGTH(priorsMatrix) == INTEGER(featureCount)[0] * INTEGER(featureCount)[0] ?\n                    &priors_matrix : 0;\n    Data data(REAL(dataMatrix), p_priors_matrix, REAL(priorsWeight)[0], INTEGER(sampleCount)[0],\n            INTEGER(featureCount)[0], INTEGER(sampleStrata), REAL(sampleWeights),\n            INTEGER(featureTypes), INTEGER(sampleStratumCount)[0], INTEGER(continuousEstimator)[0],\n            INTEGER(outX)[0] != 0, INTEGER(bootstrapCount)[0]);\n    MutualInformationMatrix mi_matrix(&data, REAL(miMatrix));\n\n    unsigned int solution_count = 1;\n    for (unsigned int i = 0; i < LENGTH(childrenCountPerLevel); ++i)\n        solution_count *= INTEGER(childrenCountPerLevel)[i];\n    unsigned int const feature_count_per_solution = LENGTH(childrenCountPerLevel);\n    unsigned int const chunk_size = solution_count * feature_count_per_solution;\n\n    SEXP result;\n    PROTECT(result = allocVector(VECSXP, 3));\n\n    SET_VECTOR_ELT(result, 0, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n    SET_VECTOR_ELT(result, 1, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n    SET_VECTOR_ELT(result, 2, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n\n    for (unsigned int i = 0; i < LENGTH(targetFeatureIndices); ++i)\n    {\n        Filter filter(INTEGER(childrenCountPerLevel), LENGTH(childrenCountPerLevel), &mi_matrix,\n                INTEGER(targetFeatureIndices)[i], INTEGER(fixedFeatureCount)[0]);\n        filter.build();\n\n        SET_VECTOR_ELT(VECTOR_ELT(result, 0), i, allocVector(INTSXP, chunk_size));\n        SET_VECTOR_ELT(VECTOR_ELT(result, 1), i, allocVector(REALSXP, INTEGER(featureCount)[0]));\n        SET_VECTOR_ELT(VECTOR_ELT(result, 2), i, allocVector(REALSXP, chunk_size));\n\n        filter.getSolutions(INTEGER(VECTOR_ELT(VECTOR_ELT(result, 0), i)));\n        filter.getScores(REAL(VECTOR_ELT(VECTOR_ELT(result, 2), i)));\n\n        for (unsigned int k = 0; k < INTEGER(featureCount)[0]; ++k)\n            REAL(VECTOR_ELT(VECTOR_ELT(result, 1), i))[k] =\n                    std::numeric_limits<double>::quiet_NaN();\n\n        Math::computeCausality(REAL(VECTOR_ELT(VECTOR_ELT(result, 1), i)), &mi_matrix,\n                INTEGER(VECTOR_ELT(VECTOR_ELT(result, 0), i)), solution_count,\n                feature_count_per_solution, INTEGER(featureCount)[0],\n                INTEGER(targetFeatureIndices)[i]);\n    }\n    //PrintValue(result);\n    UNPROTECT(1);\n\n    return result;\n}\n\nextern \"C\" SEXP\nexport_filters_bootstrap(SEXP solutionCount, SEXP solutionLength, SEXP dataMatrix,\n        SEXP priorsMatrix, SEXP priorsWeight, SEXP sampleStrata, SEXP sampleWeights,\n        SEXP featureTypes, SEXP sampleCount, SEXP featureCount, SEXP sampleStratumCount,\n        SEXP targetFeatureIndices, SEXP fixedFeatureCount, SEXP continuousEstimator, SEXP outX, SEXP bootstrapCount,\n        SEXP miMatrix)\n{\n    Matrix const priors_matrix(REAL(priorsMatrix), INTEGER(featureCount)[0],\n            INTEGER(featureCount)[0]);\n    Matrix const* const p_priors_matrix =\n            LENGTH(priorsMatrix) == INTEGER(featureCount)[0] * INTEGER(featureCount)[0] ?\n                    &priors_matrix : 0;\n    Data data(REAL(dataMatrix), p_priors_matrix, REAL(priorsWeight)[0], INTEGER(sampleCount)[0],\n            INTEGER(featureCount)[0], INTEGER(sampleStrata), REAL(sampleWeights),\n            INTEGER(featureTypes), INTEGER(sampleStratumCount)[0], INTEGER(continuousEstimator)[0],\n            INTEGER(outX)[0] != 0, INTEGER(bootstrapCount)[0]);\n    //MutualInformationMatrix mi_matrix(&data, REAL(miMatrix));\n\n    unsigned int solution_count = INTEGER(solutionCount)[0];\n    unsigned int const feature_count_per_solution = INTEGER(solutionLength)[0];\n    unsigned int const chunk_size = solution_count * feature_count_per_solution;\n\n    int* const p_children_count_per_level = new int[feature_count_per_solution];\n    for (unsigned int i = 0; i < feature_count_per_solution; ++i)\n        p_children_count_per_level[i] = 1;\n\n    SEXP result;\n    PROTECT(result = allocVector(VECSXP, 3));\n\n    SET_VECTOR_ELT(result, 0, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n    SET_VECTOR_ELT(result, 1, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n    SET_VECTOR_ELT(result, 2, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n\n    for (unsigned int i = 0; i < LENGTH(targetFeatureIndices); ++i)\n    {\n        SET_VECTOR_ELT(VECTOR_ELT(result, 0), i, allocVector(INTSXP, chunk_size));\n        SET_VECTOR_ELT(VECTOR_ELT(result, 1), i, allocVector(REALSXP, INTEGER(featureCount)[0]));\n        SET_VECTOR_ELT(VECTOR_ELT(result, 2), i, allocVector(REALSXP, chunk_size));\n\n        for (unsigned int k = 0; k < INTEGER(featureCount)[0]; ++k)\n            REAL(VECTOR_ELT(VECTOR_ELT(result, 1), i))[k] =\n                    std::numeric_limits<double>::quiet_NaN();\n    }\n\n    for (unsigned int i = 0; i < solution_count; ++i)\n    {\n        MutualInformationMatrix mi_matrix(&data);\n\n        for (unsigned int j = 0; j < LENGTH(targetFeatureIndices); ++j)\n        {\n            Filter filter(p_children_count_per_level, feature_count_per_solution, &mi_matrix,\n                    INTEGER(targetFeatureIndices)[j], INTEGER(fixedFeatureCount)[0]);\n            filter.build();\n            filter.getSolutions(\n                    INTEGER(VECTOR_ELT(VECTOR_ELT(result, 0), j))\n                            + (i * feature_count_per_solution));\n            //filter.getScores(\n             //       REAL(VECTOR_ELT(VECTOR_ELT(result, 2), i)) + (i * feature_count_per_solution));\n\n            /*            Math::computeCausality(REAL(VECTOR_ELT(VECTOR_ELT(result, 1), i)), &mi_matrix,\n             INTEGER(VECTOR_ELT(VECTOR_ELT(result, 0), i)) + (i * chunk_size), 1,\n             feature_count_per_solution, INTEGER(featureCount)[0],\n             INTEGER(targetFeatureIndices)[i]);*/\n        }\n\n        data.bootstrap();\n    }\n\n    UNPROTECT(1);\n    delete[] p_children_count_per_level;\n    return result;\n}\n\nextern \"C\" SEXP\nexport_mim(SEXP dataMatrix, SEXP priorsMatrix, SEXP priorsWeight, SEXP sampleStrata,\n        SEXP sampleWeights, SEXP featureTypes, SEXP sampleCount, SEXP featureCount,\n        SEXP sampleStratumCount, SEXP continuousEstimator, SEXP outX, SEXP bootstrapCount,\n        SEXP miMatrix)\n{\n    Matrix const priors_matrix(REAL(priorsMatrix), INTEGER(featureCount)[0],\n            INTEGER(featureCount)[0]);\n    Matrix const* const p_priors_matrix =\n            LENGTH(priorsMatrix) == INTEGER(featureCount)[0] * INTEGER(featureCount)[0] ?\n                    &priors_matrix : 0;\n    Data data(REAL(dataMatrix), p_priors_matrix, REAL(priorsWeight)[0], INTEGER(sampleCount)[0],\n            INTEGER(featureCount)[0], INTEGER(sampleStrata), REAL(sampleWeights),\n            INTEGER(featureTypes), INTEGER(sampleStratumCount)[0], INTEGER(continuousEstimator)[0],\n            INTEGER(outX)[0] != 0, INTEGER(bootstrapCount)[0]);\n    MutualInformationMatrix mi_matrix(&data, REAL(miMatrix));\n    mi_matrix.build();\n\n    return R_NilValue;\n}\n\nextern \"C\" SEXP\nget_thread_count(SEXP threadCount)\n{\n#ifdef _OPENMP\n    INTEGER(threadCount)[0] = omp_get_max_threads();\n#endif\n\n    return R_NilValue;\n}\n\nextern \"C\" SEXP\nset_thread_count(SEXP threadCount)\n{\n#ifdef _OPENMP\n    omp_set_num_threads(INTEGER(threadCount)[0]);\n#endif\n\n    return R_NilValue;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `CALLDEF` macro in this code, and how is it used?",
        "answer": "The `CALLDEF` macro is used to create entries in the `callEntries` array. It takes two arguments: the name of a function and the number of parameters it expects. The macro expands to create a structure that includes the function name as a string, a cast of the function pointer to `DL_FUNC`, and the number of parameters. This is used to register C functions that can be called from R."
      },
      {
        "question": "Explain the purpose of the `R_init_mRMRe` function and what it does.",
        "answer": "The `R_init_mRMRe` function is an initialization function for the R package. It is called when the package is loaded. Its main purpose is to register the C functions defined in `callEntries` with R, making them available for use within R. It uses `R_registerRoutines` to register the functions and `R_useDynamicSymbols` to disable dynamic symbol lookup for better performance and safety."
      },
      {
        "question": "What is the significance of the `extern \"C\"` declarations in this code?",
        "answer": "The `extern \"C\"` declarations are used to specify C linkage for the functions. This is necessary when compiling C++ code that needs to be called from C or other languages (like R in this case). It ensures that the function names are not mangled by the C++ compiler, allowing them to be correctly linked and called from R or C code."
      }
    ],
    "completion_tasks": [
      {
        "partial": "extern \"C\" SEXP\nexport_concordance_index(SEXP samplesA, SEXP samplesB, SEXP samplesC, SEXP samplesD,\n        SEXP sampleStrata, SEXP sampleWeights, SEXP sampleStratumCount, SEXP outX, SEXP ratio,\n        SEXP concordantWeights, SEXP discordantWeights, SEXP uninformativeWeights,\n        SEXP relevantWeights)\n{\n    unsigned int const sample_count = LENGTH(samplesA);\n    unsigned int** p_sample_indices_per_stratum = new unsigned int*[INTEGER(sampleStratumCount)[0]];\n    unsigned int* const p_sample_count_per_stratum =\n            new unsigned int[INTEGER(sampleStratumCount)[0]];\n    Math::placeStratificationData(INTEGER(sampleStrata), REAL(sampleWeights),\n            p_sample_indices_per_stratum, p_sample_count_per_stratum,\n            INTEGER(sampleStratumCount)[0], sample_count);\n\n    // TODO: Implement concordance index computation\n\n    delete[] p_sample_count_per_stratum;\n    for (unsigned int i = 0; i < INTEGER(sampleStratumCount)[0]; ++i)\n        delete[] p_sample_indices_per_stratum[i];\n    delete[] p_sample_indices_per_stratum;\n\n    return R_NilValue;\n}",
        "complete": "extern \"C\" SEXP\nexport_concordance_index(SEXP samplesA, SEXP samplesB, SEXP samplesC, SEXP samplesD,\n        SEXP sampleStrata, SEXP sampleWeights, SEXP sampleStratumCount, SEXP outX, SEXP ratio,\n        SEXP concordantWeights, SEXP discordantWeights, SEXP uninformativeWeights,\n        SEXP relevantWeights)\n{\n    unsigned int const sample_count = LENGTH(samplesA);\n    unsigned int** p_sample_indices_per_stratum = new unsigned int*[INTEGER(sampleStratumCount)[0]];\n    unsigned int* const p_sample_count_per_stratum =\n            new unsigned int[INTEGER(sampleStratumCount)[0]];\n    Math::placeStratificationData(INTEGER(sampleStrata), REAL(sampleWeights),\n            p_sample_indices_per_stratum, p_sample_count_per_stratum,\n            INTEGER(sampleStratumCount)[0], sample_count);\n\n    if (LENGTH(samplesD) != 0 && LENGTH(samplesC) != 0)\n        REAL(ratio)[0] = Math::computeConcordanceIndex(REAL(samplesA), REAL(samplesB),\n                REAL(samplesC), REAL(samplesD), REAL(sampleWeights), p_sample_indices_per_stratum,\n                p_sample_count_per_stratum, INTEGER(sampleStratumCount)[0], INTEGER(outX)[0] != 0,\n                REAL(concordantWeights), REAL(discordantWeights), REAL(uninformativeWeights),\n                REAL(relevantWeights));\n    else if (LENGTH(samplesC) != 0)\n        REAL(ratio)[0] = Math::computeConcordanceIndex(REAL(samplesA), REAL(samplesB),\n                REAL(samplesC), REAL(sampleWeights), p_sample_indices_per_stratum,\n                p_sample_count_per_stratum, INTEGER(sampleStratumCount)[0], INTEGER(outX)[0] != 0,\n                REAL(concordantWeights), REAL(discordantWeights), REAL(uninformativeWeights),\n                REAL(relevantWeights));\n    else\n        REAL(ratio)[0] = Math::computeConcordanceIndex(REAL(samplesA), REAL(samplesB),\n                REAL(sampleWeights), p_sample_indices_per_stratum, p_sample_count_per_stratum,\n                INTEGER(sampleStratumCount)[0], INTEGER(outX)[0] != 0, REAL(concordantWeights),\n                REAL(discordantWeights), REAL(uninformativeWeights), REAL(relevantWeights));\n\n    delete[] p_sample_count_per_stratum;\n    for (unsigned int i = 0; i < INTEGER(sampleStratumCount)[0]; ++i)\n        delete[] p_sample_indices_per_stratum[i];\n    delete[] p_sample_indices_per_stratum;\n\n    return R_NilValue;\n}"
      },
      {
        "partial": "extern \"C\" SEXP\nexport_filters(SEXP childrenCountPerLevel, SEXP dataMatrix, SEXP priorsMatrix, SEXP priorsWeight,\n        SEXP sampleStrata, SEXP sampleWeights, SEXP featureTypes, SEXP sampleCount,\n        SEXP featureCount, SEXP sampleStratumCount, SEXP targetFeatureIndices, SEXP fixedFeatureCount,\n        SEXP continuousEstimator, SEXP outX, SEXP bootstrapCount, SEXP miMatrix)\n{\n    Matrix const priors_matrix(REAL(priorsMatrix), INTEGER(featureCount)[0],\n            INTEGER(featureCount)[0]);\n    Matrix const* const p_priors_matrix =\n            LENGTH(priorsMatrix) == INTEGER(featureCount)[0] * INTEGER(featureCount)[0] ?\n                    &priors_matrix : 0;\n    Data data(REAL(dataMatrix), p_priors_matrix, REAL(priorsWeight)[0], INTEGER(sampleCount)[0],\n            INTEGER(featureCount)[0], INTEGER(sampleStrata), REAL(sampleWeights),\n            INTEGER(featureTypes), INTEGER(sampleStratumCount)[0], INTEGER(continuousEstimator)[0],\n            INTEGER(outX)[0] != 0, INTEGER(bootstrapCount)[0]);\n    MutualInformationMatrix mi_matrix(&data, REAL(miMatrix));\n\n    // TODO: Implement filter logic and result generation\n\n    return R_NilValue;\n}",
        "complete": "extern \"C\" SEXP\nexport_filters(SEXP childrenCountPerLevel, SEXP dataMatrix, SEXP priorsMatrix, SEXP priorsWeight,\n        SEXP sampleStrata, SEXP sampleWeights, SEXP featureTypes, SEXP sampleCount,\n        SEXP featureCount, SEXP sampleStratumCount, SEXP targetFeatureIndices, SEXP fixedFeatureCount,\n        SEXP continuousEstimator, SEXP outX, SEXP bootstrapCount, SEXP miMatrix)\n{\n    Matrix const priors_matrix(REAL(priorsMatrix), INTEGER(featureCount)[0],\n            INTEGER(featureCount)[0]);\n    Matrix const* const p_priors_matrix =\n            LENGTH(priorsMatrix) == INTEGER(featureCount)[0] * INTEGER(featureCount)[0] ?\n                    &priors_matrix : 0;\n    Data data(REAL(dataMatrix), p_priors_matrix, REAL(priorsWeight)[0], INTEGER(sampleCount)[0],\n            INTEGER(featureCount)[0], INTEGER(sampleStrata), REAL(sampleWeights),\n            INTEGER(featureTypes), INTEGER(sampleStratumCount)[0], INTEGER(continuousEstimator)[0],\n            INTEGER(outX)[0] != 0, INTEGER(bootstrapCount)[0]);\n    MutualInformationMatrix mi_matrix(&data, REAL(miMatrix));\n\n    unsigned int solution_count = 1;\n    for (unsigned int i = 0; i < LENGTH(childrenCountPerLevel); ++i)\n        solution_count *= INTEGER(childrenCountPerLevel)[i];\n    unsigned int const feature_count_per_solution = LENGTH(childrenCountPerLevel);\n    unsigned int const chunk_size = solution_count * feature_count_per_solution;\n\n    SEXP result;\n    PROTECT(result = allocVector(VECSXP, 3));\n\n    SET_VECTOR_ELT(result, 0, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n    SET_VECTOR_ELT(result, 1, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n    SET_VECTOR_ELT(result, 2, allocVector(VECSXP, LENGTH(targetFeatureIndices)));\n\n    for (unsigned int i = 0; i < LENGTH(targetFeatureIndices); ++i)\n    {\n        Filter filter(INTEGER(childrenCountPerLevel), LENGTH(childrenCountPerLevel), &mi_matrix,\n                INTEGER(targetFeatureIndices)[i], INTEGER(fixedFeatureCount)[0]);\n        filter.build();\n\n        SET_VECTOR_ELT(VECTOR_ELT(result, 0), i, allocVector(INTSXP, chunk_size));\n        SET_VECTOR_ELT(VECTOR_ELT(result, 1), i, allocVector(REALSXP, INTEGER(featureCount)[0]));\n        SET_VECTOR_ELT(VECTOR_ELT(result, 2), i, allocVector(REALSXP, chunk_size));\n\n        filter.getSolutions(INTEGER(VECTOR_ELT(VECTOR_ELT(result, 0), i)));\n        filter.getScores(REAL(VECTOR_ELT(VECTOR_ELT(result, 2), i)));\n\n        for (unsigned int k = 0; k < INTEGER(featureCount)[0]; ++k)\n            REAL(VECTOR_ELT(VECTOR_ELT(result, 1), i))[k] =\n                    std::numeric_limits<double>::quiet_NaN();\n\n        Math::computeCausality(REAL(VECTOR_ELT(VECTOR_ELT(result, 1), i)), &mi_matrix,\n                INTEGER(VECTOR_ELT(VECTOR_ELT(result, 0), i)), solution_count,\n                feature_count_per_solution, INTEGER(featureCount)[0],\n                INTEGER(targetFeatureIndices)[i]);\n    }\n\n    UNPROTECT(1);\n    return result;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/utils-messages.R",
    "language": "R",
    "content": "# ======================================================\n# Utilty functions for message, warnings, errors and cat\n# ------------------------------------------------------\n\n#' @title .formatMessage\n#'\n#' @description Format one or more strings to fit nicely displayed in the\n#' R console at any given width.\n#'\n#' @param ... One or more `character` vectors containing the strings to be\n#' formatted\n#' @param collapse `character(1)` A string of characters to collapse vectors\n#' inside `...` with.\n#'\n#' @md\n#' @keywords internal\n#' @export\n#' @noRd\n.formatMessage <- function(..., collapse=', ') {\n    paste0(strwrap(paste0(..., collapse=collapse)), collapse='\\n')\n}\n\n#' @title .message\n#'\n#' @description Alternative to message which respects the local package\n#' settings for verbosity in `options()`. The message is displayed if either\n#' the general R option 'verbose' is TRUE, or if '<packageName()>.verbose'\n#' is TRUE.\n#'\n#' @details\n#' Defaults for package verbosity are determined in zzz.R via the .onAttach\n#' function. When loading the package, if the session is interactive the\n#' default verbosity is TRUE, otherwise it is FALSE.\n#'\n#' @md\n#' @importFrom crayon blue\n#' @keywords internal\n#' @export\n#' @noRd\n.message <- function(...) {\n    optionName <- paste0(packageName(), '.verbose')\n    optionIsTRUE <- !is.null(getOption(optionName)) && getOption(optionName)\n    verboseIsTRUE <- getOption('verbose')\n    if (optionIsTRUE || verboseIsTRUE)\n        message(crayon::blue$bold(.formatMessage(...)))\n}\n\n#' @title .warning\n#'\n#' @description Alternative to message which respects the local package\n#' settings for verbosity in `options()`. The message is displayed if either\n#' the general R option 'verbose' is TRUE, or if '<packageName()>.verbose'\n#' is TRUE.\n#'\n#' @details\n#' Defaults for package verbosity are determined in zzz.R via the .onAttach\n#' function. When loading the package, if the session is interactive the\n#' default verbosity is TRUE, otherwise it is FALSE.\n#'\n#' @md\n#' @importFrom crayon cyan\n#' @keywords internal\n#' @export\n#' @noRd\n.warning <- function(...) {\n    warning(crayon::cyan$bold(.formatMessage(...)), call.=FALSE)\n}\n\n#' @title .error\n#'\n#' @description Alternative to error which formats the error to fit the\n#' console and prints it in magenta.\n#'\n#' @details\n#' Defaults for package verbosity are determined in zzz.R via the .onAttach\n#' function. When loading the package, if the session is interactive the\n#' default verbosity is TRUE, otherwise it is FALSE.\n#'\n#' @md\n#' @importFrom crayon magenta\n#' @keywords internal\n#' @export\n#' @noRd\n.error <- function(...) {\n    stop(crayon::magenta$bold(.formatMessage(...)), call.=FALSE)\n}\n\n#' @title .funContext\n#'\n#' @description Build a string with the package and function name for a current\n#'   function. Prepended to error message to make it easier to determine where\n#'   the error or warning came from.\n#'\n#' @param funName `character(1)` A string with the function name, prepended\n#'   with the correct connection to the package NAMESPACE. For exported functions\n#'   use '::', for non-exported functions use ':::'.\n#'\n#' @keywords internal\n#' @export\n#' @noRd\n.funContext <- function(funName) paste0('[', packageName(), funName, ']\\n')\n\n\n#' @title .parseToRoxygen\n#'\n#' @description Takes a string block of roxygen2 tags sepearated by new-line\n#'   characteres and parses it to the appropriate format for the @eval tag,\n#'   subtituting any string in { } for the argument of the same name in `...`.\n#'\n#' @keywords internal\n#' @export\n#' @noRd\n.parseToRoxygen <- function(string, ...) {\n    unlist(strsplit(\n        with(list(...), glue::glue(string)),\n    '\\n'))\n}\n\n#' @keywords internal\n.paste_ <- function(x, y) paste(x, y, sep='_')\n\n#' @keywords internal\n.paste_colon <- function(x, y) paste(x, y, sep=':')\n\n#' @keywords interanl\n.paste_slashes <- function(...) paste(..., sep='///')",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.formatMessage` function in this code snippet?",
        "answer": "The `.formatMessage` function is designed to format one or more strings to fit nicely when displayed in the R console at any given width. It takes multiple character vectors as input, collapses them with a specified separator, and then wraps the resulting string to fit the console width using `strwrap`. The formatted message is then returned as a single string with line breaks."
      },
      {
        "question": "How does the `.message` function differ from the standard R `message` function?",
        "answer": "The `.message` function is an alternative to the standard R `message` function that respects local package settings for verbosity. It only displays the message if either the general R option 'verbose' is TRUE, or if '<packageName()>.verbose' is TRUE. Additionally, it formats the message using `.formatMessage` and applies blue bold styling using the `crayon` package. This allows for more controlled and visually distinct message output based on package-specific verbosity settings."
      },
      {
        "question": "What is the purpose of the `.funContext` function and how might it be used in error handling?",
        "answer": "The `.funContext` function builds a string containing the package name and function name for the current function. Its purpose is to create a context prefix that can be prepended to error or warning messages. This makes it easier to determine the origin of an error or warning within the package. For example, it could be used in combination with the `.error` function to provide more informative error messages, like this: `.error(.funContext('::someFunction'), 'An error occurred')`."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".formatMessage <- function(..., collapse=', ') {\n    # Complete the function to format messages\n}",
        "complete": ".formatMessage <- function(..., collapse=', ') {\n    paste0(strwrap(paste0(..., collapse=collapse)), collapse='\n')\n}"
      },
      {
        "partial": ".message <- function(...) {\n    optionName <- paste0(packageName(), '.verbose')\n    optionIsTRUE <- !is.null(getOption(optionName)) && getOption(optionName)\n    verboseIsTRUE <- getOption('verbose')\n    # Complete the function to display messages based on verbosity settings\n}",
        "complete": ".message <- function(...) {\n    optionName <- paste0(packageName(), '.verbose')\n    optionIsTRUE <- !is.null(getOption(optionName)) && getOption(optionName)\n    verboseIsTRUE <- getOption('verbose')\n    if (optionIsTRUE || verboseIsTRUE)\n        message(crayon::blue$bold(.formatMessage(...)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/unitest/test_unit_set_association.R",
    "language": "R",
    "content": "library(Hmisc)\nlibrary(mRMRe)\n\n##\n## Tests\n##\n\n#\n# correlate - Testing the correlate method against other known and proven methods\n#\n\n## Cont vs Cont\n\na <- runif(100)\nb <- runif(100)\n\ntest_correlate(a,b, method = \"pearson\")\ntest_correlate(a,b, method = \"spearman\")\ntest_correlate(a,b, method = \"kendall\")\ntest_correlate(a,b, method = \"cindex\")\ntest_correlate(a,b, method = \"frequency\")\n\n## Cat vs Cont\n\na <- sample(0:3, 100, T)\nb <- runif(100)\n\ntest_correlate(a,b, method = \"pearson\")\ntest_correlate(a,b, method = \"spearman\")\ntest_correlate(a,b, method = \"kendall\")\ntest_correlate(a,b, method = \"cindex\")\ntest_correlate(a,b, method = \"frequency\")\n\n## Cat vs Cat\n\na <- sample(0:3, 100, T)\nb <- sample(0:5, 100, T)\n\ntest_correlate(a,b, method = \"pearson\")\ntest_correlate(a,b, method = \"spearman\")\ntest_correlate(a,b, method = \"kendall\")\ntest_correlate(a,b, method = \"cindex\")\ntest_correlate(a,b, method = \"frequency\")\ntest_correlate(a,b, method = \"cramersv\")\n\n#\n# mim - Testing the mim method against the correlate method\n#\n\ndd <- data.frame(\n        \"surv1\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont1\" = runif(100),\n        \"cat1\"  = factor(sample(1:5, 100, replace = TRUE), ordered = TRUE),\n        \"surv2\" = Surv(runif(100), sample(0:1, 100, replace = TRUE)),\n        \"cont2\" = runif(100),\n        \"cat2\"  = factor(sample(1:5, 100, replace = TRUE), ordered = TRUE))\n\ndata <- mRMR.data(data = dd)\ncors <- mim(data, method = \"cor\")\ncombinations <- combn(featureNames(data), 2)\n# results <- apply(combinations, 2, function(i) cors[i[[1]], i[[2]]] == correlate(i, j))\n# FIXME: Finish this test\n\n##\n## Methods\n##\n\ntest_correlate <- function(a,b, method)\n{\n\trun_correlate_test(a, b, method)\n\ta[sample(1:length(a), round(length(a) / 10))] <- NA\n\tb[sample(1:length(b), round(length(b) / 10))] <- NA\n\tmessage(\"Test with NA\")\n\trun_correlate_test(a, b, method)\n}\n\nrun_correlate_test <- function(a, b, method)\n{\n\tmessage(\"Testing with 3rd party\")\n\tif (method == \"pearson\" || method == \"spearman\" || method == \"kendall\")\n\t\tconfirmation <- cor(a, b, method = method, use = \"complete.obs\")\n\telse if (method == \"frequency\")\n\t\tconfirmation <- mean (a > b, na.rm = T)\n\telse if (method == \"cindex\")\n\t\tconfirmation <- as.numeric(Hmisc::rcorr.cens(a,b)[1])\n\telse if (method == \"cramersv\")\n\t\tconfirmation <- as.numeric(sqrt(chisq.test(a, b, correct=FALSE)$statistic /\n\t\t\t\t\t\t\t\t(length(a) * (min(length(unique(a)),length(unique(b))) - 1))))\n\t\n\tcorrelation <- correlate(a,b, method=method)$s\n\tif(abs(confirmation - correlation) > 1e5)\n\t\tstop(\"Correlation is different that confirmation\", confirmation, correlation)\n\telse\n\t\tmessage(\"\\t3rd Party OK\")\n\t\n\tmessage(\"Testing for symmetry\")\n\tif (abs(correlate(b,a, method=method)$s - correlation) > 1e10)\n\t\tstop(\"Correlation is not symmetric\")\n\telse\n\t\tmessage(\"\\tSymmetry OK\")\n\t\n\tmessage(\"Testing for stratification\")\n\ts <- as.factor(c(rep(0,length(a)),rep(1,length(a)), rep(2,length(a))))\n\tw <- c(rep(runif(1),length(a)),rep(runif(1),length(a)), rep(runif(1),length(a)))\n\ta <- c(a,a,a)\n\tb <- c(b,b,b)\n\t\n\tif (abs(correlate(a,b, method=method, strata=s)$s - correlation) > 1e10)\n\t\tstop(\"Stratification is wrong\")\n\telse if (abs(correlate(a,b, method=method, strata=s)$s - correlation) > 1e10)\n\t\tstop(\"Stratification is not symmetric\")\n\telse\n\t\tmessage(\"\\tStratification OK\")\n\tmessage(\"Testing for weights\")\n\tif (abs(correlate(a,b, method=method, weights=w)$s - correlation) > 1e10)\n\t\tstop(\"Weighting is wrong\")\n\telse if (abs(correlate(b,a, method=method, weights=w)$s - correlation) > 1e10)\n\t\tstop(\"Weighting is not symmetric\")\n\telse\n\t\tmessage(\"\\tWeighting OK\")\n\t\n\tmessage(\"Test for strata+weights\")\n\tif(abs(correlate(a,b, method=method, weights=w, strata=s)$s - correlation) > 1e10)\n\t\tstop(\"Stratification + Weighting is wrong\")\n\telse if (abs(correlate(b,a, method=method, weights=w, strata=s)$s - correlation) > 1e10)\n\t\tstop(\"Stratification + Weighting is not symmetric\")\n\telse\n\t\tmessage(\"\\tStratification + Weighting OK\")\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_correlate` function in this code snippet?",
        "answer": "The `test_correlate` function is designed to test the `correlate` method against other known and proven correlation methods. It performs the following tasks:\n1. Runs a correlation test on the input data (a and b) using the specified method.\n2. Introduces NA values to about 10% of the data points in both a and b.\n3. Runs the correlation test again with the NA values to ensure the method handles missing data correctly.\nThis function helps validate the robustness and accuracy of the `correlate` method under different conditions."
      },
      {
        "question": "How does the code test for symmetry in correlation calculations?",
        "answer": "The code tests for symmetry in correlation calculations by comparing the results of `correlate(a,b)` and `correlate(b,a)`. Specifically:\n1. It calculates the correlation between 'a' and 'b' using the specified method.\n2. It then calculates the correlation between 'b' and 'a' using the same method.\n3. The absolute difference between these two results is compared to a small threshold (1e-10).\n4. If the difference is larger than the threshold, it raises an error indicating that the correlation is not symmetric.\n5. If the difference is within the threshold, it confirms that the symmetry test passed.\nThis ensures that the correlation method produces consistent results regardless of the order of input variables."
      },
      {
        "question": "What is the purpose of testing correlation with stratification and weights in this code?",
        "answer": "The code tests correlation with stratification and weights to ensure that the `correlate` function can handle more complex scenarios accurately. Specifically:\n1. Stratification: It creates a factor variable 's' to group the data into strata and tests if the correlation remains consistent when accounting for these groups.\n2. Weights: It generates a weight vector 'w' and tests if the correlation calculation correctly incorporates these weights.\n3. Combined effect: It tests both stratification and weights together to ensure they work correctly in combination.\n\nThese tests are important because:\n- They verify that the `correlate` function can handle grouped data analysis (stratification).\n- They confirm that the function can properly account for the importance of different observations (weights).\n- They ensure that these advanced features don't introduce errors or inconsistencies in the correlation calculations.\n\nBy testing these scenarios, the code validates that the `correlate` function is robust and reliable for various types of statistical analyses."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_correlate <- function(a, b, method) {\n  run_correlate_test(a, b, method)\n  # Add NA values to a and b\n  # Run the test again with NA values\n  # TODO: Complete the function\n}",
        "complete": "test_correlate <- function(a, b, method) {\n  run_correlate_test(a, b, method)\n  a[sample(1:length(a), round(length(a) / 10))] <- NA\n  b[sample(1:length(b), round(length(b) / 10))] <- NA\n  message(\"Test with NA\")\n  run_correlate_test(a, b, method)\n}"
      },
      {
        "partial": "run_correlate_test <- function(a, b, method) {\n  message(\"Testing with 3rd party\")\n  # Calculate confirmation based on method\n  # Compare correlation with confirmation\n  # Test for symmetry\n  # Test for stratification\n  # Test for weights\n  # Test for strata+weights\n  # TODO: Complete the function\n}",
        "complete": "run_correlate_test <- function(a, b, method) {\n  message(\"Testing with 3rd party\")\n  if (method %in% c(\"pearson\", \"spearman\", \"kendall\"))\n    confirmation <- cor(a, b, method = method, use = \"complete.obs\")\n  else if (method == \"frequency\")\n    confirmation <- mean(a > b, na.rm = TRUE)\n  else if (method == \"cindex\")\n    confirmation <- as.numeric(Hmisc::rcorr.cens(a, b)[1])\n  else if (method == \"cramersv\")\n    confirmation <- as.numeric(sqrt(chisq.test(a, b, correct=FALSE)$statistic /\n                  (length(a) * (min(length(unique(a)), length(unique(b))) - 1))))\n  \n  correlation <- correlate(a, b, method=method)$s\n  if (abs(confirmation - correlation) > 1e-5)\n    stop(\"Correlation is different than confirmation\", confirmation, correlation)\n  else\n    message(\"\\t3rd Party OK\")\n  \n  message(\"Testing for symmetry\")\n  if (abs(correlate(b, a, method=method)$s - correlation) > 1e-10)\n    stop(\"Correlation is not symmetric\")\n  else\n    message(\"\\tSymmetry OK\")\n  \n  message(\"Testing for stratification\")\n  s <- as.factor(rep(0:2, each=length(a)))\n  w <- rep(runif(3), each=length(a))\n  a <- rep(a, 3)\n  b <- rep(b, 3)\n  \n  if (abs(correlate(a, b, method=method, strata=s)$s - correlation) > 1e-10)\n    stop(\"Stratification is wrong\")\n  else if (abs(correlate(b, a, method=method, strata=s)$s - correlation) > 1e-10)\n    stop(\"Stratification is not symmetric\")\n  else\n    message(\"\\tStratification OK\")\n  \n  message(\"Testing for weights\")\n  if (abs(correlate(a, b, method=method, weights=w)$s - correlation) > 1e-10)\n    stop(\"Weighting is wrong\")\n  else if (abs(correlate(b, a, method=method, weights=w)$s - correlation) > 1e-10)\n    stop(\"Weighting is not symmetric\")\n  else\n    message(\"\\tWeighting OK\")\n  \n  message(\"Test for strata+weights\")\n  if (abs(correlate(a, b, method=method, weights=w, strata=s)$s - correlation) > 1e-10)\n    stop(\"Stratification + Weighting is wrong\")\n  else if (abs(correlate(b, a, method=method, weights=w, strata=s)$s - correlation) > 1e-10)\n    stop(\"Stratification + Weighting is not symmetric\")\n  else\n    message(\"\\tStratification + Weighting OK\")\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/R/aggregate-methods.R",
    "language": "R",
    "content": "#' @include LongTable-class.R\n#' @include LongTable-accessors.R\n#' @include TreatmentResponseExperiment-class.R\nNULL\n\n#' @importFrom BiocParallel bpparam bpworkers<- bpworkers bpprogressbar<-\n#' bpprogressbar bplapply\n#' @importFrom data.table rbindlist setDT merge.data.table\n#' @importMethodsFrom S4Vectors aggregate\nNULL\n\n#' @noRd\n.docs_CoreGx_aggregate <- function(...) CoreGx:::.parseToRoxygen(\n    \"\n    @param by `character` One or more valid column names in `x` to compute\n    groups using.\n    @param ... `call` One or more aggregations to compute for each group by in x.\n    If you name aggregation calls, that will be the column name of the value\n    in the resulting `data.table` otherwise a default name will be parsed from\n    the function name and its first argument, which is assumed to be the name\n    of the column being aggregated over.\n    @param nthread `numeric(1)` Number of threads to use for split-apply-combine\n    parallelization. Uses `BiocParllel::bplapply` if nthread > 1 or you pass in\n    `BPPARAM`. Does not modify data.table threads, so be sure to use\n    setDTthreads for reasonable nested parallelism. See details for performance\n    considerations.\n    @param progress `logical(1)` Display a progress bar for parallelized\n    computations? Only works if `bpprogressbar<-` is defined for the current\n    BiocParallel back-end.\n    @param BPPARAM `BiocParallelParam` object. Use to customized the\n    the parallization back-end of bplapply. Note, nthread over-rides any\n    settings from BPPARAM as long as `bpworkers<-` is defined for that class.\n    @param enlist `logical(1)` Default is `TRUE`. Set to `FALSE` to evaluate\n    the first call in `...` within `data.table` groups. See details for more\n    information.\n    @param moreArgs `list()` A named list where each item is an argument one of\n    the calls in `...` which is not a column in the table being aggregated. Use\n    to further parameterize you calls. Please note that these are not added\n    to your aggregate calls unless you specify the names in the call.\n\n    @details\n    ## Use of Non-Standard Evaluation\n    Arguments in `...` are substituted and wrapped in a list, which is passed\n    through to the j argument of `[.data.table` internally. The function currently\n    tries to build informative column names for unnamed arguments in `...` by\n    appending the name of each function call with the name of its first argument,\n    which is assumed to be the column name being aggregated over. If an argument\n    to `...` is named, that will be the column name of its value in the resulting\n    `data.table`.\n\n    ## Enlisting\n    The primary use case for `enlist=FALSE` is to allow computation of dependent\n    aggregations, where the output from a previous aggregation is required in a\n    subsequent one. For this case, wrap your call in `{curly}` and assign intermediate\n    results to variables, returning the final results as a list where each list\n    item will become a column in the final table with the corresponding name.\n    Name inference is disabled for this case, since it is assumed you will name\n    the returned list items appropriately.\n    A major advantage over multiple calls to `aggregate` is that\n    the overhead of parallelization is paid only once even for complex multi-step\n    computations like fitting a model, capturing its paramters, and making\n    predictions using it. It also allows capturing arbitrarily complex calls\n    which can be recomputed later using the\n    `update,TreatmentResponseExperiment-method`\n    A potential disadvantage is increased RAM usage per\n    thread due to storing intermediate values in variables, as well as any\n    memory allocation overhead associate therewith.\n    \",\n    ...\n)\n\n#' Functional API for aggregation over a `LongTable` or inheriting class\n#'\n#' @description\n#' Compute a group-by operation over a `LongTable` object or it's inhering\n#' classes.\n#'\n#' @param x `LongTable` or inheriting class to compute aggregation on.\n#' @param assay `character(1)` The assay to aggregate over.\n#' @param subset `call` An R call to evaluate before perfoming an aggregate.\n#' This allows you to aggregate over a subset of columns in an assay but have\n#' it be assigned to the parent object. Default is TRUE, which includes all\n#' rows. Passed through as the `i` argument in `[.data.table`.\n#' @eval .docs_CoreGx_aggregate(curly=\"{\")\n#'\n#' @return `data.table` of aggregation results.\n#'\n#' @seealso `data.table::[.data.table`, `BiocParallel::bplapply`\n#'\n#' @export\nsetMethod(\"aggregate\", signature(x=\"LongTable\"),\n        function(x, assay, by, ...,  subset=TRUE, nthread=1, progress=TRUE,\n        BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[[assay]][eval(i), ]\n    aggregate2(\n        assay_,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n            moreArgs=moreArgs)\n})\n\n\n#' Functional S4 API for aggregation over a `data.table` object.\n#'\n#' @description\n#' Compute a group-by operation over a `data.table` in a functional, pipe\n#' compatible format.\n#'\n#' @details\n#' This S4 method override the default `aggregate` method for a `data.frame`\n#' and as such you need to call `aggregate.data.frame` directly to get the\n#' original S3 method for a `data.table`.\n#'\n#' @param x `data.table` to compute aggregation over.\n#' @param subset `call` An R call to evaluate before perfoming an aggregate.\n#' This allows you to aggregate over a subset of columns in an assay but have\n#' it be assigned to the parent object. Default is TRUE, which includes all\n#' rows. Passed through as the `i` argument in `[.data.table`.\n#' @eval .docs_CoreGx_aggregate(curly=\"{\")\n#'\n#' @return `data.table` of aggregated results with an `aggregations` attribute\n#' capturing metadata about the last aggregation performed on the table.\n#'\n#' @export\nsetMethod(\"aggregate\", signature=\"data.table\",\n        function(x, by, ..., subset=TRUE, nthread=1, progress=TRUE,\n        BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[eval(i), ]\n    aggregate2(\n        x,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n            moreArgs=moreArgs)\n})\n\n#' Functional API for data.table aggregation which allows capture of associated\n#' aggregate calls so they can be recomputed later.\n#'\n#' @param x `data.table`\n#' @eval .docs_CoreGx_aggregate(curly=\"{\")\n#'\n#' @return `data.table` of aggregation results.\n#'\n#' @seealso `data.table::[.data.table`, `BiocParallel::bplapply`\n#'\n#' @export\naggregate2 <- function(x, by, ..., nthread=1, progress=interactive(), BPPARAM=NULL,\n        enlist=TRUE, moreArgs=list()) {\n    ## TODO:: refator to checkmate\n    stopifnot(is.data.table(x))\n    stopifnot(is.character(by) && all(by %in% colnames(x)))\n    stopifnot(is.logical(progress) && length(progress) == 1)\n    stopifnot(is.logical(enlist) && length(enlist) == 1)\n    stopifnot(is.list(moreArgs))\n    stopifnot(length(moreArgs) == 0 || all(names(moreArgs) != \"\"))\n\n    # -- assign moreArgs to the function scope, if it is able to find the values\n    for (nm in names(moreArgs)) assign(nm, moreArgs[[nm]])\n\n    # -- capture dots as a call and parse dot names, adding default names if\n    # --   they are missing\n    agg_call <- if (enlist) substitute(list(...)) else substitute(...)\n    if (!enlist && ...length() > 1) warning(.warnMsg(\"Only one call can be \",\n        \"passed via ... when enlist=FALSE, ignoring all but first arugment!\"))\n    dot_names <- if (enlist) names(agg_call)[-1L] else ...names()\n    if (is.null(dot_names) && enlist) dot_names <- rep(\"\", length(agg_call) - 1)\n    for (i in which(dot_names == \"\")) {\n        dot_call <- agg_call[[i + 1]]\n        # assumes the first argument in a function call is always the column name!\n        dot_names[i] <- paste0(dot_call[1:max(2, length(dot_call))], collapse=\"_\")\n    }\n    call_idx <- if (!enlist) 2L else seq(2L, length(agg_call))\n    if (length(dot_names)) names(agg_call)[call_idx] <- dot_names\n\n    # -- compute the aggregates, parallelizing if nthread > 1\n    if (nthread == 1 && is.null(BPPARAM)) {\n        res <- x[, eval(agg_call), by=c(by)]\n    } else {\n        x <- copy(x) # prevent modifying the source by reference\n        # compute groups such that there is one table per thread\n        x[, group_id := .GRP, by=by]\n        ngrp <- x[, max(group_id)]\n        grp_size <- ceiling(ngrp / nthread)\n        x[, split_id := ceiling(group_id / grp_size)]\n        x_split <- split(x, by=\"split_id\")\n        stopifnot(length(x_split) == nthread)\n        if (is.null(BPPARAM)) {\n            BPPARAM <- BiocParallel::bpparam()\n        }\n        # optionally add progressbar\n        if (hasMethod(\"bpprogressbar<-\", signature=c(class(BPPARAM), \"logical\"))) {\n            BiocParallel::bpprogressbar(BPPARAM) <- progress\n        } else if (isTRUE(progress)) {\n            warning(.warnMsg(\n                \"Unable to set progressbar for BiocParallel backend: \",\n                class(BPPARAM)[1]), .call=FALSE)\n        }\n        # optionally set nthread\n        if (hasMethod(\"bpworkers<-\", signature=c(class(BPPARAM), \"integer\"))) {\n            BiocParallel::bpworkers(BPPARAM) <- nthread\n        } else if (nthread > 1) {\n            warning(.warnMsg(\"Unable to set nthread for BiocParallel backend: \",\n                class(BPPARAM)[1]), .call=FALSE)\n        }\n        res <- BiocParallel::bplapply(\n            x_split,\n            function(x, agg_call, by) x[, eval(substitute(agg_call)), by=c(by)],\n            agg_call=agg_call, by=by,\n            BPPARAM=BPPARAM\n        )\n        res <- rbindlist(res)\n    }\n    attributes(res)[[\"aggregations\"]] <- c(\n        attributes(res)[[\"aggregations\"]],\n        list(\n            agg_call=agg_call,\n            by=by,\n            enlist=enlist,\n            moreArgs=moreArgs\n        )\n    )\n    return(res)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.docs_CoreGx_aggregate` function in this code snippet?",
        "answer": "The `.docs_CoreGx_aggregate` function is used to generate Roxygen documentation for the `aggregate` method. It parses and formats the documentation string, including parameter descriptions, details, and other relevant information for the function's documentation."
      },
      {
        "question": "How does the `aggregate` method for the `LongTable` class differ from the `aggregate` method for the `data.table` class?",
        "answer": "The `aggregate` method for the `LongTable` class first extracts the specified assay from the `LongTable` object and applies any subset condition before calling `aggregate2`. In contrast, the `aggregate` method for the `data.table` class directly applies the subset condition to the input `data.table` before calling `aggregate2`. Both methods ultimately use the `aggregate2` function to perform the aggregation."
      },
      {
        "question": "What is the purpose of the `enlist` parameter in the `aggregate2` function, and how does it affect the aggregation process?",
        "answer": "The `enlist` parameter in the `aggregate2` function determines how the aggregation calls are processed. When `enlist=TRUE` (default), multiple aggregation calls can be passed and are wrapped in a list. When `enlist=FALSE`, only the first call in `...` is evaluated within `data.table` groups, allowing for dependent aggregations where the output from a previous aggregation is required in a subsequent one. This affects how the aggregation calls are captured and evaluated during the process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "aggregate2 <- function(x, by, ..., nthread=1, progress=interactive(), BPPARAM=NULL,\n        enlist=TRUE, moreArgs=list()) {\n    stopifnot(is.data.table(x))\n    stopifnot(is.character(by) && all(by %in% colnames(x)))\n    stopifnot(is.logical(progress) && length(progress) == 1)\n    stopifnot(is.logical(enlist) && length(enlist) == 1)\n    stopifnot(is.list(moreArgs))\n    stopifnot(length(moreArgs) == 0 || all(names(moreArgs) != \"\"))\n\n    for (nm in names(moreArgs)) assign(nm, moreArgs[[nm]])\n\n    agg_call <- if (enlist) substitute(list(...)) else substitute(...)\n    if (!enlist && ...length() > 1) warning(\"Only one call can be passed via ... when enlist=FALSE, ignoring all but first argument!\")\n    dot_names <- if (enlist) names(agg_call)[-1L] else ...names()\n    if (is.null(dot_names) && enlist) dot_names <- rep(\"\", length(agg_call) - 1)\n    for (i in which(dot_names == \"\")) {\n        dot_call <- agg_call[[i + 1]]\n        dot_names[i] <- paste0(dot_call[1:max(2, length(dot_call))], collapse=\"_\")\n    }\n    call_idx <- if (!enlist) 2L else seq(2L, length(agg_call))\n    if (length(dot_names)) names(agg_call)[call_idx] <- dot_names\n\n    # Complete the function here\n}",
        "complete": "aggregate2 <- function(x, by, ..., nthread=1, progress=interactive(), BPPARAM=NULL,\n        enlist=TRUE, moreArgs=list()) {\n    stopifnot(is.data.table(x))\n    stopifnot(is.character(by) && all(by %in% colnames(x)))\n    stopifnot(is.logical(progress) && length(progress) == 1)\n    stopifnot(is.logical(enlist) && length(enlist) == 1)\n    stopifnot(is.list(moreArgs))\n    stopifnot(length(moreArgs) == 0 || all(names(moreArgs) != \"\"))\n\n    for (nm in names(moreArgs)) assign(nm, moreArgs[[nm]])\n\n    agg_call <- if (enlist) substitute(list(...)) else substitute(...)\n    if (!enlist && ...length() > 1) warning(\"Only one call can be passed via ... when enlist=FALSE, ignoring all but first argument!\")\n    dot_names <- if (enlist) names(agg_call)[-1L] else ...names()\n    if (is.null(dot_names) && enlist) dot_names <- rep(\"\", length(agg_call) - 1)\n    for (i in which(dot_names == \"\")) {\n        dot_call <- agg_call[[i + 1]]\n        dot_names[i] <- paste0(dot_call[1:max(2, length(dot_call))], collapse=\"_\")\n    }\n    call_idx <- if (!enlist) 2L else seq(2L, length(agg_call))\n    if (length(dot_names)) names(agg_call)[call_idx] <- dot_names\n\n    if (nthread == 1 && is.null(BPPARAM)) {\n        res <- x[, eval(agg_call), by=c(by)]\n    } else {\n        x <- copy(x)\n        x[, group_id := .GRP, by=by]\n        ngrp <- x[, max(group_id)]\n        grp_size <- ceiling(ngrp / nthread)\n        x[, split_id := ceiling(group_id / grp_size)]\n        x_split <- split(x, by=\"split_id\")\n        stopifnot(length(x_split) == nthread)\n        if (is.null(BPPARAM)) BPPARAM <- BiocParallel::bpparam()\n        if (hasMethod(\"bpprogressbar<-\", signature=c(class(BPPARAM), \"logical\"))) {\n            BiocParallel::bpprogressbar(BPPARAM) <- progress\n        } else if (isTRUE(progress)) {\n            warning(\"Unable to set progressbar for BiocParallel backend: \", class(BPPARAM)[1], call.=FALSE)\n        }\n        if (hasMethod(\"bpworkers<-\", signature=c(class(BPPARAM), \"integer\"))) {\n            BiocParallel::bpworkers(BPPARAM) <- nthread\n        } else if (nthread > 1) {\n            warning(\"Unable to set nthread for BiocParallel backend: \", class(BPPARAM)[1], call.=FALSE)\n        }\n        res <- BiocParallel::bplapply(\n            x_split,\n            function(x, agg_call, by) x[, eval(substitute(agg_call)), by=c(by)],\n            agg_call=agg_call, by=by,\n            BPPARAM=BPPARAM\n        )\n        res <- rbindlist(res)\n    }\n    attributes(res)[[\"aggregations\"]] <- c(\n        attributes(res)[[\"aggregations\"]],\n        list(\n            agg_call=agg_call,\n            by=by,\n            enlist=enlist,\n            moreArgs=moreArgs\n        )\n    )\n    return(res)\n}"
      },
      {
        "partial": "setMethod(\"aggregate\", signature=\"data.table\",\n        function(x, by, ..., subset=TRUE, nthread=1, progress=TRUE,\n        BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[eval(i), ]\n    # Complete the function here\n})",
        "complete": "setMethod(\"aggregate\", signature=\"data.table\",\n        function(x, by, ..., subset=TRUE, nthread=1, progress=TRUE,\n        BPPARAM=NULL, enlist=TRUE, moreArgs=list()) {\n    i <- substitute(subset)\n    assay_ <- x[eval(i), ]\n    aggregate2(\n        assay_,\n        by=by,\n        ...,\n        nthread=nthread, progress=progress, BPPARAM=BPPARAM, enlist=enlist,\n            moreArgs=moreArgs)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/src/MutualInformationMatrix.cpp",
    "language": "cpp",
    "content": "#include \"MutualInformationMatrix.h\"\n\nMutualInformationMatrix::MutualInformationMatrix(Data const* const pData) :\n        Matrix(pData->getFeatureCount() * pData->getFeatureCount(), pData->getFeatureCount(),\n                pData->getFeatureCount()), mpData(pData)\n{\n    for (unsigned int i = 0; i < mColumnCount; ++i)\n        for (unsigned int j = 0; j < mColumnCount; ++j)\n            Matrix::at(i, j) = std::numeric_limits<double>::quiet_NaN();\n}\n\nMutualInformationMatrix::MutualInformationMatrix(Data const* const pData,\n        double* const pInternalData) :\n        Matrix(pInternalData, pData->getFeatureCount(), pData->getFeatureCount()), mpData(pData)\n{\n\n}\n\n/* virtual */\nMutualInformationMatrix::~MutualInformationMatrix()\n{\n\n}\n\n/* virtual */double&\nMutualInformationMatrix::at(unsigned int const i, unsigned int const j)\n{\n    if (Matrix::at(i, j) != Matrix::at(i, j))\n        mpData->computeMiBetweenFeatures(i, j, &Matrix::at(i, j), &Matrix::at(j, i));\n\n    return Matrix::at(i, j);\n}\n\n/* virtual */double const&\nMutualInformationMatrix::at(unsigned int const i, unsigned int const j) const\n{\n    return Matrix::at(i, j);\n}\n\nvoid const\nMutualInformationMatrix::build()\n{\n#ifdef _OPENMP\n#pragma omp parallel for schedule(static)\n#endif\n    for (unsigned int i = 0; i < mColumnCount; ++i)\n        for (unsigned int j = 0; j < mColumnCount; ++j)\n            at(i, j);\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the MutualInformationMatrix class and how does it initialize its elements in the constructor?",
        "answer": "The MutualInformationMatrix class is a specialized matrix for storing mutual information between features in a dataset. In its main constructor, it initializes all elements to quiet NaN (Not a Number) using std::numeric_limits<double>::quiet_NaN(). This initialization allows the class to lazily compute mutual information values only when they are accessed, as seen in the 'at' method."
      },
      {
        "question": "How does the 'at' method in the MutualInformationMatrix class differ from a standard matrix access method, and why is it implemented this way?",
        "answer": "The 'at' method in MutualInformationMatrix is overridden to provide lazy computation of mutual information. When an element is accessed, it first checks if the value is NaN. If it is, it computes the mutual information between the corresponding features using mpData->computeMiBetweenFeatures(). This approach saves computation time by only calculating values when they are needed, and it ensures that each pair of features has its mutual information computed only once."
      },
      {
        "question": "What is the purpose of the 'build' method in the MutualInformationMatrix class, and how does it optimize performance?",
        "answer": "The 'build' method is used to pre-compute all mutual information values in the matrix. It uses OpenMP parallelization (if available) to distribute the computation across multiple threads, potentially speeding up the process on multi-core systems. The method calls 'at(i, j)' for all pairs of features, which triggers the computation of mutual information for each pair. This can be useful when all values are needed upfront or to avoid lazy computation overhead in subsequent operations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#include \"MutualInformationMatrix.h\"\n\nMutualInformationMatrix::MutualInformationMatrix(Data const* const pData) :\n        Matrix(pData->getFeatureCount() * pData->getFeatureCount(), pData->getFeatureCount(),\n                pData->getFeatureCount()), mpData(pData)\n{\n    // TODO: Initialize matrix elements\n}",
        "complete": "#include \"MutualInformationMatrix.h\"\n\nMutualInformationMatrix::MutualInformationMatrix(Data const* const pData) :\n        Matrix(pData->getFeatureCount() * pData->getFeatureCount(), pData->getFeatureCount(),\n                pData->getFeatureCount()), mpData(pData)\n{\n    for (unsigned int i = 0; i < mColumnCount; ++i)\n        for (unsigned int j = 0; j < mColumnCount; ++j)\n            Matrix::at(i, j) = std::numeric_limits<double>::quiet_NaN();\n}"
      },
      {
        "partial": "void const\nMutualInformationMatrix::build()\n{\n    // TODO: Implement parallel computation of mutual information\n}",
        "complete": "void const\nMutualInformationMatrix::build()\n{\n#ifdef _OPENMP\n#pragma omp parallel for schedule(static)\n#endif\n    for (unsigned int i = 0; i < mColumnCount; ++i)\n        for (unsigned int j = 0; j < mColumnCount; ++j)\n            at(i, j);\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/CoreGx.git",
    "file": "../../../../repos/CoreGx/tests/testthat/test-utils-optimization.R",
    "language": "R",
    "content": "library(testthat)\ntestthat::local_edition(3)\n\n# Create some dummy data\nhillEqn <- function(x, Emin, Emax, EC50, lambda) {\n    (Emin + Emax * (x / EC50)^lambda) / (1 + (x / EC50)^lambda)\n}\n# Set parameters for function testing\ndoses <- rev(1000 / (5^(1:10)))\nlambda <- 0.6\nEmin <- 1\nEmax <- 0.5\nEC50 <- median(doses)\n# Helper to combine\nfx <- if (is_optim_compatible(hillEqn)) hillEqn else\n    make_optim_function(hillEqn, lambda=lambda, Emin=Emin)\nresponse <- hillEqn(doses, Emin=Emin, lambda=lambda, Emax=Emax, EC50=EC50)\nnresponse <- response + rnorm(length(response), sd=sd(response)*0.1)\n\n# -- Loss function tests\nlmsg <- c(\"LOSS FUNCTION: \")\ntestthat::test_that(paste0(lmsg, \".residual and .normal_loss produce equal results.\"), {\n    trunc_vals <- c(FALSE, TRUE, FALSE, TRUE)\n    nvals <- c(1, 1, 3, 3)\n    for (i in seq_along(trunc_vals)) {\n        n1 <- .residual(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, f=fx,\n            family=\"normal\", n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        n2 <- CoreGx:::.normal_loss(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, fn=fx,\n            n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        testthat::expect_equal(n1, n2,\n            info=paste0(\"trunc: \", trunc_vals[i], \", n: \", nvals[i])\n        )\n    }\n})\n\ntestthat::test_that(paste0(lmsg, \".residual and .cauchy_loss produce equal results.\"), {\n    trunc_vals <- c(FALSE, TRUE, FALSE, TRUE)\n    nvals <- c(1, 1, 3, 3)\n    for (i in seq_along(trunc_vals)) {\n        c1 <- .residual(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, f=fx,\n            family=\"Cauchy\", n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        c2 <- CoreGx:::.cauchy_loss(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, fn=fx,\n            n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        testthat::expect_equal(c1, c2,\n            info=paste0(\"trunc: \", trunc_vals[i], \", n: \", nvals[i])\n        )\n    }\n})\n\n# -- Curve fitting\ncmsg <- \"CURVE FITTING: \"\ntestthat::test_that(paste0(cmsg, \".fitCurve and .fitCurve2 produce equal results for 2-parameter Hill curve.\"), {\n    par_list <- list(c(Emax=0.1, EC50=0.1), c(Emax=0.5, EC5O=500), c(Emax=0.9, EC50=100))\n    for (i in seq_along(par_list)) {\n        pars <- par_list[[i]]\n        normal_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"normal\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses)),\n            lower_bound=c(0, min(doses)),\n            density=c(2, 10),\n            precision=1e-4,\n            step=0.5 / c(2, 10)\n        )\n        normal_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.normal_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            lambda=lambda,\n            upper=c(2, max(doses)),\n            lower=c(0, min(doses)),\n            density=c(2, 10),\n            precision=1e-4,\n            step=0.5 / c(2, 10)\n        )\n        testthat::expect_equal(normal_par1, normal_par2,\n            info=paste0(\"Emax: \", pars[1], \", EC50: \", pars[2]))\n        cauchy_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"Cauchy\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses)),\n            lower_bound=c(0, min(doses)),\n            density=c(2, 10),\n            precision=1e-4,\n            step=0.5 / c(2, 10)\n        )\n        cauchy_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.cauchy_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            lambda=lambda,\n            upper=c(2, max(doses)),\n            lower=c(0, min(doses)),\n            density=c(2, 10),\n            precision=1e-4,\n            step=0.5 / c(2, 10)\n        )\n        testthat::expect_equal(cauchy_par1, cauchy_par2,\n            info=paste0(\"Emax: \", pars[1], \", EC50: \", pars[2]))\n    }\n})\n\n\ntestthat::test_that(\n    paste0(cmsg, \".fitCurve and .fitCurve2 produce equal results for 3-parameter Hill curve.\"), {\n    par_list <- list(\n        c(Emax=0.1, EC50=0.1, lambda=1),\n        c(Emax=0.5, EC5O=500, lambda=0.75),\n        c(Emax=0.9, EC50=100, lambda=2)\n    )\n    fx <- make_optim_function(hillEqn, Emin=Emin)\n    for (i in seq_along(par_list)) {\n        pars <- par_list[[i]]\n        normal_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"normal\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses), 6),\n            lower_bound=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        normal_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.normal_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            upper=c(2, max(doses), 6),\n            lower=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        testthat::expect_equal(normal_par1, normal_par2,\n            info=paste0(\"Emax: \", pars[1], \", EC50: \", pars[2]))\n        cauchy_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"Cauchy\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses), 6),\n            lower_bound=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        cauchy_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.cauchy_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            upper=c(2, max(doses), 6),\n            lower=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        testthat::expect_equal(cauchy_par1, cauchy_par2,\n            info=paste0(\"Emax: \", pars[1], \", EC50: \", pars[2]))\n    }\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hillEqn` function in this code, and how is it used to generate test data?",
        "answer": "The `hillEqn` function represents the Hill equation, which is commonly used in dose-response modeling. In this code, it's used to generate synthetic dose-response data for testing purposes. The function takes parameters Emin (minimum effect), Emax (maximum effect), EC50 (half-maximal effective concentration), and lambda (Hill slope). Test data is created by calling this function with specific parameter values and a range of doses, then adding random noise to simulate real-world variability."
      },
      {
        "question": "How do the `.fitCurve` and `.fitCurve2` functions differ, and what is the purpose of comparing their results in the test cases?",
        "answer": "`.fitCurve` and `.fitCurve2` are two different implementations for fitting dose-response curves. The test cases compare their results to ensure they produce equivalent outputs for both 2-parameter and 3-parameter Hill curve fittings. This comparison is crucial for validating that any refactoring or optimization in the curve fitting process (likely represented by `.fitCurve2`) maintains the same functionality as the original method (`.fitCurve`). The tests check for equality under various conditions, including different initial parameter guesses, normal and Cauchy distributions, and different parameter constraints."
      },
      {
        "question": "What is the significance of the `trunc` and `n` parameters in the loss function tests, and how are they varied in the test cases?",
        "answer": "The `trunc` and `n` parameters are important options in the loss function calculations. `trunc` likely refers to whether the loss function should use truncated distributions, while `n` might represent the number of replicates or a smoothing parameter. In the test cases, these parameters are systematically varied using boolean values for `trunc` (TRUE/FALSE) and different integer values for `n` (1 and 3). This variation ensures that the loss functions (`.residual`, `.normal_loss`, and `.cauchy_loss`) produce consistent results across different configurations, thus thoroughly testing the robustness of the implementation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "testthat::test_that(paste0(lmsg, \".residual and .normal_loss produce equal results.\"), {\n    trunc_vals <- c(FALSE, TRUE, FALSE, TRUE)\n    nvals <- c(1, 1, 3, 3)\n    for (i in seq_along(trunc_vals)) {\n        n1 <- .residual(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, f=fx,\n            family=\"normal\", n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        n2 <- CoreGx:::.normal_loss(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, fn=fx,\n            n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        # Complete the test expectation\n    }\n})",
        "complete": "testthat::test_that(paste0(lmsg, \".residual and .normal_loss produce equal results.\"), {\n    trunc_vals <- c(FALSE, TRUE, FALSE, TRUE)\n    nvals <- c(1, 1, 3, 3)\n    for (i in seq_along(trunc_vals)) {\n        n1 <- .residual(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, f=fx,\n            family=\"normal\", n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        n2 <- CoreGx:::.normal_loss(par=c(Emax=0.2, EC50=10), x=doses, y=nresponse, fn=fx,\n            n=nvals[i], trunc=trunc_vals[i], scale=0.07)\n        testthat::expect_equal(n1, n2,\n            info=paste0(\"trunc: \", trunc_vals[i], \", n: \", nvals[i])\n        )\n    }\n})"
      },
      {
        "partial": "testthat::test_that(paste0(cmsg, \".fitCurve and .fitCurve2 produce equal results for 3-parameter Hill curve.\"), {\n    par_list <- list(\n        c(Emax=0.1, EC50=0.1, lambda=1),\n        c(Emax=0.5, EC5O=500, lambda=0.75),\n        c(Emax=0.9, EC50=100, lambda=2)\n    )\n    fx <- make_optim_function(hillEqn, Emin=Emin)\n    for (i in seq_along(par_list)) {\n        pars <- par_list[[i]]\n        normal_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"normal\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses), 6),\n            lower_bound=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        normal_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.normal_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            upper=c(2, max(doses), 6),\n            lower=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        # Complete the test expectation and add Cauchy distribution test\n    }\n})",
        "complete": "testthat::test_that(paste0(cmsg, \".fitCurve and .fitCurve2 produce equal results for 3-parameter Hill curve.\"), {\n    par_list <- list(\n        c(Emax=0.1, EC50=0.1, lambda=1),\n        c(Emax=0.5, EC5O=500, lambda=0.75),\n        c(Emax=0.9, EC50=100, lambda=2)\n    )\n    fx <- make_optim_function(hillEqn, Emin=Emin)\n    for (i in seq_along(par_list)) {\n        pars <- par_list[[i]]\n        normal_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"normal\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses), 6),\n            lower_bound=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        normal_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.normal_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            upper=c(2, max(doses), 6),\n            lower=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        testthat::expect_equal(normal_par1, normal_par2,\n            info=paste0(\"Emax: \", pars[1], \", EC50: \", pars[2]))\n        cauchy_par1 <- .fitCurve(\n            gritty_guess=pars,\n            x=doses,\n            y=nresponse,\n            f=fx,\n            family=\"Cauchy\",\n            trunc=FALSE,\n            median_n=1,\n            scale=0.07,\n            upper_bound=c(2, max(doses), 6),\n            lower_bound=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        cauchy_par2 <-.fitCurve2(\n            par=pars,\n            x=doses,\n            y=nresponse,\n            fn=hillEqn,\n            loss=CoreGx:::.cauchy_loss,\n            loss_args=list(trunc=FALSE, n=1, scale=0.07),\n            Emin=Emin,\n            upper=c(2, max(doses), 6),\n            lower=c(0, min(doses), 0),\n            density=c(2, 10, 5),\n            precision=1e-4,\n            step=0.5 / c(2, 10, 5)\n        )\n        testthat::expect_equal(cauchy_par1, cauchy_par2,\n            info=paste0(\"Emax: \", pars[1], \", EC50: \", pars[2]))\n    }\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/mRMRe.git",
    "file": "../../../../repos/mRMRe/R/mRMRe.Filter.R",
    "language": "R",
    "content": "## Definition\n\nsetClass(\"mRMRe.Filter\", representation(filters = \"list\", scores = \"list\", \n    mi_matrix = \"matrix\", causality_list = \"list\", sample_names = \"character\", \n    feature_names = \"character\", target_indices = \"integer\", \n    fixed_feature_count = \"numeric\", levels = \"integer\"))\n\n## Wrappers\n\n`mRMR.ensemble` <- function(solution_count, feature_count, ...)\n{\n    return(new(\"mRMRe.Filter\", levels = c(solution_count, rep(1, feature_count - 1)), ...))\n}\n\n`mRMR.classic` <- function(feature_count, ...)\n{\n    return(new(\"mRMRe.Filter\", levels = rep(1, feature_count), ...))\n}\n\n## initialize\n\nsetMethod(\"initialize\", signature(\"mRMRe.Filter\"),\n        function(.Object, data, prior_weight, target_indices, levels, \n          method = c(\"exhaustive\", \"bootstrap\"), \n          continuous_estimator = c(\"pearson\", \"spearman\", \"kendall\", \"frequency\"), \n          fixed_feature_count = 0,\n          outX = TRUE,\n          bootstrap_count = 0)\n{\n    method <- match.arg(method)\n    continuous_estimator <- match.arg(continuous_estimator)\n    \n    if (class(data) != \"mRMRe.Data\")\n        stop(\"data must be of type mRMRe.Data\")\n    \n    ## Prior Processing\n    \n    if (length(priors(data)) != 0)\n    {\n        if (missing(prior_weight))\n            stop(\"prior weight must be provided if there are priors\")\n        else if  (prior_weight < 0 || prior_weight > 1)\n            stop(\"prior weight must be a value ranging from 0 to 1\")\n    }\n    else\n        prior_weight <- 0\n    \n    ## Target Processing\n\n    if (sum(sapply(target_indices, function(index) index < 1 || index > featureCount(data))) > 1)\n        stop(\"target_indices must only contain values ranging from 1 to the number of features in data\")\n    \n    ## Level Processing\n    \n    if (missing(levels))\n      stop(\"levels must be provided\")\n    \n            \n    ## Fixed selected feature processing\n    if (fixed_feature_count > length(levels))\n        stop(\"The number of fixed selected features can not be larger the length of solutions\")\n    \n    if (fixed_feature_count > 0)\n        length(levels) <- length(levels) - fixed_feature_count\n    \n\n    .Object@fixed_feature_count <- fixed_feature_count\n    \n    .Object@target_indices <- as.integer(c(target_indices))\n    .Object@levels <- as.integer(c(levels))\n    \n    target_indices <- as.integer(.expandFeatureIndices(data, target_indices)) - 1\n    \n    \n    \n    ## Filter; Mutual Information and Causality Matrix\n\n    mi_matrix <- as.numeric(matrix(NA, ncol = ncol(data@data), nrow = ncol(data@data)))\n    \n    \n\tif(method == \"exhaustive\"){\n\t  \n\t    ## Level Processing\n\t    if ((prod(levels) - 1) > choose(featureCount(data) - 1, length(levels)))\n\t      stop(\"user cannot request for more solutions than is possible given the data set\")\n    \n    \tresult <- .Call(.C_export_filters, as.integer(.Object@levels), as.numeric(data@data),\n        \t    as.numeric(data@priors), as.numeric(prior_weight), as.integer(data@strata), as.numeric(data@weights),\n            \tas.integer(data@feature_types), as.integer(nrow(data@data)), as.integer(ncol(data@data)),\n            \tas.integer(length(unique(data@strata))), as.integer(target_indices), as.integer(fixed_feature_count),\n            \tas.integer(.map.continuous.estimator(continuous_estimator)), as.integer(outX),\n            \tas.integer(bootstrap_count), mi_matrix)\n\t}\n\telse if(method == \"bootstrap\")\n\t\tresult <- .Call(.C_export_filters_bootstrap, as.integer(.Object@levels[1]), as.integer(length(.Object@levels)),\n\t\t\t\tas.numeric(data@data), as.numeric(data@priors), as.numeric(prior_weight), as.integer(data@strata),\n\t\t\t\tas.numeric(data@weights), as.integer(data@feature_types), as.integer(nrow(data@data)),\n\t\t\t\tas.integer(ncol(data@data)), as.integer(length(unique(data@strata))), as.integer(target_indices), as.integer(fixed_feature_count),\n\t\t\t\tas.integer(.map.continuous.estimator(continuous_estimator)), as.integer(outX),\n\t\t\t\tas.integer(bootstrap_count), mi_matrix)\n\telse\n\t\tstop(\"Unrecognized method: use exhaustive or bootstrap\")\n    \n    \n    \n    .Object@filters <- lapply(result[[1]], function(solutions) matrix(.compressFeatureIndices(data, solutions + 1),\n                        nrow = length(levels), ncol = prod(levels)))\n\t\n    names(.Object@filters) <- .Object@target_indices\n    .Object@causality_list <- result[[2]]\n\t.Object@scores <- lapply(result[[3]], function(scores) matrix(scores,\tnrow = length(levels), ncol = prod(levels)))\n\tnames(.Object@scores) <- .Object@target_indices\n    \n\tcols_to_drop <- duplicated(.compressFeatureIndices(data, seq(ncol(data@data))))\n    \n    .Object@causality_list <- lapply(result[[2]], function(causality_array) causality_array[!cols_to_drop])\n    names(.Object@causality_list) <- .Object@target_indices\n    \n    .Object@mi_matrix <- .compressFeatureMatrix(data, matrix(mi_matrix, ncol = ncol(data@data), nrow = ncol(data@data)))\n    .Object@feature_names <- featureNames(data)\n    .Object@sample_names <- sampleNames(data)\n\n    return(.Object)\n})\n\n## show\n\nsetMethod(\"show\", signature(\"mRMRe.Filter\"), function(object)\n{\n    str(object)\n})\n\n## sampleCount\n\nsetMethod(\"sampleCount\", signature(\"mRMRe.Filter\"), function(object)\n{\n    return(length(object@sample_names))\n})\n\n## sampleNames\n\nsetMethod(\"sampleNames\", signature(\"mRMRe.Filter\"), function(object)\n{\n    return(object@sample_names)\n})\n\n\n## featureCount\n\nsetMethod(\"featureCount\", signature(\"mRMRe.Filter\"), function(object)\n{\n    return(length(object@feature_names))\n})\n\n## featureNames\n\nsetMethod(\"featureNames\", signature(\"mRMRe.Filter\"), function(object)\n{\n    return(object@feature_names)\n})\n\n## solutions\n\nsetMethod(\"solutions\", signature(\"mRMRe.Filter\"), \n    function(object, mi_threshold = -Inf, causality_threshold = Inf, \n    with_fixed_features = TRUE)\n{\n    # filters[[target]][solution, ] is a vector of selected features\n    # in a solution for a target; missing values denote removed features\n            \n    filters <- lapply(object@target_indices, function(target_index)\n    {\n        result_matrix <- object@filters[[as.character(target_index)]]\n        causality_dropped <- which(object@causality_list[[as.character(target_index)]] > causality_threshold &\n                        !is.na(object@causality_list[[as.character(target_index)]]))\n        mi_dropped <- which(-.5 * log(1 - object@mi_matrix[, target_index, drop = TRUE]^2) < mi_threshold)\n        result_matrix[result_matrix %in% c(causality_dropped, mi_dropped)] <- NA\n\n        pre_return_matrix <- apply(as.matrix(result_matrix), 2, rev)\n\n        return(pre_return_matrix)\n    })\n    \n\n    # Concate the fixed selected features\n    if (object@fixed_feature_count > 0 && with_fixed_features)\n    {\n  \n        prefix <- matrix(seq(object@fixed_feature_count), nrow = object@fixed_feature_count, ncol = as.numeric(dim(filters[[1]])[2]), byrow = FALSE)\n        \n        filters <- lapply(seq(length(filters)), function(i)\n        {\n            filters[[i]] <- rbind(prefix, filters[[i]])\n            return(filters[[i]])\n        })\n        \n    }\n    \n    names(filters) <- object@target_indices\n    return(filters)\n})\n\n## scores\nsetMethod(\"scores\", signature(\"mRMRe.Filter\"), function(object)\n{\n\tmi_matrix <- mim(object)\n\ttargets <- as.character(target(object))\n\tscores <- lapply(targets, function(target) {\n\t\t\t\tapply(solutions(object)[[target]], 2, function(solution) {\n\t\t\t\t\t\t\tsapply(1:length(solution), function(i) {\n\t\t\t\t\t\t\t\t\t\tfeature_i <- solution[i] \n\t\t\t\t\t\t\t\t\t\tif(i == 1)\n\t\t\t\t\t\t\t\t\t\t\treturn(mi_matrix[as.numeric(target), feature_i])\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\tancestry_score <- mean(sapply((i-1):1, function(j) mi_matrix[feature_i, solution[j]]))\n\t\t\t\t\t\t\t\t\t\treturn(mi_matrix[as.numeric(target), feature_i] - ancestry_score)\n\t\t\t\t\t\t\t\t\t})\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t})\n\t\t\t})\n\tnames(scores) <- targets\n\treturn(scores)\n})\n\n## mim\n\nsetMethod(\"mim\", signature(\"mRMRe.Filter\"), function(object, method=c(\"mi\", \"cor\"))\n{\n    method <- match.arg(method)\n    # mi_matrix[i, j] contains the biased correlation between\n    # features i and j (i -> j directionality)\n    return(object@mi_matrix)\n})\n\n## causality\n\nsetMethod(\"causality\", signature(\"mRMRe.Filter\"), function(object)\n{\n    # causality_matrix[[target]][feature] contains the causality coefficient\n    # between feature and target (feature -> target directionality)\n    return(object@causality_list)\n})\n    \n## target\n\nsetMethod(\"target\", signature(\"mRMRe.Filter\"), function(object)\n{\n    return(object@target_indices)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mRMR.ensemble` and `mRMR.classic` functions in this code?",
        "answer": "The `mRMR.ensemble` and `mRMR.classic` functions are wrapper functions that create instances of the 'mRMRe.Filter' class with specific configurations. `mRMR.ensemble` creates a filter with multiple solutions (ensemble) by setting different levels for feature selection, while `mRMR.classic` creates a filter with a single solution by setting all levels to 1. Both functions simplify the process of creating 'mRMRe.Filter' objects with predefined parameters."
      },
      {
        "question": "How does the `initialize` method handle the processing of fixed selected features?",
        "answer": "The `initialize` method processes fixed selected features as follows:\n1. It checks if the number of fixed features (`fixed_feature_count`) is greater than the length of the `levels` parameter. If so, it throws an error.\n2. If `fixed_feature_count` is greater than 0, it reduces the length of the `levels` vector by subtracting `fixed_feature_count`.\n3. It stores the `fixed_feature_count` in the object's `@fixed_feature_count` slot.\n4. Later in the method, it passes the `fixed_feature_count` to the C++ function call that performs the actual filtering.\n5. In the `solutions` method, if `fixed_feature_count` is greater than 0 and `with_fixed_features` is TRUE, it prepends the fixed features to the solution matrices."
      },
      {
        "question": "Explain the purpose and functionality of the `solutions` method in the 'mRMRe.Filter' class.",
        "answer": "The `solutions` method in the 'mRMRe.Filter' class serves to retrieve and process the feature selection solutions. Its main functionalities are:\n1. It extracts solutions for each target index from the `@filters` slot.\n2. It applies thresholds for mutual information and causality, removing features that don't meet these criteria.\n3. It reverses the order of features in each solution.\n4. If there are fixed selected features and `with_fixed_features` is TRUE, it prepends these features to the solutions.\n5. It returns a list of matrices, where each matrix represents the selected features for a target, with columns corresponding to different solutions and rows representing the selected features in order of importance.\nThis method allows users to access the feature selection results with optional filtering and inclusion of fixed features."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"initialize\", signature(\"mRMRe.Filter\"),\n        function(.Object, data, prior_weight, target_indices, levels, \n          method = c(\"exhaustive\", \"bootstrap\"), \n          continuous_estimator = c(\"pearson\", \"spearman\", \"kendall\", \"frequency\"), \n          fixed_feature_count = 0,\n          outX = TRUE,\n          bootstrap_count = 0)\n{\n    method <- match.arg(method)\n    continuous_estimator <- match.arg(continuous_estimator)\n    \n    if (class(data) != \"mRMRe.Data\")\n        stop(\"data must be of type mRMRe.Data\")\n    \n    ## Prior Processing\n    \n    if (length(priors(data)) != 0)\n    {\n        if (missing(prior_weight))\n            stop(\"prior weight must be provided if there are priors\")\n        else if  (prior_weight < 0 || prior_weight > 1)\n            stop(\"prior weight must be a value ranging from 0 to 1\")\n    }\n    else\n        prior_weight <- 0\n    \n    ## Target Processing\n\n    if (sum(sapply(target_indices, function(index) index < 1 || index > featureCount(data))) > 1)\n        stop(\"target_indices must only contain values ranging from 1 to the number of features in data\")\n    \n    ## Level Processing\n    \n    if (missing(levels))\n      stop(\"levels must be provided\")\n    \n            \n    ## Fixed selected feature processing\n    if (fixed_feature_count > length(levels))\n        stop(\"The number of fixed selected features can not be larger the length of solutions\")\n    \n    if (fixed_feature_count > 0)\n        length(levels) <- length(levels) - fixed_feature_count\n    \n\n    .Object@fixed_feature_count <- fixed_feature_count\n    \n    .Object@target_indices <- as.integer(c(target_indices))\n    .Object@levels <- as.integer(c(levels))\n    \n    target_indices <- as.integer(.expandFeatureIndices(data, target_indices)) - 1\n    \n    \n    \n    ## Filter; Mutual Information and Causality Matrix\n\n    mi_matrix <- as.numeric(matrix(NA, ncol = ncol(data@data), nrow = ncol(data@data)))\n    \n    \n\tif(method == \"exhaustive\"){\n\t  \n\t    ## Level Processing\n\t    if ((prod(levels) - 1) > choose(featureCount(data) - 1, length(levels)))\n\t      stop(\"user cannot request for more solutions than is possible given the data set\")\n    \n    \tresult <- .Call(.C_export_filters, as.integer(.Object@levels), as.numeric(data@data),\n        \t    as.numeric(data@priors), as.numeric(prior_weight), as.integer(data@strata), as.numeric(data@weights),\n            \tas.integer(data@feature_types), as.integer(nrow(data@data)), as.integer(ncol(data@data)),\n            \tas.integer(length(unique(data@strata))), as.integer(target_indices), as.integer(fixed_feature_count),\n            \tas.integer(.map.continuous.estimator(continuous_estimator)), as.integer(outX),\n            \tas.integer(bootstrap_count), mi_matrix)\n\t}\n\telse if(method == \"bootstrap\")\n\t\tresult <- .Call(.C_export_filters_bootstrap, as.integer(.Object@levels[1]), as.integer(length(.Object@levels)),\n\t\t\t\tas.numeric(data@data), as.numeric(data@priors), as.numeric(prior_weight), as.integer(data@strata),\n\t\t\t\tas.numeric(data@weights), as.integer(data@feature_types), as.integer(nrow(data@data)),\n\t\t\t\tas.integer(ncol(data@data)), as.integer(length(unique(data@strata))), as.integer(target_indices), as.integer(fixed_feature_count),\n\t\t\t\tas.integer(.map.continuous.estimator(continuous_estimator)), as.integer(outX),\n\t\t\t\tas.integer(bootstrap_count), mi_matrix)\n\telse\n\t\tstop(\"Unrecognized method: use exhaustive or bootstrap\")\n    \n    \n    \n    .Object@filters <- lapply(result[[1]], function(solutions) matrix(.compressFeatureIndices(data, solutions + 1),\n                        nrow = length(levels), ncol = prod(levels)))\n\t\n    names(.Object@filters) <- .Object@target_indices\n    .Object@causality_list <- result[[2]]\n\t.Object@scores <- lapply(result[[3]], function(scores) matrix(scores,\tnrow = length(levels), ncol = prod(levels)))\n\tnames(.Object@scores) <- .Object@target_indices\n    \n\tcols_to_drop <- duplicated(.compressFeatureIndices(data, seq(ncol(data@data))))\n    \n    .Object@causality_list <- lapply(result[[2]], function(causality_array) causality_array[!cols_to_drop])\n    names(.Object@causality_list) <- .Object@target_indices\n    \n    .Object@mi_matrix <- .compressFeatureMatrix(data, matrix(mi_matrix, ncol = ncol(data@data), nrow = ncol(data@data)))\n    .Object@feature_names <- featureNames(data)\n    .Object@sample_names <- sampleNames(data)\n\n    return(.Object)\n})",
        "complete": "setMethod(\"initialize\", signature(\"mRMRe.Filter\"),\n        function(.Object, data, prior_weight, target_indices, levels, \n          method = c(\"exhaustive\", \"bootstrap\"), \n          continuous_estimator = c(\"pearson\", \"spearman\", \"kendall\", \"frequency\"), \n          fixed_feature_count = 0,\n          outX = TRUE,\n          bootstrap_count = 0)\n{\n    method <- match.arg(method)\n    continuous_estimator <- match.arg(continuous_estimator)\n    \n    if (class(data) != \"mRMRe.Data\")\n        stop(\"data must be of type mRMRe.Data\")\n    \n    if (length(priors(data)) != 0) {\n        if (missing(prior_weight))\n            stop(\"prior weight must be provided if there are priors\")\n        else if (prior_weight < 0 || prior_weight > 1)\n            stop(\"prior weight must be a value ranging from 0 to 1\")\n    } else prior_weight <- 0\n    \n    if (sum(sapply(target_indices, function(index) index < 1 || index > featureCount(data))) > 1)\n        stop(\"target_indices must only contain values ranging from 1 to the number of features in data\")\n    \n    if (missing(levels))\n      stop(\"levels must be provided\")\n    \n    if (fixed_feature_count > length(levels))\n        stop(\"The number of fixed selected features can not be larger the length of solutions\")\n    \n    if (fixed_feature_count > 0)\n        length(levels) <- length(levels) - fixed_feature_count\n    \n    .Object@fixed_feature_count <- fixed_feature_count\n    .Object@target_indices <- as.integer(c(target_indices))\n    .Object@levels <- as.integer(c(levels))\n    \n    target_indices <- as.integer(.expandFeatureIndices(data, target_indices)) - 1\n    \n    mi_matrix <- as.numeric(matrix(NA, ncol = ncol(data@data), nrow = ncol(data@data)))\n    \n    if (method == \"exhaustive\") {\n        if ((prod(levels) - 1) > choose(featureCount(data) - 1, length(levels)))\n          stop(\"user cannot request for more solutions than is possible given the data set\")\n        \n        result <- .Call(.C_export_filters, as.integer(.Object@levels), as.numeric(data@data),\n                as.numeric(data@priors), as.numeric(prior_weight), as.integer(data@strata), as.numeric(data@weights),\n                as.integer(data@feature_types), as.integer(nrow(data@data)), as.integer(ncol(data@data)),\n                as.integer(length(unique(data@strata))), as.integer(target_indices), as.integer(fixed_feature_count),\n                as.integer(.map.continuous.estimator(continuous_estimator)), as.integer(outX),\n                as.integer(bootstrap_count), mi_matrix)\n    } else if (method == \"bootstrap\") {\n        result <- .Call(.C_export_filters_bootstrap, as.integer(.Object@levels[1]), as.integer(length(.Object@levels)),\n                as.numeric(data@data), as.numeric(data@priors), as.numeric(prior_weight), as.integer(data@strata),\n                as.numeric(data@weights), as.integer(data@feature_types), as.integer(nrow(data@data)),\n                as.integer(ncol(data@data)), as.integer(length(unique(data@strata))), as.integer(target_indices), as.integer(fixed_feature_count),\n                as.integer(.map.continuous.estimator(continuous_estimator)), as.integer(outX),\n                as.integer(bootstrap_count), mi_matrix)\n    } else stop(\"Unrecognized method: use exhaustive or bootstrap\")\n    \n    .Object@filters <- lapply(result[[1]], function(solutions) matrix(.compressFeatureIndices(data, solutions + 1),\n                        nrow = length(levels), ncol = prod(levels)))\n    names(.Object@filters) <- .Object@target_indices\n    .Object@causality_list <- result[[2]]\n    .Object@scores <- lapply(result[[3]], function(scores) matrix(scores, nrow = length(levels), ncol = prod(levels)))\n    names(.Object@scores) <- .Object@target_indices\n    \n    cols_to_drop <- duplicated(.compressFeatureIndices(data, seq(ncol(data@data))))\n    \n    .Object@causality_list <- lapply(result[[2]], function(causality_array) causality_array[!cols_to_drop])\n    names(.Object@causality_list) <- .Object@target_indices\n    \n    .Object@mi_matrix <- .compressFeatureMatrix(data, matrix(mi_matrix, ncol = ncol(data@data), nrow = ncol(data@data)))\n    .Object@feature_names <- featureNames(data)\n    .Object@sample_names <- sampleNames(data)\n\n    return(.Object)\n})"
      },
      {
        "partial": "setMethod(\"solutions\", signature(\"mRMRe.Filter\"), \n    function(object, mi_threshold = -Inf, causality_threshold = Inf, \n    with_fixed_features = TRUE)\n{\n    filters <- lapply(object@target_indices, function(target_index)\n    {\n        result_matrix <- object@filters[[as.character(target_index)]]\n        causality_dropped <- which(object@causality_list[[as.character(target_index)]] > causality_threshold &\n                        !is.na(object@causality_list[[as.character(target_index)]]))\n        mi_dropped <- which(-.5 * log(1 - object@mi_matrix[, target_index, drop = TRUE]^2) < mi_threshold)\n        result_matrix[result_matrix %in% c(causality_dropped, mi_dropped)] <- NA\n\n        pre_return_matrix <- apply(as.matrix(result_matrix), 2, rev)\n\n        return(pre_return_matrix)\n    })\n    \n\n    if (object@fixed_feature_count > 0 && with_fixed_features)\n    {\n  \n        prefix <- matrix(seq(object@fixed_feature_count), nrow = object@fixed_feature_count, ncol = as.numeric(dim(filters[[1]])[2]), byrow = FALSE)\n        \n        filters <- lapply(seq(length(filters)), function(i)\n        {\n            filters[[i]] <- rbind(prefix, filters[[i]])\n            return(filters[[i]])\n        })\n        \n    }\n    \n    names(filters) <- object@target_indices\n    return(filters)\n})",
        "complete": "setMethod(\"solutions\", signature(\"mRMRe.Filter\"), \n    function(object, mi_threshold = -Inf, causality_threshold = Inf, \n    with_fixed_features = TRUE)\n{\n    filters <- lapply(object@target_indices, function(target_index) {\n        result_matrix <- object@filters[[as.character(target_index)]]\n        causality_dropped <- which(object@causality_list[[as.character(target_index)]] > causality_threshold &\n                        !is.na(object@causality_list[[as.character(target_index)]]))\n        mi_dropped <-"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/SanityCheck.R",
    "language": "R",
    "content": "sanitizeInput <- function(conc,\n                          viability,\n                          Hill_fit,\n                          conc_as_log = FALSE,\n                          viability_as_pct = TRUE,\n                          trunc = TRUE,\n                          verbose = TRUE) # Set to 2 to see debug messageouts\n  {\n\n  if (is.logical(conc_as_log) == FALSE) {\n    message(conc_as_log)\n    stop(\"'conc_as_log' is not a logical.\")\n  }\n\n  if (is.logical(viability_as_pct) == FALSE) {\n    message(viability_as_pct)\n    stop(\"'viability_as_pct' is not a logical.\")\n  }\n\n  if (is.logical(trunc) == FALSE) {\n    message(trunc)\n    stop(\"'trunc' is not a logical.\")\n  }\n  if(!is.finite(verbose)){\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  if(!missing(viability)&&!missing(conc)&&missing(Hill_fit))\n  {\n    if (length(conc) != length(viability)) {\n      if(verbose==2){\n        message(conc)\n        message(viability)\n      }\n      stop(\"Log concentration vector is not of same length as viability vector.\")\n    }\n    if( any(is.na(conc)&(!is.na(viability)))){\n      warning(\"Missing concentrations with non-missing viability values encountered. Removing viability values correspoding to those concentrations\")\n\n      myx <- !is.na(conc)\n      conc <- as.numeric(conc[myx])\n      viability <- as.numeric(viability[myx])\n\n    }\n    if(any((!is.na(conc))&is.na(viability))){\n\n      warning(\"Missing viability with non-missing concentrations values encountered. Removing concentrations values correspoding to those viabilities\")\n      myx <- !is.na(viability)\n      conc <- as.numeric(conc[myx])\n      viability <- as.numeric(viability[myx])\n\n    }\n    conc <- as.numeric(conc[!is.na(conc)])\n    viability <- as.numeric(viability[!is.na(viability)])\n\n    #CHECK THAT FUNCTION INPUTS ARE APPROPRIATE\n    if (prod(is.finite(conc)) != 1) {\n      message(conc)\n      stop(\"Concentration vector contains elements which are not real numbers.\")\n    }\n\n    if (prod(is.finite(viability)) != 1) {\n      message(viability)\n      stop(\"Viability vector contains elements which are not real numbers.\")\n    }\n\n\n    if (min(viability) < 0) {\n      if (verbose) {\n\n        warning(\"Warning: Negative viability data.\")\n      }\n    }\n\n    if (max(viability) > (1 + 99 * viability_as_pct)) {\n      if (verbose) {\n        warning(\"Warning: Viability data exceeds negative control.\")\n      }\n    }\n\n\n    if (conc_as_log == FALSE && min(conc) < 0) {\n      if (verbose == 2) {\n        message(conc)\n        message(conc_as_log)\n      }\n      stop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n    }\n\n    if (viability_as_pct == TRUE && max(viability) < 5) {\n      warning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n      if (verbose == 2) {\n\n        message(viability)\n        message(viability_as_pct)\n      }\n    }\n\n    if (viability_as_pct == FALSE && max(viability) > 5) {\n      warning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n      if (verbose == 2) {\n        message(viability)\n        message(viability_as_pct)\n      }\n    }\n\n    if(is.unsorted(conc)){\n      warning(\"Concentration Values were unsorted. Sorting concentration and ordering viability in same order\")\n      myx <- order(conc)\n      conc <- conc[myx]\n      viability <- viability[myx]\n    }\n\n    #CONVERT DOSE-RESPONSE DATA TO APPROPRIATE INTERNAL REPRESENTATION\n    if (conc_as_log == FALSE ) {\n      ii <- which(conc == 0)\n      if(length(ii) > 0) {\n        conc <- conc[-ii]\n        viability <- viability[-ii]\n      }\n\n      log_conc <- log10(conc)\n    } else {\n      log_conc <- conc\n    }\n\n    if (viability_as_pct == TRUE) {\n      viability <- viability / 100\n    }\n    if (trunc) {\n      viability = pmin(as.numeric(viability), 1)\n      viability = pmax(as.numeric(viability), 0)\n    }\n\n    return(list(\"log_conc\"=log_conc, \"viability\"=viability))\n  }\n  if(!missing(Hill_fit) && missing(viability)){\n    if(is.list(Hill_fit)){\n\n      Hill_fit <- unlist(Hill_fit)\n    }\n    if (conc_as_log == FALSE && Hill_fit[[3]] < 0) {\n      message(\"EC50 passed in as:\")\n      message(Hill_fit[[3]])\n      stop(\"'conc_as_log' flag may be set incorrectly, as the EC50 is negative when positive value is expected.\")\n    }\n\n\n    if (viability_as_pct == FALSE && Hill_fit[[2]] > 1) {\n      message(\"Einf passed in as:\")\n      message(Hill_fit[[2]])\n\n      warning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\n    }\n    if (conc_as_log == FALSE){\n      Hill_fit[[3]] <- log10(Hill_fit[[3]])\n    }\n    if (viability_as_pct == TRUE){\n      Hill_fit[[2]] <- Hill_fit[[2]]/100\n    }\n    if(missing(conc)){\n      return(list(\"Hill_fit\"=Hill_fit))\n    } else {\n      conc <- as.numeric(conc[!is.na(conc)])\n\n      if (prod(is.finite(conc)) != 1) {\n        message(conc)\n        stop(\"Concentration vector contains elements which are not real numbers.\")\n      }\n      if (conc_as_log == FALSE && min(conc) < 0) {\n        message(conc)\n        message(conc_as_log)\n        stop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n      }\n\n      if (conc_as_log == FALSE ) {\n        ii <- which(conc == 0)\n        if(length(ii) > 0) {\n          conc <- conc[-ii]\n        }\n        log_conc <- log10(conc)\n      } else {\n        log_conc <- conc\n      }\n      if(is.unsorted(conc)){\n        myx <- order(conc)\n        conc <- conc[myx]\n      }\n      return(list(\"Hill_fit\"=Hill_fit, \"log_conc\" = log_conc))\n    }\n\n\n  }\n  if(!missing(Hill_fit)&&!missing(viability)){\n\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n  }\n  if(missing(Hill_fit)&&missing(viability)){\n\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'sanitizeInput' function in this R code, and what are its main input parameters?",
        "answer": "The 'sanitizeInput' function is designed to validate and preprocess input data for dose-response analysis. Its main input parameters are:\n1. conc: concentration values\n2. viability: corresponding viability values\n3. Hill_fit: parameters for Hill equation fitting\n4. conc_as_log: boolean indicating if concentrations are in log scale\n5. viability_as_pct: boolean indicating if viability is in percentage\n6. trunc: boolean to truncate viability values between 0 and 1\n7. verbose: controls the level of warning messages"
      },
      {
        "question": "How does the function handle missing or inconsistent data in the concentration and viability vectors?",
        "answer": "The function handles missing or inconsistent data as follows:\n1. It checks if the lengths of conc and viability vectors are equal.\n2. It removes pairs where either concentration or viability is NA.\n3. It converts both vectors to numeric and removes any remaining NA values.\n4. It sorts the concentration values and reorders the viability values accordingly.\n5. It removes concentration values of 0 when conc_as_log is FALSE.\n6. It issues warnings for negative viability data or viability exceeding control levels."
      },
      {
        "question": "What data transformations does the function perform on the input concentration and viability data before returning the result?",
        "answer": "The function performs the following transformations:\n1. If conc_as_log is FALSE, it converts concentrations to log10 scale.\n2. If viability_as_pct is TRUE, it divides viability values by 100 to convert from percentage to fraction.\n3. If trunc is TRUE, it truncates viability values to be between 0 and 1.\n4. It returns a list containing 'log_conc' (log-transformed concentrations) and 'viability' (processed viability values).\n5. When Hill_fit is provided instead of raw data, it adjusts the EC50 and Einf parameters according to the conc_as_log and viability_as_pct flags."
      }
    ],
    "completion_tasks": [
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"'conc_as_log', 'viability_as_pct', and 'trunc' must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  # Complete the function to handle the case when viability and conc are provided\n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Add code here\n  }\n  \n  # Complete the function to handle the case when Hill_fit is provided\n  if (!missing(Hill_fit) && missing(viability)) {\n    # Add code here\n  }\n  \n  # Handle error cases\n  if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n  }\n  if (missing(Hill_fit) && missing(viability)) {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"'conc_as_log', 'viability_as_pct', and 'trunc' must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    if (length(conc) != length(viability)) {\n      stop(\"Log concentration vector is not of same length as viability vector.\")\n    }\n    myx <- !is.na(conc) & !is.na(viability)\n    conc <- as.numeric(conc[myx])\n    viability <- as.numeric(viability[myx])\n    \n    if (!all(is.finite(conc)) || !all(is.finite(viability))) {\n      stop(\"Concentration or viability vector contains non-finite elements.\")\n    }\n    \n    if (min(viability) < 0 || max(viability) > (1 + 99 * viability_as_pct)) {\n      warning(\"Viability data out of expected range.\")\n    }\n    \n    if (!conc_as_log && min(conc) < 0) {\n      stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    }\n    \n    if ((viability_as_pct && max(viability) < 5) || (!viability_as_pct && max(viability) > 5)) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    if (is.unsorted(conc)) {\n      myx <- order(conc)\n      conc <- conc[myx]\n      viability <- viability[myx]\n    }\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    return(list(log_conc = log_conc, viability = viability))\n  }\n  \n  if (!missing(Hill_fit) && missing(viability)) {\n    Hill_fit <- if (is.list(Hill_fit)) unlist(Hill_fit) else Hill_fit\n    \n    if (!conc_as_log && Hill_fit[3] < 0) {\n      stop(\"'conc_as_log' flag may be set incorrectly, as the EC50 is negative when positive value is expected.\")\n    }\n    \n    if (!viability_as_pct && Hill_fit[2] > 1) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    Hill_fit[3] <- if (conc_as_log) Hill_fit[3] else log10(Hill_fit[3])\n    Hill_fit[2] <- if (viability_as_pct) Hill_fit[2] / 100 else Hill_fit[2]\n    \n    if (missing(conc)) return(list(Hill_fit = Hill_fit))\n    \n    conc <- as.numeric(conc[!is.na(conc)])\n    if (!all(is.finite(conc))) stop(\"Concentration vector contains non-finite elements.\")\n    if (!conc_as_log && min(conc) < 0) stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    if (is.unsorted(conc)) log_conc <- log_conc[order(conc)]\n    \n    return(list(Hill_fit = Hill_fit, log_conc = log_conc))\n  }\n  \n  if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n  }\n  if (missing(Hill_fit) && missing(viability)) {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  # Add input validation here\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Process viability and concentration data\n    # Add code here\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill fit data\n    # Add code here\n  } else if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability'.\")\n  } else {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!all(sapply(list(conc_as_log, viability_as_pct, trunc), is.logical))) {\n    stop(\"'conc_as_log', 'viability_as_pct', and 'trunc' must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical or numerical argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    if (length(conc) != length(viability)) {\n      stop(\"Concentration and viability vectors must have the same length.\")\n    }\n    valid_data <- !is.na(conc) & !is.na(viability)\n    conc <- as.numeric(conc[valid_data])\n    viability <- as.numeric(viability[valid_data])\n    \n    if (!all(is.finite(c(conc, viability)))) {\n      stop(\"Concentration or viability vector contains non-finite elements.\")\n    }\n    \n    if (min(viability) < 0 || max(viability) > (1 + 99 * viability_as_pct)) {\n      warning(\"Viability data out of expected range.\")\n    }\n    \n    if (!conc_as_log && min(conc) < 0) {\n      stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    }\n    \n    if ((viability_as_pct && max(viability) < 5) || (!viability_as_pct && max(viability) > 5)) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    if (is.unsorted(conc)) {\n      order <- order(conc)\n      conc <- conc[order]\n      viability <- viability[order]\n    }\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    return(list(log_conc = log_conc, viability = viability))\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    Hill_fit <- if (is.list(Hill_fit)) unlist(Hill_fit) else Hill_fit\n    \n    if (!conc_as_log && Hill_fit[3] < 0) {\n      stop(\"EC50 is negative when positive value is expected. Check 'conc_as_log' flag.\")\n    }\n    \n    if (!viability_as_pct && Hill_fit[2] > 1) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    Hill_fit[3] <- if (conc_as_log) Hill_fit[3] else log10(Hill_fit[3])\n    Hill_fit[2] <- if (viability_as_pct) Hill_fit[2] / 100 else Hill_fit[2]\n    \n    if (missing(conc)) return(list(Hill_fit = Hill_fit))\n    \n    conc <- as.numeric(conc[!is.na(conc)])\n    if (!all(is.finite(conc))) stop(\"Concentration vector contains non-finite elements.\")\n    if (!conc_as_log && min(conc) < 0) stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    if (is.unsorted(conc)) log_conc <- log_conc[order(conc)]\n    \n    return(list(Hill_fit = Hill_fit, log_conc = log_conc))\n  } else if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability'.\")\n  } else {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-summarizeMolecularProfiles.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(ToxicoGx)\n\n## TODO:: Can probably rewrite this using an apply function?\n\ncontext(\"Testing if summarizeMolecularProfiles error handling works correctly...\")\n\n# tSet\ncontext(\"...Checking for correct tSet param errors...\")\ntest_that(\"Errors if given more than one tSet as parameter.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            c(TGGATESsmall, TGGATESsmall), mDataType=\"rna\",\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        ))\n})\n\n# mDataTypes\ncontext(\"...Checking for correct mDataType param errors...\")\ntest_that(\"Warning if given more than one mDataType.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=c(\"rna\", \"cnv\"),\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall, \"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=FALSE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if given mDataType as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=1, cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Low\", \"Medium\"), summary.stat=\"mean\",\n            fill.missing=FALSE, verbose=FALSE\n        )\n    )\n})\ntest_that(\"Errors if specified mDataType is not in the tSet.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"cnv\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"first\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# cell_lines\ncontext(\"...Checking for correct cell_lines param errors...\")\ntest_that(\"Errors if given cell_lines as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=5,\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if specified cell_lines are not in the tSet\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines='NOTINtSET',\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# drugs\ncontext(\"...Checking for correct drugs param errors...\")\ntest_that(\"Errors if given drugs are type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=5, features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if specified drugs are not in the tSet.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=\"NOTINtSET\", features=fNames(TGGATESsmall,\"rna\"),\n            duration=\"8\", dose=c(\"ontrol\", \"High\"), summary.stat=\"first\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# features\ncontext(\"...Checking for correct features param errors....\")\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# duration\ncontext(\"...Checking for correct duration param errors\")\ntest_that(\"Errors if given duration as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=8,\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"NOTINtSET\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"first\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# dose\ncontext(\"...Checking for correct dose param errors\")\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(1, 2), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if specified doses are not in the tSet\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=\"NOTINTtSET\", summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeMolecularProfiles` function in this code, and what are some of the key parameters it checks for?",
        "answer": "The `summarizeMolecularProfiles` function appears to be a data analysis tool for summarizing molecular profiles in toxicogenomics studies. It checks for several key parameters including: 'tSet' (the dataset), 'mDataType' (type of molecular data), 'cell_lines', 'drugs', 'features', 'duration', 'dose', and 'summary.stat'. The function performs extensive error checking on these parameters to ensure they are of the correct type and contain valid values present in the dataset."
      },
      {
        "question": "How does the code handle error checking for the 'mDataType' parameter in the `summarizeMolecularProfiles` function?",
        "answer": "The code performs three checks for the 'mDataType' parameter: 1) It raises an error if more than one mDataType is provided. 2) It checks if the mDataType is of character type, raising an error if it's not. 3) It verifies if the specified mDataType exists in the tSet (dataset), raising an error if it doesn't. These checks ensure that the mDataType parameter is correctly specified and compatible with the given dataset."
      },
      {
        "question": "What testing framework is used in this code, and how are the tests structured?",
        "answer": "This code uses the 'testthat' library for unit testing in R. Tests are structured using 'context()' to group related tests, and 'test_that()' to define individual test cases. Each test case uses 'expect_error()' to verify that the `summarizeMolecularProfiles` function raises appropriate errors when given invalid inputs. The tests are organized by parameter, with multiple test cases for each parameter to cover different error scenarios."
      }
    ],
    "completion_tasks": [
      {
        "partial": "context(\"Testing if summarizeMolecularProfiles error handling works correctly...\")\n\ntest_that(\"Errors if given more than one tSet as parameter.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            c(TGGATESsmall, TGGATESsmall), mDataType=\"rna\",\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        ))\n})\n\ntest_that(\"Warning if given more than one mDataType.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=c(\"rna\", \"cnv\"),\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall, \"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=FALSE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given mDataType as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=1, cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Low\", \"Medium\"), summary.stat=\"mean\",\n            fill.missing=FALSE, verbose=FALSE\n        )\n    )\n})\n\n# Complete the test cases for cell_lines, drugs, and features",
        "complete": "context(\"Testing if summarizeMolecularProfiles error handling works correctly...\")\n\ntest_that(\"Errors if given more than one tSet as parameter.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            c(TGGATESsmall, TGGATESsmall), mDataType=\"rna\",\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        ))\n})\n\ntest_that(\"Warning if given more than one mDataType.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=c(\"rna\", \"cnv\"),\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall, \"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=FALSE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given mDataType as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=1, cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Low\", \"Medium\"), summary.stat=\"mean\",\n            fill.missing=FALSE, verbose=FALSE\n        )\n    )\n})\n\ntest_that(\"Errors if given cell_lines as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=5,\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given drugs are type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=5, features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})"
      },
      {
        "partial": "context(\"...Checking for correct duration and dose param errors\")\n\ntest_that(\"Errors if given duration as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=8,\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# Complete the test case for dose parameter",
        "complete": "context(\"...Checking for correct duration and dose param errors\")\n\ntest_that(\"Errors if given duration as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=8,\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given dose as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(1, 2), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/power.cor.R",
    "language": "R",
    "content": "#' @title Function for sample size calculation for correlation coefficients\n#'\n#' @description\n#' This function enables to compute the sample size requirements for estimating \n#'   pearson, kendall and spearman correlations\n#'\n#' @usage\n#' power.cor(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\"))\n#'\n#' @param rho\tCorrealtion coefficients rho (Pearson, Kendall or Spearman)\n#' @param w\ta numerical vector of weights of the same length as x giving the weights to \n#'   use for elements of x in the first class.\n#' @param alpha\talpha level\n#' @param method\ta character string specifying the method to compute the correlation \n#'   coefficient, must be one of \"pearson\" (default), \"kendall\" or \"spearman\". You can \n#'   specify just the initial letter.\n#'\n#' @return\n#' sample size requirement\n#'\n#' @references\n#' Bonett, D. G., and Wright, T. A. (2000). Sample size requirements for estimating \n#'   pearson, kendall and spearman correlations. Psychometrika, 65(1), \n#'   23-28. doi:10.1007/BF02294183\n#'\n#' @examples\n#' power.cor(rho=0.5, w=0.1, alpha=0.05, method=\"spearman\")\n#'\n#' @md\n#' @export\n#' \n## sample size calculation for correlation coefficients (Pearson, kendall and SPearman)\n## example: power.cor(rho=0.5, w=0.1, alpha=0.05, method=\"spearman\")\npower.cor <- \nfunction (rho, w, alpha=0.05, method=c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(3, 4, 3)\n  cc <- c(1, sqrt(0.437), sqrt(1 + (rho^2 / 2)))\n  names(bb) <- names(cc) <- c(\"pearson\", \"kendall\", \"spearman\")\n  bb <- bb[method]\n  cc <- cc[method]\n  nn0 <- 4 * cc^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb\n  if(nn0 < 10) { nn0t <- 10 } else { nn0t <- ceiling(nn0) }\n  w0w <- sqrt(nn0t - bb) / sqrt(nn0 - bb)\n  nn <- ceiling((nn0 - bb) * w0w^2 + bb)\n  return(nn)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `power.cor` function and what are its main parameters?",
        "answer": "The `power.cor` function is used to calculate the sample size requirements for estimating correlation coefficients (Pearson, Kendall, or Spearman). Its main parameters are: `rho` (correlation coefficient), `w` (width of confidence interval), `alpha` (significance level), and `method` (type of correlation coefficient to use)."
      },
      {
        "question": "How does the function handle different correlation methods, and what is the significance of the `bb` and `cc` vectors?",
        "answer": "The function uses `match.arg(method)` to ensure a valid correlation method is selected. The `bb` and `cc` vectors contain method-specific constants used in the sample size calculation. `bb` represents degrees of freedom adjustments, while `cc` contains scaling factors for each method. These values are selected based on the chosen correlation method."
      },
      {
        "question": "Explain the purpose of the conditional statement `if(nn0 < 10) { nn0t <- 10 } else { nn0t <- ceiling(nn0) }` in the function.",
        "answer": "This conditional statement ensures that the initial sample size estimate (`nn0t`) is at least 10. If the calculated `nn0` is less than 10, it sets `nn0t` to 10. Otherwise, it rounds up `nn0` to the nearest integer. This step helps to maintain a minimum sample size for reliable correlation estimation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(3, 4, 3)\n  cc <- c(1, sqrt(0.437), sqrt(1 + (rho^2 / 2)))\n  names(bb) <- names(cc) <- c(\"pearson\", \"kendall\", \"spearman\")\n  bb <- bb[method]\n  cc <- cc[method]\n  nn0 <- 4 * cc^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb\n  # Complete the function from here\n}",
        "complete": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(3, 4, 3)\n  cc <- c(1, sqrt(0.437), sqrt(1 + (rho^2 / 2)))\n  names(bb) <- names(cc) <- c(\"pearson\", \"kendall\", \"spearman\")\n  bb <- bb[method]\n  cc <- cc[method]\n  nn0 <- 4 * cc^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb\n  nn0t <- max(10, ceiling(nn0))\n  w0w <- sqrt(nn0t - bb) / sqrt(nn0 - bb)\n  nn <- ceiling((nn0 - bb) * w0w^2 + bb)\n  return(nn)\n}"
      },
      {
        "partial": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  # Implement the function body here\n}",
        "complete": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(pearson = 3, kendall = 4, spearman = 3)\n  cc <- c(pearson = 1, kendall = sqrt(0.437), spearman = sqrt(1 + (rho^2 / 2)))\n  nn0 <- 4 * cc[method]^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb[method]\n  nn0t <- max(10, ceiling(nn0))\n  w0w <- sqrt(nn0t - bb[method]) / sqrt(nn0 - bb[method])\n  nn <- ceiling((nn0 - bb[method]) * w0w^2 + bb[method])\n  return(nn)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcTCGA.R",
    "language": "R",
    "content": "#' @title Function to compute the prediction scores and risk classifications\n#'   for the ovarian cancer TCGA signature\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene\n#'   expression values following the algorithm developed by the TCGA consortium\n#'   for ovarian cancer.\n#'\n#' @usage\n#' ovcTCGA(data, annot,\n#'   gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"),\n#'   do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param gmap\tcharacter string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score:\tContinuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe:\tIf mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Bell D, Berchuck A, Birrer M et al. (2011) \"Integrated genomic analyses of\n#'   ovarian carcinoma\", Nature, 474(7353):609-615\n#'\n#' @seealso\n#' [genefu::sigOvcTCGA]\n#'\n#' @examples\n#' # load the ovcTCGA signature\n#' data(sigOvcTCGA)\n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \"entrezgene\"\n#' # compute relapse score\n#' ovcTCGA.nkis <- ovcTCGA(data=data.nkis, annot=annot.nkis, gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcTCGA.nkis$risk)\n#'\n#' @md\n#' @export\novcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\",\n    \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE)\n{\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    pscore <- apply(data, 1, function(x, y) { return(t.test(x ~ y)$statistic) }, y=as.numeric(sigt[ ,\"beta\"] < 0))\n\tprisk <- as.numeric(pscore >= 0)\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `ovcTCGA` function?",
        "answer": "The primary purpose of the `ovcTCGA` function is to compute signature scores and risk classifications for ovarian cancer based on gene expression data. It implements the algorithm developed by the TCGA (The Cancer Genome Atlas) consortium for ovarian cancer. The function returns continuous signature scores and binary risk classifications (high risk or low risk) based on the input gene expression data."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs gene mapping using the specified `gmap` parameter. It maps the input data to the signature genes using Entrez Gene IDs or other specified identifiers. In case of ambiguities (multiple probes mapping to the same gene), it keeps the most variant probe for each gene. The function also creates a mapping between the original probe IDs and the new gene IDs, which is returned in the output list as the 'mapping' and 'probe' elements."
      },
      {
        "question": "How are the signature scores and risk classifications calculated in the `ovcTCGA` function?",
        "answer": "The signature scores are calculated using a t-test statistic. The function applies a t-test to each sample, comparing the expression values of genes with positive and negative beta coefficients in the signature. The resulting t-statistic becomes the signature score. For risk classification, samples with a score greater than or equal to 0 are classified as high risk (1), while those with a score less than 0 are classified as low risk (0). This calculation is performed in the last few lines of the function using the `apply` function and `t.test`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        # Complete the else block\n    }\n    # Complete the function\n}",
        "complete": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    pscore <- apply(data, 1, function(x, y) { return(t.test(x ~ y)$statistic) }, y=as.numeric(sigt[ ,\"beta\"] < 0))\n    prisk <- as.numeric(pscore >= 0)\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        # Complete the do.mapping block\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    # Complete the function\n}",
        "complete": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    pscore <- apply(data, 1, function(x, y) { return(t.test(x ~ y)$statistic) }, y=as.numeric(sigt[ ,\"beta\"] < 0))\n    prisk <- as.numeric(pscore >= 0)\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/drugPerturbationSig.R",
    "language": "R",
    "content": "#' Drug perturbation analysis\n#'\n#' Creates a signature representing gene expression (or other molecular profile)\n#' change induced by administrating a drug, for use in drug effect analysis.\n#'\n#' Given a Toxicoset of the perturbation experiment type, and a character vector\n#' of drugs, the function will compute a signature for the effect of\n#' drug concentration on the molecular profile of a cell. The algorithm uses a\n#' regression model which corrects for experimental batch effects, cell specific\n#' differences, and duration of experiment to isolate the effect of the\n#' concentration of the drug applied. The function returns the estimated\n#' coefficient for concentration, the t-stat, the p-value and the false\n#' discovery rate associated with that coefficient, in a 3 dimensional array,\n#' with genes in the first direction, drugs in the second, and the selected\n#' return values in the third.\n#'\n#' @examples\n#' if (interactive()) {\n#' data(TGGATESsmall)\n#' drug.perturbation <- drugPerturbationSig(TGGATESsmall, mDataType=\"rna\", features = head(fNames(TGGATESsmall, \"rna\")), nthread=1)\n#' }\n#'\n#' @param tSet \\code{ToxicoSet} a ToxicoSet of the perturbation experiment type\n#' @param mDataType \\code{character} which one of the molecular data types to use\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv (only rna currently supported)\n#' @param drugs \\code{character} a vector of drug names for which to compute the\n#'   signatures. Should match the names used in the ToxicoSet.\n#' @param cell_lines \\code{character} a vector of cell names to use in computing the\n#'   signatures. Should match the names used in the ToxicoSet.\n#' @param features \\code{character} a vector of features for which to compute the\n#'   signatures. Should match the names used in correspondant molecular data in ToxicoSet.\n#' @param duration \\code{character} a vector of experiment durations for which to inlcude in the\n#'   computed the signatures.\n#' @param dose \\code{character} a vector of dose levels to include in the results\n#' @param nthread \\code{numeric} if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#' @param returnValues \\code{character} Which of estimate, t-stat, p-value and fdr\n#'   should the function return for each gene drug pair\n#' @param verbose \\code{bool} Should diagnostive messages be printed? (default false)\n#'\n#' @return \\code{ToxicoSig} An object composed of a 3D array with genes in the\n#'   first dimension, drugs in the second, and return values in the third.\n#'\n#' @importFrom tibble as_tibble\n#' @import ggplot2\n#'\n#' @export\n#'\ndrugPerturbationSig <- function(\n  tSet, mDataType,\n  drugs = NULL,\n  cell_lines = NULL,\n  features = NULL,\n  duration = NULL,\n  dose = NULL,\n  nthread=1,\n  returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE\n){\n  # ALLOCATE CORES FOR PARALLEL PROCESSING\n  availcore <- parallel::detectCores()\n  if ( nthread > availcore) {\n    nthread <- availcore\n  }\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  ## MISSING VALUE HANDLING FOR PARAMETERS\n  # Get named list of defualt values for missing parameters\n  argDefaultList <-\n    paramMissingHandler(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType,\n                        drugs = drugs, cell_lines = cell_lines, features = features,\n                        duration = duration, dose = dose)\n\n  # Assign any missing parameter default values to function environment\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  ##TODO:: Fix variable names output from paramMissingHandler\n  if ('durations' %in% ls()) { duration <- durations }\n\n  # ERROR HANDLING FOR PARAMETERS\n  paramErrorChecker(\"drugPerturbationSig\", tSet = tSet,\n                    mDataType = mDataType, cell_lines = cell_lines,\n                    drugs = drugs, features = features,\n                    duration = duration, dose = dose)\n\n  returnValues <- match.arg(returnValues, several.ok = TRUE)\n\n  # Add DMSO for the drugMatrix\n  if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) {\n    if (!('DMSO' %in% drugs)) {\n      drugs <- c('DMSO', drugs)\n    }\n    if (!('Control' %in% dose)) {\n      dose <- c('Control', dose)\n    }\n  }\n\n  # SUBSET tSET BASED ON PARAMETERS\n  tSetSubsetOnParams <-\n    suppressWarnings(subsetTo(tSet, mDataType = mDataType, cells = cell_lines, drugs = drugs, features = features, duration = duration))\n\n  # SUBSET SAMPLES BASED ON DOSE\n  samples <- rownames(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$dose_level %in% dose),])\n\n  # LOOP OVER DRUGS TO CALCULATE PER DRUG SUMMARY STATISTICS\n  mcres <- lapply(drugs[drugs != 'DMSO'], function(x, exprs, sampleinfo) {\n\n    # Add DMSO for the drugMatrix since it is the only control\n    if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) {\n        x <- c('DMSO', x)\n    }\n    # Subset to correct drugs\n    exprs <- exprs[which(sampleinfo[ , \"treatmentid\"] %in% x),]\n    sampleinfo <- sampleinfo[which(sampleinfo[ , \"treatmentid\"] %in% x),]\n\n    # Warning that rankGeneDrugPerturbation will return a matrix of NAs for this drug\n    if (length(unique(as.character(sampleinfo[, \"xptype\"]))) < 2) {\n      warning(paste0(\"There are only controls available at dose levels \",\n                     paste(dose, collapse = \" \") ,\" for \", x, \",\n                     summary statistics for this drug will be excluded for the results.\n                     Adding another dose level will likely generate results.\"))\n    }\n    res <- NULL\n    i <- x[x != 'DMSO']\n\n    ## using a linear model (x ~ concentration + cell + batch + duration)\n    res <- rankGeneDrugPerturbation(\n      data = exprs, drug = x, drug.id = as.character(sampleinfo[ , \"treatmentid\"]),\n      drug.concentration = as.numeric(sampleinfo[ , \"concentration\"]),\n      type = as.character(sampleinfo[ , \"sampleid\"]),\n      xp = as.character(sampleinfo[ , \"xptype\"]),\n      batch = as.character(sampleinfo[ , \"batchid\"]),\n      duration = as.character(sampleinfo[ , \"duration\"]) ,\n      single.type = FALSE, nthread = nthread,\n      verbose = FALSE)$all[ , returnValues, drop = FALSE]\n    res <- list(res)\n    names(res) <- i\n    return(res)\n  }, exprs = t(as.data.frame(molecularProfiles(tSetSubsetOnParams, mDataType)[features, samples, drop=FALSE])),\n     sampleinfo = as.data.frame(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$samplename %in% samples), ])\n  )\n\n  # ASSEMBLE RESULTS TO BE INCLUDED IN TOXICOSIG OBJECT\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE=logical(1))]\n  drug.perturbation <- array(NA, dim = c(nrow(featureInfo(tSet, mDataType)[features,, drop = FALSE]), length(res), ncol(res[[1]])), dimnames = list(rownames(featureInfo(tSet, mDataType)[features,,drop = FALSE]), names(res), colnames(res[[1]])))\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n      xx <- array(NA, dim = length(k), dimnames = list(k))\n      xx[rownames(x)] <- x[ , j, drop = FALSE]\n      return(xx)\n    }, j = j, k = rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]),\n    FUN.VALUE=numeric(dim(drug.perturbation)[1]))\n    drug.perturbation[rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]), names(res), j] <- ttt\n  }\n\n  # CREATE TOXICOSIG OBJECT\n  drug.perturbation <- ToxicoGx::ToxicoSig(drug.perturbation, tSetName = name(tSet), Call = as.character(match.call()), SigType = 'Perturbation')\n\n  # RETURN TOXICOSIG OBJECT\n  return(drug.perturbation)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `drugPerturbationSig` function?",
        "answer": "The primary purpose of the `drugPerturbationSig` function is to create a signature representing gene expression (or other molecular profile) changes induced by administering a drug. It computes a signature for the effect of drug concentration on the molecular profile of a cell, using a regression model that corrects for experimental batch effects, cell-specific differences, and experiment duration to isolate the effect of the drug concentration."
      },
      {
        "question": "How does the function handle parallel processing, and what happens if the user specifies more threads than available cores?",
        "answer": "The function uses parallel processing to speed up computations. It detects the number of available cores using `parallel::detectCores()`. If the user specifies more threads (`nthread`) than available cores, the function automatically adjusts `nthread` to match the number of available cores. The function then sets the `mc.cores` option to the adjusted `nthread` value for parallel processing."
      },
      {
        "question": "What is the structure of the returned `ToxicoSig` object, and what information does it contain?",
        "answer": "The returned `ToxicoSig` object contains a 3D array with genes in the first dimension, drugs in the second dimension, and return values in the third dimension. The return values can include 'estimate', 't-stat', 'p-value', and 'fdr' (false discovery rate) for each gene-drug pair, as specified by the `returnValues` parameter. The object also includes metadata such as the name of the ToxicoSet used, the function call, and the signature type ('Perturbation')."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugPerturbationSig <- function(tSet, mDataType, drugs = NULL, cell_lines = NULL, features = NULL, duration = NULL, dose = NULL, nthread = 1, returnValues = c(\"estimate\", \"tstat\", \"pvalue\", \"fdr\"), verbose = FALSE) {\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) nthread <- availcore\n  options(mc.cores = nthread)\n  on.exit(options(op))\n\n  argDefaultList <- paramMissingHandler(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, drugs = drugs, cell_lines = cell_lines, features = features, duration = duration, dose = dose)\n\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  if ('durations' %in% ls()) duration <- durations\n\n  paramErrorChecker(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, cell_lines = cell_lines, drugs = drugs, features = features, duration = duration, dose = dose)\n\n  returnValues <- match.arg(returnValues, several.ok = TRUE)\n\n  # Add code here to handle special cases for 'drugMatrix_rat' and 'EMEXP2458'\n\n  tSetSubsetOnParams <- suppressWarnings(subsetTo(tSet, mDataType = mDataType, cells = cell_lines, drugs = drugs, features = features, duration = duration))\n\n  samples <- rownames(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$dose_level %in% dose),])\n\n  # Add code here for the main computation loop\n\n  # Create and return ToxicoSig object\n}",
        "complete": "drugPerturbationSig <- function(tSet, mDataType, drugs = NULL, cell_lines = NULL, features = NULL, duration = NULL, dose = NULL, nthread = 1, returnValues = c(\"estimate\", \"tstat\", \"pvalue\", \"fdr\"), verbose = FALSE) {\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) nthread <- availcore\n  options(mc.cores = nthread)\n  on.exit(options(op))\n\n  argDefaultList <- paramMissingHandler(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, drugs = drugs, cell_lines = cell_lines, features = features, duration = duration, dose = dose)\n\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  if ('durations' %in% ls()) duration <- durations\n\n  paramErrorChecker(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, cell_lines = cell_lines, drugs = drugs, features = features, duration = duration, dose = dose)\n\n  returnValues <- match.arg(returnValues, several.ok = TRUE)\n\n  if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) {\n    if (!('DMSO' %in% drugs)) drugs <- c('DMSO', drugs)\n    if (!('Control' %in% dose)) dose <- c('Control', dose)\n  }\n\n  tSetSubsetOnParams <- suppressWarnings(subsetTo(tSet, mDataType = mDataType, cells = cell_lines, drugs = drugs, features = features, duration = duration))\n\n  samples <- rownames(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$dose_level %in% dose),])\n\n  mcres <- lapply(drugs[drugs != 'DMSO'], function(x, exprs, sampleinfo) {\n    if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) x <- c('DMSO', x)\n    exprs <- exprs[which(sampleinfo[, \"treatmentid\"] %in% x),]\n    sampleinfo <- sampleinfo[which(sampleinfo[, \"treatmentid\"] %in% x),]\n    if (length(unique(as.character(sampleinfo[, \"xptype\"]))) < 2) {\n      warning(paste0(\"There are only controls available at dose levels \", paste(dose, collapse = \" \"), \" for \", x, \", summary statistics for this drug will be excluded for the results. Adding another dose level will likely generate results.\"))\n    }\n    res <- NULL\n    i <- x[x != 'DMSO']\n    res <- rankGeneDrugPerturbation(data = exprs, drug = x, drug.id = as.character(sampleinfo[, \"treatmentid\"]),\n                                    drug.concentration = as.numeric(sampleinfo[, \"concentration\"]),\n                                    type = as.character(sampleinfo[, \"sampleid\"]),\n                                    xp = as.character(sampleinfo[, \"xptype\"]),\n                                    batch = as.character(sampleinfo[, \"batchid\"]),\n                                    duration = as.character(sampleinfo[, \"duration\"]),\n                                    single.type = FALSE, nthread = nthread, verbose = FALSE)$all[, returnValues, drop = FALSE]\n    res <- list(res)\n    names(res) <- i\n    return(res)\n  }, exprs = t(as.data.frame(molecularProfiles(tSetSubsetOnParams, mDataType)[features, samples, drop=FALSE])),\n     sampleinfo = as.data.frame(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$samplename %in% samples), ]))\n\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE=logical(1))]\n  drug.perturbation <- array(NA, dim = c(nrow(featureInfo(tSet, mDataType)[features,, drop = FALSE]), length(res), ncol(res[[1]])),\n                             dimnames = list(rownames(featureInfo(tSet, mDataType)[features,,drop = FALSE]), names(res), colnames(res[[1]])))\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n      xx <- array(NA, dim = length(k), dimnames = list(k))\n      xx[rownames(x)] <- x[, j, drop = FALSE]\n      return(xx)\n    }, j = j, k = rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]),\n    FUN.VALUE=numeric(dim(drug.perturbation)[1]))\n    drug.perturbation[rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]), names(res), j] <- ttt\n  }\n\n  drug.perturbation <- ToxicoGx::ToxicoSig(drug.perturbation, tSetName = name(tSet), Call = as.character(match.call()), SigType = 'Perturbation')\n  return(drug.perturbation)\n}"
      },
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type = FALSE, nthread = 1, verbose = FALSE) {\n  # Add code here to prepare data and variables\n\n  # Perform the main computation\n\n  # Process and return results\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type = FALSE, nthread = 1, verbose = FALSE) {\n  if (verbose) message(\"Ranking genes for drug: \", drug)\n  data <- as.matrix(data)\n  drug.id <- as.factor(drug.id)\n  type <- as.factor(type)\n  xp <- as.factor(xp)\n  batch <- as.factor(batch)\n  duration <- as.factor(duration)\n\n  if (single.type) {\n    mm <- model.matrix(~ drug.concentration + batch + duration)\n  } else {\n    mm <- model.matrix(~ drug.concentration + type + batch + duration)\n  }\n\n  fit <- limma::lmFit(data, mm)\n  fit <- limma::eBayes(fit)\n\n  tt <- limma::topTable(fit, coef = \"drug.concentration\", number = Inf, sort.by = \"none\")\n  tt <- tt[, c(\"logFC\", \"t\", \"P.Value\", \"adj.P.Val\")]\n  colnames(tt) <- c(\"estimate\", \"tstat\", \"pvalue\", \"fdr\")\n\n  return(list(all = tt))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/compute.pairw.cor.meta.R",
    "language": "R",
    "content": "#' @title Function to compute pairwise correlations in a meta-analytical framework\n#'\n#' @description\n#' This function computes meta-estimate of pairwise correlation coefficients for a\n#'   set of genes from a list of gene expression datasets.\n#'\n#' @usage\n#' compute.pairw.cor.meta(datas, method = c(\"pearson\", \"spearman\"))\n#'\n#' @param datas List of datasets. Each dataset is a matrix of gene expressions with\n#'   samples in rows and probes in columns, dimnames being properly defined. All the\n#'   datasets must have the same probes.\n#' @param method Estimator for correlation coefficient, can be either pearson or spearman.\n#'\n#' @return\n#' A list with items:\n#' - cor  Matrix of meta-estimate of correlation coefficients with probes in rows and\n#'   prototypes in columns\n#' - cor.n Number of samples used to compute meta-estimate of correlation coefficients.\n#'\n#' @seealso\n#' [genefu::map.datasets], [genefu::compute.proto.cor.meta]\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # load NKI dataset\n#' data(nkis)\n#' # reduce datasets\n#' ginter <- intersect(annot.vdxs[ ,\"EntrezGene.ID\"], annot.nkis[ ,\"EntrezGene.ID\"])\n#' ginter <- ginter[!is.na(ginter)][1:30]\n#' myx <- unique(c(match(ginter, annot.vdxs[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.vdxs), size=20)))\n#' data2.vdxs <- data.vdxs[ ,myx]\n#' annot2.vdxs <- annot.vdxs[myx, ]\n#' myx <- unique(c(match(ginter, annot.nkis[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.nkis), size=20)))\n#' data2.nkis <- data.nkis[ ,myx]\n#' annot2.nkis <- annot.nkis[myx, ]\n#' # mapping of datasets\n#' datas <- list(\"VDX\"=data2.vdxs,\"NKI\"=data2.nkis)\n#' annots <- list(\"VDX\"=annot2.vdxs, \"NKI\"=annot2.nkis)\n#' datas.mapped <- map.datasets(datas=datas, annots=annots, do.mapping=TRUE)\n#' # compute meta-estimate of pairwise correlation coefficients\n#' pairwcor <- compute.pairw.cor.meta(datas=datas.mapped$datas, method=\"pearson\")\n#' str(pairwcor)\n#'\n#' @md\n#' @importFrom survcomp combine.est fisherz\n#' @export\ncompute.pairw.cor.meta <-\nfunction(datas, method=c(\"pearson\", \"spearman\")) {\n\tmethod <- match.arg(method)\n\tif(!is.list(datas)) {\n\t\tmycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n\t} else {\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\t\tif(length(datas) > 1) {\n\t\t    for(k in 2:length(datas)) {\n\t\t\t    if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n\t\t    }\n\t    }\n\t\tmycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n\t\tmycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n\t\tfor(i in 1:nc) {\n\t\t\tfor(j in 1:i) {\n\t\t\t\tmycorz <- mycorz.se <- NULL\n\t\t\t\tnnt <- 0\n\t\t\t\tfor(k in 1:length(datas)) {\n\t\t\t\t\tif(sum(complete.cases(datas[[k]][ , c(i, j)])) > 1) {\n\t\t\t\t\t\tnn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n\t\t\t\t\t\tmycorz <- c(mycorz, survcomp::fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE))\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n\t\t\t\t\t\tnnt <- nnt + nn\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmycorz <- c(mycorz, NA)\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, NA)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmycor[i, j] <- mycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n\t\t\t\tmycorn[i, j] <- mycorn[j, i] <- nnt\n\t\t\t}\n\t\t}\n\t}\n\treturn(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `compute.pairw.cor.meta` function, and what are its main input parameters?",
        "answer": "The `compute.pairw.cor.meta` function computes meta-estimates of pairwise correlation coefficients for a set of genes from multiple gene expression datasets. Its main input parameters are:\n1. `datas`: A list of datasets, where each dataset is a matrix of gene expressions with samples in rows and probes in columns.\n2. `method`: The estimator for correlation coefficient, which can be either 'pearson' or 'spearman'."
      },
      {
        "question": "How does the function handle the computation of correlation coefficients when multiple datasets are provided?",
        "answer": "When multiple datasets are provided, the function:\n1. Checks if all datasets have the same variables (columns).\n2. Initializes matrices for correlation coefficients and sample counts.\n3. Iterates through each pair of variables, computing correlations for each dataset.\n4. Uses Fisher's z-transformation to combine correlation estimates across datasets.\n5. Applies inverse Fisher's z-transformation to get the final meta-estimate.\n6. Returns a list containing the meta-estimates of correlation coefficients and the total sample sizes used for each pair."
      },
      {
        "question": "What is the significance of the `fisherz` function used in this code, and why is it applied twice?",
        "answer": "The `fisherz` function is used for Fisher's z-transformation, which is important in meta-analysis of correlation coefficients. It's applied twice in this code:\n1. First, to transform individual correlation coefficients to z-scores: `survcomp::fisherz(cor(...), inv=FALSE)`.\n2. Second, to transform the combined z-score back to a correlation coefficient: `fisherz(combine.est(...)$estimate, inv=TRUE)`.\nThis process allows for proper averaging of correlation coefficients across different datasets, as z-scores can be meaningfully combined, unlike raw correlation coefficients."
      }
    ],
    "completion_tasks": [
      {
        "partial": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    if(length(datas) > 1) {\n      for(k in 2:length(datas)) {\n        if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n      }\n    }\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    for(i in 1:nc) {\n      for(j in 1:i) {\n        # Complete the code here\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
        "complete": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    if(length(datas) > 1) {\n      for(k in 2:length(datas)) {\n        if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n      }\n    }\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    for(i in 1:nc) {\n      for(j in 1:i) {\n        mycorz <- mycorz.se <- NULL\n        nnt <- 0\n        for(k in 1:length(datas)) {\n          if(sum(complete.cases(datas[[k]][ , c(i, j)])) > 1) {\n            nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n            mycorz <- c(mycorz, survcomp::fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE))\n            mycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n            nnt <- nnt + nn\n          } else {\n            mycorz <- c(mycorz, NA)\n            mycorz.se <- c(mycorz.se, NA)\n          }\n        }\n        mycor[i, j] <- mycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[i, j] <- mycorn[j, i] <- nnt\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}"
      },
      {
        "partial": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n    return(list(\"cor\"=mycor, \"cor.n\"=nrow(datas)))\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    # Add code to check if all datasets have the same variables\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    # Add code to compute correlations\n    return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n  }\n}",
        "complete": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n    return(list(\"cor\"=mycor, \"cor.n\"=nrow(datas)))\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    if(length(datas) > 1) {\n      for(k in 2:length(datas)) {\n        if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n      }\n    }\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    for(i in 1:nc) {\n      for(j in 1:i) {\n        mycorz <- mycorz.se <- NULL\n        nnt <- 0\n        for(k in 1:length(datas)) {\n          if(sum(complete.cases(datas[[k]][ , c(i, j)])) > 1) {\n            nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n            mycorz <- c(mycorz, survcomp::fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE))\n            mycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n            nnt <- nnt + nn\n          } else {\n            mycorz <- c(mycorz, NA)\n            mycorz.se <- c(mycorz.se, NA)\n          }\n        }\n        mycor[i, j] <- mycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[i, j] <- mycorn[j, i] <- nnt\n      }\n    }\n    return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/claudinLow.R",
    "language": "R",
    "content": "#' @title Claudin-low classification for Breast Cancer Data\n#'\n#' @description\n#' Subtyping method for identifying Claudin-Low Breast Cancer Samples.\n#'   Code generously provided by Aleix Prat.\n#'\n#' @usage\n#' claudinLow(x, classes=\"\", y, nGenes=\"\", priors=\"equal\",\n#'   std=FALSE, distm=\"euclidean\", centroids=FALSE)\n#'\n#' @param x the data matrix of training samples, or pre-calculated centroids.\n#' @param classes a list labels for use in coloring the points.\n#' @param y the data matrix of test samples.\n#' @param nGenes the number of genes selected when training the model.\n#' @param priors 'equal' assumes equal class priors, 'class' calculates them\n#'   based on proportion in the data.\n#' @param std when true, the training and testing samples are standardized\n#'   to mean=0 and var=1.\n#' @param distm the distance metric for determining the nearest centroid,\n#'   can be one of euclidean, pearson, or spearman.\n#' @param centroids when true, it is assumed that x consists of pre-calculated centroids.\n#'\n#' @return\n#' A list with items:\n#' - predictions\n#' - testData\n#' - distances\n#' - centroids\n#'\n#' @references\n#' Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason\n#'   I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and\n#'   molecular characterization of the claudin-low intrinsic subtype of\n#'   breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso\n#' [genefu::medianCtr()], [genefu::q]\n#'\n#' @examples\n#' data(claudinLowData)\n#'\n#' #Training Set\n#' train <- claudinLowData\n#' train$xd <-  medianCtr(train$xd)\n#' # Testing Set\n#' test <- claudinLowData\n#' test$xd <-  medianCtr(test$xd)\n#'\n#' # Generate Predictions\n#' predout <- claudinLow(x=train$xd, classes=as.matrix(train$classes$Group,ncol=1), y=test$xd)\n#'\n#' # Obtain results\n#' results <- cbind(predout$predictions, predout$distances)\n#' #write.table(results,\"T.E.9CELL.LINE_results.txt\",sep=\"\\t\",col=T, row=FALSE)\n#'\n#' @md\n#' @import limma stats utils\n#' @export\nclaudinLow <- function(x, classes=\"\", y, nGenes=\"\", priors=\"equal\", std=FALSE,\n    distm=\"euclidean\", centroids=FALSE){\n\n  dataMatrix <- x\n  features <- dim(x)[1]\n  samples <- dim(x)[2]\n  sampleNames <- dimnames(x)[[2]]\n  featureNames <- dimnames(x)[[1]]\n\n  #parse the test file - same as train file but no rows of classes\n  tdataMatrix <- y\n  tfeatures <- dim(y)[1]\n  tsamples <- dim(y)[2]\n  tsampleNames <- dimnames(y)[[2]]\n  tfeatureNames <- dimnames(y)[[1]]\n\n  #dimnames(tdataMatrix)[[2]] <- paste(\"x\",seq(1,471))\n  temp <- overlapSets(dataMatrix,tdataMatrix)\n  dataMatrix <- temp$x\n  tdataMatrix <- temp$y\n  sfeatureNames <- row.names(dataMatrix)\n\n  # standardize both sets\n  if(std){\n    dataMatrix <- scale(dataMatrix)\n    dataMatrix <- scale(tdataMatrix)\n  }\n\n  if(!centroids){\n    thisClass <- as.vector(classes[,1])\n    nClasses <- nlevels(as.factor(thisClass))\n    classLevels <- levels(as.factor(thisClass))\n    for(j in 1:nClasses){\n      thisClass[thisClass==classLevels[j]] <- j\n    }\n    thisClass <- as.numeric(thisClass)\n    dataMatrix <- dataMatrix[,!(is.na(thisClass))]\n    thisClass <- thisClass[!(is.na(thisClass))]\n\n    scores <- apply(dataMatrix,1,limma::bwss,thisClass)\n    trainscores <- vector()\n    for(j in 1:dim(dataMatrix)[1]){\n      trainscores[j] <- scores[[row.names(dataMatrix)[j]]]$bss / scores[[row.names(dataMatrix)[j]]]$wss\n    }\n\n    dataMatrix <- dataMatrix[sort.list(trainscores,decreasing=TRUE),]\n    tdataMatrix <- tdataMatrix[sort.list(trainscores,decreasing=TRUE),]\n\n    if(nGenes==\"\"){\n      nGenes <- dim(dataMatrix)[1]\n    }\n    print(paste(\"Number of genes used:\",nGenes))\n\n    dataMatrix <- dataMatrix[1:nGenes,]\n    tdataMatrix <- tdataMatrix[1:nGenes,]\n\n    centroids <- matrix(,nrow=nGenes,ncol=nClasses)\n    for(j in 1:nClasses){\n      centroids[,j] <- apply(dataMatrix[,thisClass==j],1,mean)\n    }\n    dimnames(centroids) <- list(row.names(dataMatrix),NULL)\n\n  }else{\n    nGenes <- dim(dataMatrix)[1]\n    print(paste(\"Number of genes used:\",nGenes))\n    centroids <- dataMatrix\n    nClasses <- dim(centroids)[2]\n    classLevels <- dimnames(centroids)[[2]]\n  }\n\n  distances <- matrix(ncol=nClasses,nrow=dim(tdataMatrix)[2])\n  for(j in 1:nClasses){\n    if(distm==\"euclidean\"){\n      distances[,j] <- dist(t(cbind(centroids[,j],tdataMatrix)))[1:(dim(tdataMatrix)[2])]\n    }\n    if(distm==\"correlation\" | distm==\"pearson\"){\n      distances[,j] <- t(-1*cor(cbind(centroids[,j],tdataMatrix),use=\"pairwise.complete.obs\"))[2:(dim(tdataMatrix)[2]+1)]\n    }\n    if(distm==\"spearman\"){\n      distances[,j] <- t(-1*cor(cbind(centroids[,j],tdataMatrix),method=\"spearman\",use=\"pairwise.complete.obs\"))[2:(dim(tdataMatrix)[2]+1)]\n    }\n    colnames(distances) <- c(\"euclidian distance to Claudin-low\", \"euclidian distance to Others\")\n    rownames(distances) <- tsampleNames\n\n  }\n\n  scores <- apply(distances,1,min)\n  prediction <- vector(length=tsamples)\n  for(i in 1:tsamples){\n    prediction[i] <- classLevels[match(scores[i],distances[i,])]\n  }\n  names(prediction) <- tsampleNames\n  prediction <- data.frame(Samples=tsampleNames, prediction)\n  colnames(prediction) <- c(\"Samples\", \"Call\")\n  return(list(predictions=prediction,testData=tdataMatrix,distances=distances,centroids=centroids))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `claudinLow` function in this code snippet?",
        "answer": "The `claudinLow` function is designed for subtyping and identifying Claudin-Low Breast Cancer Samples. It performs classification based on gene expression data, calculating distances between test samples and centroids of known classes, and predicts the subtype of new samples."
      },
      {
        "question": "How does the function handle the case when pre-calculated centroids are provided?",
        "answer": "When the `centroids` parameter is set to `TRUE`, the function assumes that `x` contains pre-calculated centroids. In this case, it skips the centroid calculation step and uses the provided centroids directly for classification. The number of genes and classes are determined from the dimensions of the input centroids."
      },
      {
        "question": "What distance metrics are supported by the `claudinLow` function for determining the nearest centroid, and how are they implemented?",
        "answer": "The function supports three distance metrics: euclidean, pearson (correlation), and spearman. For euclidean distance, it uses the `dist` function. For pearson and spearman correlations, it uses the `cor` function with the appropriate method and then converts the correlation to a distance by subtracting it from 1. The choice of metric is controlled by the `distm` parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "claudinLow <- function(x, classes=\"\", y, nGenes=\"\", priors=\"equal\", std=FALSE,\n    distm=\"euclidean\", centroids=FALSE){\n\n  dataMatrix <- x\n  features <- dim(x)[1]\n  samples <- dim(x)[2]\n  sampleNames <- dimnames(x)[[2]]\n  featureNames <- dimnames(x)[[1]]\n\n  tdataMatrix <- y\n  tfeatures <- dim(y)[1]\n  tsamples <- dim(y)[2]\n  tsampleNames <- dimnames(y)[[2]]\n  tfeatureNames <- dimnames(y)[[1]]\n\n  temp <- overlapSets(dataMatrix,tdataMatrix)\n  dataMatrix <- temp$x\n  tdataMatrix <- temp$y\n  sfeatureNames <- row.names(dataMatrix)\n\n  if(std){\n    dataMatrix <- scale(dataMatrix)\n    tdataMatrix <- scale(tdataMatrix)\n  }\n\n  if(!centroids){\n    # Add code here to handle non-centroid case\n  } else {\n    # Add code here to handle centroid case\n  }\n\n  # Add code here for distance calculation and prediction\n\n  return(list(predictions=prediction,testData=tdataMatrix,distances=distances,centroids=centroids))\n}",
        "complete": "claudinLow <- function(x, classes=\"\", y, nGenes=\"\", priors=\"equal\", std=FALSE,\n    distm=\"euclidean\", centroids=FALSE){\n\n  dataMatrix <- x\n  features <- dim(x)[1]\n  samples <- dim(x)[2]\n  sampleNames <- dimnames(x)[[2]]\n  featureNames <- dimnames(x)[[1]]\n\n  tdataMatrix <- y\n  tfeatures <- dim(y)[1]\n  tsamples <- dim(y)[2]\n  tsampleNames <- dimnames(y)[[2]]\n  tfeatureNames <- dimnames(y)[[1]]\n\n  temp <- overlapSets(dataMatrix,tdataMatrix)\n  dataMatrix <- temp$x\n  tdataMatrix <- temp$y\n  sfeatureNames <- row.names(dataMatrix)\n\n  if(std){\n    dataMatrix <- scale(dataMatrix)\n    tdataMatrix <- scale(tdataMatrix)\n  }\n\n  if(!centroids){\n    thisClass <- as.vector(classes[,1])\n    nClasses <- nlevels(as.factor(thisClass))\n    classLevels <- levels(as.factor(thisClass))\n    thisClass <- as.numeric(factor(thisClass, levels = classLevels))\n    dataMatrix <- dataMatrix[,!is.na(thisClass)]\n    thisClass <- thisClass[!is.na(thisClass)]\n\n    scores <- apply(dataMatrix, 1, function(x) limma::bwss(x, thisClass))\n    trainscores <- sapply(scores, function(x) x$bss / x$wss)\n\n    order <- order(trainscores, decreasing = TRUE)\n    dataMatrix <- dataMatrix[order,]\n    tdataMatrix <- tdataMatrix[order,]\n\n    nGenes <- if(nGenes == \"\") nrow(dataMatrix) else nGenes\n    print(paste(\"Number of genes used:\", nGenes))\n\n    dataMatrix <- dataMatrix[1:nGenes,]\n    tdataMatrix <- tdataMatrix[1:nGenes,]\n\n    centroids <- sapply(1:nClasses, function(j) rowMeans(dataMatrix[,thisClass == j, drop = FALSE]))\n    dimnames(centroids) <- list(rownames(dataMatrix), NULL)\n  } else {\n    nGenes <- nrow(dataMatrix)\n    print(paste(\"Number of genes used:\", nGenes))\n    centroids <- dataMatrix\n    nClasses <- ncol(centroids)\n    classLevels <- colnames(centroids)\n  }\n\n  distances <- sapply(1:nClasses, function(j) {\n    switch(distm,\n      \"euclidean\" = sqrt(colSums((tdataMatrix - centroids[,j])^2)),\n      \"correlation\" = , \"pearson\" = 1 - cor(centroids[,j], tdataMatrix),\n      \"spearman\" = 1 - cor(centroids[,j], tdataMatrix, method = \"spearman\")\n    )\n  })\n  colnames(distances) <- c(\"euclidian distance to Claudin-low\", \"euclidian distance to Others\")\n  rownames(distances) <- tsampleNames\n\n  prediction <- factor(max.col(-distances), levels = 1:nClasses, labels = classLevels)\n  names(prediction) <- tsampleNames\n  prediction <- data.frame(Samples = tsampleNames, Call = prediction)\n\n  return(list(predictions = prediction, testData = tdataMatrix, distances = distances, centroids = centroids))\n}"
      },
      {
        "partial": "overlapSets <- function(x, y) {\n  # Add code here to find overlapping features between x and y\n  # and return the subsets of x and y with only these features\n}",
        "complete": "overlapSets <- function(x, y) {\n  common_features <- intersect(rownames(x), rownames(y))\n  x_subset <- x[common_features, , drop = FALSE]\n  y_subset <- y[common_features, , drop = FALSE]\n  return(list(x = x_subset, y = y_subset))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/setcolclass.df.R",
    "language": "R",
    "content": "#' @title Function to set the class of columns in a data.frame\n#'\n#' @description\n#' This function enables to set the class of each column in a data.frame.\n#'\n#' @usage\n#' setcolclass.df(df, colclass, factor.levels)\n#'\n#' @param df data.frame for which columns' class need to be updated.\n#' @param colclass class for each column of the data.frame.\n#' @param factor.levels\tlist of levels for each factor.\n#'\n#' @return\n#' A data.frame with columns' class and levels properly set\n#'\n#' @examples\n#' tt <- data.frame(matrix(NA, nrow=3, ncol=3, dimnames=list(1:3, paste(\"column\", 1:3, sep=\".\"))), \n#'   stringsAsFactors=FALSE)\n#' tt <- setcolclass.df(df=tt, colclass=c(\"numeric\", \"factor\", \"character\"), \n#'   factor.levels=list(NULL, c(\"F1\", \"F2\", \"F3\"), NULL))\n#'\n#' @md\n#' @export\nsetcolclass.df <-\nfunction (df, colclass, factor.levels) {\n\tww <- options()$warn\n\toptions(warn=-1)\n\ttoCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n\tdf <- replace(df, , Map(toCls, x=df, cls=colclass))\n\toptions(warn=ww)\n\tiix <- FALSE\n\tif(!missing(factor.levels)) { iix <- colclass == \"factor\" & !is.null(factor.levels) }\n\tif(any(iix)) {\n\t\tfor(i in which(iix)) { levels(df[[i]]) <- factor.levels[[i]] }\n\t}\n\treturn(df)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setcolclass.df` function in R, and what are its main parameters?",
        "answer": "The `setcolclass.df` function is designed to set the class of columns in a data frame. It takes three main parameters: `df` (the input data frame), `colclass` (a vector specifying the desired class for each column), and `factor.levels` (an optional list of levels for factor columns)."
      },
      {
        "question": "How does the function handle warnings during the class conversion process?",
        "answer": "The function temporarily suppresses warnings by setting `options(warn=-1)` at the beginning of the function and restores the original warning level at the end using `options(warn=ww)`. This prevents potential warnings from being displayed during the class conversion process."
      },
      {
        "question": "Explain the purpose of the `toCls` function within `setcolclass.df` and how it is used.",
        "answer": "The `toCls` function is a helper function that converts a value `x` to a specified class `cls`. It uses `do.call` to dynamically construct and call the appropriate conversion function (e.g., `as.numeric`, `as.factor`, `as.character`). This function is then used with `Map` to apply the class conversions to all columns of the data frame."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setcolclass.df <- function(df, colclass, factor.levels) {\n  ww <- options()$warn\n  options(warn=-1)\n  toCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n  df <- replace(df, , Map(toCls, x=df, cls=colclass))\n  options(warn=ww)\n  # Complete the function to handle factor levels\n  \n  return(df)\n}",
        "complete": "setcolclass.df <- function(df, colclass, factor.levels) {\n  ww <- options()$warn\n  options(warn=-1)\n  toCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n  df <- replace(df, , Map(toCls, x=df, cls=colclass))\n  options(warn=ww)\n  iix <- !missing(factor.levels) && colclass == \"factor\" & !sapply(factor.levels, is.null)\n  if(any(iix)) {\n    for(i in which(iix)) { levels(df[[i]]) <- factor.levels[[i]] }\n  }\n  return(df)\n}"
      },
      {
        "partial": "setcolclass.df <- function(df, colclass, factor.levels) {\n  # Implement the function body\n  # Hint: Use Map() to apply class conversion\n  # Don't forget to handle factor levels\n}",
        "complete": "setcolclass.df <- function(df, colclass, factor.levels) {\n  ww <- options()$warn\n  options(warn=-1)\n  toCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n  df <- replace(df, , Map(toCls, x=df, cls=colclass))\n  options(warn=ww)\n  if(!missing(factor.levels)) {\n    iix <- colclass == \"factor\" & !sapply(factor.levels, is.null)\n    if(any(iix)) {\n      for(i in which(iix)) { levels(df[[i]]) <- factor.levels[[i]] }\n    }\n  }\n  return(df)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/subtype.cluster.predict.R",
    "language": "R",
    "content": "#' @title Function to identify breast cancer molecular subtypes using\n#'   the Subtype Clustering Model\n#'\n#' @description\n#' This function identifies the breast cancer molecular subtypes using a\n#'   Subtype Clustering Model fitted by subtype.cluster.\n#'\n#' @usage\n#' subtype.cluster.predict(sbt.model, data, annot, do.mapping = FALSE,\n#'   mapping, do.prediction.strength = FALSE,\n#'   do.BIC = FALSE, plot = FALSE, verbose = FALSE)\n#'\n#' @param sbt.model\tSubtype Clustering Model as returned by subtype.cluster.\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept\n#'   for each gene), FALSE otherwise.\n#' @param mapping\t**DEPRECATED** Matrix with columns \"EntrezGene.ID\" and\n#'   \"probe\" used to force the mapping such that the probes are not selected\n#'   based on their variance.\n#' @param do.prediction.strength TRUE if the prediction strength must be\n#'   computed (Tibshirani and Walther 2005), FALSE otherwise.\n#' @param do.BIC TRUE if the Bayesian Information Criterion must be computed\n#'   for number of clusters ranging from 1 to 10, FALSE otherwise.\n#' @param plot TRUE if the patients and their corresponding subtypes must\n#'   be plotted, FALSE otherwise.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - subtype: Subtypes identified by the Subtype Clustering Model.\n#'   Subtypes can be either \"ER-/HER2-\", \"HER2+\" or \"ER+/HER2-\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated\n#'   by the Subtype Clustering Model.\n#' - prediction.strength: Prediction strength for subtypes.\n#' - BIC: Bayesian Information Criterion for the Subtype Clustering Model\n#'   with number of clusters ranging from 1 to 10.\n#' - subtype2: Subtypes identified by the Subtype Clustering Model using\n#'   AURKA to discriminate low and high proliferative tumors. Subtypes can be\n#'   either \"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\" or\n#'   \"ER+/HER2- Low Prolif\".\n#' - subtype.proba2: Probabilities to belong to each subtype (including\n#'   discrimination between lowly and highly proliferative ER+/HER2- tumors,\n#'   see subtype2) estimated by the Subtype Clustering Model.\n#' - prediction.strength2: Prediction strength for subtypes2.\n#' - module.scores: Matrix containing ESR1, ERBB2 and AURKA module scores.\n#' - mapping: Mapping if necessary (list of matrices with 3 columns: probe,\n#'   EntrezGene.ID and new.probe).\n#'\n#' @references\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G,\n#'   Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes\n#'   associated with breast cancer clinical outcome depend on the molecular\n#'   subtypes\", Clinical Cancer Research, 14(16):5158-5165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B,\n#'   Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ\n#'   and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in\n#'   Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing\n#'   and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' Tibshirani R and Walther G (2005) \"Cluster Validation by Prediction\n#'   Strength\", Journal of Computational and Graphical Statistics,\n#'   14(3):511-528\n#'\n#' @seealso\n#' [genefu::subtype.cluster], [genefu::scmod1.robust], [genefu::scmod2.robust]\n#'\n#' @examples\n#' # without mapping (affy hgu133a or plus2 only)\n#' # load VDX data\n#' data(vdxs)\n#' data(scmgene.robust)\n#'\n#' # Subtype Clustering Model fitted on EXPO and applied on VDX\n#' sbt.vdxs <- subtype.cluster.predict(sbt.model=scmgene.robust, data=data.vdxs,\n#'   annot=annot.vdxs, do.mapping=FALSE, do.prediction.strength=FALSE,\n#'   do.BIC=FALSE, plot=TRUE, verbose=TRUE)\n#' table(sbt.vdxs$subtype)\n#' table(sbt.vdxs$subtype2)\n#'\n#' # with mapping\n#' # load NKI data\n#' data(nkis)\n#' # Subtype Clustering Model fitted on EXPO and applied on NKI\n#' sbt.nkis <- subtype.cluster.predict(sbt.model=scmgene.robust, data=data.nkis,\n#'   annot=annot.nkis, do.mapping=TRUE, do.prediction.strength=FALSE,\n#'   do.BIC=FALSE, plot=TRUE, verbose=TRUE)\n#' table(sbt.nkis$subtype)\n#' table(sbt.nkis$subtype2)\n#'\n#' @md\n#' @import graphics\n#' @export\nsubtype.cluster.predict <-\nfunction(sbt.model, data, annot, do.mapping=FALSE, mapping,\n    do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n{\n\tif(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n\tsbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n\tsbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\n\tif(is.list(sbt.model)) {\n\t\t## retrieve model\n\t\tsubtype.c <- sbt.model[!is.element(names(sbt.model), c(\"cutoff.AURKA\", \"mod\"))]\n\t\tmodel.name <- subtype.c$parameters$variance$modelName\n\t\tcc <- sbt.model$gaussian.AURKA\n\t\tmq <- sbt.model$rescale.q\n\t\tm.mod <- sbt.model$mod\n\t} else {\n\t\t## read model file\n\t\trr <- readLines(con=sbt.model, n=11)[-1]\n\t\tnn <- unlist(lapply(X=rr, FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[1], split=\" \")); x <- x[length(x)]; return(x); }))\n\t\tm.param <- c(list(rr[[1]]), lapply(X=rr[-1], FUN=function(x) { x <- as.numeric(unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \"))); x <- x[!is.na(x)]; return(x)}))\n\t\tnames(m.param) <- nn\n\t\tcn <- unlist(lapply(strsplit(nn[grep(pattern=\"mean\", x=nn)], split=\"[.]\"), FUN=function(x) { return(x[[2]]) }))\n\t\tm.mod <- read.m.file(sbt.model, comment.char=\"#\")\n\t\t#construct a fake mclust object with the parameters of the model\n\t\tsubtype.c <- NULL\n\t\ttt <- m.param$pro\n\t\tnames(tt) <- cn\n\t\tsubtype.c$parameters$pro <- tt\n\t\ttt <- sapply(X=m.param[grep(pattern=\"mean\", x=nn)], FUN=function(x) { return(x) })\n\t\tdimnames(tt) <- list(names(m.mod)[1:2], cn)\n\t\tsubtype.c$parameters$mean <- tt\n\t\tsubtype.c$parameters$variance$modelName <- model.name <- m.param$modelname\n\t\tsubtype.c$parameters$variance$d <- 2\n\t\tsubtype.c$parameters$variance$G <- 3\n\t\ttt <- matrix(0, ncol=2, nrow=2, dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2]))\n\t\tdiag(tt) <- m.param$sigma\n\t\tsubtype.c$parameters$variance$sigma <- array(tt, dim=c(2,2,3), dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2], cn))\n\t\tsubtype.c$parameters$variance$Sigma <- tt\n\t\tsubtype.c$parameters$variance$scale <- m.param$scale\n\t\tsubtype.c$parameters$variance$shape <- m.param$shape\n\t\tcc <- c(\"mean\"=m.param$gaussian.AURKA.mean, \"sigma\"=m.param$gaussian.AURKA.sigma)\n\t\tmq <- m.param$rescale.q\n\t}\n\tdo.scale <- ifelse(is.na(mq), FALSE, TRUE)\n\tsbt <- rep(NA, nrow(data))\n\tnames(sbt) <- dimnames(data)[[1]]\n\tsbt.proba <- matrix(NA, nrow(data), ncol=length(sbtn), dimnames=list(dimnames(data)[[1]], sbtn))\n\n\tsigs.esr1 <- sig.score(x=m.mod$ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsigs.erbb2 <- sig.score(x=m.mod$ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsigs.aurka <- sig.score(x=m.mod$AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\t## signature scores\n\tdd <- cbind(\"ESR1\"=sigs.esr1$score, \"ERBB2\"=sigs.erbb2$score, \"AURKA\"=sigs.aurka$score)\n\t## mapping\n\tmymap <- list(\"ESR1\"=sigs.esr1$probe, \"ERBB2\"=sigs.erbb2$probe, \"AURLA\"=sigs.aurka$probe)\n\tcln <- dimnames(subtype.c$parameters$mean)[[2]] <- as.character(1:ncol(subtype.c$parameters$mean))\n\n\tif(do.scale) {\n\t\t## the rescaling needs a large sample size!!!\n\t\t## necessary if we want to validate the classifier using a different dataset\n\t\t## the estimation of survival probabilities depends on the scale of the score\n\t\tdd <- apply(dd, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n\t}\n\trownames(dd) <- rownames(data)\n\tdd2 <- dd\n\n\tcc.ix <- complete.cases(dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE])\n\tif(all(!cc.ix)) {\n\t\tps.res <- ps.res2 <- BIC.res <- NULL\n\t\tif(do.prediction.strength) {\n\t\t\ttt <- rep(NA, length(sbtn))\n\t\t\tnames(tt) <- sbtn\n\t\t\ttt2 <- rep(NA, length(sbtn2))\n\t\t\tnames(tt2) <- sbtn2\n\t\t\tps.res <- list(\"ps\"=NA, \"ps.cluster\"=tt, \"ps.individual\"=sbt)\n\t\t\tps.res2 <- list(\"ps\"=NA, \"ps.cluster\"=tt2, \"ps.individual\"=sbt)\n\t\t}\n\t\tif(do.BIC) {\n\t\t\tBIC.res <- rep(NA, 10)\n\t\t\tnames(BIC.res) <- 1:10\n\t\t}\n\t\treturn(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt, \"prediction.strength2\"=ps.res2))\n\t}\n\tdd <- dd[cc.ix, , drop=FALSE]\n\n\temclust.ts <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=subtype.c$parameters)\n\tdimnames(emclust.ts$z) <- list(dimnames(dd)[[1]], cln)\n\tclass.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n\tnames(class.ts) <- dimnames(dd)[[1]]\n\tuclass <- sort(unique(class.ts))\n\tuclass <- uclass[!is.na(uclass)]\n\n\tps.res <- ps.res2 <- NULL\n\tif(do.prediction.strength) {\n\t\tif(nrow(dd) < 10) {\n\t\t\twarning(\"at least 10 observations are required to compute the prediction strength!\")\n\t\t\ttt <- rep(NA, length(sbtn))\n\t\t\tnames(tt) <- sbtn\n\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\tps.res <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t\ttt <- rep(NA, length(sbtn2))\n\t\t\tnames(tt) <- sbtn2\n\t\t\tps.res2 <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t} else {\n\t\t\t## computation of the prediction strength of the clustering\n\t\t\trr3 <- mclust::Mclust(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=model.name, G=3)\n\t\t\t## redefine classification to be coherent with subtypes\n\t\t\tuclass <- sort(unique(rr3$classification))\n\t\t\tuclass <- uclass[!is.na(uclass)]\n\t\t\tif(length(uclass) != 3) {\n\t\t\t\twarning(\"less than 3 subtypes are identified!\")\n\t\t\t\ttt <- rep(NA, length(sbtn))\n\t\t\t\tnames(tt) <- sbtn\n\t\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\t\tps.res <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t\t\ttt <- rep(NA, length(sbtn2))\n\t\t\t\tnames(tt) <- sbtn2\n\t\t\t\tps.res2 <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t\t} else {\n\t\t\t\tmm <- NULL\n\t\t\t\tfor(i in 1:length(uclass)) {\n\t\t\t\t\tmm <- c(mm, median(dd[rr3$classification == uclass[i],\"ERBB2\"], na.rm=TRUE) )\n\t\t\t\t}\n\t\t\t\tnclass <-  uclass[order(mm, decreasing=TRUE)[1]]\n\t\t\t\tmm <- NULL\n\t\t\t\tfor(i in 1:length(uclass[-nclass])) {\n\t\t\t\t\tmm <- c(mm, median(dd[rr3$classification == uclass[-nclass][i],\"ESR1\"], na.rm=TRUE) )\n\t\t\t\t}\n\t\t\t\tnclass <- c(uclass[-nclass][order(mm, decreasing=TRUE)[2]], nclass, uclass[-nclass][order(mm, decreasing=TRUE)[1]])\n\t\t\t\t## nclass contains the new order\n\t\t\t\tncl <- rr3$classification\n\t\t\t\tfor(i in 1:length(uclass)) {\n\t\t\t\t\tncl[rr3$classification == nclass[i]] <- i\n\t\t\t\t}\n\t\t\t\t## use the previously computed model to fit a new model in a supervised manner\n\t\t\t\tmyclass <- mclust::unmap(ncl)\n\t\t\t\tdimnames(myclass) <-  list(dimnames(dd)[[1]], sbtn)\n\t\t\t\tmclust.tr <- mclust::mstep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], z=myclass)\n\t\t\t\tdimnames(mclust.tr$z) <- dimnames(myclass)\n\t\t\t\temclust.tr <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=mclust.tr$parameters)\n\t\t\t\tdimnames(emclust.tr$z) <- dimnames(myclass)\n\t\t\t\tclass.tr <- mclust::map(emclust.tr$z, warn=FALSE)\n\t\t\t\tnames(class.tr) <- dimnames(dd)[[1]]\n\t\t\t\t## prediction strength\n\t\t\t\tps.res <- ps.cluster(cl.tr=class.ts, cl.ts=class.tr, na.rm=TRUE)\n\t\t\t\tnames(ps.res$ps.cluster) <- sbtn\n\t\t\t\t## check for missing values in ps.individual\n\t\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\t\ttt2[names(ps.res$ps.individual)] <- ps.res$ps.individual\n\t\t\t\tps.res$ps.individual <- tt2\n\t\t\t\t## prediction strength with the separation in high and low proliferative tumors\n\t\t\t\t## since proliferation is a continuum we fit a Gaussian using AURKA expression of the ER+/HER2- tumors\n\t\t\t\t## refitted model\n\t\t\t\ttt <- mclust::Mclust(dd[complete.cases(class.tr, dd[ , \"AURKA\"]) & class.tr == 3, \"AURKA\"], modelNames=\"E\", G=1)\n\t\t\t\tgauss.prolif <- c(\"mean\"=tt$parameters$mean, \"sigma\"=tt$parameters$variance$sigmasq)\n\t\t\t\tclass.tr2 <- class.tr\n\t\t\t\tclass.tr2[class.tr == 3] <- NA\n\t\t\t\t## probability that tumor is highly proliferative\n\t\t\t\tpprolif <- pnorm(q=dd[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t\t\t\t## high proliferation\n\t\t\t\tclass.tr2[class.tr == 3 & pprolif >= 0.5 & complete.cases(class.tr, pprolif)] <- 3\n\t\t\t\t## low proliferation\n\t\t\t\tclass.tr2[class.tr == 3 & pprolif < 0.5 & complete.cases(class.tr, pprolif)] <- 4\n\t\t\t\t## existing model\n\t\t\t\ttt <- mclust::Mclust(dd[complete.cases(class.ts, dd[ , \"AURKA\"]) & class.ts == 3, \"AURKA\"], modelNames=\"E\", G=1)\n\t\t\t\tgauss.prolif <- c(\"mean\"=tt$parameters$mean, \"sigma\"=tt$parameters$variance$sigmasq)\n\t\t\t\tclass.ts2 <- class.ts\n\t\t\t\tclass.ts2[class.ts == 3] <- NA\n\t\t\t\t## probability that tumor is highly proliferative\n\t\t\t\tpprolif <- pnorm(q=dd[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t\t\t\t## high proliferation\n\t\t\t\tclass.ts2[class.ts == 3 & pprolif >= 0.5 & complete.cases(class.ts, pprolif)] <- 3\n\t\t\t\t## low proliferation\n\t\t\t\tclass.ts2[class.ts == 3 & pprolif < 0.5 & complete.cases(class.ts, pprolif)] <- 4\n\t\t\t\t## compute the prediction strength\n\t\t\t\tps.res2 <- ps.cluster(cl.tr=class.ts2, cl.ts=class.tr2, na.rm=TRUE)\n\t\t\t\tnames(ps.res2$ps.cluster) <- sbtn2\n\t\t\t\t## check for missing values in ps.individual\n\t\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\t\ttt2[names(ps.res2$ps.individual)] <- ps.res2$ps.individual\n\t\t\t\tps.res2$ps.individual <- tt2\n\t\t\t}\n\t\t}\n\t}\n\n\tBIC.res <- NULL\n\tif(do.BIC) {\n\t\tif(nrow(dd) >= 10) { BIC.res <- mclust::mclustBIC(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=c(model.name), G=1:10)[ ,model.name] } else { warning(\"at least 10 observations are required to compute the BIC!\") }\n\t}\n\n\t## subtypes\n\tsbt[names(class.ts)] <- sbtn[class.ts]\n\tsbt.proba[dimnames(emclust.ts$z)[[1]], ] <- emclust.ts$z\n\t## discriminate between luminal A and B using AURKA\n\tgauss.prolif <- cc\n\tsbt2 <- sbt\n\tsbt2[sbt == sbtn[3]] <- NA\n\t## probability that tumor is highly proliferative\n\tpprolif <- pnorm(q=dd2[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t## high proliferation\n\tsbt2[sbt == sbtn[3] & pprolif >= 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[3]\n\t## low proliferation\n\tsbt2[sbt == sbtn[3] & pprolif < 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[4]\n\t## subtype probabilities for luminal B and A\n\tsbt.proba2 <- matrix(NA, nrow(data), ncol=length(sbtn2), dimnames=list(dimnames(data)[[1]], sbtn2))\n\ttt <- sbt.proba[ , sbtn[3]]\n\ttt2 <- pprolif\n\ttt <- cbind(tt * tt2, tt * (1 - tt2))\n\tcolnames(tt) <- sbtn2[3:4]\n\tsbt.proba2[ , sbtn2[1:2]] <- sbt.proba[ , sbtn[1:2]]\n\tsbt.proba2[ , sbtn2[3:4]] <- tt[ , sbtn2[3:4]]\n\n\tif(plot) {\n\t\tif(do.scale) {\n\t\t\tmyxlim <- myylim <- c(-2, 2)\n\t\t} else {\n\t\t\tmyxlim <- range(dd[ , \"ESR1\"])\n\t\t\tmyylim <- range(dd[ , \"ERBB2\"])\n\t\t}\n\t\t## plot the clusters with proliferation\n\t\tmycol <- mypch <- rep(NA, length(sbt2))\n\t\tmycol[sbt2 == sbtn2[1]] <- \"darkred\"\n\t\tmycol[sbt2 == sbtn2[2]] <- \"darkgreen\"\n\t\tmycol[sbt2 == sbtn2[3]] <- \"darkorange\"\n\t\tmycol[sbt2 == sbtn2[4]] <- \"darkviolet\"\n\t\tmypch[sbt2 == sbtn2[1]] <- 17\n\t\tmypch[sbt2 == sbtn2[2]] <- 0\n\t\tmypch[sbt2 == sbtn2[3] | sbt2 == sbtn2[4]] <- 10\n\t\tmypch <- as.numeric(mypch)\n\t\tnames(mycol) <- names(mypch) <- names(sbt2)\n\t\tplot(x=dd[ , \"ESR1\"], y=dd[ , \"ERBB2\"], xlim=myxlim, ylim=myylim, xlab=\"ESR1\", ylab=\"ERBB2\", col=mycol[dimnames(dd)[[1]]], pch=mypch[dimnames(dd)[[1]]])\n\t\tlegend(x=\"topleft\", col=c(\"darkred\", \"darkgreen\", \"darkorange\", \"darkviolet\"), legend=sbtn2, pch=c(17, 0, 10, 10), bty=\"n\")\n\t}\n\n\treturn(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt2, \"subtype.proba2\"=sbt.proba2, \"prediction.strength2\"=ps.res2, \"module.scores\"=dd2, \"mapping\"=mymap))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `subtype.cluster.predict` function?",
        "answer": "The main purpose of the `subtype.cluster.predict` function is to identify breast cancer molecular subtypes using a Subtype Clustering Model. It takes a pre-fitted model, gene expression data, and annotations as input, and returns predicted subtypes along with various related statistics and probabilities."
      },
      {
        "question": "How does the function handle the distinction between high and low proliferative tumors within the ER+/HER2- subtype?",
        "answer": "The function uses the AURKA gene expression to distinguish between high and low proliferative tumors within the ER+/HER2- subtype. It fits a Gaussian distribution to the AURKA expression of ER+/HER2- tumors and uses a threshold of 0.5 on the cumulative distribution function to classify tumors as either 'ER+/HER2- High Prolif' or 'ER+/HER2- Low Prolif'."
      },
      {
        "question": "What are the key components returned by the `subtype.cluster.predict` function?",
        "answer": "The function returns a list containing several key components: 'subtype' (predicted subtypes), 'subtype.proba' (probabilities for each subtype), 'subtype2' (subtypes including proliferation status for ER+/HER2-), 'subtype.proba2' (probabilities for subtypes including proliferation status), 'prediction.strength' (if requested), 'BIC' (Bayesian Information Criterion, if requested), 'module.scores' (ESR1, ERBB2, and AURKA module scores), and 'mapping' (probe mapping information)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE, mapping,\n    do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n{\n    if(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n    sbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n    sbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\n    if(is.list(sbt.model)) {\n        subtype.c <- sbt.model[!is.element(names(sbt.model), c(\"cutoff.AURKA\", \"mod\"))]\n        model.name <- subtype.c$parameters$variance$modelName\n        cc <- sbt.model$gaussian.AURKA\n        mq <- sbt.model$rescale.q\n        m.mod <- sbt.model$mod\n    } else {\n        # Code for reading model file\n    }\n\n    # More code...\n\n}",
        "complete": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE, mapping,\n    do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n{\n    if(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n    sbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n    sbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\n    if(is.list(sbt.model)) {\n        subtype.c <- sbt.model[!is.element(names(sbt.model), c(\"cutoff.AURKA\", \"mod\"))]\n        model.name <- subtype.c$parameters$variance$modelName\n        cc <- sbt.model$gaussian.AURKA\n        mq <- sbt.model$rescale.q\n        m.mod <- sbt.model$mod\n    } else {\n        rr <- readLines(con=sbt.model, n=11)[-1]\n        nn <- unlist(lapply(X=rr, FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[1], split=\" \")); x <- x[length(x)]; return(x); }))\n        m.param <- c(list(rr[[1]]), lapply(X=rr[-1], FUN=function(x) { x <- as.numeric(unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \"))); x <- x[!is.na(x)]; return(x)}))\n        names(m.param) <- nn\n        cn <- unlist(lapply(strsplit(nn[grep(pattern=\"mean\", x=nn)], split=\"[.]\"), FUN=function(x) { return(x[[2]]) }))\n        m.mod <- read.m.file(sbt.model, comment.char=\"#\")\n        subtype.c <- NULL\n        tt <- m.param$pro\n        names(tt) <- cn\n        subtype.c$parameters$pro <- tt\n        tt <- sapply(X=m.param[grep(pattern=\"mean\", x=nn)], FUN=function(x) { return(x) })\n        dimnames(tt) <- list(names(m.mod)[1:2], cn)\n        subtype.c$parameters$mean <- tt\n        subtype.c$parameters$variance$modelName <- model.name <- m.param$modelname\n        subtype.c$parameters$variance$d <- 2\n        subtype.c$parameters$variance$G <- 3\n        tt <- matrix(0, ncol=2, nrow=2, dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2]))\n        diag(tt) <- m.param$sigma\n        subtype.c$parameters$variance$sigma <- array(tt, dim=c(2,2,3), dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2], cn))\n        subtype.c$parameters$variance$Sigma <- tt\n        subtype.c$parameters$variance$scale <- m.param$scale\n        subtype.c$parameters$variance$shape <- m.param$shape\n        cc <- c(\"mean\"=m.param$gaussian.AURKA.mean, \"sigma\"=m.param$gaussian.AURKA.sigma)\n        mq <- m.param$rescale.q\n    }\n\n    # Rest of the function implementation...\n\n    return(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt2, \"subtype.proba2\"=sbt.proba2, \"prediction.strength2\"=ps.res2, \"module.scores\"=dd2, \"mapping\"=mymap))\n}"
      },
      {
        "partial": "sigs.esr1 <- sig.score(x=m.mod$ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.erbb2 <- sig.score(x=m.mod$ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.aurka <- sig.score(x=m.mod$AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\n# Signature scores\ndd <- cbind(\"ESR1\"=sigs.esr1$score, \"ERBB2\"=sigs.erbb2$score, \"AURKA\"=sigs.aurka$score)\n\n# Mapping\nmymap <- list(\"ESR1\"=sigs.esr1$probe, \"ERBB2\"=sigs.erbb2$probe, \"AURLA\"=sigs.aurka$probe)\ncln <- dimnames(subtype.c$parameters$mean)[[2]] <- as.character(1:ncol(subtype.c$parameters$mean))\n\nif(do.scale) {\n    dd <- apply(dd, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n}\nrownames(dd) <- rownames(data)\ndd2 <- dd\n\n# More code...",
        "complete": "sigs.esr1 <- sig.score(x=m.mod$ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.erbb2 <- sig.score(x=m.mod$ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.aurka <- sig.score(x=m.mod$AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\n# Signature scores\ndd <- cbind(\"ESR1\"=sigs.esr1$score, \"ERBB2\"=sigs.erbb2$score, \"AURKA\"=sigs.aurka$score)\n\n# Mapping\nmymap <- list(\"ESR1\"=sigs.esr1$probe, \"ERBB2\"=sigs.erbb2$probe, \"AURLA\"=sigs.aurka$probe)\ncln <- dimnames(subtype.c$parameters$mean)[[2]] <- as.character(1:ncol(subtype.c$parameters$mean))\n\nif(do.scale) {\n    dd <- apply(dd, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n}\nrownames(dd) <- rownames(data)\ndd2 <- dd\n\ncc.ix <- complete.cases(dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE])\nif(all(!cc.ix)) {\n    ps.res <- ps.res2 <- BIC.res <- NULL\n    if(do.prediction.strength) {\n        tt <- rep(NA, length(sbtn))\n        names(tt) <- sbtn\n        tt2 <- rep(NA, length(sbtn2))\n        names(tt2) <- sbtn2\n        ps.res <- list(\"ps\"=NA, \"ps.cluster\"=tt, \"ps.individual\"=sbt)\n        ps.res2 <- list(\"ps\"=NA, \"ps.cluster\"=tt2, \"ps.individual\"=sbt)\n    }\n    if(do.BIC) {\n        BIC.res <- rep(NA, 10)\n        names(BIC.res) <- 1:10\n    }\n    return(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt, \"prediction.strength2\"=ps.res2))\n}\ndd <- dd[cc.ix, , drop=FALSE]\n\nemclust.ts <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=subtype.c$parameters)\ndimnames(emclust.ts$z) <- list(dimnames(dd)[[1]], cln)\nclass.ts <- mclust::map(emclust.ts$z, warn=FALSE)\nnames(class.ts) <- dimnames(dd)[[1]]\nuclass <- sort(unique(class.ts))\nuclass <- uclass[!is.na(uclass)]\n\n# Rest of the function implementation..."
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/intrinsic.cluster.predict.R",
    "language": "R",
    "content": "#' @title Function to identify breast cancer molecular subtypes using the Single\n#'   Sample Predictor (SSP)\n#'\n#' @description\n#' This function identifies the breast cancer molecular subtypes using a Single\n#'   Sample Predictor (SSP) fitted by intrinsic.cluster.\n#'\n#' @usage\n#' intrinsic.cluster.predict(sbt.model, data, annot, do.mapping = FALSE,\n#'   mapping, do.prediction.strength = FALSE, verbose = FALSE)\n#'\n#' @param sbt.model Subtype Clustering Model as returned by intrinsic.cluster.\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot    Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed\n#'   (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#    mapping such that the probes are not selected based on their variance.\n#' @param do.prediction.strength TRUE if the prediction strength must be computed\n#'   (Tibshirani and Walther 2005), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - subtype: Subtypes identified by the SSP. For published intrinsic gene lists, subtypes\n#'   can be either \"Basal\", \"Her2\", \"LumA\", \"LumB\" or \"Normal\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated from the\n#' correlations to each centroid.\n#' - cor: Correlation coefficient to each centroid.\n#' - prediction.strength: Prediction strength for subtypes.\n#' - subtype.train: Classification (similar to subtypes) computed during fitting of\n#'   the model for prediction strength.\n#' - centroids.map: Mapped probes from the intrinsic gene list used to compute the\n#'   centroids.\n#' - profiles: Intrinsic gene expression profiles for each sample.\n#'\n#' @references\n#' T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and A. Nobel and\n#'   S. Deng and H. Johnsen and R. Pesich and S. Geister and J. Demeter and C. Perou and\n#'   P. E. Lonning and P. O. Brown and A. L. Borresen-Dale and D. Botstein (2003) \"Repeated\n#'   Observation of Breast Tumor Subtypes in Independent Gene Expression Data Sets\",\n#'   Proceedings of the National Academy of Sciences, 1(14):8418\u20138423\n#' \n#' Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and Qaqish,\n#'   Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and Dressler, Lynn and\n#'   Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer, Lynda and Wu, Junyuan and\n#'   Liu, Yudong and Nanda, Rita and Tretiakova, Maria and Orrico, Alejandra and Dreher,\n#'   Donna and Palazzo, Juan and Perreard, Laurent and Nelson, Edward and Mone, Mary and\n#'   Hansen, Heidi and Mullins, Michael and Quackenbush, John and Ellis, Matthew and Olopade,\n#'   Olufunmilayo and Bernard, Philip and Perou, Charles (2006) \"The molecular portraits of\n#'   breast tumors are conserved across microarray platforms\", BMC Genomics, 7(96)\n#' \n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, Samuel and\n#'   Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, Christiane and He,\n#'   Xiaping and Hu, Zhiyuan and Quackenbush, John F. and Stijleman, Inge J. and Palazzo,\n#'   Juan and Marron, J.S. and Nobel, Andrew B. and Mardis, Elaine and Nielsen, Torsten O.\n#'   and Ellis, Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009) \"Supervised\n#'   Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\", Journal of Clinical\n#'   Oncology, 27(8):1160\u20131167\n#' \n#' Tibshirani R and Walther G (2005) \"Cluster Validation by Prediction Strength\",\n#'   Journal of Computational and Graphical Statistics, 14(3):511\u2013528\n#'\n#' @seealso\n#' [genefu::intrinsic.cluster], [genefu::ssp2003], [genefu::ssp2006], [genefu::pam50]\n#'\n#' @examples\n#' # load SSP fitted in Sorlie et al. 2003\n#' data(ssp2003)\n#' # load NKI data\n#' data(nkis)\n#' # SSP2003 applied on NKI\n#' ssp2003.nkis <- intrinsic.cluster.predict(sbt.model=ssp2003,\n#'   data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   do.prediction.strength=FALSE, verbose=TRUE)\n#' table(ssp2003.nkis$subtype)\n#'\n#' @md\n#' @export\n#' @name intrinsic.cluster.predict\nintrinsic.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE,\n                                      mapping, do.prediction.strength=FALSE,\n                                      verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(sbt.model)) { stop(\"data, annot and sbt.mod parameters must be specified\") }\n    if (!is.matrix(data)) { data <- as.matrix(data) }\n\n    if(is.list(sbt.model)) {\n        ## retrieve model\n        centroids <- sbt.model$centroids\n        annot.centroids <- sbt.model$centroids.map\n        method.cor <- sbt.model$method.cor\n        method.centroids <- sbt.model$method.centroids\n        std <- sbt.model$std\n        mq <- sbt.model$rescale.q\n        mins <- sbt.model$mins\n    } else {\n        ## read model file\n        ## retrieve centroids\n        annot.centroids <- read.csv(sbt.model, comment.char=\"#\", stringsAsFactors=FALSE)\n        dimnames(annot.centroids)[[1]] <- annot.centroids[ , \"probe\"]\n        ## retrieve model parameters\n        rr <- readLines(con=sbt.model)[-1]\n        rr <- rr[sapply(rr, function(x) { return(substr(x=x, start=1, stop=1) == \"#\") })]\n        nn <- unlist(lapply(X=rr, FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[1], split=\" \")); x <- x[length(x)]; return(x); }))\n        rr2 <- unlist(lapply(X=rr[is.element(nn, c(\"method.cor\", \"method.centroids\", \"std\", \"rescale.q\", \"mins\"))], FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \")); x <- x[length(x)]; return(x); }))\n        method.cor <- rr2[nn == \"method.cor\"]\n        method.centroids <- rr2[nn == \"method.centroids\"]\n        std <- rr2[nn == \"std\"]\n        mq <- as.numeric(rr2[nn == \"rescale.q\"])\n        mins <- as.numeric(rr2[nn == \"mins\"])\n        rr <- rr[!is.element(nn, c(\"method.cor\", \"method.centroids\", \"std\", \"rescale.q\", \"mins\"))]\n        nn <- nn[!is.element(nn, c(\"method.cor\", \"method.centroids\", \"std\", \"rescale.q\", \"mins\"))]\n        cent <- lapply(X=rr, FUN=function(x) { x <- as.numeric(unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \"))); x <- x[!is.na(x)]; return(x)})\n        centroids <- NULL\n        for(i in 1:length(cent)) { centroids <- cbind(centroids, cent[[i]]) }\n        nn <- unlist(lapply(X=strsplit(x=nn, split=\"centroid.\"), FUN=function(x) { return(x[[2]]) }))\n        dimnames(centroids) <- list(dimnames(annot.centroids)[[1]], nn)\n    }\n\n    number.cluster <- ncol(centroids)\n    if(is.null(dimnames(centroids)[[2]])) { \n        name.cluster <- paste(\"cluster\", 1:ncol(centroids), sep=\".\") \n    } else { \n        name.cluster <- dimnames(centroids)[[2]] \n    }\n\n    gt <- nrow(centroids)\n    #mapping\n    if(do.mapping) {\n        #mapping through EntrezGene.ID\n        #remove (arbitrarily) duplicated gene ids\n        centroids.gid <- as.character(annot.centroids[, \"EntrezGene.ID\"])\n        names(centroids.gid) <- as.character(annot.centroids[, \"probe\"])\n        myx <- !duplicated(centroids.gid) & !is.na(centroids.gid)\n        centroids.gid <- centroids.gid[myx]\n        annot.centroids <- annot.centroids[myx, , drop=FALSE]\n        centroids <- centroids[myx, , drop=FALSE]\n        gid <- as.character(annot[ ,\"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[, \"probe\"])\n        if (missing(mapping)) { ## select the most variant probes using annotations\n            ## if multiple probes for one gene, keep the most variant\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=centroids.gid, verbose=FALSE)\n            nn <- match(rr$geneid2, centroids.gid)\n            nn <- nn[!is.na(nn)]\n            centroids.gid <- centroids.gid[nn]\n            annot.centroids <- annot.centroids[nn, ]\n            centroids <- centroids[nn, , drop=FALSE]\n            data <- rr$data1\n        } else { # use a predefined mapping\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            # keep only centroids genes with mapped probes\n            myx <- is.element(centroids.gid, nn)\n            centroids.gid <- centroids.gid[myx]\n            annot.centroids <- annot.centroids[myx, , drop=FALSE]\n            centroids <- centroids[myx, , drop=FALSE]\n            pp <- as.character(mapping[match(centroids.gid, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            centroids.gid <- centroids.gid[myx]\n            annot.centroids <- annot.centroids[myx, , drop=FALSE]\n            centroids <- centroids[myx, , drop=FALSE]\n            pp <- pp[myx]\n            data <- data[ , pp, drop=FALSE]\n        }\n    }\n    else {\n        if(all(!is.element(dimnames(data)[[2]], dimnames(centroids)[[1]]))) { stop(\"no probe in common -> annot or mapping parameters are necessary for the mapping process!\") }\n        ## no mapping are necessary\n        myx <- intersect(dimnames(centroids)[[1]], dimnames(data)[[2]])\n        data <- data[ ,myx, drop=FALSE]\n        centroids <- centroids[myx, , drop=FALSE]\n    }\n    centroids.map <- cbind(\"probe\"=dimnames(data)[[2]], \"probe.centroids\"=dimnames(centroids)[[1]], \"EntrezGene.ID\"=as.character(annot[dimnames(data)[[2]], \"EntrezGene.ID\"]))\n    dimnames(centroids.map)[[1]] <- dimnames(data)[[2]]\n    gm <- nrow(centroids)\n    if(gm == 0 || (sum(is.na(data)) / length(data)) > 0.9) { ## no mapping or too many missing values\n        ncl <- rep(NA, nrow(data))\n        names(ncl) <- dimnames(data)[[1]]\n        nproba <- ncor <- matrix(NA, nrow=nrow(data), ncol=ncol(centroids), dimnames=list(dimnames(data)[[1]], name.cluster))\n        ps.res <- NULL\n        if(do.prediction.strength) { ps.res <- list(\"ps\"=NA, \"ps.cluster\"=ncor[ , 1], \"ps.individual\"=ncl) }\n        tt <- matrix(NA, ncol=nrow(centroids.map), nrow=nrow(data), dimnames=list(dimnames(data)[[1]], dimnames(centroids.map)[[1]]))\n        return(list(\"subtype\"=ncl, \"subtype.proba\"=nproba, \"cor\"=ncor, \"prediction.strength\"=ps.res, \"centroids.map\"=centroids.map, \"profiles\"=tt))\n    }\n    if(verbose) { message(sprintf(\"%i/%i probes are used for clustering\", gm, gt)) }\n\n    #standardization of the gene expressions\n    switch(std,\n    \"scale\"={\n        data <- scale(data, center=TRUE, scale=TRUE)\n        if(verbose) { message(\"standardization of the gene expressions\") }\n    },\n    \"robust\"={\n        data <- apply(data, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n        if(verbose) { message(\"robust standardization of the gene expressions\") }\n    },\n    \"none\"={ if(verbose) { message(\"no standardization of the gene expressions\") } })\n\n    ## apply the nearest centroid classifier to classify the samples again\n    ncor <- t(apply(X=data, MARGIN=1, FUN=function(x, y, method.cor) {\n      rr <- array(NA, dim=ncol(y), dimnames=list(colnames(y)))\n    if (sum(complete.cases(x, y)) > 3) {\n      rr <- cor(x=x, y=y, method=method.cor, use=\"complete.obs\")\n    }\n    return (rr)\n  }, y=centroids, method.cor=method.cor))\n    #nproba <- t(apply(X=ncor, MARGIN=1, FUN=function(x) { return(abs(x) / sum(abs(x), na.rm=TRUE)) }))\n    ## negative correlations are truncated to zero since they have no meaning for subtypes identification\n    nproba <- t(apply(X=ncor, MARGIN=1, FUN=function (x) {\n    rr <- array(NA, dim=length(x), dimnames=list(names(x)))\n    x[!is.na(x) & x < 0] <- 0\n    if (!all(is.na(x))) {\n      rr <- x / sum(x, na.rm=TRUE)\n    }\n    return (rr)\n  }))\n    dimnames(ncor) <- dimnames(nproba) <- list(dimnames(data)[[1]], name.cluster)\n    ncl <- apply(X=ncor, MARGIN=1, FUN=function(x) { return(order(x, decreasing=TRUE, na.last=TRUE)[1]) })\n    names(ncl) <- dimnames(data)[[1]]\n    ## names of identified clusters\n    ncln <- name.cluster[ncl]\n    names(ncln) <- dimnames(data)[[1]]\n\n    ## if one or more subtypes have not been identified, remove them for prediction strength\n    myx <- sort(unique(ncl))\n    myx <- myx[!is.na(myx)]\n    name.cluster2 <- name.cluster[myx]\n    number.cluster2 <- length(myx)\n\n    ps.res <- ncl2 <- NULL\n    if(do.prediction.strength) {\n        ## compute the clustering and cut the dendrogram\n        ## hierarchical clustering with correlation-based distance and average linkage\n        hcl <- amap::hcluster(x=data, method=\"correlation\", link=\"average\")\n        mins.ok <- stop.ok <- FALSE\n        nbc <- number.cluster2\n        nclust.best <- 1\n        while(!mins.ok && !stop.ok) { ## until each cluster contains at least mins samples\n            cl <- cutree(tree=hcl, k=nbc)\n            tt <- table(cl)\n            if(sum(tt >= mins) >= number.cluster2) {\n                if(nbc > number.cluster2) { ## put NA for clusters with less than mins samples\n                    td <- names(tt)[tt < mins]\n                    cl[is.element(cl, td)] <- NA\n                    ## rename the clusters\n                    ucl <- sort(unique(cl))\n                    ucl <- ucl[!is.na(ucl)]\n                    cl2 <- cl\n                    for(i in 1:number.cluster2) { cl2[cl == ucl[i] & !is.na(cl)] <- i }\n                    cl <- cl2\n                }\n                nclust.best <- number.cluster2\n                mins.ok <- TRUE\n            } else {\n                if(sum(tt >= mins) > nclust.best) {\n                    nbc.best <- nbc\n                    nclust.best <- sum(tt >= mins)\n                }\n                nbc <- nbc + 1\n                if(nbc > (nrow(data) - (number.cluster2 * mins))) {\n                    warning(sprintf(\"impossible to find %i main clusters with at least %i individuals!\", number.cluster2, mins))\n                    stop.ok <- TRUE\n                }\n            }\n            if(stop.ok) { ## no convergence for the clustering with mininmum set of individuals\n                cl <- cutree(tree=hcl, k=nbc.best)\n                tt <- table(cl)\n                td <- names(tt)[tt < mins]\n                cl[is.element(cl, td)] <- NA\n                ## rename the clusters\n                ucl <- sort(unique(cl))\n                ucl <- ucl[!is.na(ucl)]\n                cl2 <- cl\n                for(i in 1:nclust.best) { cl2[cl == ucl[i] & !is.na(cl)] <- i }\n                cl <- cl2\n            }\n        }\n        ## compute the centroids\n        ## take the core samples in each cluster to compute the centroid\n        ## not feasible due to low intra correlation within clusters!!!\n        ## minimal pairwise cor of approx 0.3\n        #cl2 <- cutree(tree=hcl, h=0.7)\n        #table(cl, cl2) to detect which core cluster of samples for which cluster.\n        cl.centroids <- matrix(NA, nrow=ncol(data), ncol=nclust.best, dimnames=list(dimnames(data)[[2]], paste(\"cluster\", 1:nclust.best, sep=\".\")))\n        for(i in 1:ncol(cl.centroids)) {\n            switch(method.centroids,\n            \"mean\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], MARGIN=2, FUN=mean, na.rm=TRUE, trim=0.025) },\n            \"median\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], MARGIN=2, FUN=median, na.rm=TRUE) },\n            \"tukey\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], MARGIN=2, FUN=tbrm, na.rm=TRUE, C=9) })\n        }\n        #apply the nearest centroid classifier to classify the samples again\n        ncor2 <- t(apply(X=data, MARGIN=1, FUN=function(x, y, z) { return(cor(x, y, method=z, use=\"complete.obs\")) }, y=cl.centroids, z=method.cor))\n        nproba2 <- t(apply(X=ncor2, MARGIN=1, FUN=function(x) { return(abs(x) / sum(abs(x), na.rm=TRUE)) }))\n        dimnames(ncor2) <- dimnames(nproba2) <- list(dimnames(data)[[1]], dimnames(cl.centroids)[[2]])\n        ncl2 <- apply(X=ncor2, MARGIN=1, FUN=function(x) { return(order(x, decreasing=TRUE)[1]) })\n        names(ncl2) <- dimnames(data)[[1]]\n        ## rename clusters since we do not expect to get the same id per cluster\n        ## this avoids a warning in ps.cluster\n        uncl <- sort(unique(ncl))\n        uncl <- uncl[!is.na(uncl)]\n        nclt <- ncl\n        for(mm in 1:length(uncl)) {\n            nclt[ncl == uncl[mm]] <- mm\n        }\n        uncl2 <- sort(unique(ncl2))\n        uncl2 <- uncl2[!is.na(uncl2)]\n        ncl2t <- ncl2\n        for(mm in 1:length(uncl2)) {\n            ncl2t[ncl2 == uncl2[mm]] <- mm\n        }\n        #prediction strength\n        ps.res <- ps.cluster(cl.tr=ncl2t, cl.ts=nclt, na.rm=TRUE)\n        ## put NA for clusters which are potentially not present in the dataset\n        tt <- rep(NA, length(name.cluster))\n        names(tt) <- name.cluster\n        tt[name.cluster2] <- ps.res$ps.cluster\n        ps.res$ps.cluster <- tt\n    }\n\n\n    return(list(\"subtype\"=ncln, \"subtype.proba\"=nproba, \"cor\"=ncor, \"prediction.strength\"=ps.res, \"subtype.train\"=ncl2, \"profiles\"=data, \"centroids.map\"=centroids.map))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `intrinsic.cluster.predict` function?",
        "answer": "The primary purpose of the `intrinsic.cluster.predict` function is to identify breast cancer molecular subtypes using a Single Sample Predictor (SSP) fitted by intrinsic.cluster. It takes gene expression data, annotations, and a subtype clustering model as input, and returns predicted subtypes, probabilities, correlations, and other related information."
      },
      {
        "question": "How does the function handle missing data or mapping issues?",
        "answer": "The function handles missing data and mapping issues in several ways: 1) It performs optional mapping through EntrezGene IDs if `do.mapping` is set to TRUE. 2) It selects the most variant probes when multiple probes exist for one gene. 3) If no mapping is possible or if there are too many missing values (>90%), it returns NA values for subtypes, probabilities, and correlations. 4) It provides informative messages about the number of probes used for clustering when `verbose` is set to TRUE."
      },
      {
        "question": "What methods are available for standardizing gene expressions in this function?",
        "answer": "The function offers three methods for standardizing gene expressions, controlled by the `std` parameter: 1) 'scale': uses the `scale` function to center and scale the data. 2) 'robust': applies a robust standardization using the `rescale` function. 3) 'none': no standardization is applied. The chosen method is reported in verbose mode."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/intrinsic.cluster.R",
    "language": "R",
    "content": "#' @title Function to fit a Single Sample Predictor (SSP) as in \n#'   Perou, Sorlie, Hu, and Parker publications\n#'\n#' @description\n#' This function fits the Single Sample Predictor (SSP) as published\n#'   in Sorlie et al 2003, Hu et al 2006 and Parker et al 2009. This model \n#'   is actually a nearest centroid classifier where the centroids\n#'   representing the breast cancer molecular subtypes are identified\n#'   through hierarchical clustering using an \"intrinsic gene list\".\n#'\n#' @usage\n#' intrinsic.cluster(data, annot, do.mapping = FALSE, mapping,\n#'   std = c(\"none\", \"scale\", \"robust\"), rescale.q = 0.05, intrinsicg,\n#'   number.cluster = 3, mins = 5, method.cor = c(\"spearman\", \"pearson\"),\n#'   method.centroids = c(\"mean\", \"median\", \"tukey\"), filen, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot    Matrix of annotations with at least one column named \n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping    TRUE if the mapping through Entrez Gene ids must \n#'   be performed (in case of ambiguities, the most variant probe is kept \n#'   for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to \n#'   force the mapping such that the probes are not selected based on their \n#'   variance.\n#' @param std Standardization of gene expressions: scale for traditional \n#'   standardization based on mean and standard deviation, robust for \n#'   standardization based on the 0.025 and 0.975 quantiles, none to keep \n#'   gene expressions unchanged.\n#' @param rescale.q Proportion of expected outliers for (robust) rescaling \n#'   the gene expressions.\n#' @param intrinsicg Intrinsic gene lists. May be specified by the user as \n#'   a matrix with at least 2 columns named probe and EntrezGene.ID for the \n#'   probe names and the corresponding Entrez Gene ids. The intrinsic gene lists \n#'   published by Sorlie et al. 2003, Hu et al. 2006 and Parker et al. 2009 are \n#'   stored in ssp2003, ssp2006 and pam50 respectively.\n#' @param number.cluster The number of main clusters to be identified by \n#'   hierarchical clustering.\n#' @param mins The minimum number of samples to be in a main cluster.\n#' @param method.cor Correlation coefficient used to identified the nearest\n#'   centroid. May be spearman or pearson.\n#' @param method.centroids Method to compute a centroid from gene expressions \n#'   of a cluster of samples: mean, median or tukey (Tukey's Biweight Robust Mean).\n#' @param filen Name of the csv file where the subtype clustering model must be \n#'   stored.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - model: Single Sample Predictor\n#' - subtype: Subtypes identified by the SSP. For published intrinsic gene lists,\n#'   subtypes can be either \"Basal\", \"Her2\", \"LumA\", \"LumB\" or \"Normal\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated from the\n#'   correlations to each centroid.\n#' - cor: Correlation coefficient to each centroid.\n#'\n#' @references\n#' T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and \n#'   A. Nobel and S. Deng and H. Johnsen and R. Pesich and S. Geister and J. \n#'   Demeter and C. Perou and P. E. Lonning and P. O. Brown and A. L. Borresen-Dale \n#'   and D. Botstein (2003) \"Repeated Observation of Breast Tumor Subtypes in \n#'   Independent Gene Expression Data Sets\", Proceedings of the National Academy of \n#'   Sciences, 1(14):8418-8423\n#' \n#' Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and \n#'   Qaqish, Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and \n#'   ressler, Lynn and Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer,\n#'   Lynda and Wu, Junyuan and Liu, Yudong and Nanda, Rita and Tretiakova, Maria \n#'   and Orrico, Alejandra and Dreher, Donna and Palazzo, Juan and Perreard, \n#'   Laurent and Nelson, Edward and Mone, Mary and Hansen, Heidi and Mullins, \n#'   Michael and Quackenbush, John and Ellis, Matthew and Olopade, Olufunmilayo and \n#'   Bernard, Philip and Perou, Charles (2006) \"The molecular portraits of breast \n#'   tumors are conserved across microarray platforms\", BMC Genomics, 7(96)\n#'\n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, \n#'   Samuel and Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, \n#'   Christiane and He, Xiaping and Hu, Zhiyuan and Quackenbush, John F. and \n#'   Stijleman, Inge J. and Palazzo, Juan and Marron, J.S. and Nobel, Andrew B. \n#'   and Mardis, Elaine and Nielsen, Torsten O. and Ellis, Matthew J. and Perou, \n#'   Charles M. and Bernard, Philip S. (2009) \"Supervised Risk Predictor of Breast \n#'   Cancer Based on Intrinsic Subtypes\", Journal of Clinical Oncology, \n#'   27(8):1160-1167\n#'\n#' @seealso\n#' [genefu::subtype.cluster], [genefu::intrinsic.cluster.predict], [genefu::ssp2003], [genefu::ssp2006], [genefu::pam50]\n#'\n#' @examples\n#' # load SSP signature published in Sorlie et al. 2003\n#' data(ssp2003)\n#' # load NKI data\n#' data(nkis)\n#' # load VDX data\n#' data(vdxs)\n#' ssp2003.nkis <- intrinsic.cluster(data=data.nkis, annot=annot.nkis,\n#'   do.mapping=TRUE, std=\"robust\",\n#'   intrinsicg=ssp2003$centroids.map[ ,c(\"probe\", \"EntrezGene.ID\")],\n#'   number.cluster=5, mins=5, method.cor=\"spearman\",\n#'   method.centroids=\"mean\", verbose=TRUE)\n#' str(ssp2003.nkis, max.level=1)\n#'\n#' @md\n#' @export\nintrinsic.cluster <- function(data, annot, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        intrinsicg, number.cluster=3, mins=5, \n        method.cor=c(\"spearman\", \"pearson\"), \n        method.centroids=c(\"mean\", \"median\", \"tukey\"), filen, verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(intrinsicg)) { \n        stop(\"data, annot, and intrinsicg parameters must be specified\") \n    }\n    std <- match.arg(std)\n    method.cor <- match.arg(method.cor)\n    method.centroids <- match.arg(method.centroids)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        ## mapping through EntrezGene.ID\n        if (is.matrix(intrinsicg) || is.data.frame(intrinsicg)) {\n            ## matrix of annotations instead of a list of EntrezGene.IDs\n            tt <- as.character(intrinsicg[, \"EntrezGene.ID\"])\n            names(tt) <- as.character(intrinsicg[, \"probe\"])\n            intrinsicg <- tt\n        }\n        gt <- length(intrinsicg)\n        ## EntrezGene.IDs should be numeric or character that can be tranformed into numeric\n        ## remove (arbitrarily) duplicated gene ids\n        intrinsicg <- intrinsicg[!duplicated(intrinsicg) & !is.na(intrinsicg)]\n        gid <- as.character(annot[ , \"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[ , \"probe\"])\n        gid.intrinsic <- as.character(intrinsicg)\n        names(gid.intrinsic) <- paste(\"geneid\", gid.intrinsic, sep=\".\")\n        if(missing(mapping)) { # select the most variant probes using annotations\n            # if multiple probes for one gene, keep the most variant\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=gid.intrinsic, verbose=FALSE)\n            nn <- match(rr$geneid2, gid.intrinsic)\n            nn <- nn[!is.na(nn)]\n            intrinsicg <- intrinsicg[nn]\n            data <- rr$data1\n        } else { # use a predefined mapping\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            # keep only intrinsic genes with mapped probes\n            myx <- is.element(gid.intrinsic, nn)\n            gid.intrinsic <- gid.intrinsic[myx]\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- as.character(mapping[match(gid.intrinsic, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- pp[myx]\n            data <- data[, pp, drop=FALSE]\n        }\n    }\n    else {\n        if (is.matrix(intrinsicg) || is.data.frame(intrinsicg)) {\n            ## matrix of annotations instead of a list of EntrezGene.IDs\n            tt <- as.character(intrinsicg[ , \"probe\"])\n            names(tt) <- as.character(intrinsicg[ , \"probe\"])\n            intrinsicg <- tt\n        }\n        gt <- length(intrinsicg)\n        if(all(!is.element(dimnames(data)[[2]], intrinsicg))) { stop(\"no probe in common -> annot or mapping parameters are necessary for the mapping process!\") }\n        ## no mapping are necessary\n        intrinsicg <- intersect(intrinsicg, dimnames(data)[[2]])\n        data <- data[, intrinsicg]\n    }\n    gm <- length(intrinsicg)\n    if (gm == 0 || (sum(is.na(data)) / length(data)) > 0.9) {\n        stop(\"none of the instrinsic genes are present or too many missing values!\")\n    }\n    if (!is.null(names(intrinsicg))) {\n        centroids.map <- cbind(\"probe\"=dimnames(data)[[2]],\n            \"probe.centroids\"=names(intrinsicg),\n            \"EntrezGene.ID\"=as.character(annot[dimnames(data)[[2]],\n            \"EntrezGene.ID\"]))\n    } else {\n        centroids.map <- cbind(\"probe\"=dimnames(data)[[2]],\n            \"probe.centroids\"=NA,\n            \"EntrezGene.ID\"=as.character(annot[dimnames(data)[[2]],\n            \"EntrezGene.ID\"]))\n    }\n    dimnames(centroids.map)[[1]] <- dimnames(data)[[2]]\n\n    if (verbose) {\n        message(sprintf(\"%i/%i probes are used for clustering\", gm, gt))\n    }\n\n    switch(std,\n    \"scale\"={\n        data <- scale(data, center=TRUE, scale=TRUE)\n        if (verbose) message(\"standardization of the gene expressions\")\n    }, \n    \"robust\"={\n        data <- apply(data, 2, function(x) {\n            (rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2\n        })\n        if(verbose) message(\"robust standardization of the gene expressions\")\n    },\n    \"none\"={\n        if(verbose) message(\"no standardization of the gene expressions\")\n    })\n\n    ## compute the clustering and cut the dendrogram\n    ## hierarchical clustering with correlation-based distance and average linkage\n    hcl <- amap::hcluster(x=data, method=\"correlation\", link=\"average\")\n    mins.ok <- FALSE\n    nbc <- number.cluster\n    while (!mins.ok) { ## until each cluster contains at least mins samples\n        cl <- cutree(tree=hcl, k=nbc)\n        tt <- table(cl)\n        if (sum(tt >= mins) >= number.cluster) {\n            if (nbc > number.cluster) { ## put NA for clusters with less than mins samples\n                td <- names(tt)[tt < mins]\n                cl[is.element(cl, td)] <- NA\n                ## rename the clusters\n                ucl <- sort(unique(cl))\n                ucl <- ucl[!is.na(ucl)]\n                cl2 <- cl\n                for (i in 1:number.cluster) { \n                    cl2[cl == ucl[i] & !is.na(cl)] <- i \n                }\n                cl <- cl2\n            }\n            mins.ok <- TRUE\n        } else {\n            nbc <- nbc + 1\n            if (nbc > (nrow(data) - (number.cluster * mins))) {\n                stop(\"clusters are too small (size < mins)!\")\n            }\n        }\n    }\n\n    ## compute the centroids\n    ## take the core samples in each cluster to compute the centroid\n    ## not feasible due to low intra correlation within clusters!!!\n    ## minimal pairwise cor of approx 0.3\n    cl.centroids <- matrix(NA, nrow=ncol(data), ncol=number.cluster,\n        dimnames=list(dimnames(data)[[2]], paste(\"cluster\", 1:number.cluster, sep=\".\")))\n    for (i in 1:ncol(cl.centroids)) {\n        switch(method.centroids,\n        \"mean\"={ \n            cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], \n                MARGIN=2, FUN=mean, na.rm=TRUE, trim=0.025)\n        }, \n        \"median\"={ \n            cl.centroids[, i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], \n                MARGIN=2, FUN=median, na.rm=TRUE)\n        }, \n        \"tukey\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], \n            MARGIN=2, FUN=tbrm, na.rm=TRUE, C=9)\n        })\n    }\n\n    ## apply the nearest centroid classifier to classify the samples again\n    ncor <- t(apply(X=data, MARGIN=1, FUN=function(x, y, z) {\n        return(cor(x, y, method=z, use=\"complete.obs\"))\n    }, y=cl.centroids, z=method.cor))\n    ## negative correlations are truncated to zero since they have no meaning for subtypes identification\n    nproba <- t(apply(X=ncor, MARGIN=1, FUN=function(x) {\n        x[x < 0] <- 0; return(x / sum(x, na.rm=TRUE)); \n    }))\n    dimnames(ncor) <- dimnames(nproba) <- list(dimnames(data)[[1]], dimnames(cl.centroids)[[2]])\n    ncl <- apply(X=ncor, MARGIN=1, FUN=function(x) {\n        order(x, decreasing=TRUE, na.last=TRUE)[1]\n    })\n    ncl <- dimnames(cl.centroids)[[2]][ncl]\n    names(ncl) <- dimnames(data)[[1]]\n\n    if (!missing(filen)) {\n        #save model parameters in a csv file for reuse\n        write(x=sprintf(\"# Benjamin Haibe-Kains. All rights reserved.\"), sep=\"\", file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# method.cor: %s\", method.cor), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# method.centroids: %s\", method.centroids), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# std: %s\", std), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# rescale.q: %s\", rescale.q), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# mins: %i\", mins), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        ## centroids\n        mycent <- t(cl.centroids)\n        for(i in 1:nrow(mycent)) { \n            write(x=sprintf(\"# centroid.%s: %s\", dimnames(mycent)[[1]][i], \n                paste(mycent[i, ], collapse=\" \")), sep=\"\", append=TRUE, \n                file=paste(filen, \"csv\", sep=\".\")) \n        }\n        ## centroids.map\n        write(paste(\"\\\"\", dimnames(centroids.map)[[2]], \"\\\"\", collapse=\",\", sep=\"\"), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write.table(centroids.map, sep=\",\", col.names=FALSE, row.names=FALSE, file=paste(filen, \"csv\", sep=\".\"), append=TRUE)\n    }\n\n    return(list(\n        \"model\"=list(\"method.cor\"=method.cor, \n            \"method.centroids\"=method.centroids, \"std\"=std,\n            \"rescale.q\"=rescale.q,  \"mins\"=mins, \"centroids\"=cl.centroids,\n            \"centroids.map\"=centroids.map),\n        \"subtype\"=ncl,\n        \"subtype.proba\"=nproba,\n        \"cor\"=ncor\n        )\n    )\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `intrinsic.cluster` function?",
        "answer": "The `intrinsic.cluster` function is designed to fit a Single Sample Predictor (SSP) for breast cancer molecular subtyping. It implements a nearest centroid classifier where centroids representing breast cancer molecular subtypes are identified through hierarchical clustering using an 'intrinsic gene list'."
      },
      {
        "question": "How does the function handle gene mapping and what options are available for this process?",
        "answer": "The function offers two options for gene mapping: 1) If `do.mapping=TRUE`, it maps probes to genes using Entrez Gene IDs. By default, it selects the most variant probe for each gene. 2) If a `mapping` parameter is provided, it uses a predefined mapping. The mapping process ensures that the correct probes are used for the intrinsic gene list, handling potential ambiguities in probe-to-gene relationships."
      },
      {
        "question": "What methods are available for computing centroids in this function, and how are they implemented?",
        "answer": "The function offers three methods for computing centroids: 'mean', 'median', and 'tukey' (Tukey's Biweight Robust Mean). These are specified using the `method.centroids` parameter. The chosen method is applied to the gene expression data of samples within each cluster to compute the centroid. For example, if 'mean' is selected, it calculates the average expression for each gene across all samples in a cluster, with a 2.5% trim to remove potential outliers."
      }
    ],
    "completion_tasks": [
      {
        "partial": "intrinsic.cluster <- function(data, annot, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        intrinsicg, number.cluster=3, mins=5, \n        method.cor=c(\"spearman\", \"pearson\"), \n        method.centroids=c(\"mean\", \"median\", \"tukey\"), filen, verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(intrinsicg)) { \n        stop(\"data, annot, and intrinsicg parameters must be specified\") \n    }\n    std <- match.arg(std)\n    method.cor <- match.arg(method.cor)\n    method.centroids <- match.arg(method.centroids)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        # Implement mapping logic here\n    }\n    else {\n        # Implement non-mapping logic here\n    }\n\n    # Implement standardization logic here\n\n    # Implement clustering and centroid computation here\n\n    # Implement nearest centroid classification here\n\n    # Implement file saving logic here\n\n    return(list(\n        \"model\" = list(),\n        \"subtype\" = NULL,\n        \"subtype.proba\" = NULL,\n        \"cor\" = NULL\n    ))\n}",
        "complete": "intrinsic.cluster <- function(data, annot, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        intrinsicg, number.cluster=3, mins=5, \n        method.cor=c(\"spearman\", \"pearson\"), \n        method.centroids=c(\"mean\", \"median\", \"tukey\"), filen, verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(intrinsicg)) { \n        stop(\"data, annot, and intrinsicg parameters must be specified\") \n    }\n    std <- match.arg(std)\n    method.cor <- match.arg(method.cor)\n    method.centroids <- match.arg(method.centroids)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        intrinsicg <- intrinsicg[!duplicated(intrinsicg) & !is.na(intrinsicg)]\n        gid <- as.character(annot[, \"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[, \"probe\"])\n        gid.intrinsic <- as.character(intrinsicg)\n        names(gid.intrinsic) <- paste(\"geneid\", gid.intrinsic, sep=\".\")\n        if(missing(mapping)) {\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=gid.intrinsic, verbose=FALSE)\n            nn <- match(rr$geneid2, gid.intrinsic)\n            nn <- nn[!is.na(nn)]\n            intrinsicg <- intrinsicg[nn]\n            data <- rr$data1\n        } else {\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            myx <- is.element(gid.intrinsic, nn)\n            gid.intrinsic <- gid.intrinsic[myx]\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- as.character(mapping[match(gid.intrinsic, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- pp[myx]\n            data <- data[, pp, drop=FALSE]\n        }\n    } else {\n        intrinsicg <- intersect(intrinsicg, dimnames(data)[[2]])\n        data <- data[, intrinsicg]\n    }\n\n    switch(std,\n    \"scale\"={ data <- scale(data, center=TRUE, scale=TRUE) },\n    \"robust\"={ data <- apply(data, 2, function(x) { (rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2 }) },\n    \"none\"={ })\n\n    hcl <- amap::hcluster(x=data, method=\"correlation\", link=\"average\")\n    cl <- cutree(tree=hcl, k=number.cluster)\n\n    cl.centroids <- matrix(NA, nrow=ncol(data), ncol=number.cluster,\n        dimnames=list(dimnames(data)[[2]], paste(\"cluster\", 1:number.cluster, sep=\".\")))\n    for (i in 1:ncol(cl.centroids)) {\n        cl.centroids[, i] <- switch(method.centroids,\n            \"mean\" = apply(data[cl == i & !is.na(cl), , drop=FALSE], 2, mean, na.rm=TRUE),\n            \"median\" = apply(data[cl == i & !is.na(cl), , drop=FALSE], 2, median, na.rm=TRUE),\n            \"tukey\" = apply(data[cl == i & !is.na(cl), , drop=FALSE], 2, tbrm, na.rm=TRUE, C=9)\n        )\n    }\n\n    ncor <- t(apply(data, 1, function(x, y) cor(x, y, method=method.cor, use=\"complete.obs\"), y=cl.centroids))\n    nproba <- t(apply(ncor, 1, function(x) { x[x < 0] <- 0; x / sum(x, na.rm=TRUE) }))\n    ncl <- apply(ncor, 1, function(x) dimnames(cl.centroids)[[2]][which.max(x)])\n\n    if (!missing(filen)) {\n        write.csv(cl.centroids, file=paste(filen, \"csv\", sep=\".\"), row.names=TRUE)\n    }\n\n    return(list(\n        \"model\" = list(\"method.cor\"=method.cor, \"method.centroids\"=method.centroids, \n                      \"std\"=std, \"rescale.q\"=rescale.q, \"mins\"=mins, \n                      \"centroids\"=cl.centroids),\n        \"subtype\" = ncl,\n        \"subtype.proba\" = nproba,\n        \"cor\" = ncor\n    ))\n}"
      },
      {
        "partial": "intrinsic.cluster.predict <- function(data, annot, model, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(model)) { \n        stop(\"data, annot, and model parameters must be specified\") \n    }\n    std <- match.arg(std)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        # Implement mapping logic here\n    }\n    else {\n        # Implement non-mapping logic here\n    }\n\n    # Implement standardization logic here\n\n    # Implement prediction logic here\n\n    return(list(\n        \"subtype\" = NULL,\n        \"subtype.proba\" = NULL,\n        \"cor\" = NULL\n    ))\n}",
        "complete": "intrinsic.cluster.predict <- function(data, annot, model, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(model)) { \n        stop(\"data, annot, and model parameters must be specified\") \n    }\n    std <- match.arg(std)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        gid <- as.character(annot[, \"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[, \"probe\"])\n        gid.model <- as.character(model$centroids.map[, \"EntrezGene.ID\"])\n        names(gid.model) <- as.character(model$centroids.map[, \"probe\"])\n        if(missing(mapping)) {\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=gid.model, verbose=FALSE)\n            data <- rr$data1\n        } else {\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            pp <- as.character(mapping[match(gid.model, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            pp <- pp[myx]\n            data <- data[, pp, drop=FALSE]\n        }\n    } else {\n        common.probes <- intersect(dimnames(data)[[2]], model$centroids.map[, \"probe\"])\n        data <- data[, common.probes]\n    }\n\n    switch(std,\n    \"scale\"={ data <- scale(data, center=TRUE, scale=TRUE) },\n    \"robust\"={ data <- apply(data, 2, function(x) { (rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2 }) },\n    \"none\"={ })\n\n    ncor <- t(apply(data, 1, function(x, y) cor(x, y, method=model$method.cor, use=\"complete.obs\"), y=model$centroids))\n    nproba <- t(apply(ncor, 1, function(x) { x[x < 0] <- 0; x / sum(x, na.rm=TRUE) }))\n    ncl <- apply(ncor, 1, function(x) colnames(model$centroids)[which.max(x)])\n\n    return(list(\n        \"subtype\" = ncl,\n        \"subtype.proba\" = nproba,\n        \"cor\" = ncor\n    ))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcSigs.R",
    "language": "R",
    "content": "#' import utils\n#' @keywords internal\n#' @noRd\n`.ovcSigs` <-\nfunction(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ## annotations\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            #save(list=\"sigAngiogenic\", compress=TRUE, file=file.path(system.file(package=\"genefu\"), \"data\", \"sigAngiogenic.rda\"))\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigOvcAngiogenic.rda\")\n        },\n        \"crijns2009_sig\"={\n            ss <- \"hgnc_symbol\"\n            gid <- as.character(sig[ ,\"Gene.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"Weights\"]))\n            sigOvcCrijns <- annot\n            save(list=\"sigOvcCrijns\", compress=TRUE, file=\"sigOvcCrijns.rda\")\n        },\n        \"yoshihara2010_sig\"={\n            ss <- \"refseq_mrna\"\n            gid <- as.character(sig[ ,\"GenBank\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot2 <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot2[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot2[ ,-1,drop=FALSE], annot, \"weight\"=as.numeric(sig[ ,\"beta_ridge\"]))\n            sigOvcYoshihara <- annot\n            save(list=\"sigOvcYoshihara\", compress=TRUE, file=\"sigOvcYoshihara.rda\")\n        },\n        \"spentzos2011_sig\"={\n            ss <- \"affy_hg_u133a\"\n            gid <- as.character(sig[ ,\"probeset\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=sig[ ,\"weight\"])\n            sigOvcSpentzos <- annot\n            save(list=\"sigOvcSpentzos\", compress=TRUE, file=\"sigOvcSpentzos.rda\")\n        },\n        \"tcga2011_sig\"={\n            ss <- \"entrezgene\"\n            gid <- as.character(sig[ ,\"Entrez.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(paste(\"geneid\", gid, sep=\".\"), colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, sig[ ,c(\"Gene.set\", \"beta\", \"p.value\")])\n            sigOvcTCGA <- annot\n            save(list=\"sigOvcTCGA\", compress=TRUE, file=\"sigOvcTCGA.rda\")\n        })\n    }\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `.ovcSigs` function in this code snippet?",
        "answer": "The main purpose of the `.ovcSigs` function is to process and annotate different ovarian cancer gene signatures. It reads signature data from CSV files, retrieves gene annotations from the Ensembl database using biomaRt, and saves the annotated signatures as RDA files. The function handles five different signatures: bentink2012_angiogenic, crijns2009_sig, yoshihara2010_sig, spentzos2011_sig, and tcga2011_sig."
      },
      {
        "question": "How does the function handle different gene identifiers for each signature?",
        "answer": "The function uses different gene identifiers for each signature by setting the `ss` variable to the appropriate identifier type. For example, it uses 'illumina_humanwg_6_v2' for bentink2012_angiogenic, 'hgnc_symbol' for crijns2009_sig, 'refseq_mrna' for yoshihara2010_sig, 'affy_hg_u133a' for spentzos2011_sig, and 'entrezgene' for tcga2011_sig. This allows the function to correctly map the gene identifiers to the Ensembl database for annotation."
      },
      {
        "question": "What is the significance of the `switch` statement in this function?",
        "answer": "The `switch` statement in this function is crucial for handling the different signature types. It allows the function to execute specific code blocks for each signature, accommodating their unique data structures and annotation requirements. This design makes the function flexible and capable of processing multiple signature types within a single function, while keeping the code organized and maintainable."
      }
    ],
    "completion_tasks": [
      {
        "partial": "`.ovcSigs` <- function(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigOvcAngiogenic.rda\")\n        },\n        # Complete the switch statement for other signature types\n        )\n    }\n}",
        "complete": "`.ovcSigs` <- function(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigOvcAngiogenic.rda\")\n        },\n        \"crijns2009_sig\"={\n            ss <- \"hgnc_symbol\"\n            gid <- as.character(sig[ ,\"Gene.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"Weights\"]))\n            sigOvcCrijns <- annot\n            save(list=\"sigOvcCrijns\", compress=TRUE, file=\"sigOvcCrijns.rda\")\n        },\n        \"yoshihara2010_sig\"={\n            ss <- \"refseq_mrna\"\n            gid <- as.character(sig[ ,\"GenBank\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot2 <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot2[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot2[ ,-1,drop=FALSE], annot, \"weight\"=as.numeric(sig[ ,\"beta_ridge\"]))\n            sigOvcYoshihara <- annot\n            save(list=\"sigOvcYoshihara\", compress=TRUE, file=\"sigOvcYoshihara.rda\")\n        },\n        \"spentzos2011_sig\"={\n            ss <- \"affy_hg_u133a\"\n            gid <- as.character(sig[ ,\"probeset\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=sig[ ,\"weight\"])\n            sigOvcSpentzos <- annot\n            save(list=\"sigOvcSpentzos\", compress=TRUE, file=\"sigOvcSpentzos.rda\")\n        },\n        \"tcga2011_sig\"={\n            ss <- \"entrezgene\"\n            gid <- as.character(sig[ ,\"Entrez.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(paste(\"geneid\", gid, sep=\".\"), colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, sig[ ,c(\"Gene.set\", \"beta\", \"p.value\")])\n            sigOvcTCGA <- annot\n            save(list=\"sigOvcTCGA\", compress=TRUE, file=\"sigOvcTCGA.rda\")\n        })\n    }\n}"
      },
      {
        "partial": "`.ovcSigs` <- function(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigO"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/methods-computeLimmaDiffExpr.R",
    "language": "R",
    "content": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Conduct differential expression analysis using the limma R pacakge\n#'\n#' WARNING: This function can take a very long time to compute!\n#'\n#' @examples\n#' if (interactive()) {\n#'   data(TGGATESsmall)\n#'   analysis <- computeLimmaDiffExpr(TGGATESsmall)\n#' }\n#'\n#' @param object A [`ToxicoSet`] object with a molecular profile named 'rna'\n#' @param buildTable [`logical`] Should the result of the eBayes function\n#'  from limma be assembled into a data.table containing the result along\n#'  with the gene, compound and durations names. Default it TRUE, otherwise\n#'  this function with return the object produced by eBayes.\n#'\n#' @return A [`data.table`] containing the results the limma differential\n#'  expression analysis comparing control vs each dose level for each compound\n#'  within each duration.\n#'\n#' @import data.table\n#' @import Biobase\n#' @import limma\n#' @importFrom SummarizedExperiment SummarizedExperiment coerce\n#' @importFrom stats model.matrix model.frame\n#' @importFrom BiocParallel bplapply\n#'\n#' @export\nsetMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n\n    ## TODO:: Add messages to keep track of where the function execution is at\n    ## TODO:: Error if any of the model factors don't have enough levels\n\n    # ---- 1. Get the required input data\n    SE <- molecularProfilesSlot(object)$rna  # Extract the rna expression SummarizedExperiment\n    # work around to fix error when coercing to ESet, since protocolData is not\n    #>subset with the SummarizedExperiment in subsetTo\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')  # Coerce to an ExpressionSet\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    # ---- 2. Extract the metadata needed to build the design matrix\n\n    # Get the sample name, drug, dose and duration and cell type from the\n    #   experiments phenotypic data\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    # to prevent dropping numeric columns when converting to factors\n    targets <- data.frame(lapply(targets, as.character))\n\n    # ---- 3. Create the design matrix\n\n\n    # Construct the design matrix with an intercept at 0\n    # This follows the make the simplest design matrix possible strategy outlined\n    # on page 37 of the limma user guide.\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n\n    # Make the names valid factor names and match with contrasts\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    # ---- 4. Fit a linear model based on design matrix\n\n    # Every possible comparison between compound:dose:duration will\n    # be fit in the linear model - this is less computationally efficient but\n    # allows arbitrary comparisons to be extracted after the model is fit.\n    # We chose this strategy to avoid the need to prespecify an experimental\n    # design (less statistical planning required).\n    fit <- lmFit(eset, design)\n\n    # ---- 5. Setup the contrasts to performs our desired statistical tests\n    setDT(targets)  # Convert to data.table\n\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls)\n        columns <- c('compound', columns)\n    if (hasMultipleCells)\n        columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    # fix levels if there is only one cell type\n    if (!hasMultipleCells) {\n        controls$controlLevels <- gsub('_cell[^_]*$', '',\n            controls$controlLevels)\n        levels$treatmentLevels <- gsub('_cell[^_]*$', '',\n            levels$treatmentLevels)\n    }\n\n    contrastStrings <- unique(controls[levels,\n                              .(contrasts=paste0(treatmentLevels, '-', controlLevels)),\n                              on=columns, all=!hasSharedControls]$contrasts)\n\n\n    # Make contrasts matrix for fitting statistical testing\n    contrasts <- makeContrasts(contrasts=contrastStrings, levels=design)\n\n    # Perfrom statistical tests for the specified contrasts\n    contrFit <- contrasts.fit(fit, contrasts)\n\n    # ---- 6. Predict coefficients using emperical Bayes moderation of SE\n    # Generate a t-stat, moderated F-stat and log-odds of differential expression\n    stats <- eBayes(contrFit)\n\n    if (!buildTable) return(stats)\n\n    # ---- 7. Assemble the results into a data.table object\n    compoundNames <- make.names(treatmentInfo(object)$treatmentid)\n    compounds <- treatmentInfo(object)$treatmentid\n    cellNames <- make.names(sampleInfo(object)$sampleid)\n    cells <- sampleInfo(object)$sampleid\n    ## TODO:: refactor this into muliple lapply statements!\n    resultList <- lapply(contrastStrings, function(comparison) {\n      # Disassmble contrasts into annotations for this statistical test\n      comparisonNoLabels <- gsub('cell|compound|dose|duration',\n        '', comparison)\n      annotations <- unlist(strsplit(comparisonNoLabels, '-'))\n      annotations <- strsplit(annotations, '_')\n\n      if (!(annotations[[1]][3] == annotations[[2]][3]))\n        stop(\"Duration mismatch between treatment and control!\")\n\n      # Get the compound_id based on the contrast name then get the compound name\n      compound_id <- which(compoundNames %in% annotations[[1]][1])\n      compound <- compounds[compound_id]\n\n      # Get the sample_id based on the contrast name then get the sampleid\n      if (hasMultipleCells) {\n          ## TODO:: Should I use == instead?\n          sample_id <- which(cellNames %in% annotations[[1]][4])\n          cell <- cells[sample_id]\n      } else {\n          cell <- unique(cells)\n      }\n\n      # Get the tests per gene for the given comparison\n      statCols <- c(\"logFC\", \"B\", \"P.Value\", \"adj.P.Val\", \"AveExpr\")\n      results <- topTreat(stats, coef=comparison, sort.by=\"none\",\n                          number='all',adjust.method=\"BH\")[, statCols]\n      statCols <- c(\"fold_change\", \"log_odds\", \"p_value\", \"fdr\", \"avg_expr\")\n      colnames(results) <- statCols\n\n      DT <- data.table(\n        \"gene\" = rownames(results),\n        \"compound\" = rep(compound, nrow(results)),\n        \"dose\" = rep(annotations[[1]][2], nrow(results)),\n        \"duration\" = rep(annotations[[1]][3], nrow(results)),\n        \"cell\" = rep(cell, nrow(results)),\n        results\n      )\n      return(DT)\n    })\n    analysis <- rbindlist(resultList)\n\n    ## TODO:: Do we want to only return significant results? Will be a lot less rows\n\n    # --- 8. Annotate and return the results\n    return(analysis)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `computeLimmaDiffExpr` function, and what type of object does it expect as input?",
        "answer": "The main purpose of the `computeLimmaDiffExpr` function is to conduct differential expression analysis using the limma R package. It expects a `ToxicoSet` object with a molecular profile named 'rna' as input."
      },
      {
        "question": "How does the function handle multiple cell types in the dataset, and what changes in the design matrix when there are multiple cell types?",
        "answer": "The function checks for multiple cell types using `hasMultipleCells <- length(unique(targets$cell)) > 1`. If there are multiple cell types, the design matrix includes the cell factor: `model.matrix(~0 + compound:dose:duration:cell, data=model.frame(targets))`. Otherwise, it uses `model.matrix(~0 + compound:dose:duration, data=model.frame(targets))`."
      },
      {
        "question": "What is the purpose of the `buildTable` parameter in the function, and what does the function return when `buildTable` is set to FALSE?",
        "answer": "The `buildTable` parameter determines whether the function should assemble the results into a data.table containing the differential expression analysis results along with gene, compound, and duration names. When `buildTable` is set to FALSE, the function returns the object produced by the `eBayes` function from limma, which contains the raw statistical results of the differential expression analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    # TODO: Complete the rest of the function\n})",
        "complete": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    setDT(targets)\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls) columns <- c('compound', columns)\n    if (hasMultipleCells) columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    if (!hasMultipleCells) {\n        controls$controlLevels <- gsub('_cell[^_]*$', '', controls$controlLevels)\n        levels$treatmentLevels <- gsub('_cell[^_]*$', '', levels$treatmentLevels)\n    }\n\n    contrastStrings <- unique(controls[levels,\n                              .(contrasts=paste0(treatmentLevels, '-', controlLevels)),\n                              on=columns, all=!hasSharedControls]$contrasts)\n\n    contrasts <- makeContrasts(contrasts=contrastStrings, levels=design)\n    contrFit <- contrasts.fit(fit, contrasts)\n    stats <- eBayes(contrFit)\n\n    if (!buildTable) return(stats)\n\n    compoundNames <- make.names(treatmentInfo(object)$treatmentid)\n    compounds <- treatmentInfo(object)$treatmentid\n    cellNames <- make.names(sampleInfo(object)$sampleid)\n    cells <- sampleInfo(object)$sampleid\n\n    resultList <- lapply(contrastStrings, function(comparison) {\n      comparisonNoLabels <- gsub('cell|compound|dose|duration', '', comparison)\n      annotations <- unlist(strsplit(comparisonNoLabels, '-'))\n      annotations <- strsplit(annotations, '_')\n\n      if (!(annotations[[1]][3] == annotations[[2]][3]))\n        stop(\"Duration mismatch between treatment and control!\")\n\n      compound_id <- which(compoundNames %in% annotations[[1]][1])\n      compound <- compounds[compound_id]\n\n      if (hasMultipleCells) {\n          sample_id <- which(cellNames %in% annotations[[1]][4])\n          cell <- cells[sample_id]\n      } else {\n          cell <- unique(cells)\n      }\n\n      statCols <- c(\"logFC\", \"B\", \"P.Value\", \"adj.P.Val\", \"AveExpr\")\n      results <- topTreat(stats, coef=comparison, sort.by=\"none\",\n                          number='all',adjust.method=\"BH\")[, statCols]\n      statCols <- c(\"fold_change\", \"log_odds\", \"p_value\", \"fdr\", \"avg_expr\")\n      colnames(results) <- statCols\n\n      DT <- data.table(\n        \"gene\" = rownames(results),\n        \"compound\" = rep(compound, nrow(results)),\n        \"dose\" = rep(annotations[[1]][2], nrow(results)),\n        \"duration\" = rep(annotations[[1]][3], nrow(results)),\n        \"cell\" = rep(cell, nrow(results)),\n        results\n      )\n      return(DT)\n    })\n    analysis <- rbindlist(resultList)\n\n    return(analysis)\n})"
      },
      {
        "partial": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    setDT(targets)\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls) columns <- c('compound', columns)\n    if (hasMultipleCells) columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    # TODO: Complete the rest of the function\n})",
        "complete": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    setDT(targets)\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls) columns <- c('compound', columns)\n    if (hasMultipleCells) columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    if (!hasMultipleCells) {\n        controls$controlLevels <- gsub('_cell[^_]*$', '', controls$controlLevels)\n        levels$treatmentLevels <- gsub('_cell[^_]*$', '', levels$treatmentLevels)\n    }\n\n    contrastStrings <- unique(controls[levels,\n                              .(contrasts=paste0(treatmentLevels, '-', controlLevels)),\n                              on=columns, all=!hasSharedControls]$contrasts)\n\n    contrasts <- makeContrasts(contrasts=contrastStrings, levels=design)\n    contrFit <- contrasts.fit(fit, contrasts)\n    stats <- eBayes(contrFit)\n\n    if (!buildTable) return(stats)\n\n    compoundNames <- make.names(treatmentInfo(object)$treatmentid)\n    compounds <- treatmentInfo(object)$treatmentid\n    cellNames <- make.names(sampleInfo(object)$sampleid)\n    cells <- sampleInfo(object)$sampleid\n\n    resultList <- lapply(contrastStrings, function(comparison) {\n      comparisonNoLabels <- gsub('cell|compound|dose|duration', '', comparison)\n      annotations <- unlist(strsplit(comparisonNoLabels, '-'))\n      annotations <- strsplit(annotations, '_')\n\n      if (!(annotations[[1]][3] == annotations[[2]][3]))\n        stop(\"Duration mismatch between treatment and control!\")\n\n      compound_id <- which(compoundNames %in% annotations[[1]][1])\n      compound <- compounds[compound_id]\n\n      if (hasMultipleCells) {\n          sample_id <- which(cellNames %in% annotations[[1]][4])\n          cell <- cells[sample_id]\n      } else {\n          cell <- unique(cells)\n      "
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/hr.comp2.R",
    "language": "R",
    "content": "`hr.comp2` <-\nfunction(x1, beta1, se1, x2, beta2, se2, n) {\n\tr <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n\tif(abs(r) < 1) {\n\t\tt.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hr.comp2` function in R?",
        "answer": "The `hr.comp2` function is designed to compare two hazard ratios in survival analysis. It calculates a p-value for the difference between two log hazard ratios, considering their correlation. The function also returns the exponential of the input betas, which represent the actual hazard ratios."
      },
      {
        "question": "How does the function handle perfect correlation between x1 and x2?",
        "answer": "If the absolute value of the correlation coefficient (r) between x1 and x2 is 1 (perfect correlation), the function sets the p-value (diff.ci.p) to 1 instead of calculating the t-statistic. This is because perfect correlation would lead to division by zero in the t-statistic calculation."
      },
      {
        "question": "What statistical test is being performed in this function, and what assumptions does it make?",
        "answer": "The function performs a t-test to compare two log hazard ratios. It assumes that the difference between the log hazard ratios follows a t-distribution with n-1 degrees of freedom. The test accounts for the correlation between the predictors (x1 and x2) using Spearman's rank correlation. It assumes that the sample size (n) is sufficiently large and that the log hazard ratios are approximately normally distributed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- # Calculate t-statistic\n    diff.ci.p <- # Calculate p-value\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}",
        "complete": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      },
      {
        "partial": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- # Calculate correlation\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(# Return list with p-value and hazard ratios)\n}",
        "complete": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassErrorHandling.R",
    "language": "R",
    "content": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_that()` function in this code snippet?",
        "answer": "The `test_that()` function is used to define a unit test in R. It takes two arguments: a description of the test (as a string) and a code block containing the test expectations. In this case, it's testing that 'multiplication works'."
      },
      {
        "question": "What does the `expect_equal()` function do in this test?",
        "answer": "The `expect_equal()` function is an expectation function from the testthat package in R. It compares two values for equality. In this case, it's checking if the result of `2 * 2` is equal to `4`. If the comparison is true, the test passes; if not, it fails."
      },
      {
        "question": "How would you modify this test to check if division works correctly?",
        "answer": "To check if division works correctly, you could modify the test as follows:\n\ntest_that(\"division works\", {\n  expect_equal(8 / 2, 4)\n})\n\nThis test would verify that 8 divided by 2 equals 4, thus testing the division operation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, ___)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      },
      {
        "partial": "test_that(___, {\n  expect_equal(2 * 2, 4)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassConstructor.R",
    "language": "R",
    "content": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_that()` function in this code snippet?",
        "answer": "The `test_that()` function is used to define a unit test in R. It takes two arguments: a description of the test (as a string) and a code block containing the test expectations. In this case, it's testing that 'multiplication works'."
      },
      {
        "question": "What does the `expect_equal()` function do in this test?",
        "answer": "The `expect_equal()` function is an expectation function from the testthat package in R. It compares two values for equality. In this snippet, it's comparing the result of `2 * 2` with the expected value `4`. If the values are equal, the test passes; if not, it fails."
      },
      {
        "question": "How would you modify this test to check that 3 * 3 equals 9?",
        "answer": "To modify the test to check that 3 * 3 equals 9, you would change the arguments in the `expect_equal()` function. The modified code would look like this:\n\ntest_that(\"multiplication works\", {\n  expect_equal(3 * 3, 9)\n})"
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, ___)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      },
      {
        "partial": "test_that(___, {\n  expect_equal(2 * 2, 4)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/weighted.meanvar.R",
    "language": "R",
    "content": "#' @title Function to compute the weighted mean and weighted variance of 'x'\n#'\n#' @description\n#' This function allows for computing the weighted mean and weighted variance \n#'   of a vector of continuous values.\n#'\n#' @usage\n#' weighted.meanvar(x, w, na.rm = FALSE)\n#'\n#' @param x\tan object containing the values whose weighted mean is to be computed.\n#' @param w\ta numerical vector of weights of the same length as x giving the \n#'   weights to use for elements of x.\n#' @param na.rm\tTRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @details\n#' If w is missing then all elements of x are given the same weight, otherwise \n#'   the weights coerced to numeric by as.numeric. On the contrary of \n#'   weighted.mean the weights are NOT normalized to sum to one. If the sum \n#'   of the weights is zero or infinite, NAs will be returned.\n#'\n#' @return\n#' A numeric vector of two values that are the weighted mean and weighted \n#'   variance respectively.\n#'\n#' @references\n#' http://en.wikipedia.org/wiki/Weighted_variance#Weighted_sample_variance\n#'\n#' @seealso\n#' [stats::weighted.mean]\n#' \n#' @examples\n#' set.seed(54321)\n#' weighted.meanvar(x=rnorm(100) + 10, w=runif(100))\n#'\n#' @md\n#' @export\n## weighted mean and weighted variance\n## sources:\n## http://en.wikipedia.org/wiki/T_test\n## http://www.nicebread.de/blog/files/fc02e1635792cb0f2b3cbd1f7e6c580b-10.php\nweighted.meanvar <- \nfunction(x, w, na.rm=FALSE) {\n\tif(missing(w)) { w <- rep(1, length(x))}\n\tii <- complete.cases(x, w)\n\tif(!na.rm && sum(!ii) > 0) { stop(\"missing values are present!\") } else { \n\t\tw <- as.numeric(w[ii])\n\t\tx <- as.numeric(x[ii])\n\t} \n\tsum.w <- sum(w) \n\tsum.w2 <- sum(w^2) \n\tmean.w <- sum(x * w) / sum(w) \n\tvar.w <- (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2, na.rm=na.rm)\n\tres <- c(mean.w, var.w)\n\tnames(res) <- c(\"weighted.mean\", \"weighted.var\")\n\treturn(res)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'weighted.meanvar' function and what are its main parameters?",
        "answer": "The 'weighted.meanvar' function computes the weighted mean and weighted variance of a vector of continuous values. Its main parameters are 'x' (the vector of values), 'w' (the vector of weights), and 'na.rm' (a boolean indicating whether to remove missing values)."
      },
      {
        "question": "How does the function handle missing weights (w) and what is different about its weight normalization compared to the standard weighted.mean function?",
        "answer": "If weights (w) are missing, the function assigns equal weights to all elements of x by using 'rep(1, length(x))'. Unlike the standard weighted.mean function, this function does NOT normalize the weights to sum to one. If the sum of weights is zero or infinite, the function will return NAs."
      },
      {
        "question": "What is the mathematical formula used to calculate the weighted variance in this function?",
        "answer": "The weighted variance is calculated using the formula: var.w = (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2), where sum.w is the sum of weights, sum.w2 is the sum of squared weights, and mean.w is the weighted mean. This formula is based on the weighted sample variance as described in the Wikipedia reference provided in the function documentation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  if(missing(w)) { w <- rep(1, length(x))}\n  ii <- complete.cases(x, w)\n  if(!na.rm && sum(!ii) > 0) { stop(\"missing values are present!\") } else { \n    w <- as.numeric(w[ii])\n    x <- as.numeric(x[ii])\n  } \n  sum.w <- sum(w) \n  sum.w2 <- sum(w^2) \n  mean.w <- sum(x * w) / sum.w \n  # Complete the calculation for var.w\n  # Return the result as a named vector\n}",
        "complete": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  if(missing(w)) { w <- rep(1, length(x))}\n  ii <- complete.cases(x, w)\n  if(!na.rm && sum(!ii) > 0) { stop(\"missing values are present!\") } else { \n    w <- as.numeric(w[ii])\n    x <- as.numeric(x[ii])\n  } \n  sum.w <- sum(w) \n  sum.w2 <- sum(w^2) \n  mean.w <- sum(x * w) / sum.w \n  var.w <- (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2, na.rm=na.rm)\n  res <- c(mean.w, var.w)\n  names(res) <- c(\"weighted.mean\", \"weighted.var\")\n  return(res)\n}"
      },
      {
        "partial": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  # Handle missing weights\n  # Check for complete cases\n  # Handle missing values based on na.rm\n  # Calculate sum of weights and sum of squared weights\n  # Calculate weighted mean\n  # Calculate weighted variance\n  # Return result as named vector\n}",
        "complete": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  if(missing(w)) w <- rep(1, length(x))\n  ii <- complete.cases(x, w)\n  if(!na.rm && sum(!ii) > 0) stop(\"missing values are present!\")\n  w <- as.numeric(w[ii])\n  x <- as.numeric(x[ii])\n  sum.w <- sum(w)\n  sum.w2 <- sum(w^2)\n  mean.w <- sum(x * w) / sum.w\n  var.w <- (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2, na.rm=na.rm)\n  setNames(c(mean.w, var.w), c(\"weighted.mean\", \"weighted.var\"))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/genius.R",
    "language": "R",
    "content": "#' @title Function to compute the Gene Expression progNostic Index Using Subtypes (GENIUS)\n#'   as published by Haibe-Kains et al. 2010\n#'\n#' @description\n#' This function computes the Gene Expression progNostic Index Using Subtypes (GENIUS)\n#'   as published by Haibe-Kains et al. 2010. Subtype-specific risk scores are computed for\n#'   each subtype signature separately and an overall risk score is computed by combining\n#'   these scores with the posterior probability to belong to each of the breast cancer\n#'   molecular subtypes.\n#'\n#' @usage\n#' genius(data, annot, do.mapping = FALSE, mapping, do.scale = TRUE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in case of ambiguities,\n#'   the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#'   mapping such that the probes are not selected based on their variance.\n#' @param do.scale TRUE if the ESR1, ERBB2 and AURKA (module) scores must be rescaled\n#'   (see rescale), FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - GENIUSM1: Risk score from the ER-/HER2- subtype signature in GENIUS model.\n#' - GENIUSM2: Risk score from the HER2+ subtype signature in GENIUS model.\n#' - GENIUSM3: Risk score from the ER+/HER2- subtype signature in GENIUS model.\n#' - score: Overall risk prediction as computed by the GENIUS model.a.\n#'\n#' @references\n#' Haibe-Kains B, Desmedt C, Rothe F, Sotiriou C and Bontempi G (2010) \"A fuzzy gene\n#' expression-based computational approach improves breast cancer prognostication\", Genome Biology, 11(2):R18\n#'\n#' @seealso\n#' [genefu::subtype.cluster.predict],[genefu::sig.score]\n#'\n#' @examples\n#' # load NKI dataset\n#' data(nkis)\n#' data(scmod1.robust)\n#' data(sig.genius)\n#'\n#' # compute GENIUS risk scores based on GENIUS model fitted on VDX dataset\n#' genius.nkis <- genius(data=data.nkis, annot=annot.nkis, do.mapping=TRUE)\n#' str(genius.nkis)\n#' # the performance of GENIUS overall risk score predictions are not optimal\n#' # since only part of the NKI dataset was used\n#'\n#' @md\n#' @export\ngenius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n\n\t## predict breast cancer molecular subtypes\n\tsbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n\tusbt <- unique(sbt.id$subtype)\n\tusbt <- sort(usbt[!is.na(usbt)])\n\tpred.sbtclassif <- NULL\n\tfor(ii in 1:length(usbt)) {\n\t\tmyx <- sbt.id$subtype == usbt[ii] & !is.na(sbt.id$subtype)\n\n\t\t#compute the score from model\n\t\tscore <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\",  \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping,  mapping=mapping, verbose=FALSE)$score\n\t\tif(do.scale) {\n\t\t\t#the rescaling needs a large sample size!\n\t\t\t#necessary if we want to validate the classifier using a different dataset\n\t\t\t#the estimation of survival probabilities depends on the scale of the score\n\t\t\tscore <-  (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n\t\t}\n\t\tnames(score) <- dimnames(data)[[1]]\n\t\tpred.sbtclassif <- c(pred.sbtclassif, list(\"score\"=score))\n\t}\n\tnames(pred.sbtclassif) <- names(sig.genius)\n\t#combine classifications\n\tcc <- NULL\n\tfor(j in 1:length(pred.sbtclassif)) {\n\t\tcc <- cbind(cc, pred.sbtclassif[[j]])\n\t}\n\tww <- sbt.id$subtype.proba\n\tcombine.pred <- apply(ww * cc, 1, sum)\n\n\tpred.sbtclassif <- c(pred.sbtclassif, list(combine.pred))\n\tnames(pred.sbtclassif)[length(pred.sbtclassif)] <- \"score\"\n\treturn(pred.sbtclassif)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'genius' function and what are its main inputs?",
        "answer": "The 'genius' function computes the Gene Expression progNostic Index Using Subtypes (GENIUS) for breast cancer prognosis. Its main inputs are 'data' (a matrix of gene expressions), 'annot' (a matrix of annotations), 'do.mapping' (a boolean for gene mapping), 'mapping' (an optional matrix for forced mapping), and 'do.scale' (a boolean for score rescaling)."
      },
      {
        "question": "How does the function handle different breast cancer subtypes in the GENIUS model?",
        "answer": "The function predicts breast cancer molecular subtypes using the 'subtype.cluster.predict' function. It then computes subtype-specific risk scores for each unique subtype using the 'sig.score' function with the corresponding signature from 'sig.genius'. These scores are optionally rescaled, and finally combined using subtype probabilities to produce an overall risk score."
      },
      {
        "question": "What is the structure of the output returned by the 'genius' function?",
        "answer": "The function returns a list containing risk scores for each subtype (GENIUSM1, GENIUSM2, GENIUSM3) and an overall risk prediction score. The subtype-specific scores are named according to the 'sig.genius' list, and the overall score is named 'score'. Each score is a vector with values for each sample in the input data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- NULL\n  for(ii in 1:length(usbt)) {\n    myx <- sbt.id$subtype == usbt[ii] & !is.na(sbt.id$subtype)\n\n    score <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\",  \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping,  mapping=mapping, verbose=FALSE)$score\n    if(do.scale) {\n      score <-  (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n    }\n    names(score) <- dimnames(data)[[1]]\n    pred.sbtclassif <- c(pred.sbtclassif, list(\"score\"=score))\n  }\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  # Complete the function to combine classifications and return the result\n}",
        "complete": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- NULL\n  for(ii in 1:length(usbt)) {\n    myx <- sbt.id$subtype == usbt[ii] & !is.na(sbt.id$subtype)\n\n    score <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\",  \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping,  mapping=mapping, verbose=FALSE)$score\n    if(do.scale) {\n      score <-  (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n    }\n    names(score) <- dimnames(data)[[1]]\n    pred.sbtclassif <- c(pred.sbtclassif, list(\"score\"=score))\n  }\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  cc <- do.call(cbind, pred.sbtclassif)\n  ww <- sbt.id$subtype.proba\n  combine.pred <- rowSums(ww * cc)\n\n  pred.sbtclassif <- c(pred.sbtclassif, list(score = combine.pred))\n  return(pred.sbtclassif)\n}"
      },
      {
        "partial": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  # Predict breast cancer molecular subtypes\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- vector(\"list\", length(usbt))\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  # Complete the function to compute scores for each subtype and combine them\n}",
        "complete": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  # Predict breast cancer molecular subtypes\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- vector(\"list\", length(usbt))\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  for(ii in seq_along(usbt)) {\n    score <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\", \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)$score\n    if(do.scale) {\n      score <- (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n    }\n    names(score) <- rownames(data)\n    pred.sbtclassif[[ii]] <- score\n  }\n\n  cc <- do.call(cbind, pred.sbtclassif)\n  ww <- sbt.id$subtype.proba\n  combine.pred <- rowSums(ww * cc)\n\n  pred.sbtclassif$score <- combine.pred\n  return(pred.sbtclassif)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/summarizeSensitivityProfiles.R",
    "language": "R",
    "content": "#' Takes the sensitivity data from a ToxicoSet, and summarises them into a\n#' drug vs cell line table\n#'\n#' This function creates a table with drug as rows and cell lines as columns,\n#' summarising the drug senstitivity data of a ToxicoSet into drug-cell line\n#' pairs for a specified experiment duration.\n#'\n#' @examples\n#' data(TGGATESsmall)\n#' TGGATESauc <- summarizeSensitivityProfiles(TGGATESsmall, sensitivity.measure='auc_recomputed')\n#'\n#' @param tSet \\code{ToxicoSet} The ToxicoSet from which to extract the data\n#' @param sensitivity.measure \\code{character} which sensitivity sensitivity.measure to use? Use the\n#'   sensitivityMeasures function to find out what measures are available for each TSet.\n#' @param cell_lines \\code{character} The cell lines to be summarized.\n#'    If any cell lines has no data, it will be filled with missing values\n#' @param drugs \\code{character} The drugs to be summarized.\n#'   If any drugs has no data, it will be filled with\n#'   missing values. Defaults to include all drugs in the given tSet.\n#' @param duration \\code{numeric} The duration at which to summarize\n#'   the drug-cell combo. This is a required parameter.\n#' @param summary.stat \\code{character} which summary method to use if there are repeated\n#'   cell line-drug experiments? Choices are \"mean\", \"median\", \"first\", \"last\", \"max\", or \"min\"\n#' @param fill.missing \\code{boolean} should the missing cell lines not in the\n#'   molecular data object be filled in with missing values?\n#' @param verbose Should the function print progress messages?\n#'\n#' @return \\code{matrix} A matrix with drugs going down the rows, cell lines across\n#'   the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom stats median\n#' @importFrom reshape2 acast\n#'\n#' @export\nsummarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n\n  ## MISSING VALUE HANDLING FOR PARAMETERS\n  # Get named list of defualt values for missing parameters\n  argDefaultList <-\n    paramMissingHandler(\n      funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n      cell_lines = cell_lines, drugs = drugs, duration = duration\n    )\n  # Assign any missing parameter default values to function environment\n  ## TODO:: I think we can do a for loop over index names?\n  ## TODO:: Refactor to lapply\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  ## ERROR HANDLING FOR FUNCTION PARAMETERS\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n    )\n\n  summary.stat <- match.arg(summary.stat)\n\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n\n  if (sensitivity.measure != \"max.conc\") {\n    #if the sensitivity.measure specified is not \"max.conc\"\n    dd <- sensitivityProfiles(tSet)\n  } else {\n    #if the sensitivity.measure specified is \"max.conc\"\n    if (!\"max.conc\" %in% colnames(ToxicoGx::sensitivityInfo(tSet))) {\n      # if max.conc is not a column in sensitivityInfo:\n      # call updateMaxConc, which finds the maximum dosage within sensitivity raw, puts\n      # the value in a new column of sensitivity info called max.conc, and returns the tSet\n      tSet <- updateMaxConc(tSet)\n\n    }\n    ##dd contains the sensitivity Info of the tSet\n    dd <- ToxicoGx::sensitivityInfo(tSet)\n\n  }\n\n  #result is a matrix of NA's where # of rows, # columns is as specified:\n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines))\n  #specify the row, column names of the result matrix\n  rownames(result) <- drugs\n  colnames(result) <- cell_lines\n\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\",\"duration_h\")], \"sensitivity.measure\" = dd[, sensitivity.measure])\n\n  summary.function <- function(x) {\n    if (all(is.na(x))) {\n      return(NA_real_)\n    }\n    switch(summary.stat,\n           \"mean\" = {\n             return(mean(as.numeric(x), na.rm = TRUE))\n           },\n           \"median\" = {\n             return(median(as.numeric(x), na.rm = TRUE))\n           },\n           \"first\" = {\n             return(as.numeric(x)[[1]])\n           },\n           \"last\" = {\n             return(as.numeric(x)[[length(x)]])\n           },\n           \"max\" = {\n             return(max(as.numeric(x), na.rm = TRUE))\n           },\n           \"min\" = {\n             return(min(as.numeric(x), na.rm = TRUE))\n           })\n\n  }\n\n  pp_dd <- pp_dd[\n    pp_dd[,\"sampleid\"] %in% cell_lines &\n    pp_dd[,\"treatmentid\"] %in% drugs &\n    pp_dd[,\"duration_h\"] %in% duration,\n\n    ]\n\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, fun.aggregate = summary.function,\n                        value.var = \"sensitivity.measure\")\n\n  result[rownames(tt), colnames(tt)] <- tt\n\n  if (!fill.missing) {\n\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n  }\n  return(result)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `summarizeSensitivityProfiles` function in this code snippet?",
        "answer": "The main purpose of the `summarizeSensitivityProfiles` function is to create a summary table of drug sensitivity data from a ToxicoSet object. It generates a matrix with drugs as rows and cell lines as columns, containing the specified sensitivity measure for each drug-cell line pair at a given experiment duration."
      },
      {
        "question": "How does the function handle missing data or incomplete drug-cell line combinations?",
        "answer": "The function handles missing data in several ways: 1) It uses a `fill.missing` parameter to optionally fill in missing cell lines with NA values. 2) It creates a result matrix initially filled with NA values. 3) It uses a custom summary function that can handle NA values when aggregating data. 4) If `fill.missing` is set to FALSE, it removes rows and columns that contain only NA values from the final result."
      },
      {
        "question": "What is the purpose of the `summary.stat` parameter in this function, and what options does it provide?",
        "answer": "The `summary.stat` parameter determines how to aggregate data when there are repeated experiments for the same drug-cell line combination. It provides six options: 'mean', 'median', 'first', 'last', 'max', and 'min'. These options allow the user to choose how to summarize multiple data points for the same drug-cell line pair, such as taking the average, the median, or selecting specific values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  argDefaultList <- paramMissingHandler(\n    funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n    cell_lines = cell_lines, drugs = drugs, duration = duration\n  )\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n  )\n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  # Complete the function to process sensitivity data and return the result matrix\n}",
        "complete": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  argDefaultList <- paramMissingHandler(\n    funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n    cell_lines = cell_lines, drugs = drugs, duration = duration\n  )\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n  )\n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  dd <- if (sensitivity.measure != \"max.conc\") {\n    sensitivityProfiles(tSet)\n  } else {\n    if (!(\"max.conc\" %in% colnames(ToxicoGx::sensitivityInfo(tSet)))) {\n      tSet <- updateMaxConc(tSet)\n    }\n    ToxicoGx::sensitivityInfo(tSet)\n  }\n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines),\n                   dimnames = list(drugs, cell_lines))\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\",\"duration_h\")], \n                 \"sensitivity.measure\" = dd[, sensitivity.measure])\n  summary.function <- function(x) {\n    if (all(is.na(x))) return(NA_real_)\n    switch(summary.stat,\n           mean = mean(as.numeric(x), na.rm = TRUE),\n           median = median(as.numeric(x), na.rm = TRUE),\n           first = as.numeric(x)[[1]],\n           last = as.numeric(x)[[length(x)]],\n           max = max(as.numeric(x), na.rm = TRUE),\n           min = min(as.numeric(x), na.rm = TRUE))\n  }\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell_lines &\n                 pp_dd[,\"treatmentid\"] %in% drugs &\n                 pp_dd[,\"duration_h\"] %in% duration, ]\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, \n                        fun.aggregate = summary.function,\n                        value.var = \"sensitivity.measure\")\n  result[rownames(tt), colnames(tt)] <- tt\n  if (!fill.missing) {\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n  }\n  return(result)\n}"
      },
      {
        "partial": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  # Add code to handle missing parameters and perform error checking\n  \n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  \n  # Add code to process sensitivity data based on the sensitivity.measure\n  \n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines))\n  rownames(result) <- drugs\n  colnames(result) <- cell_lines\n  \n  # Add code to calculate and populate the result matrix\n  \n  # Add code to handle fill.missing option\n  \n  return(result)\n}",
        "complete": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  argDefaultList <- paramMissingHandler(\n    funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n    cell_lines = cell_lines, drugs = drugs, duration = duration\n  )\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n  )\n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  dd <- if (sensitivity.measure != \"max.conc\") {\n    sensitivityProfiles(tSet)\n  } else {\n    if (!(\"max.conc\" %in% colnames(ToxicoGx::sensitivityInfo(tSet)))) {\n      tSet <- updateMaxConc(tSet)\n    }\n    ToxicoGx::sensitivityInfo(tSet)\n  }\n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines),\n                   dimnames = list(drugs, cell_lines))\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\",\"duration_h\")], \n                 \"sensitivity.measure\" = dd[, sensitivity.measure])\n  summary.function <- function(x) {\n    if (all(is.na(x))) return(NA_real_)\n    switch(summary.stat,\n           mean = mean(as.numeric(x), na.rm = TRUE),\n           median = median(as.numeric(x), na.rm = TRUE),\n           first = as.numeric(x)[[1]],\n           last = as.numeric(x)[[length(x)]],\n           max = max(as.numeric(x), na.rm = TRUE),\n           min = min(as.numeric(x), na.rm = TRUE))\n  }\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell_lines &\n                 pp_dd[,\"treatmentid\"] %in% drugs &\n                 pp_dd[,\"duration_h\"] %in% duration, ]\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, \n                        fun.aggregate = summary.function,\n                        value.var = \"sensitivity.measure\")\n  result[rownames(tt), colnames(tt)] <- tt\n  if (!fill.missing) {\n    result <- result[apply(result, 1, function(x) !all(is.na(x))),\n                     apply(result, 2, function(x) !all(is.na(x)))]\n  }\n  return(result)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/vignettes/genefu.R",
    "language": "R",
    "content": "## ----install, eval=FALSE, results='hide', message=FALSE-----------------------\n#  BiocManager::install(\"genefu\")\n\n## ----load, eval=TRUE, results='hide', message=FALSE---------------------------\nlibrary(genefu)\nlibrary(xtable)\nlibrary(rmeta)\nlibrary(Biobase)\nlibrary(caret)\n\n\n## ----install_data, eval=FALSE, results='hide', message=FALSE------------------\n#  BiocManager::install(\"breastCancerMAINZ\")\n#  BiocManager::install(\"breastCancerTRANSBIG\")\n#  BiocManager::install(\"breastCancerUPP\")\n#  BiocManager::install(\"breastCancerUNT\")\n#  BiocManager::install(\"breastCancerNKI\")\n\n## ----load_data, eval=TRUE, results='hide', message=FALSE----------------------\nlibrary(breastCancerMAINZ)\nlibrary(breastCancerTRANSBIG)\nlibrary(breastCancerUPP)\nlibrary(breastCancerUNT)\nlibrary(breastCancerNKI)\n\n## ----findDuplicatedPatients,eval=TRUE,results='hide',message=FALSE------------\ndata(breastCancerData)\ncinfo <- colnames(pData(mainz7g))\ndata.all <- c(\"transbig7g\"=transbig7g, \"unt7g\"=unt7g, \"upp7g\"=upp7g,\n              \"mainz7g\"=mainz7g, \"nki7g\"=nki7g)\n\nidtoremove.all <- NULL\nduplres <- NULL\n\n## No overlaps in the MainZ and NKI datasets.\n\n## Focus on UNT vs UPP vs TRANSBIG\ndemo.all <- rbind(pData(transbig7g), pData(unt7g), pData(upp7g))\ndn2 <- c(\"TRANSBIG\", \"UNT\", \"UPP\")\n\n## Karolinska\n## Search for the VDXKIU, KIU, UPPU series\nds2 <- c(\"VDXKIU\", \"KIU\", \"UPPU\")\ndemot <- demo.all[complete.cases(demo.all[ , c(\"series\")]) &\n                    is.element(demo.all[ , \"series\"], ds2), ]\n\n# Find the duplicated patients in that series\nduplid <- sort(unique(demot[duplicated(demot[ , \"id\"]), \"id\"]))\nduplrest <- NULL\nfor(i in 1:length(duplid)) {\n  tt <- NULL\n  for(k in 1:length(dn2)) {\n    myx <- sort(row.names(demot)[complete.cases(demot[ , c(\"id\", \"dataset\")]) &\n                                   demot[ , \"id\"] == duplid[i] & \n                                   demot[ , \"dataset\"] == dn2[k]])\n    if(length(myx) > 0) { tt <- c(tt, myx) }\n  }\n  duplrest <- c(duplrest, list(tt))\n}\nnames(duplrest) <- duplid\nduplres <- c(duplres, duplrest)\n\n## Oxford\n## Search for the VVDXOXFU, OXFU series\nds2 <- c(\"VDXOXFU\", \"OXFU\")\ndemot <- demo.all[complete.cases(demo.all[ , c(\"series\")]) & \n                    is.element(demo.all[ , \"series\"], ds2), ]\n\n# Find the duplicated patients in that series\nduplid <- sort(unique(demot[duplicated(demot[ , \"id\"]), \"id\"]))\nduplrest <- NULL\nfor(i in 1:length(duplid)) {\n  tt <- NULL\n  for(k in 1:length(dn2)) {\n    myx <- sort(row.names(demot)[complete.cases(demot[ , c(\"id\", \"dataset\")]) &\n                                   demot[ , \"id\"] == duplid[i] & \n                                   demot[ , \"dataset\"] == dn2[k]])\n    if(length(myx) > 0) { tt <- c(tt, myx) }\n  }\n  duplrest <- c(duplrest, list(tt))\n}\nnames(duplrest) <- duplid\nduplres <- c(duplres, duplrest)\n\n## Full set duplicated patients\nduPL <- sort(unlist(lapply(duplres, function(x) { return(x[-1]) } )))\n\n## ----CalculateMolecularSubtypes-----------------------------------------------\ndn <- c(\"transbig\", \"unt\", \"upp\", \"mainz\", \"nki\")\ndn.platform <- c(\"affy\", \"affy\", \"affy\", \"affy\", \"agilent\")\nres <- ddemo.all <- ddemo.coln <- NULL\n\nfor(i in 1:length(dn)) {\n\n  ## load dataset\n  dd <- get(data(list=dn[i]))\n  #Remove duplicates identified first\n  message(\"obtained dataset!\")\n\n  #Extract expression set, pData, fData for each dataset\n  ddata <- t(exprs(dd))\n\n  ddemo <- phenoData(dd)@data\n\n  if(length(intersect(rownames(ddata),duPL))>0)\n  {\n  ddata<-ddata[-which(rownames(ddata) %in% duPL),]\n  ddemo<-ddemo[-which(rownames(ddemo) %in% duPL),]\n  }\n\n  dannot <- featureData(dd)@data\n\n  # MOLECULAR SUBTYPING\n  # Perform subtyping using scmod2.robust\n  # scmod2.robust: List of parameters defining the subtype clustering model\n  # (as defined by Wirapati et al)\n\n  # OBSOLETE FUNCTION CALL - OLDER VERSIONS OF GENEFU\n  # SubtypePredictions<-subtype.cluster.predict(sbt.model=scmod2.robust,data=ddata,\n  #                                               annot=dannot,do.mapping=TRUE,\n  #                                               verbose=TRUE)\n\n  # CURRENT FUNCTION CALL - NEWEST VERSION OF GENEFU\n  SubtypePredictions <- molecular.subtyping(sbt.model = \"scmod2\",data = ddata,\n                                          annot = dannot,do.mapping = TRUE)\n\n  #Get sample counts pertaining to each subtype\n  table(SubtypePredictions$subtype)\n  #Select samples pertaining to Basal Subtype\n  Basals<-names(which(SubtypePredictions$subtype == \"ER-/HER2-\"))\n  #Select samples pertaining to HER2 Subtype\n  HER2s<-names(which(SubtypePredictions$subtype == \"HER2+\"))\n  #Select samples pertaining to Luminal Subtypes\n  LuminalB<-names(which(SubtypePredictions$subtype == \"ER+/HER2- High Prolif\"))\n  LuminalA<-names(which(SubtypePredictions$subtype == \"ER+/HER2- Low Prolif\"))\n\n  #ASSIGN SUBTYPES TO EVERY SAMPLE, ADD TO THE EXISTING PHENODATA\n  ddemo$SCMOD2<-SubtypePredictions$subtype\n  ddemo[LuminalB,]$SCMOD2<-\"LumB\"\n  ddemo[LuminalA,]$SCMOD2<-\"LumA\"\n  ddemo[Basals,]$SCMOD2<-\"Basal\"\n  ddemo[HER2s,]$SCMOD2<-\"Her2\"\n\n  # Perform subtyping using PAM50\n  # Matrix should have samples as ROWS, genes as COLUMNS\n  # rownames(dannot)<-dannot$probe<-dannot$EntrezGene.ID\n\n  # OLDER FUNCTION CALL\n  # PAM50Preds<-intrinsic.cluster.predict(sbt.model=pam50,data=ddata,\n  #                                         annot=dannot,do.mapping=TRUE,\n  #                                         verbose=TRUE)\n\n  # NEWER FUNCTION CALL BASED ON MOST RECENT VERSION\n  PAM50Preds<-molecular.subtyping(sbt.model = \"pam50\",data=ddata,\n                                        annot=dannot,do.mapping=TRUE)\n\n\n  table(PAM50Preds$subtype)\n  ddemo$PAM50<-PAM50Preds$subtype\n  LumA<-names(PAM50Preds$subtype)[which(PAM50Preds$subtype == \"LumA\")]\n  LumB<-names(PAM50Preds$subtype)[which(PAM50Preds$subtype == \"LumB\")]\n  ddemo[LumA,]$PAM50<-\"LumA\"\n  ddemo[LumB,]$PAM50<-\"LumB\"\n\n  ddemo.all <- rbind(ddemo, ddemo.all)\n}\n\n## ----CompareMolecularSubtypesByConfusionMatrix--------------------------------\n# Obtain the subtype prediction counts for PAM50\ntable(ddemo.all$PAM50)\nNormals<-rownames(ddemo.all[which(ddemo.all$PAM50 == \"Normal\"),])\n\n# Obtain the subtype prediction counts for SCMOD2\ntable(ddemo.all$SCMOD2)\n\nddemo.all$PAM50<-as.character(ddemo.all$PAM50)\n# We compare the samples that are predicted as pertaining to a molecular subtyp\n# We ignore for now the samples that predict as 'Normal' by PAM50\nconfusionMatrix(\n  factor(ddemo.all[-which(rownames(ddemo.all) %in% Normals),]$SCMOD2),\n  factor(ddemo.all[-which(rownames(ddemo.all) %in% Normals),]$PAM50)\n  )\n\n## ----CompareSurvivalBySubtypes------------------------------------------------\n# http://www.inside-r.org/r-doc/survival/survfit.coxph\nlibrary(survival)\nddemo<-ddemo.all\ndata.for.survival.SCMOD2 <- ddemo[,c(\"e.os\", \"t.os\", \"SCMOD2\",\"age\")]\ndata.for.survival.PAM50 <- ddemo[,c(\"e.os\", \"t.os\", \"PAM50\",\"age\")]\n# Remove patients with missing survival information\ndata.for.survival.SCMOD2 <- \n  data.for.survival.SCMOD2[complete.cases(data.for.survival.SCMOD2),]\ndata.for.survival.PAM50 <- \n  data.for.survival.PAM50[complete.cases(data.for.survival.PAM50),]\n\ndays.per.month <- 30.4368\ndays.per.year <- 365.242\n\ndata.for.survival.PAM50$months_to_death <- \n  data.for.survival.PAM50$t.os / days.per.month\ndata.for.survival.PAM50$vital_status <- data.for.survival.PAM50$e.os == \"1\"\nsurv.obj.PAM50 <- survfit(Surv(data.for.survival.PAM50$months_to_death,\n                               data.for.survival.PAM50$vital_status) ~ \n                            data.for.survival.PAM50$PAM50)\n\ndata.for.survival.SCMOD2$months_to_death <- \n  data.for.survival.SCMOD2$t.os / days.per.month\ndata.for.survival.SCMOD2$vital_status <- data.for.survival.SCMOD2$e.os == \"1\"\nsurv.obj.SCMOD2 <- survfit(Surv(\n  data.for.survival.SCMOD2$months_to_death,\n  data.for.survival.SCMOD2$vital_status) ~ data.for.survival.SCMOD2$SCMOD2)\n\nmessage(\"KAPLAN-MEIR CURVE - USING PAM50\")\n\nplot(main = \"Surival Curves PAM50\", surv.obj.PAM50,\n     col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),lty = 1,lwd = 3,\n     xlab = \"Time (months)\",ylab = \"Probability of Survival\")\nlegend(\"topright\",\n       fill = c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),\n       legend = c(\"Basal\",\"Her2\",\"LumA\",\"LumB\",\"Normal\"),bty = \"n\")\n\nmessage(\"KAPLAN-MEIR CURVE - USING SCMOD2\")\n\nplot(main = \"Surival Curves SCMOD2\", surv.obj.SCMOD2,\n     col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\"),lty = 1,lwd = 3,\n     xlab = \"Time (months)\",ylab = \"Probability of Survival\")\nlegend(\"topright\",\n       fill = c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\"),\n       legend = c(\"Basal\",\"Her2\",\"LumA\",\"LumB\"),bty = \"n\")\n\n## GENERATE A OVERLAYED PLOT OF SURVIVAL CURVES\nmessage(\"Overlayed Surival Plots based on PAM50 and SCMOD2\")\n                          ## Basal    Her2        LuminalA  LuminalB   Normal\nplot(surv.obj.PAM50,col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),lty = 1,lwd = 3,\n     xlab = \"Time (months)\",ylab = \"Probability of Survival\",ymin = 0.2)\nlegend(\"topright\",\n       fill = c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),\n       legend = c(\"Basal\",\"Her2\",\"LumA\",\"LumB\",\"Normal\"),bty = \"n\")\n\npar(new=TRUE)\n                            ## Basal    Her2        LuminalA  LuminalB\nlines(surv.obj.SCMOD2,col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\"),lwd=2,lty=5)\nlegend(\"bottomright\",c(\"PAM50\",\"SCMOD2\"),lty=c(\"solid\", \"dashed\"))\n\n## ----CalculatedCVPL-----------------------------------------------------------\nset.seed(12345)\n\nPAM5_CVPL<-cvpl(x=data.for.survival.PAM50$age,\n                surv.time=data.for.survival.PAM50$months_to_death,\n                surv.event=data.for.survival.PAM50$vital_status,\n                strata=as.integer(factor(data.for.survival.PAM50$PAM50)),\n                nfold=10, setseed=54321)$cvpl\n\nSCMOD2_CVPL<-cvpl(x=data.for.survival.SCMOD2$age,\n                    surv.time=data.for.survival.SCMOD2$months_to_death,\n                    surv.event=data.for.survival.SCMOD2$vital_status,\n                    strata=as.integer(factor(data.for.survival.SCMOD2$SCMOD2)),\n                    nfold=10, setseed=54321)$cvpl\n\nprint.data.frame(data.frame(cbind(PAM5_CVPL,SCMOD2_CVPL)))\n\n## ----computeRiskScore---------------------------------------------------------\ndn <- c(\"transbig\", \"unt\", \"upp\", \"mainz\", \"nki\")\ndn.platform <- c(\"affy\", \"affy\", \"affy\", \"affy\", \"agilent\")\n\nres <- ddemo.all <- ddemo.coln <- NULL\nfor(i in 1:length(dn)) {\n\n  ## load dataset\n  dd <- get(data(list=dn[i]))\n\n  #Extract expression set, pData, fData for each dataset\n  ddata <- t(exprs(dd))\n  ddemo <- phenoData(dd)@data\n  dannot <- featureData(dd)@data\n  ddemo.all <- c(ddemo.all, list(ddemo))\n  if(is.null(ddemo.coln))\n  { ddemo.coln <- colnames(ddemo) } else\n  { ddemo.coln <- intersect(ddemo.coln, colnames(ddemo)) }\n  rest <- NULL\n\n  ## AURKA\n  ## if affy platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  modt <- scmgene.robust$mod$AURKA\n  ## if agilent platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"agilent\") {\n    domap <- FALSE\n    modt[ , \"probe\"] <- \"NM_003600\"\n  }\n  rest <- cbind(rest, \"AURKA\"=sig.score(x=modt, data=ddata, annot=dannot, \n                                        do.mapping=domap)$score)\n\n  ## ESR1\n  ## if affy platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  modt <- scmgene.robust$mod$ESR1\n  ## if agilent platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"agilent\") {\n    domap <- FALSE\n    modt[ , \"probe\"] <- \"NM_000125\"\n  }\n  rest <- cbind(rest, \"ESR1\"=sig.score(x=modt, data=ddata, annot=dannot, \n                                       do.mapping=domap)$score)\n\n  ## ERBB2\n  ## if affy platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  modt <- scmgene.robust$mod$ERBB2\n  ## if agilent platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"agilent\") {\n    domap <- FALSE\n    modt[ , \"probe\"] <- \"NM_004448\"\n  }\n  rest <- cbind(rest, \"ERBB2\"=sig.score(x=modt, data=ddata, annot=dannot, \n                                        do.mapping=domap)$score)\n\n  ## NPI\n  ss <- ddemo[ , \"size\"]\n  gg <- ddemo[ , \"grade\"]\n  nn <- rep(NA, nrow(ddemo))\n  nn[complete.cases(ddemo[ , \"node\"]) & ddemo[ , \"node\"] == 0] <- 1\n  nn[complete.cases(ddemo[ , \"node\"]) & ddemo[ , \"node\"] == 1] <- 3\n  names(ss) <- names(gg) <- names(nn) <- rownames(ddemo)\n  rest <- cbind(rest, \"NPI\"=npi(size=ss, grade=gg, node=nn, na.rm=TRUE)$score)\n\n  ## GGI\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"GGI\"=ggi(data=ddata, annot=dannot, \n                                do.mapping=domap)$score)\n\n  ## GENIUS\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"GENIUS\"=genius(data=ddata, annot=dannot, \n                                      do.mapping=domap)$score)\n\n  ## ENDOPREDICT\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"EndoPredict\"=endoPredict(data=ddata, annot=dannot, \n                                                do.mapping=domap)$score)\n\n  # OncotypeDx\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"OncotypeDx\"=oncotypedx(data=ddata, annot=dannot, \n                                              do.mapping=domap)$score)\n\n  ## TamR\n  # Note: risk is not implemented, the function will return NA values\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"TAMR13\"=tamr13(data=ddata, annot=dannot, \n                                      do.mapping=domap)$score)\n\n  ## GENE70\n  # Need to do mapping for Affy platforms because this is based on Agilent.\n  # Hence the mapping rule is reversed here!\n  if(dn.platform[i] == \"affy\") { domap <- TRUE } else { domap <- FALSE }\n  rest <- cbind(rest, \"GENE70\"=gene70(data=ddata, annot=dannot, std=\"none\",\n                                      do.mapping=domap)$score)\n\n  ## Pik3cags\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"PIK3CA\"=pik3cags(data=ddata, annot=dannot, \n                                        do.mapping=domap))\n\n  ## rorS\n  # Uses the pam50 algorithm. Need to do mapping for both Affy and Agilent\n  rest <- cbind(rest, \"rorS\"=rorS(data=ddata, annot=dannot, \n                                  do.mapping=TRUE)$score)\n\n  ## GENE76\n  # Mainly designed for Affy platforms. Has been excluded here\n\n  # BIND ALL TOGETHER\n  res <- rbind(res, rest)\n}\nnames(ddemo.all) <- dn\n\n## ----simplifyAndRemoveDuplicatePatients---------------------------------------\nddemot <- NULL\nfor(i in 1:length(ddemo.all)) {\n  ddemot <- rbind(ddemot, ddemo.all[[i]][ , ddemo.coln, drop=FALSE])\n}\nres[complete.cases(ddemot[ ,\"dataset\"]) & ddemot[ ,\"dataset\"] == \"VDX\", \"GENIUS\"] <- NA\n\n## select only untreated node-negative patients with all risk predictions\n## ie(incomplete cases (where risk prediction may be missing for a sample) are subsequently removed))\n# Note that increasing the number of risk prediction analyses\n# may increase the number of incomplete cases\n# In the previous vignette for genefu version1, we were only testing 4 risk predictors,\n# so we had a total of 722 complete cases remaining\n# Here, we are now testing 12 risk predictors, so we only have 713 complete cases remaining.\n# The difference of 9 cases between the two versions are all from the NKI dataset.\nmyx <- complete.cases(res, ddemot[ , c(\"node\", \"treatment\")]) &\n  ddemot[ , \"treatment\"] == 0 & ddemot[ , \"node\"] == 0 & !is.element(rownames(ddemot), duPL)\n\nres <- res[myx, , drop=FALSE]\nddemot <- ddemot[myx, , drop=FALSE]\n\n## ----cindexComputation--------------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\", \"GGI\", \"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrix <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  # Calculate cindex computation for each predictor\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(c(\"cindex\"=tt$c.index, \"cindex.se\"=tt$se, \"lower\"=tt$lower, \"upper\"=tt$upper)); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrix[[Dat]] <- rbind(resMatrix[[Dat]], cindex)\n  }\n}\n\n## ----combineEstimations-------------------------------------------------------\nfor(i in names(resMatrix)){\n  #Get a meta-estimate\n  ceData <- combine.est(x=resMatrix[[i]][,\"cindex\"], x.se=resMatrix[[i]][,\"cindex.se\"], hetero=TRUE)\n  cLower <- ceData$estimate + qnorm(0.025, lower.tail=TRUE) * ceData$se\n  cUpper <- ceData$estimate + qnorm(0.025, lower.tail=FALSE) * ceData$se\n\n  cindexO <- cbind(\"cindex\"=ceData$estimate, \"cindex.se\"=ceData$se, \"lower\"=cLower, \"upper\"=cUpper)\n  resMatrix[[i]] <- rbind(resMatrix[[i]], cindexO)\n  rownames(resMatrix[[i]]) <- c(datasetList, \"Overall\")\n}\n\n## ----computePValues-----------------------------------------------------------\npv <- sapply(resMatrix, function(x) { return(x[\"Overall\", c(\"cindex\",\"cindex.se\")]) })\npv <- apply(pv, 2, function(x) { return(pnorm((x[1] - 0.5) / x[2], lower.tail=x[1] < 0.5)) })\nprintPV <- matrix(pv,ncol=length(names(resMatrix)))\nrownames(printPV) <- \"P-value\"\ncolnames(printPV) <- names(pv)\nprintPV<-t(printPV)\n\n## ----printPvalue,results=\"asis\"-----------------------------------------------\nknitr::kable(printPV, digits=c(0, -1))\n\n## ----forestplotDatasets,echo=TRUE---------------------------------------------\nRiskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\", \"GGI\", \"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\ndatasetListF <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\", \"Overall\")\nmyspace <- \"   \"\npar(mfrow=c(2,2))\n  for (RP in RiskPList)\n  {\n\n  #<<forestplotDat,fig=TRUE>>=\n  ## Forestplot\n  tt <- rbind(resMatrix[[RP]][1:5,],\n            \"Overall\"=resMatrix[[RP]][6,])\n\n  tt <- as.data.frame(tt)\n  labeltext <- (datasetListF)\n\n  r.mean <- c(tt$cindex)\n  r.lower <- c(tt$lower)\n  r.upper <- c(tt$upper)\n\n  metaplot.surv(mn=r.mean, lower=r.lower, upper=r.upper, labels=labeltext, xlim=c(0.3,0.9),\n                boxsize=0.5, zero=0.5,\n                col=meta.colors(box=\"royalblue\",line=\"darkblue\",zero=\"firebrick\"),\n                main=paste(RP))\n\n  }\n\n## ----forestplotOverall,echo=TRUE----------------------------------------------\n## Overall Forestplot\nmybigspace <- \"       \"\ntt <- rbind(\"OverallA\"=resMatrix[[\"AURKA\"]][6,],\n            \"OverallE1\"=resMatrix[[\"ESR1\"]][6,],\n            \"OverallE2\"=resMatrix[[\"ERBB2\"]][6,],\n            \"OverallN\"=resMatrix[[\"NPI\"]][6,],\n          \"OverallM\"=resMatrix[[\"GGI\"]][6,],\n          \"OverallG\"=resMatrix[[\"GENIUS\"]][6,],\n          \"OverallE3\"=resMatrix[[\"EndoPredict\"]][6,],\n          \"OverallOD\"=resMatrix[[\"OncotypeDx\"]][6,],\n          \"OverallT\"=resMatrix[[\"TAMR13\"]][6,],\n          \"OverallG70\"=resMatrix[[\"GENE70\"]][6,],\n          \"OverallP\"=resMatrix[[\"PIK3CA\"]][6,],\n          \"OverallR\"=resMatrix[[\"rorS\"]][6,]\n          )\n\ntt <- as.data.frame(tt)\nlabeltext <- cbind(c(\"Risk Prediction\",\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\n                     \"GGI\",\"GENIUS\",\"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\"))\n\nr.mean <- c(NA,tt$cindex)\nr.lower <- c(NA,tt$lower)\nr.upper <- c(NA,tt$upper)\n\nmetaplot.surv(mn=r.mean, lower=r.lower, upper=r.upper, labels=labeltext, xlim=c(0.35,0.75),\n              boxsize=0.5, zero=0.5,\n              col=meta.colors(box=\"royalblue\",line=\"darkblue\",zero=\"firebrick\"),\n              main=\"Overall Concordance Index\")\n\n## ----computeCindexWithPvalue--------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\"GGI\",\"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrixFull <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  ## cindex and p-value computation per algorithm\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(tt); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrixFull[[Dat]] <- rbind(resMatrixFull[[Dat]], cindex)\n  }\n}\n\nfor(i in names(resMatrixFull)){\n  rownames(resMatrixFull[[i]]) <- datasetList\n}\n\nccmData <- tt <- rr <- NULL\nfor(i in 1:length(resMatrixFull)){\n  tt <- NULL\n  for(j in 1:length(resMatrixFull)){\n    if(i != j) { rr <- cindex.comp.meta(list.cindex1=resMatrixFull[[i]],\n                                        list.cindex2=resMatrixFull[[j]], hetero=TRUE)$p.value }\n    else { rr <- 1 }\n    tt <- cbind(tt, rr)\n  }\n  ccmData <- rbind(ccmData, tt)\n}\nccmData <- as.data.frame(ccmData)\ncolnames(ccmData) <- riskPList\nrownames(ccmData) <- riskPList\n\n## ----computeCCMPval-----------------------------------------------------------\nccmDataPval <- matrix(p.adjust(data.matrix(ccmData), method=\"holm\"),\n                      ncol=length(riskPList), dimnames=list(rownames(ccmData),\n                                                            colnames(ccmData)))\n\n## ----sessionInfo, echo=FALSE--------------------------------------------------\nsessionInfo()\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `findDuplicatedPatients` section in the code?",
        "answer": "The `findDuplicatedPatients` section aims to identify and remove duplicate patient records across different breast cancer datasets (TRANSBIG, UNT, UPP). It searches for patients with the same ID in different series (VDXKIU, KIU, UPPU, VDXOXFU, OXFU) and creates a list of duplicated patient IDs to be removed from further analysis."
      },
      {
        "question": "How are molecular subtypes calculated in this code, and which methods are used?",
        "answer": "Molecular subtypes are calculated using two methods: SCMOD2 and PAM50. The `molecular.subtyping` function from the genefu package is used with the arguments `sbt.model = 'scmod2'` and `sbt.model = 'pam50'` respectively. These methods classify breast cancer samples into subtypes such as Basal, HER2+, Luminal A, and Luminal B based on gene expression patterns."
      },
      {
        "question": "What is the purpose of the `cvpl` function in the code, and how is it used to compare PAM50 and SCMOD2?",
        "answer": "The `cvpl` (Cross-Validated Partial Likelihood) function is used to assess and compare the predictive performance of PAM50 and SCMOD2 subtyping methods. It calculates the CVPL score for each method using patient age, survival time, and subtype classification as inputs. The resulting CVPL scores are then compared to evaluate which subtyping method provides better predictive power for patient outcomes."
      }
    ],
    "completion_tasks": [
      {
        "partial": "## ----computeCindexWithPvalue--------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\"GGI\",\"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrixFull <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  ## cindex and p-value computation per algorithm\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(tt); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrixFull[[Dat]] <- rbind(resMatrixFull[[Dat]], cindex)\n  }\n}",
        "complete": "## ----computeCindexWithPvalue--------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\"GGI\",\"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrixFull <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  ## cindex and p-value computation per algorithm\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(tt); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrixFull[[Dat]] <- rbind(resMatrixFull[[Dat]], cindex)\n  }\n}\n\nfor(i in names(resMatrixFull)){\n  rownames(resMatrixFull[[i]]) <- datasetList\n}\n\nccmData <- tt <- rr <- NULL\nfor(i in 1:length(resMatrixFull)){\n  tt <- NULL\n  for(j in 1:length(resMatrixFull)){\n    if(i != j) { rr <- cindex.comp.meta(list.cindex1=resMatrixFull[[i]],\n                                        list.cindex2=resMatrixFull[[j]], hetero=TRUE)$p.value }\n    else { rr <- 1 }\n    tt <- cbind(tt, rr)\n  }\n  ccmData <- rbind(ccmData, tt)\n}\nccmData <- as.data.frame(ccmData)\ncolnames(ccmData) <- riskPList\nrownames(ccmData) <- riskPList"
      },
      {
        "partial": "## ----computeCCMPval-----------------------------------------------------------\nccmDataPval <- matrix(p.adjust(data.matrix(ccmData), method=\"holm\"),\n                      ncol=length(riskPList), dimnames=list(rownames(ccmData),\n                                                            colnames(ccmData)))",
        "complete": "## ----computeCCMPval-----------------------------------------------------------\nccmDataPval <- matrix(p.adjust(data.matrix(ccmData), method=\"holm\"),\n                      ncol=length(riskPList), dimnames=list(rownames(ccmData),\n                                                            colnames(ccmData)))\n\n## ----sessionInfo, echo=FALSE--------------------------------------------------\nsessionInfo()"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeIC50.R",
    "language": "R",
    "content": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose=TRUE,\n                    trunc=TRUE))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeIC50` function and how does it relate to `computeICn`?",
        "answer": "The `computeIC50` function is a wrapper for the `computeICn` function, specifically designed to calculate the IC50 (half maximal inhibitory concentration) of a drug dose-response curve. It calls `computeICn` with a fixed `n` value of 50 (or 0.5 if viability is not in percentage), while passing through most of its parameters. This function simplifies the process of calculating IC50 by providing a dedicated interface for this common use case."
      },
      {
        "question": "How does the `viability_as_pct` parameter affect the calculation of IC50?",
        "answer": "The `viability_as_pct` parameter determines how the viability data is interpreted and how the IC50 is calculated. If `viability_as_pct` is TRUE (default), the function assumes the viability data is in percentage and sets the `n` parameter in `computeICn` to 50. If FALSE, it assumes the viability data is in decimal form (0-1 range) and sets `n` to 0.5. This ensures that the IC50 is correctly calculated regardless of the input data format."
      },
      {
        "question": "What is the significance of the `@describeIn` and `@export` roxygen2 tags in this function definition?",
        "answer": "The roxygen2 tags provide metadata for documentation and package building:\n1. `@describeIn computeICn`: This tag indicates that the `computeIC50` function should be documented as part of the `computeICn` function family. It will appear in the same help file as `computeICn`, improving organization of related functions.\n2. `@export`: This tag marks the `computeIC50` function to be exported when building the R package, making it available for users to call directly after loading the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ,\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}"
      },
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(,\n                    ,\n                    ,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    ,\n                    ,\n                    ,\n                    ))\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ps.cluster.R",
    "language": "R",
    "content": "#' @title Function to compute the prediction strength of a clustering model\n#'\n#' @description\n#' This function computes the prediction strength of a clustering model as published \n#'   in R. Tibshirani and G. Walther 2005.\n#'\n#' @usage\n#' ps.cluster(cl.tr, cl.ts, na.rm = FALSE)\n#'\n#' @param cl.tr\tClusters membership as defined by the original clustering model, i.e. \n#'   the one that was not fitted on the dataset of interest.\n#' @param cl.ts\tClusters membership as defined by the clustering model fitted on the \n#'   dataset of interest.\n#' @param na.rm\tTRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - ps: the overall prediction strength (minimum of the prediction strengths at cluster level).\n#' - ps.cluster: Prediction strength for each cluster\n#' - ps.individual: Prediction strength for each sample.\n#'\n#' @references\n#' R. Tibshirani and G. Walther (2005) \"Cluster Validation by Prediction Strength\", \n#'   Journal of Computational and Graphical Statistics, 14(3):511-528.\n#'\n#' @examples\n#' # load SSP signature published in Sorlie et al. 2003\n#' data(ssp2003)\n#' # load NKI data\n#' data(nkis)\n#' # SP2003 fitted on NKI\n#' ssp2003.2nkis <- intrinsic.cluster(data=data.nkis, annot=annot.nkis,\n#'   do.mapping=TRUE, std=\"robust\",\n#'   intrinsicg=ssp2003$centroids.map[ ,c(\"probe\", \"EntrezGene.ID\")],\n#'   number.cluster=5, mins=5, method.cor=\"spearman\",\n#'   method.centroids=\"mean\", verbose=TRUE)\n#' # SP2003 published in Sorlie et al 2003 and applied in VDX\n#' ssp2003.nkis <- intrinsic.cluster.predict(sbt.model=ssp2003,\n#'   data=data.nkis, annot=annot.nkis, do.mapping=TRUE, verbose=TRUE)\n#' # prediction strength of sp2003 clustering model\n#' ps.cluster(cl.tr=ssp2003.2nkis$subtype, cl.ts=ssp2003.nkis$subtype,\n#'   na.rm = FALSE)\n#'\n#' @md\n#' @export\nps.cluster <-\nfunction(cl.tr, cl.ts, na.rm=FALSE) {\n\t## consider cl.ts as reference\n\tif(length(cl.tr) != length(cl.ts)) { stop(\"the two clustering must have the same length!\") }\n\tif(is.null(names(cl.tr))) { names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\") }\n\tcc.ix <- complete.cases(cl.tr, cl.ts)\n\tif(any(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tnn <- sum(cc.ix)\n\tcltr <- cl.tr[cc.ix]\n\tclts <- cl.ts[cc.ix]\n\tucltr <- sort(unique(cltr))\t\n\tuclts <- sort(unique(clts))\n\tif(length(ucltr) != length(uclts)) {\n\t\tmessage(\"the number of clusters should be the same\")\n\t\ttt <- rep(NA, max(length(ucltr), length(uclts)))\n\t\tnames(tt) <- ifelse(length(ucltr) > length(uclts), ucltr, uclts)\n\t\ttt2 <- rep(NA, length(cl.tr))\n\t\tnames(tt2) <- names(cl.tr)\n\t\treturn(list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2))\n\t}\n\tif(!all(ucltr == uclts)) { ## the number of clusters is the same but the labels differ\n\t\tmessage(\"the labels of the clusters differ\")\n\t\t## keep labels from cl.ts\n\t\ttixtr <- !is.element(ucltr, uclts)\n\t\ttixts <- !is.element(uclts, ucltr)\n\t\tfor(mm in 1:sum(tixtr)) {\n\t\t\tcltr[cltr == ucltr[which(tixtr)[mm]]] <- uclts[which(tixts)[mm]]\n\t\t}\n\t\tucltr <- sort(unique(cltr))\t\n\t}\n\tll <- length(uclts)\n\t\n\t## co-membership matrix for the two clusterings\n\tDtr <- matrix(NA, nrow=nn, ncol=nn, dimnames=list(names(cltr), names(cltr)))\n\tDts <- matrix(NA, nrow=nn, ncol=nn, dimnames=list(names(clts), names(clts)))\n\tfor(i in 1:(nn-1)) {\n\t\tfor(j in (i+1):nn) {\n\t\t\tif(cltr[i] == cltr[j]) { Dtr[i,j] <- Dtr[j,i] <- 1 } else { Dtr[i,j] <- Dtr[j,i] <- 0 }\n\t\t\tif(clts[i] == clts[j]) { Dts[i,j] <- Dts[j,i] <- 1 } else { Dts[i,j] <- Dts[j,i] <- 0 }\n\t\t}\n\t}\n\t\n\tnltr <- table(cltr)[as.character(ucltr)]\n\tnlts <- table(clts)[as.character(uclts)]\n\tmyps <- NULL\n\tfor(l in 1:ll) {\n\t\tal <- which(clts == uclts[l])\n\t\tif(length(al) > 1) { rr <- (1 / (nlts[l] * (nlts[l] - 1))) * sum(Dtr[al,al], na.rm=TRUE) } else { rr <- 0}\n\t\tmyps <- c(myps, rr)\t\n\t}\n\tif(length(uclts) >= length(ucltr)) { names(myps) <- uclts } else { names(myps) <- ucltr }\n\t\n\tmyps.ind <- NULL\n\tfor(i in 1:nn) {\n\t\taki <- Dts[i, ] == 1 & !is.na(Dts[i, ])\n\t\trr <- (1 / sum(aki)) * sum(Dtr[i,aki] == 1, na.rm=TRUE)\n\t\tmyps.ind <- c(myps.ind, rr)\n\t}\n\tnames(myps.ind) <- names(clts)\n\tmyps.ind2 <- rep(NA, length(cl.tr))\n\tnames(myps.ind2) <- names(cl.tr)\n\tmyps.ind2[names(myps.ind)] <- myps.ind\n\n\treturn(list(\"ps\"=min(myps), \"ps.cluster\"=myps, \"ps.individual\"=myps.ind2))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ps.cluster` function and what does it return?",
        "answer": "The `ps.cluster` function computes the prediction strength of a clustering model as described by R. Tibshirani and G. Walther (2005). It takes two cluster memberships (`cl.tr` and `cl.ts`) as input and returns a list containing three items: 1) 'ps': the overall prediction strength (minimum of the prediction strengths at cluster level), 2) 'ps.cluster': Prediction strength for each cluster, and 3) 'ps.individual': Prediction strength for each sample."
      },
      {
        "question": "How does the function handle cases where the number of clusters or cluster labels differ between `cl.tr` and `cl.ts`?",
        "answer": "The function checks if the number of clusters in `cl.tr` and `cl.ts` are the same. If they differ, it returns a list with 'ps' set to 0 and NA values for 'ps.cluster' and 'ps.individual'. If the number of clusters is the same but labels differ, it attempts to match the labels from `cl.ts` to `cl.tr`. The function issues messages in both cases to inform the user about these discrepancies."
      },
      {
        "question": "Explain the algorithm used to calculate the prediction strength for each cluster in the `ps.cluster` function.",
        "answer": "The function calculates prediction strength for each cluster using these steps: 1) Create co-membership matrices (Dtr and Dts) for both clusterings. 2) For each cluster l in cl.ts, find all samples al in that cluster. 3) Calculate the prediction strength for cluster l as the sum of co-memberships in Dtr for samples in al, divided by nlts[l] * (nlts[l] - 1), where nlts[l] is the number of samples in cluster l. 4) The overall prediction strength 'ps' is the minimum of all cluster prediction strengths. This approach measures how well the clustering structure from cl.tr predicts the clustering in cl.ts."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) { stop(\"the two clustering must have the same length!\") }\n  if(is.null(names(cl.tr))) { names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\") }\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  # Complete the function to calculate prediction strength\n  \n}",
        "complete": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) { stop(\"the two clustering must have the same length!\") }\n  if(is.null(names(cl.tr))) { names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\") }\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  if(length(ucltr) != length(uclts)) {\n    message(\"the number of clusters should be the same\")\n    tt <- rep(NA, max(length(ucltr), length(uclts)))\n    names(tt) <- ifelse(length(ucltr) > length(uclts), ucltr, uclts)\n    tt2 <- rep(NA, length(cl.tr))\n    names(tt2) <- names(cl.tr)\n    return(list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2))\n  }\n  \n  if(!all(ucltr == uclts)) {\n    message(\"the labels of the clusters differ\")\n    tixtr <- !is.element(ucltr, uclts)\n    tixts <- !is.element(uclts, ucltr)\n    for(mm in 1:sum(tixtr)) {\n      cltr[cltr == ucltr[which(tixtr)[mm]]] <- uclts[which(tixts)[mm]]\n    }\n    ucltr <- sort(unique(cltr))\t\n  }\n  \n  ll <- length(uclts)\n  Dtr <- Dts <- matrix(0, nrow=nn, ncol=nn)\n  for(i in 1:(nn-1)) {\n    for(j in (i+1):nn) {\n      Dtr[i,j] <- Dtr[j,i] <- as.integer(cltr[i] == cltr[j])\n      Dts[i,j] <- Dts[j,i] <- as.integer(clts[i] == clts[j])\n    }\n  }\n  \n  nlts <- table(clts)\n  myps <- sapply(uclts, function(l) {\n    al <- which(clts == l)\n    if(length(al) > 1) sum(Dtr[al,al]) / (nlts[as.character(l)] * (nlts[as.character(l)] - 1)) else 0\n  })\n  \n  myps.ind <- sapply(1:nn, function(i) {\n    aki <- Dts[i, ] == 1\n    sum(Dtr[i,aki]) / sum(aki)\n  })\n  names(myps.ind) <- names(clts)\n  myps.ind2 <- rep(NA, length(cl.tr))\n  names(myps.ind2) <- names(cl.tr)\n  myps.ind2[names(myps.ind)] <- myps.ind\n\n  list(\"ps\"=min(myps), \"ps.cluster\"=myps, \"ps.individual\"=myps.ind2)\n}"
      },
      {
        "partial": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) stop(\"the two clustering must have the same length!\")\n  if(is.null(names(cl.tr))) names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\")\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) stop(\"missing values are present!\")\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  # Complete the function to handle different cluster numbers and labels\n  \n}",
        "complete": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) stop(\"the two clustering must have the same length!\")\n  if(is.null(names(cl.tr))) names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\")\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) stop(\"missing values are present!\")\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  if(length(ucltr) != length(uclts)) {\n    message(\"the number of clusters should be the same\")\n    tt <- rep(NA, max(length(ucltr), length(uclts)))\n    names(tt) <- ifelse(length(ucltr) > length(uclts), ucltr, uclts)\n    tt2 <- rep(NA, length(cl.tr))\n    names(tt2) <- names(cl.tr)\n    return(list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2))\n  }\n  \n  if(!all(ucltr == uclts)) {\n    message(\"the labels of the clusters differ\")\n    tixtr <- !is.element(ucltr, uclts)\n    tixts <- !is.element(uclts, ucltr)\n    for(mm in 1:sum(tixtr)) {\n      cltr[cltr == ucltr[which(tixtr)[mm]]] <- uclts[which(tixts)[mm]]\n    }\n    ucltr <- sort(unique(cltr))\t\n  }\n  \n  ll <- length(uclts)\n  Dtr <- Dts <- matrix(0, nrow=nn, ncol=nn)\n  for(i in 1:(nn-1)) {\n    for(j in (i+1):nn) {\n      Dtr[i,j] <- Dtr[j,i] <- as.integer(cltr[i] == cltr[j])\n      Dts[i,j] <- Dts[j,i] <- as.integer(clts[i] == clts[j])\n    }\n  }\n  \n  nlts <- table(clts)\n  myps <- sapply(uclts, function(l) {\n    al <- which(clts == l)\n    if(length(al) > 1) sum(Dtr[al,al]) / (nlts[as.character(l)] * (nlts[as.character(l)] - 1)) else 0\n  })\n  \n  myps.ind <- sapply(1:nn, function(i) {\n    aki <- Dts[i, ] == 1\n    sum(Dtr[i,aki]) / sum(aki)\n  })\n  names(myps.ind) <- names(clts)\n  myps.ind2 <- rep(NA, length(cl.tr))\n  names(myps.ind2) <- names(cl.tr)\n  myps.ind2[names(myps.ind)] <- myps.ind\n\n  list(\"ps\"=min(myps), \"ps.cluster\"=myps, \"ps.individual\"=myps.ind2)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/dindex.comp.R",
    "language": "R",
    "content": "`dindex.comp` <-\nfunction(dindex1, dindex2) {\n\tif(dindex1$n != dindex2$n) { stop(\"the D indices are computed from different number of samples!\") }\n\tn <- dindex1$n\n\tx1 <- dindex1$data$z\n\tx2 <- dindex2$data$z\n\tbeta1 <- dindex1$coef\n\tbeta2 <- dindex2$coef\n\tse1 <- dindex1$se\n\tse2 <- dindex2$se\n\tr <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n\tif(abs(r) < 1) {\n\t\tt.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"dindex1\"=exp(beta1), \"dindex2\"=exp(beta2)))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dindex.comp` function in R, and what are its main inputs?",
        "answer": "The `dindex.comp` function in R is designed to compare two D indices. It takes two main inputs: `dindex1` and `dindex2`, which are expected to be objects containing D index information. The function compares these indices and returns a list with the p-value of their difference and the exponential of their coefficients."
      },
      {
        "question": "How does the function handle the case when the correlation between x1 and x2 is perfect (1 or -1)?",
        "answer": "When the absolute value of the correlation (r) between x1 and x2 is 1 (perfect correlation), the function sets the p-value (`diff.ci.p`) to 1. This is handled in the if-else statement: if `abs(r) < 1`, it calculates the t-statistic and p-value; otherwise, it directly sets `diff.ci.p <- 1`."
      },
      {
        "question": "What statistical test is being performed in this function, and how is the test statistic calculated?",
        "answer": "The function performs a t-test to compare the two D indices. The test statistic (t.stat) is calculated as the difference between the coefficients (beta1 - beta2) divided by the square root of the sum of their squared standard errors minus twice their product multiplied by the correlation: (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2). This formula accounts for the correlation between the two indices."
      }
    ],
    "completion_tasks": [
      {
        "partial": "dindex.comp <- function(dindex1, dindex2) {\n  if(dindex1$n != dindex2$n) { stop(\"the D indices are computed from different number of samples!\") }\n  n <- dindex1$n\n  x1 <- dindex1$data$z\n  x2 <- dindex2$data$z\n  beta1 <- dindex1$coef\n  beta2 <- dindex2$coef\n  se1 <- dindex1$se\n  se2 <- dindex2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    # Complete the t-statistic calculation and p-value computation\n  } else { diff.ci.p <- 1 }\n  # Return the result\n}",
        "complete": "dindex.comp <- function(dindex1, dindex2) {\n  if(dindex1$n != dindex2$n) { stop(\"the D indices are computed from different number of samples!\") }\n  n <- dindex1$n\n  x1 <- dindex1$data$z\n  x2 <- dindex2$data$z\n  beta1 <- dindex1$coef\n  beta2 <- dindex2$coef\n  se1 <- dindex1$se\n  se2 <- dindex2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"dindex1\"=exp(beta1), \"dindex2\"=exp(beta2)))\n}"
      },
      {
        "partial": "dindex.comp <- function(dindex1, dindex2) {\n  # Check if the number of samples is the same\n  # Extract necessary data from dindex1 and dindex2\n  # Calculate the correlation\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  # Return the result\n}",
        "complete": "dindex.comp <- function(dindex1, dindex2) {\n  if(dindex1$n != dindex2$n) stop(\"the D indices are computed from different number of samples!\")\n  n <- dindex1$n\n  x1 <- dindex1$data$z\n  x2 <- dindex2$data$z\n  beta1 <- dindex1$coef\n  beta2 <- dindex2$coef\n  se1 <- dindex1$se\n  se2 <- dindex2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(p.value=diff.ci.p, dindex1=exp(beta1), dindex2=exp(beta2)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/forestplot.surv.R",
    "language": "R",
    "content": "`forestplot.surv` <-\nfunction(labeltext, mean, lower, upper, align=NULL, is.summary=FALSE, clip=c(-Inf,Inf), xlab=\"\", zero= 0, graphwidth=unit(2,\"inches\"), col, xlog=FALSE, box.size=NULL, x.ticks=NULL, ...){\n\t\n  #require(\"grid\") || stop(\"`grid' package not found\")\n  #require(\"rmeta\") || stop(\"`rmeta' package not found\")\n\n  ## Function to draw a non-summary rect-plus-CI\n  drawNormalCI <- function(LL, OR, UL, size, bcol, lcol) {\n    size=0.75*size\n\n    clipupper<-convertX(unit(UL, \"native\"), \"npc\", valueOnly=TRUE) > 1\n    cliplower<-convertX(unit(LL, \"native\"), \"npc\", valueOnly=TRUE) < 0\n    box<- convertX(unit(OR, \"native\"), \"npc\", valueOnly=TRUE)\n    clipbox <- box<0 || box>1\n        \n    ## Draw arrow if exceed col range\n    ## convertX() used to convert between coordinate systems\n    if (clipupper || cliplower){\n      ends<-\"both\"\n      lims<-unit(c(0, 1), c(\"npc\", \"npc\"))\n      if (!clipupper) {\n        ends<-\"first\"\n        lims<-unit(c(0, UL), c(\"npc\",\"native\"))\n      }\n      if (!cliplower) {\n        ends<-\"last\"\n        lims<-unit(c(LL, 1), c(\"native\", \"npc\"))\n      }\n      grid.lines(x=lims, y=0.5,arrow=arrow(ends=ends,length=unit(0.05, \"inches\")),\n                 gp=gpar(col=lcol))\n\n      if (!clipbox)\n          grid.rect(x=unit(OR, \"native\"),\n                    width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                    gp=gpar(fill=bcol,col=bcol))\n      \n      } else   {\n      ## Draw line white if totally inside rect\n      grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5,\n                 gp=gpar(col=lcol))\n      grid.rect(x=unit(OR, \"native\"),\n                width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                gp=gpar(fill=bcol,col=bcol))\n      if ((convertX(unit(OR, \"native\") + unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) > UL) &&\n          (convertX(unit(OR, \"native\") - unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) < LL))\n        grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5, gp=gpar(col=lcol))\n    }\n  }\n  \n  ## Function to draw a summary \"diamond\"\n  drawSummaryCI <- function(LL, OR, UL, size, scol) {\n    grid.polygon(x=unit(c(LL, OR, UL, OR), \"native\"),\n                 y=unit(0.5 + c(0, 0.5*size, 0, -0.5*size), \"npc\"),gp=gpar(fill=scol,col=scol))\n  }\n  \n  plot.new()\n  ## calculate width based on labels with something in every column\n  widthcolumn<-!apply(is.na(labeltext),1,any)\n  if(missing(col)) { col <- rmeta::meta.colors() }\n  nc<-NCOL(labeltext)\n  nr<-NROW(labeltext)\n  labels<-vector(\"list\",nc)\n  if(length(col$lines) < nr) { col$lines <- rep(col$lines[1], nr) }\n  if(length(col$box) < nr) { col$box <- rep(col$box[1], nr) }\n  if(length(col$summary) < nr) { col$summary <- rep(col$summary[1], nr) }\n  \n  if (is.null(align))\n    align<-c(\"l\",rep(\"r\",nc-1))\n  else\n    align<-rep(align,length=nc)\n  \n  is.summary<-rep(is.summary,length=nr)\n  \n  for(j in 1:nc){\n    labels[[j]]<-vector(\"list\", nr)\n    for(i in 1:nr){\n      if (is.na(labeltext[i,j]))\n        next\n      x<-switch(align[j],l=0,r=1,c=0.5)\n      just<-switch(align[j],l=\"left\",r=\"right\",c=\"center\")\n      labels[[j]][[i]]<-textGrob(labeltext[i,j], x=x,just=just,\n                                 gp=gpar(fontface=if(is.summary[i]) \"bold\" else \"plain\",\n                                                                    col=rep(col$text,length=nr)[i]) )\n    }\n  }  \n  colgap<-unit(3,\"mm\") \n  colwidths<-unit.c(max(unit(rep(1,sum(widthcolumn)),\"grobwidth\",labels[[1]][widthcolumn])),colgap)\n  if (nc>1){\n    for(i in 2:nc)\n      colwidths<-unit.c(colwidths, max(unit(rep(1,sum(widthcolumn)),\"grobwidth\",labels[[i]][widthcolumn])),colgap)\n    \n  }\n  colwidths<-unit.c(colwidths,graphwidth)\n  \n  pushViewport(viewport(layout=grid.layout(nr+1,nc*2+1,\n                          widths=colwidths,\n                          heights=unit(c(rep(1, nr),0.5), \"lines\"))))\n  \n  cwidth<-(upper-lower)\n  xrange<-c(max(min(lower,na.rm=TRUE),clip[1]), min(max(upper,na.rm=TRUE),clip[2]))\n  if(is.null(box.size)) {\n    info<-1/cwidth\n    info<-info/max(info[!is.summary], na.rm=TRUE)\n    info[is.summary]<-1\n  }\n  else { info <- box.size }\n  for(j in 1:nc){\n    for(i in 1:nr){\n      if (!is.null(labels[[j]][[i]])){\n        pushViewport(viewport(layout.pos.row=i,layout.pos.col=2*j-1))\n        grid.draw(labels[[j]][[i]])\n          popViewport()\n      }\n    }\n  }\n  \n  pushViewport(viewport(layout.pos.col=2*nc+1, xscale=xrange))\n  grid.lines(x=unit(zero, \"native\"), y=0:1,gp=gpar(col=col$zero))\n  if (xlog){\n    ticks<-pretty(exp(xrange))\n    ticks<-ticks[ticks>0]\n    if (min(lower,na.rm=TRUE)<clip[1]) ticks<-c(exp(clip[1]),ticks)\n    if (max(upper,na.rm=TRUE)>clip[2]) ticks<-c(ticks,exp(clip[2]))\n    xax<-xaxisGrob(gp=gpar(cex=0.6,col=col$axes),at=log(ticks),name=\"xax\")\n    xax1<-editGrob(xax, gPath(\"labels\"), label=format(ticks,digits=2))\n    grid.draw(xax1)\n  } else {\n    grid.xaxis(at=x.ticks, gp=gpar(cex=1,col=col$axes))\n  }\n  grid.text(xlab, y=unit(-3, \"lines\"),gp=gpar(col=col$axes))\n  popViewport()\n  for (i in 1:nr) {\n    if (is.na(mean[i])) next\n    pushViewport(viewport(layout.pos.row=i, layout.pos.col=2*nc+1,\n                          xscale=xrange))\n    if (is.summary[i])\n      drawSummaryCI(lower[i], mean[i], upper[i], info[i], scol=col$summary[i])\n    else\n      drawNormalCI(lower[i], mean[i], upper[i], info[i], bcol=col$box[i], lcol=col$lines[i]) \n    popViewport()\n  }\n  popViewport()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drawNormalCI` function within the `forestplot.surv` function?",
        "answer": "The `drawNormalCI` function is responsible for drawing a non-summary rectangle with confidence intervals on the forest plot. It handles the visualization of individual study results, including clipping for out-of-range values and drawing arrows when necessary. The function takes care of positioning the rectangle (representing the point estimate) and the confidence interval lines, adjusting for various edge cases and clipping scenarios."
      },
      {
        "question": "How does the `forestplot.surv` function handle the alignment of text labels in the plot?",
        "answer": "The function handles text label alignment through the `align` parameter. If not provided, it defaults to left-aligned for the first column and right-aligned for the rest. The alignment is used in the `textGrob` function call within a nested loop, where `x` and `just` parameters are set based on the alignment value ('l' for left, 'r' for right, 'c' for center). This allows for flexible positioning of text labels in each column of the forest plot."
      },
      {
        "question": "What is the purpose of the `xlog` parameter in the `forestplot.surv` function, and how does it affect the x-axis?",
        "answer": "The `xlog` parameter determines whether the x-axis should be displayed on a logarithmic scale. When `xlog` is TRUE, the function calculates logarithmic tick marks using `pretty(exp(xrange))`, ensures all ticks are positive, and may add additional ticks at the clip boundaries. It then uses `xaxisGrob` to create a custom x-axis with these logarithmic ticks, formatting the labels to show the original (non-log) values. This is particularly useful for visualizing data that spans several orders of magnitude, such as odds ratios or hazard ratios in meta-analyses."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drawNormalCI <- function(LL, OR, UL, size, bcol, lcol) {\n  size = 0.75 * size\n  clipupper <- convertX(unit(UL, \"native\"), \"npc\", valueOnly=TRUE) > 1\n  cliplower <- convertX(unit(LL, \"native\"), \"npc\", valueOnly=TRUE) < 0\n  box <- convertX(unit(OR, \"native\"), \"npc\", valueOnly=TRUE)\n  clipbox <- box < 0 || box > 1\n\n  if (clipupper || cliplower) {\n    ends <- \"both\"\n    lims <- unit(c(0, 1), c(\"npc\", \"npc\"))\n    if (!clipupper) {\n      ends <- \"first\"\n      lims <- unit(c(0, UL), c(\"npc\", \"native\"))\n    }\n    if (!cliplower) {\n      ends <- \"last\"\n      lims <- unit(c(LL, 1), c(\"native\", \"npc\"))\n    }\n    grid.lines(x=lims, y=0.5, arrow=arrow(ends=ends, length=unit(0.05, \"inches\")),\n                gp=gpar(col=lcol))\n\n    if (!clipbox) {\n      grid.rect(x=unit(OR, \"native\"),\n                width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                gp=gpar(fill=bcol, col=bcol))\n    }\n  } else {\n    # Complete the else part\n  }\n}",
        "complete": "drawNormalCI <- function(LL, OR, UL, size, bcol, lcol) {\n  size = 0.75 * size\n  clipupper <- convertX(unit(UL, \"native\"), \"npc\", valueOnly=TRUE) > 1\n  cliplower <- convertX(unit(LL, \"native\"), \"npc\", valueOnly=TRUE) < 0\n  box <- convertX(unit(OR, \"native\"), \"npc\", valueOnly=TRUE)\n  clipbox <- box < 0 || box > 1\n\n  if (clipupper || cliplower) {\n    ends <- \"both\"\n    lims <- unit(c(0, 1), c(\"npc\", \"npc\"))\n    if (!clipupper) {\n      ends <- \"first\"\n      lims <- unit(c(0, UL), c(\"npc\", \"native\"))\n    }\n    if (!cliplower) {\n      ends <- \"last\"\n      lims <- unit(c(LL, 1), c(\"native\", \"npc\"))\n    }\n    grid.lines(x=lims, y=0.5, arrow=arrow(ends=ends, length=unit(0.05, \"inches\")),\n                gp=gpar(col=lcol))\n\n    if (!clipbox) {\n      grid.rect(x=unit(OR, \"native\"),\n                width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                gp=gpar(fill=bcol, col=bcol))\n    }\n  } else {\n    grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5, gp=gpar(col=lcol))\n    grid.rect(x=unit(OR, \"native\"),\n              width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n              gp=gpar(fill=bcol, col=bcol))\n    if ((convertX(unit(OR, \"native\") + unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) > UL) &&\n        (convertX(unit(OR, \"native\") - unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) < LL))\n      grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5, gp=gpar(col=lcol))\n  }\n}"
      },
      {
        "partial": "forestplot.surv <- function(labeltext, mean, lower, upper, align=NULL, is.summary=FALSE, clip=c(-Inf,Inf), xlab=\"\", zero=0, graphwidth=unit(2,\"inches\"), col, xlog=FALSE, box.size=NULL, x.ticks=NULL, ...) {\n  # ... (previous code)\n\n  pushViewport(viewport(layout=grid.layout(nr+1, nc*2+1,\n                        widths=colwidths,\n                        heights=unit(c(rep(1, nr), 0.5), \"lines\"))))\n\n  cwidth <- (upper - lower)\n  xrange <- c(max(min(lower, na.rm=TRUE), clip[1]), min(max(upper, na.rm=TRUE), clip[2]))\n  if (is.null(box.size)) {\n    info <- 1 / cwidth\n    info <- info / max(info[!is.summary], na.rm=TRUE)\n    info[is.summary] <- 1\n  } else {\n    info <- box.size\n  }\n\n  # Complete the rest of the function\n}",
        "complete": "forestplot.surv <- function(labeltext, mean, lower, upper, align=NULL, is.summary=FALSE, clip=c(-Inf,Inf), xlab=\"\", zero=0, graphwidth=unit(2,\"inches\"), col, xlog=FALSE, box.size=NULL, x.ticks=NULL, ...) {\n  # ... (previous code)\n\n  pushViewport(viewport(layout=grid.layout(nr+1, nc*2+1,\n                        widths=colwidths,\n                        heights=unit(c(rep(1, nr), 0.5), \"lines\"))))\n\n  cwidth <- (upper - lower)\n  xrange <- c(max(min(lower, na.rm=TRUE), clip[1]), min(max(upper, na.rm=TRUE), clip[2]))\n  if (is.null(box.size)) {\n    info <- 1 / cwidth\n    info <- info / max(info[!is.summary], na.rm=TRUE)\n    info[is.summary] <- 1\n  } else {\n    info <- box.size\n  }\n\n  for (j in 1:nc) {\n    for (i in 1:nr) {\n      if (!is.null(labels[[j]][[i]])) {\n        pushViewport(viewport(layout.pos.row=i, layout.pos.col=2*j-1))\n        grid.draw(labels[[j]][[i]])\n        popViewport()\n      }\n    }\n  }\n\n  pushViewport(viewport(layout.pos.col=2*nc+1, xscale=xrange))\n  grid.lines(x=unit(zero, \"native\"), y=0:1, gp=gpar(col=col$zero))\n  if (xlog) {\n    ticks <- pretty(exp(xrange))\n    ticks <- ticks[ticks > 0]\n    if (min(lower, na.rm=TRUE) < clip[1]) ticks <- c(exp(clip[1]), ticks)\n    if (max(upper, na.rm=TRUE) > clip[2]) ticks <- c(ticks, exp(clip[2]))\n    xax <- xaxisGrob(gp=gpar(cex=0.6, col=col$axes), at=log(ticks), name=\"xax\")\n    xax1 <- editGrob(xax, gPath(\"labels\"), label=format(ticks, digits=2))\n    grid.draw(xax1)\n  } else {\n    grid.xaxis(at=x.ticks, gp=gpar(cex=1, col=col$axes))\n  }\n  grid.text(xlab, y=unit(-3, \"lines\"), gp=gpar(col=col$axes))\n  popViewport()\n\n  for (i in 1:nr) {\n    if (is.na(mean[i])) next\n    pushViewport(viewport(layout.pos.row=i, layout.pos.col=2*nc+1, xscale=xrange))\n    if (is.summary[i])\n      drawSummaryCI(lower[i], mean[i], upper[i], info[i], scol=col$summary[i])\n    else\n      drawNormalCI(lower[i], mean[i], upper[i], info[i], bcol=col$box[i], lcol=col$lines[i])\n    popViewport()\n  }\n  popViewport()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ggi.R",
    "language": "R",
    "content": "#' @title Function to compute the raw and scaled Gene expression Grade Index (GGI)\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene expression\n#'   values following the algorithm used for the Gene expression Grade Index (GGI).\n#'\n#' @usage\n#' ggi(data, annot, do.mapping = FALSE, mapping, hg, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed\n#'   (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#'   mapping such that the probes are not selected based on their variance.\n#' @param hg Vector containing the histological grade (HG) status of breast cancer\n#'   patients in the dataset.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence between\n#' the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Sotiriou C, Wirapati P, Loi S, Harris A, Bergh J, Smeds J, Farmer P, Praz V,\n#'   Haibe-Kains B, Lallemand F, Buyse M, Piccart MJ and Delorenzi M (2006)\n#'   \"Gene expression profiling in breast cancer: Understanding the molecular basis\n#'   of histologic grade to improve prognosis\", Journal of National Cancer Institute,\n#'   98:262\u2013272\n#'\n#' @seealso\n#' [genefu::gene76]\n#'\n#' @examples\n#' # load GGI signature\n#' data(sig.ggi)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' ggi.nkis <- ggi(data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   hg=demo.nkis[ ,\"grade\"])\n#' table(ggi.nkis$risk)\n#'\n#' @md\n#' @export\n#' @name ggi\nggi <- function(data, annot, do.mapping=FALSE, mapping, hg, verbose=FALSE) {\n\n\t###########\n\t#internal functions\n\t###########\n\tscale.raw.ggi <- function(ggi, hg) {\n\tif(length(hg) != length(ggi)) { stop(\"bad length of hg!\") }\n\t\t\tmhg1 <- mean(ggi[hg == 1], na.rm=TRUE)\n\t\t\tmhg3 <- mean(ggi[hg == 3], na.rm=TRUE)\n\t\t\tmm <- mhg1 + (mhg3 -  mhg1) / 2\n\t\t\tres.scaled <- ((ggi - mm) / (mhg3 - mhg1)) * 2\n\t\t\treturn(res.scaled)\n\t}\n\t###########\n\tggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")], \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n\ttt <- sig.score(x=ggi.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)\n\tmyprobe <- tt$probe\n\tmymapping <- tt$mapping\n\tres <- tt$score\n\n\tif(!missing(hg)) {\n\t\tif(length(hg) != nrow(data)) { stop(\"hg must have the same length nrow(data)!\") }\n\t\tmhg1 <- mean(res[hg == 1], na.rm=TRUE)\n\t\tmhg3 <- mean(res[hg == 3], na.rm=TRUE)\n\t\tmm <- mhg1 + (mhg3 -  mhg1) / 2\n\t\tres.scaled <- ((res - mm) / (mhg3 - mhg1)) * 2\n\t\tres <- list(\"score\"=res.scaled, \"risk\"=ifelse(res.scaled >= 0, 1, 0), \"mapping\"=mymapping, \"probe\"=myprobe)\n\t} else {\n\t\triskt <- rep(NA, length(res))\n\t\tnames(riskt) <- names(res)\n\t\tres <- list(\"score\"=res, \"risk\"=riskt, \"mapping\"=mymapping, \"probe\"=myprobe)\n\t}\n\n\treturn (res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'ggi' function and what are its main inputs and outputs?",
        "answer": "The 'ggi' function computes the Gene expression Grade Index (GGI) for breast cancer patients. Its main inputs are a matrix of gene expressions ('data'), gene annotations ('annot'), and optionally the histological grade status ('hg'). The function returns a list containing the continuous signature scores, binary risk classification (high or low risk), the mapping used (if applicable), and the probe-to-gene correspondence (if mapping is performed)."
      },
      {
        "question": "How does the function handle the scaling of raw GGI scores when histological grade information is provided?",
        "answer": "When histological grade information ('hg') is provided, the function scales the raw GGI scores using the 'scale.raw.ggi' internal function. It calculates the mean GGI scores for grade 1 and grade 3 tumors, then scales the scores such that the midpoint between these means becomes 0, and the distance between them becomes 4 (ranging from -2 to 2). The scaled scores are then used to classify patients as high risk (>= 0) or low risk (< 0)."
      },
      {
        "question": "What is the significance of the 'do.mapping' parameter in the 'ggi' function, and how does it affect the function's behavior?",
        "answer": "The 'do.mapping' parameter determines whether the function should perform mapping through Entrez Gene IDs. When set to TRUE, the function handles ambiguities by keeping the most variant probe for each gene. This mapping process helps to standardize the gene identifiers used in the analysis, ensuring that the correct probes are associated with each gene in the GGI signature. The mapping results are included in the function's output, allowing users to see which probes were selected for each gene."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ggi <- function(data, annot, do.mapping=FALSE, mapping, hg, verbose=FALSE) {\n  ggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")], \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n  tt <- sig.score(x=ggi.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)\n  myprobe <- tt$probe\n  mymapping <- tt$mapping\n  res <- tt$score\n\n  if(!missing(hg)) {\n    if(length(hg) != nrow(data)) { stop(\"hg must have the same length nrow(data)!\") }\n    # Complete the code here to calculate scaled scores and risk\n  } else {\n    # Complete the code here for the case when hg is missing\n  }\n\n  return (res)\n}",
        "complete": "ggi <- function(data, annot, do.mapping=FALSE, mapping, hg, verbose=FALSE) {\n  ggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")], \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n  tt <- sig.score(x=ggi.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)\n  myprobe <- tt$probe\n  mymapping <- tt$mapping\n  res <- tt$score\n\n  if(!missing(hg)) {\n    if(length(hg) != nrow(data)) { stop(\"hg must have the same length nrow(data)!\") }\n    mhg1 <- mean(res[hg == 1], na.rm=TRUE)\n    mhg3 <- mean(res[hg == 3], na.rm=TRUE)\n    mm <- mhg1 + (mhg3 -  mhg1) / 2\n    res.scaled <- ((res - mm) / (mhg3 - mhg1)) * 2\n    res <- list(\"score\"=res.scaled, \"risk\"=ifelse(res.scaled >= 0, 1, 0), \"mapping\"=mymapping, \"probe\"=myprobe)\n  } else {\n    riskt <- rep(NA, length(res))\n    names(riskt) <- names(res)\n    res <- list(\"score\"=res, \"risk\"=riskt, \"mapping\"=mymapping, \"probe\"=myprobe)\n  }\n\n  return (res)\n}"
      },
      {
        "partial": "scale.raw.ggi <- function(ggi, hg) {\n  if(length(hg) != length(ggi)) { stop(\"bad length of hg!\") }\n  # Complete the function to scale the raw GGI scores\n}",
        "complete": "scale.raw.ggi <- function(ggi, hg) {\n  if(length(hg) != length(ggi)) { stop(\"bad length of hg!\") }\n  mhg1 <- mean(ggi[hg == 1], na.rm=TRUE)\n  mhg3 <- mean(ggi[hg == 3], na.rm=TRUE)\n  mm <- mhg1 + (mhg3 -  mhg1) / 2\n  res.scaled <- ((ggi - mm) / (mhg3 - mhg1)) * 2\n  return(res.scaled)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/gene70.R",
    "language": "R",
    "content": "#' @title Function to compute the 70 genes prognosis profile (GENE70) as published by\n#' van't Veer et al. 2002\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene expression\n#'   values following the algorithm used for the 70 genes prognosis profile (GENE70) as\n#'   published by van't Veer et al. 2002.\n#'\n#' @usage\n#' gene70(data, annot, do.mapping = FALSE, mapping,\n#'   std = c(\"none\", \"scale\", \"robust\"), verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in case\n#'   of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the mapping\n#'   such that the probes are not selected based on their variance.\n#' @param std Standardization of gene expressions: scale for traditional standardization based\n#'   on mean and standard deviation, robust for standardization based on the 0.025 and\n#'   0.975 quantiles, none to keep gene expressions unchanged.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#'\n#' @return\n#' A list with items:\n#' - score Continuous signature scores\n#' - risk Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping Mapping used if necessary.\n#' - probe If mapping is performed, this matrix contains the correspondence between\n#' the gene list (aka signature) and gene expression data\n#'\n#' @references\n#' L. J. van't Veer and H. Dai and M. J. van de Vijver and Y. D. He and A. A. Hart and\n#'   M. Mao and H. L. Peterse and K. van der Kooy and M. J. Marton and A. T. Witteveen and\n#'   G. J. Schreiber and R. M. Kerkhiven and C. Roberts and P. S. Linsley and R. Bernards\n#'   and S. H. Friend (2002) \"Gene Expression Profiling Predicts Clinical Outcome of Breast\n#'   Cancer\", Nature, 415:530\u2013536.\n#'\n#' @seealso nkis\n#'\n#' @examples\n#' # load GENE70 signature\n#' data(sig.gene70)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' rs.nkis <- gene70(data=data.nkis)\n#' table(rs.nkis$risk)\n#' # note that the discrepancies compared to the original publication\n#' # are closed to the official cutoff, raising doubts on its exact value.\n#' # computation of the signature scores on a different microarray platform\n#' # load VDX dataset\n#' data(vdxs)\n#' # compute relapse score\n#' rs.vdxs <- gene70(data=data.vdxs, annot=annot.vdxs, do.mapping=TRUE)\n#' table(rs.vdxs$risk)\n#'\n#' @md\n#' @export\n#' @name gene70\ngene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n\tif (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n\t\n\tstd <- match.arg(std)\n\tgt <- nrow(sig.gene70)\n\tif(do.mapping) {\n\t\tgid1 <- as.numeric(as.character(sig.gene70[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid1) <- dimnames(sig.gene70)[[1]]\n\t\tgid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tgm <- length(rr$geneid2)\n\t\tif(is.na(rr$geneid1[1])) {\n\t\t\tgm <- 0\n\t\t\t#no gene ids in common\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tgf <- c(\"mapped\"=0, \"total\"=gt)\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n\t\t}\n\t\tgid1 <- rr$geneid2\n\t\tgid2 <- rr$geneid1\n\t\tdata <- rr$data1\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tmyprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n\t\tsig2 <- sig.gene70[names(gid1), , drop=FALSE]\n\t\t## change the names of probes in the data\n\t\tdimnames(data)[[2]] <- names(gid2) <- names(gid1)\n\t} else {\n\t\tdata <- data[ , intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n\t\tsig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n\t\tgm <- nrow(sig2)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tmyprobe <- NA\n\t}\n\n\tif(verbose && gm != gt) { message(sprintf(\"%i/%i probes are used to compute the score\", gm, gt)) }\n\n\t## scaling\n\tswitch(std,\n\t\"scale\"={\n\t\tdata <- scale(data, center=TRUE, scale=TRUE)\n\t\tif(verbose) { message(\"standardization of the gene expressions\") }\n\t},\n\t\"robust\"={\n\t\tdata <- apply(data, 2, function(x) { return((rescale(x, q=0.05, na.rm=TRUE) - 0.5) * 2) })\n\t\tif(verbose) { message(\"robust standardization of the gene expressions\") }\n\t},\n\t\"none\"={ if(verbose) { message(\"no standardization of the gene expressions\") } })\n\n\tscore <- apply(X=data, MARGIN=1, FUN=function (x, y, method, use) {\n\t  rr <- NA\n    if (sum(complete.cases(x, y)) > 3) {\n      rr <- cor(x=x, y=y, method=method, use=use)\n    }\n    return (rr)\n\t}, y=sig2[, \"average.good.prognosis.profile\"], method=\"spearman\", use=\"complete.obs\")\n\tscore <- -score\n\tofficial.cutoff <- -0.3\n\t## cutoff leaving 59% of patients in the poor prognosis group in the original dataset\n\trisk <- ifelse(score >= official.cutoff, 1, 0)\n\n\tnames(score) <- names(risk) <- dimnames(data)[[1]]\n\n\treturn(list(\"score\"=score, \"risk\"=risk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `gene70` function and what algorithm does it implement?",
        "answer": "The `gene70` function computes signature scores and risk classifications from gene expression values, implementing the algorithm used for the 70 genes prognosis profile (GENE70) as published by van't Veer et al. 2002. It's primarily used for predicting clinical outcomes in breast cancer based on gene expression data."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if no gene IDs are in common?",
        "answer": "The function can perform gene mapping if `do.mapping=TRUE`. It uses Entrez Gene IDs to map between the signature genes and the input data. If no gene IDs are in common, the function returns NA for all scores and risks, sets the 'mapped' count to 0, and the 'probe' to NA. This is handled in the code block starting with `if(is.na(rr$geneid1[1])) {`."
      },
      {
        "question": "How is the final risk classification determined in the `gene70` function?",
        "answer": "The risk classification is determined based on the computed score. The function calculates a correlation score using Spearman's method, then inverts it (`score <- -score`). The risk is then classified as high (1) if the score is greater than or equal to the official cutoff of -0.3, and low (0) otherwise. This is implemented in the lines: `official.cutoff <- -0.3` and `risk <- ifelse(score >= official.cutoff, 1, 0)`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig.gene70[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig.gene70)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    if(is.na(rr$geneid1[1])) {\n      gm <- 0\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=gt)\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    sig2 <- sig.gene70[names(gid1), , drop=FALSE]\n    dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n  } else {\n    data <- data[ , intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n    sig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n    gm <- nrow(sig2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- NA\n  }\n\n  if(verbose && gm != gt) { message(sprintf(\"%i/%i probes are used to compute the score\", gm, gt)) }\n\n  # Complete the function by implementing the scaling and score calculation\n}",
        "complete": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig.gene70[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig.gene70)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    if(is.na(rr$geneid1[1])) {\n      gm <- 0\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=gt)\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    sig2 <- sig.gene70[names(gid1), , drop=FALSE]\n    dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n  } else {\n    data <- data[ , intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n    sig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n    gm <- nrow(sig2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- NA\n  }\n\n  if(verbose && gm != gt) { message(sprintf(\"%i/%i probes are used to compute the score\", gm, gt)) }\n\n  switch(std,\n    \"scale\"={ data <- scale(data, center=TRUE, scale=TRUE) },\n    \"robust\"={ data <- apply(data, 2, function(x) { (rescale(x, q=0.05, na.rm=TRUE) - 0.5) * 2 }) },\n    \"none\"={ if(verbose) message(\"no standardization of the gene expressions\") })\n\n  score <- apply(data, 1, function(x, y) {\n    if (sum(complete.cases(x, y)) > 3) -cor(x, y, method=\"spearman\", use=\"complete.obs\") else NA\n  }, y=sig2[, \"average.good.prognosis.profile\"])\n\n  risk <- ifelse(score >= -0.3, 1, 0)\n  names(score) <- names(risk) <- dimnames(data)[[1]]\n\n  list(\"score\"=score, \"risk\"=risk, \"mapping\"=mymapping, \"probe\"=myprobe)\n}"
      },
      {
        "partial": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  \n  # Implement the mapping logic here\n  \n  # Implement the scaling logic here\n  \n  # Implement the score calculation and risk classification here\n  \n  # Return the results\n}",
        "complete": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig.gene70[, \"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig.gene70)[[1]]\n    gid2 <- as.numeric(as.character(annot[, \"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    gid1 <- gid1[!is.na(gid1) & !duplicated(gid1)]\n    \n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    if(is.na(rr$geneid1[1])) {\n      return(list(\"score\"=rep(NA, nrow(data)), \"risk\"=rep(NA, nrow(data)), \n                  \"mapping\"=c(\"mapped\"=0, \"total\"=gt), \"probe\"=NA))\n    }\n    data <- rr$data1\n    mymapping <- c(\"mapped\"=length(rr$geneid2), \"total\"=gt)\n    myprobe <- cbind(\"probe\"=names(rr$geneid2), \"EntrezGene.ID\"=rr$geneid2, \"new.probe\"=names(rr$geneid1))\n    sig2 <- sig.gene70[names(rr$geneid2), , drop=FALSE]\n    dimnames(data)[[2]] <- names(rr$geneid1) <- names(rr$geneid2)\n  } else {\n    data <- data[, intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n    sig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n    mymapping <- c(\"mapped\"=nrow(sig2), \"total\"=gt)\n    myprobe <- NA\n  }\n  \n  data <- switch(std,\n    \"scale\" = scale(data),\n    \"robust\" = apply(data, 2, function(x) (rescale(x, q=0.05, na.rm=TRUE) - 0.5) * 2),\n    data\n  )\n  \n  score <- -apply(data, 1, function(x) {\n    if(sum(complete.cases(x, sig2[, \"average.good.prognosis.profile\"])) > 3)\n      cor(x, sig2[, \"average.good.prognosis.profile\"], method=\"spearman\", use=\"complete.obs\")\n    else NA\n  })\n  \n  risk <- ifelse(score >= -0.3, 1, 0)\n  names(score) <- names(risk) <- rownames(data)\n  \n  list(\"score\"=score, \"risk\"=risk, \"mapping\"=mymapping, \"probe\"=myprobe)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeAUC.R",
    "language": "R",
    "content": "#' Computes the AUC for a Drug Dose Viability Curve\n#'\n#' Returns the AUC (Area Under the drug response Curve) given concentration and viability as input, normalized by the concentration\n#' range of the experiment. The area returned is the response (1-Viablility) area, i.e. area under the curve when the response curve\n#' is plotted on a log10 concentration scale, with high AUC implying high sensitivity to the drug. The function can calculate both\n#' the area under a fitted Hill Curve to the data, and a trapz numeric integral of the actual data provided. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known.\n#'\n#' @examples\n#' dose <- c(\"0.0025\",\"0.008\",\"0.025\",\"0.08\",\"0.25\",\"0.8\",\"2.53\",\"8\")\n#' viability <- c(\"108.67\",\"111\",\"102.16\",\"100.27\",\"90\",\"87\",\"74\",\"57\")\n#' computeAUC(dose, viability)\n#'\n#'\n#' @param concentration `vector` is a vector of drug concentrations.\n#' @param viability `vector` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param Hill_fit `list or vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and returns AUC as a decimal. Otherwise, viability is interpreted as percent, and AUC is returned 0-100.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param area.type Should the area be computed using the actual data (\"Actual\"), or a fitted curve (\"Fitted\")\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return Numeric AUC value\n#' @export\n#' @importFrom caTools trapz\ncomputeAUC <- function (concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        area.type = c(\"Fitted\", \"Actual\"),\n                        verbose = TRUE\n) {\n\n    if (missing(concentration)){\n        stop(\"The concentration values to integrate over must always be provided.\")\n    }\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    } else {\n        area.type <- match.arg(area.type)\n    }\n    if (area.type == \"Fitted\" && missing(Hill_fit)) {\n        Hill_fit <- logLogisticRegression(concentration,\n                                          viability,\n                                          conc_as_log = conc_as_log,\n                                          viability_as_pct = viability_as_pct,\n                                          trunc = trunc,\n                                          verbose = verbose)\n        cleanData <- sanitizeInput(conc=concentration,\n                                   Hill_fit=Hill_fit,\n                                   conc_as_log = conc_as_log,\n                                   viability_as_pct = viability_as_pct,\n                                   trunc = trunc,\n                                  verbose = verbose)\n        pars <- cleanData[[\"Hill_fit\"]]\n        concentration <- cleanData[[\"log_conc\"]]\n    } else if (area.type == \"Fitted\" && !missing(Hill_fit)){\n        cleanData <- sanitizeInput(conc = concentration,\n                                   viability = viability,\n                                   Hill_fit = Hill_fit,\n                                   conc_as_log = conc_as_log,\n                                   viability_as_pct = viability_as_pct,\n                                   trunc = trunc,\n                                   verbose = verbose)\n        pars <- cleanData[[\"Hill_fit\"]]\n        concentration <- cleanData[[\"log_conc\"]]\n     } else if (area.type == \"Actual\" && !missing(viability)){\n        cleanData <- sanitizeInput(conc = concentration,\n                                   viability = viability,\n                                   conc_as_log = conc_as_log,\n                                   viability_as_pct = viability_as_pct,\n                                   trunc = trunc,\n                                   verbose = verbose)\n        concentration <- cleanData[[\"log_conc\"]]\n        viability <- cleanData[[\"viability\"]]\n    } else if (area.type == \"Actual\" && missing(viability)){\n        stop(\"To calculate the actual area using a trapezoid integral, the raw\n             viability values are needed!\")\n    }\n    if (length(concentration) < 2) {\n        return(NA)\n    }\n    a <- min(concentration)\n    b <- max(concentration)\n    if (area.type == \"Actual\") {\n        trapezoid.integral <- caTools::trapz(concentration, viability)\n        AUC <- 1 - trapezoid.integral / (b - a)\n    }\n    else {\n        if(pars[2]==1){\n            AUC <- 0\n      }else if(pars[1]==0){\n          AUC <- (1-pars[2])/2\n      } else {\n          AUC <- as.numeric((1 - pars[2]) / (pars[1] * (b - a)) *\n                              log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) /\n                                      (1 + (10 ^ (a - pars[3])) ^ pars[1])))\n      }\n    }\n    if(viability_as_pct){\n      AUC <- AUC*100\n    }\n    return(AUC)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC` function and what are its main input parameters?",
        "answer": "The `computeAUC` function computes the Area Under the Curve (AUC) for a Drug Dose Viability Curve. It calculates the response area (1-Viability) when plotted on a log10 concentration scale, with a higher AUC indicating higher drug sensitivity. The main input parameters are:\n1. `concentration`: a vector of drug concentrations\n2. `viability`: a vector of viability values corresponding to the concentrations\n3. `Hill_fit`: optional parameters of a Hill Slope (can be provided or calculated internally)\n4. `conc_as_log`: boolean indicating if concentration is given as log10 values\n5. `viability_as_pct`: boolean indicating if viability is given as a percentage"
      },
      {
        "question": "How does the function handle different types of area calculations, and what conditions determine which method is used?",
        "answer": "The function can calculate the AUC using two methods, controlled by the `area.type` parameter:\n1. 'Fitted': Uses a fitted Hill Curve. This is the default method if not specified. It requires either the `Hill_fit` parameters or calculates them using `logLogisticRegression` if not provided.\n2. 'Actual': Uses a trapezoidal numeric integral of the actual data. This requires both `concentration` and `viability` data.\n\nThe conditions determining the method are:\n- If `area.type` is 'Fitted' and `Hill_fit` is missing, it calculates the Hill parameters.\n- If `area.type` is 'Fitted' and `Hill_fit` is provided, it uses the given parameters.\n- If `area.type` is 'Actual' and `viability` is provided, it uses the trapezoidal method.\n- If `area.type` is 'Actual' and `viability` is missing, it throws an error."
      },
      {
        "question": "Explain the significance of the `sanitizeInput` function in this code and how it contributes to the overall functionality of `computeAUC`.",
        "answer": "The `sanitizeInput` function plays a crucial role in data preprocessing for the `computeAUC` function:\n1. It ensures that input data is in the correct format and scale for AUC calculations.\n2. It handles the conversion of concentration values to log scale if needed (`conc_as_log` parameter).\n3. It processes viability data, potentially converting from percentage to decimal form (`viability_as_pct` parameter).\n4. It may truncate viability values to be between 0 and 1 if the `trunc` parameter is set to true.\n5. It standardizes the Hill fit parameters if provided.\n\nBy using `sanitizeInput`, `computeAUC` ensures that regardless of the input format, the core calculation functions work with consistent, properly formatted data. This improves the function's robustness and reduces the chance of errors due to inconsistent input formats."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC <- function (concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        area.type = c(\"Fitted\", \"Actual\"),\n                        verbose = TRUE\n) {\n    if (missing(concentration)){\n        stop(\"The concentration values to integrate over must always be provided.\")\n    }\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    } else {\n        area.type <- match.arg(area.type)\n    }\n    # ... (rest of the function implementation)\n}",
        "complete": "computeAUC <- function (concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        area.type = c(\"Fitted\", \"Actual\"),\n                        verbose = TRUE\n) {\n    if (missing(concentration)){\n        stop(\"The concentration values to integrate over must always be provided.\")\n    }\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    } else {\n        area.type <- match.arg(area.type)\n    }\n    if (area.type == \"Fitted\" && missing(Hill_fit)) {\n        Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log, viability_as_pct, trunc, verbose)\n        cleanData <- sanitizeInput(conc=concentration, Hill_fit=Hill_fit, conc_as_log, viability_as_pct, trunc, verbose)\n        pars <- cleanData[\"Hill_fit\"]\n        concentration <- cleanData[\"log_conc\"]\n    } else if (area.type == \"Fitted\" && !missing(Hill_fit)){\n        cleanData <- sanitizeInput(conc=concentration, viability=viability, Hill_fit=Hill_fit, conc_as_log, viability_as_pct, trunc, verbose)\n        pars <- cleanData[\"Hill_fit\"]\n        concentration <- cleanData[\"log_conc\"]\n    } else if (area.type == \"Actual\" && !missing(viability)){\n        cleanData <- sanitizeInput(conc=concentration, viability=viability, conc_as_log, viability_as_pct, trunc, verbose)\n        concentration <- cleanData[\"log_conc\"]\n        viability <- cleanData[\"viability\"]\n    } else if (area.type == \"Actual\" && missing(viability)){\n        stop(\"To calculate the actual area using a trapezoid integral, the raw viability values are needed!\")\n    }\n    if (length(concentration) < 2) return(NA)\n    a <- min(concentration)\n    b <- max(concentration)\n    AUC <- if (area.type == \"Actual\") {\n        1 - caTools::trapz(concentration, viability) / (b - a)\n    } else {\n        if (pars[2] == 1) 0\n        else if (pars[1] == 0) (1 - pars[2]) / 2\n        else (1 - pars[2]) / (pars[1] * (b - a)) * log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) / (1 + (10 ^ (a - pars[3])) ^ pars[1]))\n    }\n    if (viability_as_pct) AUC <- AUC * 100\n    return(AUC)\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n    # ... (implementation details)\n}",
        "complete": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n    if (!conc_as_log) {\n        log_conc <- log10(as.numeric(conc))\n    } else {\n        log_conc <- as.numeric(conc)\n    }\n    if (!missing(viability)) {\n        viability <- as.numeric(viability)\n        if (viability_as_pct) {\n            viability <- viability / 100\n        }\n        if (trunc) {\n            viability[viability > 1] <- 1\n            viability[viability < 0] <- 0\n        }\n    }\n    if (!missing(Hill_fit)) {\n        if (viability_as_pct) {\n            Hill_fit[2] <- Hill_fit[2] / 100\n        }\n        if (!conc_as_log) {\n            Hill_fit[3] <- log10(Hill_fit[3])\n        }\n    }\n    return(list(log_conc = log_conc, viability = viability, Hill_fit = Hill_fit))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(ToxicoGx)\ntest_check(\"ToxicoGx\")",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'library()' function in R, and why are 'testthat' and 'ToxicoGx' being loaded in this code snippet?",
        "answer": "The 'library()' function in R is used to load and attach add-on packages. In this code snippet, 'testthat' and 'ToxicoGx' are being loaded. 'testthat' is a popular testing framework for R, while 'ToxicoGx' is likely a package related to toxicogenomics. These packages are being loaded to prepare the environment for running tests on the 'ToxicoGx' package."
      },
      {
        "question": "What does the 'test_check()' function do, and what argument is being passed to it in this code?",
        "answer": "The 'test_check()' function is part of the 'testthat' package and is used to run all the tests in a package. It automatically finds and executes all test files in the 'tests/testthat' directory of the package. In this code, the argument 'ToxicoGx' is being passed to 'test_check()', which means it will run all the tests for the 'ToxicoGx' package."
      },
      {
        "question": "How would you modify this code to run tests for a different package, and what considerations should be taken into account?",
        "answer": "To modify this code to run tests for a different package, you would need to change two things: 1) Replace 'library(ToxicoGx)' with 'library(YourPackageName)', where YourPackageName is the name of the package you want to test. 2) Change the argument in 'test_check('ToxicoGx')' to 'test_check('YourPackageName')'. Considerations include ensuring that the new package is installed, has tests written using the 'testthat' framework, and that the tests are located in the correct directory (tests/testthat) within the package structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(testthat)\nlibrary(ToxicoGx)\ntest_",
        "complete": "library(testthat)\nlibrary(ToxicoGx)\ntest_check(\"ToxicoGx\")"
      },
      {
        "partial": "library(testthat)\n# Load ToxicoGx package\n# Run tests for ToxicoGx",
        "complete": "library(testthat)\nlibrary(ToxicoGx)\ntest_check(\"ToxicoGx\")"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/paramErrorChecker.R",
    "language": "R",
    "content": "# An Error Checker for Function Parameters\n#\n# This function will take in the params of a function as well as its name.\n#   Error checking will then be conducted on each of it's parameter arguments\n#   to ensure they meet the input requirements for that function. Descriptive\n#   errors are returned if the the arguements do not meet the criteria for that\n#   function.\n#\n# @param funName [character] A string of the function name. This argument is\n#   used to match the correct parameter checking conditions with each function.\n# @param ... [pairlist] A list of all parameters passed as arguements to the\n#   function \"funName\".\n#\n# @return Returns nothing, this function works by side effects only\n#\n#' @keywords internal\nparamErrorChecker <- function(funName, tSet, ...) {\n\n  # Intersection of all function's parameter tests\n  intersectParamChecks <- c(\n    \"tSetNotIs\", \"tSetGt1\", \"cell_linesNotChar\",\"cell_linesNotIn\",\n    \"drugsNotChar\", \"durationNotChar\"\n  )\n  intersectViabPlotParamChecks <- c(\n    \"tSetGt1\", \"tSetsNotIs\", \"viabilitiesNotMissing\", \"viabilitiesNotNum\"\n  )\n\n  # Matches the correct parameter constraints to each function name\n  paramChecks <-\n    switch(funName,\n           \"drugPerturbationSig\" =\n             c(intersectParamChecks, \"mDataTypeNotChar\", \"mDataTypeNotIn\",\n               \"mDataTypeGt1\", \"featuresLt2\", \"doseLt2\",\n               \"doseNotCtl\"\n             ),\n           \"summarizeMolecularProfiles\" =\n             c(intersectParamChecks, \"mDataTypeGt1\", \"mDataTypeNotChar\",\n               \"mDataTypeNotIn\",\n               \"summary.statNotChar\", \"summary.statNotIn\", \"summary.statGt1\",\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\"\n             ),\n           \"summarizeSensitivityProfiles\" =\n             c(intersectParamChecks,\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\",\n               \"drugNotInCellLine\", \"summary.statNotIn\", \"summaryStatNotChar\",\n               \"summary.statGt1\", \"sensitivty.measureGt1\",\n               \"sensitivity.measureNotChar\"\n              ),\n           \"drugTimeResponseCurve\" =\n             c(intersectViabPlotParamChecks, 'tSetsHaveViab',\n               'viabilitiesDiffLenDur', \"durationNotChar\", \"drugsGt2\",\n               \"cell_linesGt2\"\n               ),\n           \"subsetTo\" =\n             c(#\"returnValuesGt1\",\n               \"tSetGt1\", \"tSetNotIs\", \"cell_linesNotChar\", \"cell_linesNotIn\",\n               \"drugsNotChar\", \"drugsNotIn\", \"featuresNotChar\", \"featuresNotIn\"\n               ),\n    )\n\n  ## Handle missing values\n  if (missing(tSet)) {\n    tSet <- NULL\n  }\n\n  # Conducts the parameter checks based on function matched paramChecks\n  .checkParamsForErrors(funName = funName, tSet = tSet,\n                        paramChecks = paramChecks, ...)\n\n}\n\n#' @keywords internal\n.checkParamsForErrors <- function(tSet, funName, paramChecks, ...) {\n\n  # Initialize variable names in the local environment\n  cell_lines <- concentrations <- dose <- drugs <- duration <- features <-\n    mDataType <- summary.stat <- sensitivity.measure <- tSets <- viabilities <- NULL\n\n  # Extract named arguments into local environment\n  argList <- list(...)\n  for (idx in seq_len(length(argList))) { ## TODO:: Make this work with seq_along()\n    assign(names(argList)[idx], argList[[idx]])\n  }\n\n  if (is.null(mDataType) & !is.null(tSet)) {mDataType <- names(molecularProfilesSlot(tSet))}\n\n  ## TODO:: Write a cases function that lazily evaluates LHS to replace this switch\n  ## TODO:: Benmark for loop vs apply statement for this code\n  # Runs the parameter checks specific for the given funName\n  for (check in paramChecks) {\n    invisible(\n      switch(\n        check,\n        # tSet checks\n        \"tSetNotIs\" = {if (!is(tSet, \"ToxicoSet\")) { stop(paste0(name(tSet), \" is a \", class(tSet), \", not a ToxicoSet.\")) }},\n        \"tSetGt1\" = {if (length(tSet) > 1) { stop(\"You may only pass in one tSet.\") }},\n        \"tSetHasViab\" = {if (length(ToxicoGx::sensitivityInfo(tSet) < 1)) { stop(paste0(names(tSet), ' does not contain sensitivity or perturbation data!')) }},\n        # tSets checks\n        ## TODO:: Generalize error checking to multiple tSets for this check\n        \"tSetsHaveViab\" = {if (!is.null(tSets)) {lapply(tSets, function(tSet) { if (length(ToxicoGx::sensitivityInfo(tSet)) < 1) {stop(\"The \", paste0(names(tSet), \" tSet has no viability data!\"))} })}},\n        \"tSetsNotIs\" = {if (!is.null(tSets)) { if (!all(vapply(tSets, function(tSet) { is(tSet, \"ToxicoSet\") }, FUN.VALUE = logical(1) ))) { stop(\"One or more arguments to tSets parameter is not a 'ToxicoSet'.\")}}},\n        # mDataType checks\n        \"mDataTypeGt1\" = {if (length(unlist(mDataType)) > 1) { stop(\"Please only pass in one molecular data type.\") }},\n        \"mDataTypeNotChar\" = {if (!is.character(mDataType)) { stop(\"mDataType must be a string.\") }},\n        \"mDataTypeNotIn\" = {if (!(mDataType %in% mDataNames(tSet))) { stop(paste0(\"The molecular data type(s) \", paste(mDataType[which(!(mDataType %in% mDataNames(tSet)))], collapse = \", \" ), \" is/are not present in \", name(tSet), \".\")) }},\n        # cell_lines checks\n        \"cell_linesNotChar\" = {if (!is.character(unlist(cell_lines))) { stop(\"cell_lines parameter must contain strings.\") }},\n        \"cell_linesNotIn\" = {if (all(!(cell_lines %in% sampleNames(tSet)))) { stop(paste0(\"The cell line(s) \", paste(cell_lines[which(!(cell_lines %in% sampleNames(tSet)))], collapse = \", \"), \" is/are not present in \", name(tSet), \"with the specified parameters.\")) }},\n        \"cell_linesG\" = {if (length(cell_lines) > 2) {stop(\"This plot currently only supports two cell lines at once!\")}},\n        # drugs checks\n        \"drugsNotChar\" = {if (!is.character(unlist(drugs))) { stop(\"drugs parameter must contain strings.\") }},\n        \"drugsNotIn\" = {if (all(!(drugs %in% treatmentNames(tSet)))) { stop(paste0(\"The drug(s) \", paste(drugs[which(!(drugs %in% treatmentNames(tSet)))], collapse = \", \"), \" is/are not present in \", name(tSet), \".\")) }},\n        ## TODO:: Test this works correctly once an additional cell line is added to a tSet\n        \"drugsIntersectsCellLine\" = {if (length(drugs) == 1) { if (!(drugs %in% subset(sensitivityInfo(tSet), sampleid == cell_lines, select = treatmentid)))  {stop(paste0(\"The drug \", drugs, \"is not present for cell line(s)\", paste0(cell_lines, collapse = \", \")), \"!\") }}},\n        \"drugsGt2\" = {if (length(drugs) > 2) { stop(\"This plot only supports two drugs at a time!\")  }},\n        # features checks\n        \"featuresLt2\" = {if (length(fNames(tSet, mDataType)) < 2) { stop(\"Must include at least 2 features to calculate summary statistics\") }},\n        \"featuresNotChar\" = {if (!is.character(unlist(features))) { stop(\"features parameter contain strings.\") }},\n        \"featuresNotIn\" = {if (all(!(fNames(tSet, mDataType[1]) %in% features))) { stop(paste0(\"The feature(s) \", paste(features[which(!(features %in% fNames(tSet, mDataType[1])))], collapse = \", \"), \" is/are not present in \", name(tSet), \".\")) }},\n        # duration checks\n        \"durationMissing\" = {if (is.null(duration)) { stop(paste(funName, \"requires an argument be passed to the duration parameter!\" )) }},\n        \"durationGt1\" = {if (length(duration) > 1) { stop(paste(funName, \"only accepts one duration at a time!\" )) }},\n        \"durationNotChar\" = {if (!is.character(unlist(duration))) { stop(\"duration parameter must contain strings.\") }},\n        \"durationNotIn\" = {if (all(!(duration %in% ToxicoGx::sensitivityInfo(tSet)$duration_h))) { stop(paste0(\"The duration(s) \", paste(duration[which(!(duration %in% ToxicoGx::sensitivityInfo(tSet)$duration_h))]), collapse = \", \", \"is/are not present in \", name(tSet), \".\")) }},\n        # dose checks\n        \"doseLt2\" = {if (length(dose) < 2) { stop(\"To fit a linear model we need at least two dose levels, please add anothor to the dose argument in the function call.\") }},\n        \"doseNoCtl\" = {if (!(\"Control\" %in% dose)) { stop(\"You should not calculate summary statistics without including a control! Please add 'Control' to the dose argument vector.\") }},\n        \"doseNotChar\" = {if (!is.character(dose)) { stop(\"Dose must be a string or character vector.\") }},\n        \"doseNotIn\" = {if (all(!(dose %in% phenoInfo(tSet, mDataType)$dose_level))) { stop(paste0(\"The dose level(s) \", dose, \" is/are not present in \", name(tSet), \" with the specified parameters.\")) }},\n        # summary.stat\n        \"summary.statNotChar\" = {if (!is.character(summary.stat)) { stop(\"The parameter summary.stat must be a string or character vector.\") }},\n        \"summary.statGt1\" = {if (length(summary.stat) > 1)  {stop(\"Please pick only one summary statistic\") }},\n        \"summary.statNotIn\" = {if (!(summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) { stop(paste0(\"The the statistic \", summary.stat, \" is not implemented in this package\")) }},\n        # sensitivity.measure\n        \"sensitivity.measureNotChar\" = {if (!is.character(sensitivity.measure)) { stop(\"The parameter sensitivty.measure must be a string or character vector.\") }},\n        \"sensitivity.measureGt1\" = {if (length(sensitivity.measure) > 1)  {stop(\"Please pick only one sensitivity measure\") }},\n        \"sensitivity.measureNotIn\" = {if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(tSet)), \"max_conc\"))) {\n          stop(sprintf(\"Invalid sensitivity measure for %s, choose among: %s\", name(tSet), paste0(colnames(sensitivityProfiles(tSet)), collapse = \", \")))}},\n        # viabilties checks\n        \"viabilitiesNotNum\" = {if (!is.null(viabilities)) { if (!all(vapply(viabilities, function(viability) { is(viability, \"numeric\") }, FUN.VALUE = logical(1) ))) { stop(\"Viability values must be numeric.\") }}},\n        \"viabilitiesNotMissing\" = {if (!is.null(concentrations)) { if (is.null(viabilities)) { stop(\"If you pass in an argument for concentrations, you must also pass in an argument for viabilities.\")}}},\n        \"viabilitiesDiffLenConc\" = {\n          if (!is.null(concentrations) && !is.null(viabilities)) {\n            if (length(viabilities) != length(concentrations)) {\n              stop(paste0(ifelse(is(viabilities, \"list\"), \"List\", \"Vector\"), \" of viabilities is \", length(viabilities), \"long, but concentrations is \", length(concentrations), \"long.\")) }}},\n        \"viabilitiesDiffLenDur\" = {\n          if (!is.null(duration) && !is.null(viabilities)) {\n            if (length(viabilities) != length(duration)) {\n              stop(paste0(ifelse(is(viabilities, \"list\"), \"List\", \"Vector\"), \" of viabilities is \", length(viabilities), \"long, but concentrations is \", length(duration), \"long.\")) }}},\n        # concentrations checks\n        \"concentrationsNotNum\" = {\n          if (!is.null(concentrations)) {\n            if (!all(vapply(concentrations, function(concentration) {\n              is(concentration, \"numeric\") }, FUN.VALUE = logical(1) ))) {\n              stop(\"Concentration values must be numeric.\") }\n          }},\n      )\n    )\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `paramErrorChecker` function and how does it handle different function names?",
        "answer": "The `paramErrorChecker` function is designed to perform error checking on function parameters. It takes the function name and its parameters as input, then uses a switch statement to match the correct parameter constraints to each function name. It creates a list of parameter checks specific to the given function and passes them to the `.checkParamsForErrors` function for actual error checking."
      },
      {
        "question": "How does the `.checkParamsForErrors` function handle the various parameter checks?",
        "answer": "The `.checkParamsForErrors` function uses a for loop to iterate through the list of parameter checks. For each check, it uses a switch statement to execute the corresponding error checking code. If an error condition is met, it stops execution and throws an error with a descriptive message. The function handles checks for various parameter types, including tSet, cell_lines, drugs, features, duration, dose, and others."
      },
      {
        "question": "What is the purpose of the `intersectParamChecks` and `intersectViabPlotParamChecks` variables in the `paramErrorChecker` function?",
        "answer": "The `intersectParamChecks` and `intersectViabPlotParamChecks` variables contain lists of common parameter checks that are used across multiple functions. These lists are combined with function-specific checks in the switch statement to create a comprehensive set of parameter checks for each function. This approach reduces code duplication and ensures consistent error checking across different functions that share common parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "paramErrorChecker <- function(funName, tSet, ...) {\n  intersectParamChecks <- c(\n    \"tSetNotIs\", \"tSetGt1\", \"cell_linesNotChar\",\"cell_linesNotIn\",\n    \"drugsNotChar\", \"durationNotChar\"\n  )\n  intersectViabPlotParamChecks <- c(\n    \"tSetGt1\", \"tSetsNotIs\", \"viabilitiesNotMissing\", \"viabilitiesNotNum\"\n  )\n\n  paramChecks <-\n    switch(funName,\n           \"drugPerturbationSig\" =\n             c(intersectParamChecks, \"mDataTypeNotChar\", \"mDataTypeNotIn\",\n               \"mDataTypeGt1\", \"featuresLt2\", \"doseLt2\",\n               \"doseNotCtl\"\n             ),\n           \"summarizeMolecularProfiles\" =\n             c(intersectParamChecks, \"mDataTypeGt1\", \"mDataTypeNotChar\",\n               \"mDataTypeNotIn\",\n               \"summary.statNotChar\", \"summary.statNotIn\", \"summary.statGt1\",\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\"\n             ),\n           # ... (other cases)\n    )\n\n  if (missing(tSet)) {\n    tSet <- NULL\n  }\n\n  # TODO: Implement .checkParamsForErrors function\n}",
        "complete": "paramErrorChecker <- function(funName, tSet, ...) {\n  intersectParamChecks <- c(\n    \"tSetNotIs\", \"tSetGt1\", \"cell_linesNotChar\",\"cell_linesNotIn\",\n    \"drugsNotChar\", \"durationNotChar\"\n  )\n  intersectViabPlotParamChecks <- c(\n    \"tSetGt1\", \"tSetsNotIs\", \"viabilitiesNotMissing\", \"viabilitiesNotNum\"\n  )\n\n  paramChecks <-\n    switch(funName,\n           \"drugPerturbationSig\" =\n             c(intersectParamChecks, \"mDataTypeNotChar\", \"mDataTypeNotIn\",\n               \"mDataTypeGt1\", \"featuresLt2\", \"doseLt2\",\n               \"doseNotCtl\"\n             ),\n           \"summarizeMolecularProfiles\" =\n             c(intersectParamChecks, \"mDataTypeGt1\", \"mDataTypeNotChar\",\n               \"mDataTypeNotIn\",\n               \"summary.statNotChar\", \"summary.statNotIn\", \"summary.statGt1\",\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\"\n             ),\n           \"summarizeSensitivityProfiles\" =\n             c(intersectParamChecks,\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\",\n               \"drugNotInCellLine\", \"summary.statNotIn\", \"summaryStatNotChar\",\n               \"summary.statGt1\", \"sensitivty.measureGt1\",\n               \"sensitivity.measureNotChar\"\n              ),\n           \"drugTimeResponseCurve\" =\n             c(intersectViabPlotParamChecks, 'tSetsHaveViab',\n               'viabilitiesDiffLenDur', \"durationNotChar\", \"drugsGt2\",\n               \"cell_linesGt2\"\n               ),\n           \"subsetTo\" =\n             c(\"tSetGt1\", \"tSetNotIs\", \"cell_linesNotChar\", \"cell_linesNotIn\",\n               \"drugsNotChar\", \"drugsNotIn\", \"featuresNotChar\", \"featuresNotIn\"\n               )\n    )\n\n  if (missing(tSet)) {\n    tSet <- NULL\n  }\n\n  .checkParamsForErrors(funName = funName, tSet = tSet,\n                        paramChecks = paramChecks, ...)\n}"
      },
      {
        "partial": ".checkParamsForErrors <- function(tSet, funName, paramChecks, ...) {\n  # Initialize variables\n  cell_lines <- concentrations <- dose <- drugs <- duration <- features <-\n    mDataType <- summary.stat <- sensitivity.measure <- tSets <- viabilities <- NULL\n\n  # Extract named arguments\n  argList <- list(...)\n  for (idx in seq_len(length(argList))) {\n    assign(names(argList)[idx], argList[[idx]])\n  }\n\n  if (is.null(mDataType) & !is.null(tSet)) {\n    mDataType <- names(molecularProfilesSlot(tSet))\n  }\n\n  # TODO: Implement parameter checks\n}",
        "complete": ".checkParamsForErrors <- function(tSet, funName, paramChecks, ...) {\n  # Initialize variables\n  cell_lines <- concentrations <- dose <- drugs <- duration <- features <-\n    mDataType <- summary.stat <- sensitivity.measure <- tSets <- viabilities <- NULL\n\n  # Extract named arguments\n  argList <- list(...)\n  for (idx in seq_len(length(argList))) {\n    assign(names(argList)[idx], argList[[idx]])\n  }\n\n  if (is.null(mDataType) & !is.null(tSet)) {\n    mDataType <- names(molecularProfilesSlot(tSet))\n  }\n\n  for (check in paramChecks) {\n    switch(\n      check,\n      \"tSetNotIs\" = if (!is(tSet, \"ToxicoSet\")) stop(paste0(name(tSet), \" is a \", class(tSet), \", not a ToxicoSet.\")),\n      \"tSetGt1\" = if (length(tSet) > 1) stop(\"You may only pass in one tSet.\"),\n      \"cell_linesNotChar\" = if (!is.character(unlist(cell_lines))) stop(\"cell_lines parameter must contain strings.\"),\n      \"cell_linesNotIn\" = if (all(!(cell_lines %in% sampleNames(tSet)))) stop(paste0(\"The cell line(s) \", paste(cell_lines[which(!(cell_lines %in% sampleNames(tSet)))], collapse = \", \"), \" is/are not present in \", name(tSet), \"with the specified parameters.\")),\n      \"drugsNotChar\" = if (!is.character(unlist(drugs))) stop(\"drugs parameter must contain strings.\"),\n      \"durationNotChar\" = if (!is.character(unlist(duration))) stop(\"duration parameter must contain strings.\"),\n      \"mDataTypeNotChar\" = if (!is.character(mDataType)) stop(\"mDataType must be a string.\"),\n      \"mDataTypeNotIn\" = if (!(mDataType %in% mDataNames(tSet))) stop(paste0(\"The molecular data type(s) \", paste(mDataType[which(!(mDataType %in% mDataNames(tSet)))], collapse = \", \" ), \" is/are not present in \", name(tSet), \".\")),\n      \"mDataTypeGt1\" = if (length(unlist(mDataType)) > 1) stop(\"Please only pass in one molecular data type.\"),\n      \"featuresLt2\" = if (length(fNames(tSet, mDataType)) < 2) stop(\"Must include at least 2 features to calculate summary statistics\"),\n      \"doseLt2\" = if (length(dose) < 2) stop(\"To fit a linear model we need at least two dose levels, please add another to the dose argument in the function call.\"),\n      \"doseNotCtl\" = if (!(\"Control\" %in% dose)) stop(\"You should not calculate summary statistics without including a control! Please add 'Control' to the dose argument vector.\")\n    )\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/src/foo_mrmr_surv.cpp",
    "language": "cpp",
    "content": "#include \"mrmr_surv.h\"\n\ndouble get_correlation(double data [],int namat[], int ind_x, int ind_y, int size){\n\t//compute correlation of two variables;\n\t//data: contains all data in a vector; variable-wise appended\n\t//ind_x: starting index of first variable in data\n\t//ind_y: starting index of second variable in data\n\t//size: number of samples for both variables\n\tdouble mean_data_x=0.0,mean_data_y=0.0;\n\tdouble correlation_nom=0.0,correlation_den_x=0.0,correlation_den_y=0.0;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif (namat[ind_x+i]==0 && namat[ind_y+i]==0 ) {\n\t\t\tmean_data_x+=data[ind_x+i];\n\t\t\tmean_data_y+=data[ind_y+i];\n\t\t}\n\t}\n\n\tmean_data_x=mean_data_x/size;\n\tmean_data_y=mean_data_y/size;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif(namat[ind_x+i]==0 && namat[ind_y+i]==0){\n\t\tcorrelation_nom+=(data[ind_x+i]-mean_data_x)*(data[ind_y+i]-mean_data_y);\n\t\tcorrelation_den_x+=(data[ind_x+i]-mean_data_x)*(data[ind_x+i]-mean_data_x);\n\t\tcorrelation_den_y+=(data[ind_y+i]-mean_data_y)*(data[ind_y+i]-mean_data_y);\n\t\t}\n\t}\n\treturn correlation_nom/(sqrt(correlation_den_x*correlation_den_y));\n}\n\n\nvoid build_mim_subset(double mim[],double data[], int namat [],int nvar,int nsample, int subset [],int size_subset){\n\t//compute mutual information matrix\n\t//mim:\t\t\tmatrix (stored as vector) in which the mi values will be stored\n\t//data:\t\t\tcontains all data in a vector; variable-wise appended\n\t//nvar:\t\t\tnumber of variables\n\t//nsample:\t\tnumber of samples in dataset\n\t//subset:\t\tindices of samples to be included in the bootstrapping data\n\t//size_subset:\tnumber of variables in the bootstrapped dataset\n\n\tdouble tmp;\n\tdouble *data_x;\n\tint *namat_x;\n\n\tnamat_x = (int*) R_alloc(nvar*size_subset, sizeof(int));\n\tdata_x = (double *) R_alloc(nvar*size_subset, sizeof(double));\n\n\tfor(unsigned int i=0; i< size_subset; ++i){\n\t\tfor(unsigned int j=0; j< nvar; ++j){\n\t\t\tdata_x[size_subset*j+i]=data[(subset[i])+nsample*j];\n\t\t\tnamat_x[size_subset*j+i]=namat[(subset[i])+nsample*j];\n\t\t}\n\t}\n\n\tfor(unsigned int i=0; i< nvar; ++i){\n\t\tmim[i*nvar+i]=0;\n\t\tfor(unsigned int j=i+1; j< nvar; ++j){\n\t\t\ttmp=get_correlation(data_x,namat_x,i*size_subset,j*size_subset,size_subset);\n\t\t\ttmp=tmp*tmp;\n\t\t\tif(tmp>0.999999){\n\t\t\t\ttmp=0.999999;\n\t\t\t}\n\t\t\tmim[j*nvar+i]= -0.5* log (1-tmp);\n\t\t\tmim[i*nvar+j]=mim[j*nvar+i];\n\t\t}\n\t}\n\t//delete [] namat_x;\n\t//delete [] data_x;\n\n}\n\ndouble returnConcordanceIndexC(int *msurv, int *ustrat, double *x2, int *cl2,\n\t\t\t\t\t   double *st, int *se, double *weights, int *strat, int *N, int *outx, int *lenS, int *lenU)\n{\n\n\tint lenUstrat = *lenU;\n\tint lenStrat = *lenS;\n\n\tdouble res_ch[lenStrat];\n\tdouble res_dh[lenStrat];\n\n\tdouble res_cIndex=0;\n\n\tint Ns_old = 0;\n\tint Ns = 0;\n\tfor(int s=0; s < lenUstrat; s++) {\n\t\tint ixs[lenStrat];\n\t\tfor(int i =0; i < lenStrat; i++){\n\t\t\tixs[i] = 0;\n\t\t\tif(strat[i] == ustrat[s]){\n\t\t\t\tixs[i] = 1;\n\t\t\t} else {\n\t\t\t\tixs[i] = 0;\n\t\t\t}\n\t\t}\n\t\tNs_old += Ns;\n\t\tNs = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tNs++;\n\t\t\t}\n\t\t}\n\t\tdouble xs[Ns];\n\t\tint c = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\txs[c] = x2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint cls[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tcls[c] = cl2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble sts[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tsts[c] = st[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint ses[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tses[c] = se[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble weightss[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tweightss[c] = weights[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble chs[Ns];\n\t\tdouble dhs[Ns];\n\t\tdouble uhs[Ns];\n\t\tdouble rphs[Ns];\n\t\tfor (int h=0; h < Ns; h++) {\n\t\t\tdouble chsj, dhsj, uhsj, rphsj = 0;\n\t\t\tfor (int j=0; j < Ns; j++) {\n\t\t\t\tdouble whj = weightss[h] * weightss[j];\n\t\t\t\tif((*msurv == 1 && (sts[h] < sts[j] && ses[h] == 1)) || (*msurv == 0 && cls[h] > cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] > xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t} else if (xs[h] < xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif((*msurv == 1 && (sts[h] > sts[j] && ses[j] == 1)) || (*msurv == 0 && cls[h] < cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] < xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse if (xs[h] > xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tchs[h] = chsj;\n\t\t\tdhs[h] = dhsj;\n\t\t\tuhs[h] = uhsj;\n\t\t\trphs[h] = rphsj;\n\t\t\tchsj = 0;\n\t\t\tdhsj = 0;\n\t\t\tuhsj = 0;\n\t\t\trphsj = 0;\n\t\t}\n\t\tfor(int i = 0; i < Ns; i++){\n\t\t\tint pos = i + Ns_old;\n\t\t\tres_ch [pos] =chs[i];\n\t\t\tres_dh [pos] =dhs[i];\n\t\t}\n\n\t}\n\n\tdouble tmp_ch=0, tmp_dh=0;\n\tfor(int s=0; s < lenStrat; s++) {\n\t\ttmp_ch+=res_ch[s];\n\t\ttmp_dh+=res_dh[s];\n\t}\n\n\tdouble n=*N;\n\ttmp_ch=(1/(n *(n - 1))) * tmp_ch;\n\ttmp_dh=(1/(n *(n - 1))) * tmp_dh;\n\tres_cIndex=tmp_ch/ (tmp_ch+tmp_dh);\n\n\t/// scale value to be in the intervall [-1,1] as correlation and square to be on same scale as mutual information [0,1]\n\n\tres_cIndex=2*res_cIndex-1;\n\tres_cIndex=res_cIndex*res_cIndex;\n\t//cout<<\"res cIndex \"<<res_cIndex<<endl;\n\treturn res_cIndex;\n}\n\n\nSEXP get_concordanceIndex_onevariable(SEXP Rmsurv, SEXP Rustrat, SEXP Rx2,SEXP Rcl2, SEXP Rst, SEXP Rse, SEXP Rweights, SEXP Rstrat, SEXP RN, SEXP Routx, SEXP RlenS, SEXP RlenU){\n\tdouble *res_cI , res_cIndex;\n\n\tdouble *x2, *st, *weights;\n\tint *msurv, *ustrat, *cl2, *se, *strat, *N, *outx, *lenS, *lenU;\n\n\tSEXP Rres;\n\n\tmsurv=INTEGER_POINTER(Rmsurv);\n\tustrat=INTEGER_POINTER(Rustrat);\n\tx2=NUMERIC_POINTER(Rx2);\n\tcl2=INTEGER_POINTER(Rcl2);\n\tst=NUMERIC_POINTER(Rst);\n\tse =INTEGER_POINTER(Rse);\n\tweights=NUMERIC_POINTER(Rweights);\n\tstrat=INTEGER_POINTER(Rstrat);\n\tN =INTEGER_POINTER(RN);\n\toutx=INTEGER_POINTER(Routx);\n\n\tlenS=INTEGER_POINTER(RlenS);\n\tlenU=INTEGER_POINTER(RlenU);\n\n\n\tPROTECT(Rres = NEW_NUMERIC(1));\n\tres_cI=NUMERIC_POINTER(Rres);\n\n\n\tres_cIndex=returnConcordanceIndexC(msurv, ustrat, x2, cl2,st, se, weights,strat, N, outx, lenS, lenU) ;\n\n\tres_cI[0]=res_cIndex;\n\n\tUNPROTECT(1);\n\n\treturn Rres;\n}\n\nextern \"C\" SEXP\nmrmr_cIndex(SEXP Rdata, SEXP Rnamat, SEXP RcIndex, SEXP Rnvar, SEXP Rnsample, SEXP Rthreshold){\n\tdouble *data, *cIndex;\n\tdouble *rel, *red, *res, *mim, score=1,*threshold, *res_final;\n\n\tconst int *nvar, *nsample;\n\tint *ind, *namat;\n\n\tunsigned int n, jmax=0;\n\tPROTECT(Rdata = AS_NUMERIC(Rdata));\n\tPROTECT(Rnamat = AS_INTEGER(Rnamat));\n\tPROTECT(RcIndex = AS_NUMERIC(RcIndex));\n\tPROTECT(Rnvar= AS_INTEGER(Rnvar));\n\tPROTECT(Rnsample= AS_INTEGER(Rnsample));\n\tPROTECT(Rthreshold= AS_NUMERIC(Rthreshold));\n\n\n\tdata = NUMERIC_POINTER(Rdata);\n\tnamat=INTEGER_POINTER(Rnamat);\n\tcIndex= NUMERIC_POINTER(RcIndex);\n\tnvar = INTEGER_POINTER(Rnvar);\n\tnsample = INTEGER_POINTER(Rnsample);\n\tthreshold = NUMERIC_POINTER(Rthreshold);\n\n\tn = *nvar;\n\n\t//new variables\n\tSEXP Rmim, Rres,Rred,Rrel,Rind,Rres_final;\n\n\tPROTECT(Rmim = NEW_NUMERIC(n*n));\n\tPROTECT(Rres = NEW_NUMERIC(n));\n\tPROTECT(Rres_final = NEW_NUMERIC(n));\n\tPROTECT(Rrel = NEW_NUMERIC(n));\n\tPROTECT(Rred = NEW_NUMERIC(n));\n\tPROTECT(Rind = NEW_INTEGER(*nsample));\n\n\tind = INTEGER_POINTER(Rind);\n\tmim = NUMERIC_POINTER(Rmim);\n\tres = NUMERIC_POINTER(Rres);\n\trel = NUMERIC_POINTER(Rrel);\n\tred = NUMERIC_POINTER(Rred);\n\tres_final = NUMERIC_POINTER(Rres_final);\n\n\n\tfor(unsigned int i=0;i < *nsample; ++i){\n\t\tind[i]=i;\n\t}\n\n\tbuild_mim_subset(mim, data, namat, n, *nsample, ind, *nsample);\n\n\n\tfor( unsigned int i=0; i< n; ++i ){\n\t\t\tres[i]=*threshold;\n\t\t\tres_final[i]=*threshold;\n\t}\n\n\t\t//init rel and red and select first\n\t\tfor( unsigned int j=0; j<n; ++j ) {\n\t\t\trel[j]=cIndex[j];\n\t\t\tred[j]=0;\n\t\t\tif( rel[j] > rel[jmax])\n\t\t\t\tjmax = j;\n\t\t}\n\n\t\tscore = rel[jmax];\n\t\tif( res[jmax] < score ) {\n\t\t\tres[jmax] = score;\n\t\t}\n\n\t\t//select others\n\t\tfor(unsigned int k=1; k < n+1; k++ ) {\n\t\t\tjmax = 0;\n\n\t\t\tfor(unsigned int j=1; j < n; ++j ) {\n\t\t\t\tif( (rel[j] - red[j]/k) > (rel[jmax] - red[jmax]/k) )\n\t\t\t\t\tjmax = j;\n\t\t\t}\n\t\t\tscore = (rel[jmax] - (red[jmax]/k));\n\t\t\tif( res[jmax] < score ) {\n\t\t\t\tres[jmax] = score;\n\t\t\t}\n\n\t\t\t//update rel and red\n\t\t\trel[jmax]=-1000;\n\t\t\tfor( int l=0; l<n; ++l )\n\t\t\t\tred[l] += mim[l*n+jmax];\n\n\t\t\t// stop criterion\n\t\t\tif( score < *threshold ) k=n;\n\n\t\t}\n\n\n\tfor(unsigned int i=0; i< n; ++i ) {\n\t\tres_final[i]=res[i];\n\t}\n\tUNPROTECT(12);\n\n\treturn Rres_final;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `get_correlation` function and how does it handle missing data?",
        "answer": "The `get_correlation` function calculates the correlation between two variables in a dataset. It handles missing data by using the `namat` array to check if both variables have valid values (namat[ind_x+i]==0 && namat[ind_y+i]==0) before including them in the correlation calculation. This ensures that only complete pairs of observations are used in the computation."
      },
      {
        "question": "How does the `build_mim_subset` function create a bootstrapped dataset, and what is the purpose of this operation?",
        "answer": "The `build_mim_subset` function creates a bootstrapped dataset by selecting a subset of samples from the original data. It uses the `subset` array to specify which samples to include, and creates new `data_x` and `namat_x` arrays containing only the selected samples. This operation is likely used for resampling techniques, such as bootstrap aggregating (bagging), to assess the stability of feature selection or to estimate the variability of the mutual information matrix."
      },
      {
        "question": "In the `returnConcordanceIndexC` function, what is the significance of the `outx` parameter, and how does it affect the calculation of the concordance index?",
        "answer": "The `outx` parameter in the `returnConcordanceIndexC` function determines how ties in the predictor variable (x) are handled. When `*outx == 1`, ties are considered as a separate category and contribute to the `uhsj` (undefined) count. When `*outx != 1`, ties are treated as discordant pairs and added to the `dhsj` (discordant) count. This affects the final calculation of the concordance index by influencing how tied predictor values are weighted in the overall measure of predictive discrimination."
      }
    ],
    "completion_tasks": [
      {
        "partial": "double get_correlation(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_data_x = 0.0, mean_data_y = 0.0;\n    double correlation_nom = 0.0, correlation_den_x = 0.0, correlation_den_y = 0.0;\n    int valid_samples = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x + i] == 0 && namat[ind_y + i] == 0) {\n            mean_data_x += data[ind_x + i];\n            mean_data_y += data[ind_y + i];\n            valid_samples++;\n        }\n    }\n\n    if (valid_samples == 0) return 0.0;\n\n    mean_data_x /= valid_samples;\n    mean_data_y /= valid_samples;\n\n    // Complete the function to calculate and return the correlation\n}",
        "complete": "double get_correlation(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_data_x = 0.0, mean_data_y = 0.0;\n    double correlation_nom = 0.0, correlation_den_x = 0.0, correlation_den_y = 0.0;\n    int valid_samples = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x + i] == 0 && namat[ind_y + i] == 0) {\n            mean_data_x += data[ind_x + i];\n            mean_data_y += data[ind_y + i];\n            valid_samples++;\n        }\n    }\n\n    if (valid_samples == 0) return 0.0;\n\n    mean_data_x /= valid_samples;\n    mean_data_y /= valid_samples;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x + i] == 0 && namat[ind_y + i] == 0) {\n            double dx = data[ind_x + i] - mean_data_x;\n            double dy = data[ind_y + i] - mean_data_y;\n            correlation_nom += dx * dy;\n            correlation_den_x += dx * dx;\n            correlation_den_y += dy * dy;\n        }\n    }\n\n    double denominator = sqrt(correlation_den_x * correlation_den_y);\n    return denominator == 0 ? 0 : correlation_nom / denominator;\n}"
      },
      {
        "partial": "void build_mim_subset(double mim[], double data[], int namat[], int nvar, int nsample, int subset[], int size_subset) {\n    double *data_x = (double *)R_alloc(nvar * size_subset, sizeof(double));\n    int *namat_x = (int *)R_alloc(nvar * size_subset, sizeof(int));\n\n    for (int i = 0; i < size_subset; ++i) {\n        for (int j = 0; j < nvar; ++j) {\n            data_x[size_subset * j + i] = data[subset[i] + nsample * j];\n            namat_x[size_subset * j + i] = namat[subset[i] + nsample * j];\n        }\n    }\n\n    // Complete the function to build the mutual information matrix\n}",
        "complete": "void build_mim_subset(double mim[], double data[], int namat[], int nvar, int nsample, int subset[], int size_subset) {\n    double *data_x = (double *)R_alloc(nvar * size_subset, sizeof(double));\n    int *namat_x = (int *)R_alloc(nvar * size_subset, sizeof(int));\n\n    for (int i = 0; i < size_subset; ++i) {\n        for (int j = 0; j < nvar; ++j) {\n            data_x[size_subset * j + i] = data[subset[i] + nsample * j];\n            namat_x[size_subset * j + i] = namat[subset[i] + nsample * j];\n        }\n    }\n\n    for (int i = 0; i < nvar; ++i) {\n        mim[i * nvar + i] = 0;\n        for (int j = i + 1; j < nvar; ++j) {\n            double tmp = get_correlation(data_x, namat_x, i * size_subset, j * size_subset, size_subset);\n            tmp = tmp * tmp;\n            if (tmp > 0.999999) {\n                tmp = 0.999999;\n            }\n            mim[j * nvar + i] = mim[i * nvar + j] = -0.5 * log(1 - tmp);\n        }\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/getsurv2.R",
    "language": "R",
    "content": "`getsurv2` <-\nfunction(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n\twhich.est <- match.arg(which.est)\n\tswitch(which.est,\n\t\"lower\"={\n\t\tsurvres <- as.matrix(sf$lower)\t\n\t},\n\t\"point\"={\n\t\tsurvres <- as.matrix(sf$surv)\n\t},\n\t\"upper\"={\n\t\tsurvres <- as.matrix(sf$upper)\n\t})\n\tif(time >= max(sf$time)) {\n\t\ttime.ix <- length(sf$time)\n\t} else { time.ix <- order(sf$time <= time)[1] - 1 }\n\tif(time.ix == 0) {\n\t\tres <- rep(1, ncol(sf$surv[time.ix, ]))\n\t\tnames(res) <- dimnames(sf$surv)[[2]]\n\t} else { res <- survres[time.ix, ] }\t\n\treturn(res)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getsurv2` function and what are its main parameters?",
        "answer": "The `getsurv2` function is designed to extract survival estimates from a survival object (sf) at a specified time point. Its main parameters are:\n1. sf: A survival object containing survival estimates.\n2. time: The time point at which to extract the survival estimate.\n3. which.est: Specifies which estimate to return ('point', 'lower', or 'upper')."
      },
      {
        "question": "How does the function handle cases where the requested time is greater than or equal to the maximum time in the survival object?",
        "answer": "When the requested time is greater than or equal to the maximum time in the survival object, the function sets `time.ix` to the length of `sf$time`. This effectively returns the survival estimate for the last available time point in the data."
      },
      {
        "question": "What does the function return when the calculated `time.ix` is 0, and why is this case handled separately?",
        "answer": "When `time.ix` is 0, the function returns a vector of 1s with length equal to the number of columns in `sf$surv`. This case is handled separately because it represents a time point before the first event in the survival data, where the survival probability is assumed to be 1 for all subjects. The names of the returned vector are set to match the column names of `sf$surv`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  survres <- switch(which.est,\n    \"lower\" = as.matrix(sf$lower),\n    \"point\" = as.matrix(sf$surv),\n    \"upper\" = as.matrix(sf$upper))\n  \n  # Complete the function to handle time comparison and return the result\n}",
        "complete": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  survres <- switch(which.est,\n    \"lower\" = as.matrix(sf$lower),\n    \"point\" = as.matrix(sf$surv),\n    \"upper\" = as.matrix(sf$upper))\n  \n  time.ix <- if(time >= max(sf$time)) length(sf$time) else order(sf$time <= time)[1] - 1\n  \n  if(time.ix == 0) {\n    res <- rep(1, ncol(sf$surv))\n    names(res) <- dimnames(sf$surv)[[2]]\n  } else {\n    res <- survres[time.ix, ]\n  }\n  \n  return(res)\n}"
      },
      {
        "partial": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  # Implement the function to select the appropriate survival estimate\n  # and handle time comparison\n}",
        "complete": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  survres <- switch(which.est,\n    lower = as.matrix(sf$lower),\n    point = as.matrix(sf$surv),\n    upper = as.matrix(sf$upper))\n  \n  time.ix <- if(time >= max(sf$time)) length(sf$time) else which.max(sf$time > time) - 1\n  \n  res <- if(time.ix == 0) {\n    setNames(rep(1, ncol(sf$surv)), dimnames(sf$surv)[[2]])\n  } else {\n    survres[time.ix, ]\n  }\n  \n  res\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/tamr13.R",
    "language": "R",
    "content": "#' @title Function to compute the risk scores of the tamoxifen resistance\n#' signature (TAMR13)\n#'\n#' @description\n#' This function computes signature scores from gene expression values\n#'   following the algorithm used for the Tamoxifen Resistance signature (TAMR13).\n#'\n#' @usage\n#' tamr13(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes\n#'   in columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to\n#' force the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low\n#'   risk (not implemented, the function will return NA values).\n#'\n#' @references\n#' Loi S, Haibe-Kains B, Desmedt C, Wirapati P, Lallemand F, Tutt AM, Gillet C,\n#'   Ellis P, Ryder K, Reid JF, Daidone MG, Pierotti MA, Berns EMJJ, Jansen MPHM,\n#'   Foekens JA, Delorenzi M, Bontempi G, Piccart MJ and Sotiriou C (2008)\n#'   \"Predicting prognosis using molecular profiling in estrogen receptor-\n#'   positive breast cancer treated with tamoxifen\", BMC Genomics, 9(1):239\n#'\n#' @seealso\n#' [genefu::gene76]\n#'\n#' @examples\n#' # load TAMR13 signature\n#' data(sig.tamr13)\n#' # load VDX dataset\n#' data(vdxs)\n#' # compute relapse score\n#' tamr13.vdxs <- tamr13(data=data.vdxs, annot=annot.vdxs, do.mapping=FALSE)\n#' summary(tamr13.vdxs$score)\n#'\n#' @md\n#' @export\n#' @name tamr13\ntamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\trest <- resc <- NULL\n\tfor(i in 1:length(sig.tamr13)) {\n\t\trest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n\t\tresc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n\t}\n\tdimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n\tnames(resc) <- names(sig.tamr13)\n\tres <- drop(rest %*% resc)\n\triskt <- rep(NA, length(res))\n\tnames(riskt) <- names(res)\n\tres <- list(\"score\"=res, \"risk\"=riskt)\n\n\treturn (res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `tamr13` function and what does it return?",
        "answer": "The `tamr13` function computes risk scores for the tamoxifen resistance signature (TAMR13) from gene expression data. It returns a list containing two items: 'score' (continuous signature scores) and 'risk' (binary risk classification, currently not implemented and returns NA values)."
      },
      {
        "question": "How does the function handle gene mapping, and what parameter controls this behavior?",
        "answer": "The function can perform gene mapping through Entrez Gene IDs if needed. This behavior is controlled by the `do.mapping` parameter. When `do.mapping` is TRUE, the function maps probes to genes, keeping the most variant probe for each gene in case of ambiguities. The `mapping` parameter can be used to force a specific mapping instead of selecting probes based on variance."
      },
      {
        "question": "Explain the main computation performed in the `tamr13` function and how it uses the `sig.tamr13` object.",
        "answer": "The main computation in `tamr13` involves iterating through the `sig.tamr13` object, which likely contains multiple gene signatures. For each signature, it calls the `sig.score` function to compute individual scores, storing them in `rest`. It also extracts coefficients from `sig.tamr13` attributes into `resc`. Finally, it computes the overall score by matrix multiplication of `rest` and `resc`, resulting in a weighted sum of individual signature scores."
      }
    ],
    "completion_tasks": [
      {
        "partial": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  for(i in 1:length(sig.tamr13)) {\n    rest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n    resc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n  }\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  # Complete the function by adding code to create the 'riskt' vector and return the result\n}",
        "complete": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  for(i in 1:length(sig.tamr13)) {\n    rest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n    resc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n  }\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  riskt <- rep(NA, length(res))\n  names(riskt) <- names(res)\n  list(score=res, risk=riskt)\n}"
      },
      {
        "partial": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  # Add code to calculate rest and resc\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  riskt <- rep(NA, length(res))\n  names(riskt) <- names(res)\n  list(score=res, risk=riskt)\n}",
        "complete": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  for(i in 1:length(sig.tamr13)) {\n    rest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n    resc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n  }\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  riskt <- rep(NA, length(res))\n  names(riskt) <- names(res)\n  list(score=res, risk=riskt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/fisherz.R",
    "language": "R",
    "content": "'fisherz' <-\nfunction(x, inv=FALSE, eps=1e-16) {\n\t\n\tmyfoo <- function(x, inv, eps) {\n\t\tif(is.na(x)) { return(NA) }\n\t\tif(!inv) {\n\t\t\tif((1 - abs(x)) < eps) { x <- ifelse(x < 0, -Inf, Inf) }\n\t\t\telse { x <- (log(1 + x) - log(1 - x)) / 2 }\n\t\t}\n\t\telse {\n\t\t\tif(is.infinite(x) || x > (1 / eps)) { x <- ifelse(x < 0, -1, 1) }\n\t\t\telse { x <- (exp(2 * x) - 1) / (exp(2 * x) + 1) }\n\t\t}\n\t\treturn(x)\n\t}\n\t\n\tif(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n\tif(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n\treturn(myfoo(x=x, inv=inv, eps=eps))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'fisherz' function and what transformation does it perform?",
        "answer": "The 'fisherz' function performs Fisher's z-transformation on input values. When 'inv=FALSE' (default), it applies the forward transformation: z = 0.5 * ln((1+r)/(1-r)), where r is the input correlation coefficient. When 'inv=TRUE', it applies the inverse transformation: r = (exp(2z) - 1) / (exp(2z) + 1). This transformation is useful in statistical analysis, particularly for normalizing correlation coefficients."
      },
      {
        "question": "How does the function handle different input types (scalar, vector, matrix, data frame)?",
        "answer": "The function handles different input types as follows:\n1. For matrices or data frames, it applies the transformation to each element using 'apply' with MARGIN=c(1, 2).\n2. For vectors, it uses 'sapply' to apply the transformation to each element.\n3. For scalar inputs, it directly calls the inner 'myfoo' function.\nThis design allows the function to work flexibly with various input types, applying the transformation element-wise as needed."
      },
      {
        "question": "What is the purpose of the 'eps' parameter in the 'fisherz' function, and how is it used?",
        "answer": "The 'eps' parameter (default value 1e-16) is used to handle numerical precision issues and edge cases:\n1. In the forward transformation, if |x| is very close to 1 (within 'eps'), it returns Inf or -Inf to avoid division by zero.\n2. In the inverse transformation, if |x| is very large (> 1/eps), it returns 1 or -1 to avoid overflow.\nThis ensures numerical stability and handles extreme values appropriately in the transformation process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    if(!inv) {\n      # Complete the transformation for non-inverse case\n    }\n    else {\n      # Complete the transformation for inverse case\n    }\n    return(x)\n  }\n  \n  # Complete the function to handle different input types\n}",
        "complete": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    if(!inv) {\n      if((1 - abs(x)) < eps) { x <- ifelse(x < 0, -Inf, Inf) }\n      else { x <- (log(1 + x) - log(1 - x)) / 2 }\n    }\n    else {\n      if(is.infinite(x) || x > (1 / eps)) { x <- ifelse(x < 0, -1, 1) }\n      else { x <- (exp(2 * x) - 1) / (exp(2 * x) + 1) }\n    }\n    return(x)\n  }\n  \n  if(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n  if(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n  return(myfoo(x=x, inv=inv, eps=eps))\n}"
      },
      {
        "partial": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    # Complete the transformation logic here\n  }\n  \n  if(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n  if(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n  return(myfoo(x=x, inv=inv, eps=eps))\n}",
        "complete": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    if(!inv) {\n      if((1 - abs(x)) < eps) { x <- ifelse(x < 0, -Inf, Inf) }\n      else { x <- (log(1 + x) - log(1 - x)) / 2 }\n    }\n    else {\n      if(is.infinite(x) || x > (1 / eps)) { x <- ifelse(x < 0, -1, 1) }\n      else { x <- (exp(2 * x) - 1) / (exp(2 * x) + 1) }\n    }\n    return(x)\n  }\n  \n  if(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n  if(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n  return(myfoo(x=x, inv=inv, eps=eps))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/tdrocc.R",
    "language": "R",
    "content": "`tdrocc` <-\nfunction(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n\t#require(survivalROC)\t\n\tdata <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n\tcc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n   if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n   if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n   x2 <- x[cc.ix]\n   surv.time2 <- surv.time[cc.ix]\n   surv.event2 <- surv.event[cc.ix]\n    \n   if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }\n   if(is.na(cutpts)) {\n       ux <- unique(sort(x2))\n       delta <- min(diff(ux))/2\n       cutpts <- c(ux - delta, ux[length(ux)] + delta)\n   }\n   myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n   \n\tif(all(time < surv.time2[surv.event2 == 1])) { return(list(\"spec\"=NA, \"sens\"=NA, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=NA, \"AUC\"=NA, \"data\"=data)) }\n    \n\trocco <- survivalROC::survivalROC(Stime=surv.time2, status=surv.event2, marker=x2, predict.time=time, cut.values=cutpts, entry=surv.entry, span=span, lambda=lambda, ...)\n\tres <- list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts)\n\t\n\treturn(list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=rocco$Survival, \"AUC\"=rocco$AUC, \"data\"=data))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `tdrocc` function and what are its main input parameters?",
        "answer": "The `tdrocc` function is designed to calculate time-dependent receiver operating characteristic (ROC) curves for survival data. Its main input parameters are:\n- `x`: The marker or predictor variable\n- `surv.time`: Survival time\n- `surv.event`: Survival event indicator (0 or 1)\n- `time`: The time point at which to evaluate the ROC curve\n- `cutpts`: Cut points for the marker variable (optional)\n- `na.rm`: Whether to remove NA values\n- `span` and `lambda`: Parameters for smoothing (passed to survivalROC)"
      },
      {
        "question": "How does the function handle missing values and what condition must the survival event variable meet?",
        "answer": "The function handles missing values by:\n1. Checking for complete cases using `complete.cases()`\n2. If `na.rm=FALSE` and NA values are present, it stops with an error\n3. If `na.rm=TRUE`, it removes cases with NA values and optionally prints a message about the number of removed cases\n\nThe survival event variable must take only values 0 or 1. This is checked with the condition:\n`if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }`"
      },
      {
        "question": "What does the function return and under what condition does it return early with NA values?",
        "answer": "The function returns a list containing:\n- `spec`: Specificity values\n- `sens`: Sensitivity values\n- `rule`: A function for classification based on a threshold\n- `cuts`: Cut points used\n- `time`: Time point evaluated\n- `survival`: Survival probability\n- `AUC`: Area Under the Curve\n- `data`: Original input data\n\nThe function returns early with NA values for spec, sens, and AUC if:\n`all(time < surv.time2[surv.event2 == 1])`\nThis condition checks if the specified time point is earlier than all observed event times, in which case the ROC curve cannot be calculated."
      }
    ],
    "completion_tasks": [
      {
        "partial": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  cc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n  \n  x2 <- x[cc.ix]\n  surv.time2 <- surv.time[cc.ix]\n  surv.event2 <- surv.event[cc.ix]\n  \n  if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }\n  if(is.na(cutpts)) {\n    ux <- unique(sort(x2))\n    delta <- min(diff(ux))/2\n    cutpts <- c(ux - delta, ux[length(ux)] + delta)\n  }\n  myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n  \n  # Complete the function from here\n}",
        "complete": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  cc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n  \n  x2 <- x[cc.ix]\n  surv.time2 <- surv.time[cc.ix]\n  surv.event2 <- surv.event[cc.ix]\n  \n  if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }\n  if(is.na(cutpts)) {\n    ux <- unique(sort(x2))\n    delta <- min(diff(ux))/2\n    cutpts <- c(ux - delta, ux[length(ux)] + delta)\n  }\n  myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n  \n  if(all(time < surv.time2[surv.event2 == 1])) { return(list(\"spec\"=NA, \"sens\"=NA, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=NA, \"AUC\"=NA, \"data\"=data)) }\n  \n  rocco <- survivalROC::survivalROC(Stime=surv.time2, status=surv.event2, marker=x2, predict.time=time, cut.values=cutpts, entry=surv.entry, span=span, lambda=lambda, ...)\n  \n  return(list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=rocco$Survival, \"AUC\"=rocco$AUC, \"data\"=data))\n}"
      },
      {
        "partial": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  # Add input data validation and preprocessing here\n  \n  myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n  \n  if(all(time < surv.time2[surv.event2 == 1])) {\n    # Return early if condition is met\n  }\n  \n  # Add survivalROC calculation here\n  \n  # Return the result\n}",
        "complete": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  cc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n  if (!all(cc.ix) && !na.rm) stop(\"NA values are present!\")\n  if(verbose) message(sprintf(\"%i cases are removed due to NA values\", sum(!cc.ix)))\n  \n  x2 <- x[cc.ix]\n  surv.time2 <- surv.time[cc.ix]\n  surv.event2 <- surv.event[cc.ix]\n  \n  if(!all(sort(unique(surv.event2)) == c(0, 1))) stop(\"survival event variable must take values 0 or 1\")\n  if(is.na(cutpts)) {\n    ux <- unique(sort(x2))\n    delta <- min(diff(ux))/2\n    cutpts <- c(ux - delta, ux[length(ux)] + delta)\n  }\n  \n  myrule <- function(x, thresh) ifelse(x > thresh, 1, 0)\n  \n  if(all(time < surv.time2[surv.event2 == 1])) {\n    return(list(\"spec\"=NA, \"sens\"=NA, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=NA, \"AUC\"=NA, \"data\"=data))\n  }\n  \n  rocco <- survivalROC::survivalROC(Stime=surv.time2, status=surv.event2, marker=x2, predict.time=time, cut.values=cutpts, entry=surv.entry, span=span, lambda=lambda, ...)\n  \n  list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=rocco$Survival, \"AUC\"=rocco$AUC, \"data\"=data)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcAngiogenic.R",
    "language": "R",
    "content": "#' @title Function to compute the subtype scores and risk classifications\n#'   for the angiogenic molecular subtype in ovarian cancer\n#'\n#' @description\n#' This function computes subtype scores and risk classifications from\n#'   gene expression values following the algorithm developed by Bentink,\n#'   Haibe-Kains et al. to identify the angiogenic molecular subtype in\n#'   ovarian cancer.\n#'\n#' @usage\n#' ovcAngiogenic(data, annot, hgs,\n#' gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"),\n#' do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param hgs vector of booleans with TRUE represents the ovarian cancer\n#'   patients who have a high grade, late stage, serous tumor, FALSE otherwise. This is particularly important for properly rescaling the data. If hgs is missing, all the patients will be used to rescale the subtype score.\n#' @param gmap character string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#' - subtype: data frame reporting the subtype score, maximum likelihood\n#'   classification and corresponding subtype probabilities.\n#'\n#' @references\n#' Bentink S, Haibe-Kains B, Risch T, Fan J-B, Hirsch MS, Holton K, Rubio R,\n#'   April C, Chen J, Wickham-Garcia E, Liu J, Culhane AC, Drapkin R, Quackenbush\n#'   JF, Matulonis UA (2012) \"Angiogenic mRNA and microRNA Gene Expression\n#'   Signature Predicts a Novel Subtype of Serous Ovarian Cancer\", PloS one,\n#'   7(2):e30269\n#'\n#' @seealso\n#' [genefu::sigOvcAngiogenic]\n#'\n#' @examples\n#' # load the ovcAngiogenic signature\n#' \n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \n#'   \"entrezgene\"\n#' \n#' # compute relapse score\n#' ovcAngiogenic.nkis <- ovcAngiogenic(data=data.nkis, annot=annot.nkis,\n#'   gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcAngiogenic.nkis$risk)\n#'\n#' @md\n#' @export\novcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\",\n    \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE,\n    verbose=FALSE)\n{\n    # load gene signatures if needed\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcAngiogenic[order(abs(sigOvcAngiogenic[ ,\"weight\"]), \n            decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcAngiogenic), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcAngiogenic[gix, ,drop=FALSE]\n    }\n\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    ## rescale only with the high grade, late stage, serous (hgs) patients\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    ## rescale the whole dataset\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n    emclust.ts <- mclust::estep(modelName=\"E\", data=pscore, parameters=modelOvcAngiogenic)\n    dimnames(emclust.ts$z) <- list(names(pscore), c(\"Angiogenic.proba\", \"nonAngiogenic.proba\"))\n    class.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n    names(class.ts) <- names(pscore)\n    sbt.ts <- class.ts\n    sbt.ts[class.ts == 1] <- \"Angiogenic\"\n    sbt.ts[class.ts == 2] <- \"nonAngiogenic\"\n    sbts <- data.frame(\"subtype.score\"=pscore, \"subtype\"=sbt.ts, emclust.ts$z)\n    prisk <- as.numeric(sbts[ ,\"subtype\"] == \"Angiogenic\")\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `ovcAngiogenic` function?",
        "answer": "The `ovcAngiogenic` function computes subtype scores and risk classifications for the angiogenic molecular subtype in ovarian cancer. It uses gene expression data and follows the algorithm developed by Bentink, Haibe-Kains et al. The function returns continuous signature scores, binary risk classifications, and subtype probabilities."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs mapping through Entrez Gene IDs. It selects the most variant probe for each gene in case of ambiguities. The function uses the `geneid.map` function to map gene IDs between the input data and the signature. It also creates a `myprobe` data frame that shows the correspondence between original probes, gene maps, and new probes."
      },
      {
        "question": "How are the subtype scores calculated and rescaled in the `ovcAngiogenic` function?",
        "answer": "The function calculates subtype scores using the `sig.score` function from the genefu package. It then rescales these scores using only high-grade, late-stage, serous (hgs) patients with the `rescale` function. The rescaled scores are transformed to a range of -1 to 1. Finally, it uses the Mclust package to perform clustering and calculate subtype probabilities based on these rescaled scores."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        # Add mapping logic here\n    } else {\n        # Add non-mapping logic here\n    }\n\n    # Add score calculation and classification logic here\n\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}",
        "complete": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcAngiogenic[order(abs(sigOvcAngiogenic[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcAngiogenic), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcAngiogenic[gix, ,drop=FALSE]\n    }\n\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n    emclust.ts <- mclust::estep(modelName=\"E\", data=pscore, parameters=modelOvcAngiogenic)\n    dimnames(emclust.ts$z) <- list(names(pscore), c(\"Angiogenic.proba\", \"nonAngiogenic.proba\"))\n    class.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n    names(class.ts) <- names(pscore)\n    sbt.ts <- class.ts\n    sbt.ts[class.ts == 1] <- \"Angiogenic\"\n    sbt.ts[class.ts == 2] <- \"nonAngiogenic\"\n    sbts <- data.frame(\"subtype.score\"=pscore, \"subtype\"=sbt.ts, emclust.ts$z)\n    prisk <- as.numeric(sbts[ ,\"subtype\"] == \"Angiogenic\")\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}"
      },
      {
        "partial": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    # Load required data\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n\n    # Perform mapping or non-mapping logic\n\n    # Calculate scores and classify\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n\n    # Add classification logic here\n\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}",
        "complete": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) stop(\"gmap is not a column of annot!\")\n        if(verbose) message(\"the most variant probe is selected for each gene\")\n        sigt <- sigOvcAngiogenic[order(abs(sigOvcAngiogenic[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcAngiogenic), colnames(data))\n        if(length(gix) < 2) stop(\"data do not contain enough gene from the ovcTCGA signature!\")\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcAngiogenic[gix, ,drop=FALSE]\n    }\n\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n    emclust.ts <- mclust::estep(modelName=\"E\", data=pscore, parameters=modelOvcAngiogenic)\n    dimnames(emclust.ts$z) <- list(names(pscore), c(\"Angiogenic.proba\", \"nonAngiogenic.proba\"))\n    class.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n    names(class.ts) <- names(pscore)\n    sbt.ts <- ifelse(class.ts == 1, \"Angiogenic\", \"nonAngiogenic\")\n    sbts <- data.frame(\"subtype.score\"=pscore, \"subtype\"=sbt.ts, emclust.ts$z)\n    prisk <- as.numeric(sbts[ ,\"subtype\"] == \"Angiogenic\")\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcCrijns.R",
    "language": "R",
    "content": "#' @title Function to compute the subtype scores and risk classifications\n#'   for the prognostic signature published by Crinjs et al.\n#'\n#' @description\n#' This function computes subtype scores and risk classifications from gene\n#' expression values using the weights published by Crijns et al.\n#'\n#' @usage\n#' ovcCrijns(data, annot, hgs,\n#'   gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"),\n#'   do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param hgs vector of booleans with TRUE represents the ovarian cancer\n#'   patients who have a high grade, late stage, serous tumor, FALSE otherwise.\n#'   This is particularly important for properly rescaling the data. If hgs is\n#'   missing, all the patients will be used to rescale the subtype score.\n#' @param gmap character string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for each\n#'   gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' Note that the original algorithm has not been implemented as it necessitates\n#'   refitting of the model weights in each new dataset. However the current\n#'   implementation should give similar results.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence.\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Crijns APG, Fehrmann RSN, de Jong S, Gerbens F, Meersma G J, Klip HG,\n#'   Hollema H, Hofstra RMW, te Meerman GJ, de Vries EGE, van der Zee AGJ (2009)\n#'   \"Survival-Related Profile, Pathways, and Transcription Factors in Ovarian\n#'   Cancer\" PLoS Medicine, 6(2):e1000024.\n#'\n#' @seealso\n#' [genefu::sigOvcCrijns]\n#'\n#' @examples\n#' # load the ovsCrijns signature\n#' data(sigOvcCrijns)\n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \n#'   \"entrezgene\"\n#' # compute relapse score\n#' ovcCrijns.nkis <- ovcCrijns(data=data.nkis, annot=annot.nkis, \n#'   gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcCrijns.nkis$risk)\n#'\n#' @md\n#' @export\n#' @name ovcCrijns\novcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\",\n    \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE)\n{\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    ## transform the gene expression in Z-scores\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `ovcCrijns` function and what are its key input parameters?",
        "answer": "The `ovcCrijns` function computes subtype scores and risk classifications for ovarian cancer patients based on gene expression data. Its key input parameters are:\n1. `data`: A matrix of gene expressions with samples in rows and probes in columns.\n2. `annot`: A matrix of annotations with a column named as specified in `gmap`.\n3. `hgs`: A vector of booleans indicating high-grade, late-stage, serous tumors.\n4. `gmap`: The biomaRt attribute to use for mapping (default options provided).\n5. `do.mapping`: A boolean indicating whether to perform mapping through Entrez Gene IDs.\n6. `verbose`: A boolean to control informative message output."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs the following steps:\n1. It checks if the specified `gmap` is a column in the `annot` matrix.\n2. It selects the most variant probe for each gene to handle ambiguities.\n3. It maps the gene IDs between the signature (`sigOvcCrijns`) and the input data.\n4. It updates the `data`, `annot`, and signature (`sigt`) objects to reflect the mapping.\n5. It creates a `mymapping` vector to track the number of mapped genes.\n6. It generates a `myprobe` data frame to store the mapping between original probes, gene IDs, and new probes.\nThis process ensures that the gene identifiers in the input data match those in the signature, allowing for accurate score computation."
      },
      {
        "question": "Explain the process of computing the subtype scores and risk classifications in the `ovcCrijns` function.",
        "answer": "The `ovcCrijns` function computes subtype scores and risk classifications as follows:\n1. It scales the gene expression data to Z-scores using the `scale()` function.\n2. It uses the `genefu::sig.score()` function to compute the subtype scores, passing:\n   - A data frame with probe IDs, Entrez Gene IDs, and coefficients from the signature.\n   - The scaled gene expression data.\n   - The annotation data.\n   - `do.mapping=FALSE` to prevent redundant mapping.\n   - `signed=FALSE` for unsigned scoring.\n3. It calculates risk classifications by comparing each score to the median score:\n   - Scores above the median are classified as high risk (1).\n   - Scores below or equal to the median are classified as low risk (0).\n4. Finally, it returns a list containing:\n   - `score`: The continuous signature scores.\n   - `risk`: The binary risk classifications.\n   - `mapping`: Information about the gene mapping process.\n   - `probe`: A data frame with probe mapping details."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        # Complete the else block\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        # Complete the do.mapping block\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/no.at.risk.R",
    "language": "R",
    "content": "`no.at.risk` <-\nfunction( formula.s, data.s, sub.s=\"all\", t.step, t.end ) {\n# Updated 6.6.11 to work from summary.surfvit\n\n    if( length(sub.s)==1 && sub.s==\"all\" ) sub.s <- rep(TRUE, nrow(data.s))\n    pos <- 1\n    envir = as.environment(pos)\n    assign(\"sub.s\", sub.s, envir = envir)\n\n    sf <- survfit( formula.s, data=data.s, subset=sub.s )\n    if (is.null(sf$strata))\n        sf$strata <- c(\"All\" = length(sf$time))\n    n.strata <- length(sf$strata)\n\n    t.pts <- seq(0, t.end, t.step)\n    sumsf <- summary(sf, times = t.pts, extend = TRUE)\n    tms <- with(sumsf, split(time, strata))\n    rsk <- with(sumsf, split(n.risk, strata))\n\n    nar <- do.call(\"rbind\", rsk)\n    nar <- data.frame(names(sf$strata), nar)\n    colnames(nar) <- c(\"risk.factor\", as.character(tms[[1]]))\n\n    remove(\"sub.s\", envir=.GlobalEnv)\n    nar\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `no.at.risk` function in this R code snippet?",
        "answer": "The `no.at.risk` function calculates and returns the number of subjects at risk over time for a survival analysis. It takes a survival formula, dataset, subset condition, time step, and end time as inputs. The function uses the `survfit` function to compute the survival fit, then summarizes the results at specified time points and formats the output as a data frame with risk factors and corresponding number at risk for each time point."
      },
      {
        "question": "How does the function handle the `sub.s` parameter, and what is its purpose?",
        "answer": "The `sub.s` parameter is used to specify a subset of the data for analysis. If `sub.s` is set to 'all' or not provided, the function creates a logical vector of `TRUE` values for all rows in the dataset. Otherwise, it uses the provided subset condition. The function then assigns this subset to the global environment temporarily for use in the `survfit` function call. After the calculation, it removes the `sub.s` variable from the global environment to avoid polluting the workspace."
      },
      {
        "question": "How does the function handle stratification in the survival analysis, and how is this reflected in the output?",
        "answer": "The function supports stratified survival analysis. If the `survfit` result contains strata, it processes each stratum separately. The output data frame (`nar`) includes a 'risk.factor' column that contains the names of the strata. Each row in the output represents a different stratum, with columns for each time point showing the number at risk. If there's no stratification, a single row with 'All' as the risk factor is returned."
      }
    ],
    "completion_tasks": [
      {
        "partial": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"sub.s\", sub.s, envir = envir)\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n  n.strata <- length(sf$strata)\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  # Complete the function to create and return the 'nar' data frame\n}",
        "complete": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"sub.s\", sub.s, envir = envir)\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n  n.strata <- length(sf$strata)\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  tms <- with(sumsf, split(time, strata))\n  rsk <- with(sumsf, split(n.risk, strata))\n\n  nar <- do.call(\"rbind\", rsk)\n  nar <- data.frame(names(sf$strata), nar)\n  colnames(nar) <- c(\"risk.factor\", as.character(tms[[1]]))\n\n  remove(\"sub.s\", envir=.GlobalEnv)\n  nar\n}"
      },
      {
        "partial": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  assign(\"sub.s\", sub.s, envir = as.environment(1))\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  # Complete the function to process the summary and return the result\n}",
        "complete": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  assign(\"sub.s\", sub.s, envir = as.environment(1))\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  tms <- with(sumsf, split(time, strata))\n  rsk <- with(sumsf, split(n.risk, strata))\n\n  nar <- data.frame(names(sf$strata), do.call(\"rbind\", rsk))\n  colnames(nar) <- c(\"risk.factor\", as.character(tms[[1]]))\n\n  remove(\"sub.s\", envir=.GlobalEnv)\n  nar\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ihc4.R",
    "language": "R",
    "content": "#' @name ihc4\n#' @title Function to compute the IHC4 prognostic score as published by\n#'   Paik et al. in 2004.\n#'\n#' @description\n#' This function computes the prognostic score based on four measured IHC markers\n#'   (ER, PGR, HER2, Ki-67), following the algorithm as published by Cuzick et al. 2011.\n#'   The user has the option to either obtain just the shrinkage-adjusted IHC4 score (IHC4)\n#'   or the overall score htat also combines the clinical score (IHC4+C)\n#'\n#' @usage\n#' ihc4(ER, PGR, HER2, Ki67,age,size,grade,node,ana,scoreWithClinical=FALSE, na.rm = FALSE)\n#'\n#' @param ER ER score between 0-10, calculated as (H-score/30).\n#' @param PGR Progesterone Receptor score between 0-10.\n#' @param HER2 Her2/neu status (0 or 1).\n#' @param Ki67 Ki67 score based on percentage of positively staining malignant cells.\n#' @param age patient age.\n#' @param size tumor size in cm.\n#' @param grade Histological grade, i.e. low (1), intermediate (2) and high (3) grade.\n#' @param node Nodal status.\n#' @param ana treatment with anastrozole.\n#' @param scoreWithClinical TRUE to get IHC4+C score, FALSE to get just the IHC4 score.\n#' @param na.rm TRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @return\n#' Shrinkage-adjusted IHC4 score or the Overall Prognostic Score based on IHC4+C\n#'   (IHC4+Clinical Score)\n#'\n#' @references\n#' Jack Cuzick, Mitch Dowsett, Silvia Pineda, Christopher Wale, Janine Salter, Emma Quinn,\n#'   Lila Zabaglo, Elizabeth Mallon, Andrew R. Green, Ian O. Ellis, Anthony Howell, Aman U.\n#'   Buzdar, and John F. Forbes (2011) \"Prognostic Value of a Combined Estrogen Receptor,\n#'   Progesterone Receptor, Ki-67, and Human Epidermal Growth Factor Receptor 2\n#'   Immunohistochemical Score and Comparison with the Genomic Health Recurrence Score\n#'   in Early Breast Cancer\", Journal of Clinical Oncologoy, 29(32):4273\u20134278.\n#'\n#' @examples\n#' # load NKI dataset\n#' data(nkis)\n#' # compute shrinkage-adjusted IHC4 score\n#' count<-nrow(demo.nkis)\n#' ihc4(ER=sample(x=1:10, size=count,replace=TRUE),PGR=sample(x=1:10, size=count,replace=TRUE),\n#' HER2=sample(x=0:1,size=count,replace=TRUE),Ki67=sample(x=1:100, size=count,replace=TRUE),\n#' scoreWithClinical=FALSE, na.rm=TRUE)\n#'\n#' # compute IHC4+C score\n#' ihc4(ER=sample(x=1:10, size=count,replace=TRUE),PGR=sample(x=1:10, size=count,replace=TRUE),\n#' HER2=sample(x=0:1,size=count,replace=TRUE),Ki67=sample(x=1:100, size=count,replace=TRUE),\n#' age=demo.nkis[,\"age\"],size=demo.nkis[ ,\"size\"],grade=demo.nkis[ ,\"grade\"],node=demo.nkis[ ,\"node\"],\n#' ana=sample(x=0:1,size=count,replace=TRUE), scoreWithClinical=TRUE, na.rm=TRUE)\n#'\n#' @md\n#' @export\nihc4 <- function(ER, PGR, HER2, Ki67,age,size,grade,node,ana,\n                 scoreWithClinical=FALSE,na.rm=FALSE)\n{\n    nn <- names(ER)\n    if(is.null(nn)) { nn <- paste(\"PATIENT\", 1:length(ER), sep=\".\") }\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n\n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]\n    PGR <- PGR[cc.ix]\n    HER2 <- HER2[cc.ix]\n\n    if(length(ER) != length(PGR) || length(ER) != length(HER2) || length(ER) != length(Ki67))\n    {stop(\"ER, PGR, HER2, and Ki67 scores must have the same length!\")}\n    if(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n\n    if(!all(is.element(PGR, c(\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\")))) {\n      stop(\"PGR scores must be between 0 and 10\")\n    }\n\n    if(!all(is.element(ER, c(\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\")))) {\n      stop(\"ER scores must be between 0 and 10\")\n    }\n\n    if(!all(is.element(HER2, c(\"0\", \"1\")))) {\n      #if only \"0\" and \"1\" are available, map \"0\" -> \"1\" and \"1\" -> \"3\"\n      stop(\"her2 expression must be 0 or 1!\")\n    }\n\n    if(!is.numeric(Ki67)) {\n      stop(\"Ki67 must be numeric!\")\n    }\n\n    ihc4 <- 94.7 * ((0.586*HER2)-(0.100*ER)-(0.079*PGR)+(0.240*log(1 + 10 * Ki67)))\n\n    if(scoreWithClinical==FALSE)\n    {\n        names(ihc4) <- nn[cc.ix]\n        ihc4.score <- rep(NA, length(cc.ix))\n        names(ihc4.score) <- nn\n        ihc4.score[names(ihc4)] <- ihc4\n        return(\"score\"=ihc4.score)\n    }\n\n    if(scoreWithClinical==TRUE){\n        size <- size[cc.ix]\n        grade <- grade[cc.ix]\n        node <- node[cc.ix]\n\n        if(length(size) != length(grade) || length(grade) != length(node)) {\n          stop(\"size, grade and lymph node stage must have the same length!\")\n        }\n        if(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n        if(!all(is.element(grade, c(\"1\", \"2\", \"3\")))) {\n          stop(\"grade must be 1, 2 or 3!\")\n        }\n        if(!all(is.element(node, c(\"0\",\"1\", \"2\", \"3\")))) {\n          #if only \"0\" and \"1\" are available, map \"0\" -> \"1\" and \"1\" -> \"3\"\n          stop(\"lymph node stage must be 1, 2 or 3!\")\n        }\n        if(!is.numeric(size)) {\n          stop(\"tumor size (cm) must be numeric!\")\n        }\n\n        Ana<-ana\n        N_1to3<-ifelse(node>3,0,node)\n        N4<-ifelse(node==4,node,0)\n        Age<-ifelse(age>65,age,0) #Age above 65 yrs\n        T_1to2<-ifelse(grade<=2,grade,0)\n        T_2to3<-ifelse((grade==2 | grade==3),grade,0)\n        Tabove3<-ifelse(grade>3,grade,0)\n        G2<-ifelse(grade==2,grade,0)\n        G3<-ifelse(grade==3,grade,0)\n\n\n        ClinicalScore<- 100 * ((0.417 * N_1to3) + (1.566 * N4)\n                               + (0.930 * ((0.479*T_1to2)+(0.882*T_2to3)\n                                  +(1.838*Tabove3)+(0.559*G2)+(0.970*G3)+(0.130*Age)-(0.149*Ana))))\n\n        Overall<-ihc4+ClinicalScore\n\n        names(Overall) <- nn[cc.ix]\n        Overall.score <- rep(NA, length(cc.ix))\n        names(Overall.score) <- nn\n        Overall.score[names(Overall)] <- ihc4\n        return(\"score\"=Overall.score)\n    }\n  }",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ihc4` function and what are its main input parameters?",
        "answer": "The `ihc4` function computes the IHC4 prognostic score for breast cancer patients. It takes four main IHC markers as input: ER (Estrogen Receptor), PGR (Progesterone Receptor), HER2 (Human Epidermal Growth Factor Receptor 2), and Ki67 (a proliferation marker). Additional clinical parameters like age, tumor size, grade, nodal status, and treatment with anastrozole can be included for a combined IHC4+C score. The function can return either the shrinkage-adjusted IHC4 score or the overall IHC4+C score, depending on the `scoreWithClinical` parameter."
      },
      {
        "question": "How does the function handle missing values and what input validation does it perform?",
        "answer": "The function handles missing values using the `na.rm` parameter. If `na.rm` is set to FALSE and there are missing values, the function will stop with an error message. The function performs several input validations: it checks if ER, PGR, HER2, and Ki67 scores have the same length, ensures PGR and ER scores are between 0 and 10, verifies HER2 status is either 0 or 1, and confirms Ki67 is numeric. For clinical parameters, it checks if size, grade, and node have the same length, ensures grade is 1, 2, or 3, verifies node status is 0, 1, 2, or 3, and confirms tumor size is numeric."
      },
      {
        "question": "How is the IHC4 score calculated in the function, and what additional steps are taken if the clinical score is included?",
        "answer": "The IHC4 score is calculated using the formula: 94.7 * ((0.586*HER2) - (0.100*ER) - (0.079*PGR) + (0.240*log(1 + 10 * Ki67))). If `scoreWithClinical` is TRUE, the function calculates an additional Clinical Score using factors such as nodal status, tumor size, grade, age, and treatment with anastrozole. The Clinical Score is then added to the IHC4 score to produce the Overall Prognostic Score (IHC4+C). The function returns either the IHC4 score alone or the Overall Score, depending on the `scoreWithClinical` parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    nn <- names(ER)\n    if(is.null(nn)) { nn <- paste(\"PATIENT\", 1:length(ER), sep=\".\") }\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n\n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]\n    PGR <- PGR[cc.ix]\n    HER2 <- HER2[cc.ix]\n\n    # Add input validation here\n\n    ihc4 <- 94.7 * ((0.586*HER2)-(0.100*ER)-(0.079*PGR)+(0.240*log(1 + 10 * Ki67)))\n\n    if(!scoreWithClinical) {\n        # Return IHC4 score\n    } else {\n        # Calculate and return IHC4+C score\n    }\n}",
        "complete": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    nn <- names(ER)\n    if(is.null(nn)) { nn <- paste(\"PATIENT\", 1:length(ER), sep=\".\") }\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n\n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]\n    PGR <- PGR[cc.ix]\n    HER2 <- HER2[cc.ix]\n    Ki67 <- Ki67[cc.ix]\n\n    if(length(ER) != length(PGR) || length(ER) != length(HER2) || length(ER) != length(Ki67))\n        stop(\"ER, PGR, HER2, and Ki67 scores must have the same length!\")\n    if(!all(cc.ix) & !na.rm)  stop(\"NA values are present!\")\n    if(!all(ER %in% 0:10)) stop(\"ER scores must be between 0 and 10\")\n    if(!all(PGR %in% 0:10)) stop(\"PGR scores must be between 0 and 10\")\n    if(!all(HER2 %in% c(0, 1))) stop(\"HER2 expression must be 0 or 1!\")\n    if(!is.numeric(Ki67)) stop(\"Ki67 must be numeric!\")\n\n    ihc4 <- 94.7 * ((0.586*HER2)-(0.100*ER)-(0.079*PGR)+(0.240*log(1 + 10 * Ki67)))\n\n    if(!scoreWithClinical) {\n        ihc4.score <- rep(NA, length(cc.ix))\n        names(ihc4.score) <- nn\n        ihc4.score[names(ihc4)] <- ihc4\n        return(list(score=ihc4.score))\n    } else {\n        size <- size[cc.ix]\n        grade <- grade[cc.ix]\n        node <- node[cc.ix]\n        age <- age[cc.ix]\n        ana <- ana[cc.ix]\n\n        if(length(size) != length(grade) || length(grade) != length(node))\n            stop(\"size, grade and lymph node stage must have the same length!\")\n        if(!all(grade %in% 1:3)) stop(\"grade must be 1, 2 or 3!\")\n        if(!all(node %in% 0:3)) stop(\"lymph node stage must be 0, 1, 2 or 3!\")\n        if(!is.numeric(size)) stop(\"tumor size (cm) must be numeric!\")\n\n        ClinicalScore <- 100 * ((0.417 * pmin(node, 3)) + (1.566 * (node == 4)) +\n                                (0.930 * ((0.479 * (size <= 2)) + (0.882 * (size > 2 & size <= 3)) +\n                                           (1.838 * (size > 3)) + (0.559 * (grade == 2)) +\n                                           (0.970 * (grade == 3)) + (0.130 * (age > 65)) - (0.149 * ana))))\n\n        Overall <- ihc4 + ClinicalScore\n        Overall.score <- rep(NA, length(cc.ix))\n        names(Overall.score) <- nn\n        Overall.score[names(Overall)] <- Overall\n        return(list(score=Overall.score))\n    }\n}"
      },
      {
        "partial": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    # Input validation and preprocessing\n    \n    # Calculate IHC4 score\n    \n    if(!scoreWithClinical) {\n        # Return IHC4 score\n    } else {\n        # Calculate clinical score components\n        \n        # Calculate overall score\n        \n        # Return overall score\n    }\n}",
        "complete": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    nn <- names(ER) %||% paste(\"PATIENT\", seq_along(ER), sep=\".\")\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n    \n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]; PGR <- PGR[cc.ix]; HER2 <- HER2[cc.ix]; Ki67 <- Ki67[cc.ix]\n    \n    if(length(ER) != length(PGR) || length(ER) != length(HER2) || length(ER) != length(Ki67))\n        stop(\"ER, PGR, HER2, and Ki67 scores must have the same length!\")\n    if(!all(cc.ix) && !na.rm) stop(\"NA values are present!\")\n    if(!all(ER %in% 0:10) || !all(PGR %in% 0:10)) stop(\"ER and PGR scores must be between 0 and 10\")\n    if(!all(HER2 %in% c(0, 1))) stop(\"HER2 expression must be 0 or 1!\")\n    if(!is.numeric(Ki67)) stop(\"Ki67 must be numeric!\")\n    \n    ihc4 <- 94.7 * ((0.586*HER2) - (0.100*ER) - (0.079*PGR) + (0.240*log(1 + 10*Ki67)))\n    \n    if(!scoreWithClinical) {\n        score <- rep(NA_real_, length(cc.ix))\n        names(score) <- nn\n        score[names(ihc4)] <- ihc4\n        return(list(score=score))\n    } else {\n        size <- size[cc.ix]; grade <- grade[cc.ix]; node <- node[cc.ix]\n        age <- age[cc.ix]; ana <- ana[cc.ix]\n        \n        if(length(size) != length(grade) || length(grade) != length(node))\n            stop(\"size, grade and lymph node stage must have the same length!\")\n        if(!all(grade %in% 1:3)) stop(\"grade must be 1, 2 or 3!\")\n        if(!all(node %in% 0:4)) stop(\"lymph node stage must be 0, 1, 2, 3 or 4!\")\n        if(!is.numeric(size)) stop(\"tumor size (cm) must be numeric!\")\n        \n        clinical_score <- 100 * (0.417 * pmin(node, 3) + 1.566 * (node == 4) +\n                                  0.930 * (0.479 * (size <= 2) + 0.882 * (size > 2 & size <= 3) +\n                                           1.838 * (size > 3) + 0.559 * (grade == 2) +\n                                           0.970 * (grade == 3) + 0.130 * (age > 65) - 0.149 * ana))\n        \n        overall <- ihc4 + clinical_score\n        score <- rep(NA_real_, length(cc.ix))\n        names(score) <- nn\n        score[names(overall)] <- overall\n        return(list(score=score))\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/medianCtr.R",
    "language": "R",
    "content": "#' @title Center around the median\n#'\n#' @description\n#' Utility function called within the claudinLow classifier\n#'\n#' @usage\n#' medianCtr(x)\n#'\n#' @param x\t Matrix of numbers\n#'\n#' @return\n#' A matrix of median-centered numbers\n#'\n#' @references\n#' citation(\"claudinLow\")\n#'\n#' @seealso\n#' [genefu::claudinLow]\n#'\n#' @md\n#' @export\nmedianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  dimnames(x) <- annAll\n  return(x)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `medianCtr` function in the given R code snippet?",
        "answer": "The `medianCtr` function is a utility function used within the claudinLow classifier. Its purpose is to center a matrix of numbers around their respective medians. It takes a matrix as input, calculates the median of each row, subtracts these medians from their corresponding rows, and returns the median-centered matrix while preserving the original dimension names."
      },
      {
        "question": "How does the `medianCtr` function handle missing values (NA) in the input matrix?",
        "answer": "The `medianCtr` function handles missing values (NA) by using the `na.rm=TRUE` argument in the `median` function call within `apply`. This ensures that the median is calculated by ignoring any NA values in each row. However, it's important to note that the function does not remove or replace NA values in the final output; it only ignores them when calculating the median for centering."
      },
      {
        "question": "What is the significance of the `scale` function in the `medianCtr` implementation, and how is it used?",
        "answer": "The `scale` function is used in `medianCtr` to perform the median centering operation. It's applied with `center=medians` and `scale=FALSE` arguments. This means it subtracts the calculated row medians from each element in the corresponding row (centering) without performing any scaling. The `t` function is used before and after `scale` to transpose the matrix, allowing row-wise operations to be performed as column-wise operations, which is more efficient in R."
      }
    ],
    "completion_tasks": [
      {
        "partial": "medianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  # Complete the function\n}",
        "complete": "medianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  dimnames(x) <- annAll\n  return(x)\n}"
      },
      {
        "partial": "medianCtr <- function(x){\n  # Calculate medians\n  # Center the data\n  # Restore dimension names\n  # Return the result\n}",
        "complete": "medianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  dimnames(x) <- annAll\n  return(x)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/balanced.hazard.ratio.R",
    "language": "R",
    "content": "`balanced.hazard.ratio` <-\nfunction(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...)\n{\n    #Balanced Hazard ratio\n    \n    if (missing(method.test))\n    {\n        method.test = \"logrank\"\n    }\n    if (missing(ties))\n    {\n        ties = \"breslow\"\n    }\n    if(!missing(weights))\n    {\n        if(length(weights) != length(x))\n        {\n            stop(\"bad length for parameter weights!\")\n        }\n    } else {\n        weights <- rep(1,  length(x))\n    }\n    if(!missing(strat)) {\n        if(length(strat) != length(x))\n        {\n            stop(\"bad length for parameter strat!\")\n        }\n        ## remove weights=0 because the coxph function does not deal with them properly\n        iix <- weights <= 0\n        if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n        weights[iix] <- NA\n    } else { \n        strat <- rep(1,  length(x))\n    }\n    \n    \n    #Duplicating the patients\n    cl = sort(unique(x))\n    \n    repTime = c()\n    repEvent = c()\n    repX = c()\n    repWeights = c()\n    repStrat = c()\n    \n    xOld = x\n    for(i in 1:(length(cl)))\n    {\n        x[xOld==cl[i]] = i*2\n    }\n    \n    for(i in 1:(length(cl)-1))\n    {\n        ind = c(which(x==i*2),which(x==(i+1)*2))\n        repTime = c(repTime,surv.time[ind])\n        repEvent = c(repEvent,surv.event[ind])\n        repX = c(repX,rep(i*2+1,length(ind)))\n        repWeights = c(repWeights,weights[ind])\n        repStrat = c(repStrat,strat[ind])\n    }\n    \n    #Compute the hazard ratio on the duplicated patients\n    BHr = hazard.ratio( x=c(x,repX), surv.time=c(surv.time,repTime), surv.event=c(surv.event,repEvent), weights=c(weights,repWeights), strat=c(strat,repStrat), alpha=alpha, method.test=method.test, ties=\"breslow\", ...)\n    \n    BHr$balanced.hazard.ratio = BHr$hazard.ratio\n    BHr$hazard.ratio = NULL\n    BHr$n = length(x)\n    BHr$data$x = x\n    BHr$data$surv.time = surv.time\n    BHr$data$surv.event = surv.event\n    \n    return(BHr)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `balanced.hazard.ratio` function and how does it differ from a standard hazard ratio calculation?",
        "answer": "The `balanced.hazard.ratio` function calculates a balanced hazard ratio, which is a modification of the standard hazard ratio. It works by duplicating patients and creating intermediate groups between existing groups. This approach helps to account for non-linear relationships between the predictor variable and the hazard, providing a more robust estimate of the overall effect across the range of the predictor. The function first prepares the data by duplicating and modifying it, then calculates the hazard ratio using the modified dataset."
      },
      {
        "question": "How does the function handle missing or incorrect input parameters?",
        "answer": "The function has several error handling mechanisms:\n1. It sets default values for `method.test` ('logrank') and `ties` ('breslow') if they are not provided.\n2. It checks if the length of `weights` and `strat` (if provided) match the length of `x`. If not, it stops execution with an error message.\n3. If `weights` are not provided, it creates a vector of 1s with the same length as `x`.\n4. If `strat` is not provided, it creates a vector of 1s with the same length as `x`.\n5. It issues a warning and discards samples with weights <= 0 when stratification is used.\nThese checks ensure that the function can handle various input scenarios and provide informative error messages when inputs are incorrect."
      },
      {
        "question": "Explain the process of duplicating patients in the `balanced.hazard.ratio` function. Why is this done and how does it affect the final calculation?",
        "answer": "The process of duplicating patients in the `balanced.hazard.ratio` function involves:\n1. Creating new groups between existing groups by assigning new values to `x`.\n2. Duplicating the survival times, events, and other relevant data for patients in adjacent groups.\n3. Creating a new group that sits between the two original groups.\n\nThis duplication is done to create a more continuous representation of the hazard across the range of the predictor variable. By introducing these intermediate groups, the function can capture non-linear relationships between the predictor and the hazard that might be missed by a standard hazard ratio calculation.\n\nThe final calculation uses this expanded dataset to compute the hazard ratio, which results in a 'balanced' hazard ratio that represents the average effect across the entire range of the predictor, rather than just comparing two distinct groups. This approach can provide a more robust and interpretable measure of the overall effect, especially when the relationship between the predictor and the hazard is not strictly linear."
      }
    ],
    "completion_tasks": [
      {
        "partial": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    if (missing(method.test)) method.test = \"logrank\"\n    if (missing(ties)) ties = \"breslow\"\n    if (!missing(weights)) {\n        if (length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n    } else {\n        weights <- rep(1, length(x))\n    }\n    if (!missing(strat)) {\n        if (length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n        iix <- weights <= 0\n        if (any(iix)) warning(\"samples with weight<=0 are discarded\")\n        weights[iix] <- NA\n    } else {\n        strat <- rep(1, length(x))\n    }\n    \n    # Complete the function to duplicate patients and compute balanced hazard ratio\n}",
        "complete": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    if (missing(method.test)) method.test = \"logrank\"\n    if (missing(ties)) ties = \"breslow\"\n    if (!missing(weights)) {\n        if (length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n    } else {\n        weights <- rep(1, length(x))\n    }\n    if (!missing(strat)) {\n        if (length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n        iix <- weights <= 0\n        if (any(iix)) warning(\"samples with weight<=0 are discarded\")\n        weights[iix] <- NA\n    } else {\n        strat <- rep(1, length(x))\n    }\n    \n    cl <- sort(unique(x))\n    x_old <- x\n    x[x_old %in% cl] <- match(x_old, cl) * 2\n    \n    rep_data <- lapply(1:(length(cl)-1), function(i) {\n        ind <- x %in% c(i*2, (i+1)*2)\n        list(\n            time = surv.time[ind],\n            event = surv.event[ind],\n            x = rep(i*2+1, sum(ind)),\n            weights = weights[ind],\n            strat = strat[ind]\n        )\n    })\n    \n    rep_data <- do.call(Map, c(list(c), rep_data))\n    \n    BHr <- hazard.ratio(\n        x = c(x, rep_data$x),\n        surv.time = c(surv.time, rep_data$time),\n        surv.event = c(surv.event, rep_data$event),\n        weights = c(weights, rep_data$weights),\n        strat = c(strat, rep_data$strat),\n        alpha = alpha,\n        method.test = method.test,\n        ties = \"breslow\",\n        ...\n    )\n    \n    BHr$balanced.hazard.ratio <- BHr$hazard.ratio\n    BHr$hazard.ratio <- NULL\n    BHr$n <- length(x)\n    BHr$data <- list(x = x, surv.time = surv.time, surv.event = surv.event)\n    \n    return(BHr)\n}"
      },
      {
        "partial": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    # Initialize method.test, ties, weights, and strat\n    \n    # Duplicate patients\n    cl <- sort(unique(x))\n    x_old <- x\n    x[x_old %in% cl] <- match(x_old, cl) * 2\n    \n    # Complete the function to create replicated data and compute balanced hazard ratio\n}",
        "complete": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    method.test <- if (missing(method.test)) \"logrank\" else match.arg(method.test)\n    ties <- if (missing(ties)) \"breslow\" else match.arg(ties)\n    weights <- if (missing(weights)) rep(1, length(x)) else {\n        if (length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n        weights\n    }\n    strat <- if (missing(strat)) rep(1, length(x)) else {\n        if (length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n        iix <- weights <= 0\n        if (any(iix)) warning(\"samples with weight<=0 are discarded\")\n        weights[iix] <- NA\n        strat\n    }\n    \n    cl <- sort(unique(x))\n    x_old <- x\n    x[x_old %in% cl] <- match(x_old, cl) * 2\n    \n    rep_data <- do.call(rbind, lapply(1:(length(cl)-1), function(i) {\n        ind <- x %in% c(i*2, (i+1)*2)\n        data.frame(\n            time = surv.time[ind],\n            event = surv.event[ind],\n            x = rep(i*2+1, sum(ind)),\n            weights = weights[ind],\n            strat = strat[ind]\n        )\n    }))\n    \n    BHr <- hazard.ratio(\n        x = c(x, rep_data$x),\n        surv.time = c(surv.time, rep_data$time),\n        surv.event = c(surv.event, rep_data$event),\n        weights = c(weights, rep_data$weights),\n        strat = c(strat, rep_data$strat),\n        alpha = alpha,\n        method.test = method.test,\n        ties = \"breslow\",\n        ...\n    )\n    \n    BHr$balanced.hazard.ratio <- BHr$hazard.ratio\n    BHr$hazard.ratio <- NULL\n    BHr$n <- length(x)\n    BHr$data <- list(x = x, surv.time = surv.time, surv.event = surv.event)\n    \n    return(BHr)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/data.R",
    "language": "R",
    "content": "#' @name claudinLowData\n#'\n#' @title claudinLowData for use in the claudinLow classifier. Data generously provided by Aleix Prat.\n#'\n#' @description Training and Testing Data for use with the Claudin-Low Classifier\n#'\n#' @usage data(claudinLowData)\n#'\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#'\n#' @format\n#'  - xd: Matrix of 807 features and 52 samples\n#'  - classes: factor to split samples\n#'  - nfeatures: number of features\n#'  - nsamples: number of samples\n#'  - fnames: names of features\n#'  - snames: names of samples\n#'\n#' @references Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso [genefu::claudinLow()]\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL\n\n#' @name expos\n#'\n#' @aliases data.expos annot.expos demo.expos\n#'\n#' @title Gene expression, annotations and clinical data from the International Genomics Consortium\n#'\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data from the expO dataset collected by the International Genomics Consortium ([](http://www.intgen.org/expo/)).\n#'\n#' @format expos is a dataset containing three matrices\n#'   - data.expos: Matrix containing gene expressions as measured by Affymetrix hgu133plus2 technology (single-channel, oligonucleotides)\n#'   - annot.expos: Matrix containing annotations of ffymetrix hgu133plus2 microarray platform\n#'   - demo.expos: Clinical information of the breast cancer patients whose tumors were hybridized\n#'\n#' @source [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109)\n#'\n#' @references\n#' International Genomics Consortium, http://www.intgen.org/research-services/biobanking-experience/expo/\n#' McCall MN, Bolstad BM, Irizarry RA. (2010) \"Frozen robust multiarray analysis (fRMA)\", Biostatistics, 11(2):242-253.\n#'\n#' @usage data(expos)\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL\n\n#' @name mod1\n#' @md\n#' @docType data\n#' @title Gene modules published in Desmedt et al. 2008\n#' @description List of seven gene modules published in Desmedt et a. 2008, i.e. ESR1 (estrogen receptor pathway), ERBB2 (her2/neu receptor pathway), AURKA (proliferation), STAT1 (immune response), PLAU (tumor invasion), VEGF (angogenesis) and CASP3 (apoptosis).\n#' @usage data(mod1)\n#' @details mod1 is a list of seven gene signatures, i.e. matrices with 3 columns containing the annotations and information related to the signatures themselves.\n#' @references Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend on the molecular subtypes\", Clinical Cancer Research, 14(16):5158--5165.\n#' @keywords data\nNULL\n\n#' @name mod2\n#' @md\n#' @docType data\n#' @title Gene modules published in Wirapati et al. 2008\n#' @description List of seven gene modules published in Wirapati et a. 2008, i.e. ESR1 (estrogen receptor pathway), ERBB2 (her2/neu receptor pathway) and AURKA (proliferation).\n#' @usage data(mod2)\n#' @details mod2 is a list of three gene signatures, i.e. matrices with 3 columns containing the annotations and information related to the signatures themselves.\n#' @source [http://breast-cancer-research.com/content/10/4/R65](http://breast-cancer-research.com/content/10/4/R65)\n#' @references Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' @keywords data\nNULL\n\n#' @name modelOvcAngiogenic\n#' @docType data\n#' @title Model used to classify ovarian tumors into Angiogenic and NonAngiogenic subtypes.\n#' @description Object containing the set of parameters for the mixture of Gaussians used as a model to classify ovarian tumors into Angiogenic and NonAngiogenic subtypes.\n#' @usage data(modelOvcAngiogenic)\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Bentink S, Haibe-Kains B, Risch T, Fan J-B, Hirsch MS, Holton K, Rubio R, April C, Chen J, Wickham-Garcia E, Liu J, Culhane AC, Drapkin R, Quackenbush JF, Matulonis UA (2012) \"Angiogenic mRNA and microRNA Gene Expression Signature Predicts a Novel Subtype of Serous Ovarian Cancer\", PloS one, 7(2):e30269\n#' @keywords data\nNULL\n\n#' @name nkis\n#' @md\n#' @aliases data.nkis annot.nkis demo.nkis\n#' @docType data\n#' @title Gene expression, annotations and clinical data from van de Vijver et al. 2002\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data as published in van de Vijver et al. 2002.\n#' @usage data(nkis)\n#' @details This dataset represent only partially the one published by van  de Vijver et al. in 2008. Indeed, only part of the patients (150) and gene expressions (922) in [`data.nkis`].\n#' @format nkis is a dataset containing three matrices:\n#'   - data.nkis: Matrix containing gene expressions as measured by Agilent technology (dual-channel, oligonucleotides)\n#'   - annot.nkis: Matrix containing annotations of Agilent microarray platform\n#'   - demon.nkis: Clinical information of the breast cancer patients whose tumors were hybridized\n#' @source [http://www.nature.com/nature/journal/v415/n6871/full/415530a.html](http://www.nature.com/nature/journal/v415/n6871/full/415530a.html)\n#' @references M. J. van de Vijver and Y. D. He and L. van't Veer and H. Dai and A. M. Hart and D. W. Voskuil and G. J. Schreiber and J. L. Peterse and C. Roberts and M. J. Marton and M. Parrish and D. Atsma and A. Witteveen and A. Glas and L. Delahaye and T. van der Velde and H. Bartelink and S. Rodenhuis and E. T. Rutgers and S. H. Friend and R. Bernards (2002) \"A Gene Expression Signature as a Predictor of Survival in Breast Cancer\", New England Journal of Medicine, 347(25):1999--2009\n#' @keywords data\nNULL\n\n#' @name pam50\n#' @md\n#' @aliases pam50.scale pam50.robust\n#' @docType data\n#' @title PAM50 classifier for identification of breast cancer molecular subtypes (Parker et al 2009)\n#' @description List of parameters defining the PAM50 classifier for identification of breast cancer molecular subtypes (Parker et al 2009).\n#' @usage\n#' data(pam50)\n#' data(pam50.scale)\n#' data(pam50.robust)\n#' @format List of parameters for PAM50:\n#'  - centroids: Gene expression centroids for each subtype.\n#'  - centroids.map: Mapping for centroids.\n#'  - method.cor: Method of correlation used to compute distance to the centroids.\n#'  - method.centroids: Method used to compute the centroids.\n#'  - std: Method of standardization for gene expressions (\"none\", \"scale\" or \"robust\")\n#'  - mins: Minimum number of samples within each cluster allowed during the fitting of the model.\n#' @details Three versions of the model are provided, each of ones differs by the gene expressions standardization method since it has an important impact on the subtype classification:\n#'   - pam50: Use of the official centroids without scaling of the gene expressions.\n#'   - pam50.scale: Use of the official centroids with traditional scaling of the gene expressions (see [`base::scale()`])\n#'   - pam50.robust: Use of the official centroids with robust scaling of the gene expressions (see [`genefu::rescale()`])\n#' The model `pam50.robust`` has been shown to reach the best concordance with the traditional clinical parameters (ER IHC, HER2 IHC/FISH and histological grade). However the use of this model is recommended only when the dataset is representative of a global population of breast cancer patients (no sampling bias, the 5 subtypes should be present).\n#' @source [http://jco.ascopubs.org/cgi/content/short/JCO.2008.18.1370v1](http://jco.ascopubs.org/cgi/content/short/JCO.2008.18.1370v1)\n#' @references Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, Samuel and Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, Christiane and He, Xiaping and Hu, Zhiyuan and Quackenbush, John F. and Stijleman, Inge J. and Palazzo, Juan and Marron, J.S. and Nobel, Andrew B. and Mardis, Elaine and Nielsen, Torsten O. and Ellis, Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009) \"Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\", Journal of Clinical Oncology, 27(8):1160--1167\n#' @keywords data\nNULL\n\n#' @name scmgene.robust\n#' @docType data\n#' @title Subtype Clustering Model using only ESR1, ERBB2 and AURKA genes for identification of breast cancer molecular subtypes\n#' @description List of parameters defining the Subtype Clustering Model as published in Wirapati et al 2009 and Desmedt et al 2008 but using single genes instead of gene modules.\n#' @usage data(scmgene.robust)\n#' @format List of parameters for SCMGENE:\n#'   - parameters: List of parameters for the mixture of three Gaussians (ER-/HER2-, HER2+ and ER+/HER2-) that define the Subtype Clustering Model. The structure is the same than for an [`mclust::Mclust`] object.\n#'   - cutoff.AURKA: Cutoff for AURKA module score in order to identify ER+/HER2- High Proliferation (aka Luminal B) tumors and ER+/HER2- Low Proliferation (aka Luminal A) tumors.\n#'   - mod: ESR1, ERBB2 and AURKA modules.\n#' @source [http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck](http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck)\n#' @references Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend on the molecular subtypes\", Clinical Cancer Research, 14(16):5158--5165.\n#' @keywords data\n#' @md\nNULL\n\n#' @name scmod1.robust\n#' @docType data\n#' @title Subtype Clustering Model using ESR1, ERBB2 and AURKA modules for identification of breast cancer molecular subtypes (Desmedt et al 2008)\n#' @description List of parameters defining the Subtype Clustering Model as published in Desmedt et al 2008.\n#' @usage data(scmod1.robust)\n#' @format List of parameters for SCMOD1:\n#'   - parameters: List of parameters for the mixture of three Gaussians (ER-/HER2-, HER2+ and ER+/HER2-) that define the Subtype Clustering Model. The structure is the same than for an [`mclust::Mclust()`] object.\n#'   - cutoff.AURKA: Cutoff for AURKA module score in order to identify ER+/HER2- High Proliferation (aka Luminal B) tumors and ER+/HER2- Low Proliferation (aka Luminal A) tumors.\n#'   - mod: ESR1, ERBB2 and AURKA modules.\n#' @source [http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck](http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck)\n#' @references Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend on the molecular subtypes\", _Clinical Cancer Research_, *14*(16):5158--5165.\n#' @keywords data\n#' @md\nNULL\n\n#' @name scmod2.robust\n#' @docType data\n#' @title Subtype Clustering Model using ESR1, ERBB2 and AURKA modules for identification of breast cancer molecular subtypes (Desmedt et al 2008)\n#' @description List of parameters defining the Subtype Clustering Model as published in Desmedt et al 2008.\n#' @usage data(scmod1.robust)\n#' @format List of parameters for SCMOD2:\n#'   - parameters: List of parameters for the mixture of three Gaussians (ER-/HER2-, HER2+ and ER+/HER2-) that define the Subtype Clustering Model. The structure is the same than for an [`mclust::Mclust`] object.\n#'   - cutoff.AURKA: Cutoff for AURKA module score in order to identify ER+/HER2- High Proliferation (aka Luminal B) tumors and ER+/HER2- Low Proliferation (aka Luminal A) tumors.\n#'   - mod: ESR1, ERBB2 and AURKA modules.\n#' @source [http://breast-cancer-research.com/content/10/4/R65k](http://breast-cancer-research.com/content/10/4/R65k)\n#' @references Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' @md\nNULL\n\n#' @name sig.endoPredict\n#' @docType data\n#' @title Signature used to compute the endoPredict signature as published by Filipits et al 2011\n#' @description List of 11 genes included in the endoPredict signature. The EntrezGene.ID allows for mapping and the mapping to affy probes is already provided.\n#' @usage data(sig.endoPredict)\n#' @format `sig.endoPredict` is a matrix with 5 columns containing the annotations and information related to the signature itself (including a mapping to Affymetrix HGU platform).\n#' @references Filipits, M., Rudas, M., Jakesz, R., Dubsky, P., Fitzal, F., Singer, C. F., et al. (2011). \"A new molecular predictor of distant recurrence in ER-positive, HER2-negative breast cancer adds independent information to conventional clinical risk factors.\" \\emph{Clinical Cancer Research}, \\bold{17}(18):6012--6020.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.gene70\n#' @docType data\n#' @title Signature used to compute the 70 genes prognosis profile (GENE70) as published by van't Veer et al. 2002\n#' @description List of 70 agilent probe ids representing 56 unique genes included in the GENE70 signature. The EntrezGene.ID allows for mapping and the \"average.good.prognosis.profile\" values allows for signature computation.\n#' @usage data(sig.gene70)\n#' @format sig.gene70 is a matrix with 9 columns containing the annotations and information related to the signature itself.\n#' @source [http://www.nature.com/nature/journal/v415/n6871/full/415530a.html](http://www.nature.com/nature/journal/v415/n6871/full/415530a.html)\n#' @references L. J. van't Veer and H. Dai and M. J. van de Vijver and Y. D. He and A. A. Hart and M. Mao and H. L. Peterse and K. van der Kooy and M. J. Marton and A. T. Witteveen and G. J. Schreiber and R. M. Kerkhiven and C. Roberts and P. S. Linsley and R. Bernards and S. H. Friend (2002) \"Gene Expression Profiling Predicts Clinical Outcome of Breast Cancer\", Nature, 415:530--536.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.gene76\n#' @docType data\n#' @title Signature used to compute the Relapse Score (GENE76) as published in Wang et al. 2005\n#' @description List of 76 affymetrix hgu133a probesets representing 60 unique genes included in the GENE76 signature. The EntrezGene.ID allows for mapping and the coefficient allows for signature computation.\n#' @usage data(sig.gene76)\n#' @format `sig.gene70` is a matrix with 10 columns containing the annotations and information related to the signature itself.\n#' @source [http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(05)17947-1/abstract](http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(05)17947-1/abstract)\n#' @references Y. Wang and J. G. Klijn and Y. Zhang and A. M. Sieuwerts and M. P. Look and F. Yang and D. Talantov and M. Timmermans and M. E. Meijer-van Gelder and J. Yu and T. Jatkoe and E. M. Berns and D. Atkins and J. A. Foekens (2005) \"Gene-Expression Profiles to Predict Distant Metastasis of Lymph-Node-Negative Primary Breast Cancer\", Lancet, 365(9460):671--679.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.genius\n#' @docType data\n#' @title Gene Expression progNostic Index Using Subtypes (GENIUS) as published by Haibe-Kains et al. 2010.\n#' @description List of three gene signatures which compose the Gene Expression progNostic Index Using Subtypes (GENIUS) as published by Haibe-Kains et al. 2009. GENIUSM1, GENIUSM2 and GENIUSM3  are the ER-/HER2-, HER2+ and ER+/HER2- subtype signatures respectively.\n#' @format `sig.genius` is a list a three subtype signatures.\n#' @references Haibe-Kains B, Desmedt C, Rothe F, Sotiriou C and Bontempi G (2010) \"A fuzzy gene expression-based computational approach improves breast cancer prognostication\", Genome Biology, 11(2):R18\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.ggi\n#' @docType data\n#' @title Gene expression Grade Index (GGI) as published in Sotiriou et al. 2006\n#' @description List of 128 affymetrix hgu133a probesets representing 97 unique genes included in the GGI signature. The \"EntrezGene.ID\" column allows for mapping and \"grade\" defines the up-regulation of the expressions either in histological grade 1 or 3.\n#' @usage data(sig.ggi)\n#' @format sig.ggi is a matrix with 9 columns containing the annotations and information related to the signature itself.\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Sotiriou C, Wirapati P, Loi S, Harris A, Bergh J, Smeds J, Farmer P, Praz V, Haibe-Kains B, Lallemand F, Buyse M, Piccart MJ and Delorenzi M (2006) \"Gene expression profiling in breast cancer: Understanding the molecular basis of histologic grade to improve prognosis\", Journal of National Cancer Institute, 98:262--272\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.oncotypedx\n#' @docType data\n#' @title Signature used to compute the OncotypeDX signature as published by Paik et al 2004\n#' @description List of 21 genes included in the OncotypeDX signature. The EntrezGene.ID allows for mapping and the mapping to affy probes is already provided.\n#' @usage data(sig.oncotypedx)\n#' @references S. Paik, S. Shak, G. Tang, C. Kim, J. Bakker, M. Cronin, F. L. Baehner, M. G. Walker, D. Watson, T. Park, W. Hiller, E. R. Fisher, D. L. Wickerham, J. Bryant, and N. Wolmark (2004) \"A Multigene Assay to Predict Recurrence of Tamoxifen-Treated, Node-Negative Breast Cancer\", New England Journal of Medicine, 351(27):2817--2826.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.pik3cags\n#' @docType data\n#' @title Gene expression Grade Index (GGI) as published in Sotiriou et al. 2006\n#' @description List of 278 affymetrix hgu133a probesets representing 236 unique genes included in the PIK3CA-GS signature. The \"EntrezGene.ID\" column allows for mapping and \"coefficient\" refers to to the direction of association with PIK3CA mutation.\n#' @usage data(sig.pik3cags)\n#' @format sig.pik3cags is a matrix with 3 columns containing the annotations and information related to the signature itself.\n#' @source [http://www.pnas.org/content/107/22/10208/suppl/DCSupplemental](http://www.pnas.org/content/107/22/10208/suppl/DCSupplemental)\n#' @references Loi S, Haibe-Kains B, Majjaj S, Lallemand F, Durbecq V, Larsimont D, Gonzalez-Angulo AM, Pusztai L, Symmans FW, Bardelli A, Ellis P, Tutt AN, Gillett CE, Hennessy BT., Mills GB, Phillips WA, Piccart MJ, Speed TP, McArthur GA, Sotiriou C (2010) \"PIK3CA mutations associated with gene signature of low mTORC1 signaling and better outcomes in estrogen receptor-positive breast cancer\", Proceedings of the National Academy of Sciences, 107(22):10208--10213\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.tamr13\n#' @docType data\n#' @title Tamoxifen Resistance signature composed of 13 gene clusters (TAMR13) as published by Loi et al. 2008.\n#' @description List of 13 clusters of genes (and annotations) and their corresponding coefficient as an additional attribute.\n#' @usage data(sig.tamr13)\n#' @format sig.tamr13 is a list a 13 clusters of genes with their corresponding coefficient.\n#' @references Loi S, Haibe-Kains B, Desmedt C, Wirapati P, Lallemand F, Tutt AM, Gillet C, Ellis P, Ryder K, Reid JF, Daidone MG, Pierotti MA, Berns EMJJ, Jansen MPHM, Foekens JA, Delorenzi M, Bontempi G, Piccart MJ and Sotiriou C (2008) \"Predicting prognosis using molecular profiling in estrogen receptor-positive breast cancer treated with tamoxifen\", BMC Genomics, 9(1):239\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcAngiogenic\n#' @title sigOvcAngiogenic dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Bentink S, Haibe-Kains B, Risch T, Fan J-B, Hirsch MS, Holton K, Rubio R, April C, Chen J, Wickham-Garcia E, Liu J, Culhane AC, Drapkin R, Quackenbush JF, Matulonis UA (2012) \"Angiogenic mRNA and microRNA Gene Expression Signature Predicts a Novel Subtype of Serous Ovarian Cancer\", PloS one, 7(2):e30269\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcCrijns\n#' @title sigOvcCrijns dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Crijns APG, Fehrmann RSN, de Jong S, Gerbens F, Meersma G J, Klip HG, Hollema H, Hofstra RMW, te Meerman GJ, de Vries EGE, van der Zee AGJ (2009) \"Survival-Related Profile, Pathways, and Transcription Factors in Ovarian Cancer\" PLoS Medicine, 6(2):e1000024.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcSpentzos\n#' @title sigOcvSpentzos dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Spentzos, D., Levine, D. A., Ramoni, M. F., Joseph, M., Gu, X., Boyd, J., et al. (2004). \"Gene expression signature with independent prognostic significance in epithelial ovarian cancer\". Journal of clinical oncology, 22(23), 4700--4710. doi:10.1200/JCO.2004.04.070\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcTCGA\n#' @title sigOvcTCGA dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Bell D, Berchuck A, Birrer M et al. (2011) \"Integrated genomic analyses of ovarian carcinoma\", Nature, 474(7353):609--615\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcYoshihara\n#' @title sigOvcYoshihara dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Yoshihara K, Tajima A, Yahata T, Kodama S, Fujiwara H, Suzuki M, Onishi Y, Hatae M, Sueyoshi K, Fujiwara H, Kudo, Yoshiki, Kotera K, Masuzaki H, Tashiro H, Katabuchi H, Inoue I, Tanaka K (2010) \"Gene expression profile for predicting survival in advanced-stage serous ovarian cancer across two independent datasets\", PloS one, 5(3):e9615.\n#' @keywords data\n#' @md\nNULL\n\n#' @name ssp2003\n#' @aliases ssp2003.robust ssp2003.scale\n#' @title SSP2003 classifier for identification of breast cancer molecular subtypes (Sorlie et al 2003)\n#' @description List of parameters defining the SSP2003 classifier for identification of breast cancer molecular subtypes (Sorlie et al 2003).\n#' @usage\n#' data(ssp2003)\n#' data(ssp2003.robust)\n#' data(ssp2003.scale)\n#' @docType data\n#' @format List of parameters for SSP2003:\n#'   - centroids: Gene expression centroids for each subtype.\n#'   - centroids.map: Mapping for centroids.\n#'   - method.cor: Method of correlation used to compute distance to the centroids.\n#'   - method.centroids: Method used to compute the centroids.\n#'   - std: Method used to compute the centroids.\n#'   - mins: Minimum number of samples within each cluster allowed during the fitting of the model.\n#' @source [http://www.pnas.org/content/100/14/8418](http://www.pnas.org/content/100/14/8418)\n#' @references T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and A. Nobel and S. Deng and H. Johnsen and R. Pesich and S. Geister and J. Demeter and C. Perou and P. E. Lonning and P. O. Brown and A. L. Borresen-Dale and D. Botstein (2003) \"Repeated Observation of Breast Tumor Subtypes in Independent Gene Expression Data Sets\", Proceedings of the National Academy of Sciences, 1(14):8418--8423\n#' @keywords data\n#' @md\nNULL\n\n#' @name ssp2006\n#' @aliases ssp2006.robust ssp2006.scale\n#' @title SSP2006 classifier for identification of breast cancer molecular subtypes (Hu et al 2006)\n#' @description List of parameters defining the SSP2006 classifier for identification of breast cancer molecular subtypes (Hu et al 2006).\n#' @usage\n#' data(ssp2006)\n#' data(ssp2006.robust)\n#' data(ssp2006.scale)\n#' @format List of parameters for SSP2006:\n#'   - centroids: Gene expression centroids for each subtype.\n#'   - centroids.map: Mapping for centroids.\n#'   - method.cor: Method of correlation used to compute distance to the centroids.\n#'   - method.centroids: Method used to compute the centroids.\n#'   - std: Method of standardization for gene expressions.\n#'   - mins: Minimum number of samples within each cluster allowed during the fitting of the model.\n#' @details Three versions of the model are provided, each of ones differs by the gene expressions standardization method since it has an important impact on the subtype classification:\n#'   - ssp2006: Use of the official centroids without scaling of the gene expressions.\n#'   - ssp2006.scale: Use of the official centroids with traditional scaling of the gene expressions (see [`base::scale()`])\n#'   - ssp2006.robust: Use of the official centroids with robust scaling of the gene expressions (see [`genefu::rescale()`])\n#' The model `ssp2006.robust` has been shown to reach the best concordance with the traditional clinical parameters (ER IHC, HER2 IHC/FISH and histological grade). However the use of this model is recommended only when the dataset is representative of a global population of breast cancer patients (no sampling bias, the 5 subtypes should be present).\n#' @docType data\n#' @source [http://www.biomedcentral.com/1471-2164/7/96](http://www.biomedcentral.com/1471-2164/7/96)\n#' @references Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and Qaqish, Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and Dressler, Lynn and Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer, Lynda and Wu, Junyuan and Liu, Yudong and Nanda, Rita and Tretiakova, Maria and Orrico, Alejandra and Dreher, Donna and Palazzo, Juan and Perreard, Laurent and Nelson, Edward and Mone, Mary and Hansen, Heidi and Mullins, Michael and Quackenbush, John and Ellis, Matthew and Olopade, Olufunmilayo and Bernard, Philip and Perou, Charles (2006) \"The molecular portraits of breast tumors are conserved across microarray platforms\", _BMC Genomics_, *7*(96)\n#' @keywords data\n#' @md\nNULL\n\n#' @name vdxs\n#' @aliases data.vdxs annot.vdxs demo.vdxs\n#' @docType data\n#' @title Gene expression, annotations and clinical data from Wang et al. 2005 and Minn et al 2007\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data as published in Wang et al. 2005 and Minn et al 2007.\n#' @format `vdxs` is a dataset containing three matrices:\n#'   - data.vdxs: Matrix containing gene expressions as measured by Affymetrix hgu133a technology (single-channel, oligonucleotides)\n#'   - annot.vdxs: Matrix containing annotations of ffymetrix hgu133a microarray platform\n#'   - demo.vdxs: Clinical information of the breast cancer patients whose tumors were hybridized\n#' @details This dataset represent only partially the one published by Wang et al. 2005 and Minn et al 2007. Indeed only part of the patients (150) and gene expressions (966) are contained in `data.vdxs`.\n#' @source\n#' [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2034](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2034)\n#'\n#' [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5327](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5327)\n#' @references\n#' Y. Wang and J. G. Klijn and Y. Zhang and A. M. Sieuwerts and M. P. Look and F. Yang and D. Talantov and M. Timmermans and M. E. Meijer-van Gelder and J. Yu and T. Jatkoe and E. M. Berns and D. Atkins and J. A. Foekens (2005) \"Gene-Expression Profiles to Predict Distant Metastasis of Lymph-Node-Negative Primary Breast Cancer\", _Lancet_, *365*:671--679\n#'\n#' Minn, Andy J. and Gupta, Gaorav P. and Padua, David and Bos, Paula and Nguyen, Don X. and Nuyten, Dimitry and Kreike, Bas and Zhang, Yi and Wang, Yixin and Ishwaran, Hemant and Foekens, John A. and van de Vijver, Marc and Massague, Joan (2007) \"Lung metastasis genes couple breast tumor size and metastatic spread\", _Proceedings of the National Academy of Sciences_, *104*(16):6740--6745\n#' @keywords data\n#' @md\nNULL\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'claudinLowData' dataset in this code?",
        "answer": "The 'claudinLowData' dataset is used for training and testing the Claudin-Low classifier. It contains gene expression data for 807 features across 52 samples, along with class labels and other metadata. This data is used to identify and characterize the claudin-low intrinsic subtype of breast cancer."
      },
      {
        "question": "How many versions of the PAM50 model are provided in this code, and how do they differ?",
        "answer": "Three versions of the PAM50 model are provided: pam50, pam50.scale, and pam50.robust. They differ in the method used for standardizing gene expressions. pam50 uses no scaling, pam50.scale uses traditional scaling, and pam50.robust uses robust scaling. The robust version is recommended for datasets representative of a global population of breast cancer patients, as it has shown the best concordance with traditional clinical parameters."
      },
      {
        "question": "What is the structure of the 'sig.genius' dataset and what does it represent?",
        "answer": "The 'sig.genius' dataset is a list of three gene signatures that compose the Gene Expression progNostic Index Using Subtypes (GENIUS). These signatures, GENIUSM1, GENIUSM2, and GENIUSM3, correspond to the ER-/HER2-, HER2+, and ER+/HER2- subtype signatures respectively. This dataset represents a computational approach to improve breast cancer prognostication using fuzzy gene expression-based methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @name claudinLowData\n#'\n#' @title claudinLowData for use in the claudinLow classifier. Data generously provided by Aleix Prat.\n#'\n#' @description Training and Testing Data for use with the Claudin-Low Classifier\n#'\n#' @usage data(claudinLowData)\n#'\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#'\n#' @format\n#'  - xd: Matrix of 807 features and 52 samples\n#'  - classes: factor to split samples\n#'  - nfeatures: number of features\n#'  - nsamples: number of samples\n#'  - fnames: names of features\n#'  - snames: names of samples\n#'\n#' @references Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso [genefu::claudinLow()]\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL",
        "complete": "#' @name claudinLowData\n#'\n#' @title claudinLowData for use in the claudinLow classifier. Data generously provided by Aleix Prat.\n#'\n#' @description Training and Testing Data for use with the Claudin-Low Classifier\n#'\n#' @usage data(claudinLowData)\n#'\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#'\n#' @format\n#'  - xd: Matrix of 807 features and 52 samples\n#'  - classes: factor to split samples\n#'  - nfeatures: number of features\n#'  - nsamples: number of samples\n#'  - fnames: names of features\n#'  - snames: names of samples\n#'\n#' @references Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso [genefu::claudinLow()]\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL"
      },
      {
        "partial": "#' @name expos\n#'\n#' @aliases data.expos annot.expos demo.expos\n#'\n#' @title Gene expression, annotations and clinical data from the International Genomics Consortium\n#'\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data from the expO dataset collected by the International Genomics Consortium ([](http://www.intgen.org/expo/)).\n#'\n#' @format expos is a dataset containing three matrices\n#'   - data.expos: Matrix containing gene expressions as measured by Affymetrix hgu133plus2 technology (single-channel, oligonucleotides)\n#'   - annot.expos: Matrix containing annotations of ffymetrix hgu133plus2 microarray platform\n#'   - demo.expos: Clinical information of the breast cancer patients whose tumors were hybridized\n#'\n#' @source [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109)\n#'\n#' @references\n#' International Genomics Consortium, http://www.intgen.org/research-services/biobanking-experience/expo/\n#' McCall MN, Bolstad BM, Irizarry RA. (2010) \"Frozen robust multiarray analysis (fRMA)\", Biostatistics, 11(2):242-253.\n#'\n#' @usage data(expos)\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL",
        "complete": "#' @name expos\n#'\n#' @aliases data.expos annot.expos demo.expos\n#'\n#' @title Gene expression, annotations and clinical data from the International Genomics Consortium\n#'\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data from the expO dataset collected by the International Genomics Consortium ([](http://www.intgen.org/expo/)).\n#'\n#' @format expos is a dataset containing three matrices\n#'   - data.expos: Matrix containing gene expressions as measured by Affymetrix hgu133plus2 technology (single-channel, oligonucleotides)\n#'   - annot.expos: Matrix containing annotations of ffymetrix hgu133plus2 microarray platform\n#'   - demo.expos: Clinical information of the breast cancer patients whose tumors were hybridized\n#'\n#' @source [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109)\n#'\n#' @references\n#' International Genomics Consortium, http://www.intgen.org/research-services/biobanking-experience/expo/\n#' McCall MN, Bolstad BM, Irizarry RA. (2010) \"Frozen robust multiarray analysis (fRMA)\", Biostatistics, 11(2):242-253.\n#'\n#' @usage data(expos)\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/methods-subsetTo.R",
    "language": "R",
    "content": "#' @include ToxicoSet-accessors.R\nNULL\n\n#'`[`\n#'\n#' @examples\n#' tSet <- TGGATESsmall[sampleNames(TGGATESsmall), treatmentNames(TGGATESsmall)[seq_len(3)]]\n#'\n#'@param x tSet\n#'@param i Cell lines to keep in tSet\n#'@param j Drugs to keep in tSet\n#'@param ... further arguments\n#'@param drop A boolean flag of whether to drop single dimensions or not\n#'@return Returns the subsetted tSet\n#'@export\nsetMethod(`[`, \"ToxicoSet\", function(x, i, j, ..., drop = FALSE){\n    if(is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n    }\n    else if(is.numeric(i) && is.numeric(j) &&\n            (as.integer(i)==i) && (as.integer(j)==j)) {\n        return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n})\n\n#### subsetTo ####\n\n## FIXED? TODO:: Subset function breaks if it doesnt find cell line in sensitivity info\n#' A function to subset a ToxicoSet to data containing only specified drugs, cells and genes\n#'\n#' This is the prefered method of subsetting a ToxicoSet. This function allows\n#' abstraction of the data to the level of biologically relevant objects: drugs\n#' and cells. The function will automatically go through all of the\n#' combined data in the ToxicoSet and ensure only the requested radiations\n#' and cell lines are found in any of the slots. This allows quickly picking out\n#' all the experiments for a radiation or cell of interest, as well removes the need\n#' to keep track of all the metadata conventions between different datasets.\n#'\n#' @examples\n#' TGGATESDrugNames  <- treatmentNames(TGGATESsmall)\n#' TGGATESCells <- sampleNames(TGGATESsmall)\n#' tSet <- subsetTo(TGGATESsmall,drugs = TGGATESDrugNames[1],\n#'   cells = TGGATESCells[1], duration = \"2\")\n#'\n#' @param object A \\code{ToxicoSet} to be subsetted\n#' @param cell_lines A list or vector of cell names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all cells will be left in\n#'   the dataset.\n#' @param drugs A list or vector of drug names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all drugs will be left in\n#'   the dataset.\n#' @param features A list or vector of feature names as used in the dataset from\n#'   which the object will be subsetted. If left blank that all features will\n#'   be left in.\n#' @param molecular.data.cells A list or vector of cell names to keep in the\n#'   molecular data\n#' @param duration A \\code{list} or \\code{vector} of the experimental durations\n#'   to include in the subset as strings. Defaults to all durations if parameter\n#'   is not specified.\n#' @param ... Other arguments passed to other functions within the package\n#'\n#' @return A ToxicoSet with only the selected drugs and cells\n#'\n#' @importFrom CoreGx .unionList .message .warning .error\n#' @export\n## TODO:: Include dose parmater to subset on\nsubsetTo <- function(object, cell_lines = NULL,\n                     drugs=NULL,\n                     molecular.data.cells=NULL,\n                     duration=NULL,\n                     features=NULL,\n                     ...\n) {\n    ## TODO:: Remove this or add it to the function parameters?\n    drop = FALSE\n\n    ####\n    # PARSING ARGUMENTS\n    ####\n    adArgs = list(...)\n    if (\"exps\" %in% names(adArgs)) {\n        exps <- adArgs[[\"exps\"]]\n        if(is(exps, \"data.frame\")) {\n            exps2 <- exps[[name(object)]]\n            names(exps2) <- rownames(exps)\n            exps <- exps2\n        } else{\n            exps <- exps[[name(object)]]\n        }\n    }else {\n        exps <- NULL\n    }\n\n    if (\"dose\" %in% names(adArgs)) {\n        ## TODO:: Add subsetting on dose\n        stop(\"Due to the structure of tSets, subsetting on dose can only be done on\n            specific slots - not on the entire tSet\")\n    }\n\n    ## MISSING VALUE HANDLING FOR PARAMETERS\n    # Get named list of default values for missing parameters\n    argDefaultList <-\n        paramMissingHandler(funName = \"subsetTo\", tSet = object,\n                            drugs = drugs, cell_lines = cell_lines,\n                            features = features, duration = duration)\n    # Assign any missing parameter default values to function environment\n    if (length(argDefaultList) > 0) {\n        for (idx in seq_along(argDefaultList)) {\n            assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n        }\n    }\n\n    # ERROR HANDLING FOR PARAMETERS\n    paramErrorChecker(funName = \"subsetTo\", tSet = object,\n                      cell_lines = cell_lines,\n                      drugs = drugs, features = features,\n                      duration = duration)\n\n    ##TODO:: Add a value to tSet which indicates the experimental design!\n    ##FIXME:: Don't hard code object names!\n    if (name(object) %in% c(\"drugMatrix_rat\", \"EMEXP2458\")) {\n        if (!('DMSO' %in% drugs)) {\n            drugs <- c(drugs, 'DMSO')\n        }\n    }\n\n    ######\n    # SUBSETTING MOLECULAR PROFILES SLOT\n    ######\n    ### TODO:: implement strict subsetting at this level!!!!\n\n    ### the function missing does not work as expected in the context below, because the arguments are passed to the anonymous\n    ### function in lapply, so it does not recognize them as missing\n    molecularProfilesSlot(object) <-\n        lapply(molecularProfilesSlot(object),\n               function(SE, cell_lines, drugs, molecular.data.cells, duration, features){\n\n                   if (!is.null(features)) {\n                       SE <- SE[which(rownames(SummarizedExperiment::rowData(SE)) %in% features), ]\n                   }\n\n                   ##FIXME:: Why is are all these if conditions being checked against length? Just use grepl?\n                   molecular.data.type <-\n                       ifelse(\n                           length(grep(\"rna\", S4Vectors::metadata(SE)$annotation) > 0),\n                           \"rna\",\n                           S4Vectors::metadata(SE)$annotation\n                       )\n\n                   if (length(grep(molecular.data.type, names(molecular.data.cells))) > 0) {\n                       cell_lines <- molecular.data.cells[[molecular.data.type]]\n                   }\n                   column_indices <- NULL\n\n                   if (length(cell_lines) == 0 && length(drugs) == 0) {\n                       column_indices <- seq_len(ncol(SE))\n                   }\n                   if (length(cell_lines) == 0 && datasetType(object) == \"sensitivity\") {\n                       column_indices <- seq_len(ncol(SE))\n                   }\n\n                   # Selecting indices which match the cells argument\n                   cell_line_index <- NULL\n                   if (length(cell_lines) != 0) {\n                       if (!all(cell_lines %in% sampleNames(object))) {\n                           stop(\"Some of the cell names passed to function did not match to names\n          in the ToxicoSet. Please ensure you are using cell names as\n          returned by the cellNames function\")\n                       }\n                       cell_line_index <- which(SummarizedExperiment::colData(SE)[[\"sampleid\"]] %in% cell_lines)\n                   }\n\n                   # Selecting indexes which match drugs arguement\n                   drugs_index <- NULL\n                   if (datasetType(object) == \"perturbation\" || datasetType(object) == \"both\") {\n                       if (length(drugs) != 0) {\n                           if (!all(drugs %in% treatmentNames(object))){\n                               stop(\"Some of the drug names passed to function did not match to names in the ToxicoSet Please ensure you are using drug names as returned by the drugNames function\")\n                           }\n                           drugs_index <- which(SummarizedExperiment::colData(SE)[[\"treatmentid\"]] %in% drugs)\n                       }\n                   }\n\n                   if (length(drugs_index) != 0 && length(cell_line_index) != 0) {\n                       if (length(intersect(drugs_index, cell_line_index)) == 0) {\n                           stop(\"This Drug - Cell Line combination was not tested together.\")\n                       }\n                       column_indices <- intersect(drugs_index, cell_line_index)\n                   } else {\n                       if (length(drugs_index) != 0) {\n                           column_indices <- drugs_index\n                       }\n                       if (length(cell_line_index) != 0) {\n                           column_indices <- cell_line_index\n                       }\n                   }\n\n                   # LOGIC TO SUBSET BASED ON DURATION\n                   ## TODO:: Determine if this works for other SE data types\n                   if (!is.null(duration)){\n                       if (all(!(duration %in% unique(SummarizedExperiment::colData(SE[, column_indices])$duration)))) {\n                           # Error when other parameters are passed in\n                           if ( !is.null(cell_lines) | !is.null(drugs) | !is.null(molecular.data.cells)) {\n                               stop(paste0(\n                                   \"There are no molecular profiles with duration of \",\n                                   duration, \" in the tSet with the selected parameters.\"\n                               ))\n                           } else { # Error when no other parameters are passed in\n                               stop(paste0(\n                                   \"There are no molecular profiles with duration of \",\n                                   duration, \" in the tSet.\"\n                               ))\n                           }\n                       }\n                       duration_indices <- which(SummarizedExperiment::colData(SE)$duration %in% duration)\n                       column_indices <- intersect(column_indices, duration_indices)\n                   }\n\n                   row_indices <- seq_len(nrow(SummarizedExperiment::assay(SE, 1)))\n\n                   # Final SE\n                   SE <- SE[row_indices, column_indices]\n                   return(SE)\n\n               }, cell_lines = cell_lines,\n               drugs = drugs,\n               molecular.data.cells = molecular.data.cells,\n               duration = duration,\n               features = features)\n\n\n    ######\n    # SUBSET SENSITIVITY SLOT\n    ######\n    # Logic if any \"...\" arguments are passed to subsetTo\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") & length(exps) != 0) {\n        sensitivityInfo(object) <- sensitivityInfo(object)[exps, , drop=drop]\n        rownames(sensitivityInfo(object)) <- names(exps)\n        if (length(sensitivityRaw(object)) > 0) {\n            sensitivityRaw(object) <- sensitivityRaw(object)[exps, , , drop=drop]\n            dimnames(sensitivityRaw(object))[[1]] <- names(exps)\n        }\n        sensitivityProfiles(object) <- sensitivityProfiles(object)[exps, , drop=drop]\n        rownames(sensitivityProfiles(object)) <- names(exps)\n\n        sensNumber(object) <- .summarizeSensitivityNumbers(object)\n    }\n    # Logic if drug or cell parameters are passed to subsetTo\n    else if (\n        (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") &\n        (length(drugs) != 0 | length(cell_lines) != 0 | !is.null(duration) )\n    ) {\n\n        drugs_index <- which(sensitivityInfo(object)[, \"treatmentid\"] %in% drugs)\n        cell_line_index <- which(sensitivityInfo(object)[,\"sampleid\"] %in% cell_lines)\n        if (length(drugs_index) !=0 & length(cell_line_index) !=0 ) {\n            if (length(intersect(drugs_index, cell_line_index)) == 0) {\n                stop(\"This Drug - Cell Line combination was not tested together.\")\n            }\n            row_indices <- intersect(drugs_index, cell_line_index)\n        } else {\n            if(length(drugs_index)!=0 & length(cell_lines)==0) {\n                row_indices <- drugs_index\n            } else {\n                if(length(cell_line_index)!=0 & length(drugs)==0){\n                    row_indices <- cell_line_index\n                } else {\n                    # Includes all rows if cell or drug arguments are absent\n                    row_indices <- seq_len(nrow(sensitivityInfo(object)))\n                }\n            }\n        }\n        # LOGIC TO SUBSET BASED ON DURATION\n        if(!is.null(duration)){\n            if(all(!(duration %in% unique(sensitivityInfo(object)[row_indices,]$duration_h)))) {\n                # Error when other parameters are passed in\n                if(!is.null(cell_lines) | !is.null(drugs) | !is.null(molecular.data.cells)) {\n                    stop(paste0(\n                        ## TODO:: Is sample the correct way to refer to one treatment/duration combination in TGx experiments?\n                        \"There are no samples with duration of \",\n                        duration, \" in the tSet with the selected parameters.\"\n                    ))\n                } else { # Error when no other parameters are passed in\n                    stop(paste0(\n                        \"There are no samples with duration of \",\n                        duration, \" in the tSet\"\n                    ))\n                }\n            }\n            duration_indices <- which(sensitivityInfo(object)$duration_h %in% duration)\n            row_indices <- intersect(row_indices, duration_indices)\n        }\n        sensItemNames <- names(treatmentResponse(object))\n        sensitivityVals <-\n            lapply(sensItemNames, function(sensItemName, drop){\n                if (sensItemName == \"n\") {\n                    sensItem <- treatmentResponse(object)[[sensItemName]]\n                    if (!is.null(cell_lines)) {\n                        sensItem[which(rownames(sensItem) %in% cell_lines),\n                                 which(colnames(sensItem) %in% drugs), drop = drop]\n                    } else {\n                        sensItem[ , which(colnames(sensItem) %in% drugs), drop = drop]\n                    }\n                } else {\n                    sensItem <- treatmentResponse(object)[[sensItemName]]\n                    if (length(dim(sensItem)) == 3) {\n                        sensItem[row_indices, , , drop = drop]\n                    } else {\n                        sensItem[row_indices, , drop = drop]\n                    }\n                }\n            }, drop = drop)\n        names(sensitivityVals) <- sensItemNames\n        treatmentResponse(object) <- sensitivityVals\n    }\n\n    #####\n    # SUBSET DRUG SLOT\n    #####\n    if (length(drugs) == 0) {\n        if (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\"){\n            drugs <- unique(sensitivityInfo(object)[[\"treatmentid\"]])\n        }\n        if(datasetType(object) == \"perturbation\" | datasetType(object) == \"both\"){\n            drugs <- union(drugs, na.omit(.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(SummarizedExperiment::colData(SE)[[\"treatmentid\"]])}))))\n        }\n    }\n\n    #####\n    # SUBSET CELLS SLOT\n    #####\n    if (length(cell_lines) == 0) {\n        cell_lines <- union(cell_lines, na.omit(.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(SummarizedExperiment::colData(SE)[[\"sampleid\"]])}))))\n        if (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\"){\n            cell_lines <- union(cell_lines, sensitivityInfo(object)[[\"sampleid\"]])\n        }\n    }\n    #####\n    # ASSIGN SUBSETS BACK TO TOXICOSET OBJECT\n    #####\n    treatmentInfo(object) <- treatmentInfo(object)[drugs , , drop=drop]\n    sampleInfo(object) <- sampleInfo(object)[cell_lines , , drop=drop]\n    curation(object)$treatment <- curation(object)$treatment[drugs , , drop=drop]\n    curation(object)$sample <- curation(object)$sample[cell_lines , , drop=drop]\n    curation(object)$tissue <- curation(object)$tissue[cell_lines , , drop=drop]\n    return(object)\n}\n\n#\n# END SUBSET TO FUNCTION\n#\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `subsetTo` function in the given code snippet?",
        "answer": "The `subsetTo` function is designed to subset a ToxicoSet object to data containing only specified drugs, cells, and genes. It allows for abstraction of the data to the level of biologically relevant objects (drugs and cells) and automatically ensures that only the requested treatments and cell lines are found in all slots of the ToxicoSet. This function is the preferred method of subsetting a ToxicoSet, as it simplifies the process of selecting experiments for specific treatments or cells of interest."
      },
      {
        "question": "How does the `subsetTo` function handle missing parameters?",
        "answer": "The `subsetTo` function uses a `paramMissingHandler` function to handle missing parameters. It creates a named list of default values for any missing parameters. If there are any missing parameters, their default values are assigned to the function environment using a loop. This approach allows the function to work with partial input while providing sensible defaults for unspecified parameters."
      },
      {
        "question": "What is the purpose of the `[` method defined for the ToxicoSet class in this code snippet?",
        "answer": "The `[` method defined for the ToxicoSet class provides a convenient way to subset a ToxicoSet object using either character or numeric indices. If character indices are provided for both cells and drugs, it calls the `subsetTo` function with those parameters. If numeric indices are provided (and they are integers), it translates those indices to the corresponding cell and drug names before calling `subsetTo`. This method allows for more intuitive subsetting of ToxicoSet objects, similar to how one might subset a matrix or data frame."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(`[`, \"ToxicoSet\", function(x, i, j, ..., drop = FALSE){\n    if(is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n    }\n    else if(is.numeric(i) && is.numeric(j) &&\n            (as.integer(i)==i) && (as.integer(j)==j)) {\n        return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n})",
        "complete": "setMethod(`[`, \"ToxicoSet\", function(x, i, j, ..., drop = FALSE){\n    if(is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n    }\n    else if(is.numeric(i) && is.numeric(j) &&\n            (as.integer(i)==i) && (as.integer(j)==j)) {\n        return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n    else {\n        stop(\"Invalid input types for subsetting ToxicoSet\")\n    }\n})"
      },
      {
        "partial": "subsetTo <- function(object, cell_lines = NULL, drugs = NULL, molecular.data.cells = NULL,\n                     duration = NULL, features = NULL, ...) {\n    # Parameter handling and error checking\n    argDefaultList <- paramMissingHandler(funName = \"subsetTo\", tSet = object,\n                                         drugs = drugs, cell_lines = cell_lines,\n                                         features = features, duration = duration)\n    if (length(argDefaultList) > 0) {\n        for (idx in seq_along(argDefaultList)) {\n            assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n        }\n    }\n    paramErrorChecker(funName = \"subsetTo\", tSet = object, cell_lines = cell_lines,\n                      drugs = drugs, features = features, duration = duration)\n\n    # Subset molecular profiles\n    molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object),\n        function(SE, cell_lines, drugs, molecular.data.cells, duration, features) {\n            # Subset logic here\n        }, cell_lines = cell_lines, drugs = drugs, molecular.data.cells = molecular.data.cells,\n           duration = duration, features = features)\n\n    # Subset sensitivity slot\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") &&\n        (length(drugs) != 0 | length(cell_lines) != 0 | !is.null(duration))) {\n        # Sensitivity subsetting logic here\n    }\n\n    # Subset drug and cell slots\n    # Logic for subsetting drug and cell slots here\n\n    return(object)\n}",
        "complete": "subsetTo <- function(object, cell_lines = NULL, drugs = NULL, molecular.data.cells = NULL,\n                     duration = NULL, features = NULL, ...) {\n    # Parameter handling and error checking\n    argDefaultList <- paramMissingHandler(funName = \"subsetTo\", tSet = object,\n                                         drugs = drugs, cell_lines = cell_lines,\n                                         features = features, duration = duration)\n    if (length(argDefaultList) > 0) {\n        for (idx in seq_along(argDefaultList)) {\n            assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n        }\n    }\n    paramErrorChecker(funName = \"subsetTo\", tSet = object, cell_lines = cell_lines,\n                      drugs = drugs, features = features, duration = duration)\n\n    # Subset molecular profiles\n    molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object),\n        function(SE, cell_lines, drugs, molecular.data.cells, duration, features) {\n            if (!is.null(features)) {\n                SE <- SE[which(rownames(SummarizedExperiment::rowData(SE)) %in% features), ]\n            }\n            column_indices <- getColumnIndices(SE, cell_lines, drugs, molecular.data.cells, duration)\n            SE <- SE[, column_indices]\n            return(SE)\n        }, cell_lines = cell_lines, drugs = drugs, molecular.data.cells = molecular.data.cells,\n           duration = duration, features = features)\n\n    # Subset sensitivity slot\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") &&\n        (length(drugs) != 0 | length(cell_lines) != 0 | !is.null(duration))) {\n        row_indices <- getSensitivityIndices(object, drugs, cell_lines, duration)\n        treatmentResponse(object) <- subsetSensitivityData(object, row_indices)\n    }\n\n    # Subset drug and cell slots\n    drugs <- if (length(drugs) == 0) getUniqueDrugs(object) else drugs\n    cell_lines <- if (length(cell_lines) == 0) getUniqueCells(object) else cell_lines\n\n    # Update object slots\n    treatmentInfo(object) <- treatmentInfo(object)[drugs, , drop = FALSE]\n    sampleInfo(object) <- sampleInfo(object)[cell_lines, , drop = FALSE]\n    curation(object)$treatment <- curation(object)$treatment[drugs, , drop = FALSE]\n    curation(object)$sample <- curation(object)$sample[cell_lines, , drop = FALSE]\n    curation(object)$tissue <- curation(object)$tissue[cell_lines, , drop = FALSE]\n\n    return(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeICn.R",
    "language": "R",
    "content": "#' Computes the ICn for any n in 0-100 for a Drug Dose Viability Curve\n#'\n#' Returns the ICn for any given nth percentile when given concentration and viability as input, normalized by the concentration\n#' range of the experiment. A Hill Slope is first fit to the data, and the ICn is inferred from the fitted curve. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known.\n#'\n#' @examples\n#' dose <- c(\"0.0025\",\"0.008\",\"0.025\",\"0.08\",\"0.25\",\"0.8\",\"2.53\",\"8\")\n#' viability <- c(\"108.67\",\"111\",\"102.16\",\"100.27\",\"90\",\"87\",\"74\",\"57\")\n#' computeIC50(dose, viability)\n#' computeICn(dose, viability, n=10)\n#'\n#' @param concentration `vector` is a vector of drug concentrations.\n#' @param viability `vector` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param Hill_fit `list or vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param n `numeric` The percentile concentration to compute. If viability_as_pct set, assumed to be percentage, otherwise\n#' assumed to be a decimal value.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(ICn) should be returned instead of ICn.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf passed in as decimal.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return a numeric value for the concentration of the nth precentile viability reduction\n#' @export\ncomputeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n\n    Hill_fit <- logLogisticRegression(conc = concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration,\n                               Hill_fit=Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  } else if (!missing(Hill_fit)){\n\n    cleanData <- sanitizeInput(conc = concentration,\n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n  } else {\n\n    stop(\"Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.\")\n\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n\n\n  n <- 1 - n\n\n  if (n < pars[2] || n > 1) {\n    return(NA_real_)\n  } else if (n == pars[2]) {\n\n    return(Inf)\n\n  } else if (n == 1) {\n\n    return(ifelse(conc_as_log, -Inf, 0))\n\n  } else {\n\n    return(ifelse(conc_as_log,\n                  log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n                  10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n\n  }\n\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeICn` function and what are its main input parameters?",
        "answer": "The `computeICn` function computes the ICn (Inhibitory Concentration) for any nth percentile of a Drug Dose Viability Curve. It takes the following main input parameters: `concentration` (a vector of drug concentrations), `viability` (a vector of corresponding viability values), `Hill_fit` (optional, parameters of a Hill Slope), `n` (the percentile concentration to compute), and several boolean flags for input format and processing options."
      },
      {
        "question": "How does the function handle different input formats for concentration and viability data?",
        "answer": "The function uses boolean flags to handle different input formats: `conc_as_log` determines if the concentration data is in log10 format or not, `viability_as_pct` specifies if viability is given as a percentage or decimal, and `trunc` indicates whether to truncate viability data between 0 and 1. These flags allow the function to adapt to various input formats and ensure correct processing of the data."
      },
      {
        "question": "What is the significance of the Hill Slope parameters in this function, and how are they obtained if not provided?",
        "answer": "The Hill Slope parameters are crucial for calculating the ICn value. If not provided directly through the `Hill_fit` parameter, the function calls `logLogisticRegression` to fit a Hill Slope to the given concentration and viability data. The Hill Slope parameters describe the shape of the dose-response curve and are used in the final calculation of the ICn value. This allows the function to work with either raw data or pre-calculated Hill Slope parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration,\n                               Hill_fit=Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration,\n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  # Complete the function from here\n}",
        "complete": "computeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration,\n                               Hill_fit=Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration,\n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  if(viability_as_pct) n <- n/100\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) return(NA_real_)\n  if (n == pars[2]) return(Inf)\n  if (n == 1) return(ifelse(conc_as_log, -Inf, 0))\n  \n  return(ifelse(conc_as_log,\n                 log10(10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1])),\n                 10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1])))\n}"
      },
      {
        "partial": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  # Complete the function from here\n}",
        "complete": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  if(viability_as_pct) n <- n/100\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) return(NA_real_)\n  if (n == pars[2]) return(Inf)\n  if (n == 1) return(ifelse(conc_as_log, -Inf, 0))\n  \n  ifelse(conc_as_log,\n         log10(10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1])),\n         10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1]))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/columnsAsCharacter.R",
    "language": "R",
    "content": "# Change all factor columns of tSet data.frames to character\n#\n#\n# @importFrom maggitr %<>%\n# @importFrom dplyr mutate_if\ncolumnsAsCharacter <- function(tSet) {\n    for (mDataType in names(molecularProfilesSlot(tSet))) {\n        phenoInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n\n        featureInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    if (length(sensitivityInfo(tSet)) > 0) {\n        sensitivityInfo(tSet) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    sampleInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    treatmentInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    return(tSet)\n}\n\n## TODO:: Determine how to use list.filter to recursively return all data.frames in a nested lsit\n## TODO:: Write a recursive functions to return all non-list elements in a nested list\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `columnsAsCharacter` function in the given code snippet?",
        "answer": "The main purpose of the `columnsAsCharacter` function is to convert all factor columns in various data frames within a `tSet` object to character type. It iterates through different components of the `tSet` (molecular profiles, sensitivity info, sample info, and treatment info) and applies the conversion to each."
      },
      {
        "question": "What R packages and functions are being used in this code snippet, and what are their roles?",
        "answer": "The code uses functions from several R packages: 1) `maggitr` for the `%<>%` operator (pipe and assignment), 2) `dplyr` for `mutate_if` to conditionally mutate columns, 3) `tibble` for `rownames_to_column` and `column_to_rownames` to manipulate row names. These functions are used to efficiently manipulate data frames, convert factor columns to character, and handle row names during the process."
      },
      {
        "question": "How does the function handle the case where `sensitivityInfo(tSet)` is empty?",
        "answer": "The function checks the length of `sensitivityInfo(tSet)` before processing it. If the length is greater than 0, it applies the factor-to-character conversion. If `sensitivityInfo(tSet)` is empty, this step is skipped. This conditional processing prevents errors when working with `tSet` objects that may not have sensitivity information."
      }
    ],
    "completion_tasks": [
      {
        "partial": "columnsAsCharacter <- function(tSet) {\n    for (mDataType in names(molecularProfilesSlot(tSet))) {\n        phenoInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n\n        featureInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    # Complete the function by adding code to handle sensitivityInfo, sampleInfo, and treatmentInfo\n    # Return the modified tSet\n}",
        "complete": "columnsAsCharacter <- function(tSet) {\n    for (mDataType in names(molecularProfilesSlot(tSet))) {\n        phenoInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n\n        featureInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    if (length(sensitivityInfo(tSet)) > 0) {\n        sensitivityInfo(tSet) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    sampleInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    treatmentInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    return(tSet)\n}"
      },
      {
        "partial": "# Function to recursively process nested lists\nprocessNestedList <- function(lst) {\n    # Implement the recursive function to process nested lists\n    # Convert factors to characters in data frames\n    # Return the processed list\n}",
        "complete": "# Function to recursively process nested lists\nprocessNestedList <- function(lst) {\n    if (is.data.frame(lst)) {\n        return(dplyr::mutate_if(lst, is.factor, as.character))\n    } else if (is.list(lst)) {\n        return(lapply(lst, processNestedList))\n    } else {\n        return(lst)\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/SpearmanCI.R",
    "language": "R",
    "content": "#' @title Function to compute the confidence interval for the Spearman \n#'   correelation coefficient\n#'\n#' @description\n#' This function enables to compute the confidence interval for the Spearman \n#'   correelation coefficient using the Fischer Z transformation.\n#'\n#' @usage\n#' spearmanCI(x, n, alpha = 0.05)\n#'\n#' @param x\tSpearman correlation coefficient rho.\n#' @param n\tthe sample size used to compute the Spearman rho.\n#' @param alpha\talpha level for confidence interval.\n#'\n#' @return\n#' A vector containing the lower, upper values for the confidence interval \n#'   and p-value for Spearman rho\n#'\n#' @examples\n#' spearmanCI(x=0.2, n=100, alpha=0.05)\n#'\n#' @md\n#' @export\nspearmanCI <- \nfunction (x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- zz - qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    ll <- survcomp::fisherz(ll / sqrt((n-3)/1.06), inv=TRUE)\n    uu <- zz + qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    uu <- survcomp::fisherz(uu / sqrt((n-3)/1.06), inv=TRUE)\n    pp <- pnorm(q=zz, lower.tail=x < 0)\n    res <- c(\"lower\"=ll, \"upper\"=uu, \"p.value\"=pp)\n    return(res)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `spearmanCI` function and what are its main input parameters?",
        "answer": "The `spearmanCI` function computes the confidence interval for the Spearman correlation coefficient using the Fischer Z transformation. Its main input parameters are: `x` (the Spearman correlation coefficient rho), `n` (the sample size used to compute the Spearman rho), and `alpha` (the alpha level for the confidence interval, defaulting to 0.05)."
      },
      {
        "question": "How does the function calculate the lower and upper bounds of the confidence interval?",
        "answer": "The function calculates the lower and upper bounds of the confidence interval using the following steps: 1) It applies the Fisher Z transformation to the input correlation coefficient. 2) It calculates the standard error of the transformed value. 3) It computes the lower and upper bounds using the normal distribution quantile function (`qnorm`). 4) Finally, it applies the inverse Fisher Z transformation to convert the bounds back to the correlation scale."
      },
      {
        "question": "What does the function return, and how is the p-value calculated?",
        "answer": "The function returns a vector containing three values: the lower bound of the confidence interval, the upper bound of the confidence interval, and the p-value for the Spearman rho. The p-value is calculated using the `pnorm` function, which computes the cumulative probability of the standard normal distribution. The direction of the test (lower.tail argument) is determined by whether the input correlation coefficient is negative or positive."
      }
    ],
    "completion_tasks": [
      {
        "partial": "spearmanCI <- function(x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- zz - qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    ll <- survcomp::fisherz(ll / sqrt((n-3)/1.06), inv=TRUE)\n    # Complete the function by calculating uu and pp\n    # Then return the result as a vector\n}",
        "complete": "spearmanCI <- function(x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- zz - qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    ll <- survcomp::fisherz(ll / sqrt((n-3)/1.06), inv=TRUE)\n    uu <- zz + qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    uu <- survcomp::fisherz(uu / sqrt((n-3)/1.06), inv=TRUE)\n    pp <- pnorm(q=zz, lower.tail=x < 0)\n    c(lower=ll, upper=uu, p.value=pp)\n}"
      },
      {
        "partial": "spearmanCI <- function(x, n, alpha=0.05) {\n    # Calculate zz and zz.se\n    # Then compute ll, uu, and pp\n    # Finally, return the result as a named vector\n}",
        "complete": "spearmanCI <- function(x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- survcomp::fisherz((zz - qnorm(1-alpha/2) * zz.se) / sqrt((n-3)/1.06), inv=TRUE)\n    uu <- survcomp::fisherz((zz + qnorm(1-alpha/2) * zz.se) / sqrt((n-3)/1.06), inv=TRUE)\n    pp <- pnorm(zz, lower.tail=x < 0)\n    c(lower=ll, upper=uu, p.value=pp)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/rorS.R",
    "language": "R",
    "content": "#' @title Function to compute the rorS signature as published by Parker\n#'   et al 2009\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene\n#'   expression values following the algorithm used for the rorS signature as\n#'   published by Parker et al 2009.\n#'\n#' @usage\n#' rorS(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and\n#'   probes in columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping\tTRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise. Note that for Affymetrix HGU datasets, the\n#'   mapping is not necessary.\n#' @param mapping\tMatrix with columns \"EntrezGene.ID\" and \"probe\" used to\n#'   force the mapping such that the probes are not selected based on their\n#'   variance.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwis.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung,\n#'   Samuel and Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron,\n#'   Christiane and He, Xiaping and Hu, Zhiyuan and Quackenbush, John F. and\n#'   Stijleman, Inge J. and Palazzo, Juan and Marron, J.S. and Nobel,\n#'   Andrew B. and Mardis, Elaine and Nielsen, Torsten O. and Ellis,\n#'   Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009) \"Supervised\n#'   Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\", Journal of\n#'   Clinical Oncology, 27(8):1160-1167\n#'\n#' @examples\n#' # load NKI dataset\n#' data(vdxs)\n#' data(pam50)\n#'\n#' # compute relapse score\n#' rs.vdxs <- rorS(data=data.vdxs, annot=annot.vdxs, do.mapping=TRUE)\n#'\n#' @md\n#' @export\n#' @name rorS\nrorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n  ## PAM50 classification\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  ## ROR-S\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  ## rescale between 0 and 100\n  rs <- (rs.unscaled - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) / (quantile(rs.unscaled, probs=0.975, na.rm=TRUE) - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) * 100\n  rs[!is.na(rs) & rs < 0] <- 0\n  rs[!is.na(rs) & rs > 100] <- 100\n  rsrisk[rs < 29] <- \"Low\"\n  rsrisk[rs >= 29 & rs < 53] <- \"Intermediate\"\n  rsrisk[rs >= 53] <- \"High\"\n  rsrisk <- factor(rsrisk, levels=c(\"Low\", \"Intermediate\", \"High\"))\n\n\treturn(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=sbts$centroids.map))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'rorS' function and what does it return?",
        "answer": "The 'rorS' function computes the rorS signature scores and risk classifications from gene expression values, as published by Parker et al. 2009. It returns a list containing: 'score' (continuous signature scores), 'risk' (binary risk classification), 'mapping' (mapping used if necessary), and 'probe' (correspondence between gene list and gene expression data if mapping is performed)."
      },
      {
        "question": "How does the function handle the rescaling of the risk scores?",
        "answer": "The function rescales the risk scores between 0 and 100 using the following steps: 1) It calculates the 2.5th and 97.5th percentiles of the unscaled scores. 2) It applies the formula: (unscaled_score - 2.5th_percentile) / (97.5th_percentile - 2.5th_percentile) * 100. 3) Any resulting scores below 0 are set to 0, and any scores above 100 are set to 100."
      },
      {
        "question": "How are the risk categories determined in the 'rorS' function?",
        "answer": "The risk categories are determined based on the rescaled risk scores as follows: 1) Scores below 29 are classified as 'Low' risk. 2) Scores between 29 (inclusive) and 53 are classified as 'Intermediate' risk. 3) Scores of 53 and above are classified as 'High' risk. The risk categories are then converted to a factor with levels 'Low', 'Intermediate', and 'High'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  # Complete the function by rescaling rs.unscaled and setting rsrisk\n}",
        "complete": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  rs <- (rs.unscaled - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) / (quantile(rs.unscaled, probs=0.975, na.rm=TRUE) - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) * 100\n  rs[!is.na(rs) & rs < 0] <- 0\n  rs[!is.na(rs) & rs > 100] <- 100\n  rsrisk[rs < 29] <- \"Low\"\n  rsrisk[rs >= 29 & rs < 53] <- \"Intermediate\"\n  rsrisk[rs >= 53] <- \"High\"\n  rsrisk <- factor(rsrisk, levels=c(\"Low\", \"Intermediate\", \"High\"))\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=sbts$centroids.map))\n}"
      },
      {
        "partial": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  # Load pam50 data if not exists\n  # Predict intrinsic clusters\n  # Calculate mymapping\n  # Initialize rs.unscaled, rs, and rsrisk\n  # Calculate rst\n  # Assign rst to rs.unscaled\n  # Rescale rs between 0 and 100\n  # Assign risk categories\n  # Return results\n}",
        "complete": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  rs <- (rs.unscaled - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) / (quantile(rs.unscaled, probs=0.975, na.rm=TRUE) - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) * 100\n  rs[!is.na(rs) & rs < 0] <- 0\n  rs[!is.na(rs) & rs > 100] <- 100\n  rsrisk[rs < 29] <- \"Low\"\n  rsrisk[rs >= 29 & rs < 53] <- \"Intermediate\"\n  rsrisk[rs >= 53] <- \"High\"\n  rsrisk <- factor(rsrisk, levels=c(\"Low\", \"Intermediate\", \"High\"))\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=sbts$centroids.map))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/km.coxph.plot.R",
    "language": "R",
    "content": "'km.coxph.plot' <-\nfunction(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n\n\tif (missing(sub.title)) { sub.title <- NULL }\n\tif (missing(leg.text)) { leg.text <- NULL }\n  if (missing(weight.s)) { weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s))) }\n  ## weights should be > 0\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"weight.s\", weight.s, envir = envir)\n  weighted <- length(sort(unique(weight.s))) > 1\n\n\tng <- length(leg.text)\n    old.mar <- par(\"mar\")\n    on.exit( par( mar = old.mar ) )\n    .xaxt=\"s\"\n    .xlab=x.label\n    if (show.n.risk) {\n        par(mar = old.mar + c(ng,8,3,0))\n        .xaxt=\"n\"\n        .xlab = \"\"\n    }\n\n    plot(survfit(formula.s, data=data.s, weights=weight.s), xaxt=.xaxt, col=.col, lty=.lty, lwd=.lwd, xlab=.xlab, ylab=y.label, ... )\n    title(main.title)\n\n    if (!missing(v.line) && !is.null(v.line)) { abline(v=v.line, lty=3, col=\"purple\") }\n    if (!missing(h.line) && !is.null(h.line)) { abline(h=h.line, lty=3, col=\"purple\") }\n\n    if (!is.null(leg.text)) { legend(x=leg.pos, xjust=0, yjust=1, legend=leg.text, col=.col, lty=.lty, lwd=.lwd, cex=0.9, bg=\"white\", inset=leg.inset, bty=leg.bty) }\n    if (!is.null(sub.title)) { mtext(sub.title, line=-4, outer=TRUE) }\n    if (missing(o.text)) {\n\t\t  sdf <- summary(survival::coxph(formula.s, data=data.s, weights=weight.s))\n\t    if(verbose) { print(sdf) }\n        p.val <- sdf$sctest[\"pvalue\"]\n        o.text <- sprintf(\"Logrank P = %.1E\", p.val)\n    }\n    if (is.null(o.text)) { o.text <- FALSE }\n    text(0,0, o.text, cex=0.85, pos=4)\n\n    if (show.n.risk) {\n        usr.xy <- par( \"usr\" )\n        nrisk <- no.at.risk(formula.s=formula.s, data.s=data.s, sub.s=\"all\", t.step=n.risk.step, t.end=floor(usr.xy[2]) )\n        at.loc <- seq(0, usr.xy[2], n.risk.step)\n        axis(1, at=at.loc)\n        mtext(x.label, side=1, line=2)\n        mtext(\"No. At Risk\", side=1, line=3, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex, font=2)\n        #nrsk.lbs <- sapply( strsplit(levels(nrisk[,1]),\"=\"), FUN=function(x) x[2] )\n        #if( any(is.na(nrsk.lbs)) ) nrsk.lbs <- leg.text\n        for( i in 1:nrow(nrisk) ) {\n            mtext(leg.text[i], side=1, line=3+i, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex)\n            mtext(nrisk[i,-1], side=1, at=at.loc, line=3+i, adj=1, cex=n.risk.cex)\n       }\n    }\n\n    if( exists(\"weight.s\", envir=.GlobalEnv) ) remove(\"weight.s\", envir=.GlobalEnv)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'km.coxph.plot' function and what are its main components?",
        "answer": "The 'km.coxph.plot' function is designed to create a Kaplan-Meier survival plot with Cox proportional hazards model analysis. Its main components include:\n1. Plotting the survival curve using survfit()\n2. Adding title, labels, and legend\n3. Displaying the logrank p-value\n4. Optionally showing the number of subjects at risk\n5. Handling weighted data\n6. Customizing plot appearance (colors, line types, etc.)"
      },
      {
        "question": "How does the function handle weighted data, and what precautions are taken regarding weights?",
        "answer": "The function handles weighted data by:\n1. Accepting a 'weight.s' parameter for weights\n2. Filtering out data points with NA or non-positive weights\n3. Assigning weights to the environment\n4. Checking if the data is weighted by comparing unique weight values\n5. Using weights in the survfit() and coxph() functions\n\nPrecautions:\n- Weights must be positive\n- NA weights are removed along with corresponding data points\n- The function cleans up the 'weight.s' variable from the global environment after use"
      },
      {
        "question": "How does the function implement the 'show.n.risk' feature, and what adjustments are made to the plot layout to accommodate this?",
        "answer": "The 'show.n.risk' feature is implemented as follows:\n1. If enabled, the plot margins are adjusted to make room for the risk information\n2. The x-axis is initially hidden, and the x-label is cleared\n3. After the main plot, the function calculates the number at risk using no.at.risk()\n4. It then adds a new x-axis with custom tick locations\n5. Labels for each group and their corresponding risk numbers are added below the plot\n\nAdjustments to the plot layout:\n- Margins are increased using par(mar = old.mar + c(ng,8,3,0))\n- The x-axis label is moved to a lower position\n- Additional lines of text are added below the x-axis for the risk information"
      }
    ],
    "completion_tasks": [
      {
        "partial": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  if (missing(sub.title)) { sub.title <- NULL }\n  if (missing(leg.text)) { leg.text <- NULL }\n  if (missing(weight.s)) { weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s))) }\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"weight.s\", weight.s, envir = envir)\n  weighted <- length(sort(unique(weight.s))) > 1\n\n  ng <- length(leg.text)\n  old.mar <- par(\"mar\")\n  on.exit( par( mar = old.mar ) )\n  .xaxt=\"s\"\n  .xlab=x.label\n  if (show.n.risk) {\n    par(mar = old.mar + c(ng,8,3,0))\n    .xaxt=\"n\"\n    .xlab = \"\"\n  }\n\n  # TODO: Complete the plot function and add additional features\n\n}",
        "complete": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  if (missing(sub.title)) { sub.title <- NULL }\n  if (missing(leg.text)) { leg.text <- NULL }\n  if (missing(weight.s)) { weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s))) }\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"weight.s\", weight.s, envir = envir)\n  weighted <- length(sort(unique(weight.s))) > 1\n\n  ng <- length(leg.text)\n  old.mar <- par(\"mar\")\n  on.exit( par( mar = old.mar ) )\n  .xaxt=\"s\"\n  .xlab=x.label\n  if (show.n.risk) {\n    par(mar = old.mar + c(ng,8,3,0))\n    .xaxt=\"n\"\n    .xlab = \"\"\n  }\n\n  plot(survfit(formula.s, data=data.s, weights=weight.s), xaxt=.xaxt, col=.col, lty=.lty, lwd=.lwd, xlab=.xlab, ylab=y.label, ... )\n  title(main.title)\n\n  if (!missing(v.line) && !is.null(v.line)) { abline(v=v.line, lty=3, col=\"purple\") }\n  if (!missing(h.line) && !is.null(h.line)) { abline(h=h.line, lty=3, col=\"purple\") }\n\n  if (!is.null(leg.text)) { legend(x=leg.pos, xjust=0, yjust=1, legend=leg.text, col=.col, lty=.lty, lwd=.lwd, cex=0.9, bg=\"white\", inset=leg.inset, bty=leg.bty) }\n  if (!is.null(sub.title)) { mtext(sub.title, line=-4, outer=TRUE) }\n  if (missing(o.text)) {\n    sdf <- summary(survival::coxph(formula.s, data=data.s, weights=weight.s))\n    if(verbose) { print(sdf) }\n    p.val <- sdf$sctest[\"pvalue\"]\n    o.text <- sprintf(\"Logrank P = %.1E\", p.val)\n  }\n  if (is.null(o.text)) { o.text <- FALSE }\n  text(0,0, o.text, cex=0.85, pos=4)\n\n  if (show.n.risk) {\n    usr.xy <- par( \"usr\" )\n    nrisk <- no.at.risk(formula.s=formula.s, data.s=data.s, sub.s=\"all\", t.step=n.risk.step, t.end=floor(usr.xy[2]) )\n    at.loc <- seq(0, usr.xy[2], n.risk.step)\n    axis(1, at=at.loc)\n    mtext(x.label, side=1, line=2)\n    mtext(\"No. At Risk\", side=1, line=3, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex, font=2)\n    for( i in 1:nrow(nrisk) ) {\n      mtext(leg.text[i], side=1, line=3+i, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex)\n      mtext(nrisk[i,-1], side=1, at=at.loc, line=3+i, adj=1, cex=n.risk.cex)\n    }\n  }\n\n  if( exists(\"weight.s\", envir=.GlobalEnv) ) remove(\"weight.s\", envir=.GlobalEnv)\n}"
      },
      {
        "partial": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  # TODO: Implement data preprocessing and parameter initialization\n\n  # TODO: Set up plot parameters\n\n  # TODO: Create the main plot\n\n  # TODO: Add additional plot elements (lines, legend, text)\n\n  # TODO: Handle 'show.n.risk' functionality\n\n  # TODO: Clean up global environment\n}",
        "complete": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  if (missing(sub.title)) sub.title <- NULL\n  if (missing(leg.text)) leg.text <- NULL\n  if (missing(weight.s)) weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s)))\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  assign(\"weight.s\", weight.s, envir = as.environment(1))\n\n  ng <- length(leg.text)\n  old.mar <- par(\"mar\")\n  on.exit(par(mar = old.mar))\n  .xaxt <- if(show.n.risk) {\n    par(mar = old.mar + c(ng,8,3,0))\n    \"n\"\n  } else \"s\"\n  .xlab <- if(show.n.risk) \"\" else x.label\n\n  plot(survfit(formula.s, data=data.s, weights=weight.s), xaxt=.xaxt, col=.col, lty=.lty, lwd=.lwd, xlab=.xlab, ylab=y.label, ...)\n  title(main.title)\n\n  if (!missing(v.line) && !is.null(v.line)) abline(v=v.line, lty=3, col=\"purple\")\n  if (!missing(h.line) && !is.null(h.line)) abline(h=h.line, lty=3, col=\"purple\")\n\n  if (!is.null(leg.text)) legend(x=leg.pos, legend=leg.text, col=.col, lty=.lty, lwd=.lwd, cex=0.9, bg=\"white\", inset=leg.inset, bty=leg.bty)\n  if (!is.null(sub.title)) mtext(sub.title, line=-4, outer=TRUE)\n\n  if (missing(o.text)) {\n    sdf <- summary(survival::coxph(formula.s, data=data.s, weights=weight.s))\n    if(verbose) print(sdf)\n    p.val <- sdf$sctest[\"pvalue\"]\n    o.text <- sprintf(\"Logrank P = %.1E\", p.val)\n  }\n  if (!is.null(o.text)) text(0, 0, o.text, cex=0.85, pos=4)\n\n  if (show.n.risk) {\n    usr.xy <- par(\"usr\")\n    nrisk <- no.at.risk(formula.s=formula.s, data.s=data.s, sub.s=\"all\", t.step=n.risk.step, t.end=floor(usr.xy[2]))\n    at.loc <- seq(0, usr.xy[2], n.risk.step)\n    axis(1, at=at.loc)\n    mtext(x.label, side=1, line=2)\n    mtext(\"No. At Risk\", side=1, line=3, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex, font=2)\n    for(i in 1:nrow(nrisk)) {\n      mtext(leg.text[i], side=1, line=3+i, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex)\n      mtext(nrisk[i,-1], side=1, at=at.loc, line=3+i, adj=1, cex=n.risk.cex)\n    }\n  }\n\n  if(exists(\"weight.s\", envir=.GlobalEnv)) remove(\"weight.s\", envir=.GlobalEnv)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/drugGeneResponseCurve.R",
    "language": "R",
    "content": "#' Compares gene expression for a specificed set of features over specific\n#'   drug dosages vs time\n#'\n#' This function generates a plot visualizing the relationship between gene\n#'   expression, time and dose level for the selected tSet. The plot is generated\n#'   with ggplot2 and can be customized using ggplot plot + function() syntax.\n#'\n#' @examples\n#'\n#' if (interactive()) {\n#'   drugGeneResponseCurve(TGGATESsmall, dose = c(\"Control\", \"Low\", \"Middle\"),\n#'   mDataTypes=\"rna\", drug = treatmentNames(TGGATESsmall)[1],\n#'   duration = c(\"2\", \"8\", \"24\"), features = \"ENSG00000002726_at\")\n#' }\n#'\n#' @param tSet \\code{ToxicoSet} A ToxicoSet to be plotted in this graph. Currently\n#'   only a single tSet is supported.\n#' @param dose \\code{character} A vector of dose levels to be included in the\n#'   plot. Default to include all dose levels available for a drug. If you specify\n#'   more than two features you may only pass in up to two dose levels.\n#' @param mDataTypes \\code{vector} A vector specifying the molecular data types to\n#'   include in this plot. Defaults to the first mDataType if not specified.ex\n#'   This release version only accepts one mDataType, more to be added in\n#'   forthcoming releases.\n#' @param features \\code{character} A vector of feature names to include in the\n#'   plot. If you specify more than two dose levels, you may only pass in up to\n#'   two features.\n#' @param drug \\code{character} A drug name to include in this plot.\n#'   See treatmentNames(tSet) for a list of options.\n#' @param duration \\code{character} A vector of durations to include in the plot.\n#' @param summarize_replicates \\code{logical} If TRUE will average viability\n#'   across replicates for each unique drug-dose-duration combination.\n#' @param cell_lines \\code{character} A vector of cell lines to include in the\n#'   plot.\n#' @param line_width \\code{numeric} A number specifying the thickness of lines\n#'   in the plot, as passed to size in geom_line(). Defaults to 1.\n#' @param point_size \\code{numeric} A number specifying how large points should\n#'   be in the plot, as passed to size in geom_point(). Defaults to 2.5.\n#' @param ggplot_args \\code{list} A list of ggplot2 functions which can be\n#'   called using the plot + function() syntax. This allows arbitrary\n#'   customization of the plot including changing the title, axis labels,\n#'   colours, etc. Please see the included examples for basic usage or ggplot2\n#'   documentation for advanced customization.\n#' @param verbose \\code{boolean} Should warning messages about the data passed\n#'   in be printed?\n#'\n#' @return Plot of the viabilities for each drug vs time of exposure\n#'\n#' @importFrom data.table data.table melt.data.table `:=`\n#' @import ggplot2\n#' @importFrom tibble as_tibble\n#'\n#' @export\ndrugGeneResponseCurve <- function(\n  tSet,\n  duration = NULL,\n  cell_lines = NULL,\n  mDataTypes = NULL,\n  features = NULL,\n  dose = NULL,\n  drug = NULL,\n  summarize_replicates = TRUE,\n  line_width = 1,\n  point_size = 2.5,\n  ggplot_args = NULL,\n  verbose=TRUE\n) {\n\n  # Place tSet in a list if not already\n  if (!is(tSet, \"list\")) { tSet <- list(tSet) }\n\n  ## Tempary warnings until function is finished\n  if (length(tSet) > 1) { stop(\"This function currently only supports one tSet per plot...\")}\n  if (length(drug) > 1) { stop(\"This function currently only supports one drug per plot...\")}\n  if (length(mDataTypes) > 1) {stop(\"This function currently only supports one molecular data type per plot...\")}\n  if (length(features) > 2) { if (length(dose) > 2) { stop(\"To plot more than one feature, please specify only up to two dose levels...\")}}\n  if (length(dose) > 2) { if (length(features) > 2) { stop(\"To plot more than one dose level, please specify up to two molecular feature...\")}}\n\n  # Deal with controls (i.e., treated with DMSO)\n  if (any(vapply(tSet, function(tSet) { name(tSet) %in% c(\"drugMatrix_rat\", \"EMEXP2458\")}, FUN.VALUE = logical(1)))) {\n    drug <- c(\"DMSO\", drug)\n  }\n\n  ## TODO:: Generalize this to work with multiple data types\n  if (missing(mDataTypes)) { mDataTypes <- names(molecularProfilesSlot(tSet[[1]])) }\n\n  if (is.null(features)) {\n    features <- lapply(tSet, function(tSet) {\n      rownames(featureInfo(tSet, \"rna\"))[seq_len(5)]\n    })\n  }\n\n  if (missing(cell_lines)) {cell_lines <- unique(phenoInfo(tSet[[1]], mDataTypes[1])$sampleid)}\n  if (length(cell_lines) > 1) { stop(\"Only one cell type per plot is currently supported...\")}\n\n  # Places features in list if not already\n  if (!is(features, \"list\")) {\n    features <- list(features)\n  }\n  names(features) <- vapply(tSet, function(x) name(x), FUN.VALUE = character(1))\n\n  # Subsetting the tSet based on parameter arguments\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({ToxicoGx::subsetTo(tSet, mDataType = mDataTypes, drugs = drug,\n                       duration = duration, features = unique(unlist(features)), cell_lines=unique(cell_lines))})\n  })\n\n  # Get only the dose levels available for that drug\n  dose <- intersect(dose, as.character(unique(phenoInfo(tSet[[1]], \"rna\")$dose_level)))\n\n  # Gather the plot data\n  plotData <- lapply(tSet, function(tSet) {\n    m <- lapply(mDataTypes, function(mDataType) {\n      mProf <- molecularProfiles(tSet, mDataType)\n      list(\n        \"data\" = data.table(\n          mProf,\n          keep.rownames = TRUE\n        ),\n        \"pInfo\" = data.table(as.data.frame(phenoInfo(tSet, mDataType)))\n      )\n    })\n    names(m) <- mDataTypes; m\n  })\n  names(plotData) <-  vapply(tSet, function(x) name(x), FUN.VALUE = character(1))\n\n  #### Assembling the plot data ####\n  d <- plotData[[1]][[1]]$data\n  pInfo <- plotData[[1]][[1]]$pInfo\n  data <- melt.data.table(d, id.vars = 1, variable.factor = FALSE)\n  colnames(data) <- c(\"feature\", \"samplename\", \"expression\")\n  if (is.numeric(data$samplename)) data[, samplename := as.character(samplename)]\n  if (is.numeric(pInfo$samplename)) pInfo[, samplename := as.character(samplename)]\n  fInfo <- data.table(as.data.frame(featureInfo(tSet[[1]], \"rna\")))\n  colnames(fInfo)[2] <- \"feature\"\n\n  plotData <- merge(data, pInfo[, .(samplename, individual_id,\n                                    treatmentid, dose_level, duration)],\n                    by = \"samplename\")\n  plotData <- merge(plotData, fInfo[, .(Symbol, feature)], by = \"feature\")\n  plotData[, dose_level := as.factor(dose_level)]\n  plotData[dose_level == \"Control\",\n                       expression := mean(expression),\n                       by = .(dose_level, duration, Symbol)]\n  max_rep <- max(plotData[dose_level != 'Control', unique(individual_id)])\n  plotData <- plotData[individual_id %in% seq_len(max_rep), .SD, by = .(dose_level, duration)]\n  plotData <- plotData[dose_level %in% dose, .SD]\n\n  #### Rendering the plot ####\n  if (summarize_replicates == FALSE) {\n    plot <- ggplot(as_tibble(plotData),\n           aes(x = as.numeric(duration),\n               y = expression,\n               color = factor(dose_level, levels=dose),\n               linetype = as.factor(individual_id),\n               shape = Symbol,\n               group = interaction(dose_level, individual_id, Symbol))) +\n      geom_line(size = line_width) +\n      geom_point(size = point_size)\n  } else {\n    plotData <- plotData[, expression := mean(expression), by = .(dose_level, duration, Symbol)][individual_id == 1]\n    plot <- ggplot(plotData,\n                   aes(as.numeric(duration),\n                       expression,\n                       color = factor(dose_level, levels=c(\"Control\", \"Low\", \"Middle\", \"High\")))) +\n      geom_line(aes(linetype = Symbol), size = line_width) +\n      geom_point(size = point_size)\n  }\n  plot <- plot +\n    labs(\n      title = paste0(\"Drug Gene Response Curve for \", paste(drug, collapse = \" & \"), \" in \", paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n      color = \"Dose Level\",\n      linetype = \"Replicate\",\n      shape = \"Feature\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    ) + xlab(\"Duration (hrs)\") +\n    ylab(\"Expression\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  if (!is.null(ggplot_args)) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `drugGeneResponseCurve` function, and what type of visualization does it generate?",
        "answer": "The `drugGeneResponseCurve` function generates a plot visualizing the relationship between gene expression, time, and dose level for a selected ToxicoSet. It creates a line plot using ggplot2 to show how gene expression changes over time for different drug dosages."
      },
      {
        "question": "How does the function handle multiple features and dose levels? What are the limitations?",
        "answer": "The function can handle multiple features and dose levels, but with limitations. If more than two features are specified, only up to two dose levels can be plotted. Conversely, if more than two dose levels are specified, only up to two features can be plotted. These limitations are enforced through error checks at the beginning of the function."
      },
      {
        "question": "What is the purpose of the `summarize_replicates` parameter, and how does it affect the plot generation?",
        "answer": "The `summarize_replicates` parameter determines whether to average viability across replicates for each unique drug-dose-duration combination. If set to TRUE (default), the function calculates the mean expression for each combination and plots a single line per dose level and feature. If FALSE, it plots individual lines for each replicate, using different line types to distinguish between replicates."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugGeneResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, mDataTypes = NULL, features = NULL, dose = NULL, drug = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, ggplot_args = NULL, verbose=TRUE) {\n  if (!is(tSet, \"list\")) { tSet <- list(tSet) }\n  if (length(tSet) > 1 || length(drug) > 1 || length(mDataTypes) > 1) { stop(\"Unsupported input\") }\n  if (length(features) > 2 && length(dose) > 2) { stop(\"Too many features or doses\") }\n  \n  # TODO: Implement the rest of the function\n}",
        "complete": "drugGeneResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, mDataTypes = NULL, features = NULL, dose = NULL, drug = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, ggplot_args = NULL, verbose=TRUE) {\n  if (!is(tSet, \"list\")) { tSet <- list(tSet) }\n  if (length(tSet) > 1 || length(drug) > 1 || length(mDataTypes) > 1) { stop(\"Unsupported input\") }\n  if (length(features) > 2 && length(dose) > 2) { stop(\"Too many features or doses\") }\n  \n  if (missing(mDataTypes)) { mDataTypes <- names(molecularProfilesSlot(tSet[[1]])) }\n  if (is.null(features)) { features <- list(rownames(featureInfo(tSet[[1]], \"rna\"))[1:5]) }\n  if (missing(cell_lines)) { cell_lines <- unique(phenoInfo(tSet[[1]], mDataTypes[1])$sampleid) }\n  if (length(cell_lines) > 1) { stop(\"Only one cell type per plot is currently supported\") }\n  \n  tSet <- lapply(tSet, function(ts) suppressWarnings(ToxicoGx::subsetTo(ts, mDataType = mDataTypes, drugs = drug, duration = duration, features = unique(unlist(features)), cell_lines = unique(cell_lines))))\n  \n  plotData <- lapply(tSet, function(ts) {\n    m <- lapply(mDataTypes, function(mdt) {\n      mProf <- molecularProfiles(ts, mdt)\n      list(\"data\" = data.table(mProf, keep.rownames = TRUE),\n           \"pInfo\" = data.table(as.data.frame(phenoInfo(ts, mdt))))\n    })\n    names(m) <- mDataTypes\n    m\n  })\n  \n  d <- plotData[[1]][[1]]$data\n  pInfo <- plotData[[1]][[1]]$pInfo\n  data <- melt.data.table(d, id.vars = 1, variable.factor = FALSE)\n  colnames(data) <- c(\"feature\", \"samplename\", \"expression\")\n  fInfo <- data.table(as.data.frame(featureInfo(tSet[[1]], \"rna\")))\n  colnames(fInfo)[2] <- \"feature\"\n  \n  plotData <- merge(data, pInfo[, .(samplename, individual_id, treatmentid, dose_level, duration)], by = \"samplename\")\n  plotData <- merge(plotData, fInfo[, .(Symbol, feature)], by = \"feature\")\n  plotData[, dose_level := as.factor(dose_level)]\n  plotData[dose_level == \"Control\", expression := mean(expression), by = .(dose_level, duration, Symbol)]\n  \n  if (summarize_replicates) {\n    plotData <- plotData[, .(expression = mean(expression)), by = .(dose_level, duration, Symbol)]\n  }\n  \n  ggplot(plotData, aes(x = as.numeric(duration), y = expression, color = dose_level)) +\n    geom_line(aes(linetype = Symbol), size = line_width) +\n    geom_point(size = point_size) +\n    labs(title = paste(\"Drug Gene Response Curve for\", drug, \"in\", cell_lines),\n         color = \"Dose Level\", linetype = \"Feature\") +\n    theme(plot.title = element_text(hjust = 0.5, size = 14)) +\n    xlab(\"Duration (hrs)\") + ylab(\"Expression\") +\n    scale_x_continuous(breaks = as.numeric(duration), labels = duration) +\n    if (!is.null(ggplot_args)) ggplot_args else list()\n}"
      },
      {
        "partial": "plotData <- function(tSet, mDataTypes, drug, duration, features, cell_lines) {\n  # TODO: Implement the function to process and prepare plot data\n}",
        "complete": "plotData <- function(tSet, mDataTypes, drug, duration, features, cell_lines) {\n  tSet <- suppressWarnings(ToxicoGx::subsetTo(tSet, mDataType = mDataTypes, drugs = drug, duration = duration, features = features, cell_lines = cell_lines))\n  \n  mProf <- molecularProfiles(tSet, mDataTypes)\n  pInfo <- phenoInfo(tSet, mDataTypes)\n  fInfo <- featureInfo(tSet, mDataTypes)\n  \n  data <- data.table(mProf, keep.rownames = TRUE)\n  data <- melt.data.table(data, id.vars = 1, variable.factor = FALSE)\n  colnames(data) <- c(\"feature\", \"samplename\", \"expression\")\n  \n  pInfo <- data.table(as.data.frame(pInfo))\n  fInfo <- data.table(as.data.frame(fInfo))\n  colnames(fInfo)[2] <- \"feature\"\n  \n  plotData <- merge(data, pInfo[, .(samplename, individual_id, treatmentid, dose_level, duration)], by = \"samplename\")\n  plotData <- merge(plotData, fInfo[, .(Symbol, feature)], by = \"feature\")\n  plotData[, dose_level := as.factor(dose_level)]\n  plotData[dose_level == \"Control\", expression := mean(expression), by = .(dose_level, duration, Symbol)]\n  \n  return(plotData)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/src/survcomp_init.cpp",
    "language": "cpp",
    "content": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n/* FIXME:\n Check these declarations against the C/Fortran source code.\n */\n\n/* .C calls */\nextern \"C\" {\n\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n\n    /* .Call calls */\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n\n    static const R_CallMethodDef CallEntries[] = {\n        {\"get_concordanceIndex_onevariable\", (DL_FUNC) &get_concordanceIndex_onevariable, 12},\n        {\"mrmr_cIndex\",                      (DL_FUNC) &mrmr_cIndex,                       6},\n        {\"mrmr_cIndex_ensemble_remove\",      (DL_FUNC) &mrmr_cIndex_ensemble_remove,      21},\n        {NULL, NULL, 0}\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        R_registerRoutines(dll, CEntries, CallEntries, NULL, NULL);\n        R_useDynamicSymbols(dll, FALSE);\n        R_forceSymbols(dll, TRUE);\n    }\n\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'extern \"C\"' block in this code snippet?",
        "answer": "The 'extern \"C\"' block is used to prevent name mangling of the enclosed function declarations. This ensures that the C++ compiler treats these functions as having C linkage, which is necessary for proper interaction with R's C interface. It allows R to correctly locate and call these functions from the compiled library."
      },
      {
        "question": "How are R_CMethodDef and R_CallMethodDef structures used in this code, and what is their significance?",
        "answer": "R_CMethodDef and R_CallMethodDef structures are used to register C and Call methods, respectively, that can be called from R. The CEntries array registers the 'concordanceIndexC' function as a .C method, while the CallEntries array registers 'get_concordanceIndex_onevariable', 'mrmr_cIndex', and 'mrmr_cIndex_ensemble_remove' as .Call methods. These structures provide R with the necessary information to locate and invoke these functions, including their names, addresses, and the number of arguments they expect."
      },
      {
        "question": "What is the role of the R_init_survcomp function, and why is it important?",
        "answer": "The R_init_survcomp function is an initialization function for the 'survcomp' R package. It is automatically called when the package is loaded. Its primary roles are: 1) Registering the C and Call routines using R_registerRoutines, which improves performance and safety by explicitly declaring the functions and their interfaces. 2) Disabling dynamic symbol lookup with R_useDynamicSymbols(dll, FALSE), which can improve security. 3) Enforcing use of registered symbols with R_forceSymbols(dll, TRUE), which ensures that only explicitly registered functions can be called from R."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n\n    static const R_CallMethodDef CallEntries[] = {\n        // TODO: Complete the CallEntries array\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        // TODO: Complete the R_init_survcomp function\n    }\n\n}",
        "complete": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n\n    static const R_CallMethodDef CallEntries[] = {\n        {\"get_concordanceIndex_onevariable\", (DL_FUNC) &get_concordanceIndex_onevariable, 12},\n        {\"mrmr_cIndex\",                      (DL_FUNC) &mrmr_cIndex,                       6},\n        {\"mrmr_cIndex_ensemble_remove\",      (DL_FUNC) &mrmr_cIndex_ensemble_remove,      21},\n        {NULL, NULL, 0}\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        R_registerRoutines(dll, CEntries, CallEntries, NULL, NULL);\n        R_useDynamicSymbols(dll, FALSE);\n        R_forceSymbols(dll, TRUE);\n    }\n\n}"
      },
      {
        "partial": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    // Function declarations\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    // TODO: Define CEntries and CallEntries arrays\n\n    // TODO: Implement R_init_survcomp function\n\n}",
        "complete": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    // Function declarations\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n    static const R_CallMethodDef CallEntries[] = {\n        {\"get_concordanceIndex_onevariable\", (DL_FUNC) &get_concordanceIndex_onevariable, 12},\n        {\"mrmr_cIndex\",                      (DL_FUNC) &mrmr_cIndex,                       6},\n        {\"mrmr_cIndex_ensemble_remove\",      (DL_FUNC) &mrmr_cIndex_ensemble_remove,      21},\n        {NULL, NULL, 0}\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        R_registerRoutines(dll, CEntries, CallEntries, NULL, NULL);\n        R_useDynamicSymbols(dll, FALSE);\n        R_forceSymbols(dll, TRUE);\n    }\n\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/LogLogisticRegression.R",
    "language": "R",
    "content": "#' Fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points (c, E) given by the user\n#' and returns a vector containing estimates for HS, E_inf, and EC50.\n#'\n#' By default, logLogisticRegression uses an L-BFGS algorithm to generate the fit. However, if\n#' this fails to converge to solution, logLogisticRegression samples lattice points throughout the parameter space.\n#' It then uses the lattice point with minimal least-squares residual as an initial guess for the optimal parameters,\n#' passes this guess to drm, and re-attempts the optimization. If this still fails, logLogisticRegression uses the\n#' PatternSearch algorithm to fit a log-logistic curve to the data.\n#'\n#' @examples\n#' dose <- c(\"0.0025\",\"0.008\",\"0.025\",\"0.08\",\"0.25\",\"0.8\",\"2.53\",\"8\")\n#' viability <- c(\"108.67\",\"111\",\"102.16\",\"100.27\",\"90\",\"87\",\"74\",\"57\")\n#' computeAUC(dose, viability)\n#'\n#' @param conc [vector] is a vector of drug concentrations.\n#' @param viability [vector] is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of the log_conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param density [vector] is a vector of length 3 whose components are the numbers of lattice points per unit\n#' length along the HS-, E_inf-, and base-10 logarithm of the EC50-dimensions of the parameter space, respectively.\n#' @param step [vector] is a vector of length 3 whose entries are the initial step sizes in the HS, E_inf, and\n#' base-10 logarithm of the EC50 dimensions, respectively, for the PatternSearch algorithm.\n#' @param precision is a positive real number such that when the ratio of current step size to initial step\n#' size falls below it, the PatternSearch algorithm terminates. A smaller value will cause LogisticPatternSearch\n#' to take longer to complete optimization, but will produce a more accurate estimate for the fitted parameters.\n#' @param lower_bounds [vector] is a vector of length 3 whose entries are the lower bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param upper_bounds [vector] is a vector of length 3 whose entries are the upper bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param scale is a positive real number specifying the shape parameter of the Cauchy distribution.\n#' @param family [character], if \"cauchy\", uses MLE under an assumption of Cauchy-distributed errors\n#' instead of sum-of-squared-residuals as the objective function for assessing goodness-of-fit of\n#' dose-response curves to the data. Otherwise, if \"normal\", uses MLE with a gaussian assumption of errors\n#' @param median_n If the viability points being fit were medians of measurements, they are expected to follow a median of \\code{family}\n#' distribution, which is in general quite different from the case of one measurement. Median_n is the number of measurements\n#' the median was taken of. If the measurements are means of values, then both the Normal and the Cauchy distributions are stable, so means of\n#' Cauchy or Normal distributed variables are still Cauchy and normal respectively.\n#' @param conc_as_log [logical], if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(EC50) should be returned instead of EC50.\n#' @param viability_as_pct [logical], if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf should be returned as a decimal rather than a percentage.\n#' @param trunc [logical], if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose [logical], if true, causes warnings thrown by the function to be printed.\n#' @return A vector containing estimates for HS, E_inf, and EC50\n#' @export\n#' @importFrom stats optim dcauchy dnorm pcauchy rcauchy rnorm pnorm integrate\nlogLogisticRegression <- function(conc,\n                                  viability,\n                                  density = c(2, 10, 2),\n                                  step = .5 / density,\n                                  precision = 0.05,\n                                  lower_bounds = c(0, 0, -6),\n                                  upper_bounds = c(4, 1, 6),\n                                  scale = 0.07,\n                                  family = c(\"normal\", \"Cauchy\"),\n                                  median_n = 1,\n                                  conc_as_log = FALSE,\n                                  viability_as_pct = TRUE,\n                                  trunc = TRUE,\n                                  verbose = FALSE) {\n\n  family <- match.arg(family)\n\n  if (prod(is.finite(step)) != 1) {\n    message(step)\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  if (prod(is.finite(precision)) != 1) {\n    message(precision)\n    stop(\"Precision value is not a real number.\")\n  }\n\n  if (prod(is.finite(lower_bounds)) != 1) {\n    message(lower_bounds)\n    stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(upper_bounds)) != 1) {\n    message(upper_bounds)\n    stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(density)) != 1) {\n    message(density)\n    stop(\"Density vector contains elements which are not real numbers.\")\n  }\n\n  if (is.finite(scale) == FALSE) {\n    message(scale)\n    stop(\"Scale is not a real number.\")\n  }\n\n  if (is.character(family) == FALSE) {\n    message(family)\n    stop(\"Cauchy flag is not a string.\")\n  }\n\n  if (length(density) != 3){\n    stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  }\n\n  if (!median_n==as.integer(median_n)){\n    stop(\"There can only be a integral number of samples to take a median of. Check your setting of median_n parameter, it is not an integer\")\n  }\n\n\n  if (min(upper_bounds - lower_bounds) < 0) {\n    message(rbind(lower_bounds, upper_bounds))\n    stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  }\n\n\n\n  if (min(density) <= 0) {\n    message(density)\n    stop(\"Lattice point density vector contains negative values.\")\n  }\n\n  if (precision <= 0) {\n    message(precision)\n    stop(\"Negative precision value.\")\n  }\n\n  if (min(step) <= 0) {\n    message(step)\n    stop(\"Step vector contains nonpositive numbers.\")\n  }\n\n  if (scale <= 0) {\n    message(scale)\n    stop(\"Scale parameter is a nonpositive number.\")\n  }\n\n  cleanData  <- sanitizeInput(conc=conc,\n                              viability=viability,\n                              conc_as_log = conc_as_log,\n                              viability_as_pct = viability_as_pct,\n                              trunc = trunc,\n                              verbose=verbose)\n\n  log_conc <- cleanData[[\"log_conc\"]]\n  viability <- cleanData[[\"viability\"]]\n\n\n  #ATTEMPT TO REFINE GUESS WITH L-BFGS OPTIMIZATION\n  gritty_guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n                    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n                    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3]))\n  guess <- tryCatch(optim(par=gritty_guess,#par = sieve_guess,\n                          fn = function(x) {.residual(log_conc,\n                                                      viability,\n                                                      pars = x,\n                                                      n = median_n,\n                                                      scale = scale,\n                                                      family = family,\n                                                      trunc = trunc)\n                          },\n                          lower = lower_bounds,\n                          upper = upper_bounds,\n                          method = \"L-BFGS-B\",\n  ),\n  error = function(e) {\n    list(\"par\"=gritty_guess, \"convergence\"=-1)\n  })\n  failed = guess[[\"convergence\"]] != 0\n  guess <- guess[[\"par\"]]\n\n  guess_residual <- .residual(log_conc,\n                              viability,\n                              pars = guess,\n                              n = median_n,\n                              scale = scale,\n                              family = family,\n                              trunc = trunc)\n\n\n  #GENERATE INITIAL GUESS BY OBJECTIVE FUNCTION EVALUATION AT LATTICE POINTS\n  gritty_guess_residual <- .residual(log_conc,\n                                     viability,\n                                     pars = gritty_guess,\n                                     n = median_n,\n                                     scale = scale,\n                                     family = family,\n                                     trunc = trunc)\n\n\n  #CHECK SUCCESS OF L-BFGS OPTIMIZAITON AND RE-OPTIMIZE WITH A PATTERN SEARCH IF NECESSARY\n  if (failed || any(is.na(guess)) || guess_residual >= gritty_guess_residual) {\n    #GENERATE INITIAL GUESS BY OBJECTIVE FUNCTION EVALUATION AT LATTICE POINTS\n    sieve_guess <- .meshEval(log_conc,\n                             viability,\n                             lower_bounds = lower_bounds,\n                             upper_bounds = upper_bounds,\n                             density = density,\n                             n=median_n,\n                             scale = scale,\n                             family = family,\n                             trunc = trunc)\n\n    sieve_guess_residual <- .residual(log_conc,\n                                      viability,\n                                      pars = sieve_guess,\n                                      n = median_n,\n                                      scale = scale,\n                                      family = family,\n                                      trunc = trunc)\n\n    guess <- sieve_guess\n    guess_residual <- sieve_guess_residual\n    span <- 1\n\n    while (span > precision) {\n      neighbours <- rbind(guess, guess, guess, guess, guess, guess)\n      neighbour_residuals <- matrix(NA, nrow=1, ncol=6)\n      neighbours[1, 1] <- pmin(neighbours[1, 1] + span * step[1], upper_bounds[1])\n      neighbours[2, 1] <- pmax(neighbours[2, 1] - span * step[1], lower_bounds[1])\n      neighbours[3, 2] <- pmin(neighbours[3, 2] + span * step[2], upper_bounds[2])\n      neighbours[4, 2] <- pmax(neighbours[4, 2] - span * step[2], lower_bounds[2])\n      neighbours[5, 3] <- pmin(neighbours[5, 3] + span * step[3], upper_bounds[3])\n      neighbours[6, 3] <- pmax(neighbours[6, 3] - span * step[3], lower_bounds[3])\n\n      for (i in seq_len(nrow(neighbours))) {\n        neighbour_residuals[i] <- .residual(log_conc,\n                                            viability,\n                                            pars = neighbours[i, ],\n                                            n = median_n,\n                                            scale = scale,\n                                            family = family,\n                                            trunc = trunc)\n      }\n\n      if (min(neighbour_residuals) < guess_residual) {\n        guess <- neighbours[which.min(neighbour_residuals), ]\n        guess_residual <- min(neighbour_residuals)\n      } else {\n        span <- span / 2\n      }\n    }\n  }\n\n  return(list(\"HS\" = guess[1],\n              \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n              \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logLogisticRegression` function and what type of data does it process?",
        "answer": "The `logLogisticRegression` function fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points. It processes concentration (c) and viability (E) data provided by the user, and returns estimates for the Hill Slope (HS), E_inf (efficacy at infinite concentration), and EC50 (concentration for 50% effect)."
      },
      {
        "question": "How does the function handle optimization failures, and what alternative methods does it employ?",
        "answer": "The function first attempts to use an L-BFGS algorithm for optimization. If this fails to converge, it samples lattice points throughout the parameter space and uses the point with minimal least-squares residual as an initial guess. It then tries optimization again using this guess. If this still fails, the function switches to using a PatternSearch algorithm to fit the log-logistic curve to the data."
      },
      {
        "question": "What is the significance of the `family` parameter in the `logLogisticRegression` function, and how does it affect the curve fitting process?",
        "answer": "The `family` parameter determines the error distribution assumption for the curve fitting process. If set to 'cauchy', the function uses Maximum Likelihood Estimation (MLE) under an assumption of Cauchy-distributed errors. If set to 'normal', it uses MLE with a Gaussian assumption of errors. This choice affects the objective function used for assessing the goodness-of-fit of the dose-response curves to the data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 2), step = .5 / density, precision = 0.05, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = FALSE) {\n  family <- match.arg(family)\n\n  # Input validation checks\n  if (prod(is.finite(step)) != 1) {\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  # ... (other input validations)\n\n  cleanData <- sanitizeInput(conc, viability, conc_as_log, viability_as_pct, trunc, verbose)\n  log_conc <- cleanData[['log_conc']]\n  viability <- cleanData[['viability']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Attempt L-BFGS optimization\n  guess <- tryCatch(\n    optim(\n      par = gritty_guess,\n      fn = function(x) {\n        .residual(log_conc, viability, pars = x, n = median_n, scale = scale, family = family, trunc = trunc)\n      },\n      lower = lower_bounds,\n      upper = upper_bounds,\n      method = \"L-BFGS-B\"\n    ),\n    error = function(e) {\n      list(\"par\" = gritty_guess, \"convergence\" = -1)\n    }\n  )\n\n  failed <- guess[['convergence']] != 0\n  guess <- guess[['par']]\n  guess_residual <- .residual(log_conc, viability, pars = guess, n = median_n, scale = scale, family = family, trunc = trunc)\n\n  # TODO: Implement fallback methods if L-BFGS optimization fails\n\n  return(list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  ))\n}",
        "complete": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 2), step = .5 / density, precision = 0.05, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = FALSE) {\n  family <- match.arg(family)\n\n  # Input validation checks\n  if (prod(is.finite(step)) != 1) {\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  # ... (other input validations)\n\n  cleanData <- sanitizeInput(conc, viability, conc_as_log, viability_as_pct, trunc, verbose)\n  log_conc <- cleanData[['log_conc']]\n  viability <- cleanData[['viability']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Attempt L-BFGS optimization\n  guess <- tryCatch(\n    optim(\n      par = gritty_guess,\n      fn = function(x) {\n        .residual(log_conc, viability, pars = x, n = median_n, scale = scale, family = family, trunc = trunc)\n      },\n      lower = lower_bounds,\n      upper = upper_bounds,\n      method = \"L-BFGS-B\"\n    ),\n    error = function(e) {\n      list(\"par\" = gritty_guess, \"convergence\" = -1)\n    }\n  )\n\n  failed <- guess[['convergence']] != 0\n  guess <- guess[['par']]\n  guess_residual <- .residual(log_conc, viability, pars = guess, n = median_n, scale = scale, family = family, trunc = trunc)\n\n  # Fallback methods if L-BFGS optimization fails\n  if (failed || any(is.na(guess)) || guess_residual >= .residual(log_conc, viability, pars = gritty_guess, n = median_n, scale = scale, family = family, trunc = trunc)) {\n    # Generate initial guess by objective function evaluation at lattice points\n    sieve_guess <- .meshEval(log_conc, viability, lower_bounds, upper_bounds, density, n = median_n, scale = scale, family = family, trunc = trunc)\n    guess <- sieve_guess\n    guess_residual <- .residual(log_conc, viability, pars = sieve_guess, n = median_n, scale = scale, family = family, trunc = trunc)\n    \n    # Pattern search optimization\n    span <- 1\n    while (span > precision) {\n      neighbours <- rbind(guess, guess, guess, guess, guess, guess)\n      neighbour_residuals <- numeric(6)\n      \n      for (i in 1:3) {\n        neighbours[2*i-1, i] <- pmin(neighbours[2*i-1, i] + span * step[i], upper_bounds[i])\n        neighbours[2*i, i] <- pmax(neighbours[2*i, i] - span * step[i], lower_bounds[i])\n      }\n      \n      for (i in 1:6) {\n        neighbour_residuals[i] <- .residual(log_conc, viability, pars = neighbours[i,], n = median_n, scale = scale, family = family, trunc = trunc)\n      }\n      \n      if (min(neighbour_residuals) < guess_residual) {\n        guess <- neighbours[which.min(neighbour_residuals),]\n        guess_residual <- min(neighbour_residuals)\n      } else {\n        span <- span / 2\n      }\n    }\n  }\n\n  return(list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  ))\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability, conc_as_log, viability_as_pct, trunc, verbose) {\n  # TODO: Implement input sanitization\n  # Convert concentration to log scale if necessary\n  # Convert viability to decimal if necessary\n  # Truncate viability values if required\n  # Handle missing or invalid data\n  # Return a list with 'log_conc' and 'viability'\n}",
        "complete": "sanitizeInput <- function(conc, viability, conc_as_log, viability_as_pct, trunc, verbose) {\n  # Convert inputs to numeric\n  conc <- as.numeric(conc)\n  viability <- as.numeric(viability)\n\n  # Check for missing or invalid data\n  valid_indices <- !is.na(conc) & !is.na(viability) & conc > 0\n  if (sum(!valid_indices) > 0 && verbose) {\n    warning(paste(sum(!valid_indices), \"invalid or missing data points were removed.\"))\n  }\n  conc <- conc[valid_indices]\n  viability <- viability[valid_indices]\n\n  # Convert concentration to log scale if necessary\n  log_conc <- if (conc_as_log) conc else log10(conc)\n\n  # Convert viability to decimal if necessary\n  if (viability_as_pct) {\n    viability <- viability / 100\n  }\n\n  # Truncate viability values if required\n  if (trunc) {\n    viability <- pmin(pmax(viability, 0), 1)\n  }\n\n  # Check for sufficient data points\n  if (length(log_conc) < 4) {\n    stop(\"At least 4 valid data points are required for curve fitting.\")\n  }\n\n  return(list(log_conc = log_conc, viability = viability))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/zzz.R",
    "language": "R",
    "content": "# Package Start-up Functions\n\n.onAttach <- function(libname, pkgname) {\n\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\nToxicoGx package brought to you by:\n\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\nFor more of our work visit bhklab.ca!\n\nLike ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\n        \"\n        )\n        # Prevent repeated messages when loading multiple lab packages\n        options(bhklab.startup_=FALSE)\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.onAttach` function in this R package code?",
        "answer": "The `.onAttach` function is a special function in R packages that runs when the package is attached (loaded). In this case, it displays a startup message with ASCII art and information about the ToxicoGx package, but only if the R session is interactive and the startup message hasn't been shown before."
      },
      {
        "question": "How does the code ensure that the startup message is only displayed once per R session?",
        "answer": "The code uses the `options()` function to set a flag `bhklab.startup_` to FALSE after displaying the message. Before showing the message, it checks if this option is NULL. This ensures the message is only shown once per session, even if multiple packages from the same lab are loaded."
      },
      {
        "question": "What is the purpose of the `on.exit(options(oldOpts))` line in the code?",
        "answer": "The `on.exit(options(oldOpts))` line ensures that the original options are restored when the function exits, even if an error occurs. This is good practice as it prevents the function from having unintended side effects on the global options settings."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"ToxicoGx package brought to you by BHKLab\\n\\nFor more of our work visit bhklab.ca!\\n\\nLike ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\"\n        )\n        # Complete the function\n    }\n}",
        "complete": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"ToxicoGx package brought to you by BHKLab\\n\\nFor more of our work visit bhklab.ca!\\n\\nLike ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\"\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}"
      },
      {
        "partial": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        # Set options and create on.exit handler\n\n        packageStartupMessage(\n        # Add ASCII art and message here\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}",
        "complete": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\n        ToxicoGx package brought to you by:\n\n        \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n        \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n        \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n        \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n        \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n        \\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\n        For more of our work visit bhklab.ca!\n\n        Like ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\n        \"\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/compute.proto.cor.meta.R",
    "language": "R",
    "content": "#' @title Function to compute correlations to prototypes in a\n#'   meta-analytical framework\n#'\n#' @description\n#' This function computes meta-estimate of correlation coefficients between a set of genes\n#'   and a set of prototypes from a list of gene expression datasets.\n#'\n#' @usage\n#' compute.proto.cor.meta(datas, proto, method = c(\"pearson\", \"spearman\"))\n#'\n#' @param datas List of datasets. Each dataset is a matrix of gene expressions with samples\n#'   in rows and probes in columns, dimnames being properly defined. All the datasets must have the same probes.\n#' @param proto\tNames of prototypes (e.g. their EntrezGene ID).\n#' @param method Estimator for correlation coefficient, can be either pearson or spearman\n#'\n#' @return\n#' A list with items:\n#' -cor Matrix of meta-estimate of correlation coefficients with probes in rows and prototypes in columns.\n#' -cor.n Number of samples used to compute meta-estimate of correlation coefficients.\n#'\n#' @seealso\n#' [genefu::map.datasets]\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # load NKI dataset\n#' data(nkis)\n#' # reduce datasets\n#' ginter <- intersect(annot.vdxs[ ,\"EntrezGene.ID\"], annot.nkis[ ,\"EntrezGene.ID\"])\n#' ginter <- ginter[!is.na(ginter)][1:30]\n#' myx <- unique(c(match(ginter, annot.vdxs[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.vdxs), size=20)))\n#' data2.vdxs <- data.vdxs[ ,myx]\n#' annot2.vdxs <- annot.vdxs[myx, ]\n#' myx <- unique(c(match(ginter, annot.nkis[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.nkis), size=20)))\n#' data2.nkis <- data.nkis[ ,myx]\n#' annot2.nkis <- annot.nkis[myx, ]\n#' # mapping of datasets\n#' datas <- list(\"VDX\"=data2.vdxs,\"NKI\"=data2.nkis)\n#' annots <- list(\"VDX\"=annot2.vdxs, \"NKI\"=annot2.nkis)\n#' datas.mapped <- map.datasets(datas=datas, annots=annots, do.mapping=TRUE)\n#' # define some prototypes\n#' protos <- paste(\"geneid\", ginter[1:3], sep=\".\")\n#' # compute meta-estimate of correlation coefficients to the three prototype genes\n#' probecor <- compute.proto.cor.meta(datas=datas.mapped$datas, proto=protos,\n#'   method=\"pearson\")\n#' str(probecor)\n#'\n#' @md\n#' @importFrom survcomp combine.est fisherz\n#' @export\ncompute.proto.cor.meta <-\nfunction(datas, proto, method=c(\"pearson\", \"spearman\")) {\n\tif(!is.list(datas)) {\n\t\tif(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n\t\tdatasp <- datas[ , proto, drop=FALSE]\n\t\tdatas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n\t\tmycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n\t\tmycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n\t\tfor(i in 1:length(proto)) {\n\t\t\tmycor[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y, m) { xx <- NA; if(sum(complete.cases(x, y)) > 1) xx <- cor(x=x, y=y, method=m, use=\"complete.obs\"); return(xx); }, y=datasp[ , i], m=method)\n\t\t\tmycorn[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y) { return(sum(complete.cases(x, y))) }, y=datasp[ , i])\n\t\t}\n\t} else {\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\t\tdatast <- datasp <- NULL\n\t\tfor(k in 1:length(datas)) {\n\t\t\tif(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n\t\t\tif(!all(is.element(proto, dimnames(datas[[k]])[[2]]))) { stop(\"some prototypes are not in the dataset!\") }\n\t\t\tdatasp <- c(datasp, list(datas[[k]][ , proto, drop=FALSE]))\n\t\t\tdatast <- c(datast, list(datas[[k]][ , !is.element(dimnames(datas[[k]])[[2]], proto)]))\n\t\t}\n\t\tnames(datasp) <- names(datast) <- names(datas)\n\t\tdatas <- datast\n\t\trm(datast)\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\n\t\tmycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n\t\tmycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n\t\tfor(i in 1:length(proto)) {\n\t\t\tfor(j in 1:nc) {\n\t\t\t\tmycorz <- mycorz.se <- NULL\n\t\t\t\tnnt <- 0\n\t\t\t\tfor(k in 1:length(datas)) {\n\t\t\t\t\tnn <- sum(complete.cases(datas[[k]][ , j], datasp[[k]][ , i]))\n\t\t\t\t\tif(nn > 3) {\n\t\t\t\t\t\tmycorz <- c(mycorz, fisherz(cor(x=datas[[k]][ , j], y=datasp[[k]][ , i], method=method, use=\"complete.obs\"), inv=FALSE))\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n\t\t\t\t\t\tnnt <- nnt + nn\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmycorz <- c(mycorz, NA)\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, NA)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n\t\t\t\tmycorn[j, i] <- nnt\n\t\t\t}\n\t\t}\n\t}\n\treturn(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `compute.proto.cor.meta` function?",
        "answer": "The main purpose of the `compute.proto.cor.meta` function is to compute meta-estimates of correlation coefficients between a set of genes and a set of prototypes from a list of gene expression datasets. It can handle both single and multiple datasets, and supports Pearson and Spearman correlation methods."
      },
      {
        "question": "How does the function handle multiple datasets differently from a single dataset?",
        "answer": "For a single dataset, the function directly computes correlations between prototypes and other genes. For multiple datasets, it first ensures all datasets have the same variables, then computes correlations for each dataset separately. It then combines these correlations using Fisher's z-transformation and meta-analysis techniques to produce a single meta-estimate of correlation for each gene-prototype pair."
      },
      {
        "question": "What are the key components of the function's return value?",
        "answer": "The function returns a list with two main components: 1) 'cor': a matrix of meta-estimates of correlation coefficients with probes in rows and prototypes in columns, and 2) 'cor.n': a matrix with the number of samples used to compute each meta-estimate of correlation coefficient, having the same dimensions as the 'cor' matrix."
      }
    ],
    "completion_tasks": [
      {
        "partial": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    if(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n    datasp <- datas[ , proto, drop=FALSE]\n    datas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n    mycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    mycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    for(i in 1:length(proto)) {\n      # Complete the code here\n    }\n  } else {\n    # Complete the code for list input\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
        "complete": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    if(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n    datasp <- datas[ , proto, drop=FALSE]\n    datas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n    mycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    mycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    for(i in 1:length(proto)) {\n      mycor[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y, m) {\n        xx <- NA\n        if(sum(complete.cases(x, y)) > 1) xx <- cor(x=x, y=y, method=m, use=\"complete.obs\")\n        return(xx)\n      }, y=datasp[ , i], m=method)\n      mycorn[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y) sum(complete.cases(x, y)), y=datasp[ , i])\n    }\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    datasp <- lapply(datas, function(d) d[ , proto, drop=FALSE])\n    datas <- lapply(datas, function(d) d[ , !is.element(dimnames(d)[[2]], proto)])\n    mycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    mycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    for(i in 1:length(proto)) {\n      for(j in 1:nc) {\n        mycorz <- mycorz.se <- numeric(length(datas))\n        nnt <- 0\n        for(k in seq_along(datas)) {\n          nn <- sum(complete.cases(datas[[k]][ , j], datasp[[k]][ , i]))\n          if(nn > 3) {\n            mycorz[k] <- fisherz(cor(x=datas[[k]][ , j], y=datasp[[k]][ , i], method=method, use=\"complete.obs\"), inv=FALSE)\n            mycorz.se[k] <- 1/sqrt(nn - 3)\n            nnt <- nnt + nn\n          }\n        }\n        mycor[j, i] <- fisherz(combine.est(x=mycorz, x.se=mycorz.se, na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[j, i] <- nnt\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}"
      },
      {
        "partial": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    # Complete the code for non-list input\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    datasp <- lapply(datas, function(d) d[ , proto, drop=FALSE])\n    datas <- lapply(datas, function(d) d[ , !is.element(dimnames(d)[[2]], proto)])\n    mycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    mycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    for(i in 1:length(proto)) {\n      for(j in 1:nc) {\n        # Complete the code here\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
        "complete": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    if(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n    datasp <- datas[ , proto, drop=FALSE]\n    datas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n    mycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    mycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    for(i in 1:length(proto)) {\n      mycor[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y, m) {\n        xx <- NA\n        if(sum(complete.cases(x, y)) > 1) xx <- cor(x=x, y=y, method=m, use=\"complete.obs\")\n        return(xx)\n      }, y=datasp[ , i], m=method)\n      mycorn[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y) sum(complete.cases(x, y)), y=datasp[ , i])\n    }\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    datasp <- lapply(datas, function(d) d[ , proto, drop=FALSE])\n    datas <- lapply(datas, function(d) d[ , !is.element(dimnames(d)[[2]], proto)])\n    mycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    mycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    for(i in 1:length(proto)) {\n      for(j in 1:nc) {\n        mycorz <- mycorz.se <- numeric(length(datas))\n        nnt <- 0\n        for(k in seq_along(datas)) {\n          nn <- sum(complete.cases(datas[[k]][ , j], datasp[[k]][ , i]))\n          if(nn > 3) {\n            mycorz[k] <- fisherz(cor(x=datas[[k]][ , j], y=datasp[[k]][ , i], method=method, use=\"complete.obs\"), inv=FALSE)\n            mycorz.se[k] <- 1/sqrt(nn - 3)\n            nnt <- nnt + nn\n          }\n        }\n        mycor[j, i] <- fisherz(combine.est(x=mycorz, x.se=mycorz.se, na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[j, i] <- nnt\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/logpl.R",
    "language": "R",
    "content": "`logpl` <-\nfunction(pred, surv.time, surv.event, strata, na.rm=FALSE, verbose=FALSE) {\n\n\t##############\n\t#internal function\n\t##############\n\t\n\tlogpl1 <- function(pred, surv.time, surv.event, verbose=FALSE) {\t\n\t\n\t\tn <- length(pred)\n\t\tr <- rank(surv.time)\n\t\tita <- pred\n\t\tepita <- exp(ita)\n\t\td <- rep(0, n)\n\t\tdono <- rep(0, n)\n\t\tfor(i in 1:n) {\n\t\t\td[i] <- sum(surv.event[r == r[i]])\n\t\t\tdono[i] <- sum(epita[r >= r[i]])\n\t\t}\n\t\trisk <- d/dono\n\t\trisk1 <- d/dono^{\t2}\n\t\tculrisk1 <- culrisk <- rep(0, n)\n\t\tfor(i in 1:n) {\n\t\t\tculrisk[i] <- sum(unique(risk[r <= r[i]]))\n\t\t\tculrisk1[i] <- sum(unique(risk1[r <= r[i]]))\n\t\t}\n\t\tlik <- sum((ita - log(dono)) * surv.event)\n\t\tres <- c(lik, sum(surv.event))\n\t\tnames(res) <- c(\"logpl\", \"event\")\n\t\treturn(res)\n\t}\n\t\n\t##############\n\t\n\t## remove NA values\n\tif(missing(strata)) { strata <- rep(1, length(pred)) } \n\tcc.ix <- complete.cases(surv.time, surv.event, pred, strata)\n\tsurv.time <- surv.time[cc.ix]\n\tsurv.event <- surv.event[cc.ix]\n\tpred <- pred[cc.ix]\n\tstrata <- strata[cc.ix]\n    n <- sum(cc.ix)\n    if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n    if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n    ss <- unique(strata)\n    if(length(ss) < 2) {\n    \tres <- logpl1(surv.time=surv.time, surv.event=surv.event, pred=pred, verbose=verbose)\n    }\n    else {\n    \tres1 <- 0\n    \tres2 <- 0\n    \tfor(i in 1:length(ss)) {\n    \t\tmyx <- strata == ss[i]\n    \t\trr <- logpl1(surv.time=surv.time[myx], surv.event=surv.event[myx], pred=pred[myx], verbose=verbose)\n    \t\tres1 <- res1 + rr[1]\n    \t\tres2 <- res2 + rr[2]\n    \t}\n    \tres <- c(res1, res2)\n    \tnames(res) <- c(\"logpl\", \"event\")\n    }\n    return(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logpl` function and what are its main input parameters?",
        "answer": "The `logpl` function calculates the log partial likelihood for survival data. Its main input parameters are:\n- `pred`: Predicted values or covariates\n- `surv.time`: Survival times\n- `surv.event`: Event indicators (0 for censored, 1 for event)\n- `strata`: Optional stratification variable\n- `na.rm`: Boolean to remove NA values\n- `verbose`: Boolean to print additional information"
      },
      {
        "question": "How does the function handle stratified data?",
        "answer": "The function handles stratified data by:\n1. Checking if a `strata` variable is provided\n2. If there's only one stratum, it calls `logpl1` for the entire dataset\n3. If there are multiple strata:\n   - It iterates through each unique stratum\n   - Calls `logpl1` for each stratum separately\n   - Sums up the results (log partial likelihood and number of events)\n4. This allows for different baseline hazards in each stratum while combining the overall likelihood"
      },
      {
        "question": "What is the purpose of the internal `logpl1` function and how does it calculate the log partial likelihood?",
        "answer": "The `logpl1` function calculates the log partial likelihood for a single stratum. It does this by:\n1. Ranking survival times\n2. Calculating risk sets and risk sums for each time point\n3. Computing cumulative risks\n4. Calculating the log partial likelihood as: sum((ita - log(dono)) * surv.event)\nWhere:\n- `ita` is the linear predictor (pred)\n- `dono` is the sum of exp(ita) for subjects at risk\n- `surv.event` is the event indicator\n\nIt returns both the log partial likelihood and the number of events."
      }
    ],
    "completion_tasks": [
      {
        "partial": "logpl1 <- function(pred, surv.time, surv.event, verbose=FALSE) {\n    n <- length(pred)\n    r <- rank(surv.time)\n    ita <- pred\n    epita <- exp(ita)\n    d <- rep(0, n)\n    dono <- rep(0, n)\n    for(i in 1:n) {\n        d[i] <- sum(surv.event[r == r[i]])\n        dono[i] <- sum(epita[r >= r[i]])\n    }\n    risk <- d/dono\n    risk1 <- d/dono^2\n    culrisk1 <- culrisk <- rep(0, n)\n    for(i in 1:n) {\n        culrisk[i] <- sum(unique(risk[r <= r[i]]))\n        culrisk1[i] <- sum(unique(risk1[r <= r[i]]))\n    }\n    lik <- sum((ita - log(dono)) * surv.event)\n    res <- c(lik, sum(surv.event))\n    names(res) <- c(\"logpl\", \"event\")\n    return(res)\n}",
        "complete": "logpl1 <- function(pred, surv.time, surv.event, verbose=FALSE) {\n    n <- length(pred)\n    r <- rank(surv.time)\n    epita <- exp(pred)\n    d <- tabulate(r, nbins=n)\n    dono <- rev(cumsum(rev(epita)))\n    risk <- d/dono\n    risk1 <- d/dono^2\n    culrisk <- cumsum(risk)\n    culrisk1 <- cumsum(risk1)\n    lik <- sum((pred - log(dono)) * surv.event)\n    res <- c(lik, sum(surv.event))\n    names(res) <- c(\"logpl\", \"event\")\n    return(res)\n}"
      },
      {
        "partial": "logpl <- function(pred, surv.time, surv.event, strata, na.rm=FALSE, verbose=FALSE) {\n    if(missing(strata)) { strata <- rep(1, length(pred)) } \n    cc.ix <- complete.cases(surv.time, surv.event, pred, strata)\n    surv.time <- surv.time[cc.ix]\n    surv.event <- surv.event[cc.ix]\n    pred <- pred[cc.ix]\n    strata <- strata[cc.ix]\n    n <- sum(cc.ix)\n    if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n    if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n    ss <- unique(strata)\n    if(length(ss) < 2) {\n        res <- logpl1(surv.time=surv.time, surv.event=surv.event, pred=pred, verbose=verbose)\n    }\n    else {\n        # Complete this part\n    }\n    return(res)\n}",
        "complete": "logpl <- function(pred, surv.time, surv.event, strata, na.rm=FALSE, verbose=FALSE) {\n    if(missing(strata)) { strata <- rep(1, length(pred)) } \n    cc.ix <- complete.cases(surv.time, surv.event, pred, strata)\n    surv.time <- surv.time[cc.ix]\n    surv.event <- surv.event[cc.ix]\n    pred <- pred[cc.ix]\n    strata <- strata[cc.ix]\n    n <- sum(cc.ix)\n    if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n    if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n    ss <- unique(strata)\n    if(length(ss) < 2) {\n        res <- logpl1(surv.time=surv.time, surv.event=surv.event, pred=pred, verbose=verbose)\n    }\n    else {\n        res <- sapply(ss, function(s) {\n            myx <- strata == s\n            logpl1(surv.time=surv.time[myx], surv.event=surv.event[myx], pred=pred[myx], verbose=verbose)\n        })\n        res <- c(sum(res[1,]), sum(res[2,]))\n        names(res) <- c(\"logpl\", \"event\")\n    }\n    return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeDrugSensitivity.R",
    "language": "R",
    "content": "#' @importFrom BiocParallel bpvec\n.calculateSensitivitiesStar <-\n  function (tSets = list(), exps=NULL,\n            cap=NA, na.rm=TRUE, area.type=c(\"Fitted\",\"Actual\"), nthread=1) {\n\n    # Set multicore options\n    op <- options()\n    options(mc.cores=nthread)\n    on.exit(options(op))\n\n    if (missing(area.type)) {\n      area.type <- \"Fitted\"\n    }\n    if (is.null(exps)) {\n      stop(\"expriments is empty!\")\n    }\n    for (study in names(tSets)) {\n      sensitivityProfiles(tSets[[study]])$auc_recomputed_star <- NA\n    }\n    if (!is.na(cap)) {\n      trunc <- TRUE\n    }else{\n      trunc <- FALSE\n    }\n\n    for(i in seq_len(nrow(exps))) {\n      ranges <- list()\n      for (study in names(tSets)) {\n        ranges[[study]] <- as.numeric(sensitivityRaw(tSets[[study]])[exps[i,study], ,\"Dose\"])\n      }\n      ranges <- .getCommonConcentrationRange(ranges)\n      names(ranges) <- names(tSets)\n      for(study in names(tSets)) {\n        myx <- as.numeric(sensitivityRaw(tSets[[study]])[exps[i, study],,\"Dose\"]) %in% ranges[[study]]\n        sensitivityRaw(tSets[[study]])[exps[i,study],!myx, ] <- NA\n\n      }\n    }\n    for(study in names(tSets)){\n\n      auc_recomputed_star <- unlist(bpvec(rownames(sensitivityRaw(tSets[[study]])),\n                                          function(experiment, exps, study, dataset, area.type){\n        if(!experiment %in% exps[,study]){return(NA_real_)}\n        return(computeAUC(concentration=as.numeric(dataset[experiment,,1]),\n                          viability=as.numeric(dataset[experiment,,2]),\n                          trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE, area.type=area.type)/100)\n\n\n      }, exps = exps, study = study, dataset = sensitivityRaw(tSets[[study]]), area.type=area.type))\n\n      sensitivityProfiles(tSets[[study]])$auc_recomputed_star <- auc_recomputed_star\n    }\n    return(tSets)\n  }\n\n## This function computes AUC for the whole raw sensitivity data of a pset\n.calculateFromRaw <- function(raw.sensitivity, cap=NA, nthread=1,\n                              family=c(\"normal\", \"Cauchy\"), scale = 0.07,\n                              n = 1) {\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  family <- match.arg(family)\n\n  AUC <- vector(length=dim(raw.sensitivity)[1])\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n  IC50 <- vector(length=dim(raw.sensitivity)[1])\n  names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n  if (!is.na(cap)) {trunc <- TRUE}else{trunc <- FALSE}\n\n  if (nthread == 1){\n    pars <- lapply(names(AUC), function(exp, raw.sensitivity, family, scale, n) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else{\n        logLogisticRegression(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Viability\"], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE, family=family, scale=scale, median_n=n)\n      }\n    },raw.sensitivity=raw.sensitivity, family = family, scale = scale, n = n)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(lapply(names(pars), function(exp,raw.sensitivity, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeAUC(concentration=raw.sensitivity[exp, , \"Dose\"], Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    },raw.sensitivity=raw.sensitivity, pars=pars))\n    IC50 <- unlist(lapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeIC50(Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    }, pars=pars))\n  } else {\n    pars <- BiocParallel::bplapply(names(AUC), function(exp, raw.sensitivity, family, scale, n, trunc) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else{\n        logLogisticRegression(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Viability\"], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE, family=family, scale=scale, median_n=n)\n      }\n    },raw.sensitivity=raw.sensitivity, family = family, scale = scale, n = n, trunc = trunc)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(BiocParallel::bplapply(names(pars), function(exp, raw.sensitivity, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeAUC(concentration=raw.sensitivity[exp, , \"Dose\"], Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    },raw.sensitivity=raw.sensitivity, pars=pars, trunc = trunc))\n    IC50 <- unlist(BiocParallel::bplapply(names(pars), function(exp, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeIC50(Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    }, pars=pars, trunc = trunc))\n  }\n\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n  names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n\n  return(list(\"AUC\"=AUC, \"IC50\"=IC50, \"pars\"=pars))\n}\n\n\n## This function computes intersected concentration range between a list of concentration ranges\n.getCommonConcentrationRange <- function(doses)\n{\n  min.dose <- 0\n  max.dose <- 10^100\n  for(i in seq_along(doses))\n  {\n    min.dose <- max(min.dose, min(as.numeric(doses[[i]]), na.rm = TRUE), na.rm = TRUE)\n    max.dose <- min(max.dose, max(as.numeric(doses[[i]]), na.rm = TRUE), na.rm = TRUE)\n  }\n\n  common.ranges <- list()\n  for(i in seq_along(doses))\n  {\n    common.ranges[[i]] <- doses[[i]][\n      which.min(abs(as.numeric(doses[[i]])-min.dose)):max(\n        which(abs(as.numeric(doses[[i]]) - max.dose)==min(abs(as.numeric(doses[[i]]) - max.dose), na.rm=TRUE)))]\n  }\n  return(common.ranges)\n}\n\n## predict viability from concentration data and curve parameters\n.Hill<-function(x, pars) {\n  return(pars[2] + (1 - pars[2]) / (1 + (10 ^ x / 10 ^ pars[3]) ^ pars[1]))\n}\n\n## calculate residual of fit\n## FIXME:: Why is this different from CoreGx?\n## FIXME:: Is this the same as PharmacoGx?\n.residual <- function(x, y, n, pars, scale = 0.07, family = c(\"normal\", \"Cauchy\"), trunc = FALSE) {\n  family <- match.arg(family)\n  Cauchy_flag = (family == \"Cauchy\") # Why?!\n  if (Cauchy_flag == FALSE) {\n    diffs <- .Hill(x, pars)-y\n    if (trunc == FALSE) {\n      return(sum(-log(CoreGx::.dmednnormals(diffs, n, scale))))\n    } else {\n      down_truncated <- abs(y) >= 1\n      up_truncated <- abs(y) <= 0\n\n      # For up truncated, integrate the cauchy dist up until -diff because anything less gets truncated to 0, and thus the residual is -diff, and the prob\n      # function becomes discrete\n      # For down_truncated, 1-cdf(diffs) = cdf(-diffs)\n\n      return(sum(-log(CoreGx::.dmednnormals(diffs[!(down_truncated | up_truncated)], n, scale))) + sum(-log(CoreGx::.edmednnormals(-diffs[up_truncated | down_truncated], n, scale))))\n\n    }\n\n  } else {\n    diffs <- .Hill(x, pars)-y\n    if (trunc == FALSE) {\n      return(sum(-log(CoreGx::.dmedncauchys(diffs, n, scale))))\n    } else {\n      down_truncated <- abs(y) >= 1\n      up_truncated <- abs(y) <= 0\n\n      # For up truncated, integrate the cauchy dist up until -diff because anything less gets truncated to 0, and thus the residual is -diff, and the prob\n      # function becomes discrete\n      # For down_truncated, 1-cdf(diffs) = cdf(-diffs)\n\n      return(sum(-log(CoreGx::.dmedncauchys(diffs[!(down_truncated | up_truncated)], n, scale))) + sum(-log(CoreGx::.edmedncauchys(-diffs[up_truncated | down_truncated], n, scale))))\n    }\n  }\n}\n\n## generate an initial guess for dose-response curve parameters by evaluating\n## the residuals at different lattice points of the search space\n## FIXME:: Why is this different from CoreGx?\n## FIXME:: Is this the same as PharmacoGx?\n.meshEval<-function(log_conc,\n                    viability,\n                    lower_bounds = c(0, 0, -6),\n                    upper_bounds = c(4, 1, 6),\n                    density = c(2, 10, 2),\n                    scale = 0.07,\n                    n = 1,\n                    family = c(\"normal\", \"Cauchy\"),\n                    trunc = FALSE) {\n  family <- match.arg(family)\n  guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n             pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n             pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3]))\n  guess_residual<- .residual(log_conc,\n                             viability,\n                             pars = guess,\n                             n=n,\n                             scale = scale,\n                             family = family,\n                             trunc = trunc)\n  for (i in seq(from = lower_bounds[1], to = upper_bounds[1], by = 1 / density[1])) {\n    for (j in seq(from = lower_bounds[2], to = upper_bounds[2], by = 1 / density[2])) {\n      for (k in seq(from = lower_bounds[3], to = upper_bounds[3], by = 1 / density[3])) {\n        test_guess_residual <- .residual(log_conc,\n                                         viability,\n                                         pars = c(i, j, k),\n                                         n=n,\n                                         scale = scale,\n                                         family = family,\n                                         trunc = trunc)\n        if(!is.finite(test_guess_residual)){\n          warning(paste0(\" Test Guess Residual is: \", test_guess_residual, \"\\n Other Pars: log_conc: \", paste(log_conc, collapse=\", \"), \"\\n Viability: \", paste(viability, collapse=\", \"), \"\\n Scale: \", scale, \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \", i, \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n        }\n        if(!length(test_guess_residual)){\n          warning(paste0(\" Test Guess Residual is: \", test_guess_residual, \"\\n Other Pars: log_conc: \", paste(log_conc, collapse=\", \"), \"\\n Viability: \", paste(viability, collapse=\", \"), \"\\n Scale: \", scale, \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \", i, \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n        }\n        if (test_guess_residual < guess_residual) {\n          guess <- c(i, j, k)\n          guess_residual <- test_guess_residual\n        }\n      }\n    }\n  }\n  return(guess)\n}\n\n######## TODO ADD computationg from  being passed in params\n#  Fits dose-response curves to data given by the user\n#  and returns the AUC of the fitted curve, normalized to the length of the concentration range.\n#\n#  @param concentration [vector] is a vector of drug concentrations.\n#\n#  @param viability [vector] is a vector whose entries are the viability values observed in the presence of the\n#  drug concentrations whose logarithms are in the corresponding entries of the log_conc, expressed as percentages\n#  of viability in the absence of any drug.\n#\n#  @param trunc [logical], if true, causes viability data to be truncated to lie between 0 and 1 before\n#  curve-fitting is performed.\n.computeAUCUnderFittedCurve <- function(concentration, viability, trunc=TRUE, verbose=FALSE) {\n\n  log_conc <- concentration\n  #FIT CURVE AND CALCULATE IC50\n  pars <- unlist(logLogisticRegression(log_conc,\n                                       viability,\n                                       conc_as_log = TRUE,\n                                       viability_as_pct = FALSE,\n                                       trunc = trunc))\n  x <- CoreGx::.getSupportVec(log_conc)\n  return(1 - trapz(x, .Hill(x, pars)) / (log_conc[length(log_conc)] - log_conc[1]))\n}\n#This function is being used in computeSlope\n.optimizeRegression <- function(x, y, x0 = -3, y0 = 100)\n{\n  beta1 = (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n  return(beta1)\n}\n\nupdateMaxConc <- function(tSet) {\n  sensitivityInfo(tSet)$max.conc <- apply(sensitivityRaw(tSet)[,,\"Dose\"], 1, max, na.rm=TRUE)\n  return(tSet)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.calculateSensitivitiesStar` function and how does it handle parallel processing?",
        "answer": "The `.calculateSensitivitiesStar` function calculates sensitivity profiles for drug experiments across multiple studies. It handles parallel processing by using the `bpvec` function from the BiocParallel package. The number of threads for parallel processing is controlled by the `nthread` parameter, which is used to set the `mc.cores` option. The function also ensures that the original options are restored using `on.exit(options(op))`."
      },
      {
        "question": "How does the `.calculateFromRaw` function differ in its approach to parallel processing compared to `.calculateSensitivitiesStar`?",
        "answer": "The `.calculateFromRaw` function uses a conditional approach to parallel processing. If `nthread` is 1, it uses regular `lapply` for sequential processing. If `nthread` is greater than 1, it switches to `BiocParallel::bplapply` for parallel processing. This allows the function to adapt its behavior based on the available computational resources and user preference."
      },
      {
        "question": "What is the purpose of the `.getCommonConcentrationRange` function and how does it work?",
        "answer": "The `.getCommonConcentrationRange` function finds the overlapping concentration range across multiple studies. It iterates through the provided dose ranges to find the maximum of the minimum doses and the minimum of the maximum doses. Then, it creates a list of common ranges for each study, including only the doses that fall within this intersected range. This ensures that sensitivity calculations are performed on a consistent concentration range across all studies."
      }
    ],
    "completion_tasks": [
      {
        "partial": "updateMaxConc <- function(tSet) {\n  sensitivityInfo(tSet)$max.conc <- apply(sensitivityRaw(tSet)[,,'Dose'], 1, max, na.rm=TRUE)\n  return(tSet)\n}",
        "complete": "updateMaxConc <- function(tSet) {\n  sensitivityInfo(tSet)$max.conc <- apply(sensitivityRaw(tSet)[,,'Dose'], 1, max, na.rm=TRUE)\n  return(tSet)\n}"
      },
      {
        "partial": ".optimizeRegression <- function(x, y, x0 = -3, y0 = 100) {\n  beta1 = (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n  return(beta1)\n}",
        "complete": ".optimizeRegression <- function(x, y, x0 = -3, y0 = 100) {\n  beta1 = (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n  return(beta1)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/cvpl.R",
    "language": "R",
    "content": "`cvpl` <-\nfunction(x, surv.time, surv.event, strata, nfold=1, setseed, na.rm=FALSE, verbose=FALSE) {\n\tx <- as.data.frame(x)\n\tif(is.null(dimnames(x))) { dimnames(x) <- list(names(surv.time), \"x\") }\n\tif(missing(strata)) { strata <- rep(1, length(surv.time)) }\n\t## remove NA values\n\tcc.ix <- complete.cases(x, surv.time, surv.event, strata)\n\tsurv.time <- surv.time[cc.ix]\n\tsurv.event <- surv.event[cc.ix]\n\tx <- x[cc.ix, ,drop=FALSE]\n\tstrata <- strata[cc.ix]\n\tnr <- sum(cc.ix)\n\tif (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n\tif(verbose) { message(sprintf(\"%i cases (%i cases are removed due to NA values)\", nr, sum(!cc.ix))) }\n\n\t## k-fold cross-validation\n\tif(nfold == 1) {\n\t\tk <- 1\n\t\tnfold <- nr\n\t} else { k <- floor(nr / nfold) }\n\n\t## set the random seed to use the same data partition\n\t## for the cross-validation\n\tif (!missing(setseed)) {\n\t\tset.seed(setseed)\n\t}\n\tsmpl <- sample(nr)\n\tres.cvpl <- 0\n\tconv <- pl <- NULL\n\tdd <- data.frame(\"stime\"=surv.time, \"sevent\"=surv.event, \"strat\"=strata, x)\n\tfor (i in 1:nfold) {\n\t\t#index of samples to hold out\n\t\tif(i == nfold) { s.ix <- smpl[c(((i - 1) * k + 1):nr)] } else { s.ix <- smpl[c(((i - 1) * k + 1):(i * k))] }\n\t\t## convergence ?\n\t\t#lwa <- options(\"warn\")$warn\n\t\t#options(\"warn\"=2)\n\t\tff <- sprintf(\"Surv(stime, sevent) ~ strata(strat) + %s\", paste(dimnames(dd)[[2]][4:ncol(dd)], collapse=\" + \"))\n\t\ttry(m <- coxph(formula=formula(ff), data=dd[-s.ix, , drop=FALSE]))\n\t\t#options(\"warn\"=lwa)\n\t\tif (class(m) != \"try-error\") {\n\t\t\tconv <- c(conv, TRUE)\n\t\t\tli <- m$loglik[2]\n\t\t\tmypred <- predict(object=m, newdata=dd)\n\t\t\tl <- logpl(surv.time=dd[ , \"stime\"], surv.event=dd[ , \"sevent\"], pred=mypred, strata=dd[ , \"strat\"])[1]\n\t\t} else {\n\t\t\tconv <- c(conv, FALSE)\n\t\t\tl <- NA\n\t\t\tli <- NA\n\t\t}\n\n\t\tres.cvpl <- res.cvpl + (li - l)\n\t\tpl <- c(pl, (li - l) / length(s.ix)) # dividing by the number of events instead?\n\t}\n\tres.cvpl <- res.cvpl / nr # dividing by the number of events instead?\n\tnames(conv) <- names(pl) <- paste(rep(\"split\", nfold), 1:nfold, sep=\".\")\n\treturn (list(\"cvpl\"=res.cvpl, \"pl\"=pl, \"convergence\"=conv, \"n\"=nr))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cvpl` function and what are its main input parameters?",
        "answer": "The `cvpl` function performs cross-validated partial likelihood for Cox proportional hazards models. Its main input parameters are:\n- `x`: A data frame of predictor variables\n- `surv.time`: Survival time vector\n- `surv.event`: Event indicator vector\n- `strata`: Stratification variable (optional)\n- `nfold`: Number of folds for cross-validation (default is 1)\n- `setseed`: Seed for random number generator (optional)\n- `na.rm`: Boolean to remove NA values (default is FALSE)\n- `verbose`: Boolean to print additional information (default is FALSE)"
      },
      {
        "question": "How does the function handle missing values (NA) in the input data?",
        "answer": "The function handles missing values as follows:\n1. It identifies complete cases using `complete.cases()` function.\n2. If `na.rm` is FALSE (default) and NA values are present, it stops execution with an error message.\n3. If `na.rm` is TRUE, it removes rows with NA values from all input data (x, surv.time, surv.event, strata).\n4. If `verbose` is TRUE, it prints a message indicating the number of cases removed due to NA values."
      },
      {
        "question": "Explain the cross-validation process implemented in this function.",
        "answer": "The cross-validation process in this function works as follows:\n1. If `nfold` is 1, it performs leave-one-out cross-validation (LOOCV).\n2. Otherwise, it divides the data into `nfold` subsets.\n3. For each fold:\n   a. It holds out one subset as the test set.\n   b. Fits a Cox proportional hazards model on the remaining data.\n   c. Calculates the partial likelihood for the held-out subset.\n4. It computes the cross-validated partial likelihood (CVPL) by summing the differences between the model log-likelihood and the partial likelihood for each fold, then dividing by the total number of samples.\n5. The function returns the CVPL, individual partial likelihoods for each fold, convergence status, and the number of samples used."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cvpl <- function(x, surv.time, surv.event, strata, nfold=1, setseed, na.rm=FALSE, verbose=FALSE) {\n  x <- as.data.frame(x)\n  if(is.null(dimnames(x))) { dimnames(x) <- list(names(surv.time), \"x\") }\n  if(missing(strata)) { strata <- rep(1, length(surv.time)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, strata)\n  surv.time <- surv.time[cc.ix]\n  surv.event <- surv.event[cc.ix]\n  x <- x[cc.ix, ,drop=FALSE]\n  strata <- strata[cc.ix]\n  nr <- sum(cc.ix)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases (%i cases are removed due to NA values)\", nr, sum(!cc.ix))) }\n\n  if(nfold == 1) {\n    k <- 1\n    nfold <- nr\n  } else { k <- floor(nr / nfold) }\n\n  if (!missing(setseed)) {\n    set.seed(setseed)\n  }\n  smpl <- sample(nr)\n  res.cvpl <- 0\n  conv <- pl <- NULL\n  dd <- data.frame(\"stime\"=surv.time, \"sevent\"=surv.event, \"strat\"=strata, x)\n\n  # Complete the function from here\n}",
        "complete": "cvpl <- function(x, surv.time, surv.event, strata, nfold=1, setseed, na.rm=FALSE, verbose=FALSE) {\n  x <- as.data.frame(x)\n  if(is.null(dimnames(x))) { dimnames(x) <- list(names(surv.time), \"x\") }\n  if(missing(strata)) { strata <- rep(1, length(surv.time)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, strata)\n  surv.time <- surv.time[cc.ix]\n  surv.event <- surv.event[cc.ix]\n  x <- x[cc.ix, ,drop=FALSE]\n  strata <- strata[cc.ix]\n  nr <- sum(cc.ix)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases (%i cases are removed due to NA values)\", nr, sum(!cc.ix))) }\n\n  if(nfold == 1) {\n    k <- 1\n    nfold <- nr\n  } else { k <- floor(nr / nfold) }\n\n  if (!missing(setseed)) {\n    set.seed(setseed)\n  }\n  smpl <- sample(nr)\n  res.cvpl <- 0\n  conv <- pl <- NULL\n  dd <- data.frame(\"stime\"=surv.time, \"sevent\"=surv.event, \"strat\"=strata, x)\n\n  for (i in 1:nfold) {\n    s.ix <- if(i == nfold) smpl[((i - 1) * k + 1):nr] else smpl[((i - 1) * k + 1):(i * k)]\n    ff <- sprintf(\"Surv(stime, sevent) ~ strata(strat) + %s\", paste(names(dd)[4:ncol(dd)], collapse=\" + \"))\n    m <- try(coxph(formula=formula(ff), data=dd[-s.ix, , drop=FALSE]))\n    if (!inherits(m, \"try-error\")) {\n      conv <- c(conv, TRUE)\n      li <- m$loglik[2]\n      mypred <- predict(object=m, newdata=dd)\n      l <- logpl(surv.time=dd[, \"stime\"], surv.event=dd[, \"sevent\"], pred=mypred, strata=dd[, \"strat\"])[1]\n    } else {\n      conv <- c(conv, FALSE)\n      l <- li <- NA\n    }\n    res.cvpl <- res.cvpl + (li - l)\n    pl <- c(pl, (li - l) / length(s.ix))\n  }\n  res.cvpl <- res.cvpl / nr\n  names(conv) <- names(pl) <- paste(\"split\", 1:nfold, sep=\".\")\n  list(cvpl=res.cvpl, pl=pl, convergence=conv, n=nr)\n}"
      },
      {
        "partial": "logpl <- function(surv.time, surv.event, pred, strata) {\n  # Implement the logpl function here\n}",
        "complete": "logpl <- function(surv.time, surv.event, pred, strata) {\n  ord <- order(surv.time)\n  surv.time <- surv.time[ord]\n  surv.event <- surv.event[ord]\n  pred <- pred[ord]\n  strata <- strata[ord]\n  ustrata <- unique(strata)\n  res <- 0\n  for (s in ustrata) {\n    ix <- which(strata == s)\n    ti <- surv.time[ix]\n    di <- surv.event[ix]\n    ri <- pred[ix]\n    ixe <- which(di == 1)\n    res <- res + sum(ri[ixe] - log(cumsum(exp(rev(ri)))[length(ri) - ixe + 1]))\n  }\n  c(res, length(ustrata))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/score2proba.R",
    "language": "R",
    "content": "`score2proba` <-\nfunction(data.tr, score, yr, method=c(\"cox\", \"prodlim\"), conf.int=0.95, which.est=c(\"point\", \"lower\", \"upper\")) {\n\tmethod <- match.arg(method)\n\twhich.est <- match.arg(which.est)\n\tcc.ix <- complete.cases(score)\n\tscore2 <- score[cc.ix]\n\tpred <- rep(NA, length(score))\n\tnames(pred) <- names(score)\n\tswitch(method,\n\t\"cox\"={\n\t\tpredm <- coxph(Surv(time, event) ~ score, data=data.tr)\n\t\tsf <- survfit(predm, newdata=data.frame(\"score\"=score2), conf.int=conf.int)\n\t\tpred[cc.ix] <- getsurv2(sf=sf, time=yr, which.est=which.est)\n\t},\n\t\"prodlim\"={\n\t\t#require(prodlim)\n\t\t#require(KernSmooth)\n\t\tif(which.est != \"point\") { stop(\"not implemented yet!\") }\n\t\tpredm <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr, conf.int=conf.int)\n\t\tpred[cc.ix] <- unlist(predict(predm, newdata=data.frame(\"score\"=score2), times=yr))\n\t})\n\treturn(pred)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `score2proba` function in R?",
        "answer": "The `score2proba` function is designed to convert risk scores into survival probabilities at a specified time point. It uses either Cox proportional hazards model or product-limit estimation method to calculate these probabilities based on the input data, scores, and specified time point."
      },
      {
        "question": "How does the function handle missing data in the input score vector?",
        "answer": "The function handles missing data by using `complete.cases(score)` to identify non-missing values. It performs calculations only on the complete cases and then assigns the results back to the original vector, preserving NA values for missing data points. This ensures that the output vector has the same length and structure as the input score vector."
      },
      {
        "question": "What is the difference between the 'cox' and 'prodlim' methods in this function?",
        "answer": "The 'cox' method uses the Cox proportional hazards model (coxph) and survfit functions to estimate survival probabilities. It allows for point estimates as well as lower and upper confidence intervals. The 'prodlim' method uses the product-limit estimation (prodlim function) and currently only supports point estimates. The 'prodlim' method also uses kernel smoothing for its calculations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "score2proba <- function(data.tr, score, yr, method=c(\"cox\", \"prodlim\"), conf.int=0.95, which.est=c(\"point\", \"lower\", \"upper\")) {\n  method <- match.arg(method)\n  which.est <- match.arg(which.est)\n  cc.ix <- complete.cases(score)\n  score2 <- score[cc.ix]\n  pred <- rep(NA, length(score))\n  names(pred) <- names(score)\n  switch(method,\n    \"cox\"={\n      # Complete the cox method implementation\n    },\n    \"prodlim\"={\n      # Complete the prodlim method implementation\n    })\n  return(pred)\n}",
        "complete": "score2proba <- function(data.tr, score, yr, method=c(\"cox\", \"prodlim\"), conf.int=0.95, which.est=c(\"point\", \"lower\", \"upper\")) {\n  method <- match.arg(method)\n  which.est <- match.arg(which.est)\n  cc.ix <- complete.cases(score)\n  score2 <- score[cc.ix]\n  pred <- rep(NA, length(score))\n  names(pred) <- names(score)\n  switch(method,\n    \"cox\"={\n      predm <- coxph(Surv(time, event) ~ score, data=data.tr)\n      sf <- survfit(predm, newdata=data.frame(\"score\"=score2), conf.int=conf.int)\n      pred[cc.ix] <- getsurv2(sf=sf, time=yr, which.est=which.est)\n    },\n    \"prodlim\"={\n      if(which.est != \"point\") stop(\"not implemented yet!\")\n      predm <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr, conf.int=conf.int)\n      pred[cc.ix] <- unlist(predict(predm, newdata=data.frame(\"score\"=score2), times=yr))\n    })\n  return(pred)\n}"
      },
      {
        "partial": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  # Complete the function implementation\n}",
        "complete": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  ix <- sapply(time, function(t) which.min(abs(sf$time - t)))\n  switch(which.est,\n    \"point\" = sf$surv[ix],\n    \"lower\" = sf$lower[ix],\n    \"upper\" = sf$upper[ix])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/stab.fs.ranking.R",
    "language": "R",
    "content": "#' @title Function to quantify stability of feature ranking\n#'\n#' @description\n#' This function computes several indexes to quantify feature ranking\n#'   stability for several number of selected features. This is usually\n#'   estimated through perturbation of the original dataset by generating\n#'   multiple sets of selected features.\n#'\n#' @usage\n#' stab.fs.ranking(fsets, sizes, N, method = c(\"kuncheva\", \"davis\"), ...)\n#'\n#' @param fsets\tlist or matrix of sets of selected features (in rows),\n#'   each ranking must have the same size.\n#' @param sizes\tNumber of top-ranked features for which the stability\n#'   index must be computed.\n#' @param N\ttotal number of features on which feature selection is performed\n#' @param method\tstability index (see details section).\n#' @param ...\tadditional parameters passed to stability index (penalty\n#'   that is a numeric for Davis' stability index, see details section).\n#'\n#' @details\n#' Stability indices may use different parameters. In this version only the\n#'   Davis index requires an additional parameter that is penalty, a numeric\n#'   value used as penalty term.\n#' Kuncheva index (kuncheva) lays in \\[-1, 1\\], An index of -1 means no\n#'   intersection between sets of selected features, +1 means that all the\n#'   same features are always selected and 0 is the expected stability of a\n#'   random feature selection.\n#' Davis index (davis) lays in \\[0,1\\], With a penalty term equal to 0, an index\n#'   of 0 means no intersection between sets of selected features and +1 means\n#'   that all the same features are always selected. A penalty of 1 is usually\n#'   used so that a feature selection performed with no or all features has a\n#'   Davis stability index equals to 0. None estimate of the expected Davis\n#'   stability index of a random feature selection was published.\n#'\n#' @return\n#' A vector of numeric that are stability indices for each size of the sets\n#'   of selected features given the rankings.\n#'\n#' @references\n#' Davis CA, Gerick F, Hintermair V, Friedel CC, Fundel K, Kuffner R,\n#'   Zimmer R (2006) \"Reliable gene signatures for microarray classification:\n#'  assessment of stability and performance\", Bioinformatics, 22(19):356-2363.\n#' Kuncheva LI (2007) \"A stability index for feature selection\", AIAP'07:\n#'   Proceedings of the 25th conference on Proceedings of the 25th IASTED\n#'   International Multi-Conference, pages 390-395.\n#'\n#' @seealso\n#' [genefu::stab.fs]\n#'\n#' @examples\n#' # 100 random selection of 50 features from a set of 10,000 features\n#' fsets <- lapply(as.list(1:100), function(x, size=50, N=10000) {\n#'   return(sample(1:N, size, replace=FALSE))} )\n#' names(fsets) <- paste(\"fsel\", 1:length(fsets), sep=\".\")\n#'\n#' # Kuncheva index\n#' stab.fs.ranking(fsets=fsets, sizes=c(1, 10, 20, 30, 40, 50),\n#'   N=10000, method=\"kuncheva\")\n#' # close to 0 as expected for a random feature selection\n#'\n#' # Davis index\n#' stab.fs.ranking(fsets=fsets, sizes=c(1, 10, 20, 30, 40, 50),\n#'   N=10000, method=\"davis\", penalty=1)\n#'\n#' @md\n#' @export\nstab.fs.ranking <-\nfunction(fsets, sizes, N, method=c(\"kuncheva\", \"davis\"), ...) {\n\n\t####################\n\t## internal functions\n\t####################\n\n\tkuncheva.stab.ranking <- function(fsets, N, x) {\n\t\tss <- x\n\t\tfsets <- fsets[ , 1:ss, drop=FALSE]\n\t\tkk <- nrow(fsets)\n\t\tKI <- function(f1, f2, ss, NN) {\n\t\t\t#if(length(f1) != length(f2)) { stop(\"length of the two sets of selected features must be identical!\") }\n\t\t\t#ss <- length(f1)\n\t\t\tif(ss == NN) { return(NA) }\n\t\t\trr <- length(intersect(f1, f2))\n\t\t\tki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n\t\t\treturn(ki.est)\n\t\t}\n\n\t\tstab.res <- 0\n\t\tfor(i in 1:(kk - 1)) {\n\t\t\tfor(j in (i + 1):kk) {\n\t\t\t\tstab.res <- stab.res + KI(f1=fsets[i, ], f2=fsets[j, ], ss=ss, NN=N)\n\t\t\t}\n\t\t}\n\t\treturn((2 * stab.res) / (kk * (kk - 1)))\n\t}\n\n\tdavis.stab.ranking <- function(fsets, N, x, penalty=1) {\n\t\tss <- x\n\t\tfsets <- fsets[ , 1:ss, drop=FALSE]\n\t\tkk <- nrow(fsets)\n\t\tstab.res <- sum(sort(table(fsets), decreasing=TRUE)[1:ss]) / (kk * ss)\n\t\treturn(stab.res - penalty * (ss / N))\n\t}\n\n\t####################\n\n\tmethod <- match.arg(method)\n\tif(is.list(fsets)) { ## transform list into matrix\n\t\tNn <- unique(unlist(lapply(fsets, length)))\n\t\tif(length(Nn) > 1) { stop(\"length of sets of selected features must be identical!\") }\n\t\tnam <- names(fsets)\n\t\tfsets <- t(sapply(X=1:length(fsets), FUN=function(y, x) { return(y[[x]]) }, y=fsets))\n\t\tdimnames(fsets) <- list(nam, paste(\"rank\", 1:Nn, sep=\".\"))\n\t} else { Nn <- ncol(fsets) }\n\tif(missing(N)) { N <- Nn }\n\tif(missing(sizes)) { sizes <- 1:Nn }\n\tsizes <- sizes[sizes <= Nn]\n\n\tswitch(method,\n\t\t\"kuncheva\"={\n\t\t\tstab <- unlist(sapply(X=sizes, FUN=kuncheva.stab.ranking, fsets=fsets, N=N))\n\t\t},\n\t\t\"davis\"={\n\t\t\t stab <- unlist(sapply(X=sizes, FUN=davis.stab.ranking, fsets=fsets, N=N, ...))\n\t\t})\n\t\tnames(stab) <- paste(\"size\", sizes, sep=\".\")\n\t\treturn(stab)\n}\n\n## k <- 100; fsets <- NULL; for(i in 1:k) { fsets <- c(fsets, list(sample(paste(\"feature\", 1:1000, sep=\".\")))) }; names(fsets) <- paste(\"rand\", 1:k, sep=\".\")\n## stab.fs.ranking(fsets=fsets, sizes=1:10, method=\"kuncheva\", penalty=1)",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `stab.fs.ranking` function and what are its main input parameters?",
        "answer": "The `stab.fs.ranking` function quantifies the stability of feature ranking. It computes stability indexes for several numbers of selected features, typically estimated through perturbation of the original dataset. The main input parameters are:\n- `fsets`: A list or matrix of sets of selected features\n- `sizes`: Number of top-ranked features for which the stability index must be computed\n- `N`: Total number of features on which feature selection is performed\n- `method`: Stability index to use (either 'kuncheva' or 'davis')"
      },
      {
        "question": "Explain the difference between the Kuncheva index and the Davis index as implemented in this function.",
        "answer": "The Kuncheva and Davis indices are two different methods for measuring feature ranking stability:\n\n1. Kuncheva index:\n   - Range: [-1, 1]\n   - -1 means no intersection between sets of selected features\n   - +1 means all the same features are always selected\n   - 0 is the expected stability of a random feature selection\n\n2. Davis index:\n   - Range: [0, 1]\n   - With a penalty term of 0, 0 means no intersection between sets of selected features\n   - +1 means all the same features are always selected\n   - Typically uses a penalty of 1 so that feature selection with no or all features has a Davis stability index of 0\n   - No published estimate of the expected Davis stability index for random feature selection"
      },
      {
        "question": "How does the function handle different input types for the `fsets` parameter, and what internal data transformation does it perform?",
        "answer": "The function handles different input types for the `fsets` parameter as follows:\n\n1. If `fsets` is a list:\n   - It checks if all sets have the same length\n   - Transforms the list into a matrix using `t(sapply(...))`\n   - Preserves the names of the list as row names in the matrix\n   - Creates column names as 'rank.1', 'rank.2', etc.\n\n2. If `fsets` is already a matrix:\n   - It uses the matrix as-is\n\nIn both cases, the function determines `Nn` (number of features in each set) and uses it to set `N` (total number of features) if not provided. The resulting matrix is then used in the stability index calculations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "stab.fs.ranking <- function(fsets, sizes, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  method <- match.arg(method)\n  if(is.list(fsets)) {\n    Nn <- unique(unlist(lapply(fsets, length)))\n    if(length(Nn) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    nam <- names(fsets)\n    fsets <- t(sapply(X=1:length(fsets), FUN=function(y, x) { return(y[[x]]) }, y=fsets))\n    dimnames(fsets) <- list(nam, paste(\"rank\", 1:Nn, sep=\".\"))\n  } else { Nn <- ncol(fsets) }\n  if(missing(N)) { N <- Nn }\n  if(missing(sizes)) { sizes <- 1:Nn }\n  sizes <- sizes[sizes <= Nn]\n\n  # Complete the function here\n}",
        "complete": "stab.fs.ranking <- function(fsets, sizes, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  method <- match.arg(method)\n  if(is.list(fsets)) {\n    Nn <- unique(unlist(lapply(fsets, length)))\n    if(length(Nn) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    nam <- names(fsets)\n    fsets <- t(sapply(X=1:length(fsets), FUN=function(y, x) { return(y[[x]]) }, y=fsets))\n    dimnames(fsets) <- list(nam, paste(\"rank\", 1:Nn, sep=\".\"))\n  } else { Nn <- ncol(fsets) }\n  if(missing(N)) { N <- Nn }\n  if(missing(sizes)) { sizes <- 1:Nn }\n  sizes <- sizes[sizes <= Nn]\n\n  stab <- switch(method,\n    \"kuncheva\" = sapply(sizes, kuncheva.stab.ranking, fsets=fsets, N=N),\n    \"davis\" = sapply(sizes, davis.stab.ranking, fsets=fsets, N=N, ...)\n  )\n  names(stab) <- paste(\"size\", sizes, sep=\".\")\n  return(stab)\n}"
      },
      {
        "partial": "kuncheva.stab.ranking <- function(fsets, N, x) {\n  ss <- x\n  fsets <- fsets[, 1:ss, drop=FALSE]\n  kk <- nrow(fsets)\n  KI <- function(f1, f2, ss, NN) {\n    if(ss == NN) { return(NA) }\n    rr <- length(intersect(f1, f2))\n    ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n    return(ki.est)\n  }\n\n  # Complete the function here\n}",
        "complete": "kuncheva.stab.ranking <- function(fsets, N, x) {\n  ss <- x\n  fsets <- fsets[, 1:ss, drop=FALSE]\n  kk <- nrow(fsets)\n  KI <- function(f1, f2, ss, NN) {\n    if(ss == NN) { return(NA) }\n    rr <- length(intersect(f1, f2))\n    ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n    return(ki.est)\n  }\n\n  stab.res <- sum(sapply(1:(kk-1), function(i) {\n    sapply((i+1):kk, function(j) KI(fsets[i,], fsets[j,], ss, N))\n  }))\n  return((2 * stab.res) / (kk * (kk - 1)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/ToxicoSet-accessors.R",
    "language": "R",
    "content": "#' @include ToxicoSet-class.R\nNULL\n\n# Navigating this file:\n# - Slot section names start with ----\n# - Method section names start with ==\n#\n# As a result, you can use Ctrl + f to find the slot or method you are looking\n# for quickly, assuming you know its name.\n#\n# For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n# method, while Ctrl + f '---- molecularProfiles' would take you to the slot\n# section.\n\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n#' @name ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_accessors(class_='CoreSet')\n#' @eval CoreGx:::.parseToRoxygen(\"@examples data({data_})\", data_=.local_data)\nNULL\n\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ==============\n## ---- drug slot\n## --------------\n\n\n##\n## == drugInfo\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo\n#' @aliases drugInfo\n#' @export\ndrugInfo <- function(...) treatmentInfo(...)\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo<-\n#' @aliases drugInfo<-\n#' @export\n`drugInfo<-` <- function(..., value) `treatmentInfo<-`(..., value=value)\n\n\n\n##\n## == drugNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames\n#' @aliases drugNames\n#' @export\ndrugNames <- function(...) treatmentNames(...)\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames<-\n#' @aliases drugNames<-\n#' @export\n`drugNames<-` <- function(..., value) `treatmentNames<-`(..., value=value)\n\n\n## ====================\n## ---- annotation slot\n## --------------------\n\n\n##\n## == annotation\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation\nsetMethod('annotation', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation<-\nsetReplaceMethod(\"annotation\", signature(\"ToxicoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == dateCreated\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated\nsetMethod('dateCreated', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated<-\nsetReplaceMethod('dateCreated', signature(object=\"ToxicoSet\", value=\"character\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## === name\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name\nsetMethod('name', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name<-\nsetReplaceMethod('name', signature(\"ToxicoSet\"), function(object, value) {\n    object <- callNextMethod(object, value=value)\n    return(invisible(object))\n})\n\n## ==============\n## ---- sample slot\n## --------------\n\n\n##\n## == sampleInfo\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleInfo(class_=.local_class,\n#' sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleInfo\n#' @importFrom CoreGx cellInfo\n#' @export\nsetMethod(\"sampleInfo\", \"ToxicoSet\", function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleInfo(class_=.local_class,\n#' data_=.local_data, sample_=\"cell\")\n#' @importMethodsFrom CoreGx sampleInfo<-\n#' @importFrom CoreGx cellInfo<-\n#' @export\nsetReplaceMethod(\"sampleInfo\", signature(object=\"ToxicoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\n\n##\n## == sampleNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames\nsetMethod(\"sampleNames\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames<-\nsetReplaceMethod(\"sampleNames\", signature(object=\"ToxicoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n\n## ------------------\n## ---- curation slot\n\n\n##\n## == curation\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_curation(class_=.local_class,\n#' data_=.local_data, details_=\"Contains three `data.frame`s, 'cell' with\n#' cell-line ids and 'tissue' with tissue ids and 'drug' with drug ids.\")\n#' @importMethodsFrom CoreGx curation\nsetMethod('curation', signature(object=\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_curation(class_=.local_class,\n#' data_=.local_data, details_=\"For a `ToxicoSet` object the slot should\n#' contain tissue, cell-line and drug id `data.frame`s.\")\n#' @importMethodsFrom CoreGx curation<-\nsetReplaceMethod(\"curation\", signature(object=\"ToxicoSet\", value=\"list\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ----------------------\n## ---- datasetType slot\n\n\n#\n# == datasetType\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType\nsetMethod(\"datasetType\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType<-\nsetReplaceMethod(\"datasetType\", signature(object=\"ToxicoSet\",\n    value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ---------------------------\n## ---- molecularProfiles slot\n\n\n##\n## == molecularProfiles\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles\nsetMethod(molecularProfiles, \"ToxicoSet\", function(object, mDataType, assay)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles<-\nsetReplaceMethod(\"molecularProfiles\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\", assay=\"character\", value=\"matrix\"),\n    function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\nsetReplaceMethod(\"molecularProfiles\",\n    signature(object=\"ToxicoSet\", mDataType =\"character\", assay=\"missing\",\n        value=\"matrix\"), function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\n\n\n##\n## == featureInfo\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_featureInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx featureInfo\nsetMethod(featureInfo, \"ToxicoSet\", function(object, mDataType) {\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_featureInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx featureInfo<-\nsetReplaceMethod(\"featureInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\",value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"featureInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\",value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n\n##\n## == phenoInfo\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo\nsetMethod('phenoInfo', signature(object='ToxicoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo<-\nsetReplaceMethod(\"phenoInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\", value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"phenoInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\", value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == fNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames\nsetMethod('fNames', signature(object='ToxicoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames<-\nsetReplaceMethod('fNames', signature(object='ToxicoSet', mDataType='character',\n    value='character'), function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == mDataNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames\nsetMethod(\"mDataNames\", \"ToxicoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames<-\nsetReplaceMethod(\"mDataNames\", \"ToxicoSet\", function(object, value){\n    callNextMethod(object=object, value=value)\n})\n\n\n\n##\n## == molecularProfilesSlot\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot\nsetMethod(\"molecularProfilesSlot\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot<-\nsetReplaceMethod(\"molecularProfilesSlot\", signature(\"ToxicoSet\", \"list_OR_MAE\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n# ---------------------\n## ---- sensitivity slot\n\n\n##\n## == sensitivityInfo\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo\nsetMethod('sensitivityInfo', signature(\"ToxicoSet\"),\n    function(object, dimension, ...)\n{\n    callNextMethod(object=object, dimension=dimension, ...)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo<-\nsetReplaceMethod(\"sensitivityInfo\", signature(object=\"ToxicoSet\",\n    value=\"data.frame\"), function(object, dimension, ..., value)\n{\n    callNextMethod(object=object, dimension=dimension, ..., value=value)\n})\n\n\n##\n## == sensitvityMeasures\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityMeasures(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityMeasures\nsetMethod('sensitivityMeasures', signature(object=\"ToxicoSet\"),\n    function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitityMeasures(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('sensitivityMeasures',\n    signature(object='ToxicoSet', value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensitivityProfiles\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles\nsetMethod('sensitivityProfiles', signature(object=\"ToxicoSet\"), function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles<-\nsetReplaceMethod(\"sensitivityProfiles\",\n    signature(object=\"ToxicoSet\", value=\"data.frame\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n#\n# == sensitivityRaw\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw\nsetMethod(\"sensitivityRaw\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw<-\nsetReplaceMethod('sensitivityRaw', signature(\"ToxicoSet\", \"array\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n#\n# == treatmentResponse\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentResponse(class_=.local_class,\n#'   data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentResponse\nsetMethod(\"treatmentResponse\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n\n\n#' @rdname ToxicoSet-accessors\n#' @importMethodsFrom CoreGx treatmentResponse<-\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentResponse(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('treatmentResponse', signature(object='ToxicoSet',\n    value='list_OR_LongTable'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensNumber\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber\nsetMethod('sensNumber', \"ToxicoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber<-\nsetReplaceMethod('sensNumber', signature(object=\"ToxicoSet\", value=\"matrix\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n## ======================\n## ---- perturbation slot\n\n\n##\n## == pertNumber\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber\nsetMethod('pertNumber', signature(object='ToxicoSet'), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber<-\nsetReplaceMethod('pertNumber', signature(object='ToxicoSet', value=\"array\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drugInfo` and `drugInfo<-` functions in the ToxicoSet class?",
        "answer": "The `drugInfo` and `drugInfo<-` functions are accessor methods for the `drug` slot in the ToxicoSet class. They are aliases for the `treatmentInfo` and `treatmentInfo<-` methods, respectively, allowing users to get and set drug information in a ToxicoSet object."
      },
      {
        "question": "How does the `molecularProfiles` method handle different types of molecular data in a ToxicoSet object?",
        "answer": "The `molecularProfiles` method for ToxicoSet objects takes two arguments: `mDataType` and `assay`. This allows users to retrieve specific molecular profile data based on the type of molecular data (e.g., RNA, DNA) and the specific assay used. The method is flexible to accommodate various types of molecular data stored in the ToxicoSet object."
      },
      {
        "question": "What is the purpose of the `curation` slot in the ToxicoSet class, and what kind of data does it contain?",
        "answer": "The `curation` slot in the ToxicoSet class contains three data frames: 'cell' with cell-line IDs, 'tissue' with tissue IDs, and 'drug' with drug IDs. This slot is used to store curated information about the samples, tissues, and drugs used in the toxicogenomic dataset, providing a structured way to access and manage this metadata."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('annotation', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"ToxicoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})",
        "complete": "setMethod('annotation', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"ToxicoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\nsetMethod('dateCreated', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod('dateCreated', signature(object=\"ToxicoSet\", value=\"character\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})"
      },
      {
        "partial": "setMethod(\"sampleInfo\", \"ToxicoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"ToxicoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})",
        "complete": "setMethod(\"sampleInfo\", \"ToxicoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"ToxicoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\nsetMethod(\"sampleNames\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleNames\", signature(object=\"ToxicoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/hr.comp.R",
    "language": "R",
    "content": "`hr.comp` <-\nfunction(hr1, hr2) {\n\tif(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n\tn <- hr1$n\n\tx1 <- hr1$data$x\n\tx2 <- hr2$data$x\n\tbeta1 <- hr1$coef\n\tbeta2 <- hr2$coef\n\tse1 <- hr1$se\n\tse2 <- hr2$se\n\tr <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n\tif(abs(r) < 1) {\n\t\tt.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hr.comp` function and what are its main inputs?",
        "answer": "The `hr.comp` function is designed to compare two hazard ratios. It takes two inputs, `hr1` and `hr2`, which are expected to be objects containing hazard ratio data. The function checks if the hazard ratios are computed from the same number of samples, calculates a test statistic, and returns a p-value along with the exponential of the coefficients for both hazard ratios."
      },
      {
        "question": "How does the function handle the case when the correlation between `x1` and `x2` is perfect (1 or -1)?",
        "answer": "When the absolute value of the correlation coefficient `r` between `x1` and `x2` is 1 (perfect correlation), the function sets the p-value (`diff.ci.p`) to 1 instead of calculating the test statistic. This is likely because perfect correlation would lead to division by zero in the t-statistic calculation, so this case is handled separately."
      },
      {
        "question": "What statistical test is being performed in this function, and how is the test statistic calculated?",
        "answer": "The function is performing a t-test to compare two hazard ratios. The test statistic is calculated as: t.stat = (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2), where beta1 and beta2 are the coefficients, se1 and se2 are the standard errors, and r is the Spearman correlation between x1 and x2. This test statistic accounts for the correlation between the two hazard ratios."
      }
    ],
    "completion_tasks": [
      {
        "partial": "hr.comp <- function(hr1, hr2) {\n  if(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  # Complete the return statement\n}",
        "complete": "hr.comp <- function(hr1, hr2) {\n  if(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      },
      {
        "partial": "hr.comp <- function(hr1, hr2) {\n  # Add input validation\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  # Complete the function\n}",
        "complete": "hr.comp <- function(hr1, hr2) {\n  if(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/readArray.R",
    "language": "R",
    "content": "#' @title Overlap two datasets\n#'\n#' @description\n#' Formatting function to read arrays and format for use in the claudinLow classifier.\n#'\n#' @usage\n#' readArray(dataFile,designFile=NA,hr=1,impute=TRUE,method=\"mean\")\n#'\n#' @param dataFile file with matrix to be read.\n#' @param designFile Design of file.\n#' @param hr Header rows as Present (2) or Absent (1).\n#' @param impute whether data will be imputed or not.\n#' @param method Default method is \"mean\".\n#'\n#' @return\n#' A list\n#'\n#' @references\n#' citation(\"claudinLow\")\n#'\n#' @seealso\n#' [genefu::claudinLow]\n#'\n#' @md\n#' @importFrom impute impute.knn\n#' @export\nreadArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE,\n    method=\"mean\")\n{\n\n  headerRows <- hr\n\n  x<-read.table(dataFile,sep=\"\\t\",header=FALSE,fill=TRUE,stringsAsFactors=FALSE)\n\n  if(headerRows==1){\n    sampleNames<-as.vector(t(x[1,-1]))\n    x<-x[-1,]\n    classes<-NULL\n    ids<-x[,1]\n    xd<-x[,-1]\n    xd<-apply(xd,2,as.numeric)\n    xd<-collapseIDs(xd,ids,method)\n  }else{\n    sampleNames<-as.vector(t(x[1,-1]))\n    x<-x[-1,]\n\n    classes<-x[1:(headerRows-1),]\n    dimnames(classes)[[1]]<-classes[,1]\n    classes<-classes[,-1]\n    classes[classes==\"\"]<-NA\n    classes<-t(classes)\n    rownames(classes)<-sampleNames\n    classes<-as.data.frame(classes)\n\n    xd<-x[(-1:-(headerRows-1)),]\n    ids<-as.vector(t(xd[,1]))\n    xd<-xd[,-1]\n    xd<-apply(xd,2,as.numeric)\n    xd<-collapseIDs(xd,ids,method)\n  }\n\n  features<- dim(xd)[1]\n  samples<- dim(xd)[2]\n  geneNames<-rownames(xd)\n  xd<-apply(xd,2,as.numeric)\n  rownames(xd)<-geneNames\n  colnames(xd)<-sampleNames\n\n  if(!is.na(designFile)){\n    x<-read.table(designFile,sep=\"\\t\", header=TRUE, row.names=1, fill=TRUE,\n                  stringsAsFactors=FALSE)\n    xd<-xd[,sort.list(colnames(xd))]\n    xd<-xd[,colnames(xd) %in% rownames(x)]\n    x<-x[rownames(x) %in% colnames(xd),]\n    x<-x[sort.list(rownames(x)),]\n    classes<-as.data.frame(x)\n  }\n\n  if(sum(apply(xd,2,is.na))>0 & impute){\n    #library(impute)\n    allAnn<-dimnames(xd)\n    data.imputed<-impute.knn(as.matrix(xd))$data\n    xd<-data.imputed[1:features,]\n    dimnames(xd)<-allAnn\n  }\n\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'readArray' function and what are its main parameters?",
        "answer": "The 'readArray' function is designed to read and format data arrays for use in the claudinLow classifier. Its main parameters are:\n1. dataFile: the file containing the matrix to be read\n2. designFile: the design of the file (optional)\n3. hr: header rows, can be 1 (absent) or 2 (present)\n4. impute: whether data should be imputed (default is TRUE)\n5. method: method for collapsing IDs (default is 'mean')"
      },
      {
        "question": "How does the function handle different header row configurations (hr=1 vs hr=2)?",
        "answer": "When hr=1 (header rows absent):\n- It treats the first row as sample names\n- There are no class labels\n- IDs are taken from the first column\n\nWhen hr=2 (header rows present):\n- The first row is still treated as sample names\n- Class labels are extracted from the rows between the sample names and data\n- IDs are taken from the first column of the data portion\n\nIn both cases, the function processes the data accordingly, extracting relevant information and formatting it for further use."
      },
      {
        "question": "What does the function do when imputation is required, and what external package does it use for this purpose?",
        "answer": "When imputation is required (i.e., there are NA values in the data and impute=TRUE), the function uses the impute.knn function from the 'impute' package to perform k-nearest neighbor imputation. The process is as follows:\n1. It checks if there are any NA values in the data\n2. If imputation is needed, it saves the original dimension names\n3. It calls impute.knn on the data matrix\n4. It extracts the imputed data and restores the original dimension names\n\nThis ensures that missing values are filled in using a reliable imputation method, maintaining the integrity of the dataset for further analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  headerRows <- hr\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  if(headerRows == 1) {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- NULL\n    ids <- x[,1]\n    xd <- x[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  } else {\n    # Complete the code for headerRows != 1\n  }\n\n  # Complete the rest of the function\n}",
        "complete": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  headerRows <- hr\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  if(headerRows == 1) {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- NULL\n    ids <- x[,1]\n    xd <- x[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  } else {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- x[1:(headerRows-1),]\n    dimnames(classes)[[1]] <- classes[,1]\n    classes <- classes[,-1]\n    classes[classes==\"\"] <- NA\n    classes <- t(classes)\n    rownames(classes) <- sampleNames\n    classes <- as.data.frame(classes)\n    xd <- x[(-1:-(headerRows-1)),]\n    ids <- as.vector(t(xd[,1]))\n    xd <- xd[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  }\n\n  features <- dim(xd)[1]\n  samples <- dim(xd)[2]\n  geneNames <- rownames(xd)\n  xd <- apply(xd, 2, as.numeric)\n  rownames(xd) <- geneNames\n  colnames(xd) <- sampleNames\n\n  if(!is.na(designFile)) {\n    x <- read.table(designFile, sep=\"\\t\", header=TRUE, row.names=1, fill=TRUE, stringsAsFactors=FALSE)\n    xd <- xd[,sort.list(colnames(xd))]\n    xd <- xd[,colnames(xd) %in% rownames(x)]\n    x <- x[rownames(x) %in% colnames(xd),]\n    x <- x[sort.list(rownames(x)),]\n    classes <- as.data.frame(x)\n  }\n\n  if(sum(apply(xd, 2, is.na)) > 0 & impute) {\n    allAnn <- dimnames(xd)\n    data.imputed <- impute.knn(as.matrix(xd))$data\n    xd <- data.imputed[1:features,]\n    dimnames(xd) <- allAnn\n  }\n\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}"
      },
      {
        "partial": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  # Read the data file\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  # Process the data based on header rows\n  if(hr == 1) {\n    # Complete the code for hr == 1\n  } else {\n    # Complete the code for hr != 1\n  }\n\n  # Process design file if provided\n  if(!is.na(designFile)) {\n    # Complete the code for processing design file\n  }\n\n  # Impute data if required\n  if(sum(apply(xd, 2, is.na)) > 0 & impute) {\n    # Complete the code for imputation\n  }\n\n  # Return the result\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}",
        "complete": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  if(hr == 1) {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- NULL\n    ids <- x[,1]\n    xd <- x[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  } else {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- x[1:(hr-1),]\n    dimnames(classes)[[1]] <- classes[,1]\n    classes <- classes[,-1]\n    classes[classes==\"\"] <- NA\n    classes <- t(classes)\n    rownames(classes) <- sampleNames\n    classes <- as.data.frame(classes)\n    xd <- x[(-1:-(hr-1)),]\n    ids <- as.vector(t(xd[,1]))\n    xd <- xd[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  }\n\n  features <- dim(xd)[1]\n  samples <- dim(xd)[2]\n  geneNames <- rownames(xd)\n  xd <- apply(xd, 2, as.numeric)\n  rownames(xd) <- geneNames\n  colnames(xd) <- sampleNames\n\n  if(!is.na(designFile)) {\n    x <- read.table(designFile, sep=\"\\t\", header=TRUE, row.names=1, fill=TRUE, stringsAsFactors=FALSE)\n    xd <- xd[,sort.list(colnames(xd))]\n    xd <- xd[,colnames(xd) %in% rownames(x)]\n    x <- x[rownames(x) %in% colnames(xd),]\n    x <- x[sort.list(rownames(x)),]\n    classes <- as.data.frame(x)\n  }\n\n  if(sum(apply(xd, 2, is.na)) > 0 & impute) {\n    allAnn <- dimnames(xd)\n    data.imputed <- impute.knn(as.matrix(xd))$data\n    xd <- data.imputed[1:features,]\n    dimnames(xd) <- allAnn\n  }\n\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/allGenerics.R",
    "language": "R",
    "content": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) setGeneric('methods-computeLimmaDiffExpr.R'))",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setGeneric` function in this code snippet?",
        "answer": "The `setGeneric` function is used to define a new generic method called 'computeLimmaDiffExpr'. It creates a function that can have multiple implementations (methods) for different classes of objects. This allows for method dispatch based on the class of the 'object' argument."
      },
      {
        "question": "What does the `...` argument in the function definition represent?",
        "answer": "The `...` (ellipsis) in the function definition allows for additional arguments to be passed to the method. As stated in the comment, it allows new parameters to be added to this generic method without changing its signature. This provides flexibility for different implementations to accept various parameters."
      },
      {
        "question": "What is the significance of the `@export` tag in the Roxygen comments?",
        "answer": "The `@export` tag in the Roxygen comments indicates that this function should be made publicly available when the package is built. It tells the package building system to include this function in the package's namespace, making it accessible to users who import the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) ",
        "complete": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) standardGeneric('computeLimmaDiffExpr'))"
      },
      {
        "partial": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    ",
        "complete": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) standardGeneric('computeLimmaDiffExpr'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/bimod.R",
    "language": "R",
    "content": "#' @name bimod\n#' @title Function to identify bimodality for gene expression or signature score\n#'\n#' @description\n#' This function fits a mixture of two Gaussians to identify bimodality.\n#' Useful to identify ER of HER2 status of breast tumors using\n#' ESR1 and ERBB2 expressions respectively.\n#'\n#' @usage\n#' bimod(x, data, annot, do.mapping = FALSE, mapping, model = c(\"E\", \"V\"),\n#' do.scale = TRUE, verbose = FALSE, ...)\n#'\n#' @param x Matrix containing the gene(s) in the gene list in rows and at least three columns:\n#'   \"probe\", \"EntrezGene.ID\" and \"coefficient\" standing for the name of the probe,\n#'   the NCBI Entrez Gene id and the coefficient giving the direction and the strength\n#'   of the association of each gene in the gene list.\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in case of\n#'   ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#'   mapping such that the probes are not selected based on their variance.\n#' @param model Model name used in Mclust.\n#' @param do.scale TRUE if the gene expressions or signature scores must be rescaled (see rescale), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#' @param ... Additional parameters to pass to sig.score.\n#'\n#' @return\n#' A list with items:\n#' - status: Status being 0 or 1.\n#' - status1.proba: Probability p to be of status 1, the probability to\n#'   be of status 0 being 1-p.\n#' - gaussians: Matrix of parameters fitted in the mixture of two\n#'   Gaussians. Matrix of NA values if EM algorithm did not converge.\n#' - BIC: Values (gene expressions or signature scores) used to identify bimodality.\n#' - BI: Bimodality Index (BI) as defined by Wang et al., 2009.\n#' - x: Values (gene expressions or signature scores) used to identify bimodality\n#'\n#' @references\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M,\n#'   and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend\n#'   on the molecular subtypes\", Clinical Cancer Research, 14(16):5158\u20135165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt C, Ignatiadis M,\n#'   Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M (2008) \"Meta-analysis of\n#'   Gene-Expression Profiles in Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing\n#'   and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' Fraley C and Raftery E (2002) \"Model-Based Clustering, Discriminant Analysis, and Density Estimation\",\n#'   Journal of American Statistical Asscoiation, 97(458):611\u2013631.\n#' Wang J, Wen S, Symmans FW, Pusztai L and Coombes KR (2009) \"The bimodality index: a criterion for\n#'   discovering and ranking bimodal signatures from cancer gene expression profiling data\", Cancer\n#'   Informatics, 7:199\u2013216.\n#'\n#' @seealso\n#' [mclust::Mclust]\n#'\n#' @examples\n#' # load NKI data\n#' data(nkis)\n#' # load gene modules from Desmedt et al. 2008\n#' data(mod1)\n#' # retrieve esr1 affy probe and Entrez Gene id\n#' esr1 <- mod1$ESR1[1, ,drop=FALSE]\n#' # computation of signature scores\n#' esr1.bimod <- bimod(x=esr1, data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   model=\"V\", verbose=TRUE)\n#' table(\"ER.IHC\"=demo.nkis[ ,\"er\"], \"ER.GE\"=esr1.bimod$status)\n#'\n#' @md\n#' @importFrom mclust Mclust\n#' @export\nbimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      #How many Gaussians?\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      #Only 2 Gaussians\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) { ## EM algorithm did not converge\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      res[1, ] <- rr2$parameters$mean\n      res[2, ] <- rr2$parameters$variance$sigmasq\n      res[3, ] <- rr2$parameters$pro\n\n      ## bimodality index (BI)\n      smd <- abs(res[1, 2] - res[1, 1]) / sqrt((res[2, 2] + res[2, 1]) / 2)\n      bi <- sqrt(res[3, 2] * (1 - res[3, 2])) * smd\n\n      #classification\n      mystatus[cc.ix] <- as.numeric(rr2$classification == 2)\n      mystatus.proba[cc.ix] <- rr2$z[ , 2, drop=TRUE]\n      return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=mybic,  \"BI\"=bi, \"x\"=dd))\n    } else { stop(\"Not enough data!\") }\n  }",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `bimod` function in this code snippet?",
        "answer": "The `bimod` function is designed to identify bimodality in gene expression or signature scores. It fits a mixture of two Gaussians to the data, which is particularly useful for identifying ER or HER2 status of breast tumors using ESR1 and ERBB2 expressions, respectively. The function performs clustering using the Mclust algorithm and returns various statistics including the bimodal status, probabilities, fitted Gaussian parameters, and a Bimodality Index (BI)."
      },
      {
        "question": "How does the function handle cases where there might be more or fewer than two Gaussians in the data?",
        "answer": "The function first attempts to fit 1 to 10 Gaussians using Mclust and selects the best model based on BIC (Bayesian Information Criterion). However, it then forces a fit of exactly two Gaussians, regardless of the optimal number found. If the optimal number is not 2, it issues a warning message. This approach ensures that the function always attempts to identify bimodality, even if the data might be better described by a different number of clusters."
      },
      {
        "question": "What is the significance of the `do.scale` parameter in the `bimod` function?",
        "answer": "The `do.scale` parameter, when set to TRUE (which is the default), rescales the gene expressions or signature scores using the `rescale` function. The rescaling is performed with a quantile of 0.05 and then shifted and scaled to range from -1 to 1. This preprocessing step can help normalize the data and potentially improve the performance of the clustering algorithm by putting all variables on a comparable scale. Users can set `do.scale=FALSE` if they prefer to work with the original, unscaled data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) {\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      # Complete the function here\n    } else { stop(\"Not enough data!\") }\n  }",
        "complete": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) {\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      res[1, ] <- rr2$parameters$mean\n      res[2, ] <- rr2$parameters$variance$sigmasq\n      res[3, ] <- rr2$parameters$pro\n\n      smd <- abs(res[1, 2] - res[1, 1]) / sqrt((res[2, 2] + res[2, 1]) / 2)\n      bi <- sqrt(res[3, 2] * (1 - res[3, 2])) * smd\n\n      mystatus[cc.ix] <- as.numeric(rr2$classification == 2)\n      mystatus.proba[cc.ix] <- rr2$z[ , 2, drop=TRUE]\n      return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=mybic, \"BI\"=bi, \"x\"=dd))\n    } else { stop(\"Not enough data!\") }\n  }"
      },
      {
        "partial": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      # Add code here to fit Gaussian mixture models and calculate bimodality index\n    } else { stop(\"Not enough data!\") }\n  }",
        "complete": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) {\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      res[1, ] <- rr2$parameters$mean\n      res[2, ] <- rr2$parameters$variance$sigmasq\n      res[3, ] <- rr2$parameters$pro\n\n      smd <- abs(res[1, 2] - res[1, 1]) / sqrt((res[2, 2] + res[2, 1]) / 2)\n      bi <- sqrt(res[3, 2] * (1 - res[3, 2])) * smd\n\n      mystatus[cc.ix] <- as.numeric(rr2$classification == 2)\n      mystatus.proba[cc.ix] <- rr2$z[ , 2, drop=TRUE]\n      return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=mybic, \"BI\"=bi, \"x\"=dd))\n    } else { stop(\"Not enough data!\") }\n  }"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/cindex.comp.meta.R",
    "language": "R",
    "content": "`cindex.comp.meta` <-\nfunction(list.cindex1, list.cindex2, hetero=FALSE) {\n\n\tif(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n\teps <- 1E-15\n\t\n\tn <- 0\n\tx1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n\tfor(i in 1:length(list.cindex1)) {\n\t\tnn <- list.cindex1[[i]]$n\n\t\tif(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n\t\tif(nn > 3) {\n\t\t\tn <- n + nn\n\t\t\tx1 <- c(x1, list.cindex1[[i]]$c.index)\n\t\t\tx1.se <- c(x1.se, list.cindex1[[i]]$se)\n\t\t\tx2 <- c(x2, list.cindex2[[i]]$c.index)\n\t\t\tx2.se <- c(x2.se, list.cindex2[[i]]$se)\n\t\t\tcort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n\t\t\t## since r is the spearman correlation coefficient and not the Pearson's one, we should apply a correction factor (see http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient for details)\n\t\t\tcorz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n\t\t\tcorz.se <- c(corz.se, 1 / sqrt(nn - 3))\n\t\t} else {\n\t\t\tcorz <- c(corz, NA)\n\t\t\tcorz.se <- c(corz.se, NA)\n\t\t}\n\t}\n\tx1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n\tx2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n\tif(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n\t## same concordance indices\t\n\t\treturn(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n\t}\n\trz <- combine.est(x=corz, x.se=corz.se, na.rm=TRUE, hetero=hetero)$estimate\n\t## since r is the spearman correlation coefficient and not the Pearson's one, we should apply a correction factor (see http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient for details)\n\trz <- rz / (sqrt((n - 3) / 1.06))\n\tr <- fisherz(rz, inv=TRUE)\n\tif((1 - abs(r)) > eps) {\n\t\tt.stat <- (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cindex.comp.meta` function and what are its main input parameters?",
        "answer": "The `cindex.comp.meta` function is designed to compare two lists of concordance indices and perform a meta-analysis. Its main input parameters are `list.cindex1` and `list.cindex2`, which are lists containing concordance indices, and an optional `hetero` parameter to specify whether heterogeneity should be considered in the analysis."
      },
      {
        "question": "How does the function handle the correlation between the two sets of concordance indices, and why is a correction factor applied?",
        "answer": "The function calculates the Spearman correlation coefficient between the data of the two concordance indices. A correction factor is applied to the Fisher's z-transformed correlation because Spearman's correlation is used instead of Pearson's. The correction factor is sqrt((nn - 3) / 1.06), where nn is the number of samples. This adjustment is made to account for the differences between Spearman's and Pearson's correlations in the Fisher's z-transformation."
      },
      {
        "question": "What statistical test is performed to compare the two sets of concordance indices, and under what condition is this test not applied?",
        "answer": "The function performs a t-test to compare the two sets of concordance indices. The test statistic is calculated as (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se), where r is the estimated correlation between the indices. However, if the absolute value of the correlation (r) is very close to 1 (within a small epsilon value), the test is not applied, and a p-value of 1 is returned instead. This is likely to avoid numerical instability when the correlation is extremely high."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      n <- n + nn\n      x1 <- c(x1, list.cindex1[[i]]$c.index)\n      x1.se <- c(x1.se, list.cindex1[[i]]$se)\n      x2 <- c(x2, list.cindex2[[i]]$c.index)\n      x2.se <- c(x2.se, list.cindex2[[i]]$se)\n      cort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n      corz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n      corz.se <- c(corz.se, 1 / sqrt(nn - 3))\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  # Complete the function\n}",
        "complete": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      n <- n + nn\n      x1 <- c(x1, list.cindex1[[i]]$c.index)\n      x1.se <- c(x1.se, list.cindex1[[i]]$se)\n      x2 <- c(x2, list.cindex2[[i]]$c.index)\n      x2.se <- c(x2.se, list.cindex2[[i]]$se)\n      cort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n      corz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n      corz.se <- c(corz.se, 1 / sqrt(nn - 3))\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  x1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n  x2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n  if(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n    return(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n  }\n  rz <- combine.est(x=corz, x.se=corz.se, na.rm=TRUE, hetero=hetero)$estimate\n  rz <- rz / (sqrt((n - 3) / 1.06))\n  r <- fisherz(rz, inv=TRUE)\n  if((1 - abs(r)) > eps) {\n    t.stat <- (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n}"
      },
      {
        "partial": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      # Complete the loop body\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  x1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n  x2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n  if(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n    return(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n  }\n  # Complete the function\n}",
        "complete": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      n <- n + nn\n      x1 <- c(x1, list.cindex1[[i]]$c.index)\n      x1.se <- c(x1.se, list.cindex1[[i]]$se)\n      x2 <- c(x2, list.cindex2[[i]]$c.index)\n      x2.se <- c(x2.se, list.cindex2[[i]]$se)\n      cort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n      corz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n      corz.se <- c(corz.se, 1 / sqrt(nn - 3))\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  x1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n  x2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n  if(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n    return(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n  }\n  rz <- combine.est(x=corz, x.se=corz.se, na.rm=TRUE, hetero=hetero)$estimate\n  rz <- rz / (sqrt((n - 3) / 1.06))\n  r <- fisherz(rz, inv=TRUE)\n  if((1 - abs(r)) > eps) {\n    t.stat <- (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/cordiff.dep.R",
    "language": "R",
    "content": "#' @title Function to estimate whether two dependent correlations differ\n#'\n#' @description\n#' This function tests for statistical differences between two dependent correlations\n#'   using the formula provided on page 56 of Cohen & Cohen (1983). The function returns \n#'   a t-value, the DF and the p-value.\n#'\n#' @usage\n#' cordiff.dep(r.x1y, r.x2y, r.x1x2, n,\n#'   alternative = c(\"two.sided\", \"less\", \"greater\"))\n#'\n#' @param r.x1y\tThe correlation between x1 and y where y is typically your outcome variable.\n#' @param r.x2y\tThe correlation between x2 and y where y is typically your outcome variable.\n#' @param r.x1x2 The correlation between x1 and x2 (the correlation between your two predictors).\n#' @param n The sample size.\n#' @param alternative A character string specifying the alternative hypothesis, must be\n#'   one of \"two.sided\" default), \"greater\" or \"less\". You can specify just the initial letter.\n#'\n#' @details\n#' This function is inspired from the cordif.dep.\n#'\n#' @return\n#' Vector of three values: t statistics, degree of freedom, and p-value.\n#'\n#' @references\n#' Cohen, J. & Cohen, P. (1983) \"Applied multiple regression/correlation analysis for the\n#'   behavioral sciences (2nd Ed.)\" Hillsdale, nJ: Lawrence Erlbaum Associates.\n#'\n#' @seealso\n#' [stats::cor], [stats::t.test], [genefu::compareProtoCor]\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # retrieve ESR1, AURKA and MKI67 gene expressions\n#' x1 <- data.vdxs[ ,\"208079_s_at\"]\n#' x2 <- data.vdxs[ ,\"205225_at\"]\n#' y <- data.vdxs[ ,\"212022_s_at\"]\n#' # is MKI67 significantly more correlated to AURKA than ESR1?\n#' cc.ix <- complete.cases(x1, x2, y)\n#' cordiff.dep(r.x1y=abs(cor(x=x1[cc.ix], y=y[cc.ix], use=\"everything\",\n#'   method=\"pearson\")), r.x2y=abs(cor(x=x2[cc.ix], y=y[cc.ix],\n#'   use=\"everything\", method=\"pearson\")), r.x1x2=abs(cor(x=x1[cc.ix],\n#'   y=x2[cc.ix], use=\"everything\", method=\"pearson\")), n=sum(cc.ix),\n#'   alternative=\"greater\")\n#'\n#' @md\n#' @export\ncordiff.dep <-\nfunction(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n\talternative <- match.arg(alternative)\n\trbar <- (r.x1y + r.x2y)/2\n\tbarRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n\ttvalue.num <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2)))\n\ttvalue.den <- sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n\tt.value <- tvalue.num / tvalue.den\n\tDF <- n - 3\n\tswitch(alternative,\n\t\"greater\"={\n\t\tp.value <- pt(t.value, DF, lower.tail=FALSE)\n\t},\n\t\"less\"={\n\t\tp.value <- 1 - pt(t.value, DF, lower.tail=FALSE)\n\t},\n\t\"two.sided\"={\n\t\tp.value <- (1 - pt(abs(t.value), DF)) * 2\n\t})\n\tOUT <- c(t.value, DF, p.value)\n\tnames(OUT) <- c(\"t.value\",  \"DF\",  \"p.value\")\n\t\n\treturn(OUT)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cordiff.dep` function and what statistical test does it perform?",
        "answer": "The `cordiff.dep` function is designed to estimate whether two dependent correlations differ significantly. It performs a statistical test to compare two correlations that share a common variable, based on the formula from Cohen & Cohen (1983). The function calculates a t-value, degrees of freedom, and a p-value to determine if there's a significant difference between the two correlations."
      },
      {
        "question": "How does the function handle different alternative hypotheses, and what are the available options?",
        "answer": "The function handles different alternative hypotheses using the `alternative` parameter and the `switch` statement. The available options are 'two.sided' (default), 'less', and 'greater'. For 'greater', it calculates the upper-tail probability; for 'less', it calculates the lower-tail probability; and for 'two.sided', it calculates the two-tailed probability. The p-value calculation is adjusted accordingly using the `pt` function with the appropriate tail specification."
      },
      {
        "question": "What are the key components of the t-value calculation in the `cordiff.dep` function, and how are they derived?",
        "answer": "The t-value calculation in the `cordiff.dep` function involves several key components:\n1. `rbar`: The average of the two correlations (r.x1y and r.x2y).\n2. `barRbar`: A complex term involving all three correlations.\n3. `tvalue.num`: The numerator of the t-value, which includes the difference between correlations and a term with r.x1x2.\n4. `tvalue.den`: The denominator of the t-value, involving `barRbar` and `rbar`.\nThese components are derived from the formula provided by Cohen & Cohen (1983) and are combined to calculate the final t-value, which is then used to determine the statistical significance of the difference between the two dependent correlations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  rbar <- (r.x1y + r.x2y)/2\n  barRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n  tvalue.num <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2)))\n  tvalue.den <- sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n  t.value <- tvalue.num / tvalue.den\n  DF <- n - 3\n  # Complete the function by adding the switch statement for p-value calculation\n}",
        "complete": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  rbar <- (r.x1y + r.x2y)/2\n  barRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n  tvalue.num <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2)))\n  tvalue.den <- sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n  t.value <- tvalue.num / tvalue.den\n  DF <- n - 3\n  p.value <- switch(alternative,\n    \"greater\" = pt(t.value, DF, lower.tail=FALSE),\n    \"less\" = pt(t.value, DF, lower.tail=TRUE),\n    \"two.sided\" = 2 * pt(-abs(t.value), DF)\n  )\n  c(t.value = t.value, DF = DF, p.value = p.value)\n}"
      },
      {
        "partial": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  # Calculate rbar and barRbar\n  # Calculate t-value numerator and denominator\n  # Calculate t-value and degrees of freedom\n  # Calculate p-value based on alternative hypothesis\n  # Return results\n}",
        "complete": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  rbar <- (r.x1y + r.x2y)/2\n  barRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n  t.value <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2))) /\n              sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n  DF <- n - 3\n  p.value <- switch(alternative,\n    \"greater\" = pt(t.value, DF, lower.tail=FALSE),\n    \"less\" = pt(t.value, DF, lower.tail=TRUE),\n    \"two.sided\" = 2 * pt(-abs(t.value), DF)\n  )\n  c(t.value = t.value, DF = DF, p.value = p.value)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/updateObject-methods.R",
    "language": "R",
    "content": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    names(curation(tSet)) <- gsub(\"drug\", \"treatment\", names(curation(tSet)))\n    if (\"treatment\" %in% names(curation(tSet))) {\n        colnames(curation(tSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(tSet)$treatment))\n    }\n    validObject(tSet)\n    return(tSet)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `updateObject` method for the `ToxicoSet` class?",
        "answer": "The `updateObject` method is used to update the class structure of a `ToxicoSet` object after changes in its structure or API. It converts the object to a `CoreGx` object, updates it, then converts it back to a `ToxicoSet` object with updated curation data."
      },
      {
        "question": "How does the method handle the renaming of 'drug' to 'treatment' in the curation data?",
        "answer": "The method uses the `gsub` function to replace 'drug' with 'treatment' in the names of the curation data. It does this by applying `gsub(\"drug\", \"treatment\", names(curation(tSet)))` to update the column names."
      },
      {
        "question": "What is the significance of the `validObject(tSet)` call at the end of the method?",
        "answer": "The `validObject(tSet)` call is used to ensure that the updated `ToxicoSet` object is valid according to its class definition. This helps maintain data integrity and catches any potential errors introduced during the update process before returning the object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    # Complete the function\n})",
        "complete": "#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    names(curation(tSet)) <- gsub(\"drug\", \"treatment\", names(curation(tSet)))\n    if (\"treatment\" %in% names(curation(tSet))) {\n        colnames(curation(tSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(tSet)$treatment))\n    }\n    validObject(tSet)\n    return(tSet)\n})"
      },
      {
        "partial": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    # Complete the function body\n})",
        "complete": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    names(curation(tSet)) <- gsub(\"drug\", \"treatment\", names(curation(tSet)))\n    if (\"treatment\" %in% names(curation(tSet))) {\n        colnames(curation(tSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(tSet)$treatment))\n    }\n    validObject(tSet)\n    return(tSet)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/gene76.R",
    "language": "R",
    "content": "#' @title Function to compute the Relapse Score as published by Wang et al. 2005\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene\n#'   expression values following the algorithm used for the Relapse Score (GENE76) as\n#'   published by Wang et al. 2005.\n#'\n#' @usage\n#' gene76(data, er)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param er Vector containing the estrogen receptor (ER) status of breast cancer patients in\n#'   the dataset.\n#'\n#'\n#' @return\n#' A list with items:\n#' - score Continuous signature scores\n#' - risk Binary risk classification, 1 being high risk and 0 being low risk.\n#'\n#' @references\n#' Y. Wang and J. G. Klijn and Y. Zhang and A. M. Sieuwerts and M. P. Look and F.\n#'   Yang and D. Talantov and M. Timmermans and M. E. Meijer-van Gelder and J. Yu and T.\n#'   Jatkoe and E. M. Berns and D. Atkins and J. A. Foekens (2005) \"Gene-Expression\n#'   Profiles to Predict Distant Metastasis of Lymph-Node-Negative Primary Breast Cancer\",\n#'   Lancet, 365(9460):671\u2013679.\n#'\n#' @seealso\n#' [genefu::ggi]\n#'\n#' @examples\n#' # load GENE76 signature\n#' data(sig.gene76)\n#' # load VDX dataset\n#' data(vdxs)\n#' # compute relapse score\n#' rs.vdxs <- gene76(data=data.vdxs, er=demo.vdxs[ ,\"er\"])\n#' table(rs.vdxs$risk)\n#'\n#' @md\n#' @export\n#' @name gene76\ngene76 <- function(data, er) {\n\n\tif (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n\t\n\tA <- 313.5\n\tB <- 280\n\n\tscore <- NULL\n\tfor(i in 1:nrow(data)) {\n\t\tif(is.na(er[i])) { score <- c(score, NA) }\n\t\telse {\n\t\t\tif(er[i] == 1) {\n\t\t\t\tscore <- c(score, A + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[ ,\"er\"] == 1]] * sig.gene76[sig.gene76[ ,\"er\"] == 1,\"std.cox.coefficient\"]))\n\t\t\t}\n\t\t\telse {\n\t\t\t\tscore <- c(score, B + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[ ,\"er\"] == 0]] * sig.gene76[sig.gene76[ ,\"er\"] == 0,\"std.cox.coefficient\"]))\n\t\t\t}\n\t\t}\n\t}\n\tnames(score) <- dimnames(data)[[1]]\n\trisk <- ifelse(score >= 0, 1, 0)\n\n\treturn(list(\"score\"=score, \"risk\"=risk))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'gene76' function and what are its input parameters?",
        "answer": "The 'gene76' function computes the Relapse Score (GENE76) for breast cancer patients based on gene expression data. It takes two input parameters: 'data', which is a matrix of gene expressions with samples in rows and probes in columns, and 'er', which is a vector containing the estrogen receptor (ER) status of breast cancer patients in the dataset."
      },
      {
        "question": "How does the function handle different ER statuses when calculating the score?",
        "answer": "The function uses different calculations based on the ER status. If er[i] == 1 (ER-positive), it uses the formula: A + sum(data[i, ER-positive genes] * coefficients). If er[i] == 0 (ER-negative), it uses: B + sum(data[i, ER-negative genes] * coefficients). A is set to 313.5 and B to 280. This approach allows for tailored scoring based on the patient's ER status."
      },
      {
        "question": "What does the function return, and how is the risk classification determined?",
        "answer": "The function returns a list with two items: 'score' (continuous signature scores) and 'risk' (binary risk classification). The risk classification is determined by the score: if the score is greater than or equal to 0, the risk is classified as high (1), otherwise it's classified as low (0). This binary classification provides a simple risk assessment based on the computed score."
      }
    ],
    "completion_tasks": [
      {
        "partial": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  score <- NULL\n  for(i in 1:nrow(data)) {\n    if(is.na(er[i])) { score <- c(score, NA) }\n    else {\n      # Complete the code here\n    }\n  }\n  names(score) <- dimnames(data)[[1]]\n  risk <- ifelse(score >= 0, 1, 0)\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}",
        "complete": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  score <- NULL\n  for(i in 1:nrow(data)) {\n    if(is.na(er[i])) { score <- c(score, NA) }\n    else {\n      if(er[i] == 1) {\n        score <- c(score, A + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 1]] * sig.gene76[sig.gene76[,\"er\"] == 1,\"std.cox.coefficient\"]))\n      } else {\n        score <- c(score, B + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 0]] * sig.gene76[sig.gene76[,\"er\"] == 0,\"std.cox.coefficient\"]))\n      }\n    }\n  }\n  names(score) <- dimnames(data)[[1]]\n  risk <- ifelse(score >= 0, 1, 0)\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}"
      },
      {
        "partial": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  # Complete the code here to calculate score and risk\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}",
        "complete": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  score <- sapply(1:nrow(data), function(i) {\n    if(is.na(er[i])) return(NA)\n    if(er[i] == 1) {\n      return(A + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 1]] * sig.gene76[sig.gene76[,\"er\"] == 1,\"std.cox.coefficient\"]))\n    } else {\n      return(B + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 0]] * sig.gene76[sig.gene76[,\"er\"] == 0,\"std.cox.coefficient\"]))\n    }\n  })\n  names(score) <- dimnames(data)[[1]]\n  risk <- ifelse(score >= 0, 1, 0)\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/pik3cags.R",
    "language": "R",
    "content": "#' @title Function to compute the PIK3CA gene signature (PIK3CA-GS)\n#'\n#' @description\n#' This function computes signature scores from gene expression values\n#'   following the algorithm used for the PIK3CA gene signature (PIK3CA-GS).\n#'\n#' @usage\n#' pik3cags(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force\n#'   the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' Vector of signature scores for PIK3CA-GS\n#'\n#' @references\n#' Loi S, Haibe-Kains B, Majjaj S, Lallemand F, Durbecq V, Larsimont D,\n#'   Gonzalez-Angulo AM, Pusztai L, Symmans FW, Bardelli A, Ellis P, Tutt AN,\n#'   Gillett CE, Hennessy BT., Mills GB, Phillips WA, Piccart MJ, Speed TP,\n#'   McArthur GA, Sotiriou C (2010) \"PIK3CA mutations associated with gene\n#'   signature of low mTORC1 signaling and better outcomes in estrogen\n#'   receptor-positive breast cancer\", Proceedings of the National Academy of\n#'   Sciences, 107(22):10208-10213\n#'\n#' @seealso\n#' [genefu::gene76]\n#'\n#' @examples\n#' # load GGI signature\n#' data(sig.pik3cags)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' pik3cags.nkis <- pik3cags(data=data.nkis, annot=annot.nkis, do.mapping=TRUE)\n#' head(pik3cags.nkis)\n#'\n#' @md\n#' @export\n#' @name pik3cags\npik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\tres <- sig.score(x=pik3cags.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score\n\n\treturn (res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'pik3cags' function and what does it return?",
        "answer": "The 'pik3cags' function computes the PIK3CA gene signature (PIK3CA-GS) scores from gene expression values. It returns a vector of signature scores for PIK3CA-GS based on the input gene expression data and annotations."
      },
      {
        "question": "How does the function handle gene mapping, and what parameter controls this behavior?",
        "answer": "The function can perform gene mapping through Entrez Gene IDs if needed. This behavior is controlled by the 'do.mapping' parameter. When set to TRUE, the function maps probes to genes, keeping the most variant probe for each gene in case of ambiguities. The 'mapping' parameter can be used to force a specific mapping instead of selecting probes based on variance."
      },
      {
        "question": "What is the significance of the 'sig.pik3cags' data in this function, and how is it used?",
        "answer": "The 'sig.pik3cags' data contains the PIK3CA gene signature information. If it doesn't exist in the current environment, the function loads it. It then extracts the probe, EntrezGene.ID, and coefficient information from 'sig.pik3cags' to create 'pik3cags.gl', which is used as input for the 'sig.score' function to compute the final signature scores."
      }
    ],
    "completion_tasks": [
      {
        "partial": "pik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\t# Complete the function by calling sig.score and returning the result\n\n}",
        "complete": "pik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\tres <- sig.score(x=pik3cags.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score\n\n\treturn(res)\n}"
      },
      {
        "partial": "#' @title Function to compute the PIK3CA gene signature (PIK3CA-GS)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force\n#'   the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' Vector of signature scores for PIK3CA-GS\n#'\n#' @export\n#' @name pik3cags\n# Complete the function definition and body",
        "complete": "#' @title Function to compute the PIK3CA gene signature (PIK3CA-GS)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force\n#'   the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' Vector of signature scores for PIK3CA-GS\n#'\n#' @export\n#' @name pik3cags\npik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\tres <- sig.score(x=pik3cags.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score\n\treturn(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/write.m.file.R",
    "language": "R",
    "content": "#' @title Function to write a 'csv' file containing gene lists \n#'   (aka gene signatures)\n#'\n#' @description\n#' This function allows for writing a 'csv' file containing gene signatures. \n#'   Each gene signature is composed of at least four columns: \"gene.list\" is \n#'   the name of the signature on the first line and empty fields below, \n#'   \"probes\" are the probe names, \"EntrezGene.ID\" are the EntrezGene IDs \n#'   and \"coefficient\" are the coefficients of each probe.\n#'\n#' @usage\n#' write.m.file(obj, file, ...)\n#'\n#' @param obj\tList of gene signatures.\n#' @param file Filename of the 'csv' file.\n#' @param ...\tAdditional parameters for read.csv function.\n#'\n#' @return\n#' None.\n#'\n#' @examples\n#' # load gene modules published by Demsedt et al 2009\n#' data(mod1)\n#' # write these gene modules in a 'csv' file\n#' # Not run: write.m.file(obj=mod1, file=\"desmedt2009_genemodules.csv\")\n#' \n#' @md\n#' @export\nwrite.m.file <-\nfunction(obj, file, ...) {\n\tlcn <- dimnames(obj[[1]])[[2]]\n\tc1 <- c2 <- NULL\n\tfor (i in 1:length(obj)) {\n\t\tct <- names(obj)[i]\n\t\ttt <- NULL\n\t\tfor(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n\t\tcolnames(tt) <- colnames(obj[[i]])\n\t\tc1 <- c(c1, ct, rep(\"\", nrow(tt)))\n\t\tc2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n\t}\n\tdimnames(c2)[[1]] <- 1:nrow(c2)\n\tres <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n\tdimnames(res)[[2]] <- c(\"gene.list\", lcn)\n\tdimnames(res)[[1]] <- 1:nrow(res)\n\twrite.table(res, file=file, row.names=FALSE, sep=\",\", ...)\n\tinvisible(res)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'write.m.file' function and what are its main parameters?",
        "answer": "The 'write.m.file' function is designed to write gene signatures (gene lists) to a CSV file. Its main parameters are 'obj' (a list of gene signatures) and 'file' (the filename for the output CSV). The function organizes the gene signatures into a specific format with columns for gene list names, probes, EntrezGene IDs, and coefficients."
      },
      {
        "question": "How does the function handle multiple gene signatures in the input 'obj' parameter?",
        "answer": "The function iterates through each gene signature in the 'obj' list using a nested loop structure. For each signature, it extracts the name (stored in 'ct') and the data (stored in 'tt'). It then combines this information into two main components: 'c1' for gene list names and 'c2' for the actual gene data. This process ensures that all signatures from the input are properly formatted and included in the output CSV file."
      },
      {
        "question": "What is the significance of the 'invisible(res)' statement at the end of the function?",
        "answer": "The 'invisible(res)' statement at the end of the function returns the formatted data ('res') invisibly. This means that the function will not print the result to the console when called, but the result can still be assigned to a variable if needed. This is a common practice in R for functions that primarily perform side effects (like writing to a file) but may still need to return their result for potential further use or inspection."
      }
    ],
    "completion_tasks": [
      {
        "partial": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    for(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  dimnames(c2)[[1]] <- 1:nrow(c2)\n  res <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n  dimnames(res)[[2]] <- c(\"gene.list\", lcn)\n  dimnames(res)[[1]] <- 1:nrow(res)\n  # Complete the function by writing the result to a file and returning it invisibly\n}",
        "complete": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    for(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  dimnames(c2)[[1]] <- 1:nrow(c2)\n  res <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n  dimnames(res)[[2]] <- c(\"gene.list\", lcn)\n  dimnames(res)[[1]] <- 1:nrow(res)\n  write.table(res, file=file, row.names=FALSE, sep=\",\", ...)\n  invisible(res)\n}"
      },
      {
        "partial": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    # Complete the nested loop to process each column of obj[[i]]\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  # Complete the function by finalizing the result and writing it to a file\n}",
        "complete": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    for(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  dimnames(c2)[[1]] <- 1:nrow(c2)\n  res <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n  dimnames(res)[[2]] <- c(\"gene.list\", lcn)\n  dimnames(res)[[1]] <- 1:nrow(res)\n  write.table(res, file=file, row.names=FALSE, sep=\",\", ...)\n  invisible(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/ibsc.comp.R",
    "language": "R",
    "content": "`ibsc.comp` <-\nfunction(bsc1, bsc2, time) {\n\tif((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) { stop(\"bsc1, bsc2 and time must have the same length!\") }\n\tcc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n\tbsc1 <- bsc1[cc.ix]\n\tbsc2 <- bsc2[cc.ix]\n\ttime <- time[cc.ix]\n\tdiffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n\tibsc1 <- sum(diffs * bsc1) / max(time)\n\tibsc2 <- sum(diffs * bsc2) / max(time)\n\trr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n\treturn(list(\"p.value\"=rr$p.value, \"ibsc1\"=ibsc1, \"ibsc2\"=ibsc2))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ibsc.comp` function and what are its input parameters?",
        "answer": "The `ibsc.comp` function is designed to compare two sets of BSC (possibly 'Balanced Scorecard') values over time. It takes three input parameters: `bsc1` and `bsc2` (two sets of BSC values to be compared) and `time` (the corresponding time points for the BSC values). The function calculates integrated BSC values for both sets and performs a Wilcoxon signed-rank test to compare them."
      },
      {
        "question": "How does the function handle missing or duplicate data in the input?",
        "answer": "The function handles missing or duplicate data by using the `complete.cases()` function to remove any rows with missing values across all three inputs (bsc1, bsc2, and time). It also removes duplicate time points using `!duplicated(time)`. This ensures that only complete and unique data points are used in the calculations, which is crucial for accurate results in time series analysis."
      },
      {
        "question": "What statistical test is performed in this function, and what does the result represent?",
        "answer": "The function performs a Wilcoxon signed-rank test using the `wilcox.test()` function. This is a non-parametric test used to compare two related samples. In this case, it's comparing `bsc1` and `bsc2` with the alternative hypothesis that `bsc1` is less than `bsc2` (indicated by `alternative=\"less\"`). The test is paired (indicated by `paired=TRUE`) and uses a normal approximation instead of exact p-values (indicated by `exact=FALSE`). The resulting p-value represents the probability of observing such a difference between `bsc1` and `bsc2` by chance, assuming the null hypothesis is true."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ibsc.comp <- function(bsc1, bsc2, time) {\n  if((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) {\n    stop(\"bsc1, bsc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n  ibsc1 <- sum(diffs * bsc1) / max(time)\n  ibsc2 <- sum(diffs * bsc2) / max(time)\n  # Complete the function by adding the Wilcoxon test and return statement\n}",
        "complete": "ibsc.comp <- function(bsc1, bsc2, time) {\n  if((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) {\n    stop(\"bsc1, bsc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n  ibsc1 <- sum(diffs * bsc1) / max(time)\n  ibsc2 <- sum(diffs * bsc2) / max(time)\n  rr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"ibsc1\"=ibsc1, \"ibsc2\"=ibsc2))\n}"
      },
      {
        "partial": "ibsc.comp <- function(bsc1, bsc2, time) {\n  # Add input validation\n  \n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  # Calculate diffs and ibsc values\n  \n  rr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n  # Return the results\n}",
        "complete": "ibsc.comp <- function(bsc1, bsc2, time) {\n  if((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) {\n    stop(\"bsc1, bsc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  ibsc1 <- sum(diffs * bsc1) / max(time)\n  ibsc2 <- sum(diffs * bsc2) / max(time)\n  rr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"ibsc1\"=ibsc1, \"ibsc2\"=ibsc2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/rankGeneDrugPerturbation.R",
    "language": "R",
    "content": "# Rank genes based on drug effect\n#\n# A helper function called from within `drugPerturbationSig`. This is intended\n#   for developer use only; if you aren't debugging the package, this should\n#   not be used.\n#\n# @param data: gene expression data matrix\n# @param drug: single or vector of drug(s) of interest; if a vector of drugs is\n#    provided, they will be considered as being the same drug and will be\n#    jointly analyszed\n# @param drug.id: drug used in each experiment\n# @param drug.concentration: drug concentration used in each experiment\n# @param type: cell or tissue type for each experiment\n# @param xp: type of experiment (perturbation or control)\n# @param batch: experiment batches\n# @param duration: The duration of the experiment, in a consistent unit\n# @param single.type: Should the statitsics be computed for each cell/tissue\n#   type separately?\n# @param nthread: number of parallel threads (bound to the maximum number of\n#   cores available)\n#\n# @return [list] of \\code{data.frame}s with the statistics for each gene, for\n#   each type\n#\n# @keywords internal\n# @export\nrankGeneDrugPerturbation <-\n  function(data, drug, drug.id, drug.concentration, type, xp, batch, duration,\n           single.type=FALSE, nthread=1, verbose=FALSE) {\n\n    if (nthread != 1) {\n      availcore <- parallel::detectCores()\n      if (missing(nthread) || nthread < 1 || nthread > availcore) {\n        nthread <- availcore\n      }\n      else{\n      }\n    }\n\n    #### DIMENSIONALITY CHECK\n    if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n      stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n    }\n    names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n    if (!all(complete.cases(type, xp, batch, duration))) {\n      stop(\"type, batch, duration and xp should not contain missing values!\")\n    }\n\n    # Returns a matrix of NAs if there is no viability values for the requested dose levels\n    if (length(unique(xp)) < 2 ) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      res <- c(NULL, list(rest))\n      names(res) <- list(\"all\"=type)\n      return(res)\n    }\n\n    res.type <- NULL\n\n    ## build input matrix\n    inpumat <- NULL\n\n    ## for each batch/vehicle of perturbations+controls (test within each batch/vehicle to avoid batch effect)\n    ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n    names(ubatch) <- paste0(\"batch\", ubatch)\n\n\n    for (bb in seq_len(length(ubatch))) {\n      ## identify the perturbations and corresponding control experiments\n      xpix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"perturbation\"]\n\n      ctrlix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"control\"]\n\n      if (all(!is.na(c(xpix, ctrlix))) && length(xpix) > 0 && length(ctrlix) > 0) {\n        if (!all(is.element(ctrlix, rownames(data)))) {\n          stop(\"data for some control experiments are missing!\")\n        }\n        if (verbose) {\n          cat(sprintf(\"type %s: batch %i/%i -> %i vs %i\\n\", utype[bb], bb, length(ubatch), length(xpix), length(ctrlix)))\n        }\n        ## transformation of drug concentrations values - decision made to keep in \u00b5m\n        conc <- drug.concentration # / 10^6\n        inpumat <- rbind(inpumat, data.frame(\"treated\"=c(rep(1, length(xpix)), rep(0, length(ctrlix))), \"type\"=c(type[xpix], type[ctrlix]), \"batch\"=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"), \"concentration\"=c(conc[xpix], conc[ctrlix]), \"duration\"= c(duration[xpix], duration[ctrlix])))\n      }\n    }\n\n\n    inpumat[ , \"type\"] <- factor(inpumat[ , \"type\"], ordered=FALSE)\n    inpumat[ , \"batch\"] <- factor(inpumat[ , \"batch\"], ordered=FALSE)\n\n    if (nrow(inpumat) < 3 || length(sort(unique(inpumat[ , \"concentration\"]))) < 2){ #|| length(unique(inpumat[ , \"duration\"])) < 2) {\n      ## not enough experiments in drug list\n      warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n      return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n    }\n\n    res <- NULL\n    utype <- sort(unique(as.character(inpumat[ , \"type\"])))\n    ltype <- list(\"all\"=utype)\n\n    if(single.type) {\n      ltype <- c(ltype, as.list(utype))\n      names(ltype)[-1] <- utype\n    }\n\n    for(ll in seq_along(ltype)) {\n\n      ## select the type of cell line/tissue of interest\n      inpumat2 <- inpumat[!is.na(inpumat[ , \"type\"]) & is.element(inpumat[ , \"type\"], ltype[[ll]]), , drop=FALSE]\n      inpumat2 <- inpumat2[complete.cases(inpumat2), , drop=FALSE]\n\n      if (nrow(inpumat2) < 3 || length(sort(unique(inpumat2[ , \"concentration\"]))) < 2) {\n        ## not enough experiments in data\n        nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n        rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n\n      } else {\n        ## test perturbation vs control\n        if(nthread > 1) {\n          ## parallel threads\n          splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n          ##TODO:: Can we reimplement this without using length?\n          splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n          mcres <- BiocParallel::bplapply(splitix, function(x, data, inpumat) {\n            res <- t(apply(data[rownames(inpumat), x, drop=FALSE], 2, ToxicoGx::geneDrugPerturbation, concentration=inpumat[ , \"concentration\"], type=inpumat[ , \"type\"], batch=inpumat[ , \"batch\"], duration=inpumat[,\"duration\"]))\n            return(res)\n          }, data=data, inpumat=inpumat2)\n          rest <- do.call(rbind, mcres)\n        } else {\n          rest <- t(apply(data[rownames(inpumat2), , drop=FALSE], 2, ToxicoGx::geneDrugPerturbation, concentration=inpumat2[ , \"concentration\"], type=inpumat2[ , \"type\"], batch=inpumat2[ , \"batch\"], duration=inpumat2[,\"duration\"]))\n        }\n      }\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      res <- c(res, list(rest))\n    }\n    names(res) <- names(ltype)\n    return(res)\n  }\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rankGeneDrugPerturbation` function and what are its main input parameters?",
        "answer": "The `rankGeneDrugPerturbation` function is a helper function used to rank genes based on drug effect. It's intended for developer use within the `drugPerturbationSig` function. The main input parameters include: 'data' (gene expression data matrix), 'drug' (single or vector of drugs of interest), 'drug.id' (drug used in each experiment), 'drug.concentration' (concentration used in each experiment), 'type' (cell or tissue type for each experiment), 'xp' (type of experiment: perturbation or control), 'batch' (experiment batches), and 'duration' (experiment duration)."
      },
      {
        "question": "How does the function handle parallel processing, and what condition determines if parallel processing will be used?",
        "answer": "The function handles parallel processing using the 'nthread' parameter. If 'nthread' is not equal to 1, the function will use parallel processing. It detects the available cores using `parallel::detectCores()` and sets 'nthread' to the number of available cores if the provided value is invalid. Parallel processing is implemented using `BiocParallel::bplapply` to apply the `geneDrugPerturbation` function across subsets of the data."
      },
      {
        "question": "What conditions might cause the function to return a matrix of NAs, and how does it handle different cell/tissue types?",
        "answer": "The function returns a matrix of NAs if there are fewer than two unique experiment types (perturbation and control) in the 'xp' parameter. It handles different cell/tissue types through the 'single.type' parameter. If 'single.type' is TRUE, it computes statistics for each cell/tissue type separately. The function creates a list of types to process, either containing all types combined ('all') or individual types. It then processes each type or combination of types separately, returning results for each in the final output list."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n\n  # Dimensionality check\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) {\n    stop(\"type, batch, duration and xp should not contain missing values!\")\n  }\n\n  # TODO: Implement the rest of the function\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n\n  # Dimensionality check\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) {\n    stop(\"type, batch, duration and xp should not contain missing values!\")\n  }\n\n  if (length(unique(xp)) < 2) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    rest <- cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n    return(list(all=rest))\n  }\n\n  inpumat <- NULL\n  ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n  names(ubatch) <- paste0(\"batch\", ubatch)\n\n  for (bb in seq_along(ubatch)) {\n    xpix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"perturbation\"]\n    ctrlix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"control\"]\n    if (all(!is.na(c(xpix, ctrlix))) && length(xpix) > 0 && length(ctrlix) > 0) {\n      if (!all(is.element(ctrlix, rownames(data)))) stop(\"data for some control experiments are missing!\")\n      conc <- drug.concentration\n      inpumat <- rbind(inpumat, data.frame(treated=c(rep(1, length(xpix)), rep(0, length(ctrlix))),\n                                          type=c(type[xpix], type[ctrlix]),\n                                          batch=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"),\n                                          concentration=c(conc[xpix], conc[ctrlix]),\n                                          duration=c(duration[xpix], duration[ctrlix])))\n    }\n  }\n\n  inpumat$type <- factor(inpumat$type, ordered=FALSE)\n  inpumat$batch <- factor(inpumat$batch, ordered=FALSE)\n\n  if (nrow(inpumat) < 3 || length(unique(inpumat$concentration)) < 2) {\n    warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n\n  utype <- sort(unique(as.character(inpumat$type)))\n  ltype <- list(\"all\"=utype)\n  if(single.type) ltype <- c(ltype, as.list(utype))\n\n  res <- lapply(ltype, function(types) {\n    inpumat2 <- inpumat[!is.na(inpumat$type) & inpumat$type %in% types, ]\n    inpumat2 <- inpumat2[complete.cases(inpumat2), ]\n\n    if (nrow(inpumat2) < 3 || length(unique(inpumat2$concentration)) < 2) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    } else {\n      rest <- if(nthread > 1) {\n        splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n        mcres <- BiocParallel::bplapply(splitix, function(x, data, inpumat) {\n          t(apply(data[rownames(inpumat), x, drop=FALSE], 2, ToxicoGx::geneDrugPerturbation,\n                  concentration=inpumat$concentration, type=inpumat$type,\n                  batch=inpumat$batch, duration=inpumat$duration))\n        }, data=data, inpumat=inpumat2)\n        do.call(rbind, mcres)\n      } else {\n        t(apply(data[rownames(inpumat2), , drop=FALSE], 2, ToxicoGx::geneDrugPerturbation,\n                concentration=inpumat2$concentration, type=inpumat2$type,\n                batch=inpumat2$batch, duration=inpumat2$duration))\n      }\n    }\n    cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n  })\n\n  names(res) <- names(ltype)\n  return(res)\n}"
      },
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  # TODO: Implement input validation and preprocessing\n\n  res <- NULL\n  utype <- sort(unique(as.character(inpumat[, \"type\"])))\n  ltype <- list(\"all\"=utype)\n\n  if(single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  for(ll in seq_along(ltype)) {\n    # TODO: Implement the main analysis loop\n  }\n\n  names(res) <- names(ltype)\n  return(res)\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) nthread <- availcore\n  }\n\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) stop(\"type, batch, duration and xp should not contain missing values!\")\n\n  if (length(unique(xp)) < 2) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    return(list(all=cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))))\n  }\n\n  inpumat <- NULL\n  ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n  names(ubatch) <- paste0(\"batch\", ubatch)\n\n  for (bb in ubatch) {\n    xpix <- which(complete.cases(batch, xp) & batch == bb & xp == \"perturbation\")\n    ctrlix <- which(complete.cases(batch, xp) & batch == bb & xp == \"control\")\n    if (length(xpix) > 0 && length(ctrlix) > 0) {\n      if (!all(ctrlix %in% seq_len(nrow(data)))) stop(\"data for some control experiments are missing!\")\n      if (verbose) cat(sprintf(\"type %s: batch %i/%i -> %i vs %i\\n\", utype[bb], bb, length(ubatch), length(xpix), length(ctrlix)))\n      inpumat <- rbind(inpumat, data.frame(treated=c(rep(1, length(xpix)), rep(0, length(ctrlix))),\n                                          type=c(type[xpix], type[ctrlix]),\n                                          batch=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"),\n                                          concentration=c(drug.concentration[xpix], drug.concentration[ctrlix]),\n                                          duration=c(duration[xpix], duration[ctrlix])))\n    }\n  }\n\n  inpumat$type <- factor(inpumat$type, ordered=FALSE)\n  inpumat$batch <- factor(inpumat$batch, ordered=FALSE)\n\n  if (nrow(inpumat) < 3 || length(unique(inpumat$concentration)) < 2) {\n    warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n\n  utype <- sort(unique(as.character(inpumat$type)))\n  ltype <- list(\"all\"=utype)\n  if(single.type) ltype <- c(ltype, as.list(utype))\n\n  res <- lapply(ltype, function(types) {\n    inpumat2 <- inpumat[inpumat$type %in% types, ]\n    inpumat2 <- inpumat2[complete.cases(inpumat2), ]\n\n    if (nrow(inpumat2) < 3 || length(unique(inpumat2$concentration)) < 2) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    } else {\n      rest <- if(nthread > 1) {\n        splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n        do.call(rbind, BiocParallel::bplapply(splitix, function(x, data, inpumat) {\n          t(apply(data[rownames(inpumat), x, drop=FALSE], 2, ToxicoGx::geneDrugPerturbation,\n                  concentration=inpumat$concentration, type=inpumat$type,\n                  batch=inpumat$"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/mrmr.cindex.ensemble.R",
    "language": "R",
    "content": "`mrmr.cindex.ensemble` <-\nfunction(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n\tnvar<-ncol(x)\n\tnsample<-nrow(x)\n\n\tvec_ensemble<-.Call(.C_mrmr_cIndex_ensemble_remove,data.matrix(x),as.integer(is.na(x)),maxparents,ncol(x),nrow(x),1,1,nboot,maxnsol,-1000,as.integer(as.logical(TRUE)),as.integer(sort(unique(strat))),as.integer(cl ),as.double(surv.time),as.integer(surv.event),as.double(weights),as.integer(strat),as.integer(sum(weights)),as.integer(as.logical(outx)),as.integer(length(strat)),as.integer(length(sort(unique(strat)))))\n\n\tvec_ensemble[2:vec_ensemble[1]+1]<-vec_ensemble[2:vec_ensemble[1]+1]-1\n\n\tmodels.equiv <- .extract.all.parents(x,vec_ensemble,maxparents,1)\n\tmodels.equiv[1,]<-rep(\"T\",ncol(models.equiv))\n\n\treturn(models.equiv)\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mrmr.cindex.ensemble` function and what are its main input parameters?",
        "answer": "The `mrmr.cindex.ensemble` function appears to be a statistical analysis tool for survival data. Its main purpose is likely to perform ensemble-based feature selection using the minimum redundancy maximum relevance (mRMR) criterion and concordance index (C-index) for survival analysis. The main input parameters include:\n- `x`: A matrix of predictor variables\n- `surv.time`: Survival time data\n- `surv.event`: Survival event indicator\n- `cl`: Likely cluster information for parallel processing\n- `weights`: Sample weights\n- `strat`: Stratification variable\n- `maxparents`: Maximum number of parent variables\n- `maxnsol`: Maximum number of solutions\n- `nboot`: Number of bootstrap iterations"
      },
      {
        "question": "Explain the significance of the `.Call` function in this code and what it's doing.",
        "answer": "The `.Call` function in this code is used to interface with compiled C code, specifically calling a function named `.C_mrmr_cIndex_ensemble_remove`. This approach is often used in R for performance-critical operations. The function is likely implemented in C for speed and efficiency, especially when dealing with large datasets or complex computations. The `.Call` function allows passing R objects directly to C code and receiving results back, enabling seamless integration of compiled code within the R environment. In this case, it's performing the core computational task of the ensemble method, processing the input data, and returning results that are then further processed in R."
      },
      {
        "question": "What does the function return, and how is this return value processed before being output?",
        "answer": "The function returns a matrix called `models.equiv`, which likely represents equivalent models or feature sets identified by the ensemble method. The processing of the return value involves several steps:\n1. The results from the C function call are stored in `vec_ensemble`.\n2. An index adjustment is made to `vec_ensemble` by subtracting 1 from a subset of its elements.\n3. The `.extract.all.parents` function is called with `x`, `vec_ensemble`, `maxparents`, and 1 as arguments to generate `models.equiv`.\n4. The first row of `models.equiv` is set to a vector of 'T' values.\n5. Finally, the processed `models.equiv` matrix is returned.\n\nThis processing suggests that the function is converting the raw output from the C function into a more interpretable format, possibly indicating selected features or model structures in a binary (True/False) matrix form."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  nvar <- ncol(x)\n  nsample <- nrow(x)\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  # Complete the function by adjusting vec_ensemble and extracting models\n\n}",
        "complete": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  nvar <- ncol(x)\n  nsample <- nrow(x)\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  vec_ensemble[2:vec_ensemble[1]+1] <- vec_ensemble[2:vec_ensemble[1]+1] - 1\n\n  models.equiv <- .extract.all.parents(x, vec_ensemble, maxparents, 1)\n  models.equiv[1,] <- rep(\"T\", ncol(models.equiv))\n\n  return(models.equiv)\n}"
      },
      {
        "partial": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  # Add necessary variable initializations\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  # Complete the function by adjusting vec_ensemble and extracting models\n\n}",
        "complete": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  nvar <- ncol(x)\n  nsample <- nrow(x)\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  vec_ensemble[2:vec_ensemble[1]+1] <- vec_ensemble[2:vec_ensemble[1]+1] - 1\n\n  models.equiv <- .extract.all.parents(x, vec_ensemble, maxparents, 1)\n  models.equiv[1,] <- rep(\"T\", ncol(models.equiv))\n\n  return(models.equiv)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassAssignmentMethods.R",
    "language": "R",
    "content": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_that()` function in this code snippet?",
        "answer": "The `test_that()` function is used to define a unit test in R. It takes two arguments: a description of the test (as a string) and a code block containing the test expectations. In this case, it's testing that 'multiplication works'."
      },
      {
        "question": "What does the `expect_equal()` function do in this test?",
        "answer": "The `expect_equal()` function is an expectation function from the testthat package in R. It compares two values for equality. In this case, it's checking if the result of `2 * 2` is equal to `4`. If the comparison is true, the test passes; if not, it fails."
      },
      {
        "question": "How would you modify this test to check if division works correctly?",
        "answer": "To check if division works correctly, you could modify the test as follows:\n\ntest_that(\"division works\", {\n  expect_equal(8 / 2, 4)\n})\n\nThis test would verify that 8 divided by 2 equals 4, thus testing the division operation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, ___)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      },
      {
        "partial": "test_that(___, {\n  expect_equal(2 * 2, 4)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/D.index.R",
    "language": "R",
    "content": "`D.index` <-\nfunction(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n\t\n\tstrata <- survival::strata\n\t#require(SuppDists)\n\tmethod.test <- match.arg(method.test)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n\t\t## remove weights=0 because the coxph function does not deal with them properly\n\t\tiix <- weights <= 0\n\t\tif(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n\t\tweights[iix] <- NA\n\t} else { weights <- rep(1,  length(x)) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1,  length(x)) }\n\tcc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n\tif(sum(cc.ix) < 3) {\n\t## not enough observations\n\t\tdata <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n\t\treturn(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n\t}\n\tif(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n\tsx <- x[cc.ix]\n\too <- order(sx, decreasing=FALSE)\n\tsx <- sx[oo]\n\tstime <- surv.time[cc.ix][oo]\n\tsevent <- surv.event[cc.ix][oo]\n\tsweights <- weights[cc.ix][oo]\n\tsstrat <- strat[cc.ix][oo]\n\tkap <- sqrt(8/pi)\n\tz <- kap^-1 * SuppDists::normOrder(N=length(sx))\n\t#ties?\n\tdup <- duplicated(sx)\n\tif(any(dup)) {\n\t\tudup <- unique(sx[dup])\n\t\tfor(i in 1:length(udup)) { z[sx == udup[i]] <- mean(z[sx == udup[i]]) }\n\t}\n\tz2 <- x\n\tz2[!cc.ix] <- NA\n\tz2[cc.ix] <- z[match(1:length(sx), oo)]\n\tdata <- list(\"x\"=x, \"z\"=z2, \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n\t#fit the cox model\n\toptions(warn=2)\n\trr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + z, weights=sweights, ...))\n\toptions(warn=0)\n\tif(class(rr) == \"try-error\") {\n\t\tres <- list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n\t} else {\n\t\tdicoef <- rr$coefficients\n\t\tdise <- sqrt(drop(rr$var))\n\t\tnames(dicoef) <- names(dise) <- NULL\n\t\tmystat <- NA\n\t\tswitch(method.test,\n\t\t\"logrank\"={\n\t\t\tmystat <- rr$score\n\t\t},\n\t\t\"likelihood.ratio\"={\n\t\t\tmysat <- 2 * (rr$loglik[2] - rr$loglik[1])\n\t\t},\n\t\t\"wald\"={\n\t\t\tmystats <- rr$wald.test\n\t\t\t##(hrcoef / hrse)^2\n\t\t})\n\t\tmypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n\t\tres <- list(\"d.index\"=exp(dicoef), \"coef\"=dicoef, \"se\"=dise, \"lower\"=exp(dicoef - qnorm(alpha / 2, lower.tail=FALSE) * dise), \"upper\"=exp(dicoef + qnorm(alpha / 2, lower.tail=FALSE) * dise), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n\t}\n\n\treturn(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `D.index` function and what are its main input parameters?",
        "answer": "The `D.index` function is used to calculate a D-index (discrimination index) for survival analysis. Its main input parameters are:\n- `x`: The predictor variable\n- `surv.time`: Survival time\n- `surv.event`: Event indicator\n- `weights`: Optional weights for each observation\n- `strat`: Optional stratification variable\n- `alpha`: Significance level for confidence intervals (default 0.05)\n- `method.test`: Test method ('logrank', 'likelihood.ratio', or 'wald')\n- `na.rm`: Whether to remove NA values"
      },
      {
        "question": "How does the function handle missing data and what happens if there are fewer than 3 complete cases?",
        "answer": "The function handles missing data as follows:\n1. It checks for complete cases using `complete.cases()` function.\n2. If `na.rm=FALSE` (default) and there are any NA values, it stops with an error.\n3. If there are fewer than 3 complete cases, it returns a list with NA values for most results, including the calculated indices.\n4. The function only proceeds with calculations if there are at least 3 complete cases.\n5. If `na.rm=TRUE`, it will remove NA values and continue with the analysis using the remaining complete cases."
      },
      {
        "question": "Explain the purpose of the `z` variable in the function and how it's calculated.",
        "answer": "The `z` variable in the function represents a normalized rank transformation of the input variable `x`. It's calculated as follows:\n1. The input `x` is ordered.\n2. `z` is initially calculated using `SuppDists::normOrder(N=length(sx))`, which returns the expected values of standard normal order statistics.\n3. This is then scaled by `kap^-1`, where `kap = sqrt(8/pi)`.\n4. For tied values in `x`, the corresponding `z` values are averaged.\n5. The purpose of this transformation is to create a normalized version of the input variable, which is then used in the Cox proportional hazards model to estimate the D-index."
      }
    ],
    "completion_tasks": [
      {
        "partial": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  strata <- survival::strata\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  # Complete the function here\n}",
        "complete": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  strata <- survival::strata\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  kap <- sqrt(8/pi)\n  z <- kap^-1 * SuppDists::normOrder(N=length(sx))\n  dup <- duplicated(sx)\n  if(any(dup)) {\n    udup <- unique(sx[dup])\n    for(i in seq_along(udup)) { z[sx == udup[i]] <- mean(z[sx == udup[i]]) }\n  }\n  z2 <- x\n  z2[!cc.ix] <- NA\n  z2[cc.ix] <- z[match(seq_along(sx), oo)]\n  data <- list(\"x\"=x, \"z\"=z2, \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + z, weights=sweights, ...))\n  options(warn=0)\n  if(inherits(rr, \"try-error\")) {\n    return(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  dicoef <- rr$coefficients\n  dise <- sqrt(drop(rr$var))\n  names(dicoef) <- names(dise) <- NULL\n  mystat <- switch(method.test,\n    \"logrank\" = rr$score,\n    \"likelihood.ratio\" = 2 * diff(rr$loglik),\n    \"wald\" = rr$wald.test\n  )\n  mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n  ci <- exp(dicoef + c(-1, 1) * qnorm(1 - alpha/2) * dise)\n  list(\"d.index\"=exp(dicoef), \"coef\"=dicoef, \"se\"=dise, \"lower\"=ci[1], \"upper\"=ci[2], \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n}"
      },
      {
        "partial": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  # Add necessary imports and argument checks\n  \n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    # Handle case with insufficient observations\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  \n  # Prepare data for Cox model\n  \n  # Fit Cox model and calculate statistics\n  \n  # Return results\n}",
        "complete": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  strata <- survival::strata\n  method.test <- match.arg(method.test)\n  \n  if(missing(weights)) weights <- rep(1, length(x))\n  else if(length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n  \n  if(missing(strat)) strat <- rep(1, length(x))\n  else if(length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n  \n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(x=x, z=rep(NA, length(x)), surv.time=surv.time, surv.event=surv.event, weights=weights, strat=strat)\n    return(list(d.index=NA, coef=NA, se=NA, lower=NA, upper=NA, p.value=NA, n=sum(cc.ix), coxm=NA, data=data))\n  }\n  if(any(!cc.ix) & !na.rm) stop(\"NA values are present!\")\n  \n  sx <- x[cc.ix]\n  oo <- order(sx)\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  \n  z <- sqrt(8/pi)^-1 * SuppDists::normOrder(N=length(sx))\n  z[duplicated(sx)] <- ave(z, sx, FUN = mean)[duplicated(sx)]\n  \n  z2 <- x\n  z2[cc.ix] <- z[order(oo)]\n  z2[!cc.ix] <- NA\n  \n  data <- list(x=x, z=z2, surv.time=surv.time, surv.event=surv.event, weights=weights, strat=strat)\n  \n  rr <- tryCatch({\n    coxph(Surv(stime, sevent) ~ strata(sstrat) + z, weights=sweights, ...)\n  }, error = function(e) NULL)\n  \n  if(is.null(rr)) {\n    return(list(d.index=NA, coef=NA, se=NA, lower=NA, upper=NA, p.value=NA, n=sum(cc.ix), coxm=NA, data=data))\n  }\n  \n  dicoef <- rr$coefficients\n  dise <- sqrt(drop(rr$var))\n  mystat <- switch(method.test,\n    logrank = rr$score,\n    likelihood.ratio = 2 * diff(rr$loglik),\n    wald = rr$wald.test\n  )\n  mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n  ci <- exp(dicoef + c(-1, 1) * qnorm(1 - alpha/2) * dise)\n  \n  list(d.index=exp(dicoef), coef=dicoef, se=dise, lower=ci[1], upper=ci[2],\n       p.value=mypp, n=rr$n, coxm=rr, data=data)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/zzz.R",
    "language": "R",
    "content": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", pkgname, libname)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .onLoad function in R package development?",
        "answer": "The .onLoad function is a special function in R package development that is automatically called when the package is loaded. It is used to perform initialization tasks, such as loading dynamic libraries or setting up package-specific environments. In this case, it's used to load a dynamic library named 'survcomp' using the library.dynam function."
      },
      {
        "question": "What are the parameters 'libname' and 'pkgname' in the .onLoad function, and how are they used?",
        "answer": "The parameters 'libname' and 'pkgname' are automatically provided by R when the package is loaded. 'libname' is the path where the package is installed, and 'pkgname' is the name of the package. In this function, they are used as arguments to the library.dynam function to correctly locate and load the dynamic library associated with the package."
      },
      {
        "question": "What does the library.dynam function do in this context, and why is it important?",
        "answer": "The library.dynam function is used to load compiled code (typically C or Fortran) that is part of the R package. It's important because it allows the package to include and use efficient, compiled code alongside R functions. In this case, it's loading a library named 'survcomp', which likely contains compiled functions that are essential for the package's functionality."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onLoad <- function(libname, pkgname)\n{\n  # Load the dynamic library for the package\n  \n}",
        "complete": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", pkgname, libname)\n}"
      },
      {
        "partial": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", _, _)\n}",
        "complete": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", pkgname, libname)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/oncotypedx.R",
    "language": "R",
    "content": "#' @title Function to compute the OncotypeDX signature as published by\n#'   Paik et al. in 2004.\n#'\n#' @description\n#' This function computes signature scores and risk classifications from\n#'   gene expression values following the algorithm used for the OncotypeDX\n#'   signature as published by Paik et al. 2004.\n#'\n#' @usage\n#' oncotypedx(data, annot, do.mapping = FALSE, mapping, do.scaling=TRUE,\n#'   verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and\n#'   probes in columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must\n#'   be performed (in case of ambiguities, the most variant probe is kept\n#'   for each gene), FALSE otherwise. Note that for Affymetrix HGU\n#'   datasets, the mapping is not necessary.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used\n#'   to force the mapping such that the probes are not selected based on\n#'   their variance.\n#' @param do.scaling Should the data be scaled?\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' Note that for Affymetrix HGU datasets, the mapping is not necessary.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' S. Paik, S. Shak, G. Tang, C. Kim, J. Bakker, M. Cronin, F. L. Baehner,\n#'   M. G. Walker, D. Watson, T. Park, W. Hiller, E. R. Fisher, D. L. Wickerham,\n#'   J. Bryant, and N. Wolmark (2004) \"A Multigene Assay to Predict Recurrence\n#'   of Tamoxifen-Treated, Node-Negative Breast Cancer\", New England Journal\n#'   of Medicine, 351(27):2817-2826.\n#'\n#' @examples\n#' # load GENE70 signature\n#' data(sig.oncotypedx)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' rs.nkis <- oncotypedx(data=data.nkis, annot=annot.nkis, do.mapping=TRUE)\n#' table(rs.nkis$risk)\n#'\n#' @md\n#' @export\noncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE,\n                       verbose=FALSE) {\n\t\n\tif (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n\n\t## the reference genes are not taken into account due to their absence from most platforms\n\tsig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n\tdimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n\tgt <- nrow(sig2)\n\tif(do.mapping) { ## not an affy HGU platform\n\t\tgid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid1) <- dimnames(sig2)[[1]]\n\t\tgid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\t\t## mqpping\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tgm <- length(rr$geneid2)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tif(length(rr$geneid1) != gt) { ## some genes are missing\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n\t\t}\n\t\tgid1 <- rr$geneid2\n\t\tgid2 <- rr$geneid1\n\t\tdata <- rr$data1\n\t\tmyprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n\t\t## change the names of probes in the data\n\t\tdimnames(data)[[2]] <- names(gid2) <- names(gid1)\n\t} else {\n\t\tmyprobe <- NA\n\t\tdata <- data[ ,intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])]\n\t\tgm <- ncol(data)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tif(nrow(sig2) != ncol(data)) { ## some genes are missing\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\", ncol(data), gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=myprobe))\n\t\t}\n\t}\n\t## rename gene names by the gene symbols\n\tdimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n\n\tif (do.scaling) {\n\t\t## scaling between 0 and 15\n\t\tdata <- apply(data, 2, function(x) { xx <- (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE)); return(xx * 15) })\n\t} else if (max(data, na.rm=TRUE) > 20 || min(data, na.rm=TRUE) < -5) {\n\t\t## check that each gene expression lies approximately in [0, 15]\n\t\twarning(\"The max and min values of your data suggest it is not already scaled...\n\t\t\tIf it is please ignore this message, otherwise set `do.scaling=TRUE` to scale in the function call.\"\n\t\t\t)\n\t}\n\n\t## OcotypeDX recurrence score\n\t## GRB7 group score = 0.9 * GRB7 + 0.1 * HER2 if result < 8, then result = 8\n\t## ER group score = (0.8 * ER + 1.2 * PGR + BCL2 + SCUBE2) / 4\n\t## proliferation group score = ( survivin + KI67 + MYBL2 + CCNB1 + STK15) / 5 if result < 6.5, then result = 6.5\n\t## invasion group score = (CTSL2 + MMP11) / 2\n\t## RSU = + 0.47 * GRB7 group score - 0.34 * ER group score + 1.04 * proliferation group score + 0.10 * invasion group score + 0.05 * CD68 - 0.08 GSTM1 - 0.07 * BAG1\n\n\tcc.ix <- complete.cases(data)\n\trs <- rs.unscaled <- rsrisk <- NULL\n\tfor (i in 1:nrow(data)) {\n\t\tif(cc.ix[i]) {\n\t\t\tgrb7.gs <- 0.9 * data[i, \"GRB7\"] + 0.1 * data[i, \"ERBB2\"]\n\t\t\tif (grb7.gs < 8) { grb7.gs <- 8 }\n\n\t\t\ter.gs <- (0.8 * data[i, \"ESR1\"] + 1.2 * data[i, \"PGR\"] + data[i, \"BCL2\"] + data[i, \"SCUBE2\"]) / 4\n\n\t\t\tproliferation.gs <- (data[i, \"BIRC5\"] + data[i, \"MKI67\"] + data[i, \"MYBL2\"] + data[i, \"CCNB1\"] + data[i, \"AURKA\"]) / 5\n\t\t\tif (proliferation.gs < 6.5) { proliferation.gs <- 6.5 }\n\n\t\t\tinvasion.gs <- (data[i, \"CTSL2\"] + data[i, \"MMP11\"]) / 2\n\n\t\t\trsu <- 0.47 * (grb7.gs) - 0.34 * (er.gs) + 1.04 * (proliferation.gs) + 0.1 * (invasion.gs) + 0.05 * data[i, \"CD68\"] - 0.08 * data[i, \"GSTM1\"] - 0.07 * data[i, \"BAG1\"]\n\t\t\t## rescale the score\n\t\t\trsu2 <- rsu\n\t\t\tif(rsu >= 0 & rsu <= 100) { rsu <- 20 * (rsu - 6.7) }\n\t\t\tif(rsu < 0) { rsu <- 0 }\n\t\t\tif(rsu > 100) { rsu <- 100 }\n\t\t\t## use of the official curoffs\n\t\t\tif(rsu < 18) { rsr <- 0 }\n\t\t\tif(rsu >= 18 & rsu < 31) { rsr <- 0.5 }\n\t\t\tif(rsu >= 31) { rsr <- 1 }\n\t\t}\n\t\telse { rsu <- rsr <- rsu2 <- NA }\n\t\trs.unscaled <- c(rs.unscaled, rsu2)\n\t\trs <- c(rs, rsu)\n\t\trsrisk <- c(rsrisk, rsr)\n\t}\n\tnames(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n\treturn(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  # TODO: Implement mapping logic\n  \n  # TODO: Implement scaling and score calculation\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
        "complete": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(length(rr$geneid1) != gt) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n  } else {\n    myprobe <- NA\n    data <- data[ ,intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(nrow(sig2) != ncol(data)) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", ncol(data), gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=myprobe))\n    }\n  }\n  \n  dimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n  \n  if (do.scaling) {\n    data <- apply(data, 2, function(x) { xx <- (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE)); return(xx * 15) })\n  } else if (max(data, na.rm=TRUE) > 20 || min(data, na.rm=TRUE) < -5) {\n    warning(\"The max and min values of your data suggest it is not already scaled...\\nIf it is please ignore this message, otherwise set `do.scaling=TRUE` to scale in the function call.\")\n  }\n  \n  cc.ix <- complete.cases(data)\n  rs <- rs.unscaled <- rsrisk <- NULL\n  for (i in 1:nrow(data)) {\n    if(cc.ix[i]) {\n      grb7.gs <- max(0.9 * data[i, \"GRB7\"] + 0.1 * data[i, \"ERBB2\"], 8)\n      er.gs <- (0.8 * data[i, \"ESR1\"] + 1.2 * data[i, \"PGR\"] + data[i, \"BCL2\"] + data[i, \"SCUBE2\"]) / 4\n      proliferation.gs <- max((data[i, \"BIRC5\"] + data[i, \"MKI67\"] + data[i, \"MYBL2\"] + data[i, \"CCNB1\"] + data[i, \"AURKA\"]) / 5, 6.5)\n      invasion.gs <- (data[i, \"CTSL2\"] + data[i, \"MMP11\"]) / 2\n      rsu <- 0.47 * grb7.gs - 0.34 * er.gs + 1.04 * proliferation.gs + 0.1 * invasion.gs + 0.05 * data[i, \"CD68\"] - 0.08 * data[i, \"GSTM1\"] - 0.07 * data[i, \"BAG1\"]\n      rsu2 <- rsu\n      rsu <- max(min(20 * (rsu - 6.7), 100), 0)\n      rsr <- cut(rsu, breaks=c(-Inf, 18, 31, Inf), labels=c(0, 0.5, 1))\n    } else { rsu <- rsr <- rsu2 <- NA }\n    rs.unscaled <- c(rs.unscaled, rsu2)\n    rs <- c(rs, rsu)\n    rsrisk <- c(rsrisk, rsr)\n  }\n  names(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  # TODO: Implement mapping logic\n  \n  dimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n  \n  # TODO: Implement scaling logic\n  \n  # TODO: Implement score calculation\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
        "complete": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(length(rr$geneid1) != gt) {\n      return(list(\"score\"=rep(NA, nrow(data)), \"risk\"=rep(NA, nrow(data)), \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(rr$geneid2), \"EntrezGene.ID\"=rr$geneid2, \"new.probe\"=names(rr$geneid1))\n    dimnames(data)[[2]] <- names(rr$geneid1) <- names(rr$geneid2)\n  } else {\n    data <- data[ ,intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])]\n    mymapping <- c(\"mapped\"=ncol(data), \"total\"=gt)\n    myprobe <- NA\n    if(nrow(sig2) != ncol(data)) {\n      return(list(\"score\"=rep(NA, nrow(data)), \"risk\"=rep(NA, nrow(data)), \"mapping\"=mymapping, \"probe\"=myprobe))\n    }\n  }\n  \n  dimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n  \n  if (do.scaling) {\n    data <- apply(data, 2, function(x) { (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE)) * 15 })\n  } else if (max(data, na.rm=TRUE) > 20 || min(data, na.rm=TRUE) < -5) {\n    warning(\"Data may not be scaled. Consider setting do.scaling=TRUE.\")\n  }\n  \n  cc.ix <- complete.cases(data)\n  rs <- rsrisk <- numeric(nrow(data))\n  for (i in which(cc.ix)) {\n    grb7.gs <- max(0.9 * data[i, \"GRB7\"] + 0.1 * data[i, \"ERBB2\"], 8)\n    er.gs <- (0.8 * data[i, \"ESR1\"] + 1.2 * data[i, \"PGR\"] + data[i, \"BCL2\"] + data[i, \"SCUBE2\"]) / 4\n    proliferation.gs <- max((data[i, \"BIRC5\"] + data[i, \"MKI67\"] + data[i, \"MYBL2\"] + data[i, \"CCNB1\"] + data[i, \"AURKA\"]) / 5, 6.5)\n    invasion.gs <- (data[i, \"CTSL2\"] + data[i, \"MMP11\"]) / 2\n    rsu <- 0.47 * grb7.gs - 0.34 * er.gs + 1.04 * proliferation.gs + 0.1 * invasion.gs + 0.05 * data[i, \"CD68\"] - 0.08 * data[i, \"GSTM1\"] - 0.07 * data[i, \"BAG1\"]\n    rs[i] <- max(min(20 * (rsu - 6.7), 100), 0)\n    rsrisk[i] <- cut(rs[i], breaks=c(-Inf, 18, 31, Inf), labels=c(0, 0.5, 1))\n  }\n  names(rs) <- names(rsrisk) <- rownames(data)\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/extract.all.parents.R",
    "language": "R",
    "content": "'.extract.all.parents'  <-\nfunction(data,res.main,maxparents,predn) {\n### function taking the output of the regrnet.ensemble method and returns a matrix\n### containing one equivalent model in each column. The target variable is in the first row.\n### res.main: \toutput of regrnet.ensemble\n### maxparents:\tmaxparents parameter of the netinf method\n### predn:\tlist of target variables for which ensemble method was run.\n\n\tfinal <- NULL\n\tcnt_main <- 1\n\tfor(imain in 1:length(predn)){\n\t\tres.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n\t\tif(length(res.vec)>3){\n\t\t\tcnt_main <- cnt_main+2*res.main[cnt_main]+1\n\t\t\tnsol <- sum(res.vec==0)\n\t\t\tres <- matrix(0,ncol=nsol,nrow=(maxparents+1))\n\n\t\t\tval <- res.vec[2:(res.vec[1]+1)]\n\t\t\tind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n\t\t\tres[1,1:ind[1]] <- rep(val[1],ind[1])\n\t\t\tnvar <- length(val)\n\t\t\tlevel <- 2\n\t\t\tcnt <- 1\n\t\t\tnelem <- 0\n\t\t\tsum_old <- sum(ind[1])\n\t\t\tlast_level <- FALSE\n\t\t\ti <- 2\n\t\t\tind2 <- 1\n\n\t\t\twhile(i<=nvar && !last_level){\n\t\t\t\tif(ind[i]!=0){\n\t\t\t\t\tif(ind[i]>1){\n\t\t\t\t\t\ttmp <- res[,(ind2+1):nsol]\n\t\t\t\t\t\tres[level,ind2] <- val[i]\n\t\t\t\t\t\tfor(j in 1:level){\n\t\t\t\t\t\t\tres[j,(ind2+1):(ind2+ind[i]-1)] <- rep(res[j,ind2],(ind[i]-1))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif((nsol-(ind2+ind[i]-1))>0){\n\t\t\t\t\t\t\tres <- cbind(res[,1:(ind2+ind[i]-1)],tmp[,1:(nsol-(ind2+ind[i]-1))])\n\t\t\t\t\t\t}else{\n\t\t\t\t\t\t\tres <- res[,1:(ind2+ind[i]-1)]\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tres[level,ind2:(ind2-1+ind[i])] <- rep(val[i],ind[i])\n\t\t\t\t\tind2 <- ind2+ind[i]\n\t\t\t\t}else{\n\t\t\t\t\tres[level,ind2] <- val[i]\n\t\t\t\t\tind2 <- ind2+1\n\t\t\t\t}\n\t\t\t\tnelem <- nelem+ind[i]\n\t\t\t\tif(cnt==sum_old){\n\t\t\t\t\tsum_old <- nelem\n\t\t\t\t\tif(nelem==0){\n\t\t\t\t\t\tlast_level <- TRUE\n\t\t\t\t\t}\n\t\t\t\t\tnelem <- 0\n\t\t\t\t\tcnt <- 1\n\t\t\t\t\tlevel <- level+1\n\t\t\t\t\tind2 <- 1\n\t\t\t\t}\n\t\t\t\telse if(cnt< sum_old){\n\t\t\t\t\tcnt <- cnt+1\n\t\t\t\t}\n\t\t\t\ti <- i+1\n\t\t\t}\n\t\t\tfinal <- cbind(final,res)\n\t\t}\n\t}\n\tdimension <- dim(final)\n\tfinal <- colnames(data)[final]\n\tdim(final) <- dimension\n\treturn(final)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the '.extract.all.parents' function and what are its main input parameters?",
        "answer": "The '.extract.all.parents' function is designed to process the output of the regrnet.ensemble method and return a matrix containing one equivalent model in each column, with the target variable in the first row. The main input parameters are:\n1. data: The original dataset (implied by the use of colnames(data))\n2. res.main: The output of the regrnet.ensemble method\n3. maxparents: The maxparents parameter of the netinf method\n4. predn: A list of target variables for which the ensemble method was run"
      },
      {
        "question": "Explain the significance of the nested loop structure in the function and how it processes the input data.",
        "answer": "The nested loop structure in the function is crucial for processing the complex input data structure:\n1. The outer loop (for(imain in 1:length(predn))) iterates over each target variable in the predn list.\n2. Within this loop, there's a while loop that processes the data for each target variable, constructing the result matrix (res).\n3. The while loop handles different levels of parent variables, filling the res matrix column by column.\n4. It uses multiple counters and conditions (like cnt, ind2, last_level) to keep track of the current position and state in the data processing.\nThis structure allows the function to handle varying numbers of parent variables and solutions for each target variable, creating a flexible and comprehensive output."
      },
      {
        "question": "How does the function handle the conversion of numeric indices to column names in the final output, and why is this step important?",
        "answer": "The function handles the conversion of numeric indices to column names in the final output through these steps:\n1. It first processes all the data using numeric indices, storing the results in the 'final' matrix.\n2. After all processing is complete, it captures the dimensions of the 'final' matrix.\n3. It then uses these numeric indices to select the corresponding column names from the original data: final <- colnames(data)[final]\n4. Finally, it restores the original dimensions: dim(final) <- dimension\n\nThis step is important because:\n1. It converts the numeric representation of variables to their actual names, making the output more interpretable and usable.\n2. It maintains the connection between the processed results and the original dataset.\n3. It allows users to easily identify which variables are parents for each target variable in the resulting models.\n4. It enhances the readability and usability of the output for further analysis or model interpretation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      nvar <- length(val)\n      level <- 2\n      cnt <- 1\n      nelem <- 0\n      sum_old <- sum(ind[1])\n      last_level <- FALSE\n      i <- 2\n      ind2 <- 1\n\n      while(i <= nvar && !last_level) {\n        # Complete the while loop logic here\n      }\n      final <- cbind(final, res)\n    }\n  }\n  dimension <- dim(final)\n  final <- colnames(data)[final]\n  dim(final) <- dimension\n  return(final)\n}",
        "complete": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      nvar <- length(val)\n      level <- 2\n      cnt <- 1\n      nelem <- 0\n      sum_old <- sum(ind[1])\n      last_level <- FALSE\n      i <- 2\n      ind2 <- 1\n\n      while(i <= nvar && !last_level) {\n        if(ind[i] != 0) {\n          if(ind[i] > 1) {\n            tmp <- res[,(ind2+1):nsol]\n            res[level,ind2] <- val[i]\n            for(j in 1:level) {\n              res[j,(ind2+1):(ind2+ind[i]-1)] <- rep(res[j,ind2],(ind[i]-1))\n            }\n            if((nsol-(ind2+ind[i]-1)) > 0) {\n              res <- cbind(res[,1:(ind2+ind[i]-1)], tmp[,1:(nsol-(ind2+ind[i]-1))])\n            } else {\n              res <- res[,1:(ind2+ind[i]-1)]\n            }\n          }\n          res[level,ind2:(ind2-1+ind[i])] <- rep(val[i],ind[i])\n          ind2 <- ind2 + ind[i]\n        } else {\n          res[level,ind2] <- val[i]\n          ind2 <- ind2 + 1\n        }\n        nelem <- nelem + ind[i]\n        if(cnt == sum_old) {\n          sum_old <- nelem\n          if(nelem == 0) {\n            last_level <- TRUE\n          }\n          nelem <- 0\n          cnt <- 1\n          level <- level + 1\n          ind2 <- 1\n        } else if(cnt < sum_old) {\n          cnt <- cnt + 1\n        }\n        i <- i + 1\n      }\n      final <- cbind(final, res)\n    }\n  }\n  dimension <- dim(final)\n  final <- colnames(data)[final]\n  dim(final) <- dimension\n  return(final)\n}"
      },
      {
        "partial": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      # Complete the rest of the function here\n    }\n  }\n  # Final processing steps\n}",
        "complete": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      nvar <- length(val)\n      level <- 2\n      cnt <- 1\n      nelem <- 0\n      sum_old <- sum(ind[1])\n      last_level <- FALSE\n      i <- 2\n      ind2 <- 1\n\n      while(i <= nvar && !last_level) {\n        if(ind[i] != 0) {\n          if(ind[i] > 1) {\n            tmp <- res[,(ind2+1):nsol]\n            res[level,ind2] <- val[i]\n            for(j in 1:level) {\n              res[j,(ind2+1):(ind2+ind[i]-1)] <- rep(res[j,ind2],(ind[i]-1))\n            }\n            if((nsol-(ind2+ind[i]-1)) > 0) {\n              res <- cbind(res[,1:(ind2+ind[i]-1)], tmp[,1:(nsol-(ind2+ind[i]-1))])\n            } else {\n              res <- res[,1:(ind2+ind[i]-1)]\n            }\n          }\n          res[level,ind2:(ind2-1+ind[i])] <- rep(val[i],ind[i])\n          ind2 <- ind2 + ind[i]\n        } else {\n          res[level,ind2] <- val[i]\n          ind2 <- ind2 + 1\n        }\n        nelem <- nelem + ind[i]\n        if(cnt == sum_old) {\n          sum_old <- nelem\n          if(nelem == 0) {\n            last_level <- TRUE\n          }\n          nelem <- 0\n          cnt <- 1\n          level <- level + 1\n          ind2 <- 1\n        } else if(cnt < sum_old) {\n          cnt <- cnt + 1\n        }\n        i <- i + 1\n      }\n      final <- cbind(final, res)\n    }\n  }\n  dimension <- dim(final)\n  final <- colnames(data)[final]\n  dim(final) <- dimension\n  return(final)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/mrmr.cindex.R",
    "language": "R",
    "content": "`mrmr.cindex` <-\nfunction(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\tnvar<-ncol(x)\n\tnsample<-nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) { stop(\"bad length for parameter weights!\") }\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) { stop(\"weights must be a number between 0 and 1!\") }\n\t} else { weights <- rep(1, nsample) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1, nsample) }\n\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) { stop(\"binary classes and survival data are missing!\") }\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) { stop(\"choose binary classes or survival data but not both!\") }\n\n\n\tres_cIndex<-rep(0,nvar)\n\n\t\tmsurv <- FALSE\n\t\tif(missing(cl)) { ## survival data\n\t\t\tmsurv <- TRUE\n\t\t\tcl <- rep(0, nsample)\n\t\t} else { surv.time <- surv.event <- rep(0, nsample) } ## binary classes\n\n\n\t#### get cIndex for each variable in dataset with surv.time, surv.event ####\n\tfor(ivar in 1:nvar){\n\t\tis.correct <- TRUE\n\t\txx <- x[ ,ivar]\n\t\tcc.ix <- complete.cases(xx, surv.time, surv.event, cl, weights, strat)\n\n\t\tif(sum(cc.ix) < 3) {\n\t\t\t## not enough observations\n\t\t\tif(msurv) { data <- list(\"x\"=xx, \"surv.time\"=surv.time, \"surv.event\"=surv.event) } else { data  <- list(\"x\"=xx, \"cl\"=cl) }\n\t\t\treturn(list(\"c.index\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"data\"=data, \"comppairs\"=NA))\n\t\t}\n\n\t\tif(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n\n\t\t# remove samples whose the weight is equal to 0 to speed up the computation of the concordance index\n\t\tcc.ix <- cc.ix & weights != 0\n\t\tx2 <- xx[cc.ix]\n\t\tcl2 <- cl[cc.ix]\n\t\tst <- surv.time[cc.ix]\n\t\tse <- surv.event[cc.ix]\n\t\tif(msurv && sum(se) == 0) {\n\t\t\twarning(paste(\"\\nno events, the concordance index cannot be computed for variable \", colnames(x)[ivar],\" !\"))\n\t\t\tres_cIndex[ivar]<-NA\n\t\t\tis.correct<-FALSE\n\t\t}\n\t\tif(!msurv && length(unique(cl2)) == 1) {\n\t\t\twarning(paste(\"\\nonly one class, the concordance index cannot be computed for variable\", colnames(x)[ivar],\" !\"))\n\t\t\tres_cIndex[ivar]<-NA\n\t\t\tis.correct<-FALSE\n\t\t}\n\t\tweights <- weights[cc.ix]\n\t\tstrat <- strat[cc.ix]\n\t\tstrat <- as.numeric(as.factor(strat))\n\t\tustrat <- sort(unique(strat)) ## to check later\n\t\tN <- sum(weights) ##length(x2)\n\t\tif(N <= 1) {\n\t\t\twarning(paste(\"\\nweights of observations are too small (sum should be > 1), the concordance index cannot be computed for variable\", colnames(x)[ivar],\" !\"))\n\t\t\tres_cIndex[ivar]<-NA\n\t\t\tis.correct<-FALSE\n\t\t}\n\n\t\tlenS <- length(strat)\n\t\tlenU <- length(ustrat)\n\t\tif(is.correct){\n\t\t\tres_cIndex[ivar] <- .Call(.C_get_concordanceIndex_onevariable, as.integer(as.logical(msurv)), as.integer(ustrat), as.double(x2),as.integer(cl2), as.double(st), as.integer(se), as.double(weights), as.integer(strat),as.integer(N), as.integer(as.logical(outx)), as.integer(lenS), as.integer(lenU))\n\t\t}\n\t}\n\n\tres_mrmr_cIndex <- .Call(.C_mrmr_cIndex, data.matrix(x),as.integer(is.na(x)),as.double(res_cIndex),ncol(x), nrow(x), -1000)\n\tnames(res_mrmr_cIndex)<-colnames(x)\n\treturn(res_mrmr_cIndex)\n\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mrmr.cindex` function and what are its main input parameters?",
        "answer": "The `mrmr.cindex` function calculates the concordance index for each variable in a dataset, either for survival data or binary classification. Its main input parameters are:\n- `x`: A matrix of predictor variables\n- `surv.time` and `surv.event`: Survival time and event indicators for survival analysis\n- `cl`: Binary class labels for classification\n- `weights`: Optional observation weights\n- `strat`: Optional stratification variable\n- Additional parameters for controlling the analysis method and output"
      },
      {
        "question": "How does the function handle missing data and what options are available for dealing with it?",
        "answer": "The function handles missing data in several ways:\n1. It checks for complete cases using `complete.cases()` function.\n2. If there are fewer than 3 complete observations, it returns NA values for the results.\n3. The `na.rm` parameter controls whether to stop execution when NA values are present (default) or to remove them.\n4. If `na.rm=FALSE` and NA values are present, the function stops with an error message.\n5. The function also checks for and handles cases where weights are 0, effectively removing those observations from the analysis."
      },
      {
        "question": "What is the significance of the `.Call` function used in this code and what does it do?",
        "answer": "The `.Call` function is used to invoke compiled C code from within R, which can significantly improve performance for computationally intensive tasks. In this function, it's used twice:\n1. To calculate the concordance index for each variable using the C function `get_concordanceIndex_onevariable`.\n2. To perform the MRMR (Minimum Redundancy Maximum Relevance) calculation using the C function `mrmr_cIndex`.\nThese C functions likely implement efficient algorithms for these calculations, allowing the R function to handle large datasets more quickly than if implemented purely in R."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\tnvar <- ncol(x)\n\tnsample <- nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) { stop(\"bad length for parameter weights!\") }\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) { stop(\"weights must be a number between 0 and 1!\") }\n\t} else { weights <- rep(1, nsample) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1, nsample) }\n\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) { stop(\"binary classes and survival data are missing!\") }\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) { stop(\"choose binary classes or survival data but not both!\") }\n\n\tres_cIndex <- rep(0, nvar)\n\tmsurv <- FALSE\n\tif(missing(cl)) {\n\t\tmsurv <- TRUE\n\t\tcl <- rep(0, nsample)\n\t} else { surv.time <- surv.event <- rep(0, nsample) }\n\n\t# TODO: Implement the main loop for calculating cIndex\n\n\t# TODO: Calculate mrmr_cIndex\n\n\treturn(res_mrmr_cIndex)\n}",
        "complete": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\tnvar <- ncol(x)\n\tnsample <- nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) { stop(\"bad length for parameter weights!\") }\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) { stop(\"weights must be a number between 0 and 1!\") }\n\t} else { weights <- rep(1, nsample) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1, nsample) }\n\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) { stop(\"binary classes and survival data are missing!\") }\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) { stop(\"choose binary classes or survival data but not both!\") }\n\n\tres_cIndex <- rep(0, nvar)\n\tmsurv <- FALSE\n\tif(missing(cl)) {\n\t\tmsurv <- TRUE\n\t\tcl <- rep(0, nsample)\n\t} else { surv.time <- surv.event <- rep(0, nsample) }\n\n\tfor(ivar in 1:nvar) {\n\t\txx <- x[, ivar]\n\t\tcc.ix <- complete.cases(xx, surv.time, surv.event, cl, weights, strat)\n\t\tif(sum(cc.ix) < 3 || any(!cc.ix) & !na.rm) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tcontinue\n\t\t}\n\t\tcc.ix <- cc.ix & weights != 0\n\t\tx2 <- xx[cc.ix]\n\t\tcl2 <- cl[cc.ix]\n\t\tst <- surv.time[cc.ix]\n\t\tse <- surv.event[cc.ix]\n\t\tweights <- weights[cc.ix]\n\t\tstrat <- as.numeric(as.factor(strat[cc.ix]))\n\t\tustrat <- sort(unique(strat))\n\t\tN <- sum(weights)\n\t\tif(N <= 1 || (msurv && sum(se) == 0) || (!msurv && length(unique(cl2)) == 1)) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tcontinue\n\t\t}\n\t\tres_cIndex[ivar] <- .Call(.C_get_concordanceIndex_onevariable, as.integer(msurv), as.integer(ustrat), as.double(x2), as.integer(cl2), as.double(st), as.integer(se), as.double(weights), as.integer(strat), as.integer(N), as.integer(outx), as.integer(length(strat)), as.integer(length(ustrat)))\n\t}\n\n\tres_mrmr_cIndex <- .Call(.C_mrmr_cIndex, data.matrix(x), as.integer(is.na(x)), as.double(res_cIndex), ncol(x), nrow(x), -1000)\n\tnames(res_mrmr_cIndex) <- colnames(x)\n\treturn(res_mrmr_cIndex)\n}"
      },
      {
        "partial": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\t# TODO: Initialize variables and perform input validation\n\n\t# TODO: Handle missing cl or survival data\n\n\tfor(ivar in 1:nvar) {\n\t\t# TODO: Extract and process data for current variable\n\n\t\t# TODO: Check for valid data and compute cIndex\n\n\t\t# TODO: Call C function to compute concordance index\n\t}\n\n\t# TODO: Compute mrmr_cIndex using C function\n\n\treturn(res_mrmr_cIndex)\n}",
        "complete": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\tnvar <- ncol(x)\n\tnsample <- nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) stop(\"bad length for parameter weights!\")\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) stop(\"weights must be a number between 0 and 1!\")\n\t} else weights <- rep(1, nsample)\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) stop(\"bad length for parameter strat!\")\n\t} else strat <- rep(1, nsample)\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) stop(\"binary classes and survival data are missing!\")\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) stop(\"choose binary classes or survival data but not both!\")\n\n\tres_cIndex <- rep(0, nvar)\n\tmsurv <- missing(cl)\n\tif(msurv) {\n\t\tcl <- rep(0, nsample)\n\t} else surv.time <- surv.event <- rep(0, nsample)\n\n\tfor(ivar in 1:nvar) {\n\t\txx <- x[, ivar]\n\t\tcc.ix <- complete.cases(xx, surv.time, surv.event, cl, weights, strat) & weights != 0\n\t\tif(sum(cc.ix) < 3 || (any(!cc.ix) && !na.rm)) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tnext\n\t\t}\n\t\tx2 <- xx[cc.ix]\n\t\tcl2 <- cl[cc.ix]\n\t\tst <- surv.time[cc.ix]\n\t\tse <- surv.event[cc.ix]\n\t\tweights <- weights[cc.ix]\n\t\tstrat <- as.numeric(as.factor(strat[cc.ix]))\n\t\tustrat <- sort(unique(strat))\n\t\tN <- sum(weights)\n\t\tif(N <= 1 || (msurv && sum(se) == 0) || (!msurv && length(unique(cl2)) == 1)) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tnext\n\t\t}\n\t\tres_cIndex[ivar] <- .Call(.C_get_concordanceIndex_onevariable, as.integer(msurv), as.integer(ustrat), as.double(x2), as.integer(cl2), as.double(st), as.integer(se), as.double(weights), as.integer(strat), as.integer(N), as.integer(outx), as.integer(length(strat)), as.integer(length(ustrat)))\n\t}\n\n\tres_mrmr_cIndex <- .Call(.C_mrmr_cIndex, data.matrix(x), as.integer(is.na(x)), as.double(res_cIndex), nvar, nsample, -1000)\n\tnames(res_mrmr_cIndex) <- colnames(x)\n\treturn(res_mrmr_cIndex)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/globals.R",
    "language": "R",
    "content": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', 'duration'))",
    "qa_pairs": [
      {
        "question": "What is the purpose of using utils::globalVariables() in this code snippet?",
        "answer": "The utils::globalVariables() function is used to declare global variables for dplyr and data.table column names. This prevents R CMD check from generating NOTE messages about undefined global variables when these column names are used in non-standard evaluation contexts within dplyr or data.table operations."
      },
      {
        "question": "How many and what types of variables are being declared as global in this code?",
        "answer": "The code declares 21 global variables. These include a mix of column names (e.g., 'sampleid', 'treatmentid', 'Symbol'), function names (e.g., 'read.csv'), and special symbols (e.g., '.'). The variables represent various aspects of data analysis, including sample information, treatment levels, and statistical measures."
      },
      {
        "question": "Why might a programmer choose to use globalVariables() instead of explicitly defining these variables in their code?",
        "answer": "A programmer might choose to use globalVariables() for several reasons: 1) It allows the use of these variables in dplyr or data.table operations without R CMD check warnings. 2) It keeps the code cleaner by avoiding the need to repeatedly define or pass these variables. 3) It's particularly useful when working with large datasets where column names are used frequently in data manipulation operations. However, it's important to use this approach judiciously, as overuse of global variables can make code harder to understand and maintain."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', # Complete the list",
        "complete": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', 'duration'))"
      },
      {
        "partial": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(# Add the column names here))",
        "complete": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', 'duration'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/hazard.ratio.R",
    "language": "R",
    "content": "`hazard.ratio` <-\nfunction(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n\tmethod.test <- match.arg(method.test)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n\t} else { weights <- rep(1,  length(x)) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n\t\t## remove weights=0 because the coxph function does not deal with them properly\n\t\tiix <- weights <= 0\n\t\tif(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n\t\tweights[iix] <- NA\n\t} else { strat <- rep(1,  length(x)) }\n\tcc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n\tif(sum(cc.ix) < 3) {\n\t## not enough observations\n\t\tdata <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n\t\treturn(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n\t}\n\tif(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n\tsx <- x[cc.ix]\n\too <- order(sx, decreasing=FALSE)\n\tsx <- sx[oo]\n\tstime <- surv.time[cc.ix][oo]\n\tsevent <- surv.event[cc.ix][oo]\n\tsweights <- weights[cc.ix][oo]\n\tsstrat <- strat[cc.ix][oo]\n\tdata <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n\t#fit the cox model\n\toptions(warn=2)\n\trr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n\toptions(warn=0)\n\tif(class(rr) == \"try-error\") {\n\t\tres <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n\t} else {\n\t\thrcoef <- rr$coefficients\n\t\thrse <- sqrt(drop(rr$var))\n\t\tnames(hrcoef) <- names(hrse) <- NULL\n\t\tmystat <- NA\n\t\tswitch(method.test,\n\t\t\"logrank\"={\n\t\t\tmystat <- rr$score\n\t\t},\n\t\t\"likelihood.ratio\"={\n\t\t\tmystat <- 2 * (rr$loglik[2] - rr$loglik[1])\n\t\t},\n\t\t\"wald\"={\n\t\t\tmystat <- rr$wald.test\n\t\t\t##(hrcoef / hrse)^2\n\t\t})\n\t\tmypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n\t\tres <- list(\"hazard.ratio\"=exp(hrcoef), \"coef\"=hrcoef, \"se\"=hrse, \"lower\"=exp(hrcoef - qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"upper\"=exp(hrcoef + qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n\t}\n\n\treturn(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hazard.ratio` function and what are its main input parameters?",
        "answer": "The `hazard.ratio` function is used to calculate the hazard ratio in survival analysis. Its main input parameters are:\n- `x`: The predictor variable\n- `surv.time`: Survival time\n- `surv.event`: Event indicator\n- `weights`: Optional weights for each observation\n- `strat`: Optional stratification variable\n- `alpha`: Significance level for confidence interval calculation (default 0.05)\n- `method.test`: Method for calculating the test statistic (options: 'logrank', 'likelihood.ratio', 'wald')\n- `na.rm`: Boolean indicating whether to remove NA values"
      },
      {
        "question": "How does the function handle missing data and what happens if there are fewer than 3 complete cases?",
        "answer": "The function handles missing data as follows:\n1. It checks for complete cases using `complete.cases()` function.\n2. If `na.rm=FALSE` (default) and there are any incomplete cases, it stops with an error.\n3. If there are fewer than 3 complete cases, it returns a list with NA values for all results, including the hazard ratio, coefficient, standard error, confidence interval bounds, and p-value.\n4. The function also includes a warning if any weights are less than or equal to zero, as these observations are discarded."
      },
      {
        "question": "What statistical methods are used in this function and how is the final result calculated?",
        "answer": "The function uses Cox proportional hazards regression for survival analysis. Key statistical methods include:\n1. Fitting a Cox model using `coxph()` function.\n2. Calculating the test statistic based on the chosen method (logrank, likelihood ratio, or Wald test).\n3. Computing the p-value using the chi-square distribution.\n4. Calculating the hazard ratio as exp(coefficient).\n5. Determining confidence intervals using the normal distribution.\n\nThe final result is a list containing:\n- Hazard ratio\n- Coefficient and standard error\n- Lower and upper confidence interval bounds\n- P-value\n- Number of observations\n- The fitted Cox model\n- Input data"
      }
    ],
    "completion_tasks": [
      {
        "partial": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  # Complete the function here\n}",
        "complete": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx, decreasing=FALSE)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n  options(warn=0)\n  if(class(rr) == \"try-error\") {\n    res <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n  } else {\n    hrcoef <- rr$coefficients\n    hrse <- sqrt(drop(rr$var))\n    names(hrcoef) <- names(hrse) <- NULL\n    mystat <- switch(method.test,\n      \"logrank\" = rr$score,\n      \"likelihood.ratio\" = 2 * (rr$loglik[2] - rr$loglik[1]),\n      \"wald\" = rr$wald.test\n    )\n    mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n    res <- list(\"hazard.ratio\"=exp(hrcoef), \"coef\"=hrcoef, \"se\"=hrse, \"lower\"=exp(hrcoef - qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"upper\"=exp(hrcoef + qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n  }\n  return(res)\n}"
      },
      {
        "partial": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx, decreasing=FALSE)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n  options(warn=0)\n  if(class(rr) == \"try-error\") {\n    res <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n  } else {\n    # Complete the function here\n  }\n  return(res)\n}",
        "complete": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx, decreasing=FALSE)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n  options(warn=0)\n  if(class(rr) == \"try-error\") {\n    res <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n  } else {\n    hrcoef <- rr$coefficients\n    hrse <- sqrt(drop(rr$var))\n    names(hrcoef) <- names(hrse) <- NULL\n    mystat <- switch(method.test,\n      \"logrank\" = rr$score,\n      \"likelihood.ratio\" = 2 * (rr$loglik[2] - rr$loglik[1]),\n      \"wald\" = rr$wald.test\n    )\n    mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n    res <- list(\"hazard.ratio\"=exp(hrcoef), \"coef\"=hrcoef, \"se\"=hrse, \"lower\"=exp(hrcoef - qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"upper\"=exp(hrcoef + qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n  }\n  return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/subtype.cluster.R",
    "language": "R",
    "content": "#' @title Function to fit the Subtype Clustering Model\n#'\n#' @description\n#' This function fits the Subtype Clustering Model as published in Desmedt\n#'   et al. 2008 and Wiarapati et al. 2008. This model is actually a mixture\n#'   of three Gaussians with equal shape, volume and variance (see EEI model\n#'   in Mclust). This model is adapted to breast cancer and uses ESR1, ERBB2\n#'   and AURKA dimensions to identify the molecular subtypes, i.e. ER-/HER2-,\n#'   HER2+ and ER+/HER2- (Low and High Prolif).\n#'\n#' @usage\n#' subtype.cluster(module.ESR1, module.ERBB2, module.AURKA, data, annot,\n#'   do.mapping = FALSE, mapping, do.scale = TRUE, rescale.q = 0.05,\n#'   model.name = \"EEI\", do.BIC = FALSE, plot = FALSE, filen, verbose = FALSE)\n#'\n#' @param module.ESR1\tMatrix containing the ESR1-related gene(s) in\n#'   rows and at least three columns: \"probe\", \"EntrezGene.ID\" and\n#'   \"coefficient\" standing for the name of the probe, the NCBI Entrez\n#'   Gene id and the coefficient giving the direction and the strength of\n#'   the association of each gene in the gene list.\n#' @param module.ERBB2\tIdem for ERBB2.\n#' @param module.AURKA\tIdem for AURKA.\n#' @param data\tMatrix of gene expressions with samples in rows and probes\n#'   in columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping\tTRUE if the mapping through Entrez Gene ids must\n#'   be performed (in case of ambiguities, the most variant probe is kept\n#'   for each gene), FALSE otherwise.\n#' @param mapping\t**DEPRECATED** Matrix with columns \"EntrezGene.ID\" and\n#'   \"probe\" used to force the mapping such that the probes are not selected\n#'   based on their variance.\n#' @param do.scale TRUE if the ESR1, ERBB2 and AURKA (module) scores must be\n#'   rescaled (see rescale), FALSE otherwise.\n#' @param rescale.q\tProportion of expected outliers for rescaling the gene expressions.\n#' @param do.BIC\tTRUE if the Bayesian Information Criterion must be computed\n#'   for number of clusters ranging from 1 to 10, FALSE otherwise.\n#' @param model.name Name of the model used to fit the mixture of Gaussians\n#'   with the Mclust from the mclust package; default is \"EEI\" for fitting a\n#'   mixture of Gaussians with diagonal variance, equal volume, equal shape\n#'   and identical orientation.\n#' @param plot TRUE if the patients and their corresponding subtypes must\n#'   be plotted, FALSE otherwise.\n#' @param filen Name of the csv file where the subtype clustering model must\n#' be stored.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - model: Subtype Clustering Model (mixture of three Gaussians),\n#'   like scmgene.robust, scmod1.robust and scmod2.robust when this function\n#'   is used on expO dataset (International Genomics Consortium) with the gene\n#'   modules published in the two references cited below.\n#' - BIC: Bayesian Information Criterion for the Subtype Clustering Model\n#'   with number of clusters ranging from 1 to 10.\n#' - subtype: Subtypes identified by the Subtype Clustering Model. Subtypes\n#'   can be either \"ER-/HER2-\", \"HER2+\" or \"ER+/HER2-\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated by\n#'   the Subtype Clustering Model.\n#' - subtype2: Subtypes identified by the Subtype Clustering Model using\n#'   AURKA to discriminate low and high proliferative tumors. Subtypes can\n#'   be either \"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\" or \"ER+/HER2- Low Prolif\".\n#' - subtype.proba2: Probabilities to belong to each subtype (including\n#'   discrimination between lowly and highly proliferative ER+/HER2- tumors,\n#'   see subtype2) estimated by the Subtype Clustering Model.\n#' - module.scores: Matrix containing ESR1, ERBB2 and AURKA module scores.\n#'\n#' @references\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G,\n#'   Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes\n#'   associated with breast cancer clinical outcome depend on the molecular\n#'   subtypes\", Clinical Cancer Research, 14(16):5158-5165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B,\n#'   Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ\n#'   and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in\n#'   Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing\n#'   and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#'\n#' @seealso\n#' [genefu::subtype.cluster.predict], [genefu::intrinsic.cluster],\n#' [genefu::intrinsic.cluster.predict], [genefu::scmod1.robust],\n#' [genefu::scmod2.robust]\n#'\n#' @examples\n#' # example without gene mapping\n#' # load expO data\n#' data(expos)\n#' # load gene modules\n#' data(mod1)\n#' # fit a Subtype Clustering Model\n#' scmod1.expos <- subtype.cluster(module.ESR1=mod1$ESR1, module.ERBB2=mod1$ERBB2,\n#'   module.AURKA=mod1$AURKA, data=data.expos, annot=annot.expos, do.mapping=FALSE,\n#'   do.scale=TRUE, plot=TRUE, verbose=TRUE)\n#' str(scmod1.expos, max.level=1)\n#' table(scmod1.expos$subtype2)\n#'\n#' # example with gene mapping\n#' # load NKI data\n#' data(nkis)\n#' # load gene modules\n#' data(mod1)\n#' # fit a Subtype Clustering Model\n#' scmod1.nkis <- subtype.cluster(module.ESR1=mod1$ESR1, module.ERBB2=mod1$ERBB2,\n#'   module.AURKA=mod1$AURKA, data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   do.scale=TRUE, plot=TRUE, verbose=TRUE)\n#' str(scmod1.nkis, max.level=1)\n#' table(scmod1.nkis$subtype2)\n#'\n#' @md\n#' @import mclust\n#' @import graphics\n#' @export\nsubtype.cluster <- function(module.ESR1, module.ERBB2, module.AURKA, data,\n\tannot, do.mapping=FALSE, mapping, do.scale=TRUE, rescale.q=0.05,\n\tmodel.name=\"EEI\", do.BIC=FALSE, plot=FALSE, filen, verbose=FALSE)\n{\n\tif(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n\tsbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n\tsbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\tsig.esr1 <- sig.score(x=module.ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsig.erbb2 <- sig.score(x=module.ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsig.aurka <- sig.score(x=module.AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tdd <- cbind(\"ESR1\"=sig.esr1$score, \"ERBB2\"=sig.erbb2$score, \"AURKA\"=sig.aurka$score)\n\trnn <- rownames(dd)\n\tm.mod <- list(\"ESR1\"=cbind(\"probe\"=as.character(sig.esr1$probe[ ,\"new.probe\"]), \"EntrezGene.ID\"=as.character(sig.esr1$probe[ ,\"EntrezGene.ID\"]), \"coefficient\"=module.ESR1[match(sig.esr1$probe[ ,\"probe\"], module.ESR1[ ,\"probe\"]), \"coefficient\"]), \"ERBB2\"=cbind(\"probe\"=as.character(sig.erbb2$probe[ ,\"new.probe\"]), \"EntrezGene.ID\"=as.character(sig.erbb2$probe[ ,\"EntrezGene.ID\"]), \"coefficient\"=module.ERBB2[match(sig.erbb2$probe[ ,\"probe\"], module.ERBB2[ ,\"probe\"]), \"coefficient\"]), \"AURKA\"=cbind(\"probe\"=as.character(sig.aurka$probe[ ,\"new.probe\"]), \"EntrezGene.ID\"=as.character(sig.aurka$probe[ ,\"EntrezGene.ID\"]), \"coefficient\"=module.AURKA[match(sig.aurka$probe[ ,\"probe\"], module.AURKA[ ,\"probe\"]), \"coefficient\"]))\n\tif(do.scale) {\n\t\t## the rescaling needs a large sample size!!!\n\t\t## necessary if we want to validate the classifier using a different dataset\n\t\t## the estimation of survival probabilities depends on the scale of the score\n\t\tdd <- apply(dd, 2, function(x) { return((rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2) })\n\t\trownames(dd) <- rnn\n\t} else { rescale.q <- NA }\n\trownames(dd) <- rownames(data)\n\tdd2 <- dd\n\n\tcc.ix <- complete.cases(dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE])\n\tdd <- dd[cc.ix, , drop=FALSE]\n\tif(all(!cc.ix)) { stop(\"None of ESR1 and ERBB2 genes are present!\") }\n\n\tif(do.BIC) {\n\t\t## save the BIC values for the all the methods and a number of clusters from 1 to 10\n\t\tcluster.bic <- mclust::Mclust(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=model.name, G=1:10)$BIC\n\t} else { cluster.bic <- NA }\n\n\t#identify the 3 subtypes\n\trr3 <- mclust::Mclust(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=model.name, G=3)\n\t#redefine classification to be coherent with subtypes\n\tuclass <- sort(unique(rr3$classification))\n\tuclass <- uclass[!is.na(uclass)]\n\tif(length(uclass) != 3) { stop(\"less than 3 subtypes are identified!\") }\n\tmm <- NULL\n\tfor(i in 1:length(uclass)) {\n\t\tmm <- c(mm, median(dd[rr3$classification == uclass[i],\"ERBB2\"], na.rm=TRUE) )\n\t}\n\tnclass <-  uclass[order(mm, decreasing=TRUE)[1]]\n\tmm <- NULL\n\tfor(i in 1:length(uclass[-nclass])) {\n\t\tmm <- c(mm, median(dd[rr3$classification == uclass[-nclass][i],\"ESR1\"], na.rm=TRUE))\n\t}\n\tnclass <- c(uclass[-nclass][order(mm, decreasing=TRUE)[2]], nclass, uclass[-nclass][order(mm, decreasing=TRUE)[1]])\n\t#nclass contains the new order\n\trr3$z <- rr3$z[ ,nclass, drop=FALSE]\n\tncl <- rr3$classification\n\tfor(i in 1:length(uclass)) {\n\t\tncl[rr3$classification == nclass[i]] <- i\n\t}\n\trr3$classification <- ncl\n\trr3$parameters$pro <- rr3$parameters$pro[nclass]\n\trr3$parameters$mean <- rr3$parameters$mean[ , nclass, drop=FALSE]\n\trr3$parameters$variance$sigma <- rr3$parameters$variance$sigma[ , , nclass, drop=FALSE]\n\n\tif(plot) {\n\t\tif(do.scale) {\n\t\t\tmyxlim <- myylim <- c(-2, 2)\n\t\t} else {\n\t\t\tmyxlim <- range(dd[ , \"ESR1\"])\n\t\t\tmyylim <- range(dd[ , \"ERBB2\"])\n\t\t}\n\t\t## plot the mixture of Gaussians of the model\n\t\txx <- mclust:::grid1(50, range=myxlim)\n\t\tyy <- mclust:::grid1(50, range=myylim)\n\t\txxyy <- mclust:::grid2(xx,yy)\n\t\t#density\n\t\txyDens <- mclust::dens(modelName = rr3$modelName, data = xxyy, parameters = rr3$parameters)\n\t\txyDens <- matrix(xyDens, nrow = length(xx), ncol = length(yy))\n\t\tpar(pty = \"s\")\n\t\tzz <- xyDens\n\t\t#plot\n\t\tpersp(x = xx, y = yy, z = zz, xlim=myxlim, ylim=myylim, theta=-25, phi=30, expand=0.5, xlab=\"ESR1\", ylab=\"ERBB2\", zlab=\"Density\", col=\"darkgrey\", ticktype=\"detailed\")\n\t}\n\n\t## use the previously computed model to fit a new model in a supervised manner\n\tmyclass <- mclust::unmap(rr3$classification)\n\tdimnames(myclass)[[1]] <- dimnames(dd)[[1]]\n\tmclust.tr <- mclust::mstep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], z=myclass)\n\tdimnames(mclust.tr$z) <- dimnames(myclass)\n\temclust.tr <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=mclust.tr$parameters)\n\tdimnames(emclust.tr$z) <- dimnames(myclass)\n\tclass.tr <- mclust::map(emclust.tr$z, warn=FALSE)\n\tnames(class.tr) <- dimnames(dd)[[1]]\n\tdimnames(mclust.tr$parameters$mean)[[2]] <- names(mclust.tr$parameters$pro) <- dimnames(mclust.tr$z)[[2]]\n\n\t## subtypes\n\tsbt <- rep(NA, nrow(data))\n\tnames(sbt) <- dimnames(data)[[1]]\n\tsbt[names(class.tr)] <- sbtn[class.tr]\n\tsbt.proba <- matrix(NA, nrow(data), ncol=ncol(emclust.tr$z), dimnames=list(dimnames(data)[[1]], sbtn))\n\tsbt.proba[dimnames(emclust.tr$z)[[1]], ] <- emclust.tr$z\n\t## discriminate between luminal A and B using AURKA\n\t## since proliferation is a continuum we fit a Gaussian using AURKA expression of the ER+/HER2- tumors\n\ttt <- mclust::Mclust(dd2[complete.cases(sbt, dd2[ , \"AURKA\"]) & sbt == sbtn[3], \"AURKA\"], modelNames=\"E\", G=1)\n\tgauss.prolif <- c(\"mean\"=tt$parameters$mean, \"sigma\"=tt$parameters$variance$sigmasq)\n\tsbt2 <- sbt\n\tsbt2[sbt == sbtn[3]] <- NA\n\t## probability that tumor is highly proliferative\n\tpprolif <- pnorm(q=dd2[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t## high proliferation\n\tsbt2[sbt == sbtn[3] & pprolif >= 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[3]\n\t## low proliferation\n\tsbt2[sbt == sbtn[3] & pprolif < 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[4]\n\t## subtype probabilities for luminal B and A\n\tsbt.proba2 <- matrix(NA, nrow(data), ncol=ncol(emclust.tr$z) + 1, dimnames=list(dimnames(data)[[1]], sbtn2))\n\ttt <- sbt.proba[ , sbtn[3]]\n\ttt2 <- pprolif\n\ttt <- cbind(tt * tt2, tt * (1 - tt2))\n\tcolnames(tt) <- sbtn2[3:4]\n\tsbt.proba2[ , sbtn2[1:2]] <- sbt.proba[ , sbtn[1:2]]\n\tsbt.proba2[ , sbtn2[3:4]] <- tt[ , sbtn2[3:4]]\n\n\tif(plot) {\n\t\t## plot the clusters\n\t\tmclust::mclust2Dplot(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], what=\"classification\", classification=class.tr, parameters=mclust.tr$parameters, colors=c(\"darkred\", \"darkgreen\", \"darkblue\"), xlim=myxlim, ylim=myylim)\n\t\tlegend(x=\"topleft\", col=c(\"darkred\", \"darkgreen\", \"darkblue\"), legend=sbtn, pch=mclust::mclust.options(\"classPlotSymbols\")[1:length(uclass)], bty=\"n\")\n\t\t## plot the clusters with luminals A and B\n\t\tmycol <- mypch <- sbt2\n\t\tmycol[sbt2 == sbtn2[1]] <- \"darkred\"\n\t\tmycol[sbt2 == sbtn2[2]] <- \"darkgreen\"\n\t\tmycol[sbt2 == sbtn2[3]] <- \"darkorange\"\n\t\tmycol[sbt2 == sbtn2[4]] <- \"darkviolet\"\n\t\tmypch[sbt2 == sbtn2[1]] <- 17\n\t\tmypch[sbt2 == sbtn2[2]] <- 0\n\t\tmypch[sbt2 == sbtn2[3] | sbt2 == sbtn2[4]] <- 10\n\t\tmypch <- as.numeric(mypch)\n\t\tnames(mycol) <- names(mypch) <- names(sbt2)\n\t\tplot(x=dd[ , \"ESR1\"], y=dd[ , \"ERBB2\"], xlim=myxlim, ylim=myylim, xlab=\"ESR1\", ylab=\"ERBB2\", col=mycol[dimnames(dd)[[1]]], pch=mypch[dimnames(dd)[[1]]])\n\t\tlegend(x=\"topleft\", col=c(\"darkred\", \"darkgreen\", \"darkorange\", \"darkviolet\"), legend=sbtn2, pch=c(17, 0, 10, 10), bty=\"n\")\n\t\t## display the three circles representing the Gaussians\n\t\tfor(kk in 1:3) { mclust::mvn2plot(mu=mclust.tr$parameters$mean[ ,kk], sigma=mclust.tr$parameters$variance$sigma[ , , kk]) }\n\t}\n\n\tif(!missing(filen)) {\n\t\t#save model parameters in a csv file for reuse\n\t\twrite(x=sprintf(\"# Benjamin Haibe-Kains. All rights reserved.\"), file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# model.name: %s\", model.name), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmymean <- t(mclust.tr$parameters$mean)\n\t\tif(is.null(dimnames(mymean)[[1]])) { dimnames(mymean)[[1]] <- 1:nrow(mymean) }\n\t\tfor(i in 1:nrow(mymean)) { write(x=sprintf(\"# mean.%s: %g %g\", dimnames(mymean)[[1]][i], mymean[i,1], mymean[i,2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\")) }\n\t\tmysigma <- diag(mysigma <- mclust.tr$parameters$variance$sigma[ , ,1])\n\t\twrite(x=sprintf(\"# sigma: %g %g\", mysigma[1], mysigma[2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmypro <- mclust.tr$parameters$pro\n\t\twrite(x=sprintf(\"# pro: %g %g %g\", mypro[1], mypro[2], mypro[3]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmyscale <- mclust.tr$parameters$variance$scale\n\t\twrite(x=sprintf(\"# scale: %g\", myscale), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmyshape <- mclust.tr$parameters$variance$shape\n\t\twrite(x=sprintf(\"# shape: %g %g\", myshape[1], myshape[2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# gaussian.AURKA.mean: %g\", gauss.prolif[1]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# gaussian.AURKA.sigma: %g\", gauss.prolif[2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# rescale.q: %g\", rescale.q), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(paste(\"\\\"\", c(\"module\", dimnames(m.mod[[1]])[[2]]), \"\\\"\", collapse=\",\", sep=\"\"), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite.m.file(m.mod, file=paste(filen, \"csv\", sep=\".\"), col.names=FALSE, append=TRUE)\n\t}\n\n\treturn(list(\"model\"=c(mclust.tr[\"parameters\"], list(\"gaussian.AURKA\"=gauss.prolif), list(\"rescale.q\"=rescale.q), list(\"mod\"=m.mod)), \"BIC\"=cluster.bic, \"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"subtype2\"=sbt2, \"subtype.proba2\"=sbt.proba2,  \"module.scores\"=dd2))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `subtype.cluster` function?",
        "answer": "The main purpose of the `subtype.cluster` function is to fit a Subtype Clustering Model for breast cancer molecular subtypes. It uses a mixture of three Gaussians with equal shape, volume, and variance (EEI model in Mclust) to identify molecular subtypes based on ESR1, ERBB2, and AURKA gene expression dimensions."
      },
      {
        "question": "How does the function handle the discrimination between luminal A and B subtypes?",
        "answer": "The function discriminates between luminal A and B subtypes using the AURKA expression of ER+/HER2- tumors. It fits a Gaussian distribution to the AURKA expression and calculates the probability of a tumor being highly proliferative. Tumors with a probability >= 0.5 are classified as 'ER+/HER2- High Prolif' (luminal B), while those < 0.5 are classified as 'ER+/HER2- Low Prolif' (luminal A)."
      },
      {
        "question": "What are the key components returned by the `subtype.cluster` function?",
        "answer": "The function returns a list containing: 1) the fitted model parameters, 2) Bayesian Information Criterion (BIC) values if requested, 3) subtype classifications, 4) subtype probabilities, 5) refined subtype classifications including luminal A/B distinction, 6) refined subtype probabilities, and 7) module scores for ESR1, ERBB2, and AURKA."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcYoshihara.R",
    "language": "R",
    "content": "#' @title Function to compute the subtype scores and risk classifications for\n#'   the prognostic signature published by Yoshihara et al.\n#'\n#' @description\n#' This function computes subtype scores and risk classifications from gene\n#'   expression values following the algorithm developed by Yoshihara et al,\n#'   for prognosis in ovarian cancer.\n#'\n#' @usage\n#' ovcYoshihara(data, annot, hgs,\n#'   gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"),\n#'   do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param hgs vector of booleans with TRUE represents the ovarian cancer\n#'   patients who have a high grade, late stage, serous tumor, FALSE otherwise.\n#'   This is particularly important for properly rescaling the data. If hgs is\n#'   missing, all the patients will be used to rescale the subtype score.\n#' @param gmap character string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Yoshihara K, Tajima A, Yahata T, Kodama S, Fujiwara H, Suzuki M, Onishi Y,\n#'   Hatae M, Sueyoshi K, Fujiwara H, Kudo, Yoshiki, Kotera K, Masuzaki H,\n#'   Tashiro H, Katabuchi H, Inoue I, Tanaka K (2010) \"Gene expression profile\n#'   for predicting survival in advanced-stage serous ovarian cancer across two\n#'   independent datasets\", PloS one, 5(3):e9615.\n#'\n#' @seealso\n#' [genefu::sigOvcYoshihara]\n#'\n#' @examples\n#' # load the ovcYoshihara signature\n#' data(sigOvcYoshihara)\n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \"entrezgene\"\n#' # compute relapse score\n#' ovcYoshihara.nkis <- ovcYoshihara(data=data.nkis,\n#'   annot=annot.nkis, gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcYoshihara.nkis$risk)\n#'\n#' @md\n#' @export\novcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\",\n    \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"),\n    do.mapping=FALSE, verbose=FALSE)\n{\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    ## transform the gene expression in Z-scores\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `ovcYoshihara` function, and what are its main inputs and outputs?",
        "answer": "The `ovcYoshihara` function computes subtype scores and risk classifications for ovarian cancer prognosis based on gene expression data. Its main inputs are: 'data' (gene expression matrix), 'annot' (gene annotation matrix), and 'hgs' (vector indicating high-grade serous tumors). The function returns a list containing: 'score' (continuous signature scores), 'risk' (binary risk classification), 'mapping' (gene mapping information), and 'probe' (probe-to-gene correspondence if mapping is performed)."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs gene mapping using the specified `gmap` attribute. It selects the most variant probe for each gene in case of ambiguities. The mapping process involves: 1) Ordering the signature genes by absolute weight, 2) Removing duplicates, 3) Mapping gene IDs between the signature and input data, 4) Updating the data, annotation, and signature matrices accordingly. The function also creates a 'myprobe' data frame to store the mapping information and updates the 'mymapping' vector with the number of mapped and total genes."
      },
      {
        "question": "How are the final subtype scores and risk classifications calculated in the `ovcYoshihara` function?",
        "answer": "The final calculations in the `ovcYoshihara` function involve these steps: 1) The gene expression data is transformed into Z-scores using the `scale` function. 2) The subtype scores are computed using the `genefu::sig.score` function, which combines the scaled expression data with the signature weights. 3) Risk classifications are determined by comparing each sample's score to the median score across all samples. Scores above the median are classified as high risk (1), while those below are low risk (0). The function returns both the continuous scores and binary risk classifications for each sample."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        # Complete the else block\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        # Complete the do.mapping block\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/geneid.map.R",
    "language": "R",
    "content": "#' @title Function to find the common genes between two datasets or a dataset and\n#'   a gene list\n#'\n#' @description\n#' This function allows for fast mapping between two datasets or a dataset and a gene\n#'   list. The mapping process is performed using Entrez Gene id as reference. In case of\n#'   ambiguities (several probes representing the same gene), the most variant probe is\n#'   selected.\n#'\n#' @usage\n#' geneid.map(geneid1, data1, geneid2, data2, verbose = FALSE)\n#'\n#' @param geneid1 First vector of Entrez Gene ids. The name of the vector cells must\n#'   be the name of the probes in the dataset data1.\n#' @param data1\tFirst dataset with samples in rows and probes in columns. The dimnames\n#'   must be properly defined.\n#' @param geneid2 Second vector of Entrez Gene ids. The name of the vector cells must\n#'   be the name of the probes in the dataset data1 if it is not missing, proper names must be assigned otherwise.\n#' @param data2\tFirst dataset with samples in rows and probes in columns. The dimnames\n#'   must be properly defined. It may be missing.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#'\n#' @return\n#' A list with items:\n#' - geneid1 Mapped gene list from geneid1.\n#' - data1 Mapped dataset from data1.\n#' - geneid2 Mapped gene list from geneid2.\n#' - data2 Mapped dataset from data2.\n#'\n#' @note\n#' It is mandatory that the names of geneid1 and geneid2 must be the probe names\n#'   of the microarray platform.\n#'\n#' @examples\n#' # load NKI data\n#' data(nkis)\n#' nkis.gid <- annot.nkis[ ,\"EntrezGene.ID\"]\n#' names(nkis.gid) <- dimnames(annot.nkis)[[1]]\n#' # load GGI signature\n#' data(sig.ggi)\n#' ggi.gid <- sig.ggi[ ,\"EntrezGene.ID\"]\n#' names(ggi.gid) <- as.character(sig.ggi[ ,\"probe\"])\n#' # mapping through Entrez Gene ids of NKI and GGI signature\n#' res <- geneid.map(geneid1=nkis.gid, data1=data.nkis,\n#'   geneid2=ggi.gid, verbose=FALSE)\n#' str(res)\n#'\n#' @md\n#' @export\ngeneid.map <-\nfunction(geneid1, data1, geneid2, data2, verbose=FALSE) {\n\n\tnn <- names(geneid1)\n\tgeneid1 <- as.character(geneid1)\n\tnames(geneid1) <- nn\n\tnn <- names(geneid2)\n\tgeneid2 <- as.character(geneid2)\n\tnames(geneid2) <- nn\n\tif(is.null(names(geneid1))) { names(geneid1) <- dimnames(data1)[[2]] }\n\tif(!missing(data2) && is.null(names(geneid2))) { names(geneid2) <- dimnames(data2)[[2]] }\n\tif(!missing(data1) && !missing(geneid1) && !missing(geneid2)) {\n\t\t## remove probes without any measurements\n\t\tna.ix <- apply(data1, 2, function(x) { return(all(is.na(x))) })\n\t\tdata1 <- data1[ , !na.ix, drop=FALSE]\n\t\tgeneid1 <- geneid1[!na.ix]\n\t} else { stop(\"data1, geneid1 and geneid2 parameters are mandatory!\") }\n\tif(!missing(data2)) {\n\t\t## remove probes without any measurements\n\t\tna.ix <- apply(data2, 2, function(x) { return(all(is.na(x))) })\n\t\tdata2 <- data2[ , !na.ix, drop=FALSE]\n\t\tgeneid2 <- geneid2[!na.ix]\n\t} else { data2 <- NULL }\n\n\tgix1 <- !is.na(geneid1)\n\tgix2 <- !is.na(geneid2)\n\n\tgeneid.common <- intersect(geneid1[gix1], geneid2[gix2])\n\tif(length(geneid.common) == 0) {\n\t\twarning(\"no gene ids in common!\")\n\t\treturn(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n\t}\n\n\t## dataset1\n\t## probes corresponding to common gene ids\n\tgg <- names(geneid1)[is.element(geneid1, geneid.common)]\n\tgid <- geneid1[is.element(geneid1, geneid.common)]\n\t## duplicated gene ids\n\tgid.dupl <- unique(gid[duplicated(gid)])\n\tgg.dupl <- names(geneid1)[is.element(geneid1, gid.dupl)]\n\t## unique gene ids\n\tgid.uniq <- gid[!is.element(gid, gid.dupl)]\n\tgg.uniq <- names(geneid1)[is.element(geneid1, gid.uniq)]\n\t## data corresponding to unique gene ids\n\tdatat <- data1[ ,gg.uniq,drop=FALSE]\n\t## data for duplicated gene ids\n\tif(length(gid.dupl) > 0) {\n\t\tif(verbose) { message(\"\\ndataset1 duplicates...\") }\n\t\t## compute the standard deviation with a penalization on the number of missing values\n\t\t## this should avoid selecting the most variant probe with a lot of missing values\n\t\tpena <- apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n\t\tpena <- log((nrow(data1) + 1) / (pena + 1)) + 1\n\t\t#pena <- 1\n\t\tsdr <- drop(apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n\t\tmysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid1[gg.dupl], \"sd\"=sdr)\n\t\tmysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n\t\tmysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n\t\tdatat <- cbind(datat, data1[ , mysd[ , \"probe\"], drop=FALSE])\n\t}\n\tdata1 <- datat\n\tgeneid1 <- geneid1[dimnames(data1)[[2]]]\n\n\t#dataset2\n\tif(is.null(data2)) {\n\t\t#keep arbitrarily the first occurence of each duplicated geneid\n\t\tgeneid2 <- geneid2[!duplicated(geneid2) & is.element(geneid2, geneid.common)]\n\t}\n\telse {\n\t\t## probes corresponding to common gene ids\n\t\tgg <- names(geneid2)[is.element(geneid2, geneid.common)]\n\t\tgid <- geneid2[is.element(geneid2, geneid.common)]\n\t\t## duplicated gene ids\n\t\tgid.dupl <- unique(gid[duplicated(gid)])\n\t\tgg.dupl <- names(geneid2)[is.element(geneid2, gid.dupl)]\n\t\t## unique gene ids\n\t\tgid.uniq <- gid[!is.element(gid, gid.dupl)]\n\t\tgg.uniq <- names(geneid2)[is.element(geneid2, gid.uniq)]\n\t\t## data corresponding to unique gene ids\n\t\tdatat <- data2[ ,gg.uniq,drop=FALSE]\n\t\t## data for duplicated gene ids\n\t\tif(length(gid.dupl) > 0) {\n\t\t\tif(verbose) { message(\"\\ndataset2 duplicates...\") }\n\t\t\t## compute the standard deviation with a penalization on the number of missing values\n\t\t\t## this should avoid selecting the most variant probe with a lotof missing values\n\t\t\tpena <- apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n\t\t\tpena <- log((nrow(data2) + 1) / (pena + 1)) + 1\n\t\t\t#pena <- 1\n\t\t\tsdr <- drop(apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n\t\t\tmysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid2[gg.dupl], \"sd\"=sdr)\n\t\t\tmysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n\t\t\tmysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n\t\t\tdatat <- cbind(datat, data2[ , mysd[ , \"probe\"], drop=FALSE])\n\t\t}\n\t\tdata2 <- datat\n\t\tgeneid2 <- geneid2[dimnames(data2)[[2]]]\n\t}\n\n\t#same order for the two datasets\n\trix <- match(geneid2, geneid1)\n\tgeneid1 <- geneid1[rix]\n\tdata1 <- data1[ ,rix,drop=FALSE]\n\treturn(list(\"geneid1\"=geneid1, \"data1\"=data1, \"geneid2\"=geneid2, \"data2\"=data2))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `geneid.map` function, and how does it handle ambiguities when mapping genes?",
        "answer": "The primary purpose of the `geneid.map` function is to find common genes between two datasets or a dataset and a gene list, using Entrez Gene IDs as a reference. It handles ambiguities (multiple probes representing the same gene) by selecting the most variant probe. This is done by computing the standard deviation of each probe's measurements, with a penalization for missing values, and selecting the probe with the highest penalized standard deviation for each duplicated gene ID."
      },
      {
        "question": "How does the function handle missing data in the input datasets, and what is the significance of the `pena` variable in the code?",
        "answer": "The function handles missing data in several ways: 1) It removes probes with all NA measurements from both datasets. 2) When selecting the most variant probe for duplicated gene IDs, it uses a penalization factor (`pena`) to avoid selecting probes with many missing values. The `pena` variable is calculated as `log((nrow(data) + 1) / (number_of_NAs + 1)) + 1`. This penalization is then multiplied by the standard deviation of the probe's measurements, effectively reducing the 'variance score' of probes with more missing values, making them less likely to be selected as the representative probe for a gene."
      },
      {
        "question": "What are the key steps in the `geneid.map` function for ensuring that the output datasets have matching gene IDs?",
        "answer": "The key steps for ensuring matching gene IDs in the output are: 1) Finding the intersection of gene IDs between the two input datasets. 2) For each dataset, separating unique and duplicated gene IDs. 3) For duplicated gene IDs, selecting the most variant probe. 4) Combining the data for unique and selected duplicated gene IDs. 5) If `data2` is provided, ensuring both datasets have the same order of gene IDs by using `match(geneid2, geneid1)` to reorder `geneid1` and `data1`. This process ensures that the final output contains only common genes between the two datasets, with a single representative probe for each gene, and that the gene order matches between the two datasets."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  nn <- names(geneid1)\n  geneid1 <- as.character(geneid1)\n  names(geneid1) <- nn\n  nn <- names(geneid2)\n  geneid2 <- as.character(geneid2)\n  names(geneid2) <- nn\n  if(is.null(names(geneid1))) { names(geneid1) <- dimnames(data1)[[2]] }\n  if(!missing(data2) && is.null(names(geneid2))) { names(geneid2) <- dimnames(data2)[[2]] }\n  if(!missing(data1) && !missing(geneid1) && !missing(geneid2)) {\n    na.ix <- apply(data1, 2, function(x) { return(all(is.na(x))) })\n    data1 <- data1[ , !na.ix, drop=FALSE]\n    geneid1 <- geneid1[!na.ix]\n  } else { stop(\"data1, geneid1 and geneid2 parameters are mandatory!\") }\n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) { return(all(is.na(x))) })\n    data2 <- data2[ , !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else { data2 <- NULL }\n\n  gix1 <- !is.na(geneid1)\n  gix2 <- !is.na(geneid2)\n\n  geneid.common <- intersect(geneid1[gix1], geneid2[gix2])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n\n  # Complete the function to handle dataset1 and dataset2\n  # ...\n\n}",
        "complete": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  nn <- names(geneid1)\n  geneid1 <- as.character(geneid1)\n  names(geneid1) <- nn\n  nn <- names(geneid2)\n  geneid2 <- as.character(geneid2)\n  names(geneid2) <- nn\n  if(is.null(names(geneid1))) { names(geneid1) <- dimnames(data1)[[2]] }\n  if(!missing(data2) && is.null(names(geneid2))) { names(geneid2) <- dimnames(data2)[[2]] }\n  if(!missing(data1) && !missing(geneid1) && !missing(geneid2)) {\n    na.ix <- apply(data1, 2, function(x) { return(all(is.na(x))) })\n    data1 <- data1[ , !na.ix, drop=FALSE]\n    geneid1 <- geneid1[!na.ix]\n  } else { stop(\"data1, geneid1 and geneid2 parameters are mandatory!\") }\n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) { return(all(is.na(x))) })\n    data2 <- data2[ , !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else { data2 <- NULL }\n\n  gix1 <- !is.na(geneid1)\n  gix2 <- !is.na(geneid2)\n\n  geneid.common <- intersect(geneid1[gix1], geneid2[gix2])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n\n  gg <- names(geneid1)[is.element(geneid1, geneid.common)]\n  gid <- geneid1[is.element(geneid1, geneid.common)]\n  gid.dupl <- unique(gid[duplicated(gid)])\n  gg.dupl <- names(geneid1)[is.element(geneid1, gid.dupl)]\n  gid.uniq <- gid[!is.element(gid, gid.dupl)]\n  gg.uniq <- names(geneid1)[is.element(geneid1, gid.uniq)]\n  datat <- data1[ ,gg.uniq,drop=FALSE]\n  if(length(gid.dupl) > 0) {\n    if(verbose) { message(\"\\ndataset1 duplicates...\") }\n    pena <- apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n    pena <- log((nrow(data1) + 1) / (pena + 1)) + 1\n    sdr <- drop(apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n    mysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid1[gg.dupl], \"sd\"=sdr)\n    mysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n    mysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n    datat <- cbind(datat, data1[ , mysd[ , \"probe\"], drop=FALSE])\n  }\n  data1 <- datat\n  geneid1 <- geneid1[dimnames(data1)[[2]]]\n\n  if(is.null(data2)) {\n    geneid2 <- geneid2[!duplicated(geneid2) & is.element(geneid2, geneid.common)]\n  } else {\n    gg <- names(geneid2)[is.element(geneid2, geneid.common)]\n    gid <- geneid2[is.element(geneid2, geneid.common)]\n    gid.dupl <- unique(gid[duplicated(gid)])\n    gg.dupl <- names(geneid2)[is.element(geneid2, gid.dupl)]\n    gid.uniq <- gid[!is.element(gid, gid.dupl)]\n    gg.uniq <- names(geneid2)[is.element(geneid2, gid.uniq)]\n    datat <- data2[ ,gg.uniq,drop=FALSE]\n    if(length(gid.dupl) > 0) {\n      if(verbose) { message(\"\\ndataset2 duplicates...\") }\n      pena <- apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n      pena <- log((nrow(data2) + 1) / (pena + 1)) + 1\n      sdr <- drop(apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n      mysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid2[gg.dupl], \"sd\"=sdr)\n      mysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n      mysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n      datat <- cbind(datat, data2[ , mysd[ , \"probe\"], drop=FALSE])\n    }\n    data2 <- datat\n    geneid2 <- geneid2[dimnames(data2)[[2]]]\n  }\n\n  rix <- match(geneid2, geneid1)\n  geneid1 <- geneid1[rix]\n  data1 <- data1[ ,rix,drop=FALSE]\n  return(list(\"geneid1\"=geneid1, \"data1\"=data1, \"geneid2\"=geneid2, \"data2\"=data2))\n}"
      },
      {
        "partial": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  # Initialize and validate input\n  geneid1 <- as.character(geneid1)\n  geneid2 <- as.character(geneid2)\n  names(geneid1) <- names(geneid1) %||% dimnames(data1)[[2]]\n  if(!missing(data2)) names(geneid2) <- names(geneid2) %||% dimnames(data2)[[2]]\n  \n  # Check for mandatory parameters\n  if(missing(data1) || missing(geneid1) || missing(geneid2)) {\n    stop(\"data1, geneid1 and geneid2 parameters are mandatory!\")\n  }\n  \n  # Remove probes without measurements\n  na.ix <- apply(data1, 2, function(x) all(is.na(x)))\n  data1 <- data1[, !na.ix, drop=FALSE]\n  geneid1 <- geneid1[!na.ix]\n  \n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) all(is.na(x)))\n    data2 <- data2[, !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else {\n    data2 <- NULL\n  }\n  \n  # Find common gene ids\n  geneid.common <- intersect(geneid1[!is.na(geneid1)], geneid2[!is.na(geneid2)])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n  \n  # Process dataset1\n  # ...\n  \n  # Process dataset2\n  # ...\n  \n  # Align datasets\n  # ...\n  \n  # Return result\n  # ...\n}",
        "complete": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  # Initialize and validate input\n  geneid1 <- as.character(geneid1)\n  geneid2 <- as.character(geneid2)\n  names(geneid1) <- names(geneid1) %||% dimnames(data1)[[2]]\n  if(!missing(data2)) names(geneid2) <- names(geneid2) %||% dimnames(data2)[[2]]\n  \n  # Check for mandatory parameters\n  if(missing(data1) || missing(geneid1) || missing(geneid2)) {\n    stop(\"data1, geneid1 and geneid2 parameters are mandatory!\")\n  }\n  \n  # Remove probes without measurements\n  na.ix <- apply(data1, 2, function(x) all(is.na(x)))\n  data1 <- data1[, !na.ix, drop=FALSE]\n  geneid1 <- geneid1[!na.ix]\n  \n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) all(is.na(x)))\n    data2 <- data2[, !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else {\n    data2 <- NULL\n  }\n  \n  # Find common gene ids\n  geneid.common <- intersect(geneid1[!is.na(geneid1)], geneid2[!is.na(geneid2)])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n  \n  # Process dataset1\n  process_dataset <- function(geneid, data, common_ids, verbose) {\n    gg <- names(geneid)[geneid %in% common_ids]\n    gid <- geneid[geneid %in% common_ids]\n    gid.dupl <- unique(gid[duplicated(gid)])\n    gid.uniq <- setdiff(gid, gid.dupl)\n    datat <- data[, names(geneid)[geneid %in% gid.uniq], drop=FALSE]\n    \n    if(length(gid.dupl) > 0) {\n      if(verbose) message(\"\\ndataset duplicates...\")\n      gg.dupl <- names(geneid)[geneid %in% gid.dupl]\n      pena <- apply(data[, gg.dupl, drop=FALSE], 2, function(x) sum(is.na(x)))\n      "
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/utilities.R",
    "language": "R",
    "content": "# tSet molecularProfiles from eSets to SEs\n#\n# Converts all ExpressionSet objects within the molecularProfiles slot of a\n#   ToxicoSet to SummarizedExperiments\n#\n# @param tSet \\code{S4} A ToxicoSet containing molecular data in ExpressionSets\n#\n# @return \\code{S4} A ToxicoSet containing molecular data in a SummarizedExperiments\n#\n#' @importFrom SummarizedExperiment assay assays assayNames\n#' @importClassesFrom SummarizedExperiment SummarizedExperiment Assays\n#' @importFrom Biobase exprs fData pData annotation protocolData assayData experimentData\n#' @importFrom S4Vectors SimpleList DataFrame\n#' @importFrom stats setNames\n#' @export\n#' @keywords internal\n#' @noRd\n.convertTsetMolecularProfilesToSE <- function(tSet) {\n\n  eSets <- molecularProfilesSlot(tSet) # Extract eSet data\n\n  molecularProfilesSlot(tSet) <-\n    lapply(eSets,\n           function(eSet){\n\n             # Build summarized experiment from eSet\n             SE <- SummarizedExperiment::SummarizedExperiment(\n               ## TODO:: Do we want to pass an environment for better memory efficiency?\n               assays=SimpleList(as.list(Biobase::assayData(eSet))\n               ),\n               # Switch rearrange columns so that IDs are first, probes second\n               rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                            rownames=rownames(Biobase::fData(eSet))\n               ),\n               colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                            rownames=rownames(Biobase::pData(eSet))\n               ),\n               metadata=list(\"experimentData\" = eSet@experimentData,\n                             \"annotation\" = Biobase::annotation(eSet),\n                             \"protocolData\" = Biobase::protocolData(eSet)\n               )\n             )\n             ## TODO:: Determine if this can be done in the SE constructor?\n             # Extract names from expression set\n             SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n             # Assign SE to tSet\n             mDataType <- Biobase::annotation(eSet)\n             molecularProfilesSlot(tSet)[[mDataType]] <- SE\n           })\n  setNames(molecularProfilesSlot(tSet), names(eSets))\n  tSet\n}\n\n# Validate tSet molecularProfiles Conversion\n#\n# Checks that all the information contained in an ExpressionSet molecularProfile\n#   was successfully tranferred to the SummarizedExperiment molecularProfile\n#\n# @param tSet_new \\code{S4} a tSet containing molecularProfiles as SummarizedExperiments\n# @param tSet_old \\code{S4} a tSet containing molecularProfiles as ExpressionSets\n#\n# @return \\code{message} Any slots which are not the same\n#\n##' @importFrom assertthat are_equal\n##' @importFrom SummarizedExperiment SummarizedExperiment Assays assay\n##'   assayNames assayNames<-\n##' @importFrom Biobase exprs fData pData annotation protocolData\n##'   assayDataElementNames experimentData assayData\n##' @keywords internal\n#.validateTsetMolecularProfilesToSEConversion <- function(tSet_old, tSet_new) {\n#\n#  # Testing that tSets are in correct order\n#  message(\"Checking is tSet structures are correct\")\n#\n#  if(!all(vapply(tSet_old@molecularProfiles,\n#                 function(x) { is(x, \"ExpressionSet\") },\n#                 FUN.VALUE = logical(1)))\n#  ) message(\"Old tSet doesn't contain ExpressionSet objects, maybe argument\n#            order is wrong?\")\n#\n#  if(!all(vapply(molecularProfilesSlot(tSet_new),\n#                 function(x) { is(x, \"SummarizedExperiment\") },\n#                 FUN.VALUE = logical(1)))\n#  ) message(\"New tSet doesn't contain SummarizedExperiment objects, maybe\n#            argument order is wrong?\")\n#\n#  # Comparing molecularProfiles slot data\n#  message(\"Checking molecularProfiles slots hold equivalent data.\")\n#\n#  for (i in seq_len(length(tSet_old@molecularProfiles))) {\n#    for (j in seq_along(assays(molecularProfilesSlot(tSet_new)[[i]]))) {\n#      if(!all(as.list(assayData(tSet_old@molecularProfiles[[i]]))[[j]] ==\n#              as.list(assays(molecularProfilesSlot(tSet_new)[[i]]))[[j]],\n#          na.rm = TRUE)\n#        ) message(\"The assay data is not equivalent\")\n#    }\n#  }\n#  ## TODO:: Rewrite this as an apply statement\n#  for (i in seq_len(length(tSet_old@molecularProfiles))) { # Have to compare like this due to NAs in data\n#    # Checking phenoData\n#    if(\n#      if (nrow(pData(tSet_old@molecularProfiles[[i]])) > 0) {\n#        !all(\n#          as(tSet_old@molecularProfiles[[i]]@phenoData, \"data.frame\") ==\n#            as.data.frame(molecularProfilesSlot(tSet_new)[[i]]@colData[\n#              seq_len(length(molecularProfilesSlot(tSet_new)[[i]]@colData) - 1)]),\n#          na.rm = TRUE)\n#      } else { FALSE }\n#    ) message(\"The phenoData is not equivalent\")\n#    # Checking featureData\n#    if(\n#      if (nrow(fData(tSet_old@molecularProfiles[[i]])) > 0) {\n#        !all(\n#          as(tSet_old@molecularProfiles[[i]]@featureData, \"data.frame\") ==\n#            as.data.frame(molecularProfilesSlot(tSet_new)[[i]]@elementMetadata[\n#              seq_len(length(molecularProfilesSlot(tSet_new)[[i]]@elementMetadata) - 1)]),\n#          na.rm=TRUE)\n#      } else { FALSE }\n#    ) message(\"The featureData is not equivalent\")\n#    # Checking protocolData\n#    if(\n#      !all(\n#        as(tSet_old@molecularProfiles[[i]]@protocolData, \"data.frame\") ==\n#          as(molecularProfilesSlot(tSet_new)[[i]]@metadata$protocolData, \"data.frame\"),\n#        na.rm = TRUE)\n#    ) message(\"The protocolData is not equivalent\")\n#  }\n#\n#  if(!assertthat::are_equal(\n#    lapply(tSet_old@molecularProfiles, function(x) { annotation(x) }),\n#    lapply(molecularProfilesSlot(tSet_new), function(x) { metadata(x)$annotation }))\n#  ) message(\"The annotation is not equivalent\")\n#\n#  if(!assertthat::are_equal(\n#    lapply(tSet_old@molecularProfiles, function(x) { experimentData(x) }),\n#    lapply(molecularProfilesSlot(tSet_new), function(x) { metadata(x)$experimentData }))\n#  ) message(\"The experimentData is not equivalent\")\n#\n#  # Comparing remainder of tSet slots; should not be affect by conversion\n#  message(\"Comparing remainder of tSet slots\")\n#\n#  if(!assertthat::are_equal(annotation(tSet_old), annotation(tSet_new)))\n#    message(\"The annotation slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(sampleInfo(tSet_old), sampleInfo(tSet_new)))\n#    message(\"The cell slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(treatmentInfo(tSet_old), treatmentInfo(tSet_new)))\n#    message(\"The drug slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(treatmentResponse(tSet_old), treatmentResponse(tSet_new)))\n#    message(\"The sensitivity slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(datasetType(tSet_old), datasetType(tSet_new)))\n#    message(\"The datasetType slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(tSet_old@perturbation, tSet_new@perturbation))\n#    message(\"The perturbation slots are not equivalent!\")\n#\n# if(!assertthat::are_equal(curation(tSet_old), curation(tSet_new)))\n#   message(\"The curation slots are not equivalent!\")\n#}\n\n# Utility function to resave all datasets after modifying converttSetMolecularProfiles\n#\n# Converts all example dastasets specificed as an argument from\n#   molecularProfiles as ExpressionSet to molecularProfiles as\n#   SummarizedExperiment and saves them in the data folder\n#\n# @param datasets \\code{character} A list of the example datasets to update\n#\n# @return \\code{none} Works by side effects alone to resave all example\n#   datasets in a package to have SummarizedExperiments for molecularProfiles\n#.resaveAllExampleDatasets <- function(datasets) {\n#  for (dataset in datasets) {\n#    dataDir <- paste0(grep('data', list.dirs(), value=TRUE))\n#    load(paste0(dataDir, '/', dataset, '_old.rda'))\n#    assign(dataset, .convertTsetMolecularProfilesToSE(get(dataset)))\n#    save(list=dataset, file=paste0(dataDir, '/', dataset, '.rda'), compress='xz')\n#  }\n#}\n#\n\n.eSetToSE <- function(eSet) {\n    # Build summarized experiment from eSet\n    SE <- SummarizedExperiment::SummarizedExperiment(\n        ## TODO:: Do we want to pass an environment for better memory efficiency?\n        assays=SimpleList(as.list(Biobase::assayData(eSet))\n        ),\n        # Switch rearrange columns so that IDs are first, probes second\n        rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                     rownames=rownames(Biobase::fData(eSet))\n        ),\n        colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                     rownames=rownames(Biobase::pData(eSet))\n        ),\n        metadata=list(\"experimentData\" = eSet@experimentData,\n                      \"annotation\" = Biobase::annotation(eSet),\n                      \"protocolData\" = Biobase::protocolData(eSet)\n        )\n    )\n    ## TODO:: Determine if this can be done in the SE constructor?\n    # Extract names from expression set\n    SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n    return(SE)\n}\n\n#.validateESetToSEConversions <- function(eSet, SE) {\n#        for (j in seq_along(assays(SE))) {\n#            if(!all(as.list(assayData(eSet))[[j]] ==\n#                    as.list(assays(SE))[[j]],\n#                    na.rm = TRUE)\n#            ) message(\"The assay data is not equivalent\")\n#        }\n#    ## TODO:: Rewrite this as an apply statement\n#        # Checking phenoData\n#        if(\n#            if (nrow(pData(eSet)) > 0) {\n#                !all(\n#                    as(eSet@phenoData, value=\"data.frame\") ==\n#                    as.data.frame(SE@colData[\n#                        seq_len(length(SE@colData) - 1)]),\n#                    na.rm = TRUE)\n#            } else { FALSE }\n#        ) message(\"The phenoData is not equivalent\")\n#        # Checking featureData\n#        if(\n#            if (nrow(fData(eSet)) > 0) {\n#                !all(\n#                    as(eSet@featureData, value=\"data.frame\") ==\n#                    as.data.frame(SE@elementMetadata[\n#                        seq_len(length(SE@elementMetadata) - 1)]),\n#                    na.rm=TRUE)\n#            } else { FALSE }\n#        ) message(\"The featureData is not equivalent\")\n#        # Checking protocolData\n#        if(\n#            !all(\n#                as(eSet@protocolData, value=\"data.frame\") ==\n#                as(SE@metadata$protocolData, value=\"data.frame\"),\n#                na.rm = TRUE)\n#        ) message(\"The protocolData is not equivalent\")\n#\n#    if(!assertthat::are_equal(\n#        annotation(eSet),\n#        metadata(SE)$annotation\n#    )) message(\"The annotation is not equivalent\")\n#\n#    if(!assertthat::are_equal(\n#        experimentData(eSet),\n#        metadata(SE)$experimentData\n#    )) message(\"The experimentData is not equivalent\")\n#}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `.convertTsetMolecularProfilesToSE` function?",
        "answer": "The main purpose of the `.convertTsetMolecularProfilesToSE` function is to convert all ExpressionSet objects within the molecularProfiles slot of a ToxicoSet to SummarizedExperiments. It takes a ToxicoSet containing molecular data in ExpressionSets as input and returns a ToxicoSet with molecular data in SummarizedExperiments."
      },
      {
        "question": "How does the function handle the conversion of assay data from ExpressionSet to SummarizedExperiment?",
        "answer": "The function converts assay data by using `SimpleList(as.list(Biobase::assayData(eSet)))` to create the assays for the SummarizedExperiment. This approach ensures that all assay data from the ExpressionSet is transferred to the new SummarizedExperiment structure."
      },
      {
        "question": "What additional metadata is preserved during the conversion process, and how is it stored in the resulting SummarizedExperiment?",
        "answer": "The function preserves additional metadata including experimentData, annotation, and protocolData from the ExpressionSet. These are stored in the metadata list of the SummarizedExperiment, with keys 'experimentData', 'annotation', and 'protocolData' respectively. This ensures that all relevant information from the original ExpressionSet is maintained in the new structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Convert ExpressionSet to SummarizedExperiment\n.eSetToSE <- function(eSet) {\n    SE <- SummarizedExperiment::SummarizedExperiment(\n        assays=SimpleList(as.list(Biobase::assayData(eSet))),\n        rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                     rownames=rownames(Biobase::fData(eSet))),\n        colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                     rownames=rownames(Biobase::pData(eSet))),\n        metadata=list(\"experimentData\" = eSet@experimentData,\n                      \"annotation\" = Biobase::annotation(eSet),\n                      \"protocolData\" = Biobase::protocolData(eSet))\n    )\n    # TODO: Complete the function\n    return(SE)\n}",
        "complete": "# Convert ExpressionSet to SummarizedExperiment\n.eSetToSE <- function(eSet) {\n    SE <- SummarizedExperiment::SummarizedExperiment(\n        assays=SimpleList(as.list(Biobase::assayData(eSet))),\n        rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                     rownames=rownames(Biobase::fData(eSet))),\n        colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                     rownames=rownames(Biobase::pData(eSet))),\n        metadata=list(\"experimentData\" = eSet@experimentData,\n                      \"annotation\" = Biobase::annotation(eSet),\n                      \"protocolData\" = Biobase::protocolData(eSet))\n    )\n    SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n    return(SE)\n}"
      },
      {
        "partial": "# Convert ToxicoSet molecularProfiles from eSets to SEs\n.convertTsetMolecularProfilesToSE <- function(tSet) {\n    eSets <- molecularProfilesSlot(tSet)\n    molecularProfilesSlot(tSet) <- lapply(eSets, function(eSet) {\n        SE <- .eSetToSE(eSet)\n        # TODO: Complete the function\n    })\n    # TODO: Complete the function\n    return(tSet)\n}",
        "complete": "# Convert ToxicoSet molecularProfiles from eSets to SEs\n.convertTsetMolecularProfilesToSE <- function(tSet) {\n    eSets <- molecularProfilesSlot(tSet)\n    molecularProfilesSlot(tSet) <- lapply(eSets, function(eSet) {\n        SE <- .eSetToSE(eSet)\n        mDataType <- Biobase::annotation(eSet)\n        molecularProfilesSlot(tSet)[[mDataType]] <- SE\n    })\n    setNames(molecularProfilesSlot(tSet), names(eSets))\n    return(tSet)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassAccessorMethods.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(ToxicoGx)\n\n#### Checking example tSet structure ####\ncontext(\"Testing TGGATESsmall object validity...\")\n\ntest_that(\"TGGATESsmall tSet has correct structure\", {\n    data(TGGATESsmall)\n    expect_error(checkTSetStructure(TGGATESsmall), NA)\n})\n\n#### tSet Acessor methods ####\ncontext(\"Testing tSet Class Accessor Methods...\")\n\n# @treatment Slot\ntest_that(\"@treatment slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(treatmentInfo(TGGATESsmall),\n        \"drugInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(treatmentNames(TGGATESsmall),\n        \"drugNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(treatmentInfo(TGGATESsmall), TGGATESsmall@treatment)\n    expect_equal(treatmentNames(TGGATESsmall), TGGATESsmall@treatment$treatmentid)\n})\n\n# @annotation Slot\ntest_that(\"@annotation slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(TGGATESsmall@annotation,\n        \"annotation.TGGATESsmall.rds\")\n    expect_equal_to_reference(name(TGGATESsmall), \"name.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(name(TGGATESsmall), TGGATESsmall@annotation$name)\n})\n\n# @molecularProfile Slot\ntest_that(\"@molecularProfiles slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(mDataNames(TGGATESsmall),\n        \"mDataNames.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(mDataNames(TGGATESsmall),\n        names(TGGATESsmall@molecularProfiles))\n\n    ## TODO:: Test this with incorrect tSet structure to determine if error\n    ##>messages print in the correct order\n    for (name in names(TGGATESsmall@molecularProfiles)) {\n        context(\"External validation...\")\n        expect_equal_to_reference(\n            molecularProfiles(TGGATESsmall, name)[, 1:100],\n            paste0(name, \".molecularProfiles.TGGATESsmall.rds\")\n        )\n        expect_equal_to_reference(featureInfo(TGGATESsmall, name),\n            paste0(name, \".featureInfo.TGGATESsmall.rds\"))\n        expect_equal_to_reference(fNames(TGGATESsmall, name),\n            paste0(name, \".fNames.TGGATESsmall.rds\"))\n        expect_equal_to_reference(phenoInfo(TGGATESsmall, name)[1:100, ],\n            paste0(name, \".phenoData.TGGATESsmall.rds\"))\n        context(\"Internal validation...\")\n        # expect_equal(molecularProfiles(TGGATESsmall, name),\n        #     assay(TGGATESsmall@molecularProfiles[[name]], 1))\n        expect_equal(featureInfo(TGGATESsmall, name),\n            rowData(TGGATESsmall@molecularProfiles[[name]]))\n        expect_equal(fNames(TGGATESsmall, name),\n            rownames(rowData(TGGATESsmall@molecularProfiles[[name]])))\n        expect_equal(phenoInfo(TGGATESsmall, name),\n            colData(TGGATESsmall@molecularProfiles[[name]]))\n    }\n})\n\n# @sample Slot\ntest_that(\"@sample slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(sampleInfo(TGGATESsmall),\n        \"cellInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(sampleNames(TGGATESsmall),\n        \"cellNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(sampleInfo(TGGATESsmall), TGGATESsmall@sample)\n    expect_equal(sampleNames(TGGATESsmall), TGGATESsmall@sample$sampleid)\n})\n\n# @sensitivty Slot\ntest_that(\"@treatmentResponse slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(sensitivityInfo(TGGATESsmall),\n        \"sensitivityInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(sensitivityProfiles(TGGATESsmall),\n        \"sensitivitProfiles.TGGATESsmall.rds\")\n    expect_equal_to_reference(sensitivityMeasures(TGGATESsmall),\n        \"sensitivityMeasures.TGGATESsmall.rds\")\n    expect_equal_to_reference(sensNumber(TGGATESsmall),\n        \"sensNumber.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(sensitivityInfo(TGGATESsmall), TGGATESsmall@treatmentResponse$info)\n    expect_equal(sensitivityProfiles(TGGATESsmall),\n        TGGATESsmall@treatmentResponse$profiles)\n    expect_equal(sensitivityMeasures(TGGATESsmall),\n        colnames(TGGATESsmall@treatmentResponse$profiles))\n    expect_equal(sensNumber(TGGATESsmall), TGGATESsmall@treatmentResponse$n)\n})\n\n# @perturbation Slot\ntest_that(\"@perturbation slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(TGGATESsmall@perturbation,\n        \"perturbation.TGGATESsmall.rds\")\n    expect_equal_to_reference(pertNumber(TGGATESsmall),\n        \"pertNumber.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(pertNumber(TGGATESsmall), TGGATESsmall@perturbation$n)\n})\n\n# @curation Slot\ntest_that(\"@curation slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(TGGATESsmall@curation,\n        \"curation.TGGATESsmall.rds\")\n})\n\n# subsetTo Method\ntest_that(\"subsetTo() class method produces expected results\", {\n    data(\"TGGATESsmall\")\n\n    ## TODO:: Add unit tests for `[` subset operator\n    ## TODO:: Change context() messages to be more informative when\n    ##>running devtools::test()\n    context(\"External validation...\")\n    expect_equal_to_reference(\n        subsetTo(TGGATESsmall, drugs = treatmentNames(TGGATESsmall)[1],\n            cell_lines=sampleNames(TGGATESsmall)[1]),\n        \"subsetTo.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    ## Tests that subsetting molecularProfiles on duration works\n    expect_equal(all(\n        sensitivityInfo(subsetTo(TGGATESsmall, duration = \"2\"))$duration_h\n            %in% \"2\"),\n        TRUE)\n    # Tests that relationship between sensitivity experiments and\n    #>molecularProfiles is preserved\n    #>(4 molecular Profiles / 1 sensitivity experiment)\n    for (name in names(molecularProfilesSlot(TGGATESsmall))) {\n        testthat::context(paste0(\"Testing subsetTo on molecularProfile for \",\n            name))\n        ## TODO:: Generalize duration arguement so that it uses the first\n        ##>unique duration value in tSet (replace \"8\" with this)\n        testthat::expect_equal(all(\n            SummarizedExperiment::colData(\n                ToxicoGx::subsetTo(TGGATESsmall, duration = \"8\"\n                    )@molecularProfiles[[name]])$duration %in% \"8\"),\n            TRUE)\n    }\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `checkTSetStructure` function in the given code, and how is it being used?",
        "answer": "The `checkTSetStructure` function is used to validate the structure of a ToxicoGx TSet object. In this code, it's being used within a test case to ensure that the TGGATESsmall dataset has the correct structure. The test expects that calling `checkTSetStructure(TGGATESsmall)` will not produce an error, which is checked using the `expect_error(checkTSetStructure(TGGATESsmall), NA)` assertion."
      },
      {
        "question": "How does the code validate the consistency between accessor methods and direct slot access for the TGGATESsmall object?",
        "answer": "The code validates consistency between accessor methods and direct slot access by comparing the results of accessor functions with the corresponding slot values. For example, in the '@treatment Slot' test, it checks if `treatmentInfo(TGGATESsmall)` equals `TGGATESsmall@treatment`, and if `treatmentNames(TGGATESsmall)` equals `TGGATESsmall@treatment$treatmentid`. This pattern is repeated for other slots like @annotation, @molecularProfiles, @sample, etc., ensuring that the accessor methods correctly retrieve the data stored in the object's slots."
      },
      {
        "question": "What is the purpose of the `subsetTo` method in the given code, and how is it being tested?",
        "answer": "The `subsetTo` method is used to create a subset of the TGGATESsmall dataset based on specified criteria. In the code, it's being tested in several ways:\n1. External validation: The result of subsetting by the first drug and cell line is compared to a reference file.\n2. Internal validation: It checks if subsetting by duration works correctly by verifying that all duration values in the subset are as expected.\n3. It tests if the relationship between sensitivity experiments and molecular profiles is preserved after subsetting.\n4. It also checks if subsetting works correctly for each molecular profile type in the dataset.\nThese tests ensure that the `subsetTo` method correctly filters the data while maintaining the integrity and relationships within the TSet object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"@treatment slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(treatmentInfo(TGGATESsmall),\n        \"drugInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(treatmentNames(TGGATESsmall),\n        \"drugNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(treatmentInfo(TGGATESsmall), TGGATESsmall@treatment)\n    expect_equal(treatmentNames(TGGATESsmall), TGGATESsmall@treatment$treatmentid)\n})",
        "complete": "test_that(\"@treatment slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(treatmentInfo(TGGATESsmall),\n        \"drugInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(treatmentNames(TGGATESsmall),\n        \"drugNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(treatmentInfo(TGGATESsmall), TGGATESsmall@treatment)\n    expect_equal(treatmentNames(TGGATESsmall), TGGATESsmall@treatment$treatmentid)\n})"
      },
      {
        "partial": "test_that(\"subsetTo() class method produces expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(\n        subsetTo(TGGATESsmall, drugs = treatmentNames(TGGATESsmall)[1],\n            cell_lines=sampleNames(TGGATESsmall)[1]),\n        \"subsetTo.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(all(\n        sensitivityInfo(subsetTo(TGGATESsmall, duration = \"2\"))$duration_h\n            %in% \"2\"),\n        TRUE)\n    # Complete the test for molecularProfiles\n})",
        "complete": "test_that(\"subsetTo() class method produces expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(\n        subsetTo(TGGATESsmall, drugs = treatmentNames(TGGATESsmall)[1],\n            cell_lines=sampleNames(TGGATESsmall)[1]),\n        \"subsetTo.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(all(\n        sensitivityInfo(subsetTo(TGGATESsmall, duration = \"2\"))$duration_h\n            %in% \"2\"),\n        TRUE)\n    for (name in names(molecularProfilesSlot(TGGATESsmall))) {\n        testthat::context(paste0(\"Testing subsetTo on molecularProfile for \",\n            name))\n        testthat::expect_equal(all(\n            SummarizedExperiment::colData(\n                ToxicoGx::subsetTo(TGGATESsmall, duration = \"8\"\n                    )@molecularProfiles[[name]])$duration %in% \"8\"),\n            TRUE)\n    }\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/read.m.file.R",
    "language": "R",
    "content": "#' @title Function to read a 'csv' file containing gene lists (aka gene signatures)\n#'\n#' @description\n#' This function allows for reading a 'csv' file containing gene signatures. \n#'   Each gene signature is composed of at least four columns: \"gene.list\" is the name \n#'   of the signature on the first line and empty fields below, \"probes\" are the probe \n#'   names, \"EntrezGene.ID\" are the EntrezGene IDs and \"coefficient\" are the coefficients \n#'   of each probe.\n#'\n#' @usage\n#' read.m.file(file, ...)\n#'\n#' @param file\tFilename of the 'csv' file.\n#' @param ... Additional parameters for read.csv function.\n#'\n#' @return\n#' List of gene signatures.\n#'\n#' @seealso\n#' [genefu::mod1], [genefu::mod2], 'extdata/desmedt2008_genemodules.csv', 'extdata/haibekains2009_sig_genius.csv'\n#'\n#' @examples\n#' # read the seven gene modules as published in Desmedt et al 2008\n#' genemods <- read.m.file(system.file(\"extdata/desmedt2008_genemodules.csv\",\n#'   package = \"genefu\"))\n#' str(genemods, max.level=1)\n#' # read the three subtype signtaures from GENIUS\n#' geniusm <- read.m.file(system.file(\"extdata/haibekains2009_sig_genius.csv\",\n#'   package = \"genefu\"))\n#' str(geniusm, max.level=1)\n#' \n#' @md\n#' @export\nread.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\tix.l <- ix.delim[i] - 1\n\t\t\tnp <- c(np, as.character(obj.file[ix.f,1]))\n\t\t\tgroups <- c(groups, rep(i, ix.l - ix.f + 1))\n\t\t\tnpp <- rbind(npp, obj.file[ix.f:ix.l,2:ncol(obj.file)])\n\t\t\tix.f <- ix.l + 2\n\t\t}\n\t\tugroups <- unique(groups)\n\t\tobj <- NULL\n\t\tfor (j in 1:length(ugroups)) {\n\t\t\tobj <- c(obj, list(npp[groups == ugroups[j], ]))\n\t\t}\n\tnames(obj) <- np\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read.m.file` function and what type of data does it process?",
        "answer": "The `read.m.file` function is designed to read a CSV file containing gene signatures. It processes data where each gene signature is composed of at least four columns: 'gene.list' (signature name), 'probes' (probe names), 'EntrezGene.ID' (EntrezGene IDs), and 'coefficient' (coefficients of each probe). The function returns a list of gene signatures."
      },
      {
        "question": "How does the function handle multiple gene signatures within a single CSV file?",
        "answer": "The function handles multiple gene signatures by identifying non-empty entries in the first column as delimiters between signatures. It uses these delimiters to split the data into separate groups, each representing a distinct gene signature. The function then creates a list where each element corresponds to a different gene signature, with the signature names used as list element names."
      },
      {
        "question": "What is the significance of the `groups` and `ugroups` variables in the function, and how are they used?",
        "answer": "The `groups` variable is used to assign a group number to each row of data, indicating which gene signature it belongs to. The `ugroups` variable contains the unique group numbers. These variables are crucial for separating the data into distinct gene signatures. The function iterates through `ugroups` to create separate list elements for each unique gene signature, effectively organizing the data into a structured list output."
      }
    ],
    "completion_tasks": [
      {
        "partial": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\t# Complete the code here\n\t\t}\n\t\t# Complete the rest of the function\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}",
        "complete": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\tix.l <- ix.delim[i] - 1\n\t\t\tnp <- c(np, as.character(obj.file[ix.f,1]))\n\t\t\tgroups <- c(groups, rep(i, ix.l - ix.f + 1))\n\t\t\tnpp <- rbind(npp, obj.file[ix.f:ix.l,2:ncol(obj.file)])\n\t\t\tix.f <- ix.l + 2\n\t\t}\n\t\tugroups <- unique(groups)\n\t\tobj <- NULL\n\t\tfor (j in 1:length(ugroups)) {\n\t\t\tobj <- c(obj, list(npp[groups == ugroups[j], ]))\n\t\t}\n\t\tnames(obj) <- np\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}"
      },
      {
        "partial": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\t# Complete the code here\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}",
        "complete": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\tix.l <- ix.delim[i] - 1\n\t\t\tnp <- c(np, as.character(obj.file[ix.f,1]))\n\t\t\tgroups <- c(groups, rep(i, ix.l - ix.f + 1))\n\t\t\tnpp <- rbind(npp, obj.file[ix.f:ix.l,2:ncol(obj.file)])\n\t\t\tix.f <- ix.l + 2\n\t\t}\n\t\tugroups <- unique(groups)\n\t\tobj <- NULL\n\t\tfor (j in 1:length(ugroups)) {\n\t\t\tobj <- c(obj, list(npp[groups == ugroups[j], ]))\n\t\t}\n\t\tnames(obj) <- np\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/geneDrugPerturbation.R",
    "language": "R",
    "content": "#' Compute gene-drug associations\n#'\n#' Function computing gene-drug associations from perturbation data\n#'\n#' @examples\n#' ToxicoGx::drugPerturbationSig(tSet = TGGATESsmall,\n#'   mDataType=\"rna\",\n#'   cell_lines=\"Hepatocyte\",\n#'   duration=\"24\",\n#'   dose=c(\"Control\", \"Low\"),\n#'   drugs=c(\"Omeprazole\", \"Isoniazid\"),\n#'   returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n#'   verbose=FALSE)\n#'\n#' @param x [numeric] Vector of gene expression values\n#' @param concentration [numeric] Vector with drug concentrations/doses\n#' @param type [factor] Vector of factors specifying the cell lines or type types\n#' @param batch [factor] Vector of factors specifying the batch\n#' @param duration [character] Vector of measurement times (in hours)\n#' @param model [logical] Should the full linear model be returned? Default set to FALSE\n#'\n#' @return [numeric] Vector reporting the effect size (estimateof the coefficient of drug concentration), standard error (se), sample size (n), t statistic, and F statistics and its corresponding p-value\n#'\n#' @importFrom stats complete.cases median lm anova\n#'\n#' @keywords internal\n#' @export\ngeneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n\n  if (length(sort(unique(type))) > 1) {\n    ff0 <- sprintf(\"%s + type\", ff0)\n    ff <- sprintf(\"%s + type\", ff)\n  }\n  if (length(sort(unique(batch))) > 1) {\n    ff0 <- sprintf(\"%s + batch\", ff0)\n    ff <- sprintf(\"%s + batch\", ff)\n  }\n\n  ### add experiment duration if the vector consists of more than one different value\n\n  if(length(sort(unique(duration))) > 2){\n    ff0 <- sprintf(\"%s + duration\", ff0)\n    ff <- sprintf(\"%s + duration\", ff)\n  }\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    ## extract statistics\n    tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n  }\n  names(tt) <- nc\n  ## add tissue type/cell line statistics\n  if(length(sort(unique(type))) > 1) {\n    rr <- summary(mm0)\n    ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n  } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n  tt <- c(tt, ttype)\n  ## add model\n  if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n  return(tt)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `geneDrugPerturbation` function and what are its main input parameters?",
        "answer": "The `geneDrugPerturbation` function computes gene-drug associations from perturbation data. Its main input parameters are:\n- `x`: A numeric vector of gene expression values\n- `concentration`: A numeric vector of drug concentrations/doses\n- `type`: A factor vector specifying cell lines or types\n- `batch`: A factor vector specifying the batch\n- `duration`: A character vector of measurement times (in hours)\n- `model`: A logical value indicating whether to return the full linear model (default is FALSE)"
      },
      {
        "question": "How does the function handle different scenarios in terms of the number of unique concentrations, types, batches, and durations?",
        "answer": "The function adapts its behavior based on the input data:\n1. If there are fewer than 2 unique concentrations, it returns a warning and a vector of NAs.\n2. If there are more than 1 unique type, it adds 'type' to the linear model formula.\n3. If there are more than 1 unique batch, it adds 'batch' to the formula.\n4. If there are more than 2 unique durations, it adds 'duration' to the formula.\nThis approach allows the function to create an appropriate linear model based on the available data."
      },
      {
        "question": "What statistical measures does the `geneDrugPerturbation` function return, and how are they calculated?",
        "answer": "The function returns several statistical measures:\n1. Estimate: The coefficient of drug concentration from the linear model\n2. Standard Error (se): The standard error of the estimate\n3. Sample size (n): The number of complete cases in the data\n4. T-statistic: The t-value for the concentration coefficient\n5. F-statistic: From the ANOVA comparison of models with and without concentration\n6. P-value: The p-value from the F-test\n7. Type F-statistic and p-value: If multiple types are present\n\nThese are calculated using linear models (lm) and ANOVA, comparing a model with concentration to one without it. If requested, the full linear model is also returned."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  # Add type, batch, and duration to formulas if necessary\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    # Fit models and extract statistics\n  }\n\n  # Complete the function\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  if (length(sort(unique(type))) > 1) {\n    ff0 <- sprintf(\"%s + type\", ff0)\n    ff <- sprintf(\"%s + type\", ff)\n  }\n  if (length(sort(unique(batch))) > 1) {\n    ff0 <- sprintf(\"%s + batch\", ff0)\n    ff <- sprintf(\"%s + batch\", ff)\n  }\n  if(length(sort(unique(duration))) > 2){\n    ff0 <- sprintf(\"%s + duration\", ff0)\n    ff <- sprintf(\"%s + duration\", ff)\n  }\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n  }\n  names(tt) <- nc\n  if(length(sort(unique(type))) > 1) {\n    rr <- summary(mm0)\n    ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n  } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n  tt <- c(tt, ttype)\n  if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n  return(tt)\n}"
      },
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  # Add type, batch, and duration to formulas if necessary\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    # Extract statistics\n  }\n  names(tt) <- nc\n  # Add type statistics and model if necessary\n  return(tt)\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  if (length(sort(unique(type))) > 1) {\n    ff0 <- sprintf(\"%s + type\", ff0)\n    ff <- sprintf(\"%s + type\", ff)\n  }\n  if (length(sort(unique(batch))) > 1) {\n    ff0 <- sprintf(\"%s + batch\", ff0)\n    ff <- sprintf(\"%s + batch\", ff)\n  }\n  if(length(sort(unique(duration))) > 2){\n    ff0 <- sprintf(\"%s + duration\", ff0)\n    ff <- sprintf(\"%s + duration\", ff)\n  }\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n  }\n  names(tt) <- nc\n  if(length(sort(unique(type))) > 1) {\n    rr <- summary(mm0)\n    ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n  } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n  tt <- c(tt, ttype)\n  if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n  return(tt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/endoPredict.R",
    "language": "R",
    "content": "if(getRversion() >= \"2.15.1\")  utils::globalVariables(\"sig.endoPredict\")\n\n#' @name endoPredict\n#' @title Function to compute the endoPredict signature as published by Filipits et al 2011\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene expression\n#'   values following the algorithm used for the endoPredict signature as published by\n#'   Filipits et al 2011.\n#'\n#' @usage\n#' endoPredict(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'    dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in\n#'   case ofambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#'   Note that for Affymetrix HGU datasets, the mapping is not necessary.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the mapping\n#'   such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' The function works best if data have been noralized with MAS5. Note that for Affymetrix\n#'   HGU datasets, the mapping is not necessary.\n#'\n#' @return\n#' A list with items:\n#' -score Continuous signature scores\n#' -risk Binary risk classification, 1 being high risk and 0 being low risk.\n#' -mapping Mapping used if necessary.\n#' -probe If mapping is performed, this matrix contains the correspondence between the gene\n#' list (aka signature) and gene expression data.\n#'\n#' @references\n#' Filipits, M., Rudas, M., Jakesz, R., Dubsky, P., Fitzal, F., Singer, C. F., et al. (2011).\n#'   \"A new molecular predictor of distant recurrence in ER-positive, HER2-negative\n#'   breast cancer adds independent information to conventional clinical risk factors.\"\n#'   Clinical Cancer Research, 17(18):6012\u20136020.\n#'\n#' @examples\n#' # load GENE70 signature\n#' data(sig.endoPredict)\n#' # load NKI dataset\n#' data(vdxs)\n#' # compute relapse score\n#' rs.vdxs <- endoPredict(data=data.vdxs, annot=annot.vdxs, do.mapping=FALSE)\n#'\n#' @md\n#' @export\nendoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\t## the reference genes are not taken into account due to their absence from most platforms\n  #sig2 <- sig.endoPredict\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n\trownames(sig2) <- sig2[ , \"probe.affy\"]\n\tgt <- nrow(sig2)\n\tif(do.mapping) { ## not an affy HGU platform\n\t\tgid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid1) <- dimnames(sig2)[[1]]\n\t\tgid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\t\t## mqpping\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tgm <- length(rr$geneid2)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tif(!all(is.element(sig2[sig2[ , \"group\"] == \"GOI\", \"EntrezGene.ID\"], rr$geneid1))) { ## if genes of interest are missing\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n\t\t}\n\t\tgid1 <- rr$geneid2\n\t\tgid2 <- rr$geneid1\n\t\tdata <- rr$data1\n\t\tmyprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n\t\t## change the names of probes in the data\n\t\tcolnames(data) <- names(gid2) <- names(gid1)\n    sig2 <- sig2[colnames(data), , drop=FALSE]\n\t\tgm <- ncol(data)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t} else {\n\t\tmyprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n\t\tdata <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n\t\tgm <- ncol(data)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t}\n\t## rename gene names by the gene symbols\n\tcolnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n\n\tif(do.mapping) {\n    ## transform expressions so they match approximately the scale of Affymetrix data\n    data <- apply(data, 2, function(x) {\n      xx <- (x - quantile(x, probs=0.025, na.rm=TRUE)) / (quantile(x, probs=0.975, na.rm=TRUE) - quantile(x, probs=0.025, na.rm=TRUE))\n      return((xx * 8) + 6)\n    })\n    data[!is.na(data) & data < 1] <- 1\n    data[!is.na(data) & data > 15] <- 15\n  }\n\n  data <- (data - apply(data, 1, mean, na.rm=TRUE)) + log2(500)\n  ## apply transformation factor and offset\n  datat <- t(apply(data, 1, function(x, a, b) {\n    return((x - b) / a)\n  }, a=sig2[ , \"a\"], b=sig2[ , \"b\"]))\n  data <- matrix(NA, nrow=nrow(data), ncol=ncol(data), dimnames=dimnames(data))\n  data[rownames(datat), colnames(datat)] <- datat\n\n\trs <- rs.unscaled <- rsrisk <- rep(NA, nrow(data))\n  rs.unscaled <- drop((sig2[ , \"weight\"] %*% t(data)) - 2.63)\n  rs <- sapply(rs.unscaled, function(x) {\n    if(!is.na(x)) {\n      x <- 1.5 * x + 18.95\n      if(x < 0) {\n        x <- 0\n      } else {\n        if(x > 15) {\n          x <- 15\n        }\n      }\n    }\n    return(x)\n  })\n  rsrisk <- ifelse(rs >= 5, 1, 0)\n\tnames(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n\treturn(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'endoPredict' function and what are its main inputs?",
        "answer": "The 'endoPredict' function computes signature scores and risk classifications for the endoPredict signature in breast cancer. Its main inputs are 'data' (a matrix of gene expressions), 'annot' (a matrix of annotations), 'do.mapping' (a boolean for gene mapping), 'mapping' (an optional matrix for forced mapping), and 'verbose' (a boolean for printing messages)."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if genes of interest are missing?",
        "answer": "If 'do.mapping' is TRUE, the function maps genes using Entrez Gene IDs. It removes missing and duplicated gene IDs from the gene list, then performs mapping using the 'geneid.map' function. If genes of interest are missing after mapping, the function returns NA values for scores and risks, along with mapping information."
      },
      {
        "question": "How is the final risk classification determined in the 'endoPredict' function?",
        "answer": "The function calculates an unscaled risk score (rs.unscaled) using gene weights and transformed expression data. This score is then scaled and bounded between 0 and 15. The final risk classification (rsrisk) is determined by comparing the scaled score (rs) to a threshold of 5. Scores >= 5 are classified as high risk (1), while scores < 5 are classified as low risk (0)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    # Complete the mapping process and data transformation\n  } else {\n    # Handle non-mapping case\n  }\n  \n  # Complete the function with score calculation and risk assessment\n}",
        "complete": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(!all(is.element(sig2[sig2[ , \"group\"] == \"GOI\", \"EntrezGene.ID\"], rr$geneid1))) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    colnames(data) <- names(gid2) <- names(gid1)\n    sig2 <- sig2[colnames(data), , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  } else {\n    myprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n    data <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  }\n  \n  colnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n  \n  if(do.mapping) {\n    data <- apply(data, 2, function(x) {\n      xx <- (x - quantile(x, probs=0.025, na.rm=TRUE)) / (quantile(x, probs=0.975, na.rm=TRUE) - quantile(x, probs=0.025, na.rm=TRUE))\n      return((xx * 8) + 6)\n    })\n    data[!is.na(data) & data < 1] <- 1\n    data[!is.na(data) & data > 15] <- 15\n  }\n  \n  data <- (data - apply(data, 1, mean, na.rm=TRUE)) + log2(500)\n  datat <- t(apply(data, 1, function(x, a, b) { return((x - b) / a) }, a=sig2[ , \"a\"], b=sig2[ , \"b\"]))\n  data <- matrix(NA, nrow=nrow(data), ncol=ncol(data), dimnames=dimnames(data))\n  data[rownames(datat), colnames(datat)] <- datat\n  \n  rs <- rs.unscaled <- rsrisk <- rep(NA, nrow(data))\n  rs.unscaled <- drop((sig2[ , \"weight\"] %*% t(data)) - 2.63)\n  rs <- sapply(rs.unscaled, function(x) {\n    if(!is.na(x)) {\n      x <- 1.5 * x + 18.95\n      if(x < 0) { x <- 0 } else if(x > 15) { x <- 15 }\n    }\n    return(x)\n  })\n  rsrisk <- ifelse(rs >= 5, 1, 0)\n  names(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    # Implement mapping logic\n  } else {\n    myprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n    data <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  }\n  \n  colnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n  \n  # Implement data transformation and score calculation\n}",
        "complete": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(!all(is.element(sig2[sig2[ , \"group\"] == \"GOI\", \"EntrezGene.ID\"], rr$geneid1))) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    colnames(data) <- names(gid2) <- names(gid1)\n    sig2 <- sig2[colnames(data), , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  } else {\n    myprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n    data <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  }\n  \n  colnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n  \n  if(do.mapping) {\n    data <- apply(data, 2, function(x) {\n      xx <- (x - quantile(x, probs=0.025, na.rm=TRUE)) / (quantile(x, probs=0.975, na.rm=TRUE) - quantile(x, probs=0.025, na.rm=TRUE))\n      return((xx * 8) + 6)\n    })\n    data[!is.na(data) & data < 1] <- 1\n    data[!is.na(data) & data > 15] <- 15\n  }\n  \n  data <- (data - apply(data, 1, mean, na.rm=TRUE)) + log2(500)\n  datat <- t(apply(data, 1, function(x, a, b) { return((x - b) / a) }, a=sig2[ , \"a\"], b=sig2[ , \"b\"]))\n  data <- matrix(NA, nrow=nrow(data), ncol=ncol(data), dimnames=dimnames(data))\n  data[rownames(datat), colnames(datat)] <- datat\n  \n  rs <- rs.unscaled <- rsrisk <- rep(NA, nrow(data))\n  rs.unscaled <- drop((sig2[ , \"weight\"] %*% t(data)) - 2.63)\n  rs <- sapply(rs.unscaled, function(x) {\n    if(!is.na(x)) {\n      x <- 1.5 * x + 18.95\n      if(x < 0) { x <- 0 } else if(x > 15) { x <- 15 }\n    }\n    return(x)\n  })\n  rsrisk <- ifelse(rs >= 5, 1, 0)\n  names(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/rename.duplicate.R",
    "language": "R",
    "content": "#' @title Function to rename duplicated strings\n#'\n#' @description\n#' This function renames duplicated strings by adding their number of \n#'   occurrences at the end.\n#'\n#' @usage\n#' rename.duplicate(x, sep = \"_\", verbose = FALSE)\n#'\n#' @param x\tvector of strings.\n#' @param sep\ta character to be the separator between the number added at \n#'   the end and the string itself.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - new.x:\tnew strings (without duplicates).\n#' - duplicated.x: strings which were originally duplicated.\n#'\n#' @examples\n#' nn <- sample(letters[1:10], 30, replace=TRUE)\n#' table(nn)\n#' rename.duplicate(x=nn, verbose=TRUE)\n#'\n#' @md\n#' @export\nrename.duplicate <-\nfunction (x, sep=\"_\", verbose=FALSE) {\n\n\tx <- as.character(x)\n\tduplix <- duplicated(x)\n\tduplin <- x[duplix]\n\n\tix <- numeric(length=length(unique(duplin)))\n\tnames(ix) <- unique(duplin)\n\tretval <- numeric(length=length(duplin))\n\tfor(i in 1:length(duplin)) { retval[i] <- ix[duplin[i]] <- ix[duplin[i]] + 1 }\n\tretval <- retval + 1\n\tx[duplix] <- paste(duplin, retval, sep=sep)\n\n\tif (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\t\n\treturn (list(new.x=x, duplicated.x=duplin))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rename.duplicate` function, and what are its main parameters?",
        "answer": "The `rename.duplicate` function renames duplicated strings in a vector by adding their number of occurrences at the end. Its main parameters are:\n- `x`: the input vector of strings\n- `sep`: the separator character between the original string and the added number (default is '_')\n- `verbose`: a boolean flag to print informative messages (default is FALSE)"
      },
      {
        "question": "How does the function handle the renaming of duplicates, and what does it return?",
        "answer": "The function renames duplicates by appending a number to the end of each duplicated string, separated by the specified separator. It returns a list with two items:\n1. `new.x`: the new vector of strings with duplicates renamed\n2. `duplicated.x`: the original duplicated strings"
      },
      {
        "question": "Explain the purpose and functionality of the nested loop in the `rename.duplicate` function.",
        "answer": "The nested loop in the function serves to count the occurrences of each duplicated string and assign the appropriate number to append. Here's how it works:\n1. It initializes `ix` as a named numeric vector to keep track of counts for each unique duplicated string.\n2. It iterates through the duplicated strings, incrementing the count for each occurrence in `ix`.\n3. It stores these counts in `retval`, which is then used to create the new names by adding 1 (to start counting from 2) and pasting with the original string."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  ix <- numeric(length=length(unique(duplin)))\n  names(ix) <- unique(duplin)\n  retval <- numeric(length=length(duplin))\n  for(i in 1:length(duplin)) {\n    # Complete the loop body\n  }\n  retval <- retval + 1\n  x[duplix] <- paste(duplin, retval, sep=sep)\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}",
        "complete": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  ix <- numeric(length=length(unique(duplin)))\n  names(ix) <- unique(duplin)\n  retval <- numeric(length=length(duplin))\n  for(i in 1:length(duplin)) {\n    retval[i] <- ix[duplin[i]] <- ix[duplin[i]] + 1\n  }\n  retval <- retval + 1\n  x[duplix] <- paste(duplin, retval, sep=sep)\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}"
      },
      {
        "partial": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  # Complete the code to handle duplicates\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}",
        "complete": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  ix <- numeric(length=length(unique(duplin)))\n  names(ix) <- unique(duplin)\n  retval <- numeric(length=length(duplin))\n  for(i in 1:length(duplin)) {\n    retval[i] <- ix[duplin[i]] <- ix[duplin[i]] + 1\n  }\n  retval <- retval + 1\n  x[duplix] <- paste(duplin, retval, sep=sep)\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/test.hetero.test.R",
    "language": "R",
    "content": "'test.hetero.test' <-\nfunction(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'test.hetero.test' function in R?",
        "answer": "The 'test.hetero.test' function performs a heterogeneity test on a set of p-values. It calculates a Q statistic and its corresponding p-value using a chi-square distribution. This test is often used in meta-analysis to assess whether the effect sizes across different studies are homogeneous or heterogeneous."
      },
      {
        "question": "How does the function handle missing values in the input vector 'p'?",
        "answer": "The function handles missing values in two ways: 1) If 'na.rm' is set to FALSE (default) and there are missing values, it stops execution with an error message. 2) If 'na.rm' is TRUE, it removes the missing values from both 'p' and 'weight' vectors using the '!is.na(p)' condition before performing calculations."
      },
      {
        "question": "What statistical transformation is applied to the p-values, and why?",
        "answer": "The p-values are transformed using the inverse normal cumulative distribution function (qnorm) with 'lower.tail=FALSE'. This transformation converts the p-values to z-scores. The resulting z-scores are then used to calculate the Q statistic, which follows a chi-square distribution under the null hypothesis of homogeneity."
      }
    ],
    "completion_tasks": [
      {
        "partial": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\t# Calculate Q and qpv here\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}",
        "complete": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}"
      },
      {
        "partial": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\t# Handle missing values and calculate z-scores here\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}",
        "complete": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/map.datasets.R",
    "language": "R",
    "content": "#' @title Function to map a list of datasets through EntrezGene IDs in order to \n#'   get the union of the genes\n#'\n#' @description\n#' This function maps a list of datasets through EntrezGene IDs in order to get \n#'   the union of the genes.\n#'\n#' @usage\n#' map.datasets(datas, annots, do.mapping = FALSE, \n#'   mapping.coln = \"EntrezGene.ID\", mapping, verbose = FALSE)\n#'\n#' @param datas\tList of matrices of gene expressions with samples in rows and \n#'   probes in columns, dimnames being properly defined.\n#' @param annots\tList of matrices of annotations with at least one column named \n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping\tTRUE if the mapping through Entrez Gene ids must be \n#'   performed (in case of ambiguities, the most variant probe is kept for each \n#'   gene), FALSE otherwise.\n#' @param mapping.coln\tName of the column containing the biological annotation \n#'   to be used to map the different datasets, default is \"EntrezGene.ID\".\n#' @param mapping\tMatrix with columns \"EntrezGene.ID\" and \"probe.x\" used to \n#'   force the mapping such that the probes of platform x are not selected based on \n#'   their variance.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' In case of several probes representing the same EntrezGene ID, the most \n#'   variant is selected if mapping is not specified. When a EntrezGene ID does not \n#'   exist in a specific dataset, NA values are introduced.\n#'\n#' @return\n#' A list with items:\n#' - datas: List of datasets (gene expression matrices)\n#' - annots: List of annotations (annotation matrices)\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # load NKI dataset\n#' data(nkis)\n#' # reduce datasets\n#' ginter <- intersect(annot.vdxs[ ,\"EntrezGene.ID\"], annot.nkis[ ,\"EntrezGene.ID\"])\n#' ginter <- ginter[!is.na(ginter)][1:30]\n#' myx <- unique(c(match(ginter, annot.vdxs[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.vdxs), size=20)))\n#' data2.vdxs <- data.vdxs[ ,myx]\n#' annot2.vdxs <- annot.vdxs[myx, ]\n#' myx <- unique(c(match(ginter, annot.nkis[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.nkis), size=20)))\n#' data2.nkis <- data.nkis[ ,myx]\n#' annot2.nkis <- annot.nkis[myx, ]\n#' # mapping of datasets\n#' datas <- list(\"VDX\"=data2.vdxs,\"NKI\"=data2.nkis)\n#' annots <- list(\"VDX\"=annot2.vdxs, \"NKI\"=annot2.nkis)\n#' datas.mapped <- map.datasets(datas=datas, annots=annots, do.mapping=TRUE)\n#' str(datas.mapped, max.level=2)\n#'\n#' @md\n#' @export\nmap.datasets <-\nfunction(datas, annots, do.mapping=FALSE, mapping.coln=\"EntrezGene.ID\", mapping, verbose=FALSE) {\n\tif((length(datas) != length(annots)) || !all(names(datas) == names(annots))) { stop(\"discordance between lists of datasets and annotations!\") }\n\t## do the mapping (or not) and collect the set of unique features\n\tdatas2 <- annots2 <- comid <- NULL\n\tfor(k in 1:length(datas)) {\n\t\tif(verbose) { message(sprintf(\"%s\", names(datas)[k])) }\n\t\tif(do.mapping) {\n\t\t\tgid <- as.character(annots[[k]][ ,mapping.coln])\n\t\t\tnames(gid) <- dimnames(annots[[k]])[[1]]\n\t\t\tugid <- unique(gid)\n\t\t\tugid <- ugid[!is.na(ugid)]\n\t\t\tnames(ugid) <- paste(\"geneid\", ugid, sep=\".\")\n\t\t\trr <- geneid.map(geneid1=gid, data1=datas[[k]], geneid2=ugid, verbose=FALSE)\n\t\t\ttt <- rr$data1\n\t\t\t## update gene ids since only missing values may be present for some of them\n\t\t\tugid <- rr$geneid2\n\t\t\tdimnames(tt)[[2]] <- names(ugid)\n\t\t\tdatas2 <- c(datas2, list(tt))\n\t\t\ttt <- annots[[k]][names(rr$geneid1), , drop=FALSE]\n\t\t\tdimnames(tt)[[1]] <- names(ugid)\n\t\t\tannots2 <- c(annots2, list(tt))\n\t\t\tcomid <- unique(c(comid, names(ugid)))\n\t\t\trm(rr)\n\t\t\tgc()\n\t\t} else {\n\t\t\tdatas2 <- c(datas2, list(datas[[k]]))\n\t\t\tannots2 <- c(annots2, list(annots[[k]]))\n\t\t\tcomid <- unique(c(comid, dimnames(datas[[k]])[[2]]))\n\t\t}\n\t}\n\tnames(datas2) <- names(annots2) <- names(datas)\n\t#comid <- sort(comid)\n\t## put NA values for missing features\n\tfor(k in 1:length(datas)) {\n\t\ttt <- matrix(NA, nrow=nrow(datas2[[k]]), ncol=length(comid), dimnames=list(dimnames(datas2[[k]])[[1]], comid))\n\t\ttt[dimnames(datas2[[k]])[[1]], dimnames(datas2[[k]])[[2]]] <- datas2[[k]]\n\t\tdatas2[[k]] <- tt\n\t\ttt <- rbind(annots2[[k]], matrix(NA, nrow=length(comid) - nrow(annots2[[k]]), ncol=ncol(annots2[[k]]), dimnames=list(comid[!is.element(comid, dimnames(annots2[[k]])[[1]])], dimnames(annots2[[k]])[[2]])))\n\t\ttt <- tt[comid, , drop=FALSE]\n\t\tannots2[[k]] <- tt\n\t}\n\treturn(list(\"datas\"=datas2, \"annots\"=annots2))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `map.datasets` function?",
        "answer": "The main purpose of the `map.datasets` function is to map a list of datasets through EntrezGene IDs to get the union of genes across multiple datasets. It handles cases where different datasets might use different probes or identifiers for the same genes, allowing for standardization and comparison across datasets."
      },
      {
        "question": "How does the function handle cases where multiple probes represent the same EntrezGene ID?",
        "answer": "When multiple probes represent the same EntrezGene ID and the `do.mapping` parameter is set to TRUE, the function selects the most variant probe for each gene. This is done using the `geneid.map` function, which is called within the main loop of `map.datasets`. However, if a `mapping` parameter is provided, it can be used to force specific probe selections instead of using variance-based selection."
      },
      {
        "question": "What does the function return, and how does it handle missing data across datasets?",
        "answer": "The function returns a list with two items: 'datas' (a list of gene expression matrices) and 'annots' (a list of annotation matrices). For genes that exist in some datasets but not others, the function introduces NA values in the datasets where the gene is missing. This ensures that all returned datasets have the same set of genes (union of all datasets), allowing for easier comparison and analysis across different studies or platforms."
      }
    ],
    "completion_tasks": [
      {
        "partial": "map.datasets <- function(datas, annots, do.mapping=FALSE, mapping.coln=\"EntrezGene.ID\", mapping, verbose=FALSE) {\n  if((length(datas) != length(annots)) || !all(names(datas) == names(annots))) { stop(\"discordance between lists of datasets and annotations!\") }\n  datas2 <- annots2 <- comid <- NULL\n  for(k in 1:length(datas)) {\n    if(verbose) { message(sprintf(\"%s\", names(datas)[k])) }\n    if(do.mapping) {\n      gid <- as.character(annots[[k]][ ,mapping.coln])\n      names(gid) <- dimnames(annots[[k]])[[1]]\n      ugid <- unique(gid)\n      ugid <- ugid[!is.na(ugid)]\n      names(ugid) <- paste(\"geneid\", ugid, sep=\".\")\n      # Complete the mapping process here\n    } else {\n      datas2 <- c(datas2, list(datas[[k]]))\n      annots2 <- c(annots2, list(annots[[k]]))\n      comid <- unique(c(comid, dimnames(datas[[k]])[[2]]))\n    }\n  }\n  # Complete the function here\n}",
        "complete": "map.datasets <- function(datas, annots, do.mapping=FALSE, mapping.coln=\"EntrezGene.ID\", mapping, verbose=FALSE) {\n  if((length(datas) != length(annots)) || !all(names(datas) == names(annots))) { stop(\"discordance between lists of datasets and annotations!\") }\n  datas2 <- annots2 <- comid <- NULL\n  for(k in 1:length(datas)) {\n    if(verbose) { message(sprintf(\"%s\", names(datas)[k])) }\n    if(do.mapping) {\n      gid <- as.character(annots[[k]][ ,mapping.coln])\n      names(gid) <- dimnames(annots[[k]])[[1]]\n      ugid <- unique(gid)\n      ugid <- ugid[!is.na(ugid)]\n      names(ugid) <- paste(\"geneid\", ugid, sep=\".\")\n      rr <- geneid.map(geneid1=gid, data1=datas[[k]], geneid2=ugid, verbose=FALSE)\n      tt <- rr$data1\n      ugid <- rr$geneid2\n      dimnames(tt)[[2]] <- names(ugid)\n      datas2 <- c(datas2, list(tt))\n      tt <- annots[[k]][names(rr$geneid1), , drop=FALSE]\n      dimnames(tt)[[1]] <- names(ugid)\n      annots2 <- c(annots2, list(tt))\n      comid <- unique(c(comid, names(ugid)))\n      rm(rr)\n      gc()\n    } else {\n      datas2 <- c(datas2, list(datas[[k]]))\n      annots2 <- c(annots2, list(annots[[k]]))\n      comid <- unique(c(comid, dimnames(datas[[k]])[[2]]))\n    }\n  }\n  names(datas2) <- names(annots2) <- names(datas)\n  for(k in 1:length(datas)) {\n    tt <- matrix(NA, nrow=nrow(datas2[[k]]), ncol=length(comid), dimnames=list(dimnames(datas2[[k]])[[1]], comid))\n    tt[dimnames(datas2[[k]])[[1]], dimnames(datas2[[k]])[[2]]] <- datas2[[k]]\n    datas2[[k]] <- tt\n    tt <- rbind(annots2[[k]], matrix(NA, nrow=length(comid) - nrow(annots2[[k]]), ncol=ncol(annots2[[k]]), dimnames=list(comid[!is.element(comid, dimnames(annots2[[k]])[[1]])], dimnames(annots2[[k]])[[2]])))\n    tt <- tt[comid, , drop=FALSE]\n    annots2[[k]] <- tt\n  }\n  return(list(\"datas\"=datas2, \"annots\"=annots2))\n}"
      },
      {
        "partial": "geneid.map <- function(geneid1, data1, geneid2, verbose=FALSE) {\n  # Implement the geneid.map function here\n  # This function should map gene IDs and handle data accordingly\n  # Return a list with mapped data and gene IDs\n}",
        "complete": "geneid.map <- function(geneid1, data1, geneid2, verbose=FALSE) {\n  if(verbose) { message(\"mapping gene ids...\") }\n  ii <- match(geneid2, geneid1)\n  if(any(is.na(ii))) {\n    geneid2 <- geneid2[!is.na(ii)]\n    ii <- ii[!is.na(ii)]\n  }\n  data2 <- data1[, ii, drop=FALSE]\n  dimnames(data2)[[2]] <- names(geneid2)\n  if(verbose) { message(sprintf(\"\\t%d/%d features\", ncol(data2), length(geneid1))) }\n  return(list(data1=data2, geneid1=geneid1[ii], geneid2=geneid2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/molecular.subtyping.R",
    "language": "R",
    "content": "if(getRversion() >= \"2.15.1\")\n    utils::globalVariables(c(\"scmgene.robust\",\"scmod2.robust\",\"pam50.robust\",\n                             \"ssp2006.robust\",\"ssp2003.robust\",\"claudinLowData\"))\n\n#' @title Function to identify breast cancer molecular subtypes using\n#'   the Subtype Clustering Model\n#'\n#' @description\n#' This function identifies the breast cancer molecular subtypes using a Subtype\n#'   Clustering Model fitted by subtype.cluster.\n#'\n#' @usage\n#' molecular.subtyping(sbt.model = c(\"scmgene\", \"scmod1\", \"scmod2\",\n#'   \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"),\n#'   data, annot, do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param sbt.model\tSubtyping classification model, can be either \"scmgene\", \"scmod1\",\n#'   \"scmod2\", \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\", or \"claudinLow\".\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\"\n#'   (for ssp, scm, AIMS, and claudinLow models) or \"Gene.Symbol\" (for the intClust\n#'   model), dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed\n#'   (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param verbose TRUE if informative messages should be displayed, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - subtype: Subtypes identified by the subtyping classification model.\n#' - subtype.proba: Probabilities to belong to each subtype estimated by the\n#'   subtyping classification model.\n#' - subtype.crisp: Crisp classes identified by the subtyping classification model.\n#'\n#' @references\n#' T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and A.\n#'   Nobel and S. Deng and H. Johnsen and R. Pesich and S. Geister and J. Demeter and\n#'   C. Perou and P. E. Lonning and P. O. Brown and A. L. Borresen-Dale and D. Botstein\n#'   (2003) \"Repeated Observation of Breast Tumor Subtypes in Independent Gene\n#'   Expression Data Sets\", Proceedings of the National Academy of Sciences,\n#'   1(14):8418-8423\n#' Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and\n#'   Qaqish, Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and\n#'   Dressler, Lynn and Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer,\n#'   Lynda and Wu, Junyuan and Liu, Yudong and Nanda, Rita and Tretiakova, Maria and\n#'   Orrico, Alejandra and Dreher, Donna and Palazzo, Juan and Perreard, Laurent and\n#'   Nelson, Edward and Mone, Mary and Hansen, Heidi and Mullins, Michael and\n#'   Quackenbush, John and Ellis, Matthew and Olopade, Olufunmilayo and Bernard,\n#'   Philip and Perou, Charles (2006) \"The molecular portraits of breast tumors are\n#'   conserved across microarray platforms\", BMC Genomics, 7(96)\n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, Samuel and\n#'   Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, Christiane and He,\n#'   Xiaping and Hu, Zhiyuan and Quackenbush, John F. and Stijleman, Inge J. and Palazzo,\n#'   Juan and Marron, J.S. and Nobel, Andrew B. and Mardis, Elaine and Nielsen, Torsten O.\n#'   and Ellis, Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009)\n#'   \"Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\",\n#'   Journal of Clinical Oncology, 27(8):1160-1167\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi\n#'   M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast\n#'   cancer clinical outcome depend on the molecular subtypes\", Clinical Cancer\n#'   Research, 14(16):5158-5165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt\n#'   C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M\n#'   (2008) \"Meta-analysis of Gene-Expression Profiles in Breast Cancer: Toward a\n#'   Unified Understanding of Breast Cancer Sub-typing and Prognosis Signatures\",\n#'   Breast Cancer Research, 10(4):R65.\n#' Haibe-Kains B, Desmedt C, Loi S, Culhane AC, Bontempi G, Quackenbush J, Sotiriou\n#'   C. (2012) \"A three-gene model to robustly identify breast cancer molecular\n#'   subtypes.\", J Natl Cancer Inst., 104(4):311-325.\n#' Curtis C, Shah SP, Chin SF, Turashvili G, Rueda OM, Dunning MJ, Speed D, Lynch AG,\n#'   Samarajiwa S, Yuan Y, Graf S, Ha G, Haffari G, Bashashati A, Russell R, McKinney\n#'   S; METABRIC Group, Langerod A, Green A, Provenzano E, Wishart G, Pinder S, Watson\n#'   P, Markowetz F, Murphy L, Ellis I, Purushotham A, Borresen-Dale AL, Brenton JD,\n#'   Tavare S, Caldas C, Aparicio S. (2012) \"The genomic and transcriptomic\n#'   architecture of 2,000 breast tumours reveals novel subgroups.\", Nature,\n#'   486(7403):346-352.\n#' \n#' Paquet ER, Hallett MT. (2015) \"Absolute assignment of breast cancer intrinsic\n#'   molecular subtype.\", J Natl Cancer Inst., 107(1):357.\n#' \n#' Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I\n#'   Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular\n#'   characterization of the claudin-low intrinsic subtype of breast cancer\", Breast\n#'   Cancer Research, 12(5):R68\n#'\n#' @seealso\n#' [genefu::subtype.cluster.predict], [genefu::intrinsic.cluster.predict]\n#'\n#' @examples\n#' ##### without mapping (affy hgu133a or plus2 only)\n#' # load VDX data\n#' data(vdxs)\n#' data(AIMSmodel)\n#' data(scmgene.robust)\n#'\n#' # Subtype Clustering Model fitted on EXPO and applied on VDX\n#' sbt.vdx.SCMGENE <- molecular.subtyping(sbt.model=\"scmgene\",\n#'   data=data.vdxs, annot=annot.vdxs, do.mapping=FALSE)\n#' table(sbt.vdx.SCMGENE$subtype)\n#'\n#' # Using the AIMS molecular subtyping algorithm\n#' sbt.vdxs.AIMS <- molecular.subtyping(sbt.model=\"AIMS\", data=data.vdxs,\n#'                                      annot=annot.vdxs, do.mapping=FALSE)\n#' table(sbt.vdxs.AIMS$subtype)\n#'\n#' # Using the IntClust molecular subtyping algorithm\n#' colnames(annot.vdxs)[3]<-\"Gene.Symbol\"\n#' sbt.vdxs.intClust <- molecular.subtyping(sbt.model=\"intClust\", data=data.vdxs,\n#'   annot=annot.vdxs, do.mapping=FALSE)\n#' table(sbt.vdxs.intClust$subtype)\n#'\n#' ##### with mapping\n#' # load NKI data\n#' data(nkis)\n#'\n#' # Subtype Clustering Model fitted on EXPO and applied on NKI\n#' sbt.nkis <- molecular.subtyping(sbt.model=\"scmgene\", data=data.nkis,\n#'   annot=annot.nkis, do.mapping=TRUE)\n#' table(sbt.nkis$subtype)\n#'\n#' ##### with mapping\n#' ## load vdxs data\n#' data(vdxs)\n#' data(claudinLowData)\n#'\n#' ## Claudin-Low classification of 150 VDXS samples\n#' sbt.vdxs.CL <- molecular.subtyping(sbt.model=\"claudinLow\", data=data.vdxs,\n#'   annot=annot.vdxs, do.mapping=TRUE)\n#' table(sbt.vdxs.CL$subtype)\n#'\n#' @md\n#' @export\nmolecular.subtyping <- function(sbt.model=c(\"scmgene\", \"scmod1\", \"scmod2\",\n  \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"), data, annot,\n  do.mapping=FALSE, verbose=FALSE)\n{\n\n  sbt.model <- match.arg(sbt.model)\n\n  ## convert SCM to SSP nomenclature\n  sbt.conv <- rbind(c(\"ER-/HER2-\", \"Basal\"),\n    c(\"HER2+\", \"Her2\"),\n    c(\"ER+/HER2- High Prolif\", \"LumB\"),\n    c(\"ER+/HER2- Low Prolif\", \"LumA\")\n  )\n  colnames(sbt.conv) <- c(\"SCM.nomenclature\", \"SSP.nomenclature\")\n\n  sbtn.ssp <- c(\"Basal\", \"Her2\", \"LumB\", \"LumA\", \"Normal\")\n  sbtn2.ssp <- c(\"Basal\", \"Her2\", \"Lums\", \"LumB\", \"LumA\", \"Normal\")\n\n  ## SCM family\n  if (sbt.model %in% c(\"scmgene\", \"scmod1\", \"scmod2\")) {\n    switch(sbt.model,\n      \"scmgene\" = {\n        sbts <- subtype.cluster.predict(sbt.model=scmgene.robust, data=data,\n          annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n      },\n      \"scmod1\" = {\n        sbts <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data,\n          annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n      },\n      \"scmod2\" = {\n        sbts <- subtype.cluster.predict(sbt.model=scmod2.robust, data=data,\n          annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n      }\n    )\n    names(sbts) <- c(\"subtype\", \"subtype.proba\")\n    ## compute crisp classification\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n\n    ## reorder columns\n    #ss <- sbtn2.ssp[is.element(sbtn2.ssp, colnames(sbts$subtype.proba))]\n    #sbts$subtype.proba <- sbts$subtype.proba[ , ss, drop=FALSE]\n    #sbts$subtype.crisp <- sbts$subtype.crisp[ , ss, drop=FALSE]\n\n    ## set the proper names\n    names(sbts$subtype) <- rownames(sbts$subtype.proba) <- rownames(sbts$subtype.crisp)<- rownames(data)\n  }\n\n  ## SSP family\n  if (sbt.model %in% c(\"ssp2003\", \"ssp2006\", \"pam50\")) {\n    switch(sbt.model,\n      \"pam50\" = {\n        sbts <- intrinsic.cluster.predict(sbt.model=pam50.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n      },\n      \"ssp2006\" = {\n        sbts <- intrinsic.cluster.predict(sbt.model=ssp2006.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n      },\n      \"ssp2003\" = {\n        sbts <- intrinsic.cluster.predict(sbt.model=ssp2003.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n      }\n    )\n    sbts$subtype <- factor(as.character(sbts$subtype), levels=sbtn.ssp)\n    ## compute crisp classification\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n\n    ## merge LumA and LumB: #sum the probability for LumA and LumB to get the probability for Luminals in general\n    #lums.proba <- apply(sbts$subtype.proba[ , c(\"LumB\", \"LumA\"), drop=FALSE], 1, sum, na.rm=TRUE)\n    #sbts$subtype.proba <- cbind(sbts$subtype.proba, \"Lums\"=lums.proba)\n    #lums.crisp <- as.numeric(is.element(sbts$subtype, c(\"LumA\", \"LumB\")))\n    #sbts$subtype.crisp <- cbind(sbts$subtype.crisp, \"Lums\"=lums.crisp)\n\n    ## reorder columns\n    #ss <- sbtn2.ssp[is.element(sbtn2.ssp, colnames(sbts$subtype.proba))]\n    #sbts$subtype.proba <- sbts$subtype.proba[ , ss, drop=FALSE]\n    #sbts$subtype.crisp <- sbts$subtype.crisp[ , ss, drop=FALSE]\n\n    ## set the proper names\n    names(sbts$subtype) <- rownames(sbts$subtype.proba) <- rownames(sbts$subtype.crisp)<- rownames(data)\n  }\n\n  ## IntClust family\n  if (sbt.model %in% c(\"intClust\")) {\n    #message(\"Note: Need a Gene.Symbol column in the annotation object\")\n    sbts<-NULL\n    myx <- !is.na(annot[ , \"Gene.Symbol\"]) & !duplicated(annot[ , \"Gene.Symbol\"])\n    dd <- t(data[ , myx, drop=FALSE])\n    rownames(dd) <- annot[myx, \"Gene.Symbol\"]\n    ## remove patients with more than 80% missing values\n    rix <- apply(dd, 2, function (x, y) { return ((sum(is.na(x) / length(x))) > y) }, y=0.8)\n    cix <- apply(dd, 2, function (x, y) { return ((sum(is.na(x) / length(x))) > y) }, y=0.8)\n    dd <- dd[!rix, !cix, drop=FALSE]\n    features <- iC10::matchFeatures(Exp=dd, Exp.by.feat=\"Gene.Symbol\")\n    features <- iC10::normalizeFeatures(features, method=\"scale\")\n    res <- iC10::iC10(features)\n    ## compute crisp classification\n    crisp <- t(apply(res$posterior, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n    sbts$subtype <- array(NA, dim=nrow(data), dimnames=list(rownames(data)))\n    sbts$subtype[!rix] <- res$class\n    sbts$subtype.proba <- array(NA, dim=c(nrow(data), ncol(res$posterior)), dimnames=list(rownames(data), colnames(res$posterior)))\n    sbts$subtype.proba[!rix, ] <- res$posterior\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n    ## set the proper colnames\n    colnames(sbts$subtype.proba) <- colnames(sbts$subtype.crisp) <- paste(\"iC\", colnames(sbts$subtype.proba), sep=\"\")\n    sbts$subtype <- paste(\"iC\", sbts$subtype, sep=\"\")\n    sbts$subtype <- factor(sbts$subtype, levels=colnames(sbts$subtype.proba))\n    ## set the proper rownames\n    names(sbts$subtype) <- rownames(sbts$subtype.proba) <- rownames(sbts$subtype.crisp)<- rownames(data)\n  }\n\n  ## AIMS classifier\n  if (sbt.model %in% c(\"AIMS\")) {\n      sbts <- AIMS::applyAIMS(eset=t(data), EntrezID=annot[ , \"EntrezGene.ID\"])[c(\"cl\", \"all.probs\")]\n      sbts$subtype <- sbts$cl\n      sbts$subtype.proba <- matrix(unlist(sbts$all.probs$`20`), ncol = 5, byrow = TRUE)\n      colnames(sbts$subtype.proba) <- colnames(sbts$all.probs$`20`)\n      rownames(sbts$subtype.proba) <- rownames(sbts$subtype)\n\n      ## compute crisp classification\n      sbts$subtype.crisp <- t(\n        apply(sbts$subtype.proba, 1, function (x) {\n        xx <- array(0, dim=length(x), dimnames=list(names(x)))\n        xx[which.max(x)] <- 1\n        return (xx)\n      })\n      )\n      sbts<-sbts[- which(names(sbts) %in% c(\"cl\",\"all.probs\"))]\n  }\n\n  ## CLAUDIN-LOW classifier\n  if (sbt.model %in% c(\"claudinLow\")) {\n    train<-claudinLowData\n    train$xd<- medianCtr(train$xd)\n\n    if(do.mapping) {\n      gid1 <- as.numeric(rownames(train$xd))\n      names(gid1) <- paste(\"geneid\", rownames(train$xd), sep=\".\")\n      gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n      names(gid2) <- colnames(data)\n\n      ## remove missing and duplicated geneids from the gene list\n      rm.ix <- is.na(gid1) | duplicated(gid1)\n      gid1 <- gid1[!rm.ix]\n      rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n      gt <- length(rr$geneid2)\n      if(is.na(rr$geneid1[1])) {\n        gm <- 0\n        #no gene ids in common\n        res <- rep(NA, nrow(data))\n        names(res) <- dimnames(data)[[1]]\n        gf <- c(\"mapped\"=0, \"total\"=gt)\n        if(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n        return(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n      }\n      gid1 <- rr$geneid2\n      gid2 <- rr$geneid1\n      data <- rr$data1\n      #mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n      myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n      ## change the names of probes in the data\n      dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n    }\n\n    test <- medianCtr(t(data)) #probes as rows, median-centered\n    #Run Classifier Call\n\ttrain2 <- train$xd\n\trownames(train2) <- paste(\"geneid\", rownames(train2), sep=\".\")\n    predout <- claudinLow(x=train2, classes=as.matrix(train$classes$Group,ncol=1), y=test)\n    sbts <- NULL\n    sbts$subtype <- factor(as.character(predout$predictions$Call))\n    colnames(predout$centroids) <- c(\"Claudin\",\"Others\")\n\n    ## apply the nearest centroid classifier to classify the samples again\n    ncor <- t(apply(X=data, MARGIN=1, FUN=function(x, y) {\n      rr <- array(NA, dim=ncol(y), dimnames=list(colnames(y)))\n      if (sum(complete.cases(x, y)) > 3) {\n        rr <- cor(x=x, y=y, method=\"spearman\", use=\"complete.obs\")\n      }\n      return (rr)\n    }, y=predout$centroids))\n\n    #Calculate posterior probability based on the correlationss\n   # nproba <- t(apply(X=ncor, MARGIN=1, FUN=function(x) { return(abs(x) / sum(abs(x), na.rm=TRUE)) }))\n\n    # negative correlations are truncated to zero since they have no meaning for subtypes identification\n    nproba <- t(apply(X=ncor, MARGIN=1, FUN=function (x) {\n      rr <- array(NA, dim=length(x), dimnames=list(names(x)))\n      x[!is.na(x) & x < 0] <- 0\n      if (!all(is.na(x))) {\n        rr <- x / sum(x, na.rm=TRUE)\n      }\n      return (rr)\n    }))\n\n    sbts$subtype.proba<-nproba\n\n    ## compute crisp classification - in this case, really based on the binary call from the CL classifier\n    #     sbts$subtype.crisp <- t(\n    #       apply(sbts$subtype.proba, 1, function (x) {\n    #         xx <- array(0, dim=length(x), dimnames=list(names(x)))\n    #         xx[which.max(x)] <- 1\n    #         return (xx)\n    #       })\n    #     )\n    #     colnames(sbts$subtype.crisp)<-c(\"Claudin\",\"Others\")\n\n    # In this case, really based on the binary call from the CL classifier. Use that for accuracy\n      CLsubtypes<-c(\"Claudin\",\"Others\")\n      sbts$subtype.crisp <- matrix(0, nrow=nrow(predout$predictions), ncol=2,dimnames=list(rownames(predout$predictions),CLsubtypes))\n      for(count in 1:nrow(predout$predictions))\n      {\n        if(predout$predictions$Call[count]==\"Others\")\n          sbts$subtype.crisp[count,2]<-1\n        else sbts$subtype.crisp[count,1]<-1\n      }\n  }\n  return (sbts)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `molecular.subtyping` function and what are its main input parameters?",
        "answer": "The `molecular.subtyping` function is used to identify breast cancer molecular subtypes using various subtyping classification models. Its main input parameters are: `sbt.model` (the subtyping classification model to use), `data` (a matrix of gene expressions), `annot` (a matrix of annotations), `do.mapping` (boolean indicating whether to perform mapping through Entrez Gene ids), and `verbose` (boolean for displaying informative messages)."
      },
      {
        "question": "How does the function handle different subtyping models, and what are some of the available options?",
        "answer": "The function uses a switch statement to handle different subtyping models. Available options include 'scmgene', 'scmod1', 'scmod2' (SCM family), 'pam50', 'ssp2006', 'ssp2003' (SSP family), 'intClust', 'AIMS', and 'claudinLow'. Each model uses a specific prediction method and may require different data processing steps."
      },
      {
        "question": "What are the main components of the function's return value, and what do they represent?",
        "answer": "The function returns a list with three main components: 'subtype' (identified subtypes), 'subtype.proba' (probabilities of belonging to each subtype), and 'subtype.crisp' (crisp classes identified by the model). These represent the classification results, providing both the final subtype assignment and the underlying probabilities for each sample."
      }
    ],
    "completion_tasks": [
      {
        "partial": "molecular.subtyping <- function(sbt.model=c(\"scmgene\", \"scmod1\", \"scmod2\", \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"), data, annot, do.mapping=FALSE, verbose=FALSE) {\n  sbt.model <- match.arg(sbt.model)\n\n  if (sbt.model %in% c(\"scmgene\", \"scmod1\", \"scmod2\")) {\n    # SCM family implementation\n  } else if (sbt.model %in% c(\"ssp2003\", \"ssp2006\", \"pam50\")) {\n    # SSP family implementation\n  } else if (sbt.model == \"intClust\") {\n    # IntClust implementation\n  } else if (sbt.model == \"AIMS\") {\n    # AIMS implementation\n  } else if (sbt.model == \"claudinLow\") {\n    # CLAUDIN-LOW implementation\n  }\n\n  # Return results\n}",
        "complete": "molecular.subtyping <- function(sbt.model=c(\"scmgene\", \"scmod1\", \"scmod2\", \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"), data, annot, do.mapping=FALSE, verbose=FALSE) {\n  sbt.model <- match.arg(sbt.model)\n\n  if (sbt.model %in% c(\"scmgene\", \"scmod1\", \"scmod2\")) {\n    sbts <- switch(sbt.model,\n      \"scmgene\" = subtype.cluster.predict(sbt.model=scmgene.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")],\n      \"scmod1\" = subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")],\n      \"scmod2\" = subtype.cluster.predict(sbt.model=scmod2.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n    )\n    names(sbts) <- c(\"subtype\", \"subtype.proba\")\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function(x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return(xx)\n    }))\n  } else if (sbt.model %in% c(\"ssp2003\", \"ssp2006\", \"pam50\")) {\n    sbts <- switch(sbt.model,\n      \"pam50\" = intrinsic.cluster.predict(sbt.model=pam50.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")],\n      \"ssp2006\" = intrinsic.cluster.predict(sbt.model=ssp2006.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")],\n      \"ssp2003\" = intrinsic.cluster.predict(sbt.model=ssp2003.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n    )\n    sbts$subtype <- factor(as.character(sbts$subtype), levels=c(\"Basal\", \"Her2\", \"LumB\", \"LumA\", \"Normal\"))\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function(x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return(xx)\n    }))\n  } else if (sbt.model == \"intClust\") {\n    # IntClust implementation (omitted for brevity)\n  } else if (sbt.model == \"AIMS\") {\n    sbts <- AIMS::applyAIMS(eset=t(data), EntrezID=annot[, \"EntrezGene.ID\"])[c(\"cl\", \"all.probs\")]\n    sbts$subtype <- sbts$cl\n    sbts$subtype.proba <- matrix(unlist(sbts$all.probs$`20`), ncol = 5, byrow = TRUE)\n    colnames(sbts$subtype.proba) <- colnames(sbts$all.probs$`20`)\n    rownames(sbts$subtype.proba) <- rownames(sbts$subtype)\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function(x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return(xx)\n    }))\n    sbts <- sbts[- which(names(sbts) %in% c(\"cl\",\"all.probs\"))]\n  } else if (sbt.model == \"claudinLow\") {\n    # CLAUDIN-LOW implementation (omitted for brevity)\n  }\n\n  return(sbts)\n}"
      },
      {
        "partial": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE) {\n  # Function implementation\n}",
        "complete": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE) {\n  if (do.mapping) {\n    mapped_data <- map_data(data, annot)\n  } else {\n    mapped_data <- data\n  }\n  \n  subtype_probabilities <- predict(sbt.model, newdata=mapped_data)\n  subtypes <- apply(subtype_probabilities, 1, which.max)\n  subtype_names <- colnames(subtype_probabilities)[subtypes]\n  \n  return(list(\n    subtype2 = factor(subtype_names, levels=colnames(subtype_probabilities)),\n    subtype.proba2 = subtype_probabilities\n  ))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/td.sens.spec.R",
    "language": "R",
    "content": "'td.sens.spec' <-\nfunction(cl, surv.time, surv.event, time, span=0, sampling=FALSE, na.rm=FALSE, ...) {\n\t#require(survivalROC)\n\t\n\tif((length(cl) + length(surv.time) + length(surv.event)) != (3 * length(cl))) { stop(\"paramaters cl, surv.time and surv.event must have the same length!\") }\n\tif(is.null(names(cl))) { names(cl) <- names(surv.time) <- names(surv.event) <- paste(\"X\", 1:length(cl), sep=\".\") }\n\tcc.ix <- complete.cases(cl, surv.time, surv.event)\n\tif(all(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tcl2 <- cl[cc.ix]\n\tucl2 <- sort(unique(cl2))\n\tif(length(ucl2) != 2) { stop(\"cl must be binary!\") }\n\too <- order(cl2, decreasing=FALSE)\n\tcl2 <- cl2[oo]\n\tst <- surv.time[cc.ix][oo]\n\tse <- surv.event[cc.ix][oo]\n\tmarker.fake <- 1:length(cl2)\n\tnames(marker.fake) <- names(cl2)\n\tmycutoff <- sum(cl2 == ucl2[1])\n\t\n\t##using the survival.C function\n\trr <- survivalROC::survivalROC.C(Stime=st, status=se, marker=marker.fake, predict.time=time, span=span,  ...)\n\t#rr <- survivalROC::survivalROC(Stime=st, status=se, marker=marker.fake, cut.values=mycutoff, predict.time=time, span=span, lambda=lambda, ...)\n\n\tsens.se <- spec.se <- NA\n\tif(sampling) {\n\t\t#require(bootstrap)\n\t\t\n\t\ttheta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\t\t\tcl <- cl[x]\n\t\t\too <- order(cl, decreasing=FALSE)\n\t\t\tcl <- cl[oo]\n\t\t\tsurv.time <- surv.time[x][oo]\n\t\t\tsurv.event <- surv.event[x][oo]\n\t\t\tucl <- sort(unique(cl))\n\t\t\tmarker.fake <- 1:length(cl)\n\t\t\tnames(marker.fake) <- names(cl)\n\t\t\tmycutoff <- sum(cl == ucl[1])\n\t\t\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake,  predict.time=time, span=span, ...)\n\t\t\t#rr <- survivalROC::survivalROC(Stime=surv.time, status=surv.event, marker=marker.fake, cut.values=mycutoff, predict.time=time, span=span, lambda=lambda, ...)\n\t\t\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)]))\n\t\t}\n\t\tmyx <- 1:length(cl2)\n\t\tmyx2 <- myx\n\t\tsens.values <- spec.values <- rep(NA, length(cl2))\n\t\tnames(sens.values) <- names(spec.values) <- names(cl2)\n\t\tfor(i in length(myx2):1) {\n\t\t\ttt <- theta.foo1(x=myx[-myx2[i]], cl=cl2, surv.time=st, surv.event=se, time=time, ...)\n\t\t\tsens.values[is.na(sens.values) & myx >= myx2[i]] <- tt$sens\n\t\t\tspec.values[is.na(spec.values) & myx >= myx2[i]] <- tt$spec\n\t\t}\n\t\t\n\t\ttheta.foo2 <- function(x, stat=c(\"sens\", \"spec\"), xdata) {\n\t\t\tstat <- match.arg(stat)\n\t\t\txdata <- unlist(xdata[stat])\n\t\t\treturn(xdata[-x])\t\n\t\t}\n\t\t\n\t\tsens.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"sens\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\n\t\tspec.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"spec\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\t\n\t}\n\t\n\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"sens.se\"=sens.se, \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)], \"spec.se\"=spec.se))\t\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'td.sens.spec' function and what are its main input parameters?",
        "answer": "The 'td.sens.spec' function calculates sensitivity and specificity for time-dependent ROC analysis in survival data. Its main input parameters are: 'cl' (a binary classifier), 'surv.time' (survival times), 'surv.event' (event indicators), and 'time' (the time point for ROC analysis). It also has optional parameters like 'span' for smoothing, 'sampling' for bootstrap estimation, and 'na.rm' for handling missing values."
      },
      {
        "question": "How does the function handle missing values and ensure data consistency?",
        "answer": "The function handles missing values and ensures data consistency by: 1) Checking if all input vectors have the same length. 2) Using 'complete.cases()' to identify and remove rows with missing values. 3) Stopping execution if all cases are incomplete and 'na.rm' is FALSE. 4) Verifying that the classifier 'cl' is binary. These checks help maintain data integrity and prevent errors in subsequent calculations."
      },
      {
        "question": "What is the purpose of the 'sampling' parameter, and how does it affect the function's output?",
        "answer": "The 'sampling' parameter enables bootstrap estimation of standard errors for sensitivity and specificity. When set to TRUE, the function uses jackknife resampling to calculate standard errors (sens.se and spec.se). This provides additional information about the variability of the estimates. The output list includes these standard errors when sampling is enabled, allowing for more comprehensive statistical inference."
      }
    ],
    "completion_tasks": [
      {
        "partial": "function(cl, surv.time, surv.event, time, span=0, sampling=FALSE, na.rm=FALSE, ...) {\n\tif((length(cl) + length(surv.time) + length(surv.event)) != (3 * length(cl))) { stop(\"paramaters cl, surv.time and surv.event must have the same length!\") }\n\tif(is.null(names(cl))) { names(cl) <- names(surv.time) <- names(surv.event) <- paste(\"X\", 1:length(cl), sep=\".\") }\n\tcc.ix <- complete.cases(cl, surv.time, surv.event)\n\tif(all(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tcl2 <- cl[cc.ix]\n\tucl2 <- sort(unique(cl2))\n\tif(length(ucl2) != 2) { stop(\"cl must be binary!\") }\n\too <- order(cl2, decreasing=FALSE)\n\tcl2 <- cl2[oo]\n\tst <- surv.time[cc.ix][oo]\n\tse <- surv.event[cc.ix][oo]\n\tmarker.fake <- 1:length(cl2)\n\tnames(marker.fake) <- names(cl2)\n\tmycutoff <- sum(cl2 == ucl2[1])\n\n\trr <- survivalROC::survivalROC.C(Stime=st, status=se, marker=marker.fake, predict.time=time, span=span, ...)\n\n\t# Complete the function by adding code for sampling and return statement\n}",
        "complete": "'td.sens.spec' <-\nfunction(cl, surv.time, surv.event, time, span=0, sampling=FALSE, na.rm=FALSE, ...) {\n\tif((length(cl) + length(surv.time) + length(surv.event)) != (3 * length(cl))) { stop(\"paramaters cl, surv.time and surv.event must have the same length!\") }\n\tif(is.null(names(cl))) { names(cl) <- names(surv.time) <- names(surv.event) <- paste(\"X\", 1:length(cl), sep=\".\") }\n\tcc.ix <- complete.cases(cl, surv.time, surv.event)\n\tif(all(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tcl2 <- cl[cc.ix]\n\tucl2 <- sort(unique(cl2))\n\tif(length(ucl2) != 2) { stop(\"cl must be binary!\") }\n\too <- order(cl2, decreasing=FALSE)\n\tcl2 <- cl2[oo]\n\tst <- surv.time[cc.ix][oo]\n\tse <- surv.event[cc.ix][oo]\n\tmarker.fake <- 1:length(cl2)\n\tnames(marker.fake) <- names(cl2)\n\tmycutoff <- sum(cl2 == ucl2[1])\n\n\trr <- survivalROC::survivalROC.C(Stime=st, status=se, marker=marker.fake, predict.time=time, span=span, ...)\n\n\tsens.se <- spec.se <- NA\n\tif(sampling) {\n\t\ttheta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\t\t\tcl <- cl[x]\n\t\t\too <- order(cl, decreasing=FALSE)\n\t\t\tcl <- cl[oo]\n\t\t\tsurv.time <- surv.time[x][oo]\n\t\t\tsurv.event <- surv.event[x][oo]\n\t\t\tucl <- sort(unique(cl))\n\t\t\tmarker.fake <- 1:length(cl)\n\t\t\tnames(marker.fake) <- names(cl)\n\t\t\tmycutoff <- sum(cl == ucl[1])\n\t\t\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake, predict.time=time, span=span, ...)\n\t\t\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)]))\n\t\t}\n\t\tmyx <- 1:length(cl2)\n\t\tmyx2 <- myx\n\t\tsens.values <- spec.values <- rep(NA, length(cl2))\n\t\tnames(sens.values) <- names(spec.values) <- names(cl2)\n\t\tfor(i in length(myx2):1) {\n\t\t\ttt <- theta.foo1(x=myx[-myx2[i]], cl=cl2, surv.time=st, surv.event=se, time=time, ...)\n\t\t\tsens.values[is.na(sens.values) & myx >= myx2[i]] <- tt$sens\n\t\t\tspec.values[is.na(spec.values) & myx >= myx2[i]] <- tt$spec\n\t\t}\n\t\t\n\t\ttheta.foo2 <- function(x, stat=c(\"sens\", \"spec\"), xdata) {\n\t\t\tstat <- match.arg(stat)\n\t\t\txdata <- unlist(xdata[stat])\n\t\t\treturn(xdata[-x])\t\n\t\t}\n\t\t\n\t\tsens.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"sens\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\n\t\tspec.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"spec\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\t\n\t}\n\t\n\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"sens.se\"=sens.se, \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)], \"spec.se\"=spec.se))\t\n}"
      },
      {
        "partial": "theta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\tcl <- cl[x]\n\too <- order(cl, decreasing=FALSE)\n\tcl <- cl[oo]\n\tsurv.time <- surv.time[x][oo]\n\tsurv.event <- surv.event[x][oo]\n\tucl <- sort(unique(cl))\n\tmarker.fake <- 1:length(cl)\n\tnames(marker.fake) <- names(cl)\n\tmycutoff <- sum(cl == ucl[1])\n\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake, predict.time=time, span=span, ...)\n\t# Complete the function by adding the return statement\n}",
        "complete": "theta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\tcl <- cl[x]\n\too <- order(cl, decreasing=FALSE)\n\tcl <- cl[oo]\n\tsurv.time <- surv.time[x][oo]\n\tsurv.event <- surv.event[x][oo]\n\tucl <- sort(unique(cl))\n\tmarker.fake <- 1:length(cl)\n\tnames(marker.fake) <- names(cl)\n\tmycutoff <- sum(cl == ucl[1])\n\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake, predict.time=time, span=span, ...)\n\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)]))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/compute.pairw.cor.z.R",
    "language": "R",
    "content": "#' @title Function to compute the Z transformation of the pairwise\n#'   correlations for a list of datasets\n#'\n#' @description\n#' This function computes the Z transformation of the meta-estimate of pairwise correlation\n#'   coefficients for a set of genes from a list of gene expression datasets.\n#'\n#' @usage\n#' compute.pairw.cor.z(datas, method = c(\"pearson\"))\n#'\n#' @param datas List of datasets. Each dataset is a matrix of gene expressions with samples in\n#'   rows and probes in columns, dimnames being properly defined. All the datasets must have\n#'   the same probes.\n#' @param method Estimator for correlation coefficient, can be either pearson or spearman.\n#'\n#' @return\n#' A list with items:\n#' -z Z transformation of the meta-estimate of correlation coefficients.\n#' -se Standard error of the Z transformation of the meta-estimate of\n#'   correlation coefficients.\n#' -nn Number of samples used to compute the meta-estimate of correlation coefficients.\n#'\n#' @seealso\n#' [genefu::map.datasets], [genefu::compute.pairw.cor.meta], [genefu::compute.proto.cor.meta]\n#'\n#' @md\n#' @importFrom survcomp fisherz\n#' @export\ncompute.pairw.cor.z <-\nfunction(datas, method=c(\"pearson\")) {\n\tif(!is.list(datas)) {\n\t\tmycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n\t\tfor(i in 1:ncol(datas)) {\n\t\t\tfor(j in 1:i) {\n\t\t\t\tmycorz[i, j] <- mycorz[j, i] <- fisherz(cor(x=datas[ , i], y=datas[ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n\t\t\t\tmycorz.se[i, j] <- mycorz.se[j, i] <- 1/sqrt(sum(complete.cases(datas[ , c(i, j)])) - 3)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\t\tmycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn,  ncn,  names(datas)))\n\t\tmycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn,  ncn,  names(datas)))\n\t\tfor(k in 1:length(datas)) {\n\t\t\tif(nc != ncol(datas[[k]])) { stop(\"all the datasets have not the same number of columns!\") }\n\t\t\tfor(i in 1:ncol(datas[[k]])) {\n\t\t\t\tfor(j in 1:i) {\n\t\t\t\t\tnn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n\t\t\t\t\tmycorz[i, j, k] <- mycorz[j, i, k] <- fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n\t\t\t\t\tmycorz.se[i, j, k] <- mycorz.se[j, i, k] <- 1/sqrt(nn - 3)\n\t\t\t\t\tmycorn[i, j, k] <- mycorn[j, i, k] <- nn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `compute.pairw.cor.z` function and what are its main inputs and outputs?",
        "answer": "The `compute.pairw.cor.z` function computes the Z transformation of the meta-estimate of pairwise correlation coefficients for a set of genes from a list of gene expression datasets. Its main inputs are: 1) `datas`: a list of datasets or a single dataset matrix, where each dataset has samples in rows and probes in columns, and 2) `method`: the correlation coefficient estimator (default is 'pearson'). The function returns a list with three items: 'z' (Z transformation of the meta-estimate of correlation coefficients), 'se' (standard error of the Z transformation), and 'nn' (number of samples used for computation)."
      },
      {
        "question": "How does the function handle the computation differently when `datas` is a single matrix versus a list of matrices?",
        "answer": "When `datas` is a single matrix, the function computes the pairwise correlations and their Z transformations directly on this matrix. It uses nested loops to calculate correlations between all pairs of columns. When `datas` is a list of matrices, the function computes pairwise correlations for each matrix in the list separately, storing results in 3D arrays. It also checks that all matrices in the list have the same number of columns. This approach allows for meta-analysis across multiple datasets."
      },
      {
        "question": "What is the significance of the `fisherz` function used in the correlation calculations, and how is the standard error computed?",
        "answer": "The `fisherz` function is used to apply Fisher's Z transformation to the computed correlation coefficients. This transformation helps to stabilize the variance of the correlation estimate and make it approximately normally distributed. The standard error for each Z-transformed correlation is computed as 1 / sqrt(n - 3), where n is the number of complete cases (non-missing pairs of observations) used in the correlation calculation. This formula is derived from the approximate variance of the Fisher Z transformation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    mycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n    for(i in 1:ncol(datas)) {\n      for(j in 1:i) {\n        # Complete the code here\n      }\n    }\n  } else {\n    # Complete the code for list input\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}",
        "complete": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    mycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n    for(i in 1:ncol(datas)) {\n      for(j in 1:i) {\n        mycorz[i, j] <- mycorz[j, i] <- fisherz(cor(x=datas[ , i], y=datas[ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n        mycorz.se[i, j] <- mycorz.se[j, i] <- 1/sqrt(sum(complete.cases(datas[ , c(i, j)])) - 3)\n      }\n    }\n    mycorn <- NULL\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    mycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    mycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    for(k in 1:length(datas)) {\n      if(nc != ncol(datas[[k]])) stop(\"all the datasets have not the same number of columns!\")\n      for(i in 1:nc) {\n        for(j in 1:i) {\n          nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n          mycorz[i, j, k] <- mycorz[j, i, k] <- fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n          mycorz.se[i, j, k] <- mycorz.se[j, i, k] <- 1/sqrt(nn - 3)\n          mycorn[i, j, k] <- mycorn[j, i, k] <- nn\n        }\n      }\n    }\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}"
      },
      {
        "partial": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    # Complete the code for non-list input\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    mycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    mycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    for(k in 1:length(datas)) {\n      if(nc != ncol(datas[[k]])) stop(\"all the datasets have not the same number of columns!\")\n      # Complete the nested loops and calculations here\n    }\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}",
        "complete": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    mycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n    for(i in 1:ncol(datas)) {\n      for(j in 1:i) {\n        mycorz[i, j] <- mycorz[j, i] <- fisherz(cor(x=datas[ , i], y=datas[ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n        mycorz.se[i, j] <- mycorz.se[j, i] <- 1/sqrt(sum(complete.cases(datas[ , c(i, j)])) - 3)\n      }\n    }\n    mycorn <- NULL\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    mycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    mycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    for(k in 1:length(datas)) {\n      if(nc != ncol(datas[[k]])) stop(\"all the datasets have not the same number of columns!\")\n      for(i in 1:nc) {\n        for(j in 1:i) {\n          nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n          mycorz[i, j, k] <- mycorz[j, i, k] <- fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n          mycorz.se[i, j, k] <- mycorz.se[j, i, k] <- 1/sqrt(nn - 3)\n          mycorn[i, j, k] <- mycorn[j, i, k] <- nn\n        }\n      }\n    }\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/sig.score.R",
    "language": "R",
    "content": "#' @title Function to compute signature scores as linear combination of \n#'   gene expressions\n#'\n#' @description\n#' This function computes a signature score from a gene list (aka gene \n#'   signature), i.e. a signed average as published in Sotiriou et al. 2006 \n#'   and Haibe-Kains et al. 2009.\n#'\n#' @usage\n#' sig.score(x, data, annot, do.mapping = FALSE, mapping, size = 0,\n#'   cutoff = NA, signed = TRUE, verbose = FALSE)\n#'\n#' @param x\tMatrix containing the gene(s) in the gene list in rows and at \n#'   least three columns: \"probe\", \"EntrezGene.ID\" and \"coefficient\" standing \n#'   for the name of the probe, the NCBI Entrez Gene id and the coefficient \n#'   giving the direction and the strength of the association of each gene \n#'   in the gene list.\n#' @param data Matrix of gene expressions with samples in rows and probes \n#'   in columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be \n#'   performed (in case of ambiguities, the most variant probe is kept for \n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to \n#'   force the mapping such that the probes are not selected based on their \n#'   variance.\n#' @param size Integer specifying the number of probes to be considered in \n#'   signature computation. The probes will be sorted by absolute value of \n#'   coefficients.\n#' @param cutoff Only the probes with coefficient greater than cutoff will\n#'    be considered in signature computation.\n#' @param signed TRUE if only the sign of the coefficient must be considered \n#'   in signature computation, FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Signature score.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence \n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Sotiriou C, Wirapati P, Loi S, Harris A, Bergh J, Smeds J, Farmer P, Praz \n#'   V, Haibe-Kains B, Lallemand F, Buyse M, Piccart MJ and Delorenzi M \n#'   (2006) \"Gene expression profiling in breast cancer: Understanding the \n#'   molecular basis of histologic grade to improve prognosis\", Journal of \n#'   National Cancer Institute, 98:262-272\n#' Haibe-Kains B (2009) \"Identification and Assessment of Gene Signatures \n#'   in Human Breast Cancer\", PhD thesis at Universite Libre de Bruxelles,\n#'   http://theses.ulb.ac.be/ETD-db/collection/available/ULBetd-02182009-083101/\n#'\n#' @examples\n#' # load NKI data\n#' data(nkis)\n#' # load GGI signature\n#' data(sig.ggi)\n#' # make of ggi signature a gene list\n#' ggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")],\n#'   \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n#' # computation of signature scores\n#' ggi.score <- sig.score(x=ggi.gl, data=data.nkis, annot=annot.nkis,\n#'   do.mapping=TRUE, signed=TRUE, verbose=TRUE)\n#' str(ggi.score)\n#'\n#' @md\n#' @export\nsig.score <-\nfunction(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n\t\n\tif(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n\tx <- as.data.frame(x, stringsAsFactors=FALSE)\n\tif(nrow(x) == 0) { stop(\"empty gene list!\"); }\n\n\tmyprobe <- as.character(x[ ,\"probe\"])\n\tmygid <- as.character(x[ ,\"EntrezGene.ID\"])\n\tmycoef <- as.numeric(x[ ,\"coefficient\"])\n\tnames(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n\n\tnix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n\tmyprobe <- myprobe[nix]\n\tmygid <- mygid[nix]\n\tmycoef <- mycoef[nix]\n   \n   if(do.mapping) { ## mapping is requested\n\t\tgid1 <- mygid\n\t\tgid2 <- as.character(annot[ ,\"EntrezGene.ID\"])\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\t\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tif(is.na(rr$geneid1[1])) {\n\t\t\t#no gene ids in common\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tgf <- c(\"mapped\"=0, \"total\"=nrow(x))\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: 0/%i\", nrow(x))) }\n\t\t\treturn(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=NA, \"EntrezGene.ID\"=NA, \"new.probe\"=NA)))\n\t\t}\n\t\tnix <- match(rr$geneid2, mygid)\n\t\tmyprobe <- myprobe[nix]\n\t\tmygid <- mygid[nix]\n\t\tmycoef <- mycoef[nix]\n\t\tgid1 <- rr$geneid2\n\t\tif(is.null(names(gid1))) { stop(\"problem with annotations!\") }\n\t\tgid2 <- rr$geneid1\n\t\tif(is.null(names(gid2))) { stop(\"problem with annotations!\") }\n\t\tdata <- rr$data1\n\t\n\t\t#change the names of probes in x and data\n\t\tnames(mycoef) <- names(mygid) <- mygid <- names(myprobe) <- myprobe <- as.character(gid1)\n\t\tdimnames(data)[[2]] <- as.character(gid2)\n\t} else { ## no mapping\n\t\tnix <- is.element(myprobe, dimnames(data)[[2]])\n\t\tmyprobe <- myprobe[nix]\n\t\tmygid <- mygid[nix]\n\t\tmycoef <- mycoef[nix]\n\t\tgid1 <- gid2 <- mygid\n\t\tdata <- data[ ,myprobe,drop=FALSE]\n\t}\n\tif(length(myprobe) == 0) {\n\t\tif(verbose) { message(sprintf(\"probe candidates: 0/%i\", size)) }\n\t\ttt <- rep(NA, nrow(data))\n\t\tnames(tt) <- dimnames(data)[[1]]\n\t\treturn(list(\"score\"=tt, \"mapping\"=c(\"mapped\"=0, \"total\"=nrow(x)), \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n\t}\n\t\n\tif(size == 0 || size > nrow(x)) { size <- length(myprobe) }\n\tnix <- 1:size\n\tmyprobe <- myprobe[nix]\n\tmygid <- mygid[nix]\n\tmycoef <- mycoef[nix]\n\tgid1 <- gid1[nix]\n\tgid2 <- gid2[nix]\n\tif(!is.na(cutoff)) {\n\t\tnix <- abs(mycoef) > cutoff\n\t\tmyprobe <- myprobe[nix]\n\t\tmygid <- mygid[nix]\n\t\tmycoef <- mycoef[nix]\n\t\tgid1 <- gid1[nix]\n\t\tgid2 <- gid2[nix]\n   }\n\tprobe.candp <- myprobe[mycoef >= 0]\n\tprobe.candn <- myprobe[mycoef < 0]\n\tgf <- length(myprobe)\n\n\tgf <- c(\"mapped\"=gf, \"total\"=nrow(x))\n\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\",gf[1], gf[2])) }\n\n\tnprobe <- c(probe.candp, probe.candn)\n\tmyw <- c(\"p\"=length(probe.candp) / length(nprobe), \"n\"=length(probe.candn) / length(nprobe))\n\tres <- rep(0, nrow(data))\n\t\n\tif(signed) {\n\t\t## consider only the sign of the coefficients\n\t\tif(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n\t\tif(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n\t} else {\n\t\t## consider the exact value of the coefficients\n\t\tif(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candp]))) }\n\t\tif(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candn]))) }\n\t}\n\treturn(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'sig.score' function and what are its main input parameters?",
        "answer": "The 'sig.score' function computes a signature score from a gene list (gene signature) as a linear combination of gene expressions. Its main input parameters are:\n- x: Matrix containing gene information (probe, EntrezGene.ID, coefficient)\n- data: Matrix of gene expressions\n- annot: Matrix of annotations\n- do.mapping: Boolean indicating whether to perform mapping through Entrez Gene IDs\n- size: Number of probes to consider in signature computation\n- signed: Boolean indicating whether to consider only the sign of coefficients"
      },
      {
        "question": "How does the function handle gene mapping when 'do.mapping' is set to TRUE?",
        "answer": "When 'do.mapping' is TRUE, the function:\n1. Removes missing and duplicated gene IDs from the gene list\n2. Uses 'geneid.map' function to map gene IDs between the gene list and annotation data\n3. Updates the probe names, gene IDs, and coefficients based on the mapping results\n4. Modifies the data matrix to use the mapped gene IDs as column names\nIf no gene IDs are in common after mapping, the function returns NA scores with appropriate mapping information."
      },
      {
        "question": "How does the 'sig.score' function calculate the final signature score?",
        "answer": "The function calculates the signature score as follows:\n1. Separates probes into positive and negative groups based on their coefficients\n2. Calculates weights for positive and negative groups based on their sizes\n3. If 'signed' is TRUE, it uses only the sign of coefficients:\n   - For positive probes: (sum of expressions) / (number of non-NA values) * weight\n   - For negative probes: -(sum of expressions) / (number of non-NA values) * weight\n4. If 'signed' is FALSE, it uses the exact coefficient values:\n   - For positive probes: (sum of expressions * abs(coefficients)) / (sum of abs(coefficients) for non-NA values) * weight\n   - For negative probes: -(sum of expressions * abs(coefficients)) / (sum of abs(coefficients) for non-NA values) * weight\n5. The final score is the sum of positive and negative contributions"
      }
    ],
    "completion_tasks": [
      {
        "partial": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n\n  # TODO: Implement mapping logic\n\n  # TODO: Implement score calculation\n\n  # TODO: Return result\n}",
        "complete": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n   \n  if(do.mapping) {\n    gid1 <- mygid\n    gid2 <- as.character(annot[ ,\"EntrezGene.ID\"])\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    \n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    if(is.na(rr$geneid1[1])) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=nrow(x))\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", nrow(x))) }\n      return(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=NA, \"EntrezGene.ID\"=NA, \"new.probe\"=NA)))\n    }\n    nix <- match(rr$geneid2, mygid)\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    \n    names(mycoef) <- names(mygid) <- mygid <- names(myprobe) <- myprobe <- as.character(gid1)\n    dimnames(data)[[2]] <- as.character(gid2)\n  } else {\n    nix <- is.element(myprobe, dimnames(data)[[2]])\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- gid2 <- mygid\n    data <- data[ ,myprobe,drop=FALSE]\n  }\n\n  if(length(myprobe) == 0) {\n    if(verbose) { message(sprintf(\"probe candidates: 0/%i\", size)) }\n    tt <- rep(NA, nrow(data))\n    names(tt) <- dimnames(data)[[1]]\n    return(list(\"score\"=tt, \"mapping\"=c(\"mapped\"=0, \"total\"=nrow(x)), \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n  }\n  \n  if(size == 0 || size > nrow(x)) { size <- length(myprobe) }\n  nix <- 1:size\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n  gid1 <- gid1[nix]\n  gid2 <- gid2[nix]\n  if(!is.na(cutoff)) {\n    nix <- abs(mycoef) > cutoff\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- gid1[nix]\n    gid2 <- gid2[nix]\n  }\n  probe.candp <- myprobe[mycoef >= 0]\n  probe.candn <- myprobe[mycoef < 0]\n  gf <- length(myprobe)\n\n  gf <- c(\"mapped\"=gf, \"total\"=nrow(x))\n  if(verbose) { message(sprintf(\"probe candidates: %i/%i\",gf[1], gf[2])) }\n\n  nprobe <- c(probe.candp, probe.candn)\n  myw <- c(\"p\"=length(probe.candp) / length(nprobe), \"n\"=length(probe.candn) / length(nprobe))\n  res <- rep(0, nrow(data))\n  \n  if(signed) {\n    if(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n    if(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n  } else {\n    if(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candp]))) }\n    if(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candn]))) }\n  }\n  return(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n}"
      },
      {
        "partial": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  # Input validation\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  # Extract and sort probe information\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n\n  # TODO: Implement mapping logic if do.mapping is TRUE\n\n  # TODO: Apply size and cutoff filters\n\n  # TODO: Calculate scores\n\n  # TODO: Return results\n}",
        "complete": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  # Input validation\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  # Extract and sort probe information\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n\n  # Mapping logic\n  if(do.mapping) {\n    gid1 <- mygid\n    gid2 <- as.character(annot[ ,\"EntrezGene.ID\"])\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    \n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    if(is.na(rr$geneid1[1])) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=nrow(x))\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", nrow(x))) }\n      return(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=NA, \"EntrezGene.ID\"=NA, \"new.probe\"=NA)))\n    }\n    nix <- match(rr$geneid2, mygid)\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    \n    names(mycoef) <- names(mygid) <- mygid <- names(myprobe) <- myprobe <- as.character(gid1)\n    dimnames(data)[[2]] <- as.character(gid2)\n  } else {\n    nix <- is.element(myprobe, dimnames(data)[[2]])\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- gid2 <- mygid\n    data <- data[ ,myprobe,drop=FALSE]\n  }\n\n  # Apply size and cutoff filters\n  if(size == 0 || size > nrow(x)) { size <- length(myprobe) }\n  nix <- 1:size\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n  gid1 <- gid1["
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/src/foo_mrmr_ensemble_surv.cpp",
    "language": "cpp",
    "content": "#include \"foo_mrmr_ensemble_surv.h\"\n\ndouble get_correlation_ensemble(double data [],int namat[], int ind_x, int ind_y, int size){\n\t//compute correlation of two variables;\n\t//data: contains all data in a vector; variable-wise appended\n\t//ind_x: starting index of first variable in data\n\t//ind_y: starting index of second variable in data\n\t//size: number of samples for both variables\n\tdouble mean_data_x=0.0,mean_data_y=0.0;\n\tdouble correlation_nom=0.0,correlation_den_x=0.0,correlation_den_y=0.0;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif (namat[ind_x+i]==0 && namat[ind_y+i]==0 ) {\n\t\t\tmean_data_x+=data[ind_x+i];\n\t\t\tmean_data_y+=data[ind_y+i];\n\t\t}\n\t}\n\n\tmean_data_x=mean_data_x/size;\n\tmean_data_y=mean_data_y/size;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif(namat[ind_x+i]==0 && namat[ind_y+i]==0){\n\t\tcorrelation_nom+=(data[ind_x+i]-mean_data_x)*(data[ind_y+i]-mean_data_y);\n\t\tcorrelation_den_x+=(data[ind_x+i]-mean_data_x)*(data[ind_x+i]-mean_data_x);\n\t\tcorrelation_den_y+=(data[ind_y+i]-mean_data_y)*(data[ind_y+i]-mean_data_y);\n\t\t}\n\t}\n\treturn correlation_nom/(sqrt(correlation_den_x*correlation_den_y));\n}\n\n\ndouble returnConcordanceIndexC(int *msurv, int *ustrat, double *x2, int *cl2,\n\t\t\t\t\t   double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU)\n{\n\n\tint lenUstrat = *lenU;\n\tint lenStrat = lenS;\n\n\tdouble res_ch[lenStrat];\n\tdouble res_dh[lenStrat];\n\n\tdouble res_cIndex=0;\n\n\tint Ns_old = 0;\n\tint Ns = 0;\n\tfor(int s=0; s < lenUstrat; s++) {\n\t\tint ixs[lenStrat];\n\t\tfor(int i =0; i < lenStrat; i++){\n\t\t\tixs[i] = 0;\n\t\t\tif(strat[i] == ustrat[s]){\n\t\t\t\tixs[i] = 1;\n\t\t\t} else {\n\t\t\t\tixs[i] = 0;\n\t\t\t}\n\t\t}\n\t\tNs_old += Ns;\n\t\tNs = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tNs++;\n\t\t\t}\n\t\t}\n\t\tdouble xs[Ns];\n\t\tint c = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\txs[c] = x2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint cls[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tcls[c] = cl2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble sts[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tsts[c] = st[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint ses[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tses[c] = se[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble weightss[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tweightss[c] = weights[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble chs[Ns];\n\t\tdouble dhs[Ns];\n\t\tdouble uhs[Ns];\n\t\tdouble rphs[Ns];\n\t\tfor (int h=0; h < Ns; h++) {\n\t\t\tdouble chsj, dhsj, uhsj, rphsj = 0;\n\t\t\tfor (int j=0; j < Ns; j++) {\n\t\t\t\tdouble whj = weightss[h] * weightss[j];\n\t\t\t\tif((*msurv == 1 && (sts[h] < sts[j] && ses[h] == 1)) || (*msurv == 0 && cls[h] > cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] > xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t} else if (xs[h] < xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif((*msurv == 1 && (sts[h] > sts[j] && ses[j] == 1)) || (*msurv == 0 && cls[h] < cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] < xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse if (xs[h] > xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tchs[h] = chsj;\n\t\t\tdhs[h] = dhsj;\n\t\t\tuhs[h] = uhsj;\n\t\t\trphs[h] = rphsj;\n\t\t\tchsj = 0;\n\t\t\tdhsj = 0;\n\t\t\tuhsj = 0;\n\t\t\trphsj = 0;\n\t\t}\n\t\tfor(int i = 0; i < Ns; i++){\n\t\t\tint pos = i + Ns_old;\n\t\t\tres_ch [pos] =chs[i];\n\t\t\tres_dh [pos] =dhs[i];\n\t\t}\n\n\t}\n\n\tdouble tmp_ch=0, tmp_dh=0;\n\tfor(int s=0; s < lenStrat; s++) {\n\t\ttmp_ch+=res_ch[s];\n\t\ttmp_dh+=res_dh[s];\n\t}\n\n\tdouble n=*N;\n\ttmp_ch=(1/(n *(n - 1))) * tmp_ch;\n\ttmp_dh=(1/(n *(n - 1))) * tmp_dh;\n\tres_cIndex=tmp_ch/ (tmp_ch+tmp_dh);\n\n\t/// scale value to be in the intervall [-1,1] as correlation and square to be on same scale as mutual information [0,1]\n\n\tres_cIndex=2*res_cIndex-1;\n\tres_cIndex=res_cIndex*res_cIndex;\n\treturn res_cIndex;\n}\n\n\nvoid build_mim_cIndex_subset(double mim[],double data[], int namat [],int nvar,int nsample, int subset [],int size_subset,int *msurv, int *ustrat, int *cl2, double *st, int *se, double *weights, int *strat, int *N, int *outx, int *lenU){\n\t//compute mutual information matrix\n\t//mim:\t\t\tmatrix (stored as vector) in which the mi values will be stored\n\t//data:\t\t\tcontains all data in a vector; variable-wise appended\n\t//nvar:\t\t\tnumber of variables\n\t//nsample:\t\tnumber of samples in dataset\n\t//subset:\t\tindices of samples to be included in the bootstrapping data\n\t//size_subset:\tnumber of variables in the bootstrapped dataset\n\n\tdouble tmp;\n\tdouble *data_x, *st_x, *weights_x;\n\tint *namat_x, *msurv_x, *ustrat_x, *cl2_x, *se_x, *strat_x;\n\n\tnamat_x = (int*) R_alloc(nvar*size_subset, sizeof(int));\n\tcl2_x = (int*) R_alloc(size_subset, sizeof(int));\n\tse_x = (int*) R_alloc(size_subset, sizeof(int));\n\tstrat_x = (int*) R_alloc(size_subset, sizeof(int));\n\n\tdata_x = (double *) R_alloc(nvar*size_subset, sizeof(double));\n\tst_x = (double *) R_alloc(size_subset, sizeof(double));\n\tweights_x = (double *) R_alloc(size_subset, sizeof(double));\n\n\tfor(unsigned int i=0; i< size_subset; ++i){\n\t\tfor(unsigned int j=0; j< (nvar-1); ++j){\n\t\t\tdata_x[size_subset*j+i]=data[(subset[i])+nsample*j];\n\t\t\tnamat_x[size_subset*j+i]=namat[(subset[i])+nsample*j];\n\t\t}\n\n\t\tcl2_x[i]=cl2[subset[i]];\n\t\tse_x[i]=se[subset[i]];\n\t\tstrat_x[i]=strat[subset[i]];\n\t\tst_x[i]=st[subset[i]];\n\t\tweights_x[i]=weights[subset[i]];\n\t}\n\tfor(unsigned int i=0; i< nvar-1; ++i){\n\t\tmim[(i+1)*(nvar)+(i+1)]=0;\n\t\tfor(unsigned int j=i+1; j< nvar-1; ++j){\n\t\t\ttmp=get_correlation_ensemble(data_x,namat_x,i*size_subset,j*size_subset,size_subset);\n\t\t\ttmp=tmp*tmp;\n\t\t\tif(tmp>0.999999){\n\t\t\t\ttmp=0.999999;\n\t\t\t}\n\t\t\tmim[(j+1)*(nvar)+i+1]= -0.5* log (1-tmp);\n\t\t\tmim[(i+1)*(nvar)+j+1]=mim[(j+1)*(nvar)+i+1];\n\t\t}\n\t}\n\n\tdouble *data_small;\n\tdata_small =(double*) R_alloc(size_subset, sizeof(double));\n\n\tfor(int j=0;j< nvar-1 ;++j){\n\t\tfor(int i=0;i< size_subset;++i){\n\t\t\tdata_small[i]=data_x[j* (nvar-1)+i];\n\t\t}\n\t\tmim[j+1]=returnConcordanceIndexC(msurv, ustrat, data_small, cl2_x,st_x, se_x, weights_x,strat_x, N, outx, size_subset, lenU) ;\n\t\tmim[(nvar)*(j+1)] = mim[j+1];\n\t}\n\n}\n\nvoid remove_childless_nodes( tree<int>& res, tree<double>&res_mean, int max_elements_tmp){\n\ttree<int>::pre_order_iterator it_tmp,it_back, it=res.begin();\n\ttree<double>::pre_order_iterator  it_mean_tmp,it_mean_back, it_mean=res_mean.begin();\n\ttree<int>::leaf_iterator li;\n\ttree<double>::leaf_iterator li_mean;\n\tbool found_child, multiple;\n\n\tint depth_max=0;\n\t//determine max depth\n\twhile(it!=res.end()){\n\t\tif(depth_max<res.depth(it)){\n\t\t\tdepth_max=res.depth(it);\n\t\t}\n\t\tit++;\n\t}\n\tit=res.begin();\n\twhile(it!=res.end() && max_elements_tmp<= (depth_max+1)) {\n\t\tif(res.depth(it)<=(max_elements_tmp-2) && res.number_of_children(it)==0){ //advance through the tree\n\t\t\tit_tmp=res.parent(it);it_mean_tmp=res_mean.parent(it_mean);\n\t\t\tfound_child=false; multiple=false;\n\t\t\twhile(!found_child && (it_tmp!=res.begin() || res.number_of_children(it_tmp)>1)){ //end loop if there is a node with more than one child or if back at top and number of children==1 for top node\n\t\t\t\tif(res.number_of_children(it_tmp)==1){\n\t\t\t\t\tit_back=it_tmp;it_mean_back=it_mean_tmp; //if this was the last level for which there is only one child\n\t\t\t\t\tmultiple=true;\n\t\t\t\t\tif(it_tmp!=res.begin()){\n\t\t\t\t\t\tit_tmp=res.parent(it_tmp); it_mean_tmp=res_mean.parent(it_mean_tmp);\n\t\t\t\t\t}else{//in case of having arrived at the top node\n\t\t\t\t\t\tres.erase(it_back); res_mean.erase(it_mean_back);\n\t\t\t\t\t\tfound_child=true;\n\t\t\t\t\t}\n\t\t\t\t}else{\n\t\t\t\t\tif(multiple){\n\t\t\t\t\t\tres.erase(it_back); res_mean.erase(it_mean_back);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tres.erase(it); res_mean.erase(it_mean);\n\t\t\t\t\t}\n\t\t\t\t\tfound_child=true;\n\t\t\t\t}\n\t\t\t}\n\t\t\tit=it_tmp; it_mean=it_mean_tmp;\n\t\t}else{\n\t\t\t++it; ++it_mean;\n\t\t}\n\t}\n}\nvoid bootstrap_tree(tree<int>& res,tree<double>& res_mrmr, double data[],int namat[], int nsamples,int n, int rep_boot, int *msurv, int *ustrat, int *cl2,double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU){\n\tint  nsub, *prev_sel,nsamples_boot=nsamples,*to_remove;\n\ttree<int>::iterator li=res.begin_leaf(),li2;\n\ttree<double>::iterator li_mrmr=res_mrmr.begin_leaf(),li2_mrmr;\n\tdouble *mean, *sd;\n\tint cnt_leafs=0;\n\tint max_depth=res.depth(li),index;\n\twhile (li!=res.end()) {\n\t\tif(res.depth(li)==max_depth){\n\t\t\tcnt_leafs++;\n\t\t}\n\t\tli++;\n\t}\n\n\tli=res.begin_leaf();\n\n\tmean =(double*) R_alloc(cnt_leafs, sizeof(double));\n\tsd =(double*) R_alloc(cnt_leafs, sizeof(double));\n\tto_remove=(int*) R_alloc(cnt_leafs, sizeof(int));\n\n\tfor(int k=0;k<cnt_leafs;k++){\n\t\tmean[k]=0;sd[k]=0;\n\t}\n\n\tint target=*res.begin();\n\tint nto_remove=0;\n\n\tprev_sel=(int*) R_alloc(max_depth, sizeof(int));\n\n\tint k=0;\n\twhile (li!=res.end()) {\n\t\tif(res.depth(li)==max_depth){\n\t\t\tli2=li;\n\t\t\tprev_sel[max_depth-1]=*(li);\n\t\t\tli2=res.parent(li2);\n\t\t\tindex=max_depth-2;\n\t\t\twhile (li2!=res.begin()) {\n\t\t\t\tprev_sel[index]=*(li2);\n\t\t\t\tindex--;\n\t\t\t\tli2=res.parent(li2);\n\t\t\t}\n\t\t\tbootstrap_mrmr(mean[k], sd[k], data,namat,n, rep_boot,nsamples_boot,nsamples, target, prev_sel[max_depth-1], max_depth-1,prev_sel, msurv, ustrat, cl2,st, se, weights,strat, N, outx, lenS, lenU);\n\t\t\tk++;\n\t\t}\n\t\tli++;\n\t}\n\tdouble max_mrmr=-1000;\n\tint max_mrmr_ind=-1;\n\tfor(int k=0;k<cnt_leafs;k++){\n\t\tif(mean[k]>max_mrmr){\n\t\t\tmax_mrmr=mean[k];\n\t\t\tmax_mrmr_ind=k;\n\t\t}\n\t}\n\tfor(int k=0;k<cnt_leafs;k++){\n\t\tif(k!=max_mrmr_ind && (mean[k] < max_mrmr-sd[max_mrmr_ind])){\n\t\t\tto_remove[nto_remove]=k;\n\t\t\tnto_remove++;\n\t\t}\n\t}\n\n\tint cnt2=nto_remove;\n\tif(cnt2>0){\n\t\tli=res.begin_leaf(res.end());\n\t\tsort(to_remove,to_remove+cnt2);\n\t\tli_mrmr=res_mrmr.begin_leaf(res_mrmr.end());\n\t\tint cnt_back=cnt2;\n\t\twhile (cnt_leafs>=0 && cnt2>0) {\n\t\t\tli2=li;\n\t\t\tli2_mrmr=li_mrmr;\n\t\t\tli--;li_mrmr--;\n\n\t\t\twhile(res.depth(li)<max_depth && li2!=res.begin_leaf(res.begin())) {\n\t\t\t\tli--;li_mrmr--;\n\t\t\t}\n\n\t\t\tif(to_remove[cnt2-1]==cnt_leafs){\n\t\t\t\tres.erase(li2);res_mrmr.erase(li2_mrmr);\n\t\t\t\tcnt2--;\n\t\t\t}\n\t\t\tcnt_leafs--;\n\t\t}\n\t}\n\tremove_childless_nodes(res, res_mrmr,max_depth+1);\n\n}\nvoid bootstrap_mrmr(double &mean, double &sd, double data[],int namat[],int size, int rep_boot, int size_boot,int nsamples, int var_target, int var_interest, int nprev_sel,int* var_ind, int *msurv, int *ustrat, int *cl2,\n\t\t\t\t\tdouble *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU)\n{\n\t//mean\n\t//sd\n\t//data\n\t//size\n\t//rep_boot\n\t//size_boot\n\t//nsamples\n\t//var_target\n\t//var_interest\n\t//nprev_sel\n\t//var_ind\n\n\tint *ind;\n\tdouble *mim, *boot_val, *mat_info;\n\n\tind=(int*) R_alloc(size_boot, sizeof(int));\n\tboot_val =(double*) R_alloc(rep_boot, sizeof(double));\n\tmat_info =(double*) R_alloc(((size)*(size )), sizeof(double));\n\n\tfor(unsigned int k=0; k< rep_boot; ++k){\n\t\t//in total there will be rep_boot times the mrmr sampled\n\t\t//determine the subset of samples that should be used (in total size_boot samples will be selected)\n\t\tfor(unsigned int i=1;i<= size_boot;++i){\n\t\t\tind[i-1]=(int)unif_rand () %nsamples;\n\t\t}\n\t\t// compute mi matrix for the subset\n\t\tfor( unsigned int i=0; i< size ; ++i ){\n\t\t\tfor( unsigned int j=0; j< size ; ++j ){\n\t\t\t\tmat_info[i+(size )*j]=0;\n\t\t\t}\n\t\t}\n\n\t\tbuild_mim_cIndex_subset(mat_info, data, namat, (size) , nsamples, ind, size_boot ,msurv, ustrat, cl2,st, se, weights,strat, N, outx,  lenU);\n\t\tboot_val[k]=mrnet_onegene(mat_info, size, nprev_sel, var_ind, var_target, var_interest);\n\n\t}\n\n\t// determine mean and variance of bootstrapped values\n\tfor(unsigned int i=0;i< rep_boot;++i){\n\t\tif(boot_val[i]==boot_val[i]){\n\t\t\tmean+=boot_val[i];\n\t\t}\n\t}\n\tmean=mean/rep_boot;\n\n\tfor(unsigned int i=0;i< rep_boot;++i){\n\t\tif(boot_val[i]==boot_val[i]){\n\t\t\tsd+=(boot_val[i]-mean) * (boot_val[i]-mean);\n\t\t}\n\t}\n\tsd=sqrt(sd/rep_boot);\n\n}\n\n\ndouble mrnet_onegene(double mim [], int size, int nbvar,int *var_ind,int var_target, int var_interest){\n\t// mim:\t\t\tmutual information matrix\n\t// size:\t\ttotal number of variables\n\t// nbvar:\t\tnumber of previously selected variables (not the target)\n\t// var_ind:\t\tthe indices of the previously selected variables as vector\n\t// var_target:\tthe index of the target gene\n\t// var_interest: the variable for which the mrmr score with the target has to be computed; will be used for bootstrapping it\n\n\tunsigned int jmax;\n\tdouble rel, red,res;\n\tdouble max_val=-1000;\n\n\tjmax=var_target-1;\n\t//initialize the remaining entries to zero\n\tred=0;\n\t// the relevance for variable of interest with the target is simply its mutual information with it\n\trel=mim[(var_target-1)*size+var_interest-1];\n\tif(nbvar > 0){\n\t\t// in case other variables have been previously selected; compute their redundancy with the variable of interest\n\t\tfor(unsigned int j=0;j< nbvar; j++){\n\t\t\tred+=mim[(var_ind[j]-1)*size+var_interest-1];\n\t\t}\n\t\tres=rel-red/nbvar ;\n\t}else{\n\t\tres=rel;\n\t}\n\treturn res;\n}\n\nint power(int a, int b)\n{\n\tint c=a;\n\tfor (int n=b; n>1; n--) c*=a;\n\treturn c;\n}\nint verify_equivalentset_nparents (tree<int>& tr, tree<int>::pre_order_iterator it, tree<int>::pre_order_iterator end,tree<double>& tr_mrmr, int maxnsol){\n\tif(!tr.is_valid(it)) return 0;\n\n\tbool found=false;\n\tint number_elements_to_remove=0, cnt=1, index=0;\n\ttree<int>::leaf_iterator li=tr.begin_leaf(it), li_tmp=tr.begin_leaf(tr.begin()), li_tmp2=li_tmp;\n\ttree<double>::leaf_iterator li_mrmr=tr_mrmr.begin_leaf(tr_mrmr.begin()), li_mrmr2=li_mrmr;\n\tint depth=tr.depth(li);\n\ttree<int>::pre_order_iterator it2;\n\tint vec_old[depth+1];\n\tint mat_res [power((maxnsol+1),(depth))][depth+2];\n\tint number_leafs=power((maxnsol+1),(depth));\n\tint to_remove[number_leafs];\n\n\tfor (int k=0; k< number_leafs ; k++) {\n\t\tto_remove[k]=0;\n\t}\n\tint cnt2=0,cnt_leafs=0;\n\n\twhile( li!=tr.end_leaf(it) ){\n\t\tvec_old[0]=*(li);\n\t\tit2=li;\n\n\t\twhile(it2!=tr.begin()){\n\t\t\tit2=tr.parent(it2);\n\t\t\tvec_old[cnt]=*(it2);\n\t\t\tcnt++;\n\t\t}\n\n\t\tsort(vec_old,vec_old+depth+1);\n\t\tmat_res[index][depth+1]=0;\n\n\t\tfor(int k=0;k<=depth;k++){\n\t\t\tmat_res[index][k]=vec_old[k];\n\t\t\tmat_res[index][depth+1]+=vec_old[k]+power(2,k);\n\t\t}\n\t\tindex++;\n\t\tcnt=1;\n\t\tli++;\n\t\tcnt_leafs++;\n\t}\n\n\n\tindex=0;\n\tbool found1=false,found2;\n\n\tfor(int k=0;k<(cnt_leafs-1) && !found1;k++){\n\t\tfor(int j=k+1;j<(cnt_leafs);j++){\n\t\t\tfound2=false;\n\t\t\tif(mat_res[k][depth+1]==mat_res[j][depth+1]){\n\t\t\t\tfor(int i=0;i<=depth && !found2;i++){\n\t\t\t\t\tif(mat_res[j][i]!=mat_res[k][i]){\n\t\t\t\t\t\tfound2=true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tfound2=true;\n\t\t\t}\n\t\t\tif(!found2){\n\t\t\t\tnumber_elements_to_remove++;\n\t\t\t\tint tmp=j;\n\t\t\t\twhile(tmp>0){\n\t\t\t\t\tli_tmp++,li_mrmr++;\n\t\t\t\t\ttmp--;\n\t\t\t\t}\n\n\n\t\t\t\ttmp=k;\n\t\t\t\twhile(tmp>0){\n\t\t\t\t\tli_tmp2++,li_mrmr2++;\n\t\t\t\t\ttmp--;\n\t\t\t\t}\n\n\t\t\t\tif(*(li_mrmr)< *(li_mrmr2)){\n\t\t\t\t\tto_remove[cnt2]=j;\n\t\t\t\t}else {\n\t\t\t\t\tto_remove[cnt2]=k;\n\t\t\t\t}\n\t\t\t\tcnt2++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif(cnt2>0){\n\t\tli=tr.begin_leaf(tr.end());\n\t\tsort(to_remove,to_remove+cnt2);\n\t\tli_mrmr=tr_mrmr.begin_leaf(tr_mrmr.end());\n\t\tint cnt_back=cnt2;\n\t\twhile (cnt_leafs>=0 && cnt2>0) {\n\t\t\tit2=li;li_mrmr2=li_mrmr;\n\t\t\tli--;li_mrmr--;\n\t\t\twhile(to_remove[cnt2-1]==to_remove[cnt2] && cnt2!=cnt_back){\n\t\t\t\tcnt2--;\n\t\t\t}\n\t\t\tif(to_remove[cnt2-1]==cnt_leafs){\n\t\t\t\ttr.erase(it2);\n\t\t\t\ttr_mrmr.erase(li_mrmr2);\n\t\t\t\tcnt2--;\n\t\t\t}\n\t\t\tcnt_leafs--;\n\t\t}\n\t}\n\treturn number_elements_to_remove;\n}\n\n\nvoid mrmr_ensemble_one_gene_remove (tree<int>& res, tree<int>::pre_order_iterator one, double data[], int namat[], int nsamples,int n , int max_elements, int predn , int rep_boot, int maxnsol, double threshold, int *msurv, int *ustrat, int *cl2,\n\t\t\tdouble *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU){\n\t//n\t\t\t\t\tnumber of variables\n\t//predn:\t\t\tindex of target node\n\n\t// nsub: the variables which have been previously selected + target; prev_sel=nsub-target\n\t// number of samples to use for bootstrapping is equal to total number of samples\n\n\tint  *nsub, *prev_sel,nsamples_boot=nsamples, tmp_val_max_ind, *prev_sel_tmp,*vec_sol_local,ndelete;\n\tdouble *vec_mean, *vec_sort, *vec_sd,  *vec_local_max_mean, *vec_local_max_sd,tmp_val_max, *mrmr_vec_sort,*vec_sol_local_mrmr;\n\tdouble *mat_info;\n\n\n\tint cnt=0, max_elements_tmp=1; //current depth in the tree\n\tint *ind;\n\tvec_mean =(double*) R_alloc(n, sizeof(double));\n\tvec_sd =(double*) R_alloc(n, sizeof(double));\n\tmrmr_vec_sort =(double*) R_alloc(n, sizeof(double));\n\tvec_local_max_mean =(double*) R_alloc(max_elements, sizeof(double));\n\tvec_local_max_sd =(double*) R_alloc(max_elements, sizeof(double));\n\tmat_info =(double*) R_alloc((n*n), sizeof(double));\n\tind=(int*) R_alloc(nsamples, sizeof(int));\n\n\tfor(unsigned int i=1;i<= nsamples;++i){\n\t\tind[i-1]=i-1;\n\t}\n\tfor( unsigned int i=0; i< n; ++i ){\n\t\tfor( unsigned int j=0; j< n; ++j ){\n\t\t\tmat_info[i+(n)*j]=0;\n\t\t}\n\t}\n\n\tbuild_mim_cIndex_subset(mat_info, data, namat, n, nsamples, ind, nsamples ,msurv, ustrat, cl2,st, se, weights,strat, N, outx,  lenU);\n\n\tfor(unsigned int k=0;k< max_elements ;++k){\n\t\tvec_local_max_mean[k]=-1000;\n\t}\n\n\tprev_sel=(int*) R_alloc(max_elements, sizeof(int));\n\tnsub=(int*) R_alloc(max_elements, sizeof(int));\n\n\ttree<int> res_tmp_new=res ;\n\ttree<int>::iterator it_local=res_tmp_new.begin(),it_local2=it_local;\n\n\tvec_sol_local=(int*) R_alloc(maxnsol, sizeof(int));\n\tvec_sol_local_mrmr=(double*) R_alloc(maxnsol, sizeof(double));\n\t//mrmr score should not be predicted for the target node\n\tvec_mean[predn-1]=-1000; vec_sd[predn-1]=-1000;\n\tprev_sel[0]=0; nsub[0]=predn;\n\n\ttree<double> res_mrmr;\n\ttree<double>::iterator top_mrmr;\n\n\n\ttop_mrmr=res_mrmr.begin();\n\tres_mrmr.insert(top_mrmr, predn);\n\ttree<double>::iterator it_mrmr_local=res_mrmr.begin(),it_mrmr_local2=it_mrmr_local;\n\tint target_depth=max_elements, max_depth=0;\n\tint max_depth_local=2;\n\n\twhile (res_tmp_new.depth(it_local)<target_depth && it_local!=res_tmp_new.end()) {\n\t\tmax_depth=res_tmp_new.depth(it_local);\n\t\twhile(it_local!=res_tmp_new.end()) {\n\t\t\tif(cnt!=0){\n\t\t\t\tit_local2=it_local; it_mrmr_local2=it_mrmr_local;\n\t\t\t\twhile(res_tmp_new.depth(it_local2)<max_depth){\n\t\t\t\t\tit_local2++;it_mrmr_local2++;\n\n\t\t\t\t}\n\t\t\t\twhile(it_local2!=res_tmp_new.begin()){\n\t\t\t\t\tnsub[res_tmp_new.depth(it_local2)]=*(it_local2);\n\n\t\t\t\t\tit_local2=res_tmp_new.parent(it_local2);\n\t\t\t\t\tit_mrmr_local2=res_mrmr.parent(it_mrmr_local2);\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor (unsigned int i=0;i<=max_depth;++i){\n\t\t\t\tprev_sel[i]=nsub[i+1];\n\t\t\t}\n\n\t\t\t////////////\n\t\t\t// initialize vec_mean and vec_sd for bootstrapping to -1000 if variable is not supposed to be tested (target or prev selected) otherwise 0\n\t\t\t////////////\n\n\t\t\tfor(unsigned int k=0;k< n;++k){\n\t\t\t\tvec_mean[k]=0;vec_sd[k]=0;\n\t\t\t}\n\t\t\tfor(unsigned int k=0;k<=max(res_tmp_new.depth(it_local),max_depth) ;++k){\n\t\t\t\tvec_mean[nsub[k]-1]=-1000;\tvec_sd[nsub[k]-1]=-1000;\n\t\t\t}\n\n\t\t\tfor(unsigned int k=0;k< n;++k){\n\t\t\t\tif(vec_mean[k]!= (-1000)){\n\t\t\t\t\tvec_mean[k]=mrnet_onegene( mat_info, n,min(cnt,max_elements_tmp),prev_sel, nsub[0], (k+1)); vec_sd[k]=0;\n\n\t\t\t\t}\n\t\t\t\tmrmr_vec_sort[k]=vec_mean[k];\n\t\t\t}\n\n\t\t\tsort(mrmr_vec_sort,mrmr_vec_sort+n);\n\n\t\t\ttmp_val_max=mrmr_vec_sort[n-maxnsol-1];\n\t\t\tint cnt_loop_max=0;\n\n\t\t\twhile (res_tmp_new.depth(it_local)<max_depth) {\n\t\t\t\tit_local++;it_mrmr_local++;\n\t\t\t}\n\t\t\tit_local2=it_local;it_mrmr_local2=it_mrmr_local;\n\t\t\tit_local2++;it_mrmr_local2++;\n\n\n\t\t\tfor(int k=0;k<n;k++){\n\t\t\t\tif(vec_mean[k]>tmp_val_max){\n\t\t\t\t\tvec_sol_local[cnt_loop_max]=k+1;\n\t\t\t\t\tvec_sol_local_mrmr[cnt_loop_max]=vec_mean[k];\n\t\t\t\t\tcnt_loop_max++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor(int k=maxnsol-1;k>=0;k--){\n\t\t\t\tres_tmp_new.append_child(it_local,vec_sol_local[k]);\n\t\t\t\tres_mrmr.append_child(it_mrmr_local,vec_sol_local_mrmr[k]);\n\t\t\t}\n\n\t\t\tif(res_tmp_new.depth(it_local)>0){\n\t\t\t\tit_local=it_local2;it_mrmr_local=it_mrmr_local2;\n\t\t\t}else{\n\t\t\t\tit_local++;it_mrmr_local++;\n\t\t\t}\n\t\t\tcnt++;\n\t\t}\n\t\tcnt++; max_elements_tmp++;\n\t\tndelete= -1;\n\t\twhile (ndelete!=0 ) {\n\t\t\tndelete=verify_equivalentset_nparents (res_tmp_new, res_tmp_new.begin(),res_tmp_new.end(),res_mrmr, maxnsol);\n\t\t}\n\n\t\tremove_childless_nodes(res_tmp_new, res_mrmr,max_depth_local+1);\n\n\t\tit_local=res_tmp_new.begin_leaf();it_mrmr_local=res_mrmr.begin_leaf();\n\t\tmax_depth_local++;\n\t}\n\tres=res_tmp_new;\n\n\tbootstrap_tree(res,res_mrmr, data, namat,  nsamples, n, rep_boot, msurv, ustrat, cl2,st, se, weights,strat, N, outx, lenS, lenU);\n\n}\n\n\nextern \"C\" SEXP\nmrmr_cIndex_ensemble_remove( SEXP Rdata, SEXP Rnamat, SEXP Rmaxparents, SEXP Rnvar, SEXP Rnsample, SEXP Rpredn, SEXP Rnpredn, SEXP Rrep_boot, SEXP Rmaxnsol, SEXP Rthreshold, SEXP Rmsurv, SEXP Rustrat, SEXP Rcl2, SEXP Rst, SEXP Rse, SEXP Rweights, SEXP Rstrat, SEXP RN, SEXP Routx, SEXP RlenS, SEXP RlenU){\n\t// Rdata:\t\tdata should be passed as vector, variable-wise appended\n\t// Rmaxparents:\tnumber of maximum number of parents\n\t// Rnvar:\t\tnumber of variables in the dataset\n\t// Rnsample:\tnumber of samples in the dataset\n\t// Rpredn:\t\tvector of target genes to consider\n\t// Rnpredn:\t\tnumber of target genes (number of elements in Rpredn)\n\t// Rrep_boot:\thow many bootstrap iterations\n\t// Rmaxnsol:\tmaximum number of children for each node at each step\n\n\tdouble *data, *threshold;\n\tconst int* maxparents, * nvar, *nsample, *maxnsol;\n\n\tint *predn, *rep_boot,*res,*res_all,*res_all2, *namat;\n\tint vec_tmp;\n\tconst int *npredn;\n\n\n\tdouble  *st, *weights;\n\tint *msurv, *ustrat, *cl2, *se, *strat, *N, *outx, *lenS, *lenU;\n\n\tSEXP Rres;\n\n\t//srand (time(NULL));\n\tPROTECT(Rdata = AS_NUMERIC(Rdata));\n\tPROTECT(Rnamat = AS_INTEGER(Rnamat));\n\tPROTECT(Rmaxparents= AS_INTEGER(Rmaxparents));\n\tPROTECT(Rnvar= AS_INTEGER(Rnvar));\n\tPROTECT(Rnsample= AS_INTEGER(Rnsample));\n\tPROTECT(Rpredn = AS_INTEGER(Rpredn));\n\tPROTECT(Rnpredn = AS_INTEGER(Rnpredn));\n\tPROTECT(Rrep_boot = AS_INTEGER(Rrep_boot));\n\tPROTECT(Rmaxnsol= AS_INTEGER(Rmaxnsol));\n\tPROTECT(Rthreshold= AS_NUMERIC(Rthreshold));\n\n\tdata=NUMERIC_POINTER(Rdata);\n\tnamat=INTEGER_POINTER(Rnamat);\n\tmaxparents = INTEGER_POINTER(Rmaxparents);\n\tnvar= INTEGER_POINTER(Rnvar);\n\tnsample= INTEGER_POINTER(Rnsample);\n\tpredn= INTEGER_POINTER(Rpredn);\n\tnpredn= INTEGER_POINTER(Rnpredn);\n\trep_boot= INTEGER_POINTER(Rrep_boot);\n\tmaxnsol= INTEGER_POINTER(Rmaxnsol);\n\tthreshold = NUMERIC_POINTER(Rthreshold);\n\n\tmsurv=INTEGER_POINTER(Rmsurv);\n\tustrat=INTEGER_POINTER(Rustrat);\n\tcl2=INTEGER_POINTER(Rcl2);\n\tst=NUMERIC_POINTER(Rst);\n\tse =INTEGER_POINTER(Rse);\n\tweights=NUMERIC_POINTER(Rweights);\n\tstrat=INTEGER_POINTER(Rstrat);\n\tN =INTEGER_POINTER(RN);\n\toutx=INTEGER_POINTER(Routx);\n\n\tlenS=INTEGER_POINTER(RlenS);\n\tlenU=INTEGER_POINTER(RlenU);\n\n\ttree<int> res_tree;\n\ttree<int>::iterator top,one;\n\ttree<int>::breadth_first_queued_iterator it_final;\n\n\ttop=res_tree.begin();\n\tint length_res=0;\n\tint length_res_old;\n\tfor(unsigned int i=0;i< *npredn;++i){\n\t//\tstd::cout<<\"model for node \"<<predn[i]<< \" is being built!\"<<std::endl;\n\t\tone=res_tree.insert(top, predn[i]);\n\n\t\t//build ensemble tree\n\t\tmrmr_ensemble_one_gene_remove(res_tree, one, data,namat,*nsample,(*nvar+1),*maxparents,predn[i],*rep_boot, *maxnsol, *threshold, msurv, ustrat, cl2,st, se, weights,strat, N, outx, *lenS, lenU);\n\n\t\t////////////////////////\n\t\t//convert tree to vector\n\t\t////////////////////////\n\t\tint *tmp_nchildren,*res_tmp;\n\t\tres_tmp=new int [2*(res_tree.size())+1];\n\t\ttmp_nchildren= new int [(res_tree.size())];\n\n\t\tit_final=res_tree.begin_breadth_first();\n\t\tint cnt=1,cnt2=0;\n\n\t\tres_tmp[0]=res_tree.size();\n\t\tint rootdepth=res_tree.depth(it_final);\n\t\twhile(it_final!=res_tree.end_breadth_first()) {\n\t\t\tres_tmp[cnt]=*it_final;\n\t\t\ttmp_nchildren[cnt-1]=(res_tree.number_of_children(it_final));\n\t\t\tcnt++;\n\t\t\t++it_final;\n\t\t}\n\t\t////////////////\n\t\t//save in final result vector\n\t\t////////////////\n\t\tlength_res_old=length_res;\n\t\tlength_res+=2*(res_tree.size())+1;\n\t\tint *res_all, *res_old;\n\t\tint ind=0;\n\t\tres_all=new int[length_res];\n\t\tif(length_res_old>0){\n\t\t\tfor(unsigned int k=0;k<length_res_old;k++){\n\t\t\t\tres_all[k]=res_old[k];\n\t\t\t}\n\t\t}\n\n\t\tfor(unsigned int k=0;k<=res_tree.size();k++){\n\t\t\tres_all[length_res_old+k]=res_tmp[k];\n\t\t}\n\t\tfor(unsigned int k=0;k<res_tree.size();k++){\n\t\t\tres_all[length_res_old+k+res_tree.size()+1]=tmp_nchildren[k];\n\t\t}\n\n\t\tdelete [] res_old;\n\t\tres_old=new int[length_res];\n\t\tfor(unsigned int k=0;k<length_res;k++){\n\t\t\tres_old[k]=res_all[k];\n\t\t}\n\n\t\tdelete [] res_all;\n\t\tif(i==(*npredn-1)){\n\n\t\t\tPROTECT(Rres = NEW_INTEGER(length_res));\n\t\t\tres = INTEGER_POINTER(Rres);\n\t\t\tfor(unsigned int k=0;k<length_res;k++){\n\t\t\t\tres[k]=res_old[k];\n\t\t\t}\n\t\t\tdelete [] res_old;\n\t\t}\n\n\t\t////////////////\n\t\t//erase old tree\n\t\t////////////////\n\n\t\tdelete [] tmp_nchildren;\n\t\tdelete [] res_tmp;\n\t\tres_tree.erase(res_tree.begin());\n\t}\n\tUNPROTECT(11);\n\n\treturn Rres;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the get_correlation_ensemble function?",
        "answer": "The get_correlation_ensemble function computes the correlation between two variables in a dataset. It takes as input the data array, an array indicating missing values, indices of the two variables, and the number of samples. It calculates the means of the variables, then computes the correlation using the formula for Pearson correlation coefficient."
      },
      {
        "question": "How does the returnConcordanceIndexC function handle stratification?",
        "answer": "The returnConcordanceIndexC function handles stratification by iterating through each unique stratum (ustrat). For each stratum, it creates subsets of the data (xs, cls, sts, ses, weightss) containing only the samples belonging to that stratum. It then computes the concordance index for each stratum separately and combines the results."
      },
      {
        "question": "What is the purpose of the bootstrap_tree function and how does it work?",
        "answer": "The bootstrap_tree function performs bootstrap resampling to assess the stability of the tree structure. It iterates through the leaf nodes of the tree, computes bootstrap estimates of the mean and standard deviation of the MRMR scores for each leaf, and then prunes branches of the tree that are not statistically significant based on these bootstrap estimates. This helps to reduce overfitting and identify the most robust features in the model."
      }
    ],
    "completion_tasks": [
      {
        "partial": "double get_correlation_ensemble(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_x = 0, mean_y = 0, nom = 0, den_x = 0, den_y = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x+i] == 0 && namat[ind_y+i] == 0) {\n            mean_x += data[ind_x+i];\n            mean_y += data[ind_y+i];\n        }\n    }\n\n    mean_x /= size;\n    mean_y /= size;\n\n    // Complete the function by calculating correlation\n}",
        "complete": "double get_correlation_ensemble(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_x = 0, mean_y = 0, nom = 0, den_x = 0, den_y = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x+i] == 0 && namat[ind_y+i] == 0) {\n            mean_x += data[ind_x+i];\n            mean_y += data[ind_y+i];\n        }\n    }\n\n    mean_x /= size;\n    mean_y /= size;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x+i] == 0 && namat[ind_y+i] == 0) {\n            double dx = data[ind_x+i] - mean_x;\n            double dy = data[ind_y+i] - mean_y;\n            nom += dx * dy;\n            den_x += dx * dx;\n            den_y += dy * dy;\n        }\n    }\n\n    return nom / sqrt(den_x * den_y);\n}"
      },
      {
        "partial": "void bootstrap_mrmr(double &mean, double &sd, double data[], int namat[], int size, int rep_boot, int size_boot, int nsamples, int var_target, int var_interest, int nprev_sel, int* var_ind, int *msurv, int *ustrat, int *cl2, double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU) {\n    int *ind = (int*) R_alloc(size_boot, sizeof(int));\n    double *boot_val = (double*) R_alloc(rep_boot, sizeof(double));\n    double *mat_info = (double*) R_alloc((size * size), sizeof(double));\n\n    for (int k = 0; k < rep_boot; ++k) {\n        // Select random samples\n        for (int i = 0; i < size_boot; ++i) {\n            ind[i] = (int)unif_rand() % nsamples;\n        }\n\n        // Initialize mat_info\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < size; ++j) {\n                mat_info[i + size * j] = 0;\n            }\n        }\n\n        // Complete the function by computing mrmr score\n    }\n\n    // Calculate mean and standard deviation\n}",
        "complete": "void bootstrap_mrmr(double &mean, double &sd, double data[], int namat[], int size, int rep_boot, int size_boot, int nsamples, int var_target, int var_interest, int nprev_sel, int* var_ind, int *msurv, int *ustrat, int *cl2, double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU) {\n    int *ind = (int*) R_alloc(size_boot, sizeof(int));\n    double *boot_val = (double*) R_alloc(rep_boot, sizeof(double));\n    double *mat_info = (double*) R_alloc((size * size), sizeof(double));\n\n    for (int k = 0; k < rep_boot; ++k) {\n        // Select random samples\n        for (int i = 0; i < size_boot; ++i) {\n            ind[i] = (int)unif_rand() % nsamples;\n        }\n\n        // Initialize mat_info\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < size; ++j) {\n                mat_info[i + size * j] = 0;\n            }\n        }\n\n        build_mim_cIndex_subset(mat_info, data, namat, size, nsamples, ind, size_boot, msurv, ustrat, cl2, st, se, weights, strat, N, outx, lenU);\n        boot_val[k] = mrnet_onegene(mat_info, size, nprev_sel, var_ind, var_target, var_interest);\n    }\n\n    // Calculate mean and standard deviation\n    mean = 0;\n    for (int i = 0; i < rep_boot; ++i) {\n        if (!isnan(boot_val[i])) {\n            mean += boot_val[i];\n        }\n    }\n    mean /= rep_boot;\n\n    sd = 0;\n    for (int i = 0; i < rep_boot; ++i) {\n        if (!isnan(boot_val[i])) {\n            sd += (boot_val[i] - mean) * (boot_val[i] - mean);\n        }\n    }\n    sd = sqrt(sd / rep_boot);\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/iauc.comp.R",
    "language": "R",
    "content": "`iauc.comp` <-\nfunction(auc1, auc2, time) {\n\tif((length(auc1) + length(auc2) + length(time)) != 3 * length(time)) { stop(\"auc1, auc2 and time must have the same length!\") }\n\tcc.ix <- complete.cases(auc1, auc2, time)\n\tauc1 <- auc1[cc.ix]\n\tauc2 <- auc2[cc.ix]\n\ttime <- time[cc.ix]\n\tdiffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n\tiauc1 <- sum(diffs * auc1) / max(time)\n\tiauc2 <- sum(diffs * auc2) / max(time)\n\trr <- wilcox.test(x=auc1, y=auc2, alternative=\"greater\", paired=TRUE, exact=FALSE)\n\treturn(list(\"p.value\"=rr$p.value, \"iauc1\"=iauc1, \"iauc2\"=iauc2))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `iauc.comp` function and what are its input parameters?",
        "answer": "The `iauc.comp` function compares two sets of AUC (Area Under the Curve) values over time. It takes three input parameters: `auc1` and `auc2` (two vectors of AUC values to be compared) and `time` (a vector of corresponding time points). The function calculates the integrated AUC for both sets and performs a statistical test to compare them."
      },
      {
        "question": "How does the function handle missing or incomplete data in the input vectors?",
        "answer": "The function handles missing or incomplete data by using the `complete.cases()` function. It creates an index `cc.ix` of complete cases across all three input vectors (auc1, auc2, and time). Then, it subsets all three vectors using this index, effectively removing any rows with missing data before performing calculations."
      },
      {
        "question": "What statistical test is used to compare the AUC values, and what does the function return?",
        "answer": "The function uses the Wilcoxon signed-rank test (implemented through `wilcox.test()`) to compare the AUC values. It's a paired test with the alternative hypothesis that `auc1` is greater than `auc2`. The function returns a list containing three elements: the p-value from the Wilcoxon test, the integrated AUC for the first set (iauc1), and the integrated AUC for the second set (iauc2)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "iauc.comp <- function(auc1, auc2, time) {\n  if((length(auc1) + length(auc2) + length(time)) != 3 * length(time)) {\n    stop(\"auc1, auc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(auc1, auc2, time)\n  auc1 <- auc1[cc.ix]\n  auc2 <- auc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  iauc1 <- sum(diffs * auc1) / max(time)\n  iauc2 <- sum(diffs * auc2) / max(time)\n  # Complete the function by adding the Wilcoxon test and return statement\n}",
        "complete": "iauc.comp <- function(auc1, auc2, time) {\n  if((length(auc1) + length(auc2) + length(time)) != 3 * length(time)) {\n    stop(\"auc1, auc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(auc1, auc2, time)\n  auc1 <- auc1[cc.ix]\n  auc2 <- auc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  iauc1 <- sum(diffs * auc1) / max(time)\n  iauc2 <- sum(diffs * auc2) / max(time)\n  rr <- wilcox.test(x=auc1, y=auc2, alternative=\"greater\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"iauc1\"=iauc1, \"iauc2\"=iauc2))\n}"
      },
      {
        "partial": "iauc.comp <- function(auc1, auc2, time) {\n  # Add input validation\n  # Filter out incomplete cases\n  # Calculate time differences\n  # Calculate integrated AUCs\n  # Perform Wilcoxon test\n  # Return results\n}",
        "complete": "iauc.comp <- function(auc1, auc2, time) {\n  if(length(auc1) != length(auc2) || length(auc1) != length(time)) {\n    stop(\"auc1, auc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(auc1, auc2, time)\n  auc1 <- auc1[cc.ix]\n  auc2 <- auc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  iauc1 <- sum(diffs * auc1) / max(time)\n  iauc2 <- sum(diffs * auc2) / max(time)\n  rr <- wilcox.test(x=auc1, y=auc2, alternative=\"greater\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"iauc1\"=iauc1, \"iauc2\"=iauc2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/class-signature.R",
    "language": "R",
    "content": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.ToxicoSig <- setClass('ToxicoSig', slots=list(\n            Arguments = \"list\",\n            tSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n#' ToxicoSig Constructor\n#'\n#' A user friendly constructor to create ToxicoSig class objects. This function\n#'   is implemented as an internal and should only be called for development purposes\n#'\n#' @param Data `array`` An array contiaining the data for constructing the ToxicoSig object\n#' @param tSetName `character(1)` The name of the tSet used in the constructor\n#' @param DateCreated `date` The data at time of running the constructor\n#' @param SigType `character`A string of the experiment type\n#' @param SessionInfo `sessionInfo`The current session info\n#' @param Call `character(1)` A string\n#' @param Arguments `list` A list of arguments passed to the constructor\n#'\n#' @return `object` A new ToxicoSig object\n#'\n#' @keywords internal\n#' @export\nToxicoSig <- function(Data=array(NA, dim=c(0,0,0)), tSetName='', DateCreated=date(), SigType='sensitivity', SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n  return(.ToxicoSig(Data, Arguments = Arguments, tSetName=tSetName, DateCreated=DateCreated, SigType=SigType, SessionInfo=SessionInfo, Call=Call))}\n\n#' Show ToxicoGx Signatures\n#'\n#' @examples\n#' data(TGGATESsmall)\n#' drug.perturbation <- drugPerturbationSig(TGGATESsmall, mDataType=\"rna\", nthread = 1, duration = \"2\",\n#'      drugs = head(treatmentNames(TGGATESsmall)), features = fNames(TGGATESsmall, \"rna\")[seq_len(2)])\n#' drug.perturbation\n#'\n#' @param object \\code{ToxicoSig}\n#'\n#' @return Prints the ToxicoGx Signatures object to the output stream, and returns invisible NULL.\n#'\n#' @export\nsetMethod(\"show\", signature=signature(object='ToxicoSig'),\n          function(object) {\n            cat('ToxicoSet Name: ', attr(object, 'PSetName'), \"\\n\")\n            cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n            cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n            cat(\"Number of Drugs: \", dim(object)[[2]], \"\\n\")\n            cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n          })\n\n#' Show the Annotations of a signature object\n#'\n#' This funtion prints out the information about the call used to compute the drug signatures, and the session info\n#' for the session in which the computation was done. Useful for determining the exact conditions used to generate signatures.\n#'\n#' @examples\n#' data(TGGATESsmall)\n#' drug.perturbation <- drugPerturbationSig(TGGATESsmall, mDataType=\"rna\", nthread=1, duration = \"2\",\n#'      drugs = head(treatmentNames(TGGATESsmall)), features = fNames(TGGATESsmall, \"rna\")[seq_len(2)])\n#' showSigAnnot(drug.perturbation)\n#'\n#' @param Sigs An object of the \\code{ToxicoSig} Class, as returned by \\code{drugPerturbationSig}\n#'\n#' @return Prints the ToxicoGx Signatures annotations to the output stream, and returns invisible NULL.\n#'\n#' @export\nshowSigAnnot <- function(Sigs){\n\n  print(Sigs@Call)\n  print(Sigs@SessionInfo)\n  return(invisible(NULL))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ToxicoSig` class and its constructor function?",
        "answer": "The `ToxicoSig` class is designed to store and manage toxicogenomic signature data. Its constructor function creates a new `ToxicoSig` object with specified attributes such as data, tSet name, creation date, signature type, session info, and call information. This allows for organized storage and manipulation of toxicogenomic data within the R environment."
      },
      {
        "question": "How does the `show` method for the `ToxicoSig` class work, and what information does it display?",
        "answer": "The `show` method for the `ToxicoSig` class is implemented using `setMethod`. When called on a `ToxicoSig` object, it displays key information about the object, including the ToxicoSet name, signature type, creation date, number of drugs, and number of genes/probes. This provides a quick summary of the object's contents without revealing the entire dataset."
      },
      {
        "question": "What is the purpose of the `showSigAnnot` function, and how does it differ from the `show` method?",
        "answer": "The `showSigAnnot` function is designed to display detailed annotations of a `ToxicoSig` object. Unlike the `show` method, which provides a summary, `showSigAnnot` prints out the specific call used to compute the drug signatures and the session information in which the computation was done. This function is useful for determining the exact conditions used to generate signatures, aiding in reproducibility and debugging."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.ToxicoSig <- setClass('ToxicoSig', slots=list(\n            Arguments = \"list\",\n            tSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\nToxicoSig <- function(Data=array(NA, dim=c(0,0,0)), tSetName='', DateCreated=date(), SigType='sensitivity', SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n  # Complete the function body\n}",
        "complete": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.ToxicoSig <- setClass('ToxicoSig', slots=list(\n            Arguments = \"list\",\n            tSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\nToxicoSig <- function(Data=array(NA, dim=c(0,0,0)), tSetName='', DateCreated=date(), SigType='sensitivity', SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n  return(.ToxicoSig(Data, Arguments = Arguments, tSetName=tSetName, DateCreated=DateCreated, SigType=SigType, SessionInfo=SessionInfo, Call=Call))\n}"
      },
      {
        "partial": "setMethod(\"show\", signature=signature(object='ToxicoSig'),\n          function(object) {\n            # Complete the function body\n          })",
        "complete": "setMethod(\"show\", signature=signature(object='ToxicoSig'),\n          function(object) {\n            cat('ToxicoSet Name: ', attr(object, 'PSetName'), \"\\n\")\n            cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n            cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n            cat(\"Number of Drugs: \", dim(object)[[2]], \"\\n\")\n            cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n          })"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/censor.time.R",
    "language": "R",
    "content": "`censor.time` <-\nfunction(surv.time, surv.event, time.cens=0) {\n\tstc <- surv.time\n   sec <- surv.event\n   cc.ix <- complete.cases(stc, sec)\n   if(time.cens != 0) { \n   \tstc[cc.ix][surv.time[cc.ix] > time.cens] <- time.cens\n   \tsec[cc.ix][surv.time[cc.ix] > time.cens] <- 0\n   }\n   return(list(\"surv.time.cens\"=stc, \"surv.event.cens\"=sec))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `censor.time` function in R?",
        "answer": "The `censor.time` function is used to apply right censoring to survival data. It takes survival times and event indicators as input, and optionally a censoring time. If a censoring time is provided, it modifies the survival times and event indicators for observations that exceed this time, effectively censoring the data at that point."
      },
      {
        "question": "How does the function handle missing data in the input vectors?",
        "answer": "The function uses `complete.cases(stc, sec)` to identify rows where both survival time and event indicator are non-missing. This creates an index `cc.ix` which is then used to apply censoring only to complete cases, effectively ignoring any rows with missing data in either input vector."
      },
      {
        "question": "What does the function return, and in what format?",
        "answer": "The function returns a list containing two elements: 'surv.time.cens' and 'surv.event.cens'. These are the censored versions of the input survival times and event indicators, respectively. The return format allows easy access to both censored vectors while keeping them associated."
      }
    ],
    "completion_tasks": [
      {
        "partial": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  stc <- surv.time\n  sec <- surv.event\n  cc.ix <- complete.cases(stc, sec)\n  if(time.cens != 0) {\n    # Complete the censoring logic here\n  }\n  return(list(\"surv.time.cens\"=stc, \"surv.event.cens\"=sec))\n}",
        "complete": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  stc <- surv.time\n  sec <- surv.event\n  cc.ix <- complete.cases(stc, sec)\n  if(time.cens != 0) {\n    cens_idx <- cc.ix & surv.time > time.cens\n    stc[cens_idx] <- time.cens\n    sec[cens_idx] <- 0\n  }\n  return(list(\"surv.time.cens\"=stc, \"surv.event.cens\"=sec))\n}"
      },
      {
        "partial": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  # Initialize variables and handle censoring\n  # Return the censored data\n}",
        "complete": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  stc <- surv.time\n  sec <- surv.event\n  if(time.cens != 0) {\n    cc.ix <- complete.cases(stc, sec)\n    cens_idx <- which(cc.ix & surv.time > time.cens)\n    stc[cens_idx] <- time.cens\n    sec[cens_idx] <- 0\n  }\n  list(surv.time.cens=stc, surv.event.cens=sec)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/sbrier.score2proba.R",
    "language": "R",
    "content": "`sbrier.score2proba` <-\nfunction(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n\t## require(ipred)\n\tmethod <- match.arg(method)\n\t## remove missing values and sort the data for the test set\n\tcc.ix <- complete.cases(data.ts)\n\tot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n\tdata.ts <- data.ts[ot, ,drop=FALSE]\n\tsurv.time.ts <- data.ts$time\n\tsurv.event.ts <- data.ts$event\n\tscore.ts <- data.ts$score\n\tbtime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n\tutime <- unique(surv.time.ts[surv.event.ts == 1])\n\tbsc <- rep(NA, length(btime))\n\tswitch(method,\n\t\"cox\"={\n\t\t##require(survival)\n\t\t## fit the cox model for the training set\n\t\tcoxm <- survival::coxph(Surv(time, event) ~ score, data=data.tr)\n\t\t## compute survival probabilities using the cox model fitted on the training set and the score from the test set\n\t\t#sf <- survfit(coxm, newdata=data.ts)\n\t\tdd <- data.frame(\"score\"=score.ts)\n\t\tsf <- survfit(coxm, newdata=dd)\n\t\tfor(i in 1:length(utime)) {\n\t\t\tmypred <- getsurv2(sf=sf, time=utime[i])\n\t\t\tbsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n\t\t}\t\n\t},\n\t\"prodlim\"={\n\t\t#require(KernSmooth)\n\t\tprodlim.m <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr)\n\t\tlpred <- predict(prodlim.m, newdata=data.ts, times=utime)\n\t\tnames(lpred) <- dimnames(data.ts)[[1]]\n\t\tbsc <- rep(NA, length(btime))\n\t\tfor(i in 1:length(utime)) {\n\t\t\tmypred <- unlist(lapply(lpred, function(x, ix) { return(x[[ix]]) }, ix=i))\n\t\t\tbsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n\t\t}\n\t})\n\tif(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n\tdiffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n\tbsc.int <- sum(diffs * bsc)/max(btime)\n\treturn(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `sbrier.score2proba` function in this R code?",
        "answer": "The `sbrier.score2proba` function calculates the Brier score for survival data using either Cox proportional hazards model or product-limit estimation. It takes training and test datasets as input, computes survival probabilities, and returns time-dependent Brier scores along with an integrated Brier score."
      },
      {
        "question": "How does the function handle missing values in the test dataset?",
        "answer": "The function handles missing values in the test dataset by first removing incomplete cases using `complete.cases(data.ts)`. It then sorts the remaining data by time and adjusts the indices accordingly. This ensures that only complete cases are used in the subsequent calculations."
      },
      {
        "question": "What are the two methods available for calculating survival probabilities, and how are they implemented?",
        "answer": "The two methods available are 'cox' and 'prodlim'. For the 'cox' method, it fits a Cox proportional hazards model using the training data and computes survival probabilities for the test set using this model. For the 'prodlim' method, it uses product-limit estimation to calculate survival probabilities. The choice between these methods is made using the `method` argument and implemented through a `switch` statement."
      }
    ],
    "completion_tasks": [
      {
        "partial": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  # Complete the function body here\n  \n  if(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n  diffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n  bsc.int <- sum(diffs * bsc)/max(btime)\n  return(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}",
        "complete": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  switch(method,\n    \"cox\"={\n      coxm <- survival::coxph(Surv(time, event) ~ score, data=data.tr)\n      dd <- data.frame(\"score\"=score.ts)\n      sf <- survfit(coxm, newdata=dd)\n      for(i in 1:length(utime)) {\n        mypred <- getsurv2(sf=sf, time=utime[i])\n        bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n      }\n    },\n    \"prodlim\"={\n      prodlim.m <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr)\n      lpred <- predict(prodlim.m, newdata=data.ts, times=utime)\n      names(lpred) <- dimnames(data.ts)[[1]]\n      for(i in 1:length(utime)) {\n        mypred <- unlist(lapply(lpred, function(x, ix) { return(x[[ix]]) }, ix=i))\n        bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n      }\n    }\n  )\n  \n  if(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n  diffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n  bsc.int <- sum(diffs * bsc)/max(btime)\n  return(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}"
      },
      {
        "partial": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  if(method == \"cox\") {\n    # Complete the cox method here\n  } else {\n    # Complete the prodlim method here\n  }\n  \n  # Complete the final calculations here\n}",
        "complete": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  if(method == \"cox\") {\n    coxm <- survival::coxph(Surv(time, event) ~ score, data=data.tr)\n    dd <- data.frame(\"score\"=score.ts)\n    sf <- survfit(coxm, newdata=dd)\n    for(i in 1:length(utime)) {\n      mypred <- getsurv2(sf=sf, time=utime[i])\n      bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n    }\n  } else {\n    prodlim.m <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr)\n    lpred <- predict(prodlim.m, newdata=data.ts, times=utime)\n    names(lpred) <- dimnames(data.ts)[[1]]\n    for(i in 1:length(utime)) {\n      mypred <- unlist(lapply(lpred, function(x, ix) { return(x[[ix]]) }, ix=i))\n      bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n    }\n  }\n  \n  if(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n  diffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n  bsc.int <- sum(diffs * bsc)/max(btime)\n  return(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/drugTimeResponseCurve.R",
    "language": "R",
    "content": "#' Compares viabilities at a given dose over different experimental durations\n#'\n#' This function generates a plot visualizing the relationship between gene\n#'   expression, time and dose level for the selected tSet. The plot is generated\n#'   with ggplot2 and can be customized using ggplot plot + function() syntax.\n#'\n#' @examples\n#'   library(ggplot2)\n#'\n#'   # Default settings\n#'   plot <- drugTimeResponseCurve(TGGATESsmall, cell_lines = \"Hepatocyte\",\n#'   dose = c(\"Control\", \"Low\", \"Middle\"), drugs = treatmentNames(TGGATESsmall)[6],\n#'   duration = c(\"2\", \"8\", \"24\"))\n#'\n#'   # Customize title, x/y labels, x/y limits, colour palette and define\n#'   # custom ticks for x axis using the function argument ggplot2_args\n#'   customizations <- list(labs(title= 'My Custom Title', ylab = 'The y-axis'),\n#'                          xlim(c(2, 24)), ylim(c(99,105)),\n#'                          scale_color_brewer(palette=\"Set1\"),\n#'                          scale_x_continuous(breaks=c(2, 8, 24),\n#'                            labels = c(\"Two\", \"Eight\", \"Twenty-Four\"))\n#'                          )\n#'\n#'    if(interactive()) {\n#'       drugTimeResponseCurve(TGGATESsmall, cell_lines = \"Hepatocyte\",\n#'         dose = c(\"Control\", \"Low\", \"Middle\"),\n#'         drugs = treatmentNames(TGGATESsmall)[6], duration = c(\"2\", \"8\", \"24\"),\n#'         ggplot_args = customizations)\n#'    }\n#'\n#'    # Customize the plot using standard ggplot2 syntax\n#'    if(interactive()) {\n#'       plot + labs(title= 'My Custom Title', ylab = 'The y-axis') +\n#'         xlim(c(2, 24)) + ylim(c(99,105)) + scale_color_brewer(palette=\"Set1\")\n#'    }\n#'\n#' @param tSet \\code{ToxicoSet} A ToxicoSet to be plotted in\n#'   this figure\n#' @param dose \\code{character} A vector of dose levels to be included in the\n#'   plot. Default to include all dose levels available for a drug. Must include\n#'   at minimum two dose levels, one of witch is \"Control\".\n#' @param drugs \\code{character} A drugs or pair of drugs to be plotted.\n#' @param duration \\code{character} A vector of durations to include in the plot.\n#' @param summarize_replicates \\code{logical} If TRUE will average viability\n#'   across replicates for each unique drug-dose-duration combination.\n#' @param cell_lines \\code{character} A vector of cell lines to include in the\n#'   plot.\n#' @param line_width \\code{numeric} A number specifying the thickness of lines\n#'   in the plot, as passed to size in geom_line(). Defaults to 1.\n#' @param point_size \\code{numeric} A number specifying how large points should\n#'   be in the plot, as passed to size in geom_point(). Defaults to 2.5.\n#' @param verbose \\code{boolean} Should warning messages about the data passed\n#'   in be printed?\n#' @param ggplot_args \\code{list} A list of ggplot2 functions which can be\n#'   called using the plot + function() syntax. This allows arbitrary\n#'   customization of the plot including changing the title, axis labels,\n#'   colours, etc. Please see the included examples for basic usage or ggplot2\n#'   documentation for advanced customization. Alternatively, you could assign\n#'   the return value to a variable and add the customization yourself using\n#'   plot + function().\n#'\n#' @return Plot of the viabilities for each drugs vs time of exposure\n#'\n#' @import ggplot2\n#' @importFrom magrittr %<>%\n#' @importFrom dplyr %>% filter group_by mutate\n#' @importFrom tidyr gather\n#'\n#' @export\ndrugTimeResponseCurve <- function(\n  tSet,\n  duration = NULL,\n  cell_lines = NULL,\n  dose = NULL,\n  drugs = NULL,\n  summarize_replicates = TRUE,\n  line_width = 1,\n  point_size = 2.5,\n  verbose=TRUE,\n  ggplot_args=NULL\n) {\n  # Place tSet in a list if not already\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n\n  paramErrorChecker(\"drugTimeResponseCurve\",\n                    tSets = tSet, drugs = drugs, duration = duration,\n                    cell_lines = cell_lines, dose = dose)\n\n  ## TODO:: Throw warning if a dose level or time point is not available for\n    # a specific drug\n\n  # Subsetting the tSet based on parameter arguments\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs,\n                               duration = duration, cells = cell_lines)})\n  })\n\n  # Gather data for the plot\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  for (data in plotData) {\n    if (summarize_replicates) {\n      data %<>% group_by(dose_level, duration_h) %>% mutate(viability = mean(viability))\n      plot <- ggplot(as_tibble(data) %>% filter(replicate == 1),\n                     aes(as.numeric(duration_h), viability, color = dose_level)) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    } else {\n      plot <- ggplot(as_tibble(data), aes(as.numeric(duration_h), viability,\n                                          color = dose_level,\n                                          shape = as.factor(replicate),\n                                          linetype = as.factor(replicate))) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    }\n  }\n\n  plot <- plot + labs(\n    title = paste0(\"Drug Response Curve for \",\n                   paste(drugs, collapse = \" & \"), \" in \",\n                   paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n    color = \"Dose Level\",\n    shape = \"Replicate\"\n  ) +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    ) +\n    xlab(\"Duration (hrs)\") +\n    ylab(\"Viability (%)\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  # Pass in any additional ggplot2 customizations\n  if (!(is.null(ggplot_args))) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `drugTimeResponseCurve` function, and what type of visualization does it generate?",
        "answer": "The `drugTimeResponseCurve` function generates a plot that visualizes the relationship between gene expression, time, and dose level for a selected ToxicoSet (tSet). It creates a line plot using ggplot2 to show how viability changes over different experimental durations for various dose levels of a drug or pair of drugs."
      },
      {
        "question": "How does the function handle data for multiple replicates when `summarize_replicates` is set to TRUE?",
        "answer": "When `summarize_replicates` is set to TRUE, the function averages the viability across replicates for each unique drug-dose-duration combination. It does this by grouping the data by dose_level and duration_h, then calculating the mean viability. The plot is then generated using only the first replicate's data points, as they now represent the average values."
      },
      {
        "question": "What customization options does the function provide for the generated plot, and how can users apply these customizations?",
        "answer": "The function provides several customization options: 1) Users can adjust line width and point size using the `line_width` and `point_size` parameters. 2) The `ggplot_args` parameter allows passing a list of ggplot2 functions for further customization (e.g., changing title, axis labels, colors). 3) Users can also customize the plot after it's generated using standard ggplot2 syntax by assigning the return value to a variable and adding customizations with the + operator."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  # Complete the function by adding the plotting logic\n}",
        "complete": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  for (data in plotData) {\n    if (summarize_replicates) {\n      data %<>% group_by(dose_level, duration_h) %>% mutate(viability = mean(viability))\n      plot <- ggplot(as_tibble(data) %>% filter(replicate == 1),\n                     aes(as.numeric(duration_h), viability, color = dose_level)) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    } else {\n      plot <- ggplot(as_tibble(data), aes(as.numeric(duration_h), viability,\n                                          color = dose_level,\n                                          shape = as.factor(replicate),\n                                          linetype = as.factor(replicate))) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    }\n  }\n\n  plot <- plot + labs(\n    title = paste0(\"Drug Response Curve for \",\n                   paste(drugs, collapse = \" & \"), \" in \",\n                   paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n    color = \"Dose Level\",\n    shape = \"Replicate\"\n  ) +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    ) +\n    xlab(\"Duration (hrs)\") +\n    ylab(\"Viability (%)\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  if (!(is.null(ggplot_args))) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}"
      },
      {
        "partial": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  # Place tSet in a list if not already\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  # Subsetting the tSet based on parameter arguments\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  # Gather data for the plot\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  # Complete the function by adding the plotting logic and returning the plot\n}",
        "complete": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  for (data in plotData) {\n    if (summarize_replicates) {\n      data %<>% group_by(dose_level, duration_h) %>% mutate(viability = mean(viability))\n      plot <- ggplot(as_tibble(data) %>% filter(replicate == 1),\n                     aes(as.numeric(duration_h), viability, color = dose_level)) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    } else {\n      plot <- ggplot(as_tibble(data), aes(as.numeric(duration_h), viability,\n                                          color = dose_level,\n                                          shape = as.factor(replicate),\n                                          linetype = as.factor(replicate))) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    }\n  }\n\n  plot <- plot + labs(\n    title = paste0(\"Drug Response Curve for \",\n                   paste(drugs, collapse = \" & \"), \" in \",\n                   paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n    color = \"Dose Level\",\n    shape = \"Replicate\"\n  ) +\n    theme(plot.title = element_text(hjust = 0.5, size = 14)) +\n    xlab(\"Duration (hrs)\") +\n    ylab(\"Viability (%)\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  if (!(is.null(ggplot_args))) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/stab.fs.R",
    "language": "R",
    "content": "#' @title Function to quantify stability of feature selection\n#'\n#' @description\n#' This function computes several indexes to quantify feature selection\n#'   stability. This is usually estimated through perturbation of the original\n#'   dataset by generating multiple sets of selected features.\n#'\n#' @usage\n#' stab.fs(fsets, N, method = c(\"kuncheva\", \"davis\"), ...)\n#'\n#' @param fsets\tlist of sets of selected features, each set of selected\n#'   features may have different size.\n#' @param N\ttotal number of features on which feature selection is performed.\n#' @param method\tstability index (see details section).\n#' @param ...\tadditional parameters passed to stability index (penalty\n#'   that is a numeric for Davis' stability index, see details section).\n#'\n#' @details\n#' Stability indices may use different parameters. In this version only the\n#'   Davis index requires an additional parameter that is penalty, a numeric\n#'   value used as penalty term.\n#' Kuncheva index (kuncheva) lays in \\[-1, 1\\], An index of -1 means no\n#'   intersection between sets of selected features, +1 means that all the\n#'   same features are always selected and 0 is the expected stability of a\n#'   random feature selection.\n#' Davis index (davis) lays in \\[0,1\\], With a penalty term equal to 0, an\n#'   index of 0 means no intersection between sets of selected features\n#'   and +1 means that all the same features are always selected. A penalty\n#'   of 1 is usually used so that a feature selection performed with no or\n#'   all features has a Davis stability index equals to 0. None estimate of\n#'   the expected Davis stability index of a random feature selection was\n#'   published.\n#'\n#' @return\n#' A numeric that is the stability index.\n#'\n#' @references\n#' Davis CA, Gerick F, Hintermair V, Friedel CC, Fundel K, Kuffner R, Zimmer R\n#'   (2006) \"Reliable gene signatures for microarray classification: assessment\n#'   of stability and performance\", Bioinformatics, 22(19):356-2363.\n#' Kuncheva LI (2007) \"A stability index for feature selection\", AIAP'07:\n#'   Proceedings of the 25th conference on Proceedings of the 25th IASTED\n#'   International Multi-Conference, pages 390-395.\n#'\n#' @seealso\n#' [genefu::stab.fs.ranking]\n#'\n#' @examples\n#' set.seed(54321)\n#' # 100 random selection of 50 features from a set of 10,000 features\n#' fsets <- lapply(as.list(1:100), function(x, size=50, N=10000) {\n#'   return(sample(1:N, size, replace=FALSE))} )\n#' names(fsets) <- paste(\"fsel\", 1:length(fsets), sep=\".\")\n#'\n#' # Kuncheva index\n#' stab.fs(fsets=fsets, N=10000, method=\"kuncheva\")\n#' # close to 0 as expected for a random feature selection\n#'\n#' # Davis index\n#' stab.fs(fsets=fsets, N=10000, method=\"davis\", penalty=1)\n#'\n#' @md\n#' @export\nstab.fs <-\nfunction(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n\n\t####################\n\t## internal functions\n\t####################\n\n\tkuncheva.stab <- function(fsets, N) {\n\t\tkk <- length(fsets)\n\t\tKI <- function(f1, f2, ss, NN) {\n\t\t\t#if(length(f1) != length(f2)) { stop(\"length of the two sets of selected features must be identical!\") }\n\t\t\t#ss <- length(f1)\n\t\t\tif(ss == NN) { return(NA) }\n\t\t\trr <- length(intersect(f1, f2))\n\t\t\tki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n\t\t\treturn(ki.est)\n\t\t}\n\t\tss <- unique(unlist(lapply(fsets, length)))\n\t\tif(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n\t\tstab.res <- 0\n\t\tfor(i in 1:(kk - 1)) {\n\t\t\tfor(j in (i + 1):kk) {\n\t\t\t\tstab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n\t\t\t}\n\t\t}\n\t\treturn((2 * stab.res) / (kk * (kk - 1)))\n\t}\n\n\tdavis.stab <- function(fsets, N, penalty=1) {\n\t\tkk <- length(fsets)\n\t\tss <- unique(unlist(lapply(fsets, length)))\n\t\tif(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n\t\tstab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n\t\treturn(stab.res - penalty * (ss / N))\n\t}\n\n\t####################\n\n\tmethod <- match.arg(method)\n\tif(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n\tswitch(method,\n\t\t\"kuncheva\"={\n\t\t\tstab <- kuncheva.stab(fsets=fsets, N=N)\n\t\t},\n\t\t\"davis\"={\n\t\t\t stab <- davis.stab(fsets=fsets, N=N, ...)\n\t\t})\n\t\treturn(stab)\n}\n\n## k <- 1000; fsets <- NULL; for(i in 1:k) { fsets <- c(fsets, list(sample(paste(\"feature\", 1:10000, sep=\".\"), 200))) }; names(fsets) <- paste(\"rand\", 1:k, sep=\".\")\n## kuncheva.stab(fsets=fsets, N=10000)\n## davis.stab(fsets=fsets, N=10000, penalty=1)",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `stab.fs` function and what are the two stability indices it can calculate?",
        "answer": "The `stab.fs` function quantifies the stability of feature selection. It can calculate two stability indices: Kuncheva index and Davis index. The Kuncheva index ranges from -1 to 1, where -1 indicates no intersection between sets of selected features, 1 means the same features are always selected, and 0 is the expected stability of random feature selection. The Davis index ranges from 0 to 1, where 0 (with penalty term 0) means no intersection between sets, and 1 means the same features are always selected."
      },
      {
        "question": "How does the function handle different lengths of feature sets for the Kuncheva index calculation?",
        "answer": "The function checks if all feature sets have the same length using `ss <- unique(unlist(lapply(fsets, length)))`. If `length(ss) > 1`, it throws an error with the message 'length of sets of selected features must be identical!'. This ensures that the Kuncheva index is only calculated for feature sets of equal length, which is a requirement for this stability measure."
      },
      {
        "question": "What is the significance of the `penalty` parameter in the Davis stability index calculation?",
        "answer": "The `penalty` parameter in the Davis stability index calculation is used to adjust the index based on the number of selected features relative to the total number of features. It's subtracted from the initial stability calculation as `penalty * (ss / N)`, where `ss` is the number of selected features and `N` is the total number of features. A penalty of 1 is typically used to ensure that feature selections with no features or all features result in a Davis stability index of 0. This penalization helps to discourage trivial feature selections."
      }
    ],
    "completion_tasks": [
      {
        "partial": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    kk <- length(fsets)\n    KI <- function(f1, f2, ss, NN) {\n      if(ss == NN) { return(NA) }\n      rr <- length(intersect(f1, f2))\n      ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n      return(ki.est)\n    }\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- 0\n    for(i in 1:(kk - 1)) {\n      for(j in (i + 1):kk) {\n        stab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n      }\n    }\n    return((2 * stab.res) / (kk * (kk - 1)))\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    # Complete this function\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}",
        "complete": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    kk <- length(fsets)\n    KI <- function(f1, f2, ss, NN) {\n      if(ss == NN) { return(NA) }\n      rr <- length(intersect(f1, f2))\n      ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n      return(ki.est)\n    }\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- 0\n    for(i in 1:(kk - 1)) {\n      for(j in (i + 1):kk) {\n        stab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n      }\n    }\n    return((2 * stab.res) / (kk * (kk - 1)))\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    kk <- length(fsets)\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n    return(stab.res - penalty * (ss / N))\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}"
      },
      {
        "partial": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    # Complete this function\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    kk <- length(fsets)\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n    return(stab.res - penalty * (ss / N))\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}",
        "complete": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    kk <- length(fsets)\n    KI <- function(f1, f2, ss, NN) {\n      if(ss == NN) { return(NA) }\n      rr <- length(intersect(f1, f2))\n      ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n      return(ki.est)\n    }\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- 0\n    for(i in 1:(kk - 1)) {\n      for(j in (i + 1):kk) {\n        stab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n      }\n    }\n    return((2 * stab.res) / (kk * (kk - 1)))\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    kk <- length(fsets)\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n    return(stab.res - penalty * (ss / N))\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/combine.test.R",
    "language": "R",
    "content": "'combine.test' <-\nfunction(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n\tif(hetero) { stop(\"function to deal with heterogeneity is not implemented yet!\") }\n\tmethod <- match.arg(method)\n\tna.ix <- is.na(p)\n\tif(any(na.ix) && !na.rm) { stop(\"missing values are present!\") }\n\tif(all(na.ix)) { return(NA) } ## all p-values are missing\n\tp <- p[!na.ix]\n\tk <- length(p)\n\tif(k == 1) { return(p) }\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tswitch(method,  \n\t\"fisher\"={\n\t\tcp <- pchisq(-2 * sum(log(p)), df=2*k, lower.tail=FALSE)\n\t}, \n\t\"z.transform\"={\n\t\tz <- qnorm(p, lower.tail=FALSE)\n\t\tcp <- pnorm(sum(weight * z) / sqrt(sum(weight^2)), lower.tail=FALSE)\n\t}, \n\t\"logit\"={\n\t\ttt <- (- sum(log(p / (1 - p)))) / sqrt(k * pi^2 * (5 * k + 2) / (3 * (5 * k + 4)))\n\t\tcp <- pt(tt,df=5*k+4, lower.tail=FALSE)\n\t})\n\treturn(cp)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'combine.test' function in R, and what are the different methods it supports for combining p-values?",
        "answer": "The 'combine.test' function in R is designed to combine multiple p-values into a single p-value. It supports three methods for combining p-values: Fisher's method, Z-transform method, and logit method. These methods are specified using the 'method' parameter, which can be set to 'fisher', 'z.transform', or 'logit'."
      },
      {
        "question": "How does the function handle missing values (NA) in the input p-values?",
        "answer": "The function handles missing values (NA) in the input p-values as follows: 1) If any NA values are present and na.rm=FALSE (default), the function stops with an error message. 2) If na.rm=TRUE, the function removes NA values before processing. 3) If all p-values are NA, the function returns NA. 4) The function uses logical indexing to exclude NA values from further calculations."
      },
      {
        "question": "Explain the implementation of the Z-transform method in the 'combine.test' function.",
        "answer": "The Z-transform method in the 'combine.test' function is implemented as follows: 1) It converts p-values to Z-scores using the inverse normal distribution (qnorm function). 2) It calculates a weighted sum of these Z-scores. 3) It divides this sum by the square root of the sum of squared weights. 4) Finally, it converts the result back to a p-value using the standard normal distribution (pnorm function). This method allows for weighted combination of p-values, with weights specified by the 'weight' parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  if(hetero) { stop(\"function to deal with heterogeneity is not implemented yet!\") }\n  method <- match.arg(method)\n  na.ix <- is.na(p)\n  if(any(na.ix) && !na.rm) { stop(\"missing values are present!\") }\n  if(all(na.ix)) { return(NA) }\n  p <- p[!na.ix]\n  k <- length(p)\n  if(k == 1) { return(p) }\n  if(missing(weight)) { weight <- rep(1, k) }\n  switch(method,\n    \"fisher\"={\n      # Complete the Fisher's method calculation\n    },\n    \"z.transform\"={\n      # Complete the Z-transform method calculation\n    },\n    \"logit\"={\n      # Complete the logit method calculation\n    })\n  return(cp)\n}",
        "complete": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  if(hetero) { stop(\"function to deal with heterogeneity is not implemented yet!\") }\n  method <- match.arg(method)\n  na.ix <- is.na(p)\n  if(any(na.ix) && !na.rm) { stop(\"missing values are present!\") }\n  if(all(na.ix)) { return(NA) }\n  p <- p[!na.ix]\n  k <- length(p)\n  if(k == 1) { return(p) }\n  if(missing(weight)) { weight <- rep(1, k) }\n  switch(method,\n    \"fisher\"={\n      cp <- pchisq(-2 * sum(log(p)), df=2*k, lower.tail=FALSE)\n    },\n    \"z.transform\"={\n      z <- qnorm(p, lower.tail=FALSE)\n      cp <- pnorm(sum(weight * z) / sqrt(sum(weight^2)), lower.tail=FALSE)\n    },\n    \"logit\"={\n      tt <- (-sum(log(p / (1 - p)))) / sqrt(k * pi^2 * (5 * k + 2) / (3 * (5 * k + 4)))\n      cp <- pt(tt, df=5*k+4, lower.tail=FALSE)\n    })\n  return(cp)\n}"
      },
      {
        "partial": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  # Add input validation and preprocessing\n  \n  # Implement the chosen method\n  cp <- switch(method,\n    \"fisher\"={\n      # Fisher's method\n    },\n    \"z.transform\"={\n      # Z-transform method\n    },\n    \"logit\"={\n      # Logit method\n    })\n  \n  return(cp)\n}",
        "complete": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  if(hetero) stop(\"function to deal with heterogeneity is not implemented yet!\")\n  method <- match.arg(method)\n  na.ix <- is.na(p)\n  if(any(na.ix) && !na.rm) stop(\"missing values are present!\")\n  if(all(na.ix)) return(NA)\n  p <- p[!na.ix]\n  k <- length(p)\n  if(k == 1) return(p)\n  if(missing(weight)) weight <- rep(1, k)\n  \n  cp <- switch(method,\n    \"fisher\" = pchisq(-2 * sum(log(p)), df=2*k, lower.tail=FALSE),\n    \"z.transform\" = {\n      z <- qnorm(p, lower.tail=FALSE)\n      pnorm(sum(weight * z) / sqrt(sum(weight^2)), lower.tail=FALSE)\n    },\n    \"logit\" = {\n      tt <- (-sum(log(p / (1 - p)))) / sqrt(k * pi^2 * (5 * k + 2) / (3 * (5 * k + 4)))\n      pt(tt, df=5*k+4, lower.tail=FALSE)\n    })\n  \n  return(cp)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/npi.R",
    "language": "R",
    "content": "#' @title Function to compute the Nottingham Prognostic Index\n#'\n#' @description\n#' This function computes the Nottingham Prognostic Index (NPI) as published\n#'   in Galeat et al, 1992. NPI is a clinical index shown to be highly prognostic \n#'   in breast cancer.\n#'\n#' @usage\n#' npi(size, grade, node, na.rm = FALSE)\n#'\n#' @param size tumor size in cm.\n#' @param grade\tHistological grade, i.e. low (1), intermediate (2) and high (3) grade.\n#' @param node Nodal status. If only binary nodal status (0/1) is available, \n#'   map 0 to 1 and 1 to 3.\n#' @param na.rm\tTRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @details\n#' The risk prediction is either Good if score < 3.4, Intermediate \n#'   if 3.4 <= score <- 5.4, or Poor if score > 5.4.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n\n\n#' @references\n#' Galea MH, Blamey RW, Elston CE, and Ellis IO (1992) \"The nottingham \n#'   prognostic index in primary breast cancer\", Breast Cancer Reasearch \n#'   and Treatment, 22(3):207-219.\n#'\n#' @seealso\n#' [genefu::st.gallen]\n#'\n#' @examples\n#' # load NKI dataset\n#' data(nkis)\n#' # compute NPI score and risk classification\n#' npi(size=demo.nkis[ ,\"size\"], grade=demo.nkis[ ,\"grade\"],\n#'   node=ifelse(demo.nkis[ ,\"node\"] == 0, 1, 3), na.rm=TRUE)\n#' \n#' @md\n#' @export\nnpi <-\nfunction(size, grade, node, na.rm=FALSE) {\n\n\tnn <- names(size)\n\tif(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n\tcc.ix <- complete.cases(size, grade, node)\n\tif(all(!cc.ix)) {\n\t\ttt <- rep(NA, length(size))\n\t\tnames(tt) <- nn\n\t\treturn(list(\"score\"=tt, \"risk\"=tt))\n\t}\n\tsize <- size[cc.ix]\n\tgrade <- grade[cc.ix]\n\tnode <- node[cc.ix]\n\t\n\tif(length(size) != length(grade) || length(grade) != length(node)) {\n\t\tstop(\"size, grade and lymph node stage must have the same length!\")\n\t}\n\tif(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n\tif(!all(is.element(grade, c(\"1\", \"2\", \"3\")))) {\n\t\tstop(\"grade must be 1, 2 or 3!\")\n\t}\n\tif(!all(is.element(node, c(\"1\", \"2\", \"3\")))) {\n\t\t#if only \"0\" and \"1\" are available, map \"0\" -> \"1\" and \"1\" -> \"3\"\n\t\tstop(\"lymph node stage must be 1, 2 or 3!\")\n\t}\n\tif(!is.numeric(size)) {\n\t\tstop(\"tumor size (cm) must be numeric!\")\n\t}\n\t\n\tnpi <- 0.2 * size + grade + node\n\tnames(npi) <- nn[cc.ix]\n\t\n\tnpi.score <- rep(NA, length(cc.ix))\n\tnames(npi.score) <- nn\n\tnpi.score[names(npi)] <- npi\n\t\n\tnpi.c <- npi\n\tnpi.c[npi < 3.4] <- \"Good\"\n\tnpi.c[npi > 5.4] <- \"Poor\"\n\tnpi.c[npi >= 3.4 & npi <= 5.4] <- \"Intermediate\"\n\t\n\tnpi.classif <- rep(NA, length(cc.ix))\n\tnames(npi.classif) <- nn\n\tnpi.classif[names(npi.c)] <- npi.c\n\t#npi.classif <- as.factor(npi.classif)\n\t\n\treturn(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'npi' function and what does it calculate?",
        "answer": "The 'npi' function calculates the Nottingham Prognostic Index (NPI) for breast cancer prognosis. It computes a score based on tumor size, histological grade, and nodal status, and then classifies the risk as 'Good', 'Intermediate', or 'Poor' based on the calculated score."
      },
      {
        "question": "How does the function handle missing values and what input validation does it perform?",
        "answer": "The function checks for complete cases using 'complete.cases()'. If 'na.rm=FALSE' and NA values are present, it stops with an error. It also validates that grade and node are within the correct range (1, 2, or 3) and that size is numeric. If all values are NA, it returns NA for both score and risk."
      },
      {
        "question": "What is the formula used to calculate the NPI score, and how is the risk classification determined?",
        "answer": "The NPI score is calculated using the formula: 0.2 * size + grade + node. The risk classification is determined as follows: 'Good' if score < 3.4, 'Intermediate' if 3.4 <= score <= 5.4, and 'Poor' if score > 5.4. The function returns both the continuous score and the risk classification."
      }
    ],
    "completion_tasks": [
      {
        "partial": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  # Add input validation here\n  \n  npi <- 0.2 * size + grade + node\n  names(npi) <- nn[cc.ix]\n  \n  npi.score <- rep(NA, length(cc.ix))\n  names(npi.score) <- nn\n  npi.score[names(npi)] <- npi\n  \n  # Add risk classification logic here\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}",
        "complete": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  if(length(size) != length(grade) || length(grade) != length(node)) {\n    stop(\"size, grade and lymph node stage must have the same length!\")\n  }\n  if(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n  if(!all(is.element(grade, c(\"1\", \"2\", \"3\")))) {\n    stop(\"grade must be 1, 2 or 3!\")\n  }\n  if(!all(is.element(node, c(\"1\", \"2\", \"3\")))) {\n    stop(\"lymph node stage must be 1, 2 or 3!\")\n  }\n  if(!is.numeric(size)) {\n    stop(\"tumor size (cm) must be numeric!\")\n  }\n  \n  npi <- 0.2 * size + grade + node\n  names(npi) <- nn[cc.ix]\n  \n  npi.score <- rep(NA, length(cc.ix))\n  names(npi.score) <- nn\n  npi.score[names(npi)] <- npi\n  \n  npi.c <- cut(npi, breaks=c(-Inf, 3.4, 5.4, Inf), labels=c(\"Good\", \"Intermediate\", \"Poor\"))\n  \n  npi.classif <- rep(NA, length(cc.ix))\n  names(npi.classif) <- nn\n  npi.classif[names(npi.c)] <- npi.c\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}"
      },
      {
        "partial": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  # Add input validation here\n  \n  # Calculate NPI score\n  \n  # Classify risk\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}",
        "complete": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  if(!all(cc.ix) & !na.rm) stop(\"NA values are present!\")\n  if(!all(grade %in% 1:3)) stop(\"grade must be 1, 2 or 3!\")\n  if(!all(node %in% 1:3)) stop(\"lymph node stage must be 1, 2 or 3!\")\n  if(!is.numeric(size)) stop(\"tumor size (cm) must be numeric!\")\n  \n  npi <- 0.2 * size + as.numeric(grade) + as.numeric(node)\n  \n  npi.score <- rep(NA, length(cc.ix))\n  names(npi.score) <- nn\n  npi.score[cc.ix] <- npi\n  \n  npi.classif <- cut(npi, breaks=c(-Inf, 3.4, 5.4, Inf), labels=c(\"Good\", \"Intermediate\", \"Poor\"))\n  npi.risk <- rep(NA, length(cc.ix))\n  names(npi.risk) <- nn\n  npi.risk[cc.ix] <- npi.classif\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.risk))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  }
]