[
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/SanityCheck.R",
    "language": "R",
    "content": "sanitizeInput <- function(conc,\n                          viability,\n                          Hill_fit,\n                          conc_as_log = FALSE,\n                          viability_as_pct = TRUE,\n                          trunc = TRUE,\n                          verbose = TRUE) # Set to 2 to see debug messageouts\n  {\n\n  if (is.logical(conc_as_log) == FALSE) {\n    message(conc_as_log)\n    stop(\"'conc_as_log' is not a logical.\")\n  }\n\n  if (is.logical(viability_as_pct) == FALSE) {\n    message(viability_as_pct)\n    stop(\"'viability_as_pct' is not a logical.\")\n  }\n\n  if (is.logical(trunc) == FALSE) {\n    message(trunc)\n    stop(\"'trunc' is not a logical.\")\n  }\n  if(!is.finite(verbose)){\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  if(!missing(viability)&&!missing(conc)&&missing(Hill_fit))\n  {\n    if (length(conc) != length(viability)) {\n      if(verbose==2){\n        message(conc)\n        message(viability)\n      }\n      stop(\"Log concentration vector is not of same length as viability vector.\")\n    }\n    if( any(is.na(conc)&(!is.na(viability)))){\n      warning(\"Missing concentrations with non-missing viability values encountered. Removing viability values correspoding to those concentrations\")\n\n      myx <- !is.na(conc)\n      conc <- as.numeric(conc[myx])\n      viability <- as.numeric(viability[myx])\n\n    }\n    if(any((!is.na(conc))&is.na(viability))){\n\n      warning(\"Missing viability with non-missing concentrations values encountered. Removing concentrations values correspoding to those viabilities\")\n      myx <- !is.na(viability)\n      conc <- as.numeric(conc[myx])\n      viability <- as.numeric(viability[myx])\n\n    }\n    conc <- as.numeric(conc[!is.na(conc)])\n    viability <- as.numeric(viability[!is.na(viability)])\n\n    #CHECK THAT FUNCTION INPUTS ARE APPROPRIATE\n    if (prod(is.finite(conc)) != 1) {\n      message(conc)\n      stop(\"Concentration vector contains elements which are not real numbers.\")\n    }\n\n    if (prod(is.finite(viability)) != 1) {\n      message(viability)\n      stop(\"Viability vector contains elements which are not real numbers.\")\n    }\n\n\n    if (min(viability) < 0) {\n      if (verbose) {\n\n        warning(\"Warning: Negative viability data.\")\n      }\n    }\n\n    if (max(viability) > (1 + 99 * viability_as_pct)) {\n      if (verbose) {\n        warning(\"Warning: Viability data exceeds negative control.\")\n      }\n    }\n\n\n    if (conc_as_log == FALSE && min(conc) < 0) {\n      if (verbose == 2) {\n        message(conc)\n        message(conc_as_log)\n      }\n      stop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n    }\n\n    if (viability_as_pct == TRUE && max(viability) < 5) {\n      warning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n      if (verbose == 2) {\n\n        message(viability)\n        message(viability_as_pct)\n      }\n    }\n\n    if (viability_as_pct == FALSE && max(viability) > 5) {\n      warning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n      if (verbose == 2) {\n        message(viability)\n        message(viability_as_pct)\n      }\n    }\n\n    if(is.unsorted(conc)){\n      warning(\"Concentration Values were unsorted. Sorting concentration and ordering viability in same order\")\n      myx <- order(conc)\n      conc <- conc[myx]\n      viability <- viability[myx]\n    }\n\n    #CONVERT DOSE-RESPONSE DATA TO APPROPRIATE INTERNAL REPRESENTATION\n    if (conc_as_log == FALSE ) {\n      ii <- which(conc == 0)\n      if(length(ii) > 0) {\n        conc <- conc[-ii]\n        viability <- viability[-ii]\n      }\n\n      log_conc <- log10(conc)\n    } else {\n      log_conc <- conc\n    }\n\n    if (viability_as_pct == TRUE) {\n      viability <- viability / 100\n    }\n    if (trunc) {\n      viability = pmin(as.numeric(viability), 1)\n      viability = pmax(as.numeric(viability), 0)\n    }\n\n    return(list(\"log_conc\"=log_conc, \"viability\"=viability))\n  }\n  if(!missing(Hill_fit) && missing(viability)){\n    if(is.list(Hill_fit)){\n\n      Hill_fit <- unlist(Hill_fit)\n    }\n    if (conc_as_log == FALSE && Hill_fit[[3]] < 0) {\n      message(\"EC50 passed in as:\")\n      message(Hill_fit[[3]])\n      stop(\"'conc_as_log' flag may be set incorrectly, as the EC50 is negative when positive value is expected.\")\n    }\n\n\n    if (viability_as_pct == FALSE && Hill_fit[[2]] > 1) {\n      message(\"Einf passed in as:\")\n      message(Hill_fit[[2]])\n\n      warning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\n    }\n    if (conc_as_log == FALSE){\n      Hill_fit[[3]] <- log10(Hill_fit[[3]])\n    }\n    if (viability_as_pct == TRUE){\n      Hill_fit[[2]] <- Hill_fit[[2]]/100\n    }\n    if(missing(conc)){\n      return(list(\"Hill_fit\"=Hill_fit))\n    } else {\n      conc <- as.numeric(conc[!is.na(conc)])\n\n      if (prod(is.finite(conc)) != 1) {\n        message(conc)\n        stop(\"Concentration vector contains elements which are not real numbers.\")\n      }\n      if (conc_as_log == FALSE && min(conc) < 0) {\n        message(conc)\n        message(conc_as_log)\n        stop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n      }\n\n      if (conc_as_log == FALSE ) {\n        ii <- which(conc == 0)\n        if(length(ii) > 0) {\n          conc <- conc[-ii]\n        }\n        log_conc <- log10(conc)\n      } else {\n        log_conc <- conc\n      }\n      if(is.unsorted(conc)){\n        myx <- order(conc)\n        conc <- conc[myx]\n      }\n      return(list(\"Hill_fit\"=Hill_fit, \"log_conc\" = log_conc))\n    }\n\n\n  }\n  if(!missing(Hill_fit)&&!missing(viability)){\n\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n  }\n  if(missing(Hill_fit)&&missing(viability)){\n\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'sanitizeInput' function in this R code, and what are its main input parameters?",
        "answer": "The 'sanitizeInput' function is designed to validate and preprocess input data for dose-response analysis. Its main input parameters are:\n1. conc: concentration values\n2. viability: corresponding viability values\n3. Hill_fit: parameters for Hill equation fitting\n4. conc_as_log: boolean indicating if concentrations are in log scale\n5. viability_as_pct: boolean indicating if viability is in percentage\n6. trunc: boolean to truncate viability values between 0 and 1\n7. verbose: controls the level of warning messages"
      },
      {
        "question": "How does the function handle missing or inconsistent data in the concentration and viability vectors?",
        "answer": "The function handles missing or inconsistent data as follows:\n1. It checks if the lengths of conc and viability vectors are equal.\n2. It removes pairs where either concentration or viability is NA.\n3. It converts both vectors to numeric and removes any remaining NA values.\n4. It sorts the concentration values and reorders the viability values accordingly.\n5. It removes concentration values of 0 when conc_as_log is FALSE.\n6. It issues warnings for negative viability data or viability exceeding control levels."
      },
      {
        "question": "What data transformations does the function perform on the input concentration and viability data before returning the result?",
        "answer": "The function performs the following transformations:\n1. If conc_as_log is FALSE, it converts concentrations to log10 scale.\n2. If viability_as_pct is TRUE, it divides viability values by 100 to convert from percentage to fraction.\n3. If trunc is TRUE, it truncates viability values to be between 0 and 1.\n4. It returns a list containing 'log_conc' (log-transformed concentrations) and 'viability' (processed viability values).\n5. When Hill_fit is provided instead of raw data, it adjusts the EC50 and Einf parameters according to the conc_as_log and viability_as_pct flags."
      }
    ],
    "completion_tasks": [
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"'conc_as_log', 'viability_as_pct', and 'trunc' must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  # Complete the function to handle the case when viability and conc are provided\n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Add code here\n  }\n  \n  # Complete the function to handle the case when Hill_fit is provided\n  if (!missing(Hill_fit) && missing(viability)) {\n    # Add code here\n  }\n  \n  # Handle error cases\n  if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n  }\n  if (missing(Hill_fit) && missing(viability)) {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"'conc_as_log', 'viability_as_pct', and 'trunc' must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    if (length(conc) != length(viability)) {\n      stop(\"Log concentration vector is not of same length as viability vector.\")\n    }\n    myx <- !is.na(conc) & !is.na(viability)\n    conc <- as.numeric(conc[myx])\n    viability <- as.numeric(viability[myx])\n    \n    if (!all(is.finite(conc)) || !all(is.finite(viability))) {\n      stop(\"Concentration or viability vector contains non-finite elements.\")\n    }\n    \n    if (min(viability) < 0 || max(viability) > (1 + 99 * viability_as_pct)) {\n      warning(\"Viability data out of expected range.\")\n    }\n    \n    if (!conc_as_log && min(conc) < 0) {\n      stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    }\n    \n    if ((viability_as_pct && max(viability) < 5) || (!viability_as_pct && max(viability) > 5)) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    if (is.unsorted(conc)) {\n      myx <- order(conc)\n      conc <- conc[myx]\n      viability <- viability[myx]\n    }\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    return(list(log_conc = log_conc, viability = viability))\n  }\n  \n  if (!missing(Hill_fit) && missing(viability)) {\n    Hill_fit <- if (is.list(Hill_fit)) unlist(Hill_fit) else Hill_fit\n    \n    if (!conc_as_log && Hill_fit[3] < 0) {\n      stop(\"'conc_as_log' flag may be set incorrectly, as the EC50 is negative when positive value is expected.\")\n    }\n    \n    if (!viability_as_pct && Hill_fit[2] > 1) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    Hill_fit[3] <- if (conc_as_log) Hill_fit[3] else log10(Hill_fit[3])\n    Hill_fit[2] <- if (viability_as_pct) Hill_fit[2] / 100 else Hill_fit[2]\n    \n    if (missing(conc)) return(list(Hill_fit = Hill_fit))\n    \n    conc <- as.numeric(conc[!is.na(conc)])\n    if (!all(is.finite(conc))) stop(\"Concentration vector contains non-finite elements.\")\n    if (!conc_as_log && min(conc) < 0) stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    if (is.unsorted(conc)) log_conc <- log_conc[order(conc)]\n    \n    return(list(Hill_fit = Hill_fit, log_conc = log_conc))\n  }\n  \n  if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n  }\n  if (missing(Hill_fit) && missing(viability)) {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  # Add input validation here\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Process viability and concentration data\n    # Add code here\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill fit data\n    # Add code here\n  } else if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability'.\")\n  } else {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!all(sapply(list(conc_as_log, viability_as_pct, trunc), is.logical))) {\n    stop(\"'conc_as_log', 'viability_as_pct', and 'trunc' must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical or numerical argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    if (length(conc) != length(viability)) {\n      stop(\"Concentration and viability vectors must have the same length.\")\n    }\n    valid_data <- !is.na(conc) & !is.na(viability)\n    conc <- as.numeric(conc[valid_data])\n    viability <- as.numeric(viability[valid_data])\n    \n    if (!all(is.finite(c(conc, viability)))) {\n      stop(\"Concentration or viability vector contains non-finite elements.\")\n    }\n    \n    if (min(viability) < 0 || max(viability) > (1 + 99 * viability_as_pct)) {\n      warning(\"Viability data out of expected range.\")\n    }\n    \n    if (!conc_as_log && min(conc) < 0) {\n      stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    }\n    \n    if ((viability_as_pct && max(viability) < 5) || (!viability_as_pct && max(viability) > 5)) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    if (is.unsorted(conc)) {\n      order <- order(conc)\n      conc <- conc[order]\n      viability <- viability[order]\n    }\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    return(list(log_conc = log_conc, viability = viability))\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    Hill_fit <- if (is.list(Hill_fit)) unlist(Hill_fit) else Hill_fit\n    \n    if (!conc_as_log && Hill_fit[3] < 0) {\n      stop(\"EC50 is negative when positive value is expected. Check 'conc_as_log' flag.\")\n    }\n    \n    if (!viability_as_pct && Hill_fit[2] > 1) {\n      warning(\"'viability_as_pct' flag may be set incorrectly.\")\n    }\n    \n    Hill_fit[3] <- if (conc_as_log) Hill_fit[3] else log10(Hill_fit[3])\n    Hill_fit[2] <- if (viability_as_pct) Hill_fit[2] / 100 else Hill_fit[2]\n    \n    if (missing(conc)) return(list(Hill_fit = Hill_fit))\n    \n    conc <- as.numeric(conc[!is.na(conc)])\n    if (!all(is.finite(conc))) stop(\"Concentration vector contains non-finite elements.\")\n    if (!conc_as_log && min(conc) < 0) stop(\"Negative concentrations encountered. Check data or 'conc_as_log' flag.\")\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc != 0])\n    if (is.unsorted(conc)) log_conc <- log_conc[order(conc)]\n    \n    return(list(Hill_fit = Hill_fit, log_conc = log_conc))\n  } else if (!missing(Hill_fit) && !missing(viability)) {\n    stop(\"Please pass in only one of 'Hill_fit' and 'viability'.\")\n  } else {\n    stop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-summarizeMolecularProfiles.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(ToxicoGx)\n\n## TODO:: Can probably rewrite this using an apply function?\n\ncontext(\"Testing if summarizeMolecularProfiles error handling works correctly...\")\n\n# tSet\ncontext(\"...Checking for correct tSet param errors...\")\ntest_that(\"Errors if given more than one tSet as parameter.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            c(TGGATESsmall, TGGATESsmall), mDataType=\"rna\",\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        ))\n})\n\n# mDataTypes\ncontext(\"...Checking for correct mDataType param errors...\")\ntest_that(\"Warning if given more than one mDataType.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=c(\"rna\", \"cnv\"),\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall, \"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=FALSE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if given mDataType as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=1, cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Low\", \"Medium\"), summary.stat=\"mean\",\n            fill.missing=FALSE, verbose=FALSE\n        )\n    )\n})\ntest_that(\"Errors if specified mDataType is not in the tSet.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"cnv\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"first\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# cell_lines\ncontext(\"...Checking for correct cell_lines param errors...\")\ntest_that(\"Errors if given cell_lines as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=5,\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if specified cell_lines are not in the tSet\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines='NOTINtSET',\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# drugs\ncontext(\"...Checking for correct drugs param errors...\")\ntest_that(\"Errors if given drugs are type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=5, features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if specified drugs are not in the tSet.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=\"NOTINtSET\", features=fNames(TGGATESsmall,\"rna\"),\n            duration=\"8\", dose=c(\"ontrol\", \"High\"), summary.stat=\"first\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# features\ncontext(\"...Checking for correct features param errors....\")\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# duration\ncontext(\"...Checking for correct duration param errors\")\ntest_that(\"Errors if given duration as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=8,\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"NOTINtSET\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"first\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# dose\ncontext(\"...Checking for correct dose param errors\")\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(1, 2), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\ntest_that(\"Errors if specified doses are not in the tSet\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=\"NOTINTtSET\", summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeMolecularProfiles` function in this code, and what are some of the key parameters it checks for?",
        "answer": "The `summarizeMolecularProfiles` function appears to be a data analysis tool for summarizing molecular profiles in toxicogenomics studies. It checks for several key parameters including: 'tSet' (the dataset), 'mDataType' (type of molecular data), 'cell_lines', 'drugs', 'features', 'duration', 'dose', and 'summary.stat'. The function performs extensive error checking on these parameters to ensure they are of the correct type and contain valid values present in the dataset."
      },
      {
        "question": "How does the code handle error checking for the 'mDataType' parameter in the `summarizeMolecularProfiles` function?",
        "answer": "The code performs three checks for the 'mDataType' parameter: 1) It raises an error if more than one mDataType is provided. 2) It checks if the mDataType is of character type, raising an error if it's not. 3) It verifies if the specified mDataType exists in the tSet (dataset), raising an error if it doesn't. These checks ensure that the mDataType parameter is correctly specified and compatible with the given dataset."
      },
      {
        "question": "What testing framework is used in this code, and how are the tests structured?",
        "answer": "This code uses the 'testthat' library for unit testing in R. Tests are structured using 'context()' to group related tests, and 'test_that()' to define individual test cases. Each test case uses 'expect_error()' to verify that the `summarizeMolecularProfiles` function raises appropriate errors when given invalid inputs. The tests are organized by parameter, with multiple test cases for each parameter to cover different error scenarios."
      }
    ],
    "completion_tasks": [
      {
        "partial": "context(\"Testing if summarizeMolecularProfiles error handling works correctly...\")\n\ntest_that(\"Errors if given more than one tSet as parameter.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            c(TGGATESsmall, TGGATESsmall), mDataType=\"rna\",\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        ))\n})\n\ntest_that(\"Warning if given more than one mDataType.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=c(\"rna\", \"cnv\"),\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall, \"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=FALSE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given mDataType as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=1, cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Low\", \"Medium\"), summary.stat=\"mean\",\n            fill.missing=FALSE, verbose=FALSE\n        )\n    )\n})\n\n# Complete the test cases for cell_lines, drugs, and features",
        "complete": "context(\"Testing if summarizeMolecularProfiles error handling works correctly...\")\n\ntest_that(\"Errors if given more than one tSet as parameter.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            c(TGGATESsmall, TGGATESsmall), mDataType=\"rna\",\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=TRUE, verbose=TRUE\n        ))\n})\n\ntest_that(\"Warning if given more than one mDataType.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=c(\"rna\", \"cnv\"),\n            cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall, \"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"median\",\n            fill.missing=FALSE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given mDataType as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=1, cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Low\", \"Medium\"), summary.stat=\"mean\",\n            fill.missing=FALSE, verbose=FALSE\n        )\n    )\n})\n\ntest_that(\"Errors if given cell_lines as type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=5,\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given drugs are type other than character.\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=5, features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given features as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=c(5), duration=\"8\",\n            dose=c(\"Control\", \"High\"), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})"
      },
      {
        "partial": "context(\"...Checking for correct duration and dose param errors\")\n\ntest_that(\"Errors if given duration as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=8,\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\n# Complete the test case for dose parameter",
        "complete": "context(\"...Checking for correct duration and dose param errors\")\n\ntest_that(\"Errors if given duration as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=8,\n            dose=c(\"Control\", \"High\"), summary.stat=\"mean\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})\n\ntest_that(\"Errors if given dose as type other than character\", {\n    expect_error(\n        summarizeMolecularProfiles(\n            TGGATESsmall, mDataType=\"rna\", cell_lines=sampleNames(TGGATESsmall),\n            drugs=head(treatmentNames(TGGATESsmall)),\n            features=fNames(TGGATESsmall,\"rna\"), duration=\"8\",\n            dose=c(1, 2), summary.stat=\"last\",\n            fill.missing=TRUE, verbose=TRUE\n        )\n    )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/power.cor.R",
    "language": "R",
    "content": "#' @title Function for sample size calculation for correlation coefficients\n#'\n#' @description\n#' This function enables to compute the sample size requirements for estimating \n#'   pearson, kendall and spearman correlations\n#'\n#' @usage\n#' power.cor(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\"))\n#'\n#' @param rho\tCorrealtion coefficients rho (Pearson, Kendall or Spearman)\n#' @param w\ta numerical vector of weights of the same length as x giving the weights to \n#'   use for elements of x in the first class.\n#' @param alpha\talpha level\n#' @param method\ta character string specifying the method to compute the correlation \n#'   coefficient, must be one of \"pearson\" (default), \"kendall\" or \"spearman\". You can \n#'   specify just the initial letter.\n#'\n#' @return\n#' sample size requirement\n#'\n#' @references\n#' Bonett, D. G., and Wright, T. A. (2000). Sample size requirements for estimating \n#'   pearson, kendall and spearman correlations. Psychometrika, 65(1), \n#'   23-28. doi:10.1007/BF02294183\n#'\n#' @examples\n#' power.cor(rho=0.5, w=0.1, alpha=0.05, method=\"spearman\")\n#'\n#' @md\n#' @export\n#' \n## sample size calculation for correlation coefficients (Pearson, kendall and SPearman)\n## example: power.cor(rho=0.5, w=0.1, alpha=0.05, method=\"spearman\")\npower.cor <- \nfunction (rho, w, alpha=0.05, method=c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(3, 4, 3)\n  cc <- c(1, sqrt(0.437), sqrt(1 + (rho^2 / 2)))\n  names(bb) <- names(cc) <- c(\"pearson\", \"kendall\", \"spearman\")\n  bb <- bb[method]\n  cc <- cc[method]\n  nn0 <- 4 * cc^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb\n  if(nn0 < 10) { nn0t <- 10 } else { nn0t <- ceiling(nn0) }\n  w0w <- sqrt(nn0t - bb) / sqrt(nn0 - bb)\n  nn <- ceiling((nn0 - bb) * w0w^2 + bb)\n  return(nn)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `power.cor` function and what are its main parameters?",
        "answer": "The `power.cor` function is used to calculate the sample size requirements for estimating correlation coefficients (Pearson, Kendall, or Spearman). Its main parameters are: `rho` (correlation coefficient), `w` (width of confidence interval), `alpha` (significance level), and `method` (type of correlation coefficient to use)."
      },
      {
        "question": "How does the function handle different correlation methods, and what is the significance of the `bb` and `cc` vectors?",
        "answer": "The function uses `match.arg(method)` to ensure a valid correlation method is selected. The `bb` and `cc` vectors contain method-specific constants used in the sample size calculation. `bb` represents degrees of freedom adjustments, while `cc` contains scaling factors for each method. These values are selected based on the chosen correlation method."
      },
      {
        "question": "Explain the purpose of the conditional statement `if(nn0 < 10) { nn0t <- 10 } else { nn0t <- ceiling(nn0) }` in the function.",
        "answer": "This conditional statement ensures that the initial sample size estimate (`nn0t`) is at least 10. If the calculated `nn0` is less than 10, it sets `nn0t` to 10. Otherwise, it rounds up `nn0` to the nearest integer. This step helps to maintain a minimum sample size for reliable correlation estimation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(3, 4, 3)\n  cc <- c(1, sqrt(0.437), sqrt(1 + (rho^2 / 2)))\n  names(bb) <- names(cc) <- c(\"pearson\", \"kendall\", \"spearman\")\n  bb <- bb[method]\n  cc <- cc[method]\n  nn0 <- 4 * cc^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb\n  # Complete the function from here\n}",
        "complete": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(3, 4, 3)\n  cc <- c(1, sqrt(0.437), sqrt(1 + (rho^2 / 2)))\n  names(bb) <- names(cc) <- c(\"pearson\", \"kendall\", \"spearman\")\n  bb <- bb[method]\n  cc <- cc[method]\n  nn0 <- 4 * cc^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb\n  nn0t <- max(10, ceiling(nn0))\n  w0w <- sqrt(nn0t - bb) / sqrt(nn0 - bb)\n  nn <- ceiling((nn0 - bb) * w0w^2 + bb)\n  return(nn)\n}"
      },
      {
        "partial": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  # Implement the function body here\n}",
        "complete": "power.cor <- function(rho, w, alpha = 0.05, method = c(\"pearson\", \"kendall\", \"spearman\")) {\n  method <- match.arg(method)\n  bb <- c(pearson = 3, kendall = 4, spearman = 3)\n  cc <- c(pearson = 1, kendall = sqrt(0.437), spearman = sqrt(1 + (rho^2 / 2)))\n  nn0 <- 4 * cc[method]^2 * (1 - rho^2)^2 * (qnorm(p=alpha/2, lower.tail=FALSE) / w) + bb[method]\n  nn0t <- max(10, ceiling(nn0))\n  w0w <- sqrt(nn0t - bb[method]) / sqrt(nn0 - bb[method])\n  nn <- ceiling((nn0 - bb[method]) * w0w^2 + bb[method])\n  return(nn)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcTCGA.R",
    "language": "R",
    "content": "#' @title Function to compute the prediction scores and risk classifications\n#'   for the ovarian cancer TCGA signature\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene\n#'   expression values following the algorithm developed by the TCGA consortium\n#'   for ovarian cancer.\n#'\n#' @usage\n#' ovcTCGA(data, annot,\n#'   gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"),\n#'   do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param gmap\tcharacter string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score:\tContinuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe:\tIf mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Bell D, Berchuck A, Birrer M et al. (2011) \"Integrated genomic analyses of\n#'   ovarian carcinoma\", Nature, 474(7353):609-615\n#'\n#' @seealso\n#' [genefu::sigOvcTCGA]\n#'\n#' @examples\n#' # load the ovcTCGA signature\n#' data(sigOvcTCGA)\n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \"entrezgene\"\n#' # compute relapse score\n#' ovcTCGA.nkis <- ovcTCGA(data=data.nkis, annot=annot.nkis, gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcTCGA.nkis$risk)\n#'\n#' @md\n#' @export\novcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\",\n    \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE)\n{\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    pscore <- apply(data, 1, function(x, y) { return(t.test(x ~ y)$statistic) }, y=as.numeric(sigt[ ,\"beta\"] < 0))\n\tprisk <- as.numeric(pscore >= 0)\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `ovcTCGA` function?",
        "answer": "The primary purpose of the `ovcTCGA` function is to compute signature scores and risk classifications for ovarian cancer based on gene expression data. It implements the algorithm developed by the TCGA (The Cancer Genome Atlas) consortium for ovarian cancer. The function returns continuous signature scores and binary risk classifications (high risk or low risk) based on the input gene expression data."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs gene mapping using the specified `gmap` parameter. It maps the input data to the signature genes using Entrez Gene IDs or other specified identifiers. In case of ambiguities (multiple probes mapping to the same gene), it keeps the most variant probe for each gene. The function also creates a mapping between the original probe IDs and the new gene IDs, which is returned in the output list as the 'mapping' and 'probe' elements."
      },
      {
        "question": "How are the signature scores and risk classifications calculated in the `ovcTCGA` function?",
        "answer": "The signature scores are calculated using a t-test statistic. The function applies a t-test to each sample, comparing the expression values of genes with positive and negative beta coefficients in the signature. The resulting t-statistic becomes the signature score. For risk classification, samples with a score greater than or equal to 0 are classified as high risk (1), while those with a score less than 0 are classified as low risk (0). This calculation is performed in the last few lines of the function using the `apply` function and `t.test`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        # Complete the else block\n    }\n    # Complete the function\n}",
        "complete": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    pscore <- apply(data, 1, function(x, y) { return(t.test(x ~ y)$statistic) }, y=as.numeric(sigt[ ,\"beta\"] < 0))\n    prisk <- as.numeric(pscore >= 0)\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        # Complete the do.mapping block\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    # Complete the function\n}",
        "complete": "ovcTCGA <- function(data, annot, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcTCGA')) data(sigOvcTCGA, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcTCGA[order(sigOvcTCGA[ ,\"p.value\"], decreasing=TRUE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcTCGA), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough genes from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcTCGA))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcTCGA[gix, ,drop=FALSE]\n    }\n    pscore <- apply(data, 1, function(x, y) { return(t.test(x ~ y)$statistic) }, y=as.numeric(sigt[ ,\"beta\"] < 0))\n    prisk <- as.numeric(pscore >= 0)\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/drugPerturbationSig.R",
    "language": "R",
    "content": "#' Drug perturbation analysis\n#'\n#' Creates a signature representing gene expression (or other molecular profile)\n#' change induced by administrating a drug, for use in drug effect analysis.\n#'\n#' Given a Toxicoset of the perturbation experiment type, and a character vector\n#' of drugs, the function will compute a signature for the effect of\n#' drug concentration on the molecular profile of a cell. The algorithm uses a\n#' regression model which corrects for experimental batch effects, cell specific\n#' differences, and duration of experiment to isolate the effect of the\n#' concentration of the drug applied. The function returns the estimated\n#' coefficient for concentration, the t-stat, the p-value and the false\n#' discovery rate associated with that coefficient, in a 3 dimensional array,\n#' with genes in the first direction, drugs in the second, and the selected\n#' return values in the third.\n#'\n#' @examples\n#' if (interactive()) {\n#' data(TGGATESsmall)\n#' drug.perturbation <- drugPerturbationSig(TGGATESsmall, mDataType=\"rna\", features = head(fNames(TGGATESsmall, \"rna\")), nthread=1)\n#' }\n#'\n#' @param tSet \\code{ToxicoSet} a ToxicoSet of the perturbation experiment type\n#' @param mDataType \\code{character} which one of the molecular data types to use\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv (only rna currently supported)\n#' @param drugs \\code{character} a vector of drug names for which to compute the\n#'   signatures. Should match the names used in the ToxicoSet.\n#' @param cell_lines \\code{character} a vector of cell names to use in computing the\n#'   signatures. Should match the names used in the ToxicoSet.\n#' @param features \\code{character} a vector of features for which to compute the\n#'   signatures. Should match the names used in correspondant molecular data in ToxicoSet.\n#' @param duration \\code{character} a vector of experiment durations for which to inlcude in the\n#'   computed the signatures.\n#' @param dose \\code{character} a vector of dose levels to include in the results\n#' @param nthread \\code{numeric} if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#' @param returnValues \\code{character} Which of estimate, t-stat, p-value and fdr\n#'   should the function return for each gene drug pair\n#' @param verbose \\code{bool} Should diagnostive messages be printed? (default false)\n#'\n#' @return \\code{ToxicoSig} An object composed of a 3D array with genes in the\n#'   first dimension, drugs in the second, and return values in the third.\n#'\n#' @importFrom tibble as_tibble\n#' @import ggplot2\n#'\n#' @export\n#'\ndrugPerturbationSig <- function(\n  tSet, mDataType,\n  drugs = NULL,\n  cell_lines = NULL,\n  features = NULL,\n  duration = NULL,\n  dose = NULL,\n  nthread=1,\n  returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE\n){\n  # ALLOCATE CORES FOR PARALLEL PROCESSING\n  availcore <- parallel::detectCores()\n  if ( nthread > availcore) {\n    nthread <- availcore\n  }\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  ## MISSING VALUE HANDLING FOR PARAMETERS\n  # Get named list of defualt values for missing parameters\n  argDefaultList <-\n    paramMissingHandler(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType,\n                        drugs = drugs, cell_lines = cell_lines, features = features,\n                        duration = duration, dose = dose)\n\n  # Assign any missing parameter default values to function environment\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  ##TODO:: Fix variable names output from paramMissingHandler\n  if ('durations' %in% ls()) { duration <- durations }\n\n  # ERROR HANDLING FOR PARAMETERS\n  paramErrorChecker(\"drugPerturbationSig\", tSet = tSet,\n                    mDataType = mDataType, cell_lines = cell_lines,\n                    drugs = drugs, features = features,\n                    duration = duration, dose = dose)\n\n  returnValues <- match.arg(returnValues, several.ok = TRUE)\n\n  # Add DMSO for the drugMatrix\n  if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) {\n    if (!('DMSO' %in% drugs)) {\n      drugs <- c('DMSO', drugs)\n    }\n    if (!('Control' %in% dose)) {\n      dose <- c('Control', dose)\n    }\n  }\n\n  # SUBSET tSET BASED ON PARAMETERS\n  tSetSubsetOnParams <-\n    suppressWarnings(subsetTo(tSet, mDataType = mDataType, cells = cell_lines, drugs = drugs, features = features, duration = duration))\n\n  # SUBSET SAMPLES BASED ON DOSE\n  samples <- rownames(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$dose_level %in% dose),])\n\n  # LOOP OVER DRUGS TO CALCULATE PER DRUG SUMMARY STATISTICS\n  mcres <- lapply(drugs[drugs != 'DMSO'], function(x, exprs, sampleinfo) {\n\n    # Add DMSO for the drugMatrix since it is the only control\n    if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) {\n        x <- c('DMSO', x)\n    }\n    # Subset to correct drugs\n    exprs <- exprs[which(sampleinfo[ , \"treatmentid\"] %in% x),]\n    sampleinfo <- sampleinfo[which(sampleinfo[ , \"treatmentid\"] %in% x),]\n\n    # Warning that rankGeneDrugPerturbation will return a matrix of NAs for this drug\n    if (length(unique(as.character(sampleinfo[, \"xptype\"]))) < 2) {\n      warning(paste0(\"There are only controls available at dose levels \",\n                     paste(dose, collapse = \" \") ,\" for \", x, \",\n                     summary statistics for this drug will be excluded for the results.\n                     Adding another dose level will likely generate results.\"))\n    }\n    res <- NULL\n    i <- x[x != 'DMSO']\n\n    ## using a linear model (x ~ concentration + cell + batch + duration)\n    res <- rankGeneDrugPerturbation(\n      data = exprs, drug = x, drug.id = as.character(sampleinfo[ , \"treatmentid\"]),\n      drug.concentration = as.numeric(sampleinfo[ , \"concentration\"]),\n      type = as.character(sampleinfo[ , \"sampleid\"]),\n      xp = as.character(sampleinfo[ , \"xptype\"]),\n      batch = as.character(sampleinfo[ , \"batchid\"]),\n      duration = as.character(sampleinfo[ , \"duration\"]) ,\n      single.type = FALSE, nthread = nthread,\n      verbose = FALSE)$all[ , returnValues, drop = FALSE]\n    res <- list(res)\n    names(res) <- i\n    return(res)\n  }, exprs = t(as.data.frame(molecularProfiles(tSetSubsetOnParams, mDataType)[features, samples, drop=FALSE])),\n     sampleinfo = as.data.frame(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$samplename %in% samples), ])\n  )\n\n  # ASSEMBLE RESULTS TO BE INCLUDED IN TOXICOSIG OBJECT\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE=logical(1))]\n  drug.perturbation <- array(NA, dim = c(nrow(featureInfo(tSet, mDataType)[features,, drop = FALSE]), length(res), ncol(res[[1]])), dimnames = list(rownames(featureInfo(tSet, mDataType)[features,,drop = FALSE]), names(res), colnames(res[[1]])))\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n      xx <- array(NA, dim = length(k), dimnames = list(k))\n      xx[rownames(x)] <- x[ , j, drop = FALSE]\n      return(xx)\n    }, j = j, k = rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]),\n    FUN.VALUE=numeric(dim(drug.perturbation)[1]))\n    drug.perturbation[rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]), names(res), j] <- ttt\n  }\n\n  # CREATE TOXICOSIG OBJECT\n  drug.perturbation <- ToxicoGx::ToxicoSig(drug.perturbation, tSetName = name(tSet), Call = as.character(match.call()), SigType = 'Perturbation')\n\n  # RETURN TOXICOSIG OBJECT\n  return(drug.perturbation)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `drugPerturbationSig` function?",
        "answer": "The primary purpose of the `drugPerturbationSig` function is to create a signature representing gene expression (or other molecular profile) changes induced by administering a drug. It computes a signature for the effect of drug concentration on the molecular profile of a cell, using a regression model that corrects for experimental batch effects, cell-specific differences, and experiment duration to isolate the effect of the drug concentration."
      },
      {
        "question": "How does the function handle parallel processing, and what happens if the user specifies more threads than available cores?",
        "answer": "The function uses parallel processing to speed up computations. It detects the number of available cores using `parallel::detectCores()`. If the user specifies more threads (`nthread`) than available cores, the function automatically adjusts `nthread` to match the number of available cores. The function then sets the `mc.cores` option to the adjusted `nthread` value for parallel processing."
      },
      {
        "question": "What is the structure of the returned `ToxicoSig` object, and what information does it contain?",
        "answer": "The returned `ToxicoSig` object contains a 3D array with genes in the first dimension, drugs in the second dimension, and return values in the third dimension. The return values can include 'estimate', 't-stat', 'p-value', and 'fdr' (false discovery rate) for each gene-drug pair, as specified by the `returnValues` parameter. The object also includes metadata such as the name of the ToxicoSet used, the function call, and the signature type ('Perturbation')."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugPerturbationSig <- function(tSet, mDataType, drugs = NULL, cell_lines = NULL, features = NULL, duration = NULL, dose = NULL, nthread = 1, returnValues = c(\"estimate\", \"tstat\", \"pvalue\", \"fdr\"), verbose = FALSE) {\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) nthread <- availcore\n  options(mc.cores = nthread)\n  on.exit(options(op))\n\n  argDefaultList <- paramMissingHandler(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, drugs = drugs, cell_lines = cell_lines, features = features, duration = duration, dose = dose)\n\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  if ('durations' %in% ls()) duration <- durations\n\n  paramErrorChecker(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, cell_lines = cell_lines, drugs = drugs, features = features, duration = duration, dose = dose)\n\n  returnValues <- match.arg(returnValues, several.ok = TRUE)\n\n  # Add code here to handle special cases for 'drugMatrix_rat' and 'EMEXP2458'\n\n  tSetSubsetOnParams <- suppressWarnings(subsetTo(tSet, mDataType = mDataType, cells = cell_lines, drugs = drugs, features = features, duration = duration))\n\n  samples <- rownames(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$dose_level %in% dose),])\n\n  # Add code here for the main computation loop\n\n  # Create and return ToxicoSig object\n}",
        "complete": "drugPerturbationSig <- function(tSet, mDataType, drugs = NULL, cell_lines = NULL, features = NULL, duration = NULL, dose = NULL, nthread = 1, returnValues = c(\"estimate\", \"tstat\", \"pvalue\", \"fdr\"), verbose = FALSE) {\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) nthread <- availcore\n  options(mc.cores = nthread)\n  on.exit(options(op))\n\n  argDefaultList <- paramMissingHandler(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, drugs = drugs, cell_lines = cell_lines, features = features, duration = duration, dose = dose)\n\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  if ('durations' %in% ls()) duration <- durations\n\n  paramErrorChecker(\"drugPerturbationSig\", tSet = tSet, mDataType = mDataType, cell_lines = cell_lines, drugs = drugs, features = features, duration = duration, dose = dose)\n\n  returnValues <- match.arg(returnValues, several.ok = TRUE)\n\n  if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) {\n    if (!('DMSO' %in% drugs)) drugs <- c('DMSO', drugs)\n    if (!('Control' %in% dose)) dose <- c('Control', dose)\n  }\n\n  tSetSubsetOnParams <- suppressWarnings(subsetTo(tSet, mDataType = mDataType, cells = cell_lines, drugs = drugs, features = features, duration = duration))\n\n  samples <- rownames(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$dose_level %in% dose),])\n\n  mcres <- lapply(drugs[drugs != 'DMSO'], function(x, exprs, sampleinfo) {\n    if (name(tSet) %in% c('drugMatrix_rat', 'EMEXP2458')) x <- c('DMSO', x)\n    exprs <- exprs[which(sampleinfo[, \"treatmentid\"] %in% x),]\n    sampleinfo <- sampleinfo[which(sampleinfo[, \"treatmentid\"] %in% x),]\n    if (length(unique(as.character(sampleinfo[, \"xptype\"]))) < 2) {\n      warning(paste0(\"There are only controls available at dose levels \", paste(dose, collapse = \" \"), \" for \", x, \", summary statistics for this drug will be excluded for the results. Adding another dose level will likely generate results.\"))\n    }\n    res <- NULL\n    i <- x[x != 'DMSO']\n    res <- rankGeneDrugPerturbation(data = exprs, drug = x, drug.id = as.character(sampleinfo[, \"treatmentid\"]),\n                                    drug.concentration = as.numeric(sampleinfo[, \"concentration\"]),\n                                    type = as.character(sampleinfo[, \"sampleid\"]),\n                                    xp = as.character(sampleinfo[, \"xptype\"]),\n                                    batch = as.character(sampleinfo[, \"batchid\"]),\n                                    duration = as.character(sampleinfo[, \"duration\"]),\n                                    single.type = FALSE, nthread = nthread, verbose = FALSE)$all[, returnValues, drop = FALSE]\n    res <- list(res)\n    names(res) <- i\n    return(res)\n  }, exprs = t(as.data.frame(molecularProfiles(tSetSubsetOnParams, mDataType)[features, samples, drop=FALSE])),\n     sampleinfo = as.data.frame(phenoInfo(tSetSubsetOnParams, mDataType)[which(phenoInfo(tSetSubsetOnParams, mDataType)$samplename %in% samples), ]))\n\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE=logical(1))]\n  drug.perturbation <- array(NA, dim = c(nrow(featureInfo(tSet, mDataType)[features,, drop = FALSE]), length(res), ncol(res[[1]])),\n                             dimnames = list(rownames(featureInfo(tSet, mDataType)[features,,drop = FALSE]), names(res), colnames(res[[1]])))\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n      xx <- array(NA, dim = length(k), dimnames = list(k))\n      xx[rownames(x)] <- x[, j, drop = FALSE]\n      return(xx)\n    }, j = j, k = rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]),\n    FUN.VALUE=numeric(dim(drug.perturbation)[1]))\n    drug.perturbation[rownames(featureInfo(tSet, mDataType)[features,, drop = FALSE]), names(res), j] <- ttt\n  }\n\n  drug.perturbation <- ToxicoGx::ToxicoSig(drug.perturbation, tSetName = name(tSet), Call = as.character(match.call()), SigType = 'Perturbation')\n  return(drug.perturbation)\n}"
      },
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type = FALSE, nthread = 1, verbose = FALSE) {\n  # Add code here to prepare data and variables\n\n  # Perform the main computation\n\n  # Process and return results\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type = FALSE, nthread = 1, verbose = FALSE) {\n  if (verbose) message(\"Ranking genes for drug: \", drug)\n  data <- as.matrix(data)\n  drug.id <- as.factor(drug.id)\n  type <- as.factor(type)\n  xp <- as.factor(xp)\n  batch <- as.factor(batch)\n  duration <- as.factor(duration)\n\n  if (single.type) {\n    mm <- model.matrix(~ drug.concentration + batch + duration)\n  } else {\n    mm <- model.matrix(~ drug.concentration + type + batch + duration)\n  }\n\n  fit <- limma::lmFit(data, mm)\n  fit <- limma::eBayes(fit)\n\n  tt <- limma::topTable(fit, coef = \"drug.concentration\", number = Inf, sort.by = \"none\")\n  tt <- tt[, c(\"logFC\", \"t\", \"P.Value\", \"adj.P.Val\")]\n  colnames(tt) <- c(\"estimate\", \"tstat\", \"pvalue\", \"fdr\")\n\n  return(list(all = tt))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/compute.pairw.cor.meta.R",
    "language": "R",
    "content": "#' @title Function to compute pairwise correlations in a meta-analytical framework\n#'\n#' @description\n#' This function computes meta-estimate of pairwise correlation coefficients for a\n#'   set of genes from a list of gene expression datasets.\n#'\n#' @usage\n#' compute.pairw.cor.meta(datas, method = c(\"pearson\", \"spearman\"))\n#'\n#' @param datas List of datasets. Each dataset is a matrix of gene expressions with\n#'   samples in rows and probes in columns, dimnames being properly defined. All the\n#'   datasets must have the same probes.\n#' @param method Estimator for correlation coefficient, can be either pearson or spearman.\n#'\n#' @return\n#' A list with items:\n#' - cor  Matrix of meta-estimate of correlation coefficients with probes in rows and\n#'   prototypes in columns\n#' - cor.n Number of samples used to compute meta-estimate of correlation coefficients.\n#'\n#' @seealso\n#' [genefu::map.datasets], [genefu::compute.proto.cor.meta]\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # load NKI dataset\n#' data(nkis)\n#' # reduce datasets\n#' ginter <- intersect(annot.vdxs[ ,\"EntrezGene.ID\"], annot.nkis[ ,\"EntrezGene.ID\"])\n#' ginter <- ginter[!is.na(ginter)][1:30]\n#' myx <- unique(c(match(ginter, annot.vdxs[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.vdxs), size=20)))\n#' data2.vdxs <- data.vdxs[ ,myx]\n#' annot2.vdxs <- annot.vdxs[myx, ]\n#' myx <- unique(c(match(ginter, annot.nkis[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.nkis), size=20)))\n#' data2.nkis <- data.nkis[ ,myx]\n#' annot2.nkis <- annot.nkis[myx, ]\n#' # mapping of datasets\n#' datas <- list(\"VDX\"=data2.vdxs,\"NKI\"=data2.nkis)\n#' annots <- list(\"VDX\"=annot2.vdxs, \"NKI\"=annot2.nkis)\n#' datas.mapped <- map.datasets(datas=datas, annots=annots, do.mapping=TRUE)\n#' # compute meta-estimate of pairwise correlation coefficients\n#' pairwcor <- compute.pairw.cor.meta(datas=datas.mapped$datas, method=\"pearson\")\n#' str(pairwcor)\n#'\n#' @md\n#' @importFrom survcomp combine.est fisherz\n#' @export\ncompute.pairw.cor.meta <-\nfunction(datas, method=c(\"pearson\", \"spearman\")) {\n\tmethod <- match.arg(method)\n\tif(!is.list(datas)) {\n\t\tmycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n\t} else {\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\t\tif(length(datas) > 1) {\n\t\t    for(k in 2:length(datas)) {\n\t\t\t    if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n\t\t    }\n\t    }\n\t\tmycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n\t\tmycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n\t\tfor(i in 1:nc) {\n\t\t\tfor(j in 1:i) {\n\t\t\t\tmycorz <- mycorz.se <- NULL\n\t\t\t\tnnt <- 0\n\t\t\t\tfor(k in 1:length(datas)) {\n\t\t\t\t\tif(sum(complete.cases(datas[[k]][ , c(i, j)])) > 1) {\n\t\t\t\t\t\tnn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n\t\t\t\t\t\tmycorz <- c(mycorz, survcomp::fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE))\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n\t\t\t\t\t\tnnt <- nnt + nn\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmycorz <- c(mycorz, NA)\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, NA)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmycor[i, j] <- mycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n\t\t\t\tmycorn[i, j] <- mycorn[j, i] <- nnt\n\t\t\t}\n\t\t}\n\t}\n\treturn(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `compute.pairw.cor.meta` function, and what are its main input parameters?",
        "answer": "The `compute.pairw.cor.meta` function computes meta-estimates of pairwise correlation coefficients for a set of genes from multiple gene expression datasets. Its main input parameters are:\n1. `datas`: A list of datasets, where each dataset is a matrix of gene expressions with samples in rows and probes in columns.\n2. `method`: The estimator for correlation coefficient, which can be either 'pearson' or 'spearman'."
      },
      {
        "question": "How does the function handle the computation of correlation coefficients when multiple datasets are provided?",
        "answer": "When multiple datasets are provided, the function:\n1. Checks if all datasets have the same variables (columns).\n2. Initializes matrices for correlation coefficients and sample counts.\n3. Iterates through each pair of variables, computing correlations for each dataset.\n4. Uses Fisher's z-transformation to combine correlation estimates across datasets.\n5. Applies inverse Fisher's z-transformation to get the final meta-estimate.\n6. Returns a list containing the meta-estimates of correlation coefficients and the total sample sizes used for each pair."
      },
      {
        "question": "What is the significance of the `fisherz` function used in this code, and why is it applied twice?",
        "answer": "The `fisherz` function is used for Fisher's z-transformation, which is important in meta-analysis of correlation coefficients. It's applied twice in this code:\n1. First, to transform individual correlation coefficients to z-scores: `survcomp::fisherz(cor(...), inv=FALSE)`.\n2. Second, to transform the combined z-score back to a correlation coefficient: `fisherz(combine.est(...)$estimate, inv=TRUE)`.\nThis process allows for proper averaging of correlation coefficients across different datasets, as z-scores can be meaningfully combined, unlike raw correlation coefficients."
      }
    ],
    "completion_tasks": [
      {
        "partial": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    if(length(datas) > 1) {\n      for(k in 2:length(datas)) {\n        if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n      }\n    }\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    for(i in 1:nc) {\n      for(j in 1:i) {\n        # Complete the code here\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
        "complete": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    if(length(datas) > 1) {\n      for(k in 2:length(datas)) {\n        if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n      }\n    }\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    for(i in 1:nc) {\n      for(j in 1:i) {\n        mycorz <- mycorz.se <- NULL\n        nnt <- 0\n        for(k in 1:length(datas)) {\n          if(sum(complete.cases(datas[[k]][ , c(i, j)])) > 1) {\n            nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n            mycorz <- c(mycorz, survcomp::fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE))\n            mycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n            nnt <- nnt + nn\n          } else {\n            mycorz <- c(mycorz, NA)\n            mycorz.se <- c(mycorz.se, NA)\n          }\n        }\n        mycor[i, j] <- mycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[i, j] <- mycorn[j, i] <- nnt\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}"
      },
      {
        "partial": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n    return(list(\"cor\"=mycor, \"cor.n\"=nrow(datas)))\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    # Add code to check if all datasets have the same variables\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    # Add code to compute correlations\n    return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n  }\n}",
        "complete": "compute.pairw.cor.meta <- function(datas, method=c(\"pearson\", \"spearman\")) {\n  method <- match.arg(method)\n  if(!is.list(datas)) {\n    mycor <- cor(x=datas, method=method, use=\"pairwise.complete.obs\")\n    return(list(\"cor\"=mycor, \"cor.n\"=nrow(datas)))\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    if(length(datas) > 1) {\n      for(k in 2:length(datas)) {\n        if(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n      }\n    }\n    mycor <- matrix(NA, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    mycorn <- matrix(0, nrow=nc, ncol=nc, dimnames=list(ncn, ncn))\n    for(i in 1:nc) {\n      for(j in 1:i) {\n        mycorz <- mycorz.se <- NULL\n        nnt <- 0\n        for(k in 1:length(datas)) {\n          if(sum(complete.cases(datas[[k]][ , c(i, j)])) > 1) {\n            nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n            mycorz <- c(mycorz, survcomp::fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE))\n            mycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n            nnt <- nnt + nn\n          } else {\n            mycorz <- c(mycorz, NA)\n            mycorz.se <- c(mycorz.se, NA)\n          }\n        }\n        mycor[i, j] <- mycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[i, j] <- mycorn[j, i] <- nnt\n      }\n    }\n    return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/claudinLow.R",
    "language": "R",
    "content": "#' @title Claudin-low classification for Breast Cancer Data\n#'\n#' @description\n#' Subtyping method for identifying Claudin-Low Breast Cancer Samples.\n#'   Code generously provided by Aleix Prat.\n#'\n#' @usage\n#' claudinLow(x, classes=\"\", y, nGenes=\"\", priors=\"equal\",\n#'   std=FALSE, distm=\"euclidean\", centroids=FALSE)\n#'\n#' @param x the data matrix of training samples, or pre-calculated centroids.\n#' @param classes a list labels for use in coloring the points.\n#' @param y the data matrix of test samples.\n#' @param nGenes the number of genes selected when training the model.\n#' @param priors 'equal' assumes equal class priors, 'class' calculates them\n#'   based on proportion in the data.\n#' @param std when true, the training and testing samples are standardized\n#'   to mean=0 and var=1.\n#' @param distm the distance metric for determining the nearest centroid,\n#'   can be one of euclidean, pearson, or spearman.\n#' @param centroids when true, it is assumed that x consists of pre-calculated centroids.\n#'\n#' @return\n#' A list with items:\n#' - predictions\n#' - testData\n#' - distances\n#' - centroids\n#'\n#' @references\n#' Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason\n#'   I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and\n#'   molecular characterization of the claudin-low intrinsic subtype of\n#'   breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso\n#' [genefu::medianCtr()], [genefu::q]\n#'\n#' @examples\n#' data(claudinLowData)\n#'\n#' #Training Set\n#' train <- claudinLowData\n#' train$xd <-  medianCtr(train$xd)\n#' # Testing Set\n#' test <- claudinLowData\n#' test$xd <-  medianCtr(test$xd)\n#'\n#' # Generate Predictions\n#' predout <- claudinLow(x=train$xd, classes=as.matrix(train$classes$Group,ncol=1), y=test$xd)\n#'\n#' # Obtain results\n#' results <- cbind(predout$predictions, predout$distances)\n#' #write.table(results,\"T.E.9CELL.LINE_results.txt\",sep=\"\\t\",col=T, row=FALSE)\n#'\n#' @md\n#' @import limma stats utils\n#' @export\nclaudinLow <- function(x, classes=\"\", y, nGenes=\"\", priors=\"equal\", std=FALSE,\n    distm=\"euclidean\", centroids=FALSE){\n\n  dataMatrix <- x\n  features <- dim(x)[1]\n  samples <- dim(x)[2]\n  sampleNames <- dimnames(x)[[2]]\n  featureNames <- dimnames(x)[[1]]\n\n  #parse the test file - same as train file but no rows of classes\n  tdataMatrix <- y\n  tfeatures <- dim(y)[1]\n  tsamples <- dim(y)[2]\n  tsampleNames <- dimnames(y)[[2]]\n  tfeatureNames <- dimnames(y)[[1]]\n\n  #dimnames(tdataMatrix)[[2]] <- paste(\"x\",seq(1,471))\n  temp <- overlapSets(dataMatrix,tdataMatrix)\n  dataMatrix <- temp$x\n  tdataMatrix <- temp$y\n  sfeatureNames <- row.names(dataMatrix)\n\n  # standardize both sets\n  if(std){\n    dataMatrix <- scale(dataMatrix)\n    dataMatrix <- scale(tdataMatrix)\n  }\n\n  if(!centroids){\n    thisClass <- as.vector(classes[,1])\n    nClasses <- nlevels(as.factor(thisClass))\n    classLevels <- levels(as.factor(thisClass))\n    for(j in 1:nClasses){\n      thisClass[thisClass==classLevels[j]] <- j\n    }\n    thisClass <- as.numeric(thisClass)\n    dataMatrix <- dataMatrix[,!(is.na(thisClass))]\n    thisClass <- thisClass[!(is.na(thisClass))]\n\n    scores <- apply(dataMatrix,1,limma::bwss,thisClass)\n    trainscores <- vector()\n    for(j in 1:dim(dataMatrix)[1]){\n      trainscores[j] <- scores[[row.names(dataMatrix)[j]]]$bss / scores[[row.names(dataMatrix)[j]]]$wss\n    }\n\n    dataMatrix <- dataMatrix[sort.list(trainscores,decreasing=TRUE),]\n    tdataMatrix <- tdataMatrix[sort.list(trainscores,decreasing=TRUE),]\n\n    if(nGenes==\"\"){\n      nGenes <- dim(dataMatrix)[1]\n    }\n    print(paste(\"Number of genes used:\",nGenes))\n\n    dataMatrix <- dataMatrix[1:nGenes,]\n    tdataMatrix <- tdataMatrix[1:nGenes,]\n\n    centroids <- matrix(,nrow=nGenes,ncol=nClasses)\n    for(j in 1:nClasses){\n      centroids[,j] <- apply(dataMatrix[,thisClass==j],1,mean)\n    }\n    dimnames(centroids) <- list(row.names(dataMatrix),NULL)\n\n  }else{\n    nGenes <- dim(dataMatrix)[1]\n    print(paste(\"Number of genes used:\",nGenes))\n    centroids <- dataMatrix\n    nClasses <- dim(centroids)[2]\n    classLevels <- dimnames(centroids)[[2]]\n  }\n\n  distances <- matrix(ncol=nClasses,nrow=dim(tdataMatrix)[2])\n  for(j in 1:nClasses){\n    if(distm==\"euclidean\"){\n      distances[,j] <- dist(t(cbind(centroids[,j],tdataMatrix)))[1:(dim(tdataMatrix)[2])]\n    }\n    if(distm==\"correlation\" | distm==\"pearson\"){\n      distances[,j] <- t(-1*cor(cbind(centroids[,j],tdataMatrix),use=\"pairwise.complete.obs\"))[2:(dim(tdataMatrix)[2]+1)]\n    }\n    if(distm==\"spearman\"){\n      distances[,j] <- t(-1*cor(cbind(centroids[,j],tdataMatrix),method=\"spearman\",use=\"pairwise.complete.obs\"))[2:(dim(tdataMatrix)[2]+1)]\n    }\n    colnames(distances) <- c(\"euclidian distance to Claudin-low\", \"euclidian distance to Others\")\n    rownames(distances) <- tsampleNames\n\n  }\n\n  scores <- apply(distances,1,min)\n  prediction <- vector(length=tsamples)\n  for(i in 1:tsamples){\n    prediction[i] <- classLevels[match(scores[i],distances[i,])]\n  }\n  names(prediction) <- tsampleNames\n  prediction <- data.frame(Samples=tsampleNames, prediction)\n  colnames(prediction) <- c(\"Samples\", \"Call\")\n  return(list(predictions=prediction,testData=tdataMatrix,distances=distances,centroids=centroids))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `claudinLow` function in this code snippet?",
        "answer": "The `claudinLow` function is designed for subtyping and identifying Claudin-Low Breast Cancer Samples. It performs classification based on gene expression data, calculating distances between test samples and centroids of known classes, and predicts the subtype of new samples."
      },
      {
        "question": "How does the function handle the case when pre-calculated centroids are provided?",
        "answer": "When the `centroids` parameter is set to `TRUE`, the function assumes that `x` contains pre-calculated centroids. In this case, it skips the centroid calculation step and uses the provided centroids directly for classification. The number of genes and classes are determined from the dimensions of the input centroids."
      },
      {
        "question": "What distance metrics are supported by the `claudinLow` function for determining the nearest centroid, and how are they implemented?",
        "answer": "The function supports three distance metrics: euclidean, pearson (correlation), and spearman. For euclidean distance, it uses the `dist` function. For pearson and spearman correlations, it uses the `cor` function with the appropriate method and then converts the correlation to a distance by subtracting it from 1. The choice of metric is controlled by the `distm` parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "claudinLow <- function(x, classes=\"\", y, nGenes=\"\", priors=\"equal\", std=FALSE,\n    distm=\"euclidean\", centroids=FALSE){\n\n  dataMatrix <- x\n  features <- dim(x)[1]\n  samples <- dim(x)[2]\n  sampleNames <- dimnames(x)[[2]]\n  featureNames <- dimnames(x)[[1]]\n\n  tdataMatrix <- y\n  tfeatures <- dim(y)[1]\n  tsamples <- dim(y)[2]\n  tsampleNames <- dimnames(y)[[2]]\n  tfeatureNames <- dimnames(y)[[1]]\n\n  temp <- overlapSets(dataMatrix,tdataMatrix)\n  dataMatrix <- temp$x\n  tdataMatrix <- temp$y\n  sfeatureNames <- row.names(dataMatrix)\n\n  if(std){\n    dataMatrix <- scale(dataMatrix)\n    tdataMatrix <- scale(tdataMatrix)\n  }\n\n  if(!centroids){\n    # Add code here to handle non-centroid case\n  } else {\n    # Add code here to handle centroid case\n  }\n\n  # Add code here for distance calculation and prediction\n\n  return(list(predictions=prediction,testData=tdataMatrix,distances=distances,centroids=centroids))\n}",
        "complete": "claudinLow <- function(x, classes=\"\", y, nGenes=\"\", priors=\"equal\", std=FALSE,\n    distm=\"euclidean\", centroids=FALSE){\n\n  dataMatrix <- x\n  features <- dim(x)[1]\n  samples <- dim(x)[2]\n  sampleNames <- dimnames(x)[[2]]\n  featureNames <- dimnames(x)[[1]]\n\n  tdataMatrix <- y\n  tfeatures <- dim(y)[1]\n  tsamples <- dim(y)[2]\n  tsampleNames <- dimnames(y)[[2]]\n  tfeatureNames <- dimnames(y)[[1]]\n\n  temp <- overlapSets(dataMatrix,tdataMatrix)\n  dataMatrix <- temp$x\n  tdataMatrix <- temp$y\n  sfeatureNames <- row.names(dataMatrix)\n\n  if(std){\n    dataMatrix <- scale(dataMatrix)\n    tdataMatrix <- scale(tdataMatrix)\n  }\n\n  if(!centroids){\n    thisClass <- as.vector(classes[,1])\n    nClasses <- nlevels(as.factor(thisClass))\n    classLevels <- levels(as.factor(thisClass))\n    thisClass <- as.numeric(factor(thisClass, levels = classLevels))\n    dataMatrix <- dataMatrix[,!is.na(thisClass)]\n    thisClass <- thisClass[!is.na(thisClass)]\n\n    scores <- apply(dataMatrix, 1, function(x) limma::bwss(x, thisClass))\n    trainscores <- sapply(scores, function(x) x$bss / x$wss)\n\n    order <- order(trainscores, decreasing = TRUE)\n    dataMatrix <- dataMatrix[order,]\n    tdataMatrix <- tdataMatrix[order,]\n\n    nGenes <- if(nGenes == \"\") nrow(dataMatrix) else nGenes\n    print(paste(\"Number of genes used:\", nGenes))\n\n    dataMatrix <- dataMatrix[1:nGenes,]\n    tdataMatrix <- tdataMatrix[1:nGenes,]\n\n    centroids <- sapply(1:nClasses, function(j) rowMeans(dataMatrix[,thisClass == j, drop = FALSE]))\n    dimnames(centroids) <- list(rownames(dataMatrix), NULL)\n  } else {\n    nGenes <- nrow(dataMatrix)\n    print(paste(\"Number of genes used:\", nGenes))\n    centroids <- dataMatrix\n    nClasses <- ncol(centroids)\n    classLevels <- colnames(centroids)\n  }\n\n  distances <- sapply(1:nClasses, function(j) {\n    switch(distm,\n      \"euclidean\" = sqrt(colSums((tdataMatrix - centroids[,j])^2)),\n      \"correlation\" = , \"pearson\" = 1 - cor(centroids[,j], tdataMatrix),\n      \"spearman\" = 1 - cor(centroids[,j], tdataMatrix, method = \"spearman\")\n    )\n  })\n  colnames(distances) <- c(\"euclidian distance to Claudin-low\", \"euclidian distance to Others\")\n  rownames(distances) <- tsampleNames\n\n  prediction <- factor(max.col(-distances), levels = 1:nClasses, labels = classLevels)\n  names(prediction) <- tsampleNames\n  prediction <- data.frame(Samples = tsampleNames, Call = prediction)\n\n  return(list(predictions = prediction, testData = tdataMatrix, distances = distances, centroids = centroids))\n}"
      },
      {
        "partial": "overlapSets <- function(x, y) {\n  # Add code here to find overlapping features between x and y\n  # and return the subsets of x and y with only these features\n}",
        "complete": "overlapSets <- function(x, y) {\n  common_features <- intersect(rownames(x), rownames(y))\n  x_subset <- x[common_features, , drop = FALSE]\n  y_subset <- y[common_features, , drop = FALSE]\n  return(list(x = x_subset, y = y_subset))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/setcolclass.df.R",
    "language": "R",
    "content": "#' @title Function to set the class of columns in a data.frame\n#'\n#' @description\n#' This function enables to set the class of each column in a data.frame.\n#'\n#' @usage\n#' setcolclass.df(df, colclass, factor.levels)\n#'\n#' @param df data.frame for which columns' class need to be updated.\n#' @param colclass class for each column of the data.frame.\n#' @param factor.levels\tlist of levels for each factor.\n#'\n#' @return\n#' A data.frame with columns' class and levels properly set\n#'\n#' @examples\n#' tt <- data.frame(matrix(NA, nrow=3, ncol=3, dimnames=list(1:3, paste(\"column\", 1:3, sep=\".\"))), \n#'   stringsAsFactors=FALSE)\n#' tt <- setcolclass.df(df=tt, colclass=c(\"numeric\", \"factor\", \"character\"), \n#'   factor.levels=list(NULL, c(\"F1\", \"F2\", \"F3\"), NULL))\n#'\n#' @md\n#' @export\nsetcolclass.df <-\nfunction (df, colclass, factor.levels) {\n\tww <- options()$warn\n\toptions(warn=-1)\n\ttoCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n\tdf <- replace(df, , Map(toCls, x=df, cls=colclass))\n\toptions(warn=ww)\n\tiix <- FALSE\n\tif(!missing(factor.levels)) { iix <- colclass == \"factor\" & !is.null(factor.levels) }\n\tif(any(iix)) {\n\t\tfor(i in which(iix)) { levels(df[[i]]) <- factor.levels[[i]] }\n\t}\n\treturn(df)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setcolclass.df` function in R, and what are its main parameters?",
        "answer": "The `setcolclass.df` function is designed to set the class of columns in a data frame. It takes three main parameters: `df` (the input data frame), `colclass` (a vector specifying the desired class for each column), and `factor.levels` (an optional list of levels for factor columns)."
      },
      {
        "question": "How does the function handle warnings during the class conversion process?",
        "answer": "The function temporarily suppresses warnings by setting `options(warn=-1)` at the beginning of the function and restores the original warning level at the end using `options(warn=ww)`. This prevents potential warnings from being displayed during the class conversion process."
      },
      {
        "question": "Explain the purpose of the `toCls` function within `setcolclass.df` and how it is used.",
        "answer": "The `toCls` function is a helper function that converts a value `x` to a specified class `cls`. It uses `do.call` to dynamically construct and call the appropriate conversion function (e.g., `as.numeric`, `as.factor`, `as.character`). This function is then used with `Map` to apply the class conversions to all columns of the data frame."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setcolclass.df <- function(df, colclass, factor.levels) {\n  ww <- options()$warn\n  options(warn=-1)\n  toCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n  df <- replace(df, , Map(toCls, x=df, cls=colclass))\n  options(warn=ww)\n  # Complete the function to handle factor levels\n  \n  return(df)\n}",
        "complete": "setcolclass.df <- function(df, colclass, factor.levels) {\n  ww <- options()$warn\n  options(warn=-1)\n  toCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n  df <- replace(df, , Map(toCls, x=df, cls=colclass))\n  options(warn=ww)\n  iix <- !missing(factor.levels) && colclass == \"factor\" & !sapply(factor.levels, is.null)\n  if(any(iix)) {\n    for(i in which(iix)) { levels(df[[i]]) <- factor.levels[[i]] }\n  }\n  return(df)\n}"
      },
      {
        "partial": "setcolclass.df <- function(df, colclass, factor.levels) {\n  # Implement the function body\n  # Hint: Use Map() to apply class conversion\n  # Don't forget to handle factor levels\n}",
        "complete": "setcolclass.df <- function(df, colclass, factor.levels) {\n  ww <- options()$warn\n  options(warn=-1)\n  toCls <- function(x, cls) { do.call(paste(\"as\", cls, sep = \".\"), list(x)) }\n  df <- replace(df, , Map(toCls, x=df, cls=colclass))\n  options(warn=ww)\n  if(!missing(factor.levels)) {\n    iix <- colclass == \"factor\" & !sapply(factor.levels, is.null)\n    if(any(iix)) {\n      for(i in which(iix)) { levels(df[[i]]) <- factor.levels[[i]] }\n    }\n  }\n  return(df)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/subtype.cluster.predict.R",
    "language": "R",
    "content": "#' @title Function to identify breast cancer molecular subtypes using\n#'   the Subtype Clustering Model\n#'\n#' @description\n#' This function identifies the breast cancer molecular subtypes using a\n#'   Subtype Clustering Model fitted by subtype.cluster.\n#'\n#' @usage\n#' subtype.cluster.predict(sbt.model, data, annot, do.mapping = FALSE,\n#'   mapping, do.prediction.strength = FALSE,\n#'   do.BIC = FALSE, plot = FALSE, verbose = FALSE)\n#'\n#' @param sbt.model\tSubtype Clustering Model as returned by subtype.cluster.\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept\n#'   for each gene), FALSE otherwise.\n#' @param mapping\t**DEPRECATED** Matrix with columns \"EntrezGene.ID\" and\n#'   \"probe\" used to force the mapping such that the probes are not selected\n#'   based on their variance.\n#' @param do.prediction.strength TRUE if the prediction strength must be\n#'   computed (Tibshirani and Walther 2005), FALSE otherwise.\n#' @param do.BIC TRUE if the Bayesian Information Criterion must be computed\n#'   for number of clusters ranging from 1 to 10, FALSE otherwise.\n#' @param plot TRUE if the patients and their corresponding subtypes must\n#'   be plotted, FALSE otherwise.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - subtype: Subtypes identified by the Subtype Clustering Model.\n#'   Subtypes can be either \"ER-/HER2-\", \"HER2+\" or \"ER+/HER2-\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated\n#'   by the Subtype Clustering Model.\n#' - prediction.strength: Prediction strength for subtypes.\n#' - BIC: Bayesian Information Criterion for the Subtype Clustering Model\n#'   with number of clusters ranging from 1 to 10.\n#' - subtype2: Subtypes identified by the Subtype Clustering Model using\n#'   AURKA to discriminate low and high proliferative tumors. Subtypes can be\n#'   either \"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\" or\n#'   \"ER+/HER2- Low Prolif\".\n#' - subtype.proba2: Probabilities to belong to each subtype (including\n#'   discrimination between lowly and highly proliferative ER+/HER2- tumors,\n#'   see subtype2) estimated by the Subtype Clustering Model.\n#' - prediction.strength2: Prediction strength for subtypes2.\n#' - module.scores: Matrix containing ESR1, ERBB2 and AURKA module scores.\n#' - mapping: Mapping if necessary (list of matrices with 3 columns: probe,\n#'   EntrezGene.ID and new.probe).\n#'\n#' @references\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G,\n#'   Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes\n#'   associated with breast cancer clinical outcome depend on the molecular\n#'   subtypes\", Clinical Cancer Research, 14(16):5158-5165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B,\n#'   Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ\n#'   and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in\n#'   Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing\n#'   and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' Tibshirani R and Walther G (2005) \"Cluster Validation by Prediction\n#'   Strength\", Journal of Computational and Graphical Statistics,\n#'   14(3):511-528\n#'\n#' @seealso\n#' [genefu::subtype.cluster], [genefu::scmod1.robust], [genefu::scmod2.robust]\n#'\n#' @examples\n#' # without mapping (affy hgu133a or plus2 only)\n#' # load VDX data\n#' data(vdxs)\n#' data(scmgene.robust)\n#'\n#' # Subtype Clustering Model fitted on EXPO and applied on VDX\n#' sbt.vdxs <- subtype.cluster.predict(sbt.model=scmgene.robust, data=data.vdxs,\n#'   annot=annot.vdxs, do.mapping=FALSE, do.prediction.strength=FALSE,\n#'   do.BIC=FALSE, plot=TRUE, verbose=TRUE)\n#' table(sbt.vdxs$subtype)\n#' table(sbt.vdxs$subtype2)\n#'\n#' # with mapping\n#' # load NKI data\n#' data(nkis)\n#' # Subtype Clustering Model fitted on EXPO and applied on NKI\n#' sbt.nkis <- subtype.cluster.predict(sbt.model=scmgene.robust, data=data.nkis,\n#'   annot=annot.nkis, do.mapping=TRUE, do.prediction.strength=FALSE,\n#'   do.BIC=FALSE, plot=TRUE, verbose=TRUE)\n#' table(sbt.nkis$subtype)\n#' table(sbt.nkis$subtype2)\n#'\n#' @md\n#' @import graphics\n#' @export\nsubtype.cluster.predict <-\nfunction(sbt.model, data, annot, do.mapping=FALSE, mapping,\n    do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n{\n\tif(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n\tsbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n\tsbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\n\tif(is.list(sbt.model)) {\n\t\t## retrieve model\n\t\tsubtype.c <- sbt.model[!is.element(names(sbt.model), c(\"cutoff.AURKA\", \"mod\"))]\n\t\tmodel.name <- subtype.c$parameters$variance$modelName\n\t\tcc <- sbt.model$gaussian.AURKA\n\t\tmq <- sbt.model$rescale.q\n\t\tm.mod <- sbt.model$mod\n\t} else {\n\t\t## read model file\n\t\trr <- readLines(con=sbt.model, n=11)[-1]\n\t\tnn <- unlist(lapply(X=rr, FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[1], split=\" \")); x <- x[length(x)]; return(x); }))\n\t\tm.param <- c(list(rr[[1]]), lapply(X=rr[-1], FUN=function(x) { x <- as.numeric(unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \"))); x <- x[!is.na(x)]; return(x)}))\n\t\tnames(m.param) <- nn\n\t\tcn <- unlist(lapply(strsplit(nn[grep(pattern=\"mean\", x=nn)], split=\"[.]\"), FUN=function(x) { return(x[[2]]) }))\n\t\tm.mod <- read.m.file(sbt.model, comment.char=\"#\")\n\t\t#construct a fake mclust object with the parameters of the model\n\t\tsubtype.c <- NULL\n\t\ttt <- m.param$pro\n\t\tnames(tt) <- cn\n\t\tsubtype.c$parameters$pro <- tt\n\t\ttt <- sapply(X=m.param[grep(pattern=\"mean\", x=nn)], FUN=function(x) { return(x) })\n\t\tdimnames(tt) <- list(names(m.mod)[1:2], cn)\n\t\tsubtype.c$parameters$mean <- tt\n\t\tsubtype.c$parameters$variance$modelName <- model.name <- m.param$modelname\n\t\tsubtype.c$parameters$variance$d <- 2\n\t\tsubtype.c$parameters$variance$G <- 3\n\t\ttt <- matrix(0, ncol=2, nrow=2, dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2]))\n\t\tdiag(tt) <- m.param$sigma\n\t\tsubtype.c$parameters$variance$sigma <- array(tt, dim=c(2,2,3), dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2], cn))\n\t\tsubtype.c$parameters$variance$Sigma <- tt\n\t\tsubtype.c$parameters$variance$scale <- m.param$scale\n\t\tsubtype.c$parameters$variance$shape <- m.param$shape\n\t\tcc <- c(\"mean\"=m.param$gaussian.AURKA.mean, \"sigma\"=m.param$gaussian.AURKA.sigma)\n\t\tmq <- m.param$rescale.q\n\t}\n\tdo.scale <- ifelse(is.na(mq), FALSE, TRUE)\n\tsbt <- rep(NA, nrow(data))\n\tnames(sbt) <- dimnames(data)[[1]]\n\tsbt.proba <- matrix(NA, nrow(data), ncol=length(sbtn), dimnames=list(dimnames(data)[[1]], sbtn))\n\n\tsigs.esr1 <- sig.score(x=m.mod$ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsigs.erbb2 <- sig.score(x=m.mod$ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsigs.aurka <- sig.score(x=m.mod$AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\t## signature scores\n\tdd <- cbind(\"ESR1\"=sigs.esr1$score, \"ERBB2\"=sigs.erbb2$score, \"AURKA\"=sigs.aurka$score)\n\t## mapping\n\tmymap <- list(\"ESR1\"=sigs.esr1$probe, \"ERBB2\"=sigs.erbb2$probe, \"AURLA\"=sigs.aurka$probe)\n\tcln <- dimnames(subtype.c$parameters$mean)[[2]] <- as.character(1:ncol(subtype.c$parameters$mean))\n\n\tif(do.scale) {\n\t\t## the rescaling needs a large sample size!!!\n\t\t## necessary if we want to validate the classifier using a different dataset\n\t\t## the estimation of survival probabilities depends on the scale of the score\n\t\tdd <- apply(dd, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n\t}\n\trownames(dd) <- rownames(data)\n\tdd2 <- dd\n\n\tcc.ix <- complete.cases(dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE])\n\tif(all(!cc.ix)) {\n\t\tps.res <- ps.res2 <- BIC.res <- NULL\n\t\tif(do.prediction.strength) {\n\t\t\ttt <- rep(NA, length(sbtn))\n\t\t\tnames(tt) <- sbtn\n\t\t\ttt2 <- rep(NA, length(sbtn2))\n\t\t\tnames(tt2) <- sbtn2\n\t\t\tps.res <- list(\"ps\"=NA, \"ps.cluster\"=tt, \"ps.individual\"=sbt)\n\t\t\tps.res2 <- list(\"ps\"=NA, \"ps.cluster\"=tt2, \"ps.individual\"=sbt)\n\t\t}\n\t\tif(do.BIC) {\n\t\t\tBIC.res <- rep(NA, 10)\n\t\t\tnames(BIC.res) <- 1:10\n\t\t}\n\t\treturn(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt, \"prediction.strength2\"=ps.res2))\n\t}\n\tdd <- dd[cc.ix, , drop=FALSE]\n\n\temclust.ts <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=subtype.c$parameters)\n\tdimnames(emclust.ts$z) <- list(dimnames(dd)[[1]], cln)\n\tclass.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n\tnames(class.ts) <- dimnames(dd)[[1]]\n\tuclass <- sort(unique(class.ts))\n\tuclass <- uclass[!is.na(uclass)]\n\n\tps.res <- ps.res2 <- NULL\n\tif(do.prediction.strength) {\n\t\tif(nrow(dd) < 10) {\n\t\t\twarning(\"at least 10 observations are required to compute the prediction strength!\")\n\t\t\ttt <- rep(NA, length(sbtn))\n\t\t\tnames(tt) <- sbtn\n\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\tps.res <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t\ttt <- rep(NA, length(sbtn2))\n\t\t\tnames(tt) <- sbtn2\n\t\t\tps.res2 <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t} else {\n\t\t\t## computation of the prediction strength of the clustering\n\t\t\trr3 <- mclust::Mclust(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=model.name, G=3)\n\t\t\t## redefine classification to be coherent with subtypes\n\t\t\tuclass <- sort(unique(rr3$classification))\n\t\t\tuclass <- uclass[!is.na(uclass)]\n\t\t\tif(length(uclass) != 3) {\n\t\t\t\twarning(\"less than 3 subtypes are identified!\")\n\t\t\t\ttt <- rep(NA, length(sbtn))\n\t\t\t\tnames(tt) <- sbtn\n\t\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\t\tps.res <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t\t\ttt <- rep(NA, length(sbtn2))\n\t\t\t\tnames(tt) <- sbtn2\n\t\t\t\tps.res2 <- list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2)\n\t\t\t} else {\n\t\t\t\tmm <- NULL\n\t\t\t\tfor(i in 1:length(uclass)) {\n\t\t\t\t\tmm <- c(mm, median(dd[rr3$classification == uclass[i],\"ERBB2\"], na.rm=TRUE) )\n\t\t\t\t}\n\t\t\t\tnclass <-  uclass[order(mm, decreasing=TRUE)[1]]\n\t\t\t\tmm <- NULL\n\t\t\t\tfor(i in 1:length(uclass[-nclass])) {\n\t\t\t\t\tmm <- c(mm, median(dd[rr3$classification == uclass[-nclass][i],\"ESR1\"], na.rm=TRUE) )\n\t\t\t\t}\n\t\t\t\tnclass <- c(uclass[-nclass][order(mm, decreasing=TRUE)[2]], nclass, uclass[-nclass][order(mm, decreasing=TRUE)[1]])\n\t\t\t\t## nclass contains the new order\n\t\t\t\tncl <- rr3$classification\n\t\t\t\tfor(i in 1:length(uclass)) {\n\t\t\t\t\tncl[rr3$classification == nclass[i]] <- i\n\t\t\t\t}\n\t\t\t\t## use the previously computed model to fit a new model in a supervised manner\n\t\t\t\tmyclass <- mclust::unmap(ncl)\n\t\t\t\tdimnames(myclass) <-  list(dimnames(dd)[[1]], sbtn)\n\t\t\t\tmclust.tr <- mclust::mstep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], z=myclass)\n\t\t\t\tdimnames(mclust.tr$z) <- dimnames(myclass)\n\t\t\t\temclust.tr <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=mclust.tr$parameters)\n\t\t\t\tdimnames(emclust.tr$z) <- dimnames(myclass)\n\t\t\t\tclass.tr <- mclust::map(emclust.tr$z, warn=FALSE)\n\t\t\t\tnames(class.tr) <- dimnames(dd)[[1]]\n\t\t\t\t## prediction strength\n\t\t\t\tps.res <- ps.cluster(cl.tr=class.ts, cl.ts=class.tr, na.rm=TRUE)\n\t\t\t\tnames(ps.res$ps.cluster) <- sbtn\n\t\t\t\t## check for missing values in ps.individual\n\t\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\t\ttt2[names(ps.res$ps.individual)] <- ps.res$ps.individual\n\t\t\t\tps.res$ps.individual <- tt2\n\t\t\t\t## prediction strength with the separation in high and low proliferative tumors\n\t\t\t\t## since proliferation is a continuum we fit a Gaussian using AURKA expression of the ER+/HER2- tumors\n\t\t\t\t## refitted model\n\t\t\t\ttt <- mclust::Mclust(dd[complete.cases(class.tr, dd[ , \"AURKA\"]) & class.tr == 3, \"AURKA\"], modelNames=\"E\", G=1)\n\t\t\t\tgauss.prolif <- c(\"mean\"=tt$parameters$mean, \"sigma\"=tt$parameters$variance$sigmasq)\n\t\t\t\tclass.tr2 <- class.tr\n\t\t\t\tclass.tr2[class.tr == 3] <- NA\n\t\t\t\t## probability that tumor is highly proliferative\n\t\t\t\tpprolif <- pnorm(q=dd[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t\t\t\t## high proliferation\n\t\t\t\tclass.tr2[class.tr == 3 & pprolif >= 0.5 & complete.cases(class.tr, pprolif)] <- 3\n\t\t\t\t## low proliferation\n\t\t\t\tclass.tr2[class.tr == 3 & pprolif < 0.5 & complete.cases(class.tr, pprolif)] <- 4\n\t\t\t\t## existing model\n\t\t\t\ttt <- mclust::Mclust(dd[complete.cases(class.ts, dd[ , \"AURKA\"]) & class.ts == 3, \"AURKA\"], modelNames=\"E\", G=1)\n\t\t\t\tgauss.prolif <- c(\"mean\"=tt$parameters$mean, \"sigma\"=tt$parameters$variance$sigmasq)\n\t\t\t\tclass.ts2 <- class.ts\n\t\t\t\tclass.ts2[class.ts == 3] <- NA\n\t\t\t\t## probability that tumor is highly proliferative\n\t\t\t\tpprolif <- pnorm(q=dd[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t\t\t\t## high proliferation\n\t\t\t\tclass.ts2[class.ts == 3 & pprolif >= 0.5 & complete.cases(class.ts, pprolif)] <- 3\n\t\t\t\t## low proliferation\n\t\t\t\tclass.ts2[class.ts == 3 & pprolif < 0.5 & complete.cases(class.ts, pprolif)] <- 4\n\t\t\t\t## compute the prediction strength\n\t\t\t\tps.res2 <- ps.cluster(cl.tr=class.ts2, cl.ts=class.tr2, na.rm=TRUE)\n\t\t\t\tnames(ps.res2$ps.cluster) <- sbtn2\n\t\t\t\t## check for missing values in ps.individual\n\t\t\t\ttt2 <- rep(NA, nrow(dd2))\n\t\t\t\tnames(tt2) <- dimnames(dd2)[[1]]\n\t\t\t\ttt2[names(ps.res2$ps.individual)] <- ps.res2$ps.individual\n\t\t\t\tps.res2$ps.individual <- tt2\n\t\t\t}\n\t\t}\n\t}\n\n\tBIC.res <- NULL\n\tif(do.BIC) {\n\t\tif(nrow(dd) >= 10) { BIC.res <- mclust::mclustBIC(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=c(model.name), G=1:10)[ ,model.name] } else { warning(\"at least 10 observations are required to compute the BIC!\") }\n\t}\n\n\t## subtypes\n\tsbt[names(class.ts)] <- sbtn[class.ts]\n\tsbt.proba[dimnames(emclust.ts$z)[[1]], ] <- emclust.ts$z\n\t## discriminate between luminal A and B using AURKA\n\tgauss.prolif <- cc\n\tsbt2 <- sbt\n\tsbt2[sbt == sbtn[3]] <- NA\n\t## probability that tumor is highly proliferative\n\tpprolif <- pnorm(q=dd2[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t## high proliferation\n\tsbt2[sbt == sbtn[3] & pprolif >= 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[3]\n\t## low proliferation\n\tsbt2[sbt == sbtn[3] & pprolif < 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[4]\n\t## subtype probabilities for luminal B and A\n\tsbt.proba2 <- matrix(NA, nrow(data), ncol=length(sbtn2), dimnames=list(dimnames(data)[[1]], sbtn2))\n\ttt <- sbt.proba[ , sbtn[3]]\n\ttt2 <- pprolif\n\ttt <- cbind(tt * tt2, tt * (1 - tt2))\n\tcolnames(tt) <- sbtn2[3:4]\n\tsbt.proba2[ , sbtn2[1:2]] <- sbt.proba[ , sbtn[1:2]]\n\tsbt.proba2[ , sbtn2[3:4]] <- tt[ , sbtn2[3:4]]\n\n\tif(plot) {\n\t\tif(do.scale) {\n\t\t\tmyxlim <- myylim <- c(-2, 2)\n\t\t} else {\n\t\t\tmyxlim <- range(dd[ , \"ESR1\"])\n\t\t\tmyylim <- range(dd[ , \"ERBB2\"])\n\t\t}\n\t\t## plot the clusters with proliferation\n\t\tmycol <- mypch <- rep(NA, length(sbt2))\n\t\tmycol[sbt2 == sbtn2[1]] <- \"darkred\"\n\t\tmycol[sbt2 == sbtn2[2]] <- \"darkgreen\"\n\t\tmycol[sbt2 == sbtn2[3]] <- \"darkorange\"\n\t\tmycol[sbt2 == sbtn2[4]] <- \"darkviolet\"\n\t\tmypch[sbt2 == sbtn2[1]] <- 17\n\t\tmypch[sbt2 == sbtn2[2]] <- 0\n\t\tmypch[sbt2 == sbtn2[3] | sbt2 == sbtn2[4]] <- 10\n\t\tmypch <- as.numeric(mypch)\n\t\tnames(mycol) <- names(mypch) <- names(sbt2)\n\t\tplot(x=dd[ , \"ESR1\"], y=dd[ , \"ERBB2\"], xlim=myxlim, ylim=myylim, xlab=\"ESR1\", ylab=\"ERBB2\", col=mycol[dimnames(dd)[[1]]], pch=mypch[dimnames(dd)[[1]]])\n\t\tlegend(x=\"topleft\", col=c(\"darkred\", \"darkgreen\", \"darkorange\", \"darkviolet\"), legend=sbtn2, pch=c(17, 0, 10, 10), bty=\"n\")\n\t}\n\n\treturn(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt2, \"subtype.proba2\"=sbt.proba2, \"prediction.strength2\"=ps.res2, \"module.scores\"=dd2, \"mapping\"=mymap))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `subtype.cluster.predict` function?",
        "answer": "The main purpose of the `subtype.cluster.predict` function is to identify breast cancer molecular subtypes using a Subtype Clustering Model. It takes a pre-fitted model, gene expression data, and annotations as input, and returns predicted subtypes along with various related statistics and probabilities."
      },
      {
        "question": "How does the function handle the distinction between high and low proliferative tumors within the ER+/HER2- subtype?",
        "answer": "The function uses the AURKA gene expression to distinguish between high and low proliferative tumors within the ER+/HER2- subtype. It fits a Gaussian distribution to the AURKA expression of ER+/HER2- tumors and uses a threshold of 0.5 on the cumulative distribution function to classify tumors as either 'ER+/HER2- High Prolif' or 'ER+/HER2- Low Prolif'."
      },
      {
        "question": "What are the key components returned by the `subtype.cluster.predict` function?",
        "answer": "The function returns a list containing several key components: 'subtype' (predicted subtypes), 'subtype.proba' (probabilities for each subtype), 'subtype2' (subtypes including proliferation status for ER+/HER2-), 'subtype.proba2' (probabilities for subtypes including proliferation status), 'prediction.strength' (if requested), 'BIC' (Bayesian Information Criterion, if requested), 'module.scores' (ESR1, ERBB2, and AURKA module scores), and 'mapping' (probe mapping information)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE, mapping,\n    do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n{\n    if(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n    sbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n    sbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\n    if(is.list(sbt.model)) {\n        subtype.c <- sbt.model[!is.element(names(sbt.model), c(\"cutoff.AURKA\", \"mod\"))]\n        model.name <- subtype.c$parameters$variance$modelName\n        cc <- sbt.model$gaussian.AURKA\n        mq <- sbt.model$rescale.q\n        m.mod <- sbt.model$mod\n    } else {\n        # Code for reading model file\n    }\n\n    # More code...\n\n}",
        "complete": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE, mapping,\n    do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n{\n    if(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n    sbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n    sbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\n    if(is.list(sbt.model)) {\n        subtype.c <- sbt.model[!is.element(names(sbt.model), c(\"cutoff.AURKA\", \"mod\"))]\n        model.name <- subtype.c$parameters$variance$modelName\n        cc <- sbt.model$gaussian.AURKA\n        mq <- sbt.model$rescale.q\n        m.mod <- sbt.model$mod\n    } else {\n        rr <- readLines(con=sbt.model, n=11)[-1]\n        nn <- unlist(lapply(X=rr, FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[1], split=\" \")); x <- x[length(x)]; return(x); }))\n        m.param <- c(list(rr[[1]]), lapply(X=rr[-1], FUN=function(x) { x <- as.numeric(unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \"))); x <- x[!is.na(x)]; return(x)}))\n        names(m.param) <- nn\n        cn <- unlist(lapply(strsplit(nn[grep(pattern=\"mean\", x=nn)], split=\"[.]\"), FUN=function(x) { return(x[[2]]) }))\n        m.mod <- read.m.file(sbt.model, comment.char=\"#\")\n        subtype.c <- NULL\n        tt <- m.param$pro\n        names(tt) <- cn\n        subtype.c$parameters$pro <- tt\n        tt <- sapply(X=m.param[grep(pattern=\"mean\", x=nn)], FUN=function(x) { return(x) })\n        dimnames(tt) <- list(names(m.mod)[1:2], cn)\n        subtype.c$parameters$mean <- tt\n        subtype.c$parameters$variance$modelName <- model.name <- m.param$modelname\n        subtype.c$parameters$variance$d <- 2\n        subtype.c$parameters$variance$G <- 3\n        tt <- matrix(0, ncol=2, nrow=2, dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2]))\n        diag(tt) <- m.param$sigma\n        subtype.c$parameters$variance$sigma <- array(tt, dim=c(2,2,3), dimnames=list(names(m.mod)[1:2], names(m.mod)[1:2], cn))\n        subtype.c$parameters$variance$Sigma <- tt\n        subtype.c$parameters$variance$scale <- m.param$scale\n        subtype.c$parameters$variance$shape <- m.param$shape\n        cc <- c(\"mean\"=m.param$gaussian.AURKA.mean, \"sigma\"=m.param$gaussian.AURKA.sigma)\n        mq <- m.param$rescale.q\n    }\n\n    # Rest of the function implementation...\n\n    return(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt2, \"subtype.proba2\"=sbt.proba2, \"prediction.strength2\"=ps.res2, \"module.scores\"=dd2, \"mapping\"=mymap))\n}"
      },
      {
        "partial": "sigs.esr1 <- sig.score(x=m.mod$ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.erbb2 <- sig.score(x=m.mod$ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.aurka <- sig.score(x=m.mod$AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\n# Signature scores\ndd <- cbind(\"ESR1\"=sigs.esr1$score, \"ERBB2\"=sigs.erbb2$score, \"AURKA\"=sigs.aurka$score)\n\n# Mapping\nmymap <- list(\"ESR1\"=sigs.esr1$probe, \"ERBB2\"=sigs.erbb2$probe, \"AURLA\"=sigs.aurka$probe)\ncln <- dimnames(subtype.c$parameters$mean)[[2]] <- as.character(1:ncol(subtype.c$parameters$mean))\n\nif(do.scale) {\n    dd <- apply(dd, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n}\nrownames(dd) <- rownames(data)\ndd2 <- dd\n\n# More code...",
        "complete": "sigs.esr1 <- sig.score(x=m.mod$ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.erbb2 <- sig.score(x=m.mod$ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\nsigs.aurka <- sig.score(x=m.mod$AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\n# Signature scores\ndd <- cbind(\"ESR1\"=sigs.esr1$score, \"ERBB2\"=sigs.erbb2$score, \"AURKA\"=sigs.aurka$score)\n\n# Mapping\nmymap <- list(\"ESR1\"=sigs.esr1$probe, \"ERBB2\"=sigs.erbb2$probe, \"AURLA\"=sigs.aurka$probe)\ncln <- dimnames(subtype.c$parameters$mean)[[2]] <- as.character(1:ncol(subtype.c$parameters$mean))\n\nif(do.scale) {\n    dd <- apply(dd, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n}\nrownames(dd) <- rownames(data)\ndd2 <- dd\n\ncc.ix <- complete.cases(dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE])\nif(all(!cc.ix)) {\n    ps.res <- ps.res2 <- BIC.res <- NULL\n    if(do.prediction.strength) {\n        tt <- rep(NA, length(sbtn))\n        names(tt) <- sbtn\n        tt2 <- rep(NA, length(sbtn2))\n        names(tt2) <- sbtn2\n        ps.res <- list(\"ps\"=NA, \"ps.cluster\"=tt, \"ps.individual\"=sbt)\n        ps.res2 <- list(\"ps\"=NA, \"ps.cluster\"=tt2, \"ps.individual\"=sbt)\n    }\n    if(do.BIC) {\n        BIC.res <- rep(NA, 10)\n        names(BIC.res) <- 1:10\n    }\n    return(list(\"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"prediction.strength\"=ps.res, \"BIC\"=BIC.res, \"subtype2\"=sbt, \"prediction.strength2\"=ps.res2))\n}\ndd <- dd[cc.ix, , drop=FALSE]\n\nemclust.ts <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=subtype.c$parameters)\ndimnames(emclust.ts$z) <- list(dimnames(dd)[[1]], cln)\nclass.ts <- mclust::map(emclust.ts$z, warn=FALSE)\nnames(class.ts) <- dimnames(dd)[[1]]\nuclass <- sort(unique(class.ts))\nuclass <- uclass[!is.na(uclass)]\n\n# Rest of the function implementation..."
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/intrinsic.cluster.predict.R",
    "language": "R",
    "content": "#' @title Function to identify breast cancer molecular subtypes using the Single\n#'   Sample Predictor (SSP)\n#'\n#' @description\n#' This function identifies the breast cancer molecular subtypes using a Single\n#'   Sample Predictor (SSP) fitted by intrinsic.cluster.\n#'\n#' @usage\n#' intrinsic.cluster.predict(sbt.model, data, annot, do.mapping = FALSE,\n#'   mapping, do.prediction.strength = FALSE, verbose = FALSE)\n#'\n#' @param sbt.model Subtype Clustering Model as returned by intrinsic.cluster.\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot    Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed\n#'   (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#    mapping such that the probes are not selected based on their variance.\n#' @param do.prediction.strength TRUE if the prediction strength must be computed\n#'   (Tibshirani and Walther 2005), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - subtype: Subtypes identified by the SSP. For published intrinsic gene lists, subtypes\n#'   can be either \"Basal\", \"Her2\", \"LumA\", \"LumB\" or \"Normal\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated from the\n#' correlations to each centroid.\n#' - cor: Correlation coefficient to each centroid.\n#' - prediction.strength: Prediction strength for subtypes.\n#' - subtype.train: Classification (similar to subtypes) computed during fitting of\n#'   the model for prediction strength.\n#' - centroids.map: Mapped probes from the intrinsic gene list used to compute the\n#'   centroids.\n#' - profiles: Intrinsic gene expression profiles for each sample.\n#'\n#' @references\n#' T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and A. Nobel and\n#'   S. Deng and H. Johnsen and R. Pesich and S. Geister and J. Demeter and C. Perou and\n#'   P. E. Lonning and P. O. Brown and A. L. Borresen-Dale and D. Botstein (2003) \"Repeated\n#'   Observation of Breast Tumor Subtypes in Independent Gene Expression Data Sets\",\n#'   Proceedings of the National Academy of Sciences, 1(14):8418\u20138423\n#' \n#' Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and Qaqish,\n#'   Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and Dressler, Lynn and\n#'   Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer, Lynda and Wu, Junyuan and\n#'   Liu, Yudong and Nanda, Rita and Tretiakova, Maria and Orrico, Alejandra and Dreher,\n#'   Donna and Palazzo, Juan and Perreard, Laurent and Nelson, Edward and Mone, Mary and\n#'   Hansen, Heidi and Mullins, Michael and Quackenbush, John and Ellis, Matthew and Olopade,\n#'   Olufunmilayo and Bernard, Philip and Perou, Charles (2006) \"The molecular portraits of\n#'   breast tumors are conserved across microarray platforms\", BMC Genomics, 7(96)\n#' \n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, Samuel and\n#'   Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, Christiane and He,\n#'   Xiaping and Hu, Zhiyuan and Quackenbush, John F. and Stijleman, Inge J. and Palazzo,\n#'   Juan and Marron, J.S. and Nobel, Andrew B. and Mardis, Elaine and Nielsen, Torsten O.\n#'   and Ellis, Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009) \"Supervised\n#'   Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\", Journal of Clinical\n#'   Oncology, 27(8):1160\u20131167\n#' \n#' Tibshirani R and Walther G (2005) \"Cluster Validation by Prediction Strength\",\n#'   Journal of Computational and Graphical Statistics, 14(3):511\u2013528\n#'\n#' @seealso\n#' [genefu::intrinsic.cluster], [genefu::ssp2003], [genefu::ssp2006], [genefu::pam50]\n#'\n#' @examples\n#' # load SSP fitted in Sorlie et al. 2003\n#' data(ssp2003)\n#' # load NKI data\n#' data(nkis)\n#' # SSP2003 applied on NKI\n#' ssp2003.nkis <- intrinsic.cluster.predict(sbt.model=ssp2003,\n#'   data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   do.prediction.strength=FALSE, verbose=TRUE)\n#' table(ssp2003.nkis$subtype)\n#'\n#' @md\n#' @export\n#' @name intrinsic.cluster.predict\nintrinsic.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE,\n                                      mapping, do.prediction.strength=FALSE,\n                                      verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(sbt.model)) { stop(\"data, annot and sbt.mod parameters must be specified\") }\n    if (!is.matrix(data)) { data <- as.matrix(data) }\n\n    if(is.list(sbt.model)) {\n        ## retrieve model\n        centroids <- sbt.model$centroids\n        annot.centroids <- sbt.model$centroids.map\n        method.cor <- sbt.model$method.cor\n        method.centroids <- sbt.model$method.centroids\n        std <- sbt.model$std\n        mq <- sbt.model$rescale.q\n        mins <- sbt.model$mins\n    } else {\n        ## read model file\n        ## retrieve centroids\n        annot.centroids <- read.csv(sbt.model, comment.char=\"#\", stringsAsFactors=FALSE)\n        dimnames(annot.centroids)[[1]] <- annot.centroids[ , \"probe\"]\n        ## retrieve model parameters\n        rr <- readLines(con=sbt.model)[-1]\n        rr <- rr[sapply(rr, function(x) { return(substr(x=x, start=1, stop=1) == \"#\") })]\n        nn <- unlist(lapply(X=rr, FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[1], split=\" \")); x <- x[length(x)]; return(x); }))\n        rr2 <- unlist(lapply(X=rr[is.element(nn, c(\"method.cor\", \"method.centroids\", \"std\", \"rescale.q\", \"mins\"))], FUN=function(x) { x <- unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \")); x <- x[length(x)]; return(x); }))\n        method.cor <- rr2[nn == \"method.cor\"]\n        method.centroids <- rr2[nn == \"method.centroids\"]\n        std <- rr2[nn == \"std\"]\n        mq <- as.numeric(rr2[nn == \"rescale.q\"])\n        mins <- as.numeric(rr2[nn == \"mins\"])\n        rr <- rr[!is.element(nn, c(\"method.cor\", \"method.centroids\", \"std\", \"rescale.q\", \"mins\"))]\n        nn <- nn[!is.element(nn, c(\"method.cor\", \"method.centroids\", \"std\", \"rescale.q\", \"mins\"))]\n        cent <- lapply(X=rr, FUN=function(x) { x <- as.numeric(unlist(strsplit(x=unlist(strsplit(x=x, split=\":\"))[2], split=\" \"))); x <- x[!is.na(x)]; return(x)})\n        centroids <- NULL\n        for(i in 1:length(cent)) { centroids <- cbind(centroids, cent[[i]]) }\n        nn <- unlist(lapply(X=strsplit(x=nn, split=\"centroid.\"), FUN=function(x) { return(x[[2]]) }))\n        dimnames(centroids) <- list(dimnames(annot.centroids)[[1]], nn)\n    }\n\n    number.cluster <- ncol(centroids)\n    if(is.null(dimnames(centroids)[[2]])) { \n        name.cluster <- paste(\"cluster\", 1:ncol(centroids), sep=\".\") \n    } else { \n        name.cluster <- dimnames(centroids)[[2]] \n    }\n\n    gt <- nrow(centroids)\n    #mapping\n    if(do.mapping) {\n        #mapping through EntrezGene.ID\n        #remove (arbitrarily) duplicated gene ids\n        centroids.gid <- as.character(annot.centroids[, \"EntrezGene.ID\"])\n        names(centroids.gid) <- as.character(annot.centroids[, \"probe\"])\n        myx <- !duplicated(centroids.gid) & !is.na(centroids.gid)\n        centroids.gid <- centroids.gid[myx]\n        annot.centroids <- annot.centroids[myx, , drop=FALSE]\n        centroids <- centroids[myx, , drop=FALSE]\n        gid <- as.character(annot[ ,\"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[, \"probe\"])\n        if (missing(mapping)) { ## select the most variant probes using annotations\n            ## if multiple probes for one gene, keep the most variant\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=centroids.gid, verbose=FALSE)\n            nn <- match(rr$geneid2, centroids.gid)\n            nn <- nn[!is.na(nn)]\n            centroids.gid <- centroids.gid[nn]\n            annot.centroids <- annot.centroids[nn, ]\n            centroids <- centroids[nn, , drop=FALSE]\n            data <- rr$data1\n        } else { # use a predefined mapping\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            # keep only centroids genes with mapped probes\n            myx <- is.element(centroids.gid, nn)\n            centroids.gid <- centroids.gid[myx]\n            annot.centroids <- annot.centroids[myx, , drop=FALSE]\n            centroids <- centroids[myx, , drop=FALSE]\n            pp <- as.character(mapping[match(centroids.gid, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            centroids.gid <- centroids.gid[myx]\n            annot.centroids <- annot.centroids[myx, , drop=FALSE]\n            centroids <- centroids[myx, , drop=FALSE]\n            pp <- pp[myx]\n            data <- data[ , pp, drop=FALSE]\n        }\n    }\n    else {\n        if(all(!is.element(dimnames(data)[[2]], dimnames(centroids)[[1]]))) { stop(\"no probe in common -> annot or mapping parameters are necessary for the mapping process!\") }\n        ## no mapping are necessary\n        myx <- intersect(dimnames(centroids)[[1]], dimnames(data)[[2]])\n        data <- data[ ,myx, drop=FALSE]\n        centroids <- centroids[myx, , drop=FALSE]\n    }\n    centroids.map <- cbind(\"probe\"=dimnames(data)[[2]], \"probe.centroids\"=dimnames(centroids)[[1]], \"EntrezGene.ID\"=as.character(annot[dimnames(data)[[2]], \"EntrezGene.ID\"]))\n    dimnames(centroids.map)[[1]] <- dimnames(data)[[2]]\n    gm <- nrow(centroids)\n    if(gm == 0 || (sum(is.na(data)) / length(data)) > 0.9) { ## no mapping or too many missing values\n        ncl <- rep(NA, nrow(data))\n        names(ncl) <- dimnames(data)[[1]]\n        nproba <- ncor <- matrix(NA, nrow=nrow(data), ncol=ncol(centroids), dimnames=list(dimnames(data)[[1]], name.cluster))\n        ps.res <- NULL\n        if(do.prediction.strength) { ps.res <- list(\"ps\"=NA, \"ps.cluster\"=ncor[ , 1], \"ps.individual\"=ncl) }\n        tt <- matrix(NA, ncol=nrow(centroids.map), nrow=nrow(data), dimnames=list(dimnames(data)[[1]], dimnames(centroids.map)[[1]]))\n        return(list(\"subtype\"=ncl, \"subtype.proba\"=nproba, \"cor\"=ncor, \"prediction.strength\"=ps.res, \"centroids.map\"=centroids.map, \"profiles\"=tt))\n    }\n    if(verbose) { message(sprintf(\"%i/%i probes are used for clustering\", gm, gt)) }\n\n    #standardization of the gene expressions\n    switch(std,\n    \"scale\"={\n        data <- scale(data, center=TRUE, scale=TRUE)\n        if(verbose) { message(\"standardization of the gene expressions\") }\n    },\n    \"robust\"={\n        data <- apply(data, 2, function(x) { return((rescale(x, q=mq, na.rm=TRUE) - 0.5) * 2) })\n        if(verbose) { message(\"robust standardization of the gene expressions\") }\n    },\n    \"none\"={ if(verbose) { message(\"no standardization of the gene expressions\") } })\n\n    ## apply the nearest centroid classifier to classify the samples again\n    ncor <- t(apply(X=data, MARGIN=1, FUN=function(x, y, method.cor) {\n      rr <- array(NA, dim=ncol(y), dimnames=list(colnames(y)))\n    if (sum(complete.cases(x, y)) > 3) {\n      rr <- cor(x=x, y=y, method=method.cor, use=\"complete.obs\")\n    }\n    return (rr)\n  }, y=centroids, method.cor=method.cor))\n    #nproba <- t(apply(X=ncor, MARGIN=1, FUN=function(x) { return(abs(x) / sum(abs(x), na.rm=TRUE)) }))\n    ## negative correlations are truncated to zero since they have no meaning for subtypes identification\n    nproba <- t(apply(X=ncor, MARGIN=1, FUN=function (x) {\n    rr <- array(NA, dim=length(x), dimnames=list(names(x)))\n    x[!is.na(x) & x < 0] <- 0\n    if (!all(is.na(x))) {\n      rr <- x / sum(x, na.rm=TRUE)\n    }\n    return (rr)\n  }))\n    dimnames(ncor) <- dimnames(nproba) <- list(dimnames(data)[[1]], name.cluster)\n    ncl <- apply(X=ncor, MARGIN=1, FUN=function(x) { return(order(x, decreasing=TRUE, na.last=TRUE)[1]) })\n    names(ncl) <- dimnames(data)[[1]]\n    ## names of identified clusters\n    ncln <- name.cluster[ncl]\n    names(ncln) <- dimnames(data)[[1]]\n\n    ## if one or more subtypes have not been identified, remove them for prediction strength\n    myx <- sort(unique(ncl))\n    myx <- myx[!is.na(myx)]\n    name.cluster2 <- name.cluster[myx]\n    number.cluster2 <- length(myx)\n\n    ps.res <- ncl2 <- NULL\n    if(do.prediction.strength) {\n        ## compute the clustering and cut the dendrogram\n        ## hierarchical clustering with correlation-based distance and average linkage\n        hcl <- amap::hcluster(x=data, method=\"correlation\", link=\"average\")\n        mins.ok <- stop.ok <- FALSE\n        nbc <- number.cluster2\n        nclust.best <- 1\n        while(!mins.ok && !stop.ok) { ## until each cluster contains at least mins samples\n            cl <- cutree(tree=hcl, k=nbc)\n            tt <- table(cl)\n            if(sum(tt >= mins) >= number.cluster2) {\n                if(nbc > number.cluster2) { ## put NA for clusters with less than mins samples\n                    td <- names(tt)[tt < mins]\n                    cl[is.element(cl, td)] <- NA\n                    ## rename the clusters\n                    ucl <- sort(unique(cl))\n                    ucl <- ucl[!is.na(ucl)]\n                    cl2 <- cl\n                    for(i in 1:number.cluster2) { cl2[cl == ucl[i] & !is.na(cl)] <- i }\n                    cl <- cl2\n                }\n                nclust.best <- number.cluster2\n                mins.ok <- TRUE\n            } else {\n                if(sum(tt >= mins) > nclust.best) {\n                    nbc.best <- nbc\n                    nclust.best <- sum(tt >= mins)\n                }\n                nbc <- nbc + 1\n                if(nbc > (nrow(data) - (number.cluster2 * mins))) {\n                    warning(sprintf(\"impossible to find %i main clusters with at least %i individuals!\", number.cluster2, mins))\n                    stop.ok <- TRUE\n                }\n            }\n            if(stop.ok) { ## no convergence for the clustering with mininmum set of individuals\n                cl <- cutree(tree=hcl, k=nbc.best)\n                tt <- table(cl)\n                td <- names(tt)[tt < mins]\n                cl[is.element(cl, td)] <- NA\n                ## rename the clusters\n                ucl <- sort(unique(cl))\n                ucl <- ucl[!is.na(ucl)]\n                cl2 <- cl\n                for(i in 1:nclust.best) { cl2[cl == ucl[i] & !is.na(cl)] <- i }\n                cl <- cl2\n            }\n        }\n        ## compute the centroids\n        ## take the core samples in each cluster to compute the centroid\n        ## not feasible due to low intra correlation within clusters!!!\n        ## minimal pairwise cor of approx 0.3\n        #cl2 <- cutree(tree=hcl, h=0.7)\n        #table(cl, cl2) to detect which core cluster of samples for which cluster.\n        cl.centroids <- matrix(NA, nrow=ncol(data), ncol=nclust.best, dimnames=list(dimnames(data)[[2]], paste(\"cluster\", 1:nclust.best, sep=\".\")))\n        for(i in 1:ncol(cl.centroids)) {\n            switch(method.centroids,\n            \"mean\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], MARGIN=2, FUN=mean, na.rm=TRUE, trim=0.025) },\n            \"median\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], MARGIN=2, FUN=median, na.rm=TRUE) },\n            \"tukey\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], MARGIN=2, FUN=tbrm, na.rm=TRUE, C=9) })\n        }\n        #apply the nearest centroid classifier to classify the samples again\n        ncor2 <- t(apply(X=data, MARGIN=1, FUN=function(x, y, z) { return(cor(x, y, method=z, use=\"complete.obs\")) }, y=cl.centroids, z=method.cor))\n        nproba2 <- t(apply(X=ncor2, MARGIN=1, FUN=function(x) { return(abs(x) / sum(abs(x), na.rm=TRUE)) }))\n        dimnames(ncor2) <- dimnames(nproba2) <- list(dimnames(data)[[1]], dimnames(cl.centroids)[[2]])\n        ncl2 <- apply(X=ncor2, MARGIN=1, FUN=function(x) { return(order(x, decreasing=TRUE)[1]) })\n        names(ncl2) <- dimnames(data)[[1]]\n        ## rename clusters since we do not expect to get the same id per cluster\n        ## this avoids a warning in ps.cluster\n        uncl <- sort(unique(ncl))\n        uncl <- uncl[!is.na(uncl)]\n        nclt <- ncl\n        for(mm in 1:length(uncl)) {\n            nclt[ncl == uncl[mm]] <- mm\n        }\n        uncl2 <- sort(unique(ncl2))\n        uncl2 <- uncl2[!is.na(uncl2)]\n        ncl2t <- ncl2\n        for(mm in 1:length(uncl2)) {\n            ncl2t[ncl2 == uncl2[mm]] <- mm\n        }\n        #prediction strength\n        ps.res <- ps.cluster(cl.tr=ncl2t, cl.ts=nclt, na.rm=TRUE)\n        ## put NA for clusters which are potentially not present in the dataset\n        tt <- rep(NA, length(name.cluster))\n        names(tt) <- name.cluster\n        tt[name.cluster2] <- ps.res$ps.cluster\n        ps.res$ps.cluster <- tt\n    }\n\n\n    return(list(\"subtype\"=ncln, \"subtype.proba\"=nproba, \"cor\"=ncor, \"prediction.strength\"=ps.res, \"subtype.train\"=ncl2, \"profiles\"=data, \"centroids.map\"=centroids.map))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `intrinsic.cluster.predict` function?",
        "answer": "The primary purpose of the `intrinsic.cluster.predict` function is to identify breast cancer molecular subtypes using a Single Sample Predictor (SSP) fitted by intrinsic.cluster. It takes gene expression data, annotations, and a subtype clustering model as input, and returns predicted subtypes, probabilities, correlations, and other related information."
      },
      {
        "question": "How does the function handle missing data or mapping issues?",
        "answer": "The function handles missing data and mapping issues in several ways: 1) It performs optional mapping through EntrezGene IDs if `do.mapping` is set to TRUE. 2) It selects the most variant probes when multiple probes exist for one gene. 3) If no mapping is possible or if there are too many missing values (>90%), it returns NA values for subtypes, probabilities, and correlations. 4) It provides informative messages about the number of probes used for clustering when `verbose` is set to TRUE."
      },
      {
        "question": "What methods are available for standardizing gene expressions in this function?",
        "answer": "The function offers three methods for standardizing gene expressions, controlled by the `std` parameter: 1) 'scale': uses the `scale` function to center and scale the data. 2) 'robust': applies a robust standardization using the `rescale` function. 3) 'none': no standardization is applied. The chosen method is reported in verbose mode."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/intrinsic.cluster.R",
    "language": "R",
    "content": "#' @title Function to fit a Single Sample Predictor (SSP) as in \n#'   Perou, Sorlie, Hu, and Parker publications\n#'\n#' @description\n#' This function fits the Single Sample Predictor (SSP) as published\n#'   in Sorlie et al 2003, Hu et al 2006 and Parker et al 2009. This model \n#'   is actually a nearest centroid classifier where the centroids\n#'   representing the breast cancer molecular subtypes are identified\n#'   through hierarchical clustering using an \"intrinsic gene list\".\n#'\n#' @usage\n#' intrinsic.cluster(data, annot, do.mapping = FALSE, mapping,\n#'   std = c(\"none\", \"scale\", \"robust\"), rescale.q = 0.05, intrinsicg,\n#'   number.cluster = 3, mins = 5, method.cor = c(\"spearman\", \"pearson\"),\n#'   method.centroids = c(\"mean\", \"median\", \"tukey\"), filen, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot    Matrix of annotations with at least one column named \n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping    TRUE if the mapping through Entrez Gene ids must \n#'   be performed (in case of ambiguities, the most variant probe is kept \n#'   for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to \n#'   force the mapping such that the probes are not selected based on their \n#'   variance.\n#' @param std Standardization of gene expressions: scale for traditional \n#'   standardization based on mean and standard deviation, robust for \n#'   standardization based on the 0.025 and 0.975 quantiles, none to keep \n#'   gene expressions unchanged.\n#' @param rescale.q Proportion of expected outliers for (robust) rescaling \n#'   the gene expressions.\n#' @param intrinsicg Intrinsic gene lists. May be specified by the user as \n#'   a matrix with at least 2 columns named probe and EntrezGene.ID for the \n#'   probe names and the corresponding Entrez Gene ids. The intrinsic gene lists \n#'   published by Sorlie et al. 2003, Hu et al. 2006 and Parker et al. 2009 are \n#'   stored in ssp2003, ssp2006 and pam50 respectively.\n#' @param number.cluster The number of main clusters to be identified by \n#'   hierarchical clustering.\n#' @param mins The minimum number of samples to be in a main cluster.\n#' @param method.cor Correlation coefficient used to identified the nearest\n#'   centroid. May be spearman or pearson.\n#' @param method.centroids Method to compute a centroid from gene expressions \n#'   of a cluster of samples: mean, median or tukey (Tukey's Biweight Robust Mean).\n#' @param filen Name of the csv file where the subtype clustering model must be \n#'   stored.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - model: Single Sample Predictor\n#' - subtype: Subtypes identified by the SSP. For published intrinsic gene lists,\n#'   subtypes can be either \"Basal\", \"Her2\", \"LumA\", \"LumB\" or \"Normal\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated from the\n#'   correlations to each centroid.\n#' - cor: Correlation coefficient to each centroid.\n#'\n#' @references\n#' T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and \n#'   A. Nobel and S. Deng and H. Johnsen and R. Pesich and S. Geister and J. \n#'   Demeter and C. Perou and P. E. Lonning and P. O. Brown and A. L. Borresen-Dale \n#'   and D. Botstein (2003) \"Repeated Observation of Breast Tumor Subtypes in \n#'   Independent Gene Expression Data Sets\", Proceedings of the National Academy of \n#'   Sciences, 1(14):8418-8423\n#' \n#' Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and \n#'   Qaqish, Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and \n#'   ressler, Lynn and Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer,\n#'   Lynda and Wu, Junyuan and Liu, Yudong and Nanda, Rita and Tretiakova, Maria \n#'   and Orrico, Alejandra and Dreher, Donna and Palazzo, Juan and Perreard, \n#'   Laurent and Nelson, Edward and Mone, Mary and Hansen, Heidi and Mullins, \n#'   Michael and Quackenbush, John and Ellis, Matthew and Olopade, Olufunmilayo and \n#'   Bernard, Philip and Perou, Charles (2006) \"The molecular portraits of breast \n#'   tumors are conserved across microarray platforms\", BMC Genomics, 7(96)\n#'\n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, \n#'   Samuel and Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, \n#'   Christiane and He, Xiaping and Hu, Zhiyuan and Quackenbush, John F. and \n#'   Stijleman, Inge J. and Palazzo, Juan and Marron, J.S. and Nobel, Andrew B. \n#'   and Mardis, Elaine and Nielsen, Torsten O. and Ellis, Matthew J. and Perou, \n#'   Charles M. and Bernard, Philip S. (2009) \"Supervised Risk Predictor of Breast \n#'   Cancer Based on Intrinsic Subtypes\", Journal of Clinical Oncology, \n#'   27(8):1160-1167\n#'\n#' @seealso\n#' [genefu::subtype.cluster], [genefu::intrinsic.cluster.predict], [genefu::ssp2003], [genefu::ssp2006], [genefu::pam50]\n#'\n#' @examples\n#' # load SSP signature published in Sorlie et al. 2003\n#' data(ssp2003)\n#' # load NKI data\n#' data(nkis)\n#' # load VDX data\n#' data(vdxs)\n#' ssp2003.nkis <- intrinsic.cluster(data=data.nkis, annot=annot.nkis,\n#'   do.mapping=TRUE, std=\"robust\",\n#'   intrinsicg=ssp2003$centroids.map[ ,c(\"probe\", \"EntrezGene.ID\")],\n#'   number.cluster=5, mins=5, method.cor=\"spearman\",\n#'   method.centroids=\"mean\", verbose=TRUE)\n#' str(ssp2003.nkis, max.level=1)\n#'\n#' @md\n#' @export\nintrinsic.cluster <- function(data, annot, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        intrinsicg, number.cluster=3, mins=5, \n        method.cor=c(\"spearman\", \"pearson\"), \n        method.centroids=c(\"mean\", \"median\", \"tukey\"), filen, verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(intrinsicg)) { \n        stop(\"data, annot, and intrinsicg parameters must be specified\") \n    }\n    std <- match.arg(std)\n    method.cor <- match.arg(method.cor)\n    method.centroids <- match.arg(method.centroids)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        ## mapping through EntrezGene.ID\n        if (is.matrix(intrinsicg) || is.data.frame(intrinsicg)) {\n            ## matrix of annotations instead of a list of EntrezGene.IDs\n            tt <- as.character(intrinsicg[, \"EntrezGene.ID\"])\n            names(tt) <- as.character(intrinsicg[, \"probe\"])\n            intrinsicg <- tt\n        }\n        gt <- length(intrinsicg)\n        ## EntrezGene.IDs should be numeric or character that can be tranformed into numeric\n        ## remove (arbitrarily) duplicated gene ids\n        intrinsicg <- intrinsicg[!duplicated(intrinsicg) & !is.na(intrinsicg)]\n        gid <- as.character(annot[ , \"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[ , \"probe\"])\n        gid.intrinsic <- as.character(intrinsicg)\n        names(gid.intrinsic) <- paste(\"geneid\", gid.intrinsic, sep=\".\")\n        if(missing(mapping)) { # select the most variant probes using annotations\n            # if multiple probes for one gene, keep the most variant\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=gid.intrinsic, verbose=FALSE)\n            nn <- match(rr$geneid2, gid.intrinsic)\n            nn <- nn[!is.na(nn)]\n            intrinsicg <- intrinsicg[nn]\n            data <- rr$data1\n        } else { # use a predefined mapping\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            # keep only intrinsic genes with mapped probes\n            myx <- is.element(gid.intrinsic, nn)\n            gid.intrinsic <- gid.intrinsic[myx]\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- as.character(mapping[match(gid.intrinsic, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- pp[myx]\n            data <- data[, pp, drop=FALSE]\n        }\n    }\n    else {\n        if (is.matrix(intrinsicg) || is.data.frame(intrinsicg)) {\n            ## matrix of annotations instead of a list of EntrezGene.IDs\n            tt <- as.character(intrinsicg[ , \"probe\"])\n            names(tt) <- as.character(intrinsicg[ , \"probe\"])\n            intrinsicg <- tt\n        }\n        gt <- length(intrinsicg)\n        if(all(!is.element(dimnames(data)[[2]], intrinsicg))) { stop(\"no probe in common -> annot or mapping parameters are necessary for the mapping process!\") }\n        ## no mapping are necessary\n        intrinsicg <- intersect(intrinsicg, dimnames(data)[[2]])\n        data <- data[, intrinsicg]\n    }\n    gm <- length(intrinsicg)\n    if (gm == 0 || (sum(is.na(data)) / length(data)) > 0.9) {\n        stop(\"none of the instrinsic genes are present or too many missing values!\")\n    }\n    if (!is.null(names(intrinsicg))) {\n        centroids.map <- cbind(\"probe\"=dimnames(data)[[2]],\n            \"probe.centroids\"=names(intrinsicg),\n            \"EntrezGene.ID\"=as.character(annot[dimnames(data)[[2]],\n            \"EntrezGene.ID\"]))\n    } else {\n        centroids.map <- cbind(\"probe\"=dimnames(data)[[2]],\n            \"probe.centroids\"=NA,\n            \"EntrezGene.ID\"=as.character(annot[dimnames(data)[[2]],\n            \"EntrezGene.ID\"]))\n    }\n    dimnames(centroids.map)[[1]] <- dimnames(data)[[2]]\n\n    if (verbose) {\n        message(sprintf(\"%i/%i probes are used for clustering\", gm, gt))\n    }\n\n    switch(std,\n    \"scale\"={\n        data <- scale(data, center=TRUE, scale=TRUE)\n        if (verbose) message(\"standardization of the gene expressions\")\n    }, \n    \"robust\"={\n        data <- apply(data, 2, function(x) {\n            (rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2\n        })\n        if(verbose) message(\"robust standardization of the gene expressions\")\n    },\n    \"none\"={\n        if(verbose) message(\"no standardization of the gene expressions\")\n    })\n\n    ## compute the clustering and cut the dendrogram\n    ## hierarchical clustering with correlation-based distance and average linkage\n    hcl <- amap::hcluster(x=data, method=\"correlation\", link=\"average\")\n    mins.ok <- FALSE\n    nbc <- number.cluster\n    while (!mins.ok) { ## until each cluster contains at least mins samples\n        cl <- cutree(tree=hcl, k=nbc)\n        tt <- table(cl)\n        if (sum(tt >= mins) >= number.cluster) {\n            if (nbc > number.cluster) { ## put NA for clusters with less than mins samples\n                td <- names(tt)[tt < mins]\n                cl[is.element(cl, td)] <- NA\n                ## rename the clusters\n                ucl <- sort(unique(cl))\n                ucl <- ucl[!is.na(ucl)]\n                cl2 <- cl\n                for (i in 1:number.cluster) { \n                    cl2[cl == ucl[i] & !is.na(cl)] <- i \n                }\n                cl <- cl2\n            }\n            mins.ok <- TRUE\n        } else {\n            nbc <- nbc + 1\n            if (nbc > (nrow(data) - (number.cluster * mins))) {\n                stop(\"clusters are too small (size < mins)!\")\n            }\n        }\n    }\n\n    ## compute the centroids\n    ## take the core samples in each cluster to compute the centroid\n    ## not feasible due to low intra correlation within clusters!!!\n    ## minimal pairwise cor of approx 0.3\n    cl.centroids <- matrix(NA, nrow=ncol(data), ncol=number.cluster,\n        dimnames=list(dimnames(data)[[2]], paste(\"cluster\", 1:number.cluster, sep=\".\")))\n    for (i in 1:ncol(cl.centroids)) {\n        switch(method.centroids,\n        \"mean\"={ \n            cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], \n                MARGIN=2, FUN=mean, na.rm=TRUE, trim=0.025)\n        }, \n        \"median\"={ \n            cl.centroids[, i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], \n                MARGIN=2, FUN=median, na.rm=TRUE)\n        }, \n        \"tukey\"={ cl.centroids[ ,i] <- apply(X=data[cl == i & !is.na(cl), ,drop=FALSE], \n            MARGIN=2, FUN=tbrm, na.rm=TRUE, C=9)\n        })\n    }\n\n    ## apply the nearest centroid classifier to classify the samples again\n    ncor <- t(apply(X=data, MARGIN=1, FUN=function(x, y, z) {\n        return(cor(x, y, method=z, use=\"complete.obs\"))\n    }, y=cl.centroids, z=method.cor))\n    ## negative correlations are truncated to zero since they have no meaning for subtypes identification\n    nproba <- t(apply(X=ncor, MARGIN=1, FUN=function(x) {\n        x[x < 0] <- 0; return(x / sum(x, na.rm=TRUE)); \n    }))\n    dimnames(ncor) <- dimnames(nproba) <- list(dimnames(data)[[1]], dimnames(cl.centroids)[[2]])\n    ncl <- apply(X=ncor, MARGIN=1, FUN=function(x) {\n        order(x, decreasing=TRUE, na.last=TRUE)[1]\n    })\n    ncl <- dimnames(cl.centroids)[[2]][ncl]\n    names(ncl) <- dimnames(data)[[1]]\n\n    if (!missing(filen)) {\n        #save model parameters in a csv file for reuse\n        write(x=sprintf(\"# Benjamin Haibe-Kains. All rights reserved.\"), sep=\"\", file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# method.cor: %s\", method.cor), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# method.centroids: %s\", method.centroids), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# std: %s\", std), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# rescale.q: %s\", rescale.q), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write(x=sprintf(\"# mins: %i\", mins), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        ## centroids\n        mycent <- t(cl.centroids)\n        for(i in 1:nrow(mycent)) { \n            write(x=sprintf(\"# centroid.%s: %s\", dimnames(mycent)[[1]][i], \n                paste(mycent[i, ], collapse=\" \")), sep=\"\", append=TRUE, \n                file=paste(filen, \"csv\", sep=\".\")) \n        }\n        ## centroids.map\n        write(paste(\"\\\"\", dimnames(centroids.map)[[2]], \"\\\"\", collapse=\",\", sep=\"\"), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n        write.table(centroids.map, sep=\",\", col.names=FALSE, row.names=FALSE, file=paste(filen, \"csv\", sep=\".\"), append=TRUE)\n    }\n\n    return(list(\n        \"model\"=list(\"method.cor\"=method.cor, \n            \"method.centroids\"=method.centroids, \"std\"=std,\n            \"rescale.q\"=rescale.q,  \"mins\"=mins, \"centroids\"=cl.centroids,\n            \"centroids.map\"=centroids.map),\n        \"subtype\"=ncl,\n        \"subtype.proba\"=nproba,\n        \"cor\"=ncor\n        )\n    )\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `intrinsic.cluster` function?",
        "answer": "The `intrinsic.cluster` function is designed to fit a Single Sample Predictor (SSP) for breast cancer molecular subtyping. It implements a nearest centroid classifier where centroids representing breast cancer molecular subtypes are identified through hierarchical clustering using an 'intrinsic gene list'."
      },
      {
        "question": "How does the function handle gene mapping and what options are available for this process?",
        "answer": "The function offers two options for gene mapping: 1) If `do.mapping=TRUE`, it maps probes to genes using Entrez Gene IDs. By default, it selects the most variant probe for each gene. 2) If a `mapping` parameter is provided, it uses a predefined mapping. The mapping process ensures that the correct probes are used for the intrinsic gene list, handling potential ambiguities in probe-to-gene relationships."
      },
      {
        "question": "What methods are available for computing centroids in this function, and how are they implemented?",
        "answer": "The function offers three methods for computing centroids: 'mean', 'median', and 'tukey' (Tukey's Biweight Robust Mean). These are specified using the `method.centroids` parameter. The chosen method is applied to the gene expression data of samples within each cluster to compute the centroid. For example, if 'mean' is selected, it calculates the average expression for each gene across all samples in a cluster, with a 2.5% trim to remove potential outliers."
      }
    ],
    "completion_tasks": [
      {
        "partial": "intrinsic.cluster <- function(data, annot, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        intrinsicg, number.cluster=3, mins=5, \n        method.cor=c(\"spearman\", \"pearson\"), \n        method.centroids=c(\"mean\", \"median\", \"tukey\"), filen, verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(intrinsicg)) { \n        stop(\"data, annot, and intrinsicg parameters must be specified\") \n    }\n    std <- match.arg(std)\n    method.cor <- match.arg(method.cor)\n    method.centroids <- match.arg(method.centroids)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        # Implement mapping logic here\n    }\n    else {\n        # Implement non-mapping logic here\n    }\n\n    # Implement standardization logic here\n\n    # Implement clustering and centroid computation here\n\n    # Implement nearest centroid classification here\n\n    # Implement file saving logic here\n\n    return(list(\n        \"model\" = list(),\n        \"subtype\" = NULL,\n        \"subtype.proba\" = NULL,\n        \"cor\" = NULL\n    ))\n}",
        "complete": "intrinsic.cluster <- function(data, annot, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        intrinsicg, number.cluster=3, mins=5, \n        method.cor=c(\"spearman\", \"pearson\"), \n        method.centroids=c(\"mean\", \"median\", \"tukey\"), filen, verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(intrinsicg)) { \n        stop(\"data, annot, and intrinsicg parameters must be specified\") \n    }\n    std <- match.arg(std)\n    method.cor <- match.arg(method.cor)\n    method.centroids <- match.arg(method.centroids)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        intrinsicg <- intrinsicg[!duplicated(intrinsicg) & !is.na(intrinsicg)]\n        gid <- as.character(annot[, \"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[, \"probe\"])\n        gid.intrinsic <- as.character(intrinsicg)\n        names(gid.intrinsic) <- paste(\"geneid\", gid.intrinsic, sep=\".\")\n        if(missing(mapping)) {\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=gid.intrinsic, verbose=FALSE)\n            nn <- match(rr$geneid2, gid.intrinsic)\n            nn <- nn[!is.na(nn)]\n            intrinsicg <- intrinsicg[nn]\n            data <- rr$data1\n        } else {\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            myx <- is.element(gid.intrinsic, nn)\n            gid.intrinsic <- gid.intrinsic[myx]\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- as.character(mapping[match(gid.intrinsic, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            intrinsicg <- intrinsicg[myx, ]\n            pp <- pp[myx]\n            data <- data[, pp, drop=FALSE]\n        }\n    } else {\n        intrinsicg <- intersect(intrinsicg, dimnames(data)[[2]])\n        data <- data[, intrinsicg]\n    }\n\n    switch(std,\n    \"scale\"={ data <- scale(data, center=TRUE, scale=TRUE) },\n    \"robust\"={ data <- apply(data, 2, function(x) { (rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2 }) },\n    \"none\"={ })\n\n    hcl <- amap::hcluster(x=data, method=\"correlation\", link=\"average\")\n    cl <- cutree(tree=hcl, k=number.cluster)\n\n    cl.centroids <- matrix(NA, nrow=ncol(data), ncol=number.cluster,\n        dimnames=list(dimnames(data)[[2]], paste(\"cluster\", 1:number.cluster, sep=\".\")))\n    for (i in 1:ncol(cl.centroids)) {\n        cl.centroids[, i] <- switch(method.centroids,\n            \"mean\" = apply(data[cl == i & !is.na(cl), , drop=FALSE], 2, mean, na.rm=TRUE),\n            \"median\" = apply(data[cl == i & !is.na(cl), , drop=FALSE], 2, median, na.rm=TRUE),\n            \"tukey\" = apply(data[cl == i & !is.na(cl), , drop=FALSE], 2, tbrm, na.rm=TRUE, C=9)\n        )\n    }\n\n    ncor <- t(apply(data, 1, function(x, y) cor(x, y, method=method.cor, use=\"complete.obs\"), y=cl.centroids))\n    nproba <- t(apply(ncor, 1, function(x) { x[x < 0] <- 0; x / sum(x, na.rm=TRUE) }))\n    ncl <- apply(ncor, 1, function(x) dimnames(cl.centroids)[[2]][which.max(x)])\n\n    if (!missing(filen)) {\n        write.csv(cl.centroids, file=paste(filen, \"csv\", sep=\".\"), row.names=TRUE)\n    }\n\n    return(list(\n        \"model\" = list(\"method.cor\"=method.cor, \"method.centroids\"=method.centroids, \n                      \"std\"=std, \"rescale.q\"=rescale.q, \"mins\"=mins, \n                      \"centroids\"=cl.centroids),\n        \"subtype\" = ncl,\n        \"subtype.proba\" = nproba,\n        \"cor\" = ncor\n    ))\n}"
      },
      {
        "partial": "intrinsic.cluster.predict <- function(data, annot, model, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(model)) { \n        stop(\"data, annot, and model parameters must be specified\") \n    }\n    std <- match.arg(std)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        # Implement mapping logic here\n    }\n    else {\n        # Implement non-mapping logic here\n    }\n\n    # Implement standardization logic here\n\n    # Implement prediction logic here\n\n    return(list(\n        \"subtype\" = NULL,\n        \"subtype.proba\" = NULL,\n        \"cor\" = NULL\n    ))\n}",
        "complete": "intrinsic.cluster.predict <- function(data, annot, model, do.mapping=FALSE,\n        mapping, std=c(\"none\", \"scale\", \"robust\"), rescale.q=0.05,\n        verbose=FALSE) {\n\n    if(missing(data) || missing(annot) || missing(model)) { \n        stop(\"data, annot, and model parameters must be specified\") \n    }\n    std <- match.arg(std)\n    if (!is.matrix(data)) {\n        data <- as.matrix(data)\n    }\n\n    ## mapping\n    if (do.mapping) {\n        gid <- as.character(annot[, \"EntrezGene.ID\"])\n        names(gid) <- as.character(annot[, \"probe\"])\n        gid.model <- as.character(model$centroids.map[, \"EntrezGene.ID\"])\n        names(gid.model) <- as.character(model$centroids.map[, \"probe\"])\n        if(missing(mapping)) {\n            rr <- geneid.map(geneid1=gid, data1=data, geneid2=gid.model, verbose=FALSE)\n            data <- rr$data1\n        } else {\n            nn <- as.character(mapping[, \"EntrezGene.ID\"])\n            pp <- as.character(mapping[match(gid.model, nn), \"probe\"])\n            myx <- is.element(pp, dimnames(data)[[2]])\n            pp <- pp[myx]\n            data <- data[, pp, drop=FALSE]\n        }\n    } else {\n        common.probes <- intersect(dimnames(data)[[2]], model$centroids.map[, \"probe\"])\n        data <- data[, common.probes]\n    }\n\n    switch(std,\n    \"scale\"={ data <- scale(data, center=TRUE, scale=TRUE) },\n    \"robust\"={ data <- apply(data, 2, function(x) { (rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2 }) },\n    \"none\"={ })\n\n    ncor <- t(apply(data, 1, function(x, y) cor(x, y, method=model$method.cor, use=\"complete.obs\"), y=model$centroids))\n    nproba <- t(apply(ncor, 1, function(x) { x[x < 0] <- 0; x / sum(x, na.rm=TRUE) }))\n    ncl <- apply(ncor, 1, function(x) colnames(model$centroids)[which.max(x)])\n\n    return(list(\n        \"subtype\" = ncl,\n        \"subtype.proba\" = nproba,\n        \"cor\" = ncor\n    ))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcSigs.R",
    "language": "R",
    "content": "#' import utils\n#' @keywords internal\n#' @noRd\n`.ovcSigs` <-\nfunction(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ## annotations\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            #save(list=\"sigAngiogenic\", compress=TRUE, file=file.path(system.file(package=\"genefu\"), \"data\", \"sigAngiogenic.rda\"))\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigOvcAngiogenic.rda\")\n        },\n        \"crijns2009_sig\"={\n            ss <- \"hgnc_symbol\"\n            gid <- as.character(sig[ ,\"Gene.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"Weights\"]))\n            sigOvcCrijns <- annot\n            save(list=\"sigOvcCrijns\", compress=TRUE, file=\"sigOvcCrijns.rda\")\n        },\n        \"yoshihara2010_sig\"={\n            ss <- \"refseq_mrna\"\n            gid <- as.character(sig[ ,\"GenBank\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot2 <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot2[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot2[ ,-1,drop=FALSE], annot, \"weight\"=as.numeric(sig[ ,\"beta_ridge\"]))\n            sigOvcYoshihara <- annot\n            save(list=\"sigOvcYoshihara\", compress=TRUE, file=\"sigOvcYoshihara.rda\")\n        },\n        \"spentzos2011_sig\"={\n            ss <- \"affy_hg_u133a\"\n            gid <- as.character(sig[ ,\"probeset\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=sig[ ,\"weight\"])\n            sigOvcSpentzos <- annot\n            save(list=\"sigOvcSpentzos\", compress=TRUE, file=\"sigOvcSpentzos.rda\")\n        },\n        \"tcga2011_sig\"={\n            ss <- \"entrezgene\"\n            gid <- as.character(sig[ ,\"Entrez.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(paste(\"geneid\", gid, sep=\".\"), colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, sig[ ,c(\"Gene.set\", \"beta\", \"p.value\")])\n            sigOvcTCGA <- annot\n            save(list=\"sigOvcTCGA\", compress=TRUE, file=\"sigOvcTCGA.rda\")\n        })\n    }\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `.ovcSigs` function in this code snippet?",
        "answer": "The main purpose of the `.ovcSigs` function is to process and annotate different ovarian cancer gene signatures. It reads signature data from CSV files, retrieves gene annotations from the Ensembl database using biomaRt, and saves the annotated signatures as RDA files. The function handles five different signatures: bentink2012_angiogenic, crijns2009_sig, yoshihara2010_sig, spentzos2011_sig, and tcga2011_sig."
      },
      {
        "question": "How does the function handle different gene identifiers for each signature?",
        "answer": "The function uses different gene identifiers for each signature by setting the `ss` variable to the appropriate identifier type. For example, it uses 'illumina_humanwg_6_v2' for bentink2012_angiogenic, 'hgnc_symbol' for crijns2009_sig, 'refseq_mrna' for yoshihara2010_sig, 'affy_hg_u133a' for spentzos2011_sig, and 'entrezgene' for tcga2011_sig. This allows the function to correctly map the gene identifiers to the Ensembl database for annotation."
      },
      {
        "question": "What is the significance of the `switch` statement in this function?",
        "answer": "The `switch` statement in this function is crucial for handling the different signature types. It allows the function to execute specific code blocks for each signature, accommodating their unique data structures and annotation requirements. This design makes the function flexible and capable of processing multiple signature types within a single function, while keeping the code organized and maintainable."
      }
    ],
    "completion_tasks": [
      {
        "partial": "`.ovcSigs` <- function(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigOvcAngiogenic.rda\")\n        },\n        # Complete the switch statement for other signature types\n        )\n    }\n}",
        "complete": "`.ovcSigs` <- function(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigOvcAngiogenic.rda\")\n        },\n        \"crijns2009_sig\"={\n            ss <- \"hgnc_symbol\"\n            gid <- as.character(sig[ ,\"Gene.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"Weights\"]))\n            sigOvcCrijns <- annot\n            save(list=\"sigOvcCrijns\", compress=TRUE, file=\"sigOvcCrijns.rda\")\n        },\n        \"yoshihara2010_sig\"={\n            ss <- \"refseq_mrna\"\n            gid <- as.character(sig[ ,\"GenBank\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot2 <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot2[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot2[ ,-1,drop=FALSE], annot, \"weight\"=as.numeric(sig[ ,\"beta_ridge\"]))\n            sigOvcYoshihara <- annot\n            save(list=\"sigOvcYoshihara\", compress=TRUE, file=\"sigOvcYoshihara.rda\")\n        },\n        \"spentzos2011_sig\"={\n            ss <- \"affy_hg_u133a\"\n            gid <- as.character(sig[ ,\"probeset\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=sig[ ,\"weight\"])\n            sigOvcSpentzos <- annot\n            save(list=\"sigOvcSpentzos\", compress=TRUE, file=\"sigOvcSpentzos.rda\")\n        },\n        \"tcga2011_sig\"={\n            ss <- \"entrezgene\"\n            gid <- as.character(sig[ ,\"Entrez.Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"hgnc_symbol\", \"ensembl_gene_id\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(paste(\"geneid\", gid, sep=\".\"), colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, sig[ ,c(\"Gene.set\", \"beta\", \"p.value\")])\n            sigOvcTCGA <- annot\n            save(list=\"sigOvcTCGA\", compress=TRUE, file=\"sigOvcTCGA.rda\")\n        })\n    }\n}"
      },
      {
        "partial": "`.ovcSigs` <- function(sigs=c(\"bentink2012_angiogenic\", \"crijns2009_sig\", \"yoshihara2010_sig\", \"spentzos2011_sig\", \"tcga2011_sig\")) {\n    for(i in 1:length(sigs)) {\n        sig <- read.csv(system.file(file.path(\"extdata\", sprintf(\"%s.csv\", sigs[i])), package=\"genefu\"), stringsAsFactors=FALSE)\n        ensembl.db <- biomaRt::useMart(\"ensembl\", dataset=\"hsapiens_gene_ensembl\")\n        switch(sigs[i],\n        \"bentink2012_angiogenic\"={\n            ss <- \"illumina_humanwg_6_v2\"\n            gid <- as.character(sig[ ,\"Probe_Id\"])\n            gene.an <- biomaRt::getBM(attributes=c(ss, \"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"description\", \"chromosome_name\", \"start_position\", \"end_position\", \"strand\", \"band\"), filters=ss, values=sort(unique(gid)), mart=ensembl.db)\n            gene.an[gene.an == \"\" | gene.an == \" \"] <- NA\n            gene.an <- gene.an[!is.na(gene.an[ , ss]) & !duplicated(gene.an[ , ss]) & is.element(gene.an[ , ss], gid), , drop=FALSE]\n            annot <- data.frame(matrix(NA, nrow=nrow(sig), ncol=ncol(gene.an), dimnames=list(gid, colnames(gene.an))))\n            annot[match(gene.an[ , ss], gid), colnames(gene.an)] <- gene.an\n            annot <- data.frame(\"probe\"=gid, annot, \"weight\"=as.numeric(sig[ ,\"weights\"]))\n            sigOvcAngiogenic <- annot\n            save(list=\"sigOvcAngiogenic\", compress=TRUE, file=\"sigO"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/methods-computeLimmaDiffExpr.R",
    "language": "R",
    "content": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Conduct differential expression analysis using the limma R pacakge\n#'\n#' WARNING: This function can take a very long time to compute!\n#'\n#' @examples\n#' if (interactive()) {\n#'   data(TGGATESsmall)\n#'   analysis <- computeLimmaDiffExpr(TGGATESsmall)\n#' }\n#'\n#' @param object A [`ToxicoSet`] object with a molecular profile named 'rna'\n#' @param buildTable [`logical`] Should the result of the eBayes function\n#'  from limma be assembled into a data.table containing the result along\n#'  with the gene, compound and durations names. Default it TRUE, otherwise\n#'  this function with return the object produced by eBayes.\n#'\n#' @return A [`data.table`] containing the results the limma differential\n#'  expression analysis comparing control vs each dose level for each compound\n#'  within each duration.\n#'\n#' @import data.table\n#' @import Biobase\n#' @import limma\n#' @importFrom SummarizedExperiment SummarizedExperiment coerce\n#' @importFrom stats model.matrix model.frame\n#' @importFrom BiocParallel bplapply\n#'\n#' @export\nsetMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n\n    ## TODO:: Add messages to keep track of where the function execution is at\n    ## TODO:: Error if any of the model factors don't have enough levels\n\n    # ---- 1. Get the required input data\n    SE <- molecularProfilesSlot(object)$rna  # Extract the rna expression SummarizedExperiment\n    # work around to fix error when coercing to ESet, since protocolData is not\n    #>subset with the SummarizedExperiment in subsetTo\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')  # Coerce to an ExpressionSet\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    # ---- 2. Extract the metadata needed to build the design matrix\n\n    # Get the sample name, drug, dose and duration and cell type from the\n    #   experiments phenotypic data\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    # to prevent dropping numeric columns when converting to factors\n    targets <- data.frame(lapply(targets, as.character))\n\n    # ---- 3. Create the design matrix\n\n\n    # Construct the design matrix with an intercept at 0\n    # This follows the make the simplest design matrix possible strategy outlined\n    # on page 37 of the limma user guide.\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n\n    # Make the names valid factor names and match with contrasts\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    # ---- 4. Fit a linear model based on design matrix\n\n    # Every possible comparison between compound:dose:duration will\n    # be fit in the linear model - this is less computationally efficient but\n    # allows arbitrary comparisons to be extracted after the model is fit.\n    # We chose this strategy to avoid the need to prespecify an experimental\n    # design (less statistical planning required).\n    fit <- lmFit(eset, design)\n\n    # ---- 5. Setup the contrasts to performs our desired statistical tests\n    setDT(targets)  # Convert to data.table\n\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls)\n        columns <- c('compound', columns)\n    if (hasMultipleCells)\n        columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    # fix levels if there is only one cell type\n    if (!hasMultipleCells) {\n        controls$controlLevels <- gsub('_cell[^_]*$', '',\n            controls$controlLevels)\n        levels$treatmentLevels <- gsub('_cell[^_]*$', '',\n            levels$treatmentLevels)\n    }\n\n    contrastStrings <- unique(controls[levels,\n                              .(contrasts=paste0(treatmentLevels, '-', controlLevels)),\n                              on=columns, all=!hasSharedControls]$contrasts)\n\n\n    # Make contrasts matrix for fitting statistical testing\n    contrasts <- makeContrasts(contrasts=contrastStrings, levels=design)\n\n    # Perfrom statistical tests for the specified contrasts\n    contrFit <- contrasts.fit(fit, contrasts)\n\n    # ---- 6. Predict coefficients using emperical Bayes moderation of SE\n    # Generate a t-stat, moderated F-stat and log-odds of differential expression\n    stats <- eBayes(contrFit)\n\n    if (!buildTable) return(stats)\n\n    # ---- 7. Assemble the results into a data.table object\n    compoundNames <- make.names(treatmentInfo(object)$treatmentid)\n    compounds <- treatmentInfo(object)$treatmentid\n    cellNames <- make.names(sampleInfo(object)$sampleid)\n    cells <- sampleInfo(object)$sampleid\n    ## TODO:: refactor this into muliple lapply statements!\n    resultList <- lapply(contrastStrings, function(comparison) {\n      # Disassmble contrasts into annotations for this statistical test\n      comparisonNoLabels <- gsub('cell|compound|dose|duration',\n        '', comparison)\n      annotations <- unlist(strsplit(comparisonNoLabels, '-'))\n      annotations <- strsplit(annotations, '_')\n\n      if (!(annotations[[1]][3] == annotations[[2]][3]))\n        stop(\"Duration mismatch between treatment and control!\")\n\n      # Get the compound_id based on the contrast name then get the compound name\n      compound_id <- which(compoundNames %in% annotations[[1]][1])\n      compound <- compounds[compound_id]\n\n      # Get the sample_id based on the contrast name then get the sampleid\n      if (hasMultipleCells) {\n          ## TODO:: Should I use == instead?\n          sample_id <- which(cellNames %in% annotations[[1]][4])\n          cell <- cells[sample_id]\n      } else {\n          cell <- unique(cells)\n      }\n\n      # Get the tests per gene for the given comparison\n      statCols <- c(\"logFC\", \"B\", \"P.Value\", \"adj.P.Val\", \"AveExpr\")\n      results <- topTreat(stats, coef=comparison, sort.by=\"none\",\n                          number='all',adjust.method=\"BH\")[, statCols]\n      statCols <- c(\"fold_change\", \"log_odds\", \"p_value\", \"fdr\", \"avg_expr\")\n      colnames(results) <- statCols\n\n      DT <- data.table(\n        \"gene\" = rownames(results),\n        \"compound\" = rep(compound, nrow(results)),\n        \"dose\" = rep(annotations[[1]][2], nrow(results)),\n        \"duration\" = rep(annotations[[1]][3], nrow(results)),\n        \"cell\" = rep(cell, nrow(results)),\n        results\n      )\n      return(DT)\n    })\n    analysis <- rbindlist(resultList)\n\n    ## TODO:: Do we want to only return significant results? Will be a lot less rows\n\n    # --- 8. Annotate and return the results\n    return(analysis)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `computeLimmaDiffExpr` function, and what type of object does it expect as input?",
        "answer": "The main purpose of the `computeLimmaDiffExpr` function is to conduct differential expression analysis using the limma R package. It expects a `ToxicoSet` object with a molecular profile named 'rna' as input."
      },
      {
        "question": "How does the function handle multiple cell types in the dataset, and what changes in the design matrix when there are multiple cell types?",
        "answer": "The function checks for multiple cell types using `hasMultipleCells <- length(unique(targets$cell)) > 1`. If there are multiple cell types, the design matrix includes the cell factor: `model.matrix(~0 + compound:dose:duration:cell, data=model.frame(targets))`. Otherwise, it uses `model.matrix(~0 + compound:dose:duration, data=model.frame(targets))`."
      },
      {
        "question": "What is the purpose of the `buildTable` parameter in the function, and what does the function return when `buildTable` is set to FALSE?",
        "answer": "The `buildTable` parameter determines whether the function should assemble the results into a data.table containing the differential expression analysis results along with gene, compound, and duration names. When `buildTable` is set to FALSE, the function returns the object produced by the `eBayes` function from limma, which contains the raw statistical results of the differential expression analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    # TODO: Complete the rest of the function\n})",
        "complete": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    setDT(targets)\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls) columns <- c('compound', columns)\n    if (hasMultipleCells) columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    if (!hasMultipleCells) {\n        controls$controlLevels <- gsub('_cell[^_]*$', '', controls$controlLevels)\n        levels$treatmentLevels <- gsub('_cell[^_]*$', '', levels$treatmentLevels)\n    }\n\n    contrastStrings <- unique(controls[levels,\n                              .(contrasts=paste0(treatmentLevels, '-', controlLevels)),\n                              on=columns, all=!hasSharedControls]$contrasts)\n\n    contrasts <- makeContrasts(contrasts=contrastStrings, levels=design)\n    contrFit <- contrasts.fit(fit, contrasts)\n    stats <- eBayes(contrFit)\n\n    if (!buildTable) return(stats)\n\n    compoundNames <- make.names(treatmentInfo(object)$treatmentid)\n    compounds <- treatmentInfo(object)$treatmentid\n    cellNames <- make.names(sampleInfo(object)$sampleid)\n    cells <- sampleInfo(object)$sampleid\n\n    resultList <- lapply(contrastStrings, function(comparison) {\n      comparisonNoLabels <- gsub('cell|compound|dose|duration', '', comparison)\n      annotations <- unlist(strsplit(comparisonNoLabels, '-'))\n      annotations <- strsplit(annotations, '_')\n\n      if (!(annotations[[1]][3] == annotations[[2]][3]))\n        stop(\"Duration mismatch between treatment and control!\")\n\n      compound_id <- which(compoundNames %in% annotations[[1]][1])\n      compound <- compounds[compound_id]\n\n      if (hasMultipleCells) {\n          sample_id <- which(cellNames %in% annotations[[1]][4])\n          cell <- cells[sample_id]\n      } else {\n          cell <- unique(cells)\n      }\n\n      statCols <- c(\"logFC\", \"B\", \"P.Value\", \"adj.P.Val\", \"AveExpr\")\n      results <- topTreat(stats, coef=comparison, sort.by=\"none\",\n                          number='all',adjust.method=\"BH\")[, statCols]\n      statCols <- c(\"fold_change\", \"log_odds\", \"p_value\", \"fdr\", \"avg_expr\")\n      colnames(results) <- statCols\n\n      DT <- data.table(\n        \"gene\" = rownames(results),\n        \"compound\" = rep(compound, nrow(results)),\n        \"dose\" = rep(annotations[[1]][2], nrow(results)),\n        \"duration\" = rep(annotations[[1]][3], nrow(results)),\n        \"cell\" = rep(cell, nrow(results)),\n        results\n      )\n      return(DT)\n    })\n    analysis <- rbindlist(resultList)\n\n    return(analysis)\n})"
      },
      {
        "partial": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    setDT(targets)\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls) columns <- c('compound', columns)\n    if (hasMultipleCells) columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    # TODO: Complete the rest of the function\n})",
        "complete": "setMethod('computeLimmaDiffExpr', signature(object='ToxicoSet'),\n    function(object, buildTable=TRUE) {\n    SE <- molecularProfilesSlot(object)$rna\n    metadata(SE)$protocolData <- NULL\n    eset <- as(SE, 'ExpressionSet')\n    eset$treatmentid <- make.names(eset$treatmentid)\n    eset$sampleid <- make.names(eset$sampleid)\n\n    targets <- as.data.frame(pData(eset)[, c(\"samplename\", \"sampleid\", \"treatmentid\",\n                                             \"dose_level\", \"duration\")])\n    colnames(targets) <- c('sample', 'cell', 'compound', 'dose', 'duration')\n    targets <- data.frame(lapply(targets, as.character))\n\n    hasMultipleCells <- length(unique(targets$cell)) > 1\n    if (hasMultipleCells) {\n        design <- model.matrix(~0 + compound:dose:duration:cell,\n            data=model.frame(targets))\n    } else {\n        design <- model.matrix(~0 + compound:dose:duration,\n            data=model.frame(targets))\n    }\n\n    colnames(design) <- gsub(':', '_', colnames(design))\n\n    fit <- lmFit(eset, design)\n\n    setDT(targets)\n    columns <- 'duration'\n    hasSharedControls <- name(object) %in% c('drugMatrix_rat', 'EMEXP2458')\n    if (!hasSharedControls) columns <- c('compound', columns)\n    if (hasMultipleCells) columns <- c('cell', columns)\n\n    controls <- unique(targets[dose == 'Control',\n                        .(controlLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                        by=c('cell', 'compound', 'duration')])\n    levels <- unique(targets[dose != 'Control',\n                      .(treatmentLevels=paste0('compound', compound, '_dose',\n                                               dose, '_duration', duration,\n                                               '_cell', cell)),\n                      by=c('cell', 'compound', 'duration')])\n\n    if (!hasMultipleCells) {\n        controls$controlLevels <- gsub('_cell[^_]*$', '', controls$controlLevels)\n        levels$treatmentLevels <- gsub('_cell[^_]*$', '', levels$treatmentLevels)\n    }\n\n    contrastStrings <- unique(controls[levels,\n                              .(contrasts=paste0(treatmentLevels, '-', controlLevels)),\n                              on=columns, all=!hasSharedControls]$contrasts)\n\n    contrasts <- makeContrasts(contrasts=contrastStrings, levels=design)\n    contrFit <- contrasts.fit(fit, contrasts)\n    stats <- eBayes(contrFit)\n\n    if (!buildTable) return(stats)\n\n    compoundNames <- make.names(treatmentInfo(object)$treatmentid)\n    compounds <- treatmentInfo(object)$treatmentid\n    cellNames <- make.names(sampleInfo(object)$sampleid)\n    cells <- sampleInfo(object)$sampleid\n\n    resultList <- lapply(contrastStrings, function(comparison) {\n      comparisonNoLabels <- gsub('cell|compound|dose|duration', '', comparison)\n      annotations <- unlist(strsplit(comparisonNoLabels, '-'))\n      annotations <- strsplit(annotations, '_')\n\n      if (!(annotations[[1]][3] == annotations[[2]][3]))\n        stop(\"Duration mismatch between treatment and control!\")\n\n      compound_id <- which(compoundNames %in% annotations[[1]][1])\n      compound <- compounds[compound_id]\n\n      if (hasMultipleCells) {\n          sample_id <- which(cellNames %in% annotations[[1]][4])\n          cell <- cells[sample_id]\n      } else {\n          cell <- unique(cells)\n      "
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/hr.comp2.R",
    "language": "R",
    "content": "`hr.comp2` <-\nfunction(x1, beta1, se1, x2, beta2, se2, n) {\n\tr <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n\tif(abs(r) < 1) {\n\t\tt.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hr.comp2` function in R?",
        "answer": "The `hr.comp2` function is designed to compare two hazard ratios in survival analysis. It calculates a p-value for the difference between two log hazard ratios, considering their correlation. The function also returns the exponential of the input betas, which represent the actual hazard ratios."
      },
      {
        "question": "How does the function handle perfect correlation between x1 and x2?",
        "answer": "If the absolute value of the correlation coefficient (r) between x1 and x2 is 1 (perfect correlation), the function sets the p-value (diff.ci.p) to 1 instead of calculating the t-statistic. This is because perfect correlation would lead to division by zero in the t-statistic calculation."
      },
      {
        "question": "What statistical test is being performed in this function, and what assumptions does it make?",
        "answer": "The function performs a t-test to compare two log hazard ratios. It assumes that the difference between the log hazard ratios follows a t-distribution with n-1 degrees of freedom. The test accounts for the correlation between the predictors (x1 and x2) using Spearman's rank correlation. It assumes that the sample size (n) is sufficiently large and that the log hazard ratios are approximately normally distributed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- # Calculate t-statistic\n    diff.ci.p <- # Calculate p-value\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}",
        "complete": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      },
      {
        "partial": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- # Calculate correlation\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(# Return list with p-value and hazard ratios)\n}",
        "complete": "hr.comp2 <- function(x1, beta1, se1, x2, beta2, se2, n) {\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassErrorHandling.R",
    "language": "R",
    "content": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_that()` function in this code snippet?",
        "answer": "The `test_that()` function is used to define a unit test in R. It takes two arguments: a description of the test (as a string) and a code block containing the test expectations. In this case, it's testing that 'multiplication works'."
      },
      {
        "question": "What does the `expect_equal()` function do in this test?",
        "answer": "The `expect_equal()` function is an expectation function from the testthat package in R. It compares two values for equality. In this case, it's checking if the result of `2 * 2` is equal to `4`. If the comparison is true, the test passes; if not, it fails."
      },
      {
        "question": "How would you modify this test to check if division works correctly?",
        "answer": "To check if division works correctly, you could modify the test as follows:\n\ntest_that(\"division works\", {\n  expect_equal(8 / 2, 4)\n})\n\nThis test would verify that 8 divided by 2 equals 4, thus testing the division operation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, ___)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      },
      {
        "partial": "test_that(___, {\n  expect_equal(2 * 2, 4)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassConstructor.R",
    "language": "R",
    "content": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_that()` function in this code snippet?",
        "answer": "The `test_that()` function is used to define a unit test in R. It takes two arguments: a description of the test (as a string) and a code block containing the test expectations. In this case, it's testing that 'multiplication works'."
      },
      {
        "question": "What does the `expect_equal()` function do in this test?",
        "answer": "The `expect_equal()` function is an expectation function from the testthat package in R. It compares two values for equality. In this snippet, it's comparing the result of `2 * 2` with the expected value `4`. If the values are equal, the test passes; if not, it fails."
      },
      {
        "question": "How would you modify this test to check that 3 * 3 equals 9?",
        "answer": "To modify the test to check that 3 * 3 equals 9, you would change the arguments in the `expect_equal()` function. The modified code would look like this:\n\ntest_that(\"multiplication works\", {\n  expect_equal(3 * 3, 9)\n})"
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, ___)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      },
      {
        "partial": "test_that(___, {\n  expect_equal(2 * 2, 4)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/weighted.meanvar.R",
    "language": "R",
    "content": "#' @title Function to compute the weighted mean and weighted variance of 'x'\n#'\n#' @description\n#' This function allows for computing the weighted mean and weighted variance \n#'   of a vector of continuous values.\n#'\n#' @usage\n#' weighted.meanvar(x, w, na.rm = FALSE)\n#'\n#' @param x\tan object containing the values whose weighted mean is to be computed.\n#' @param w\ta numerical vector of weights of the same length as x giving the \n#'   weights to use for elements of x.\n#' @param na.rm\tTRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @details\n#' If w is missing then all elements of x are given the same weight, otherwise \n#'   the weights coerced to numeric by as.numeric. On the contrary of \n#'   weighted.mean the weights are NOT normalized to sum to one. If the sum \n#'   of the weights is zero or infinite, NAs will be returned.\n#'\n#' @return\n#' A numeric vector of two values that are the weighted mean and weighted \n#'   variance respectively.\n#'\n#' @references\n#' http://en.wikipedia.org/wiki/Weighted_variance#Weighted_sample_variance\n#'\n#' @seealso\n#' [stats::weighted.mean]\n#' \n#' @examples\n#' set.seed(54321)\n#' weighted.meanvar(x=rnorm(100) + 10, w=runif(100))\n#'\n#' @md\n#' @export\n## weighted mean and weighted variance\n## sources:\n## http://en.wikipedia.org/wiki/T_test\n## http://www.nicebread.de/blog/files/fc02e1635792cb0f2b3cbd1f7e6c580b-10.php\nweighted.meanvar <- \nfunction(x, w, na.rm=FALSE) {\n\tif(missing(w)) { w <- rep(1, length(x))}\n\tii <- complete.cases(x, w)\n\tif(!na.rm && sum(!ii) > 0) { stop(\"missing values are present!\") } else { \n\t\tw <- as.numeric(w[ii])\n\t\tx <- as.numeric(x[ii])\n\t} \n\tsum.w <- sum(w) \n\tsum.w2 <- sum(w^2) \n\tmean.w <- sum(x * w) / sum(w) \n\tvar.w <- (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2, na.rm=na.rm)\n\tres <- c(mean.w, var.w)\n\tnames(res) <- c(\"weighted.mean\", \"weighted.var\")\n\treturn(res)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'weighted.meanvar' function and what are its main parameters?",
        "answer": "The 'weighted.meanvar' function computes the weighted mean and weighted variance of a vector of continuous values. Its main parameters are 'x' (the vector of values), 'w' (the vector of weights), and 'na.rm' (a boolean indicating whether to remove missing values)."
      },
      {
        "question": "How does the function handle missing weights (w) and what is different about its weight normalization compared to the standard weighted.mean function?",
        "answer": "If weights (w) are missing, the function assigns equal weights to all elements of x by using 'rep(1, length(x))'. Unlike the standard weighted.mean function, this function does NOT normalize the weights to sum to one. If the sum of weights is zero or infinite, the function will return NAs."
      },
      {
        "question": "What is the mathematical formula used to calculate the weighted variance in this function?",
        "answer": "The weighted variance is calculated using the formula: var.w = (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2), where sum.w is the sum of weights, sum.w2 is the sum of squared weights, and mean.w is the weighted mean. This formula is based on the weighted sample variance as described in the Wikipedia reference provided in the function documentation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  if(missing(w)) { w <- rep(1, length(x))}\n  ii <- complete.cases(x, w)\n  if(!na.rm && sum(!ii) > 0) { stop(\"missing values are present!\") } else { \n    w <- as.numeric(w[ii])\n    x <- as.numeric(x[ii])\n  } \n  sum.w <- sum(w) \n  sum.w2 <- sum(w^2) \n  mean.w <- sum(x * w) / sum.w \n  # Complete the calculation for var.w\n  # Return the result as a named vector\n}",
        "complete": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  if(missing(w)) { w <- rep(1, length(x))}\n  ii <- complete.cases(x, w)\n  if(!na.rm && sum(!ii) > 0) { stop(\"missing values are present!\") } else { \n    w <- as.numeric(w[ii])\n    x <- as.numeric(x[ii])\n  } \n  sum.w <- sum(w) \n  sum.w2 <- sum(w^2) \n  mean.w <- sum(x * w) / sum.w \n  var.w <- (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2, na.rm=na.rm)\n  res <- c(mean.w, var.w)\n  names(res) <- c(\"weighted.mean\", \"weighted.var\")\n  return(res)\n}"
      },
      {
        "partial": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  # Handle missing weights\n  # Check for complete cases\n  # Handle missing values based on na.rm\n  # Calculate sum of weights and sum of squared weights\n  # Calculate weighted mean\n  # Calculate weighted variance\n  # Return result as named vector\n}",
        "complete": "weighted.meanvar <- function(x, w, na.rm=FALSE) {\n  if(missing(w)) w <- rep(1, length(x))\n  ii <- complete.cases(x, w)\n  if(!na.rm && sum(!ii) > 0) stop(\"missing values are present!\")\n  w <- as.numeric(w[ii])\n  x <- as.numeric(x[ii])\n  sum.w <- sum(w)\n  sum.w2 <- sum(w^2)\n  mean.w <- sum(x * w) / sum.w\n  var.w <- (sum.w / (sum.w^2 - sum.w2)) * sum(w * (x - mean.w)^2, na.rm=na.rm)\n  setNames(c(mean.w, var.w), c(\"weighted.mean\", \"weighted.var\"))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/genius.R",
    "language": "R",
    "content": "#' @title Function to compute the Gene Expression progNostic Index Using Subtypes (GENIUS)\n#'   as published by Haibe-Kains et al. 2010\n#'\n#' @description\n#' This function computes the Gene Expression progNostic Index Using Subtypes (GENIUS)\n#'   as published by Haibe-Kains et al. 2010. Subtype-specific risk scores are computed for\n#'   each subtype signature separately and an overall risk score is computed by combining\n#'   these scores with the posterior probability to belong to each of the breast cancer\n#'   molecular subtypes.\n#'\n#' @usage\n#' genius(data, annot, do.mapping = FALSE, mapping, do.scale = TRUE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in case of ambiguities,\n#'   the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#'   mapping such that the probes are not selected based on their variance.\n#' @param do.scale TRUE if the ESR1, ERBB2 and AURKA (module) scores must be rescaled\n#'   (see rescale), FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - GENIUSM1: Risk score from the ER-/HER2- subtype signature in GENIUS model.\n#' - GENIUSM2: Risk score from the HER2+ subtype signature in GENIUS model.\n#' - GENIUSM3: Risk score from the ER+/HER2- subtype signature in GENIUS model.\n#' - score: Overall risk prediction as computed by the GENIUS model.a.\n#'\n#' @references\n#' Haibe-Kains B, Desmedt C, Rothe F, Sotiriou C and Bontempi G (2010) \"A fuzzy gene\n#' expression-based computational approach improves breast cancer prognostication\", Genome Biology, 11(2):R18\n#'\n#' @seealso\n#' [genefu::subtype.cluster.predict],[genefu::sig.score]\n#'\n#' @examples\n#' # load NKI dataset\n#' data(nkis)\n#' data(scmod1.robust)\n#' data(sig.genius)\n#'\n#' # compute GENIUS risk scores based on GENIUS model fitted on VDX dataset\n#' genius.nkis <- genius(data=data.nkis, annot=annot.nkis, do.mapping=TRUE)\n#' str(genius.nkis)\n#' # the performance of GENIUS overall risk score predictions are not optimal\n#' # since only part of the NKI dataset was used\n#'\n#' @md\n#' @export\ngenius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n\n\t## predict breast cancer molecular subtypes\n\tsbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n\tusbt <- unique(sbt.id$subtype)\n\tusbt <- sort(usbt[!is.na(usbt)])\n\tpred.sbtclassif <- NULL\n\tfor(ii in 1:length(usbt)) {\n\t\tmyx <- sbt.id$subtype == usbt[ii] & !is.na(sbt.id$subtype)\n\n\t\t#compute the score from model\n\t\tscore <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\",  \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping,  mapping=mapping, verbose=FALSE)$score\n\t\tif(do.scale) {\n\t\t\t#the rescaling needs a large sample size!\n\t\t\t#necessary if we want to validate the classifier using a different dataset\n\t\t\t#the estimation of survival probabilities depends on the scale of the score\n\t\t\tscore <-  (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n\t\t}\n\t\tnames(score) <- dimnames(data)[[1]]\n\t\tpred.sbtclassif <- c(pred.sbtclassif, list(\"score\"=score))\n\t}\n\tnames(pred.sbtclassif) <- names(sig.genius)\n\t#combine classifications\n\tcc <- NULL\n\tfor(j in 1:length(pred.sbtclassif)) {\n\t\tcc <- cbind(cc, pred.sbtclassif[[j]])\n\t}\n\tww <- sbt.id$subtype.proba\n\tcombine.pred <- apply(ww * cc, 1, sum)\n\n\tpred.sbtclassif <- c(pred.sbtclassif, list(combine.pred))\n\tnames(pred.sbtclassif)[length(pred.sbtclassif)] <- \"score\"\n\treturn(pred.sbtclassif)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'genius' function and what are its main inputs?",
        "answer": "The 'genius' function computes the Gene Expression progNostic Index Using Subtypes (GENIUS) for breast cancer prognosis. Its main inputs are 'data' (a matrix of gene expressions), 'annot' (a matrix of annotations), 'do.mapping' (a boolean for gene mapping), 'mapping' (an optional matrix for forced mapping), and 'do.scale' (a boolean for score rescaling)."
      },
      {
        "question": "How does the function handle different breast cancer subtypes in the GENIUS model?",
        "answer": "The function predicts breast cancer molecular subtypes using the 'subtype.cluster.predict' function. It then computes subtype-specific risk scores for each unique subtype using the 'sig.score' function with the corresponding signature from 'sig.genius'. These scores are optionally rescaled, and finally combined using subtype probabilities to produce an overall risk score."
      },
      {
        "question": "What is the structure of the output returned by the 'genius' function?",
        "answer": "The function returns a list containing risk scores for each subtype (GENIUSM1, GENIUSM2, GENIUSM3) and an overall risk prediction score. The subtype-specific scores are named according to the 'sig.genius' list, and the overall score is named 'score'. Each score is a vector with values for each sample in the input data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- NULL\n  for(ii in 1:length(usbt)) {\n    myx <- sbt.id$subtype == usbt[ii] & !is.na(sbt.id$subtype)\n\n    score <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\",  \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping,  mapping=mapping, verbose=FALSE)$score\n    if(do.scale) {\n      score <-  (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n    }\n    names(score) <- dimnames(data)[[1]]\n    pred.sbtclassif <- c(pred.sbtclassif, list(\"score\"=score))\n  }\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  # Complete the function to combine classifications and return the result\n}",
        "complete": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- NULL\n  for(ii in 1:length(usbt)) {\n    myx <- sbt.id$subtype == usbt[ii] & !is.na(sbt.id$subtype)\n\n    score <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\",  \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping,  mapping=mapping, verbose=FALSE)$score\n    if(do.scale) {\n      score <-  (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n    }\n    names(score) <- dimnames(data)[[1]]\n    pred.sbtclassif <- c(pred.sbtclassif, list(\"score\"=score))\n  }\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  cc <- do.call(cbind, pred.sbtclassif)\n  ww <- sbt.id$subtype.proba\n  combine.pred <- rowSums(ww * cc)\n\n  pred.sbtclassif <- c(pred.sbtclassif, list(score = combine.pred))\n  return(pred.sbtclassif)\n}"
      },
      {
        "partial": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  # Predict breast cancer molecular subtypes\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- vector(\"list\", length(usbt))\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  # Complete the function to compute scores for each subtype and combine them\n}",
        "complete": "genius <- function(data, annot, do.mapping=FALSE, mapping, do.scale=TRUE) {\n  # Predict breast cancer molecular subtypes\n  sbt.id <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, do.prediction.strength=FALSE, do.BIC=FALSE, plot=FALSE, verbose=FALSE)\n\n  usbt <- unique(sbt.id$subtype)\n  usbt <- sort(usbt[!is.na(usbt)])\n  pred.sbtclassif <- vector(\"list\", length(usbt))\n  names(pred.sbtclassif) <- names(sig.genius)\n\n  for(ii in seq_along(usbt)) {\n    score <- sig.score(x=sig.genius[[ii]][ , c(\"probe\", \"EntrezGene.ID\", \"coefficient\")], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)$score\n    if(do.scale) {\n      score <- (rescale(score, q=0.05, na.rm=TRUE) - 0.5) * 2\n    }\n    names(score) <- rownames(data)\n    pred.sbtclassif[[ii]] <- score\n  }\n\n  cc <- do.call(cbind, pred.sbtclassif)\n  ww <- sbt.id$subtype.proba\n  combine.pred <- rowSums(ww * cc)\n\n  pred.sbtclassif$score <- combine.pred\n  return(pred.sbtclassif)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/summarizeSensitivityProfiles.R",
    "language": "R",
    "content": "#' Takes the sensitivity data from a ToxicoSet, and summarises them into a\n#' drug vs cell line table\n#'\n#' This function creates a table with drug as rows and cell lines as columns,\n#' summarising the drug senstitivity data of a ToxicoSet into drug-cell line\n#' pairs for a specified experiment duration.\n#'\n#' @examples\n#' data(TGGATESsmall)\n#' TGGATESauc <- summarizeSensitivityProfiles(TGGATESsmall, sensitivity.measure='auc_recomputed')\n#'\n#' @param tSet \\code{ToxicoSet} The ToxicoSet from which to extract the data\n#' @param sensitivity.measure \\code{character} which sensitivity sensitivity.measure to use? Use the\n#'   sensitivityMeasures function to find out what measures are available for each TSet.\n#' @param cell_lines \\code{character} The cell lines to be summarized.\n#'    If any cell lines has no data, it will be filled with missing values\n#' @param drugs \\code{character} The drugs to be summarized.\n#'   If any drugs has no data, it will be filled with\n#'   missing values. Defaults to include all drugs in the given tSet.\n#' @param duration \\code{numeric} The duration at which to summarize\n#'   the drug-cell combo. This is a required parameter.\n#' @param summary.stat \\code{character} which summary method to use if there are repeated\n#'   cell line-drug experiments? Choices are \"mean\", \"median\", \"first\", \"last\", \"max\", or \"min\"\n#' @param fill.missing \\code{boolean} should the missing cell lines not in the\n#'   molecular data object be filled in with missing values?\n#' @param verbose Should the function print progress messages?\n#'\n#' @return \\code{matrix} A matrix with drugs going down the rows, cell lines across\n#'   the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom stats median\n#' @importFrom reshape2 acast\n#'\n#' @export\nsummarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n\n  ## MISSING VALUE HANDLING FOR PARAMETERS\n  # Get named list of defualt values for missing parameters\n  argDefaultList <-\n    paramMissingHandler(\n      funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n      cell_lines = cell_lines, drugs = drugs, duration = duration\n    )\n  # Assign any missing parameter default values to function environment\n  ## TODO:: I think we can do a for loop over index names?\n  ## TODO:: Refactor to lapply\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n\n  ## ERROR HANDLING FOR FUNCTION PARAMETERS\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n    )\n\n  summary.stat <- match.arg(summary.stat)\n\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n\n  if (sensitivity.measure != \"max.conc\") {\n    #if the sensitivity.measure specified is not \"max.conc\"\n    dd <- sensitivityProfiles(tSet)\n  } else {\n    #if the sensitivity.measure specified is \"max.conc\"\n    if (!\"max.conc\" %in% colnames(ToxicoGx::sensitivityInfo(tSet))) {\n      # if max.conc is not a column in sensitivityInfo:\n      # call updateMaxConc, which finds the maximum dosage within sensitivity raw, puts\n      # the value in a new column of sensitivity info called max.conc, and returns the tSet\n      tSet <- updateMaxConc(tSet)\n\n    }\n    ##dd contains the sensitivity Info of the tSet\n    dd <- ToxicoGx::sensitivityInfo(tSet)\n\n  }\n\n  #result is a matrix of NA's where # of rows, # columns is as specified:\n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines))\n  #specify the row, column names of the result matrix\n  rownames(result) <- drugs\n  colnames(result) <- cell_lines\n\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\",\"duration_h\")], \"sensitivity.measure\" = dd[, sensitivity.measure])\n\n  summary.function <- function(x) {\n    if (all(is.na(x))) {\n      return(NA_real_)\n    }\n    switch(summary.stat,\n           \"mean\" = {\n             return(mean(as.numeric(x), na.rm = TRUE))\n           },\n           \"median\" = {\n             return(median(as.numeric(x), na.rm = TRUE))\n           },\n           \"first\" = {\n             return(as.numeric(x)[[1]])\n           },\n           \"last\" = {\n             return(as.numeric(x)[[length(x)]])\n           },\n           \"max\" = {\n             return(max(as.numeric(x), na.rm = TRUE))\n           },\n           \"min\" = {\n             return(min(as.numeric(x), na.rm = TRUE))\n           })\n\n  }\n\n  pp_dd <- pp_dd[\n    pp_dd[,\"sampleid\"] %in% cell_lines &\n    pp_dd[,\"treatmentid\"] %in% drugs &\n    pp_dd[,\"duration_h\"] %in% duration,\n\n    ]\n\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, fun.aggregate = summary.function,\n                        value.var = \"sensitivity.measure\")\n\n  result[rownames(tt), colnames(tt)] <- tt\n\n  if (!fill.missing) {\n\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n  }\n  return(result)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `summarizeSensitivityProfiles` function in this code snippet?",
        "answer": "The main purpose of the `summarizeSensitivityProfiles` function is to create a summary table of drug sensitivity data from a ToxicoSet object. It generates a matrix with drugs as rows and cell lines as columns, containing the specified sensitivity measure for each drug-cell line pair at a given experiment duration."
      },
      {
        "question": "How does the function handle missing data or incomplete drug-cell line combinations?",
        "answer": "The function handles missing data in several ways: 1) It uses a `fill.missing` parameter to optionally fill in missing cell lines with NA values. 2) It creates a result matrix initially filled with NA values. 3) It uses a custom summary function that can handle NA values when aggregating data. 4) If `fill.missing` is set to FALSE, it removes rows and columns that contain only NA values from the final result."
      },
      {
        "question": "What is the purpose of the `summary.stat` parameter in this function, and what options does it provide?",
        "answer": "The `summary.stat` parameter determines how to aggregate data when there are repeated experiments for the same drug-cell line combination. It provides six options: 'mean', 'median', 'first', 'last', 'max', and 'min'. These options allow the user to choose how to summarize multiple data points for the same drug-cell line pair, such as taking the average, the median, or selecting specific values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  argDefaultList <- paramMissingHandler(\n    funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n    cell_lines = cell_lines, drugs = drugs, duration = duration\n  )\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n  )\n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  # Complete the function to process sensitivity data and return the result matrix\n}",
        "complete": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  argDefaultList <- paramMissingHandler(\n    funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n    cell_lines = cell_lines, drugs = drugs, duration = duration\n  )\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n  )\n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  dd <- if (sensitivity.measure != \"max.conc\") {\n    sensitivityProfiles(tSet)\n  } else {\n    if (!(\"max.conc\" %in% colnames(ToxicoGx::sensitivityInfo(tSet)))) {\n      tSet <- updateMaxConc(tSet)\n    }\n    ToxicoGx::sensitivityInfo(tSet)\n  }\n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines),\n                   dimnames = list(drugs, cell_lines))\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\",\"duration_h\")], \n                 \"sensitivity.measure\" = dd[, sensitivity.measure])\n  summary.function <- function(x) {\n    if (all(is.na(x))) return(NA_real_)\n    switch(summary.stat,\n           mean = mean(as.numeric(x), na.rm = TRUE),\n           median = median(as.numeric(x), na.rm = TRUE),\n           first = as.numeric(x)[[1]],\n           last = as.numeric(x)[[length(x)]],\n           max = max(as.numeric(x), na.rm = TRUE),\n           min = min(as.numeric(x), na.rm = TRUE))\n  }\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell_lines &\n                 pp_dd[,\"treatmentid\"] %in% drugs &\n                 pp_dd[,\"duration_h\"] %in% duration, ]\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, \n                        fun.aggregate = summary.function,\n                        value.var = \"sensitivity.measure\")\n  result[rownames(tt), colnames(tt)] <- tt\n  if (!fill.missing) {\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n  }\n  return(result)\n}"
      },
      {
        "partial": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  # Add code to handle missing parameters and perform error checking\n  \n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  \n  # Add code to process sensitivity data based on the sensitivity.measure\n  \n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines))\n  rownames(result) <- drugs\n  colnames(result) <- cell_lines\n  \n  # Add code to calculate and populate the result matrix\n  \n  # Add code to handle fill.missing option\n  \n  return(result)\n}",
        "complete": "summarizeSensitivityProfiles <- function(tSet,\n                                         duration = NULL,\n                                         cell_lines = NULL,\n                                         drugs = NULL,\n                                         sensitivity.measure=\"auc_recomputed\",\n                                         summary.stat = c(\"mean\",\n                                                          \"median\", \"first\",\n                                                          \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE)\n  {\n  argDefaultList <- paramMissingHandler(\n    funName = \"summarizeSensitivityProfiles\", tSet = tSet,\n    cell_lines = cell_lines, drugs = drugs, duration = duration\n  )\n  if (length(argDefaultList) > 0) {\n    for (idx in seq_along(argDefaultList)) {\n      assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n    }\n  }\n  paramErrorChecker(\n    \"summarizeSensitivtyProfiles\", tSet = tSet, drugs = drugs,\n    sensivity.measure = sensitivity.measure, duration = duration,\n    summary.stat = summary.stat\n  )\n  summary.stat <- match.arg(summary.stat)\n  pp <- ToxicoGx::sensitivityInfo(tSet)\n  dd <- if (sensitivity.measure != \"max.conc\") {\n    sensitivityProfiles(tSet)\n  } else {\n    if (!(\"max.conc\" %in% colnames(ToxicoGx::sensitivityInfo(tSet)))) {\n      tSet <- updateMaxConc(tSet)\n    }\n    ToxicoGx::sensitivityInfo(tSet)\n  }\n  result <- matrix(NA_real_, nrow = length(drugs), ncol = length(cell_lines),\n                   dimnames = list(drugs, cell_lines))\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\",\"duration_h\")], \n                 \"sensitivity.measure\" = dd[, sensitivity.measure])\n  summary.function <- function(x) {\n    if (all(is.na(x))) return(NA_real_)\n    switch(summary.stat,\n           mean = mean(as.numeric(x), na.rm = TRUE),\n           median = median(as.numeric(x), na.rm = TRUE),\n           first = as.numeric(x)[[1]],\n           last = as.numeric(x)[[length(x)]],\n           max = max(as.numeric(x), na.rm = TRUE),\n           min = min(as.numeric(x), na.rm = TRUE))\n  }\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell_lines &\n                 pp_dd[,\"treatmentid\"] %in% drugs &\n                 pp_dd[,\"duration_h\"] %in% duration, ]\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, \n                        fun.aggregate = summary.function,\n                        value.var = \"sensitivity.measure\")\n  result[rownames(tt), colnames(tt)] <- tt\n  if (!fill.missing) {\n    result <- result[apply(result, 1, function(x) !all(is.na(x))),\n                     apply(result, 2, function(x) !all(is.na(x)))]\n  }\n  return(result)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/vignettes/genefu.R",
    "language": "R",
    "content": "## ----install, eval=FALSE, results='hide', message=FALSE-----------------------\n#  BiocManager::install(\"genefu\")\n\n## ----load, eval=TRUE, results='hide', message=FALSE---------------------------\nlibrary(genefu)\nlibrary(xtable)\nlibrary(rmeta)\nlibrary(Biobase)\nlibrary(caret)\n\n\n## ----install_data, eval=FALSE, results='hide', message=FALSE------------------\n#  BiocManager::install(\"breastCancerMAINZ\")\n#  BiocManager::install(\"breastCancerTRANSBIG\")\n#  BiocManager::install(\"breastCancerUPP\")\n#  BiocManager::install(\"breastCancerUNT\")\n#  BiocManager::install(\"breastCancerNKI\")\n\n## ----load_data, eval=TRUE, results='hide', message=FALSE----------------------\nlibrary(breastCancerMAINZ)\nlibrary(breastCancerTRANSBIG)\nlibrary(breastCancerUPP)\nlibrary(breastCancerUNT)\nlibrary(breastCancerNKI)\n\n## ----findDuplicatedPatients,eval=TRUE,results='hide',message=FALSE------------\ndata(breastCancerData)\ncinfo <- colnames(pData(mainz7g))\ndata.all <- c(\"transbig7g\"=transbig7g, \"unt7g\"=unt7g, \"upp7g\"=upp7g,\n              \"mainz7g\"=mainz7g, \"nki7g\"=nki7g)\n\nidtoremove.all <- NULL\nduplres <- NULL\n\n## No overlaps in the MainZ and NKI datasets.\n\n## Focus on UNT vs UPP vs TRANSBIG\ndemo.all <- rbind(pData(transbig7g), pData(unt7g), pData(upp7g))\ndn2 <- c(\"TRANSBIG\", \"UNT\", \"UPP\")\n\n## Karolinska\n## Search for the VDXKIU, KIU, UPPU series\nds2 <- c(\"VDXKIU\", \"KIU\", \"UPPU\")\ndemot <- demo.all[complete.cases(demo.all[ , c(\"series\")]) &\n                    is.element(demo.all[ , \"series\"], ds2), ]\n\n# Find the duplicated patients in that series\nduplid <- sort(unique(demot[duplicated(demot[ , \"id\"]), \"id\"]))\nduplrest <- NULL\nfor(i in 1:length(duplid)) {\n  tt <- NULL\n  for(k in 1:length(dn2)) {\n    myx <- sort(row.names(demot)[complete.cases(demot[ , c(\"id\", \"dataset\")]) &\n                                   demot[ , \"id\"] == duplid[i] & \n                                   demot[ , \"dataset\"] == dn2[k]])\n    if(length(myx) > 0) { tt <- c(tt, myx) }\n  }\n  duplrest <- c(duplrest, list(tt))\n}\nnames(duplrest) <- duplid\nduplres <- c(duplres, duplrest)\n\n## Oxford\n## Search for the VVDXOXFU, OXFU series\nds2 <- c(\"VDXOXFU\", \"OXFU\")\ndemot <- demo.all[complete.cases(demo.all[ , c(\"series\")]) & \n                    is.element(demo.all[ , \"series\"], ds2), ]\n\n# Find the duplicated patients in that series\nduplid <- sort(unique(demot[duplicated(demot[ , \"id\"]), \"id\"]))\nduplrest <- NULL\nfor(i in 1:length(duplid)) {\n  tt <- NULL\n  for(k in 1:length(dn2)) {\n    myx <- sort(row.names(demot)[complete.cases(demot[ , c(\"id\", \"dataset\")]) &\n                                   demot[ , \"id\"] == duplid[i] & \n                                   demot[ , \"dataset\"] == dn2[k]])\n    if(length(myx) > 0) { tt <- c(tt, myx) }\n  }\n  duplrest <- c(duplrest, list(tt))\n}\nnames(duplrest) <- duplid\nduplres <- c(duplres, duplrest)\n\n## Full set duplicated patients\nduPL <- sort(unlist(lapply(duplres, function(x) { return(x[-1]) } )))\n\n## ----CalculateMolecularSubtypes-----------------------------------------------\ndn <- c(\"transbig\", \"unt\", \"upp\", \"mainz\", \"nki\")\ndn.platform <- c(\"affy\", \"affy\", \"affy\", \"affy\", \"agilent\")\nres <- ddemo.all <- ddemo.coln <- NULL\n\nfor(i in 1:length(dn)) {\n\n  ## load dataset\n  dd <- get(data(list=dn[i]))\n  #Remove duplicates identified first\n  message(\"obtained dataset!\")\n\n  #Extract expression set, pData, fData for each dataset\n  ddata <- t(exprs(dd))\n\n  ddemo <- phenoData(dd)@data\n\n  if(length(intersect(rownames(ddata),duPL))>0)\n  {\n  ddata<-ddata[-which(rownames(ddata) %in% duPL),]\n  ddemo<-ddemo[-which(rownames(ddemo) %in% duPL),]\n  }\n\n  dannot <- featureData(dd)@data\n\n  # MOLECULAR SUBTYPING\n  # Perform subtyping using scmod2.robust\n  # scmod2.robust: List of parameters defining the subtype clustering model\n  # (as defined by Wirapati et al)\n\n  # OBSOLETE FUNCTION CALL - OLDER VERSIONS OF GENEFU\n  # SubtypePredictions<-subtype.cluster.predict(sbt.model=scmod2.robust,data=ddata,\n  #                                               annot=dannot,do.mapping=TRUE,\n  #                                               verbose=TRUE)\n\n  # CURRENT FUNCTION CALL - NEWEST VERSION OF GENEFU\n  SubtypePredictions <- molecular.subtyping(sbt.model = \"scmod2\",data = ddata,\n                                          annot = dannot,do.mapping = TRUE)\n\n  #Get sample counts pertaining to each subtype\n  table(SubtypePredictions$subtype)\n  #Select samples pertaining to Basal Subtype\n  Basals<-names(which(SubtypePredictions$subtype == \"ER-/HER2-\"))\n  #Select samples pertaining to HER2 Subtype\n  HER2s<-names(which(SubtypePredictions$subtype == \"HER2+\"))\n  #Select samples pertaining to Luminal Subtypes\n  LuminalB<-names(which(SubtypePredictions$subtype == \"ER+/HER2- High Prolif\"))\n  LuminalA<-names(which(SubtypePredictions$subtype == \"ER+/HER2- Low Prolif\"))\n\n  #ASSIGN SUBTYPES TO EVERY SAMPLE, ADD TO THE EXISTING PHENODATA\n  ddemo$SCMOD2<-SubtypePredictions$subtype\n  ddemo[LuminalB,]$SCMOD2<-\"LumB\"\n  ddemo[LuminalA,]$SCMOD2<-\"LumA\"\n  ddemo[Basals,]$SCMOD2<-\"Basal\"\n  ddemo[HER2s,]$SCMOD2<-\"Her2\"\n\n  # Perform subtyping using PAM50\n  # Matrix should have samples as ROWS, genes as COLUMNS\n  # rownames(dannot)<-dannot$probe<-dannot$EntrezGene.ID\n\n  # OLDER FUNCTION CALL\n  # PAM50Preds<-intrinsic.cluster.predict(sbt.model=pam50,data=ddata,\n  #                                         annot=dannot,do.mapping=TRUE,\n  #                                         verbose=TRUE)\n\n  # NEWER FUNCTION CALL BASED ON MOST RECENT VERSION\n  PAM50Preds<-molecular.subtyping(sbt.model = \"pam50\",data=ddata,\n                                        annot=dannot,do.mapping=TRUE)\n\n\n  table(PAM50Preds$subtype)\n  ddemo$PAM50<-PAM50Preds$subtype\n  LumA<-names(PAM50Preds$subtype)[which(PAM50Preds$subtype == \"LumA\")]\n  LumB<-names(PAM50Preds$subtype)[which(PAM50Preds$subtype == \"LumB\")]\n  ddemo[LumA,]$PAM50<-\"LumA\"\n  ddemo[LumB,]$PAM50<-\"LumB\"\n\n  ddemo.all <- rbind(ddemo, ddemo.all)\n}\n\n## ----CompareMolecularSubtypesByConfusionMatrix--------------------------------\n# Obtain the subtype prediction counts for PAM50\ntable(ddemo.all$PAM50)\nNormals<-rownames(ddemo.all[which(ddemo.all$PAM50 == \"Normal\"),])\n\n# Obtain the subtype prediction counts for SCMOD2\ntable(ddemo.all$SCMOD2)\n\nddemo.all$PAM50<-as.character(ddemo.all$PAM50)\n# We compare the samples that are predicted as pertaining to a molecular subtyp\n# We ignore for now the samples that predict as 'Normal' by PAM50\nconfusionMatrix(\n  factor(ddemo.all[-which(rownames(ddemo.all) %in% Normals),]$SCMOD2),\n  factor(ddemo.all[-which(rownames(ddemo.all) %in% Normals),]$PAM50)\n  )\n\n## ----CompareSurvivalBySubtypes------------------------------------------------\n# http://www.inside-r.org/r-doc/survival/survfit.coxph\nlibrary(survival)\nddemo<-ddemo.all\ndata.for.survival.SCMOD2 <- ddemo[,c(\"e.os\", \"t.os\", \"SCMOD2\",\"age\")]\ndata.for.survival.PAM50 <- ddemo[,c(\"e.os\", \"t.os\", \"PAM50\",\"age\")]\n# Remove patients with missing survival information\ndata.for.survival.SCMOD2 <- \n  data.for.survival.SCMOD2[complete.cases(data.for.survival.SCMOD2),]\ndata.for.survival.PAM50 <- \n  data.for.survival.PAM50[complete.cases(data.for.survival.PAM50),]\n\ndays.per.month <- 30.4368\ndays.per.year <- 365.242\n\ndata.for.survival.PAM50$months_to_death <- \n  data.for.survival.PAM50$t.os / days.per.month\ndata.for.survival.PAM50$vital_status <- data.for.survival.PAM50$e.os == \"1\"\nsurv.obj.PAM50 <- survfit(Surv(data.for.survival.PAM50$months_to_death,\n                               data.for.survival.PAM50$vital_status) ~ \n                            data.for.survival.PAM50$PAM50)\n\ndata.for.survival.SCMOD2$months_to_death <- \n  data.for.survival.SCMOD2$t.os / days.per.month\ndata.for.survival.SCMOD2$vital_status <- data.for.survival.SCMOD2$e.os == \"1\"\nsurv.obj.SCMOD2 <- survfit(Surv(\n  data.for.survival.SCMOD2$months_to_death,\n  data.for.survival.SCMOD2$vital_status) ~ data.for.survival.SCMOD2$SCMOD2)\n\nmessage(\"KAPLAN-MEIR CURVE - USING PAM50\")\n\nplot(main = \"Surival Curves PAM50\", surv.obj.PAM50,\n     col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),lty = 1,lwd = 3,\n     xlab = \"Time (months)\",ylab = \"Probability of Survival\")\nlegend(\"topright\",\n       fill = c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),\n       legend = c(\"Basal\",\"Her2\",\"LumA\",\"LumB\",\"Normal\"),bty = \"n\")\n\nmessage(\"KAPLAN-MEIR CURVE - USING SCMOD2\")\n\nplot(main = \"Surival Curves SCMOD2\", surv.obj.SCMOD2,\n     col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\"),lty = 1,lwd = 3,\n     xlab = \"Time (months)\",ylab = \"Probability of Survival\")\nlegend(\"topright\",\n       fill = c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\"),\n       legend = c(\"Basal\",\"Her2\",\"LumA\",\"LumB\"),bty = \"n\")\n\n## GENERATE A OVERLAYED PLOT OF SURVIVAL CURVES\nmessage(\"Overlayed Surival Plots based on PAM50 and SCMOD2\")\n                          ## Basal    Her2        LuminalA  LuminalB   Normal\nplot(surv.obj.PAM50,col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),lty = 1,lwd = 3,\n     xlab = \"Time (months)\",ylab = \"Probability of Survival\",ymin = 0.2)\nlegend(\"topright\",\n       fill = c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\", \"#000000\"),\n       legend = c(\"Basal\",\"Her2\",\"LumA\",\"LumB\",\"Normal\"),bty = \"n\")\n\npar(new=TRUE)\n                            ## Basal    Her2        LuminalA  LuminalB\nlines(surv.obj.SCMOD2,col =c(\"#006d2c\", \"#8856a7\",\"#a50f15\", \"#08519c\"),lwd=2,lty=5)\nlegend(\"bottomright\",c(\"PAM50\",\"SCMOD2\"),lty=c(\"solid\", \"dashed\"))\n\n## ----CalculatedCVPL-----------------------------------------------------------\nset.seed(12345)\n\nPAM5_CVPL<-cvpl(x=data.for.survival.PAM50$age,\n                surv.time=data.for.survival.PAM50$months_to_death,\n                surv.event=data.for.survival.PAM50$vital_status,\n                strata=as.integer(factor(data.for.survival.PAM50$PAM50)),\n                nfold=10, setseed=54321)$cvpl\n\nSCMOD2_CVPL<-cvpl(x=data.for.survival.SCMOD2$age,\n                    surv.time=data.for.survival.SCMOD2$months_to_death,\n                    surv.event=data.for.survival.SCMOD2$vital_status,\n                    strata=as.integer(factor(data.for.survival.SCMOD2$SCMOD2)),\n                    nfold=10, setseed=54321)$cvpl\n\nprint.data.frame(data.frame(cbind(PAM5_CVPL,SCMOD2_CVPL)))\n\n## ----computeRiskScore---------------------------------------------------------\ndn <- c(\"transbig\", \"unt\", \"upp\", \"mainz\", \"nki\")\ndn.platform <- c(\"affy\", \"affy\", \"affy\", \"affy\", \"agilent\")\n\nres <- ddemo.all <- ddemo.coln <- NULL\nfor(i in 1:length(dn)) {\n\n  ## load dataset\n  dd <- get(data(list=dn[i]))\n\n  #Extract expression set, pData, fData for each dataset\n  ddata <- t(exprs(dd))\n  ddemo <- phenoData(dd)@data\n  dannot <- featureData(dd)@data\n  ddemo.all <- c(ddemo.all, list(ddemo))\n  if(is.null(ddemo.coln))\n  { ddemo.coln <- colnames(ddemo) } else\n  { ddemo.coln <- intersect(ddemo.coln, colnames(ddemo)) }\n  rest <- NULL\n\n  ## AURKA\n  ## if affy platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  modt <- scmgene.robust$mod$AURKA\n  ## if agilent platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"agilent\") {\n    domap <- FALSE\n    modt[ , \"probe\"] <- \"NM_003600\"\n  }\n  rest <- cbind(rest, \"AURKA\"=sig.score(x=modt, data=ddata, annot=dannot, \n                                        do.mapping=domap)$score)\n\n  ## ESR1\n  ## if affy platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  modt <- scmgene.robust$mod$ESR1\n  ## if agilent platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"agilent\") {\n    domap <- FALSE\n    modt[ , \"probe\"] <- \"NM_000125\"\n  }\n  rest <- cbind(rest, \"ESR1\"=sig.score(x=modt, data=ddata, annot=dannot, \n                                       do.mapping=domap)$score)\n\n  ## ERBB2\n  ## if affy platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  modt <- scmgene.robust$mod$ERBB2\n  ## if agilent platform consider the probe published in Desmedt et al., CCR, 2008\n  if(dn.platform[i] == \"agilent\") {\n    domap <- FALSE\n    modt[ , \"probe\"] <- \"NM_004448\"\n  }\n  rest <- cbind(rest, \"ERBB2\"=sig.score(x=modt, data=ddata, annot=dannot, \n                                        do.mapping=domap)$score)\n\n  ## NPI\n  ss <- ddemo[ , \"size\"]\n  gg <- ddemo[ , \"grade\"]\n  nn <- rep(NA, nrow(ddemo))\n  nn[complete.cases(ddemo[ , \"node\"]) & ddemo[ , \"node\"] == 0] <- 1\n  nn[complete.cases(ddemo[ , \"node\"]) & ddemo[ , \"node\"] == 1] <- 3\n  names(ss) <- names(gg) <- names(nn) <- rownames(ddemo)\n  rest <- cbind(rest, \"NPI\"=npi(size=ss, grade=gg, node=nn, na.rm=TRUE)$score)\n\n  ## GGI\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"GGI\"=ggi(data=ddata, annot=dannot, \n                                do.mapping=domap)$score)\n\n  ## GENIUS\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"GENIUS\"=genius(data=ddata, annot=dannot, \n                                      do.mapping=domap)$score)\n\n  ## ENDOPREDICT\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"EndoPredict\"=endoPredict(data=ddata, annot=dannot, \n                                                do.mapping=domap)$score)\n\n  # OncotypeDx\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"OncotypeDx\"=oncotypedx(data=ddata, annot=dannot, \n                                              do.mapping=domap)$score)\n\n  ## TamR\n  # Note: risk is not implemented, the function will return NA values\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"TAMR13\"=tamr13(data=ddata, annot=dannot, \n                                      do.mapping=domap)$score)\n\n  ## GENE70\n  # Need to do mapping for Affy platforms because this is based on Agilent.\n  # Hence the mapping rule is reversed here!\n  if(dn.platform[i] == \"affy\") { domap <- TRUE } else { domap <- FALSE }\n  rest <- cbind(rest, \"GENE70\"=gene70(data=ddata, annot=dannot, std=\"none\",\n                                      do.mapping=domap)$score)\n\n  ## Pik3cags\n  if(dn.platform[i] == \"affy\") { domap <- FALSE } else { domap <- TRUE }\n  rest <- cbind(rest, \"PIK3CA\"=pik3cags(data=ddata, annot=dannot, \n                                        do.mapping=domap))\n\n  ## rorS\n  # Uses the pam50 algorithm. Need to do mapping for both Affy and Agilent\n  rest <- cbind(rest, \"rorS\"=rorS(data=ddata, annot=dannot, \n                                  do.mapping=TRUE)$score)\n\n  ## GENE76\n  # Mainly designed for Affy platforms. Has been excluded here\n\n  # BIND ALL TOGETHER\n  res <- rbind(res, rest)\n}\nnames(ddemo.all) <- dn\n\n## ----simplifyAndRemoveDuplicatePatients---------------------------------------\nddemot <- NULL\nfor(i in 1:length(ddemo.all)) {\n  ddemot <- rbind(ddemot, ddemo.all[[i]][ , ddemo.coln, drop=FALSE])\n}\nres[complete.cases(ddemot[ ,\"dataset\"]) & ddemot[ ,\"dataset\"] == \"VDX\", \"GENIUS\"] <- NA\n\n## select only untreated node-negative patients with all risk predictions\n## ie(incomplete cases (where risk prediction may be missing for a sample) are subsequently removed))\n# Note that increasing the number of risk prediction analyses\n# may increase the number of incomplete cases\n# In the previous vignette for genefu version1, we were only testing 4 risk predictors,\n# so we had a total of 722 complete cases remaining\n# Here, we are now testing 12 risk predictors, so we only have 713 complete cases remaining.\n# The difference of 9 cases between the two versions are all from the NKI dataset.\nmyx <- complete.cases(res, ddemot[ , c(\"node\", \"treatment\")]) &\n  ddemot[ , \"treatment\"] == 0 & ddemot[ , \"node\"] == 0 & !is.element(rownames(ddemot), duPL)\n\nres <- res[myx, , drop=FALSE]\nddemot <- ddemot[myx, , drop=FALSE]\n\n## ----cindexComputation--------------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\", \"GGI\", \"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrix <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  # Calculate cindex computation for each predictor\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(c(\"cindex\"=tt$c.index, \"cindex.se\"=tt$se, \"lower\"=tt$lower, \"upper\"=tt$upper)); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrix[[Dat]] <- rbind(resMatrix[[Dat]], cindex)\n  }\n}\n\n## ----combineEstimations-------------------------------------------------------\nfor(i in names(resMatrix)){\n  #Get a meta-estimate\n  ceData <- combine.est(x=resMatrix[[i]][,\"cindex\"], x.se=resMatrix[[i]][,\"cindex.se\"], hetero=TRUE)\n  cLower <- ceData$estimate + qnorm(0.025, lower.tail=TRUE) * ceData$se\n  cUpper <- ceData$estimate + qnorm(0.025, lower.tail=FALSE) * ceData$se\n\n  cindexO <- cbind(\"cindex\"=ceData$estimate, \"cindex.se\"=ceData$se, \"lower\"=cLower, \"upper\"=cUpper)\n  resMatrix[[i]] <- rbind(resMatrix[[i]], cindexO)\n  rownames(resMatrix[[i]]) <- c(datasetList, \"Overall\")\n}\n\n## ----computePValues-----------------------------------------------------------\npv <- sapply(resMatrix, function(x) { return(x[\"Overall\", c(\"cindex\",\"cindex.se\")]) })\npv <- apply(pv, 2, function(x) { return(pnorm((x[1] - 0.5) / x[2], lower.tail=x[1] < 0.5)) })\nprintPV <- matrix(pv,ncol=length(names(resMatrix)))\nrownames(printPV) <- \"P-value\"\ncolnames(printPV) <- names(pv)\nprintPV<-t(printPV)\n\n## ----printPvalue,results=\"asis\"-----------------------------------------------\nknitr::kable(printPV, digits=c(0, -1))\n\n## ----forestplotDatasets,echo=TRUE---------------------------------------------\nRiskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\", \"GGI\", \"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\ndatasetListF <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\", \"Overall\")\nmyspace <- \"   \"\npar(mfrow=c(2,2))\n  for (RP in RiskPList)\n  {\n\n  #<<forestplotDat,fig=TRUE>>=\n  ## Forestplot\n  tt <- rbind(resMatrix[[RP]][1:5,],\n            \"Overall\"=resMatrix[[RP]][6,])\n\n  tt <- as.data.frame(tt)\n  labeltext <- (datasetListF)\n\n  r.mean <- c(tt$cindex)\n  r.lower <- c(tt$lower)\n  r.upper <- c(tt$upper)\n\n  metaplot.surv(mn=r.mean, lower=r.lower, upper=r.upper, labels=labeltext, xlim=c(0.3,0.9),\n                boxsize=0.5, zero=0.5,\n                col=meta.colors(box=\"royalblue\",line=\"darkblue\",zero=\"firebrick\"),\n                main=paste(RP))\n\n  }\n\n## ----forestplotOverall,echo=TRUE----------------------------------------------\n## Overall Forestplot\nmybigspace <- \"       \"\ntt <- rbind(\"OverallA\"=resMatrix[[\"AURKA\"]][6,],\n            \"OverallE1\"=resMatrix[[\"ESR1\"]][6,],\n            \"OverallE2\"=resMatrix[[\"ERBB2\"]][6,],\n            \"OverallN\"=resMatrix[[\"NPI\"]][6,],\n          \"OverallM\"=resMatrix[[\"GGI\"]][6,],\n          \"OverallG\"=resMatrix[[\"GENIUS\"]][6,],\n          \"OverallE3\"=resMatrix[[\"EndoPredict\"]][6,],\n          \"OverallOD\"=resMatrix[[\"OncotypeDx\"]][6,],\n          \"OverallT\"=resMatrix[[\"TAMR13\"]][6,],\n          \"OverallG70\"=resMatrix[[\"GENE70\"]][6,],\n          \"OverallP\"=resMatrix[[\"PIK3CA\"]][6,],\n          \"OverallR\"=resMatrix[[\"rorS\"]][6,]\n          )\n\ntt <- as.data.frame(tt)\nlabeltext <- cbind(c(\"Risk Prediction\",\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\n                     \"GGI\",\"GENIUS\",\"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\"))\n\nr.mean <- c(NA,tt$cindex)\nr.lower <- c(NA,tt$lower)\nr.upper <- c(NA,tt$upper)\n\nmetaplot.surv(mn=r.mean, lower=r.lower, upper=r.upper, labels=labeltext, xlim=c(0.35,0.75),\n              boxsize=0.5, zero=0.5,\n              col=meta.colors(box=\"royalblue\",line=\"darkblue\",zero=\"firebrick\"),\n              main=\"Overall Concordance Index\")\n\n## ----computeCindexWithPvalue--------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\"GGI\",\"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrixFull <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  ## cindex and p-value computation per algorithm\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(tt); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrixFull[[Dat]] <- rbind(resMatrixFull[[Dat]], cindex)\n  }\n}\n\nfor(i in names(resMatrixFull)){\n  rownames(resMatrixFull[[i]]) <- datasetList\n}\n\nccmData <- tt <- rr <- NULL\nfor(i in 1:length(resMatrixFull)){\n  tt <- NULL\n  for(j in 1:length(resMatrixFull)){\n    if(i != j) { rr <- cindex.comp.meta(list.cindex1=resMatrixFull[[i]],\n                                        list.cindex2=resMatrixFull[[j]], hetero=TRUE)$p.value }\n    else { rr <- 1 }\n    tt <- cbind(tt, rr)\n  }\n  ccmData <- rbind(ccmData, tt)\n}\nccmData <- as.data.frame(ccmData)\ncolnames(ccmData) <- riskPList\nrownames(ccmData) <- riskPList\n\n## ----computeCCMPval-----------------------------------------------------------\nccmDataPval <- matrix(p.adjust(data.matrix(ccmData), method=\"holm\"),\n                      ncol=length(riskPList), dimnames=list(rownames(ccmData),\n                                                            colnames(ccmData)))\n\n## ----sessionInfo, echo=FALSE--------------------------------------------------\nsessionInfo()\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `findDuplicatedPatients` section in the code?",
        "answer": "The `findDuplicatedPatients` section aims to identify and remove duplicate patient records across different breast cancer datasets (TRANSBIG, UNT, UPP). It searches for patients with the same ID in different series (VDXKIU, KIU, UPPU, VDXOXFU, OXFU) and creates a list of duplicated patient IDs to be removed from further analysis."
      },
      {
        "question": "How are molecular subtypes calculated in this code, and which methods are used?",
        "answer": "Molecular subtypes are calculated using two methods: SCMOD2 and PAM50. The `molecular.subtyping` function from the genefu package is used with the arguments `sbt.model = 'scmod2'` and `sbt.model = 'pam50'` respectively. These methods classify breast cancer samples into subtypes such as Basal, HER2+, Luminal A, and Luminal B based on gene expression patterns."
      },
      {
        "question": "What is the purpose of the `cvpl` function in the code, and how is it used to compare PAM50 and SCMOD2?",
        "answer": "The `cvpl` (Cross-Validated Partial Likelihood) function is used to assess and compare the predictive performance of PAM50 and SCMOD2 subtyping methods. It calculates the CVPL score for each method using patient age, survival time, and subtype classification as inputs. The resulting CVPL scores are then compared to evaluate which subtyping method provides better predictive power for patient outcomes."
      }
    ],
    "completion_tasks": [
      {
        "partial": "## ----computeCindexWithPvalue--------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\"GGI\",\"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrixFull <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  ## cindex and p-value computation per algorithm\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(tt); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrixFull[[Dat]] <- rbind(resMatrixFull[[Dat]], cindex)\n  }\n}",
        "complete": "## ----computeCindexWithPvalue--------------------------------------------------\ncc.res <- complete.cases(res)\ndatasetList <- c(\"MAINZ\",\"TRANSBIG\",\"UPP\",\"UNT\",\"NKI\")\nriskPList <- c(\"AURKA\",\"ESR1\",\"ERBB2\",\"NPI\",\"GGI\",\"GENIUS\",\n               \"EndoPredict\",\"OncotypeDx\",\"TAMR13\",\"GENE70\",\"PIK3CA\",\"rorS\")\nsetT <- setE <- NULL\nresMatrixFull <- as.list(NULL)\n\nfor(i in datasetList)\n{\n  dataset.only <- ddemot[,\"dataset\"] == i\n  patientsAll <- cc.res & dataset.only\n\n  ## set type of available survival data\n  if(i == \"UPP\") {\n    setT <- \"t.rfs\"\n    setE <- \"e.rfs\"\n  } else {\n    setT <- \"t.dmfs\"\n    setE <- \"e.dmfs\"\n  }\n\n  ## cindex and p-value computation per algorithm\n  for (Dat in riskPList)\n  {\n    cindex <- t(apply(X=t(res[patientsAll,Dat]), MARGIN=1, function(x, y, z) {\n    tt <- concordance.index(x=x, surv.time=y, surv.event=z, method=\"noether\", na.rm=TRUE);\n    return(tt); },\n    y=ddemot[patientsAll,setT], z=ddemot[patientsAll, setE]))\n\n    resMatrixFull[[Dat]] <- rbind(resMatrixFull[[Dat]], cindex)\n  }\n}\n\nfor(i in names(resMatrixFull)){\n  rownames(resMatrixFull[[i]]) <- datasetList\n}\n\nccmData <- tt <- rr <- NULL\nfor(i in 1:length(resMatrixFull)){\n  tt <- NULL\n  for(j in 1:length(resMatrixFull)){\n    if(i != j) { rr <- cindex.comp.meta(list.cindex1=resMatrixFull[[i]],\n                                        list.cindex2=resMatrixFull[[j]], hetero=TRUE)$p.value }\n    else { rr <- 1 }\n    tt <- cbind(tt, rr)\n  }\n  ccmData <- rbind(ccmData, tt)\n}\nccmData <- as.data.frame(ccmData)\ncolnames(ccmData) <- riskPList\nrownames(ccmData) <- riskPList"
      },
      {
        "partial": "## ----computeCCMPval-----------------------------------------------------------\nccmDataPval <- matrix(p.adjust(data.matrix(ccmData), method=\"holm\"),\n                      ncol=length(riskPList), dimnames=list(rownames(ccmData),\n                                                            colnames(ccmData)))",
        "complete": "## ----computeCCMPval-----------------------------------------------------------\nccmDataPval <- matrix(p.adjust(data.matrix(ccmData), method=\"holm\"),\n                      ncol=length(riskPList), dimnames=list(rownames(ccmData),\n                                                            colnames(ccmData)))\n\n## ----sessionInfo, echo=FALSE--------------------------------------------------\nsessionInfo()"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeIC50.R",
    "language": "R",
    "content": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose=TRUE,\n                    trunc=TRUE))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeIC50` function and how does it relate to `computeICn`?",
        "answer": "The `computeIC50` function is a wrapper for the `computeICn` function, specifically designed to calculate the IC50 (half maximal inhibitory concentration) of a drug dose-response curve. It calls `computeICn` with a fixed `n` value of 50 (or 0.5 if viability is not in percentage), while passing through most of its parameters. This function simplifies the process of calculating IC50 by providing a dedicated interface for this common use case."
      },
      {
        "question": "How does the `viability_as_pct` parameter affect the calculation of IC50?",
        "answer": "The `viability_as_pct` parameter determines how the viability data is interpreted and how the IC50 is calculated. If `viability_as_pct` is TRUE (default), the function assumes the viability data is in percentage and sets the `n` parameter in `computeICn` to 50. If FALSE, it assumes the viability data is in decimal form (0-1 range) and sets `n` to 0.5. This ensures that the IC50 is correctly calculated regardless of the input data format."
      },
      {
        "question": "What is the significance of the `@describeIn` and `@export` roxygen2 tags in this function definition?",
        "answer": "The roxygen2 tags provide metadata for documentation and package building:\n1. `@describeIn computeICn`: This tag indicates that the `computeIC50` function should be documented as part of the `computeICn` function family. It will appear in the same help file as `computeICn`, improving organization of related functions.\n2. `@export`: This tag marks the `computeIC50` function to be exported when building the R package, making it available for users to call directly after loading the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ,\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}"
      },
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(,\n                    ,\n                    ,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    ,\n                    ,\n                    ,\n                    ))\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#' @export\ncomputeIC50 <- function(concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        verbose = TRUE,\n                        trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ps.cluster.R",
    "language": "R",
    "content": "#' @title Function to compute the prediction strength of a clustering model\n#'\n#' @description\n#' This function computes the prediction strength of a clustering model as published \n#'   in R. Tibshirani and G. Walther 2005.\n#'\n#' @usage\n#' ps.cluster(cl.tr, cl.ts, na.rm = FALSE)\n#'\n#' @param cl.tr\tClusters membership as defined by the original clustering model, i.e. \n#'   the one that was not fitted on the dataset of interest.\n#' @param cl.ts\tClusters membership as defined by the clustering model fitted on the \n#'   dataset of interest.\n#' @param na.rm\tTRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - ps: the overall prediction strength (minimum of the prediction strengths at cluster level).\n#' - ps.cluster: Prediction strength for each cluster\n#' - ps.individual: Prediction strength for each sample.\n#'\n#' @references\n#' R. Tibshirani and G. Walther (2005) \"Cluster Validation by Prediction Strength\", \n#'   Journal of Computational and Graphical Statistics, 14(3):511-528.\n#'\n#' @examples\n#' # load SSP signature published in Sorlie et al. 2003\n#' data(ssp2003)\n#' # load NKI data\n#' data(nkis)\n#' # SP2003 fitted on NKI\n#' ssp2003.2nkis <- intrinsic.cluster(data=data.nkis, annot=annot.nkis,\n#'   do.mapping=TRUE, std=\"robust\",\n#'   intrinsicg=ssp2003$centroids.map[ ,c(\"probe\", \"EntrezGene.ID\")],\n#'   number.cluster=5, mins=5, method.cor=\"spearman\",\n#'   method.centroids=\"mean\", verbose=TRUE)\n#' # SP2003 published in Sorlie et al 2003 and applied in VDX\n#' ssp2003.nkis <- intrinsic.cluster.predict(sbt.model=ssp2003,\n#'   data=data.nkis, annot=annot.nkis, do.mapping=TRUE, verbose=TRUE)\n#' # prediction strength of sp2003 clustering model\n#' ps.cluster(cl.tr=ssp2003.2nkis$subtype, cl.ts=ssp2003.nkis$subtype,\n#'   na.rm = FALSE)\n#'\n#' @md\n#' @export\nps.cluster <-\nfunction(cl.tr, cl.ts, na.rm=FALSE) {\n\t## consider cl.ts as reference\n\tif(length(cl.tr) != length(cl.ts)) { stop(\"the two clustering must have the same length!\") }\n\tif(is.null(names(cl.tr))) { names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\") }\n\tcc.ix <- complete.cases(cl.tr, cl.ts)\n\tif(any(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tnn <- sum(cc.ix)\n\tcltr <- cl.tr[cc.ix]\n\tclts <- cl.ts[cc.ix]\n\tucltr <- sort(unique(cltr))\t\n\tuclts <- sort(unique(clts))\n\tif(length(ucltr) != length(uclts)) {\n\t\tmessage(\"the number of clusters should be the same\")\n\t\ttt <- rep(NA, max(length(ucltr), length(uclts)))\n\t\tnames(tt) <- ifelse(length(ucltr) > length(uclts), ucltr, uclts)\n\t\ttt2 <- rep(NA, length(cl.tr))\n\t\tnames(tt2) <- names(cl.tr)\n\t\treturn(list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2))\n\t}\n\tif(!all(ucltr == uclts)) { ## the number of clusters is the same but the labels differ\n\t\tmessage(\"the labels of the clusters differ\")\n\t\t## keep labels from cl.ts\n\t\ttixtr <- !is.element(ucltr, uclts)\n\t\ttixts <- !is.element(uclts, ucltr)\n\t\tfor(mm in 1:sum(tixtr)) {\n\t\t\tcltr[cltr == ucltr[which(tixtr)[mm]]] <- uclts[which(tixts)[mm]]\n\t\t}\n\t\tucltr <- sort(unique(cltr))\t\n\t}\n\tll <- length(uclts)\n\t\n\t## co-membership matrix for the two clusterings\n\tDtr <- matrix(NA, nrow=nn, ncol=nn, dimnames=list(names(cltr), names(cltr)))\n\tDts <- matrix(NA, nrow=nn, ncol=nn, dimnames=list(names(clts), names(clts)))\n\tfor(i in 1:(nn-1)) {\n\t\tfor(j in (i+1):nn) {\n\t\t\tif(cltr[i] == cltr[j]) { Dtr[i,j] <- Dtr[j,i] <- 1 } else { Dtr[i,j] <- Dtr[j,i] <- 0 }\n\t\t\tif(clts[i] == clts[j]) { Dts[i,j] <- Dts[j,i] <- 1 } else { Dts[i,j] <- Dts[j,i] <- 0 }\n\t\t}\n\t}\n\t\n\tnltr <- table(cltr)[as.character(ucltr)]\n\tnlts <- table(clts)[as.character(uclts)]\n\tmyps <- NULL\n\tfor(l in 1:ll) {\n\t\tal <- which(clts == uclts[l])\n\t\tif(length(al) > 1) { rr <- (1 / (nlts[l] * (nlts[l] - 1))) * sum(Dtr[al,al], na.rm=TRUE) } else { rr <- 0}\n\t\tmyps <- c(myps, rr)\t\n\t}\n\tif(length(uclts) >= length(ucltr)) { names(myps) <- uclts } else { names(myps) <- ucltr }\n\t\n\tmyps.ind <- NULL\n\tfor(i in 1:nn) {\n\t\taki <- Dts[i, ] == 1 & !is.na(Dts[i, ])\n\t\trr <- (1 / sum(aki)) * sum(Dtr[i,aki] == 1, na.rm=TRUE)\n\t\tmyps.ind <- c(myps.ind, rr)\n\t}\n\tnames(myps.ind) <- names(clts)\n\tmyps.ind2 <- rep(NA, length(cl.tr))\n\tnames(myps.ind2) <- names(cl.tr)\n\tmyps.ind2[names(myps.ind)] <- myps.ind\n\n\treturn(list(\"ps\"=min(myps), \"ps.cluster\"=myps, \"ps.individual\"=myps.ind2))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ps.cluster` function and what does it return?",
        "answer": "The `ps.cluster` function computes the prediction strength of a clustering model as described by R. Tibshirani and G. Walther (2005). It takes two cluster memberships (`cl.tr` and `cl.ts`) as input and returns a list containing three items: 1) 'ps': the overall prediction strength (minimum of the prediction strengths at cluster level), 2) 'ps.cluster': Prediction strength for each cluster, and 3) 'ps.individual': Prediction strength for each sample."
      },
      {
        "question": "How does the function handle cases where the number of clusters or cluster labels differ between `cl.tr` and `cl.ts`?",
        "answer": "The function checks if the number of clusters in `cl.tr` and `cl.ts` are the same. If they differ, it returns a list with 'ps' set to 0 and NA values for 'ps.cluster' and 'ps.individual'. If the number of clusters is the same but labels differ, it attempts to match the labels from `cl.ts` to `cl.tr`. The function issues messages in both cases to inform the user about these discrepancies."
      },
      {
        "question": "Explain the algorithm used to calculate the prediction strength for each cluster in the `ps.cluster` function.",
        "answer": "The function calculates prediction strength for each cluster using these steps: 1) Create co-membership matrices (Dtr and Dts) for both clusterings. 2) For each cluster l in cl.ts, find all samples al in that cluster. 3) Calculate the prediction strength for cluster l as the sum of co-memberships in Dtr for samples in al, divided by nlts[l] * (nlts[l] - 1), where nlts[l] is the number of samples in cluster l. 4) The overall prediction strength 'ps' is the minimum of all cluster prediction strengths. This approach measures how well the clustering structure from cl.tr predicts the clustering in cl.ts."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) { stop(\"the two clustering must have the same length!\") }\n  if(is.null(names(cl.tr))) { names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\") }\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  # Complete the function to calculate prediction strength\n  \n}",
        "complete": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) { stop(\"the two clustering must have the same length!\") }\n  if(is.null(names(cl.tr))) { names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\") }\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  if(length(ucltr) != length(uclts)) {\n    message(\"the number of clusters should be the same\")\n    tt <- rep(NA, max(length(ucltr), length(uclts)))\n    names(tt) <- ifelse(length(ucltr) > length(uclts), ucltr, uclts)\n    tt2 <- rep(NA, length(cl.tr))\n    names(tt2) <- names(cl.tr)\n    return(list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2))\n  }\n  \n  if(!all(ucltr == uclts)) {\n    message(\"the labels of the clusters differ\")\n    tixtr <- !is.element(ucltr, uclts)\n    tixts <- !is.element(uclts, ucltr)\n    for(mm in 1:sum(tixtr)) {\n      cltr[cltr == ucltr[which(tixtr)[mm]]] <- uclts[which(tixts)[mm]]\n    }\n    ucltr <- sort(unique(cltr))\t\n  }\n  \n  ll <- length(uclts)\n  Dtr <- Dts <- matrix(0, nrow=nn, ncol=nn)\n  for(i in 1:(nn-1)) {\n    for(j in (i+1):nn) {\n      Dtr[i,j] <- Dtr[j,i] <- as.integer(cltr[i] == cltr[j])\n      Dts[i,j] <- Dts[j,i] <- as.integer(clts[i] == clts[j])\n    }\n  }\n  \n  nlts <- table(clts)\n  myps <- sapply(uclts, function(l) {\n    al <- which(clts == l)\n    if(length(al) > 1) sum(Dtr[al,al]) / (nlts[as.character(l)] * (nlts[as.character(l)] - 1)) else 0\n  })\n  \n  myps.ind <- sapply(1:nn, function(i) {\n    aki <- Dts[i, ] == 1\n    sum(Dtr[i,aki]) / sum(aki)\n  })\n  names(myps.ind) <- names(clts)\n  myps.ind2 <- rep(NA, length(cl.tr))\n  names(myps.ind2) <- names(cl.tr)\n  myps.ind2[names(myps.ind)] <- myps.ind\n\n  list(\"ps\"=min(myps), \"ps.cluster\"=myps, \"ps.individual\"=myps.ind2)\n}"
      },
      {
        "partial": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) stop(\"the two clustering must have the same length!\")\n  if(is.null(names(cl.tr))) names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\")\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) stop(\"missing values are present!\")\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  # Complete the function to handle different cluster numbers and labels\n  \n}",
        "complete": "ps.cluster <- function(cl.tr, cl.ts, na.rm=FALSE) {\n  if(length(cl.tr) != length(cl.ts)) stop(\"the two clustering must have the same length!\")\n  if(is.null(names(cl.tr))) names(cl.tr) <- names(cl.ts) <- paste(\"X\", 1:length(cl.tr), sep=\".\")\n  cc.ix <- complete.cases(cl.tr, cl.ts)\n  if(any(!cc.ix) & !na.rm) stop(\"missing values are present!\")\n  nn <- sum(cc.ix)\n  cltr <- cl.tr[cc.ix]\n  clts <- cl.ts[cc.ix]\n  ucltr <- sort(unique(cltr))\t\n  uclts <- sort(unique(clts))\n  \n  if(length(ucltr) != length(uclts)) {\n    message(\"the number of clusters should be the same\")\n    tt <- rep(NA, max(length(ucltr), length(uclts)))\n    names(tt) <- ifelse(length(ucltr) > length(uclts), ucltr, uclts)\n    tt2 <- rep(NA, length(cl.tr))\n    names(tt2) <- names(cl.tr)\n    return(list(\"ps\"=0, \"ps.cluster\"=tt, \"ps.individual\"=tt2))\n  }\n  \n  if(!all(ucltr == uclts)) {\n    message(\"the labels of the clusters differ\")\n    tixtr <- !is.element(ucltr, uclts)\n    tixts <- !is.element(uclts, ucltr)\n    for(mm in 1:sum(tixtr)) {\n      cltr[cltr == ucltr[which(tixtr)[mm]]] <- uclts[which(tixts)[mm]]\n    }\n    ucltr <- sort(unique(cltr))\t\n  }\n  \n  ll <- length(uclts)\n  Dtr <- Dts <- matrix(0, nrow=nn, ncol=nn)\n  for(i in 1:(nn-1)) {\n    for(j in (i+1):nn) {\n      Dtr[i,j] <- Dtr[j,i] <- as.integer(cltr[i] == cltr[j])\n      Dts[i,j] <- Dts[j,i] <- as.integer(clts[i] == clts[j])\n    }\n  }\n  \n  nlts <- table(clts)\n  myps <- sapply(uclts, function(l) {\n    al <- which(clts == l)\n    if(length(al) > 1) sum(Dtr[al,al]) / (nlts[as.character(l)] * (nlts[as.character(l)] - 1)) else 0\n  })\n  \n  myps.ind <- sapply(1:nn, function(i) {\n    aki <- Dts[i, ] == 1\n    sum(Dtr[i,aki]) / sum(aki)\n  })\n  names(myps.ind) <- names(clts)\n  myps.ind2 <- rep(NA, length(cl.tr))\n  names(myps.ind2) <- names(cl.tr)\n  myps.ind2[names(myps.ind)] <- myps.ind\n\n  list(\"ps\"=min(myps), \"ps.cluster\"=myps, \"ps.individual\"=myps.ind2)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/dindex.comp.R",
    "language": "R",
    "content": "`dindex.comp` <-\nfunction(dindex1, dindex2) {\n\tif(dindex1$n != dindex2$n) { stop(\"the D indices are computed from different number of samples!\") }\n\tn <- dindex1$n\n\tx1 <- dindex1$data$z\n\tx2 <- dindex2$data$z\n\tbeta1 <- dindex1$coef\n\tbeta2 <- dindex2$coef\n\tse1 <- dindex1$se\n\tse2 <- dindex2$se\n\tr <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n\tif(abs(r) < 1) {\n\t\tt.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"dindex1\"=exp(beta1), \"dindex2\"=exp(beta2)))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dindex.comp` function in R, and what are its main inputs?",
        "answer": "The `dindex.comp` function in R is designed to compare two D indices. It takes two main inputs: `dindex1` and `dindex2`, which are expected to be objects containing D index information. The function compares these indices and returns a list with the p-value of their difference and the exponential of their coefficients."
      },
      {
        "question": "How does the function handle the case when the correlation between x1 and x2 is perfect (1 or -1)?",
        "answer": "When the absolute value of the correlation (r) between x1 and x2 is 1 (perfect correlation), the function sets the p-value (`diff.ci.p`) to 1. This is handled in the if-else statement: if `abs(r) < 1`, it calculates the t-statistic and p-value; otherwise, it directly sets `diff.ci.p <- 1`."
      },
      {
        "question": "What statistical test is being performed in this function, and how is the test statistic calculated?",
        "answer": "The function performs a t-test to compare the two D indices. The test statistic (t.stat) is calculated as the difference between the coefficients (beta1 - beta2) divided by the square root of the sum of their squared standard errors minus twice their product multiplied by the correlation: (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2). This formula accounts for the correlation between the two indices."
      }
    ],
    "completion_tasks": [
      {
        "partial": "dindex.comp <- function(dindex1, dindex2) {\n  if(dindex1$n != dindex2$n) { stop(\"the D indices are computed from different number of samples!\") }\n  n <- dindex1$n\n  x1 <- dindex1$data$z\n  x2 <- dindex2$data$z\n  beta1 <- dindex1$coef\n  beta2 <- dindex2$coef\n  se1 <- dindex1$se\n  se2 <- dindex2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    # Complete the t-statistic calculation and p-value computation\n  } else { diff.ci.p <- 1 }\n  # Return the result\n}",
        "complete": "dindex.comp <- function(dindex1, dindex2) {\n  if(dindex1$n != dindex2$n) { stop(\"the D indices are computed from different number of samples!\") }\n  n <- dindex1$n\n  x1 <- dindex1$data$z\n  x2 <- dindex2$data$z\n  beta1 <- dindex1$coef\n  beta2 <- dindex2$coef\n  se1 <- dindex1$se\n  se2 <- dindex2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"dindex1\"=exp(beta1), \"dindex2\"=exp(beta2)))\n}"
      },
      {
        "partial": "dindex.comp <- function(dindex1, dindex2) {\n  # Check if the number of samples is the same\n  # Extract necessary data from dindex1 and dindex2\n  # Calculate the correlation\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  # Return the result\n}",
        "complete": "dindex.comp <- function(dindex1, dindex2) {\n  if(dindex1$n != dindex2$n) stop(\"the D indices are computed from different number of samples!\")\n  n <- dindex1$n\n  x1 <- dindex1$data$z\n  x2 <- dindex2$data$z\n  beta1 <- dindex1$coef\n  beta2 <- dindex2$coef\n  se1 <- dindex1$se\n  se2 <- dindex2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(p.value=diff.ci.p, dindex1=exp(beta1), dindex2=exp(beta2)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/forestplot.surv.R",
    "language": "R",
    "content": "`forestplot.surv` <-\nfunction(labeltext, mean, lower, upper, align=NULL, is.summary=FALSE, clip=c(-Inf,Inf), xlab=\"\", zero= 0, graphwidth=unit(2,\"inches\"), col, xlog=FALSE, box.size=NULL, x.ticks=NULL, ...){\n\t\n  #require(\"grid\") || stop(\"`grid' package not found\")\n  #require(\"rmeta\") || stop(\"`rmeta' package not found\")\n\n  ## Function to draw a non-summary rect-plus-CI\n  drawNormalCI <- function(LL, OR, UL, size, bcol, lcol) {\n    size=0.75*size\n\n    clipupper<-convertX(unit(UL, \"native\"), \"npc\", valueOnly=TRUE) > 1\n    cliplower<-convertX(unit(LL, \"native\"), \"npc\", valueOnly=TRUE) < 0\n    box<- convertX(unit(OR, \"native\"), \"npc\", valueOnly=TRUE)\n    clipbox <- box<0 || box>1\n        \n    ## Draw arrow if exceed col range\n    ## convertX() used to convert between coordinate systems\n    if (clipupper || cliplower){\n      ends<-\"both\"\n      lims<-unit(c(0, 1), c(\"npc\", \"npc\"))\n      if (!clipupper) {\n        ends<-\"first\"\n        lims<-unit(c(0, UL), c(\"npc\",\"native\"))\n      }\n      if (!cliplower) {\n        ends<-\"last\"\n        lims<-unit(c(LL, 1), c(\"native\", \"npc\"))\n      }\n      grid.lines(x=lims, y=0.5,arrow=arrow(ends=ends,length=unit(0.05, \"inches\")),\n                 gp=gpar(col=lcol))\n\n      if (!clipbox)\n          grid.rect(x=unit(OR, \"native\"),\n                    width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                    gp=gpar(fill=bcol,col=bcol))\n      \n      } else   {\n      ## Draw line white if totally inside rect\n      grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5,\n                 gp=gpar(col=lcol))\n      grid.rect(x=unit(OR, \"native\"),\n                width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                gp=gpar(fill=bcol,col=bcol))\n      if ((convertX(unit(OR, \"native\") + unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) > UL) &&\n          (convertX(unit(OR, \"native\") - unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) < LL))\n        grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5, gp=gpar(col=lcol))\n    }\n  }\n  \n  ## Function to draw a summary \"diamond\"\n  drawSummaryCI <- function(LL, OR, UL, size, scol) {\n    grid.polygon(x=unit(c(LL, OR, UL, OR), \"native\"),\n                 y=unit(0.5 + c(0, 0.5*size, 0, -0.5*size), \"npc\"),gp=gpar(fill=scol,col=scol))\n  }\n  \n  plot.new()\n  ## calculate width based on labels with something in every column\n  widthcolumn<-!apply(is.na(labeltext),1,any)\n  if(missing(col)) { col <- rmeta::meta.colors() }\n  nc<-NCOL(labeltext)\n  nr<-NROW(labeltext)\n  labels<-vector(\"list\",nc)\n  if(length(col$lines) < nr) { col$lines <- rep(col$lines[1], nr) }\n  if(length(col$box) < nr) { col$box <- rep(col$box[1], nr) }\n  if(length(col$summary) < nr) { col$summary <- rep(col$summary[1], nr) }\n  \n  if (is.null(align))\n    align<-c(\"l\",rep(\"r\",nc-1))\n  else\n    align<-rep(align,length=nc)\n  \n  is.summary<-rep(is.summary,length=nr)\n  \n  for(j in 1:nc){\n    labels[[j]]<-vector(\"list\", nr)\n    for(i in 1:nr){\n      if (is.na(labeltext[i,j]))\n        next\n      x<-switch(align[j],l=0,r=1,c=0.5)\n      just<-switch(align[j],l=\"left\",r=\"right\",c=\"center\")\n      labels[[j]][[i]]<-textGrob(labeltext[i,j], x=x,just=just,\n                                 gp=gpar(fontface=if(is.summary[i]) \"bold\" else \"plain\",\n                                                                    col=rep(col$text,length=nr)[i]) )\n    }\n  }  \n  colgap<-unit(3,\"mm\") \n  colwidths<-unit.c(max(unit(rep(1,sum(widthcolumn)),\"grobwidth\",labels[[1]][widthcolumn])),colgap)\n  if (nc>1){\n    for(i in 2:nc)\n      colwidths<-unit.c(colwidths, max(unit(rep(1,sum(widthcolumn)),\"grobwidth\",labels[[i]][widthcolumn])),colgap)\n    \n  }\n  colwidths<-unit.c(colwidths,graphwidth)\n  \n  pushViewport(viewport(layout=grid.layout(nr+1,nc*2+1,\n                          widths=colwidths,\n                          heights=unit(c(rep(1, nr),0.5), \"lines\"))))\n  \n  cwidth<-(upper-lower)\n  xrange<-c(max(min(lower,na.rm=TRUE),clip[1]), min(max(upper,na.rm=TRUE),clip[2]))\n  if(is.null(box.size)) {\n    info<-1/cwidth\n    info<-info/max(info[!is.summary], na.rm=TRUE)\n    info[is.summary]<-1\n  }\n  else { info <- box.size }\n  for(j in 1:nc){\n    for(i in 1:nr){\n      if (!is.null(labels[[j]][[i]])){\n        pushViewport(viewport(layout.pos.row=i,layout.pos.col=2*j-1))\n        grid.draw(labels[[j]][[i]])\n          popViewport()\n      }\n    }\n  }\n  \n  pushViewport(viewport(layout.pos.col=2*nc+1, xscale=xrange))\n  grid.lines(x=unit(zero, \"native\"), y=0:1,gp=gpar(col=col$zero))\n  if (xlog){\n    ticks<-pretty(exp(xrange))\n    ticks<-ticks[ticks>0]\n    if (min(lower,na.rm=TRUE)<clip[1]) ticks<-c(exp(clip[1]),ticks)\n    if (max(upper,na.rm=TRUE)>clip[2]) ticks<-c(ticks,exp(clip[2]))\n    xax<-xaxisGrob(gp=gpar(cex=0.6,col=col$axes),at=log(ticks),name=\"xax\")\n    xax1<-editGrob(xax, gPath(\"labels\"), label=format(ticks,digits=2))\n    grid.draw(xax1)\n  } else {\n    grid.xaxis(at=x.ticks, gp=gpar(cex=1,col=col$axes))\n  }\n  grid.text(xlab, y=unit(-3, \"lines\"),gp=gpar(col=col$axes))\n  popViewport()\n  for (i in 1:nr) {\n    if (is.na(mean[i])) next\n    pushViewport(viewport(layout.pos.row=i, layout.pos.col=2*nc+1,\n                          xscale=xrange))\n    if (is.summary[i])\n      drawSummaryCI(lower[i], mean[i], upper[i], info[i], scol=col$summary[i])\n    else\n      drawNormalCI(lower[i], mean[i], upper[i], info[i], bcol=col$box[i], lcol=col$lines[i]) \n    popViewport()\n  }\n  popViewport()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drawNormalCI` function within the `forestplot.surv` function?",
        "answer": "The `drawNormalCI` function is responsible for drawing a non-summary rectangle with confidence intervals on the forest plot. It handles the visualization of individual study results, including clipping for out-of-range values and drawing arrows when necessary. The function takes care of positioning the rectangle (representing the point estimate) and the confidence interval lines, adjusting for various edge cases and clipping scenarios."
      },
      {
        "question": "How does the `forestplot.surv` function handle the alignment of text labels in the plot?",
        "answer": "The function handles text label alignment through the `align` parameter. If not provided, it defaults to left-aligned for the first column and right-aligned for the rest. The alignment is used in the `textGrob` function call within a nested loop, where `x` and `just` parameters are set based on the alignment value ('l' for left, 'r' for right, 'c' for center). This allows for flexible positioning of text labels in each column of the forest plot."
      },
      {
        "question": "What is the purpose of the `xlog` parameter in the `forestplot.surv` function, and how does it affect the x-axis?",
        "answer": "The `xlog` parameter determines whether the x-axis should be displayed on a logarithmic scale. When `xlog` is TRUE, the function calculates logarithmic tick marks using `pretty(exp(xrange))`, ensures all ticks are positive, and may add additional ticks at the clip boundaries. It then uses `xaxisGrob` to create a custom x-axis with these logarithmic ticks, formatting the labels to show the original (non-log) values. This is particularly useful for visualizing data that spans several orders of magnitude, such as odds ratios or hazard ratios in meta-analyses."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drawNormalCI <- function(LL, OR, UL, size, bcol, lcol) {\n  size = 0.75 * size\n  clipupper <- convertX(unit(UL, \"native\"), \"npc\", valueOnly=TRUE) > 1\n  cliplower <- convertX(unit(LL, \"native\"), \"npc\", valueOnly=TRUE) < 0\n  box <- convertX(unit(OR, \"native\"), \"npc\", valueOnly=TRUE)\n  clipbox <- box < 0 || box > 1\n\n  if (clipupper || cliplower) {\n    ends <- \"both\"\n    lims <- unit(c(0, 1), c(\"npc\", \"npc\"))\n    if (!clipupper) {\n      ends <- \"first\"\n      lims <- unit(c(0, UL), c(\"npc\", \"native\"))\n    }\n    if (!cliplower) {\n      ends <- \"last\"\n      lims <- unit(c(LL, 1), c(\"native\", \"npc\"))\n    }\n    grid.lines(x=lims, y=0.5, arrow=arrow(ends=ends, length=unit(0.05, \"inches\")),\n                gp=gpar(col=lcol))\n\n    if (!clipbox) {\n      grid.rect(x=unit(OR, \"native\"),\n                width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                gp=gpar(fill=bcol, col=bcol))\n    }\n  } else {\n    # Complete the else part\n  }\n}",
        "complete": "drawNormalCI <- function(LL, OR, UL, size, bcol, lcol) {\n  size = 0.75 * size\n  clipupper <- convertX(unit(UL, \"native\"), \"npc\", valueOnly=TRUE) > 1\n  cliplower <- convertX(unit(LL, \"native\"), \"npc\", valueOnly=TRUE) < 0\n  box <- convertX(unit(OR, \"native\"), \"npc\", valueOnly=TRUE)\n  clipbox <- box < 0 || box > 1\n\n  if (clipupper || cliplower) {\n    ends <- \"both\"\n    lims <- unit(c(0, 1), c(\"npc\", \"npc\"))\n    if (!clipupper) {\n      ends <- \"first\"\n      lims <- unit(c(0, UL), c(\"npc\", \"native\"))\n    }\n    if (!cliplower) {\n      ends <- \"last\"\n      lims <- unit(c(LL, 1), c(\"native\", \"npc\"))\n    }\n    grid.lines(x=lims, y=0.5, arrow=arrow(ends=ends, length=unit(0.05, \"inches\")),\n                gp=gpar(col=lcol))\n\n    if (!clipbox) {\n      grid.rect(x=unit(OR, \"native\"),\n                width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n                gp=gpar(fill=bcol, col=bcol))\n    }\n  } else {\n    grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5, gp=gpar(col=lcol))\n    grid.rect(x=unit(OR, \"native\"),\n              width=unit(size, \"snpc\"), height=unit(size, \"snpc\"),\n              gp=gpar(fill=bcol, col=bcol))\n    if ((convertX(unit(OR, \"native\") + unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) > UL) &&\n        (convertX(unit(OR, \"native\") - unit(0.5*size, \"lines\"), \"native\", valueOnly=TRUE) < LL))\n      grid.lines(x=unit(c(LL, UL), \"native\"), y=0.5, gp=gpar(col=lcol))\n  }\n}"
      },
      {
        "partial": "forestplot.surv <- function(labeltext, mean, lower, upper, align=NULL, is.summary=FALSE, clip=c(-Inf,Inf), xlab=\"\", zero=0, graphwidth=unit(2,\"inches\"), col, xlog=FALSE, box.size=NULL, x.ticks=NULL, ...) {\n  # ... (previous code)\n\n  pushViewport(viewport(layout=grid.layout(nr+1, nc*2+1,\n                        widths=colwidths,\n                        heights=unit(c(rep(1, nr), 0.5), \"lines\"))))\n\n  cwidth <- (upper - lower)\n  xrange <- c(max(min(lower, na.rm=TRUE), clip[1]), min(max(upper, na.rm=TRUE), clip[2]))\n  if (is.null(box.size)) {\n    info <- 1 / cwidth\n    info <- info / max(info[!is.summary], na.rm=TRUE)\n    info[is.summary] <- 1\n  } else {\n    info <- box.size\n  }\n\n  # Complete the rest of the function\n}",
        "complete": "forestplot.surv <- function(labeltext, mean, lower, upper, align=NULL, is.summary=FALSE, clip=c(-Inf,Inf), xlab=\"\", zero=0, graphwidth=unit(2,\"inches\"), col, xlog=FALSE, box.size=NULL, x.ticks=NULL, ...) {\n  # ... (previous code)\n\n  pushViewport(viewport(layout=grid.layout(nr+1, nc*2+1,\n                        widths=colwidths,\n                        heights=unit(c(rep(1, nr), 0.5), \"lines\"))))\n\n  cwidth <- (upper - lower)\n  xrange <- c(max(min(lower, na.rm=TRUE), clip[1]), min(max(upper, na.rm=TRUE), clip[2]))\n  if (is.null(box.size)) {\n    info <- 1 / cwidth\n    info <- info / max(info[!is.summary], na.rm=TRUE)\n    info[is.summary] <- 1\n  } else {\n    info <- box.size\n  }\n\n  for (j in 1:nc) {\n    for (i in 1:nr) {\n      if (!is.null(labels[[j]][[i]])) {\n        pushViewport(viewport(layout.pos.row=i, layout.pos.col=2*j-1))\n        grid.draw(labels[[j]][[i]])\n        popViewport()\n      }\n    }\n  }\n\n  pushViewport(viewport(layout.pos.col=2*nc+1, xscale=xrange))\n  grid.lines(x=unit(zero, \"native\"), y=0:1, gp=gpar(col=col$zero))\n  if (xlog) {\n    ticks <- pretty(exp(xrange))\n    ticks <- ticks[ticks > 0]\n    if (min(lower, na.rm=TRUE) < clip[1]) ticks <- c(exp(clip[1]), ticks)\n    if (max(upper, na.rm=TRUE) > clip[2]) ticks <- c(ticks, exp(clip[2]))\n    xax <- xaxisGrob(gp=gpar(cex=0.6, col=col$axes), at=log(ticks), name=\"xax\")\n    xax1 <- editGrob(xax, gPath(\"labels\"), label=format(ticks, digits=2))\n    grid.draw(xax1)\n  } else {\n    grid.xaxis(at=x.ticks, gp=gpar(cex=1, col=col$axes))\n  }\n  grid.text(xlab, y=unit(-3, \"lines\"), gp=gpar(col=col$axes))\n  popViewport()\n\n  for (i in 1:nr) {\n    if (is.na(mean[i])) next\n    pushViewport(viewport(layout.pos.row=i, layout.pos.col=2*nc+1, xscale=xrange))\n    if (is.summary[i])\n      drawSummaryCI(lower[i], mean[i], upper[i], info[i], scol=col$summary[i])\n    else\n      drawNormalCI(lower[i], mean[i], upper[i], info[i], bcol=col$box[i], lcol=col$lines[i])\n    popViewport()\n  }\n  popViewport()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ggi.R",
    "language": "R",
    "content": "#' @title Function to compute the raw and scaled Gene expression Grade Index (GGI)\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene expression\n#'   values following the algorithm used for the Gene expression Grade Index (GGI).\n#'\n#' @usage\n#' ggi(data, annot, do.mapping = FALSE, mapping, hg, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed\n#'   (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#'   mapping such that the probes are not selected based on their variance.\n#' @param hg Vector containing the histological grade (HG) status of breast cancer\n#'   patients in the dataset.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence between\n#' the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Sotiriou C, Wirapati P, Loi S, Harris A, Bergh J, Smeds J, Farmer P, Praz V,\n#'   Haibe-Kains B, Lallemand F, Buyse M, Piccart MJ and Delorenzi M (2006)\n#'   \"Gene expression profiling in breast cancer: Understanding the molecular basis\n#'   of histologic grade to improve prognosis\", Journal of National Cancer Institute,\n#'   98:262\u2013272\n#'\n#' @seealso\n#' [genefu::gene76]\n#'\n#' @examples\n#' # load GGI signature\n#' data(sig.ggi)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' ggi.nkis <- ggi(data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   hg=demo.nkis[ ,\"grade\"])\n#' table(ggi.nkis$risk)\n#'\n#' @md\n#' @export\n#' @name ggi\nggi <- function(data, annot, do.mapping=FALSE, mapping, hg, verbose=FALSE) {\n\n\t###########\n\t#internal functions\n\t###########\n\tscale.raw.ggi <- function(ggi, hg) {\n\tif(length(hg) != length(ggi)) { stop(\"bad length of hg!\") }\n\t\t\tmhg1 <- mean(ggi[hg == 1], na.rm=TRUE)\n\t\t\tmhg3 <- mean(ggi[hg == 3], na.rm=TRUE)\n\t\t\tmm <- mhg1 + (mhg3 -  mhg1) / 2\n\t\t\tres.scaled <- ((ggi - mm) / (mhg3 - mhg1)) * 2\n\t\t\treturn(res.scaled)\n\t}\n\t###########\n\tggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")], \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n\ttt <- sig.score(x=ggi.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)\n\tmyprobe <- tt$probe\n\tmymapping <- tt$mapping\n\tres <- tt$score\n\n\tif(!missing(hg)) {\n\t\tif(length(hg) != nrow(data)) { stop(\"hg must have the same length nrow(data)!\") }\n\t\tmhg1 <- mean(res[hg == 1], na.rm=TRUE)\n\t\tmhg3 <- mean(res[hg == 3], na.rm=TRUE)\n\t\tmm <- mhg1 + (mhg3 -  mhg1) / 2\n\t\tres.scaled <- ((res - mm) / (mhg3 - mhg1)) * 2\n\t\tres <- list(\"score\"=res.scaled, \"risk\"=ifelse(res.scaled >= 0, 1, 0), \"mapping\"=mymapping, \"probe\"=myprobe)\n\t} else {\n\t\triskt <- rep(NA, length(res))\n\t\tnames(riskt) <- names(res)\n\t\tres <- list(\"score\"=res, \"risk\"=riskt, \"mapping\"=mymapping, \"probe\"=myprobe)\n\t}\n\n\treturn (res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'ggi' function and what are its main inputs and outputs?",
        "answer": "The 'ggi' function computes the Gene expression Grade Index (GGI) for breast cancer patients. Its main inputs are a matrix of gene expressions ('data'), gene annotations ('annot'), and optionally the histological grade status ('hg'). The function returns a list containing the continuous signature scores, binary risk classification (high or low risk), the mapping used (if applicable), and the probe-to-gene correspondence (if mapping is performed)."
      },
      {
        "question": "How does the function handle the scaling of raw GGI scores when histological grade information is provided?",
        "answer": "When histological grade information ('hg') is provided, the function scales the raw GGI scores using the 'scale.raw.ggi' internal function. It calculates the mean GGI scores for grade 1 and grade 3 tumors, then scales the scores such that the midpoint between these means becomes 0, and the distance between them becomes 4 (ranging from -2 to 2). The scaled scores are then used to classify patients as high risk (>= 0) or low risk (< 0)."
      },
      {
        "question": "What is the significance of the 'do.mapping' parameter in the 'ggi' function, and how does it affect the function's behavior?",
        "answer": "The 'do.mapping' parameter determines whether the function should perform mapping through Entrez Gene IDs. When set to TRUE, the function handles ambiguities by keeping the most variant probe for each gene. This mapping process helps to standardize the gene identifiers used in the analysis, ensuring that the correct probes are associated with each gene in the GGI signature. The mapping results are included in the function's output, allowing users to see which probes were selected for each gene."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ggi <- function(data, annot, do.mapping=FALSE, mapping, hg, verbose=FALSE) {\n  ggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")], \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n  tt <- sig.score(x=ggi.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)\n  myprobe <- tt$probe\n  mymapping <- tt$mapping\n  res <- tt$score\n\n  if(!missing(hg)) {\n    if(length(hg) != nrow(data)) { stop(\"hg must have the same length nrow(data)!\") }\n    # Complete the code here to calculate scaled scores and risk\n  } else {\n    # Complete the code here for the case when hg is missing\n  }\n\n  return (res)\n}",
        "complete": "ggi <- function(data, annot, do.mapping=FALSE, mapping, hg, verbose=FALSE) {\n  ggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")], \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n  tt <- sig.score(x=ggi.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)\n  myprobe <- tt$probe\n  mymapping <- tt$mapping\n  res <- tt$score\n\n  if(!missing(hg)) {\n    if(length(hg) != nrow(data)) { stop(\"hg must have the same length nrow(data)!\") }\n    mhg1 <- mean(res[hg == 1], na.rm=TRUE)\n    mhg3 <- mean(res[hg == 3], na.rm=TRUE)\n    mm <- mhg1 + (mhg3 -  mhg1) / 2\n    res.scaled <- ((res - mm) / (mhg3 - mhg1)) * 2\n    res <- list(\"score\"=res.scaled, \"risk\"=ifelse(res.scaled >= 0, 1, 0), \"mapping\"=mymapping, \"probe\"=myprobe)\n  } else {\n    riskt <- rep(NA, length(res))\n    names(riskt) <- names(res)\n    res <- list(\"score\"=res, \"risk\"=riskt, \"mapping\"=mymapping, \"probe\"=myprobe)\n  }\n\n  return (res)\n}"
      },
      {
        "partial": "scale.raw.ggi <- function(ggi, hg) {\n  if(length(hg) != length(ggi)) { stop(\"bad length of hg!\") }\n  # Complete the function to scale the raw GGI scores\n}",
        "complete": "scale.raw.ggi <- function(ggi, hg) {\n  if(length(hg) != length(ggi)) { stop(\"bad length of hg!\") }\n  mhg1 <- mean(ggi[hg == 1], na.rm=TRUE)\n  mhg3 <- mean(ggi[hg == 3], na.rm=TRUE)\n  mm <- mhg1 + (mhg3 -  mhg1) / 2\n  res.scaled <- ((ggi - mm) / (mhg3 - mhg1)) * 2\n  return(res.scaled)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/gene70.R",
    "language": "R",
    "content": "#' @title Function to compute the 70 genes prognosis profile (GENE70) as published by\n#' van't Veer et al. 2002\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene expression\n#'   values following the algorithm used for the 70 genes prognosis profile (GENE70) as\n#'   published by van't Veer et al. 2002.\n#'\n#' @usage\n#' gene70(data, annot, do.mapping = FALSE, mapping,\n#'   std = c(\"none\", \"scale\", \"robust\"), verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in case\n#'   of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the mapping\n#'   such that the probes are not selected based on their variance.\n#' @param std Standardization of gene expressions: scale for traditional standardization based\n#'   on mean and standard deviation, robust for standardization based on the 0.025 and\n#'   0.975 quantiles, none to keep gene expressions unchanged.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#'\n#' @return\n#' A list with items:\n#' - score Continuous signature scores\n#' - risk Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping Mapping used if necessary.\n#' - probe If mapping is performed, this matrix contains the correspondence between\n#' the gene list (aka signature) and gene expression data\n#'\n#' @references\n#' L. J. van't Veer and H. Dai and M. J. van de Vijver and Y. D. He and A. A. Hart and\n#'   M. Mao and H. L. Peterse and K. van der Kooy and M. J. Marton and A. T. Witteveen and\n#'   G. J. Schreiber and R. M. Kerkhiven and C. Roberts and P. S. Linsley and R. Bernards\n#'   and S. H. Friend (2002) \"Gene Expression Profiling Predicts Clinical Outcome of Breast\n#'   Cancer\", Nature, 415:530\u2013536.\n#'\n#' @seealso nkis\n#'\n#' @examples\n#' # load GENE70 signature\n#' data(sig.gene70)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' rs.nkis <- gene70(data=data.nkis)\n#' table(rs.nkis$risk)\n#' # note that the discrepancies compared to the original publication\n#' # are closed to the official cutoff, raising doubts on its exact value.\n#' # computation of the signature scores on a different microarray platform\n#' # load VDX dataset\n#' data(vdxs)\n#' # compute relapse score\n#' rs.vdxs <- gene70(data=data.vdxs, annot=annot.vdxs, do.mapping=TRUE)\n#' table(rs.vdxs$risk)\n#'\n#' @md\n#' @export\n#' @name gene70\ngene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n\tif (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n\t\n\tstd <- match.arg(std)\n\tgt <- nrow(sig.gene70)\n\tif(do.mapping) {\n\t\tgid1 <- as.numeric(as.character(sig.gene70[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid1) <- dimnames(sig.gene70)[[1]]\n\t\tgid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tgm <- length(rr$geneid2)\n\t\tif(is.na(rr$geneid1[1])) {\n\t\t\tgm <- 0\n\t\t\t#no gene ids in common\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tgf <- c(\"mapped\"=0, \"total\"=gt)\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n\t\t}\n\t\tgid1 <- rr$geneid2\n\t\tgid2 <- rr$geneid1\n\t\tdata <- rr$data1\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tmyprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n\t\tsig2 <- sig.gene70[names(gid1), , drop=FALSE]\n\t\t## change the names of probes in the data\n\t\tdimnames(data)[[2]] <- names(gid2) <- names(gid1)\n\t} else {\n\t\tdata <- data[ , intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n\t\tsig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n\t\tgm <- nrow(sig2)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tmyprobe <- NA\n\t}\n\n\tif(verbose && gm != gt) { message(sprintf(\"%i/%i probes are used to compute the score\", gm, gt)) }\n\n\t## scaling\n\tswitch(std,\n\t\"scale\"={\n\t\tdata <- scale(data, center=TRUE, scale=TRUE)\n\t\tif(verbose) { message(\"standardization of the gene expressions\") }\n\t},\n\t\"robust\"={\n\t\tdata <- apply(data, 2, function(x) { return((rescale(x, q=0.05, na.rm=TRUE) - 0.5) * 2) })\n\t\tif(verbose) { message(\"robust standardization of the gene expressions\") }\n\t},\n\t\"none\"={ if(verbose) { message(\"no standardization of the gene expressions\") } })\n\n\tscore <- apply(X=data, MARGIN=1, FUN=function (x, y, method, use) {\n\t  rr <- NA\n    if (sum(complete.cases(x, y)) > 3) {\n      rr <- cor(x=x, y=y, method=method, use=use)\n    }\n    return (rr)\n\t}, y=sig2[, \"average.good.prognosis.profile\"], method=\"spearman\", use=\"complete.obs\")\n\tscore <- -score\n\tofficial.cutoff <- -0.3\n\t## cutoff leaving 59% of patients in the poor prognosis group in the original dataset\n\trisk <- ifelse(score >= official.cutoff, 1, 0)\n\n\tnames(score) <- names(risk) <- dimnames(data)[[1]]\n\n\treturn(list(\"score\"=score, \"risk\"=risk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `gene70` function and what algorithm does it implement?",
        "answer": "The `gene70` function computes signature scores and risk classifications from gene expression values, implementing the algorithm used for the 70 genes prognosis profile (GENE70) as published by van't Veer et al. 2002. It's primarily used for predicting clinical outcomes in breast cancer based on gene expression data."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if no gene IDs are in common?",
        "answer": "The function can perform gene mapping if `do.mapping=TRUE`. It uses Entrez Gene IDs to map between the signature genes and the input data. If no gene IDs are in common, the function returns NA for all scores and risks, sets the 'mapped' count to 0, and the 'probe' to NA. This is handled in the code block starting with `if(is.na(rr$geneid1[1])) {`."
      },
      {
        "question": "How is the final risk classification determined in the `gene70` function?",
        "answer": "The risk classification is determined based on the computed score. The function calculates a correlation score using Spearman's method, then inverts it (`score <- -score`). The risk is then classified as high (1) if the score is greater than or equal to the official cutoff of -0.3, and low (0) otherwise. This is implemented in the lines: `official.cutoff <- -0.3` and `risk <- ifelse(score >= official.cutoff, 1, 0)`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig.gene70[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig.gene70)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    if(is.na(rr$geneid1[1])) {\n      gm <- 0\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=gt)\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    sig2 <- sig.gene70[names(gid1), , drop=FALSE]\n    dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n  } else {\n    data <- data[ , intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n    sig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n    gm <- nrow(sig2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- NA\n  }\n\n  if(verbose && gm != gt) { message(sprintf(\"%i/%i probes are used to compute the score\", gm, gt)) }\n\n  # Complete the function by implementing the scaling and score calculation\n}",
        "complete": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig.gene70[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig.gene70)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    if(is.na(rr$geneid1[1])) {\n      gm <- 0\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=gt)\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    sig2 <- sig.gene70[names(gid1), , drop=FALSE]\n    dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n  } else {\n    data <- data[ , intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n    sig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n    gm <- nrow(sig2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    myprobe <- NA\n  }\n\n  if(verbose && gm != gt) { message(sprintf(\"%i/%i probes are used to compute the score\", gm, gt)) }\n\n  switch(std,\n    \"scale\"={ data <- scale(data, center=TRUE, scale=TRUE) },\n    \"robust\"={ data <- apply(data, 2, function(x) { (rescale(x, q=0.05, na.rm=TRUE) - 0.5) * 2 }) },\n    \"none\"={ if(verbose) message(\"no standardization of the gene expressions\") })\n\n  score <- apply(data, 1, function(x, y) {\n    if (sum(complete.cases(x, y)) > 3) -cor(x, y, method=\"spearman\", use=\"complete.obs\") else NA\n  }, y=sig2[, \"average.good.prognosis.profile\"])\n\n  risk <- ifelse(score >= -0.3, 1, 0)\n  names(score) <- names(risk) <- dimnames(data)[[1]]\n\n  list(\"score\"=score, \"risk\"=risk, \"mapping\"=mymapping, \"probe\"=myprobe)\n}"
      },
      {
        "partial": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  \n  # Implement the mapping logic here\n  \n  # Implement the scaling logic here\n  \n  # Implement the score calculation and risk classification here\n  \n  # Return the results\n}",
        "complete": "gene70 <- function(data, annot, do.mapping=FALSE, mapping,\n                   std=c(\"none\", \"scale\", \"robust\"), verbose=FALSE) {\n\n  if (!exists('sig.gene70')) data(sig.gene70, envir=environment())\n  \n  std <- match.arg(std)\n  gt <- nrow(sig.gene70)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig.gene70[, \"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig.gene70)[[1]]\n    gid2 <- as.numeric(as.character(annot[, \"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    gid1 <- gid1[!is.na(gid1) & !duplicated(gid1)]\n    \n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    if(is.na(rr$geneid1[1])) {\n      return(list(\"score\"=rep(NA, nrow(data)), \"risk\"=rep(NA, nrow(data)), \n                  \"mapping\"=c(\"mapped\"=0, \"total\"=gt), \"probe\"=NA))\n    }\n    data <- rr$data1\n    mymapping <- c(\"mapped\"=length(rr$geneid2), \"total\"=gt)\n    myprobe <- cbind(\"probe\"=names(rr$geneid2), \"EntrezGene.ID\"=rr$geneid2, \"new.probe\"=names(rr$geneid1))\n    sig2 <- sig.gene70[names(rr$geneid2), , drop=FALSE]\n    dimnames(data)[[2]] <- names(rr$geneid1) <- names(rr$geneid2)\n  } else {\n    data <- data[, intersect(dimnames(sig.gene70)[[1]], dimnames(data)[[2]])]\n    sig2 <- sig.gene70[dimnames(data)[[2]], , drop=FALSE]\n    mymapping <- c(\"mapped\"=nrow(sig2), \"total\"=gt)\n    myprobe <- NA\n  }\n  \n  data <- switch(std,\n    \"scale\" = scale(data),\n    \"robust\" = apply(data, 2, function(x) (rescale(x, q=0.05, na.rm=TRUE) - 0.5) * 2),\n    data\n  )\n  \n  score <- -apply(data, 1, function(x) {\n    if(sum(complete.cases(x, sig2[, \"average.good.prognosis.profile\"])) > 3)\n      cor(x, sig2[, \"average.good.prognosis.profile\"], method=\"spearman\", use=\"complete.obs\")\n    else NA\n  })\n  \n  risk <- ifelse(score >= -0.3, 1, 0)\n  names(score) <- names(risk) <- rownames(data)\n  \n  list(\"score\"=score, \"risk\"=risk, \"mapping\"=mymapping, \"probe\"=myprobe)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeAUC.R",
    "language": "R",
    "content": "#' Computes the AUC for a Drug Dose Viability Curve\n#'\n#' Returns the AUC (Area Under the drug response Curve) given concentration and viability as input, normalized by the concentration\n#' range of the experiment. The area returned is the response (1-Viablility) area, i.e. area under the curve when the response curve\n#' is plotted on a log10 concentration scale, with high AUC implying high sensitivity to the drug. The function can calculate both\n#' the area under a fitted Hill Curve to the data, and a trapz numeric integral of the actual data provided. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known.\n#'\n#' @examples\n#' dose <- c(\"0.0025\",\"0.008\",\"0.025\",\"0.08\",\"0.25\",\"0.8\",\"2.53\",\"8\")\n#' viability <- c(\"108.67\",\"111\",\"102.16\",\"100.27\",\"90\",\"87\",\"74\",\"57\")\n#' computeAUC(dose, viability)\n#'\n#'\n#' @param concentration `vector` is a vector of drug concentrations.\n#' @param viability `vector` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param Hill_fit `list or vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and returns AUC as a decimal. Otherwise, viability is interpreted as percent, and AUC is returned 0-100.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param area.type Should the area be computed using the actual data (\"Actual\"), or a fitted curve (\"Fitted\")\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return Numeric AUC value\n#' @export\n#' @importFrom caTools trapz\ncomputeAUC <- function (concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        area.type = c(\"Fitted\", \"Actual\"),\n                        verbose = TRUE\n) {\n\n    if (missing(concentration)){\n        stop(\"The concentration values to integrate over must always be provided.\")\n    }\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    } else {\n        area.type <- match.arg(area.type)\n    }\n    if (area.type == \"Fitted\" && missing(Hill_fit)) {\n        Hill_fit <- logLogisticRegression(concentration,\n                                          viability,\n                                          conc_as_log = conc_as_log,\n                                          viability_as_pct = viability_as_pct,\n                                          trunc = trunc,\n                                          verbose = verbose)\n        cleanData <- sanitizeInput(conc=concentration,\n                                   Hill_fit=Hill_fit,\n                                   conc_as_log = conc_as_log,\n                                   viability_as_pct = viability_as_pct,\n                                   trunc = trunc,\n                                  verbose = verbose)\n        pars <- cleanData[[\"Hill_fit\"]]\n        concentration <- cleanData[[\"log_conc\"]]\n    } else if (area.type == \"Fitted\" && !missing(Hill_fit)){\n        cleanData <- sanitizeInput(conc = concentration,\n                                   viability = viability,\n                                   Hill_fit = Hill_fit,\n                                   conc_as_log = conc_as_log,\n                                   viability_as_pct = viability_as_pct,\n                                   trunc = trunc,\n                                   verbose = verbose)\n        pars <- cleanData[[\"Hill_fit\"]]\n        concentration <- cleanData[[\"log_conc\"]]\n     } else if (area.type == \"Actual\" && !missing(viability)){\n        cleanData <- sanitizeInput(conc = concentration,\n                                   viability = viability,\n                                   conc_as_log = conc_as_log,\n                                   viability_as_pct = viability_as_pct,\n                                   trunc = trunc,\n                                   verbose = verbose)\n        concentration <- cleanData[[\"log_conc\"]]\n        viability <- cleanData[[\"viability\"]]\n    } else if (area.type == \"Actual\" && missing(viability)){\n        stop(\"To calculate the actual area using a trapezoid integral, the raw\n             viability values are needed!\")\n    }\n    if (length(concentration) < 2) {\n        return(NA)\n    }\n    a <- min(concentration)\n    b <- max(concentration)\n    if (area.type == \"Actual\") {\n        trapezoid.integral <- caTools::trapz(concentration, viability)\n        AUC <- 1 - trapezoid.integral / (b - a)\n    }\n    else {\n        if(pars[2]==1){\n            AUC <- 0\n      }else if(pars[1]==0){\n          AUC <- (1-pars[2])/2\n      } else {\n          AUC <- as.numeric((1 - pars[2]) / (pars[1] * (b - a)) *\n                              log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) /\n                                      (1 + (10 ^ (a - pars[3])) ^ pars[1])))\n      }\n    }\n    if(viability_as_pct){\n      AUC <- AUC*100\n    }\n    return(AUC)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC` function and what are its main input parameters?",
        "answer": "The `computeAUC` function computes the Area Under the Curve (AUC) for a Drug Dose Viability Curve. It calculates the response area (1-Viability) when plotted on a log10 concentration scale, with a higher AUC indicating higher drug sensitivity. The main input parameters are:\n1. `concentration`: a vector of drug concentrations\n2. `viability`: a vector of viability values corresponding to the concentrations\n3. `Hill_fit`: optional parameters of a Hill Slope (can be provided or calculated internally)\n4. `conc_as_log`: boolean indicating if concentration is given as log10 values\n5. `viability_as_pct`: boolean indicating if viability is given as a percentage"
      },
      {
        "question": "How does the function handle different types of area calculations, and what conditions determine which method is used?",
        "answer": "The function can calculate the AUC using two methods, controlled by the `area.type` parameter:\n1. 'Fitted': Uses a fitted Hill Curve. This is the default method if not specified. It requires either the `Hill_fit` parameters or calculates them using `logLogisticRegression` if not provided.\n2. 'Actual': Uses a trapezoidal numeric integral of the actual data. This requires both `concentration` and `viability` data.\n\nThe conditions determining the method are:\n- If `area.type` is 'Fitted' and `Hill_fit` is missing, it calculates the Hill parameters.\n- If `area.type` is 'Fitted' and `Hill_fit` is provided, it uses the given parameters.\n- If `area.type` is 'Actual' and `viability` is provided, it uses the trapezoidal method.\n- If `area.type` is 'Actual' and `viability` is missing, it throws an error."
      },
      {
        "question": "Explain the significance of the `sanitizeInput` function in this code and how it contributes to the overall functionality of `computeAUC`.",
        "answer": "The `sanitizeInput` function plays a crucial role in data preprocessing for the `computeAUC` function:\n1. It ensures that input data is in the correct format and scale for AUC calculations.\n2. It handles the conversion of concentration values to log scale if needed (`conc_as_log` parameter).\n3. It processes viability data, potentially converting from percentage to decimal form (`viability_as_pct` parameter).\n4. It may truncate viability values to be between 0 and 1 if the `trunc` parameter is set to true.\n5. It standardizes the Hill fit parameters if provided.\n\nBy using `sanitizeInput`, `computeAUC` ensures that regardless of the input format, the core calculation functions work with consistent, properly formatted data. This improves the function's robustness and reduces the chance of errors due to inconsistent input formats."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC <- function (concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        area.type = c(\"Fitted\", \"Actual\"),\n                        verbose = TRUE\n) {\n    if (missing(concentration)){\n        stop(\"The concentration values to integrate over must always be provided.\")\n    }\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    } else {\n        area.type <- match.arg(area.type)\n    }\n    # ... (rest of the function implementation)\n}",
        "complete": "computeAUC <- function (concentration,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        area.type = c(\"Fitted\", \"Actual\"),\n                        verbose = TRUE\n) {\n    if (missing(concentration)){\n        stop(\"The concentration values to integrate over must always be provided.\")\n    }\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    } else {\n        area.type <- match.arg(area.type)\n    }\n    if (area.type == \"Fitted\" && missing(Hill_fit)) {\n        Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log, viability_as_pct, trunc, verbose)\n        cleanData <- sanitizeInput(conc=concentration, Hill_fit=Hill_fit, conc_as_log, viability_as_pct, trunc, verbose)\n        pars <- cleanData[\"Hill_fit\"]\n        concentration <- cleanData[\"log_conc\"]\n    } else if (area.type == \"Fitted\" && !missing(Hill_fit)){\n        cleanData <- sanitizeInput(conc=concentration, viability=viability, Hill_fit=Hill_fit, conc_as_log, viability_as_pct, trunc, verbose)\n        pars <- cleanData[\"Hill_fit\"]\n        concentration <- cleanData[\"log_conc\"]\n    } else if (area.type == \"Actual\" && !missing(viability)){\n        cleanData <- sanitizeInput(conc=concentration, viability=viability, conc_as_log, viability_as_pct, trunc, verbose)\n        concentration <- cleanData[\"log_conc\"]\n        viability <- cleanData[\"viability\"]\n    } else if (area.type == \"Actual\" && missing(viability)){\n        stop(\"To calculate the actual area using a trapezoid integral, the raw viability values are needed!\")\n    }\n    if (length(concentration) < 2) return(NA)\n    a <- min(concentration)\n    b <- max(concentration)\n    AUC <- if (area.type == \"Actual\") {\n        1 - caTools::trapz(concentration, viability) / (b - a)\n    } else {\n        if (pars[2] == 1) 0\n        else if (pars[1] == 0) (1 - pars[2]) / 2\n        else (1 - pars[2]) / (pars[1] * (b - a)) * log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) / (1 + (10 ^ (a - pars[3])) ^ pars[1]))\n    }\n    if (viability_as_pct) AUC <- AUC * 100\n    return(AUC)\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n    # ... (implementation details)\n}",
        "complete": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n    if (!conc_as_log) {\n        log_conc <- log10(as.numeric(conc))\n    } else {\n        log_conc <- as.numeric(conc)\n    }\n    if (!missing(viability)) {\n        viability <- as.numeric(viability)\n        if (viability_as_pct) {\n            viability <- viability / 100\n        }\n        if (trunc) {\n            viability[viability > 1] <- 1\n            viability[viability < 0] <- 0\n        }\n    }\n    if (!missing(Hill_fit)) {\n        if (viability_as_pct) {\n            Hill_fit[2] <- Hill_fit[2] / 100\n        }\n        if (!conc_as_log) {\n            Hill_fit[3] <- log10(Hill_fit[3])\n        }\n    }\n    return(list(log_conc = log_conc, viability = viability, Hill_fit = Hill_fit))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(ToxicoGx)\ntest_check(\"ToxicoGx\")",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'library()' function in R, and why are 'testthat' and 'ToxicoGx' being loaded in this code snippet?",
        "answer": "The 'library()' function in R is used to load and attach add-on packages. In this code snippet, 'testthat' and 'ToxicoGx' are being loaded. 'testthat' is a popular testing framework for R, while 'ToxicoGx' is likely a package related to toxicogenomics. These packages are being loaded to prepare the environment for running tests on the 'ToxicoGx' package."
      },
      {
        "question": "What does the 'test_check()' function do, and what argument is being passed to it in this code?",
        "answer": "The 'test_check()' function is part of the 'testthat' package and is used to run all the tests in a package. It automatically finds and executes all test files in the 'tests/testthat' directory of the package. In this code, the argument 'ToxicoGx' is being passed to 'test_check()', which means it will run all the tests for the 'ToxicoGx' package."
      },
      {
        "question": "How would you modify this code to run tests for a different package, and what considerations should be taken into account?",
        "answer": "To modify this code to run tests for a different package, you would need to change two things: 1) Replace 'library(ToxicoGx)' with 'library(YourPackageName)', where YourPackageName is the name of the package you want to test. 2) Change the argument in 'test_check('ToxicoGx')' to 'test_check('YourPackageName')'. Considerations include ensuring that the new package is installed, has tests written using the 'testthat' framework, and that the tests are located in the correct directory (tests/testthat) within the package structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(testthat)\nlibrary(ToxicoGx)\ntest_",
        "complete": "library(testthat)\nlibrary(ToxicoGx)\ntest_check(\"ToxicoGx\")"
      },
      {
        "partial": "library(testthat)\n# Load ToxicoGx package\n# Run tests for ToxicoGx",
        "complete": "library(testthat)\nlibrary(ToxicoGx)\ntest_check(\"ToxicoGx\")"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/paramErrorChecker.R",
    "language": "R",
    "content": "# An Error Checker for Function Parameters\n#\n# This function will take in the params of a function as well as its name.\n#   Error checking will then be conducted on each of it's parameter arguments\n#   to ensure they meet the input requirements for that function. Descriptive\n#   errors are returned if the the arguements do not meet the criteria for that\n#   function.\n#\n# @param funName [character] A string of the function name. This argument is\n#   used to match the correct parameter checking conditions with each function.\n# @param ... [pairlist] A list of all parameters passed as arguements to the\n#   function \"funName\".\n#\n# @return Returns nothing, this function works by side effects only\n#\n#' @keywords internal\nparamErrorChecker <- function(funName, tSet, ...) {\n\n  # Intersection of all function's parameter tests\n  intersectParamChecks <- c(\n    \"tSetNotIs\", \"tSetGt1\", \"cell_linesNotChar\",\"cell_linesNotIn\",\n    \"drugsNotChar\", \"durationNotChar\"\n  )\n  intersectViabPlotParamChecks <- c(\n    \"tSetGt1\", \"tSetsNotIs\", \"viabilitiesNotMissing\", \"viabilitiesNotNum\"\n  )\n\n  # Matches the correct parameter constraints to each function name\n  paramChecks <-\n    switch(funName,\n           \"drugPerturbationSig\" =\n             c(intersectParamChecks, \"mDataTypeNotChar\", \"mDataTypeNotIn\",\n               \"mDataTypeGt1\", \"featuresLt2\", \"doseLt2\",\n               \"doseNotCtl\"\n             ),\n           \"summarizeMolecularProfiles\" =\n             c(intersectParamChecks, \"mDataTypeGt1\", \"mDataTypeNotChar\",\n               \"mDataTypeNotIn\",\n               \"summary.statNotChar\", \"summary.statNotIn\", \"summary.statGt1\",\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\"\n             ),\n           \"summarizeSensitivityProfiles\" =\n             c(intersectParamChecks,\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\",\n               \"drugNotInCellLine\", \"summary.statNotIn\", \"summaryStatNotChar\",\n               \"summary.statGt1\", \"sensitivty.measureGt1\",\n               \"sensitivity.measureNotChar\"\n              ),\n           \"drugTimeResponseCurve\" =\n             c(intersectViabPlotParamChecks, 'tSetsHaveViab',\n               'viabilitiesDiffLenDur', \"durationNotChar\", \"drugsGt2\",\n               \"cell_linesGt2\"\n               ),\n           \"subsetTo\" =\n             c(#\"returnValuesGt1\",\n               \"tSetGt1\", \"tSetNotIs\", \"cell_linesNotChar\", \"cell_linesNotIn\",\n               \"drugsNotChar\", \"drugsNotIn\", \"featuresNotChar\", \"featuresNotIn\"\n               ),\n    )\n\n  ## Handle missing values\n  if (missing(tSet)) {\n    tSet <- NULL\n  }\n\n  # Conducts the parameter checks based on function matched paramChecks\n  .checkParamsForErrors(funName = funName, tSet = tSet,\n                        paramChecks = paramChecks, ...)\n\n}\n\n#' @keywords internal\n.checkParamsForErrors <- function(tSet, funName, paramChecks, ...) {\n\n  # Initialize variable names in the local environment\n  cell_lines <- concentrations <- dose <- drugs <- duration <- features <-\n    mDataType <- summary.stat <- sensitivity.measure <- tSets <- viabilities <- NULL\n\n  # Extract named arguments into local environment\n  argList <- list(...)\n  for (idx in seq_len(length(argList))) { ## TODO:: Make this work with seq_along()\n    assign(names(argList)[idx], argList[[idx]])\n  }\n\n  if (is.null(mDataType) & !is.null(tSet)) {mDataType <- names(molecularProfilesSlot(tSet))}\n\n  ## TODO:: Write a cases function that lazily evaluates LHS to replace this switch\n  ## TODO:: Benmark for loop vs apply statement for this code\n  # Runs the parameter checks specific for the given funName\n  for (check in paramChecks) {\n    invisible(\n      switch(\n        check,\n        # tSet checks\n        \"tSetNotIs\" = {if (!is(tSet, \"ToxicoSet\")) { stop(paste0(name(tSet), \" is a \", class(tSet), \", not a ToxicoSet.\")) }},\n        \"tSetGt1\" = {if (length(tSet) > 1) { stop(\"You may only pass in one tSet.\") }},\n        \"tSetHasViab\" = {if (length(ToxicoGx::sensitivityInfo(tSet) < 1)) { stop(paste0(names(tSet), ' does not contain sensitivity or perturbation data!')) }},\n        # tSets checks\n        ## TODO:: Generalize error checking to multiple tSets for this check\n        \"tSetsHaveViab\" = {if (!is.null(tSets)) {lapply(tSets, function(tSet) { if (length(ToxicoGx::sensitivityInfo(tSet)) < 1) {stop(\"The \", paste0(names(tSet), \" tSet has no viability data!\"))} })}},\n        \"tSetsNotIs\" = {if (!is.null(tSets)) { if (!all(vapply(tSets, function(tSet) { is(tSet, \"ToxicoSet\") }, FUN.VALUE = logical(1) ))) { stop(\"One or more arguments to tSets parameter is not a 'ToxicoSet'.\")}}},\n        # mDataType checks\n        \"mDataTypeGt1\" = {if (length(unlist(mDataType)) > 1) { stop(\"Please only pass in one molecular data type.\") }},\n        \"mDataTypeNotChar\" = {if (!is.character(mDataType)) { stop(\"mDataType must be a string.\") }},\n        \"mDataTypeNotIn\" = {if (!(mDataType %in% mDataNames(tSet))) { stop(paste0(\"The molecular data type(s) \", paste(mDataType[which(!(mDataType %in% mDataNames(tSet)))], collapse = \", \" ), \" is/are not present in \", name(tSet), \".\")) }},\n        # cell_lines checks\n        \"cell_linesNotChar\" = {if (!is.character(unlist(cell_lines))) { stop(\"cell_lines parameter must contain strings.\") }},\n        \"cell_linesNotIn\" = {if (all(!(cell_lines %in% sampleNames(tSet)))) { stop(paste0(\"The cell line(s) \", paste(cell_lines[which(!(cell_lines %in% sampleNames(tSet)))], collapse = \", \"), \" is/are not present in \", name(tSet), \"with the specified parameters.\")) }},\n        \"cell_linesG\" = {if (length(cell_lines) > 2) {stop(\"This plot currently only supports two cell lines at once!\")}},\n        # drugs checks\n        \"drugsNotChar\" = {if (!is.character(unlist(drugs))) { stop(\"drugs parameter must contain strings.\") }},\n        \"drugsNotIn\" = {if (all(!(drugs %in% treatmentNames(tSet)))) { stop(paste0(\"The drug(s) \", paste(drugs[which(!(drugs %in% treatmentNames(tSet)))], collapse = \", \"), \" is/are not present in \", name(tSet), \".\")) }},\n        ## TODO:: Test this works correctly once an additional cell line is added to a tSet\n        \"drugsIntersectsCellLine\" = {if (length(drugs) == 1) { if (!(drugs %in% subset(sensitivityInfo(tSet), sampleid == cell_lines, select = treatmentid)))  {stop(paste0(\"The drug \", drugs, \"is not present for cell line(s)\", paste0(cell_lines, collapse = \", \")), \"!\") }}},\n        \"drugsGt2\" = {if (length(drugs) > 2) { stop(\"This plot only supports two drugs at a time!\")  }},\n        # features checks\n        \"featuresLt2\" = {if (length(fNames(tSet, mDataType)) < 2) { stop(\"Must include at least 2 features to calculate summary statistics\") }},\n        \"featuresNotChar\" = {if (!is.character(unlist(features))) { stop(\"features parameter contain strings.\") }},\n        \"featuresNotIn\" = {if (all(!(fNames(tSet, mDataType[1]) %in% features))) { stop(paste0(\"The feature(s) \", paste(features[which(!(features %in% fNames(tSet, mDataType[1])))], collapse = \", \"), \" is/are not present in \", name(tSet), \".\")) }},\n        # duration checks\n        \"durationMissing\" = {if (is.null(duration)) { stop(paste(funName, \"requires an argument be passed to the duration parameter!\" )) }},\n        \"durationGt1\" = {if (length(duration) > 1) { stop(paste(funName, \"only accepts one duration at a time!\" )) }},\n        \"durationNotChar\" = {if (!is.character(unlist(duration))) { stop(\"duration parameter must contain strings.\") }},\n        \"durationNotIn\" = {if (all(!(duration %in% ToxicoGx::sensitivityInfo(tSet)$duration_h))) { stop(paste0(\"The duration(s) \", paste(duration[which(!(duration %in% ToxicoGx::sensitivityInfo(tSet)$duration_h))]), collapse = \", \", \"is/are not present in \", name(tSet), \".\")) }},\n        # dose checks\n        \"doseLt2\" = {if (length(dose) < 2) { stop(\"To fit a linear model we need at least two dose levels, please add anothor to the dose argument in the function call.\") }},\n        \"doseNoCtl\" = {if (!(\"Control\" %in% dose)) { stop(\"You should not calculate summary statistics without including a control! Please add 'Control' to the dose argument vector.\") }},\n        \"doseNotChar\" = {if (!is.character(dose)) { stop(\"Dose must be a string or character vector.\") }},\n        \"doseNotIn\" = {if (all(!(dose %in% phenoInfo(tSet, mDataType)$dose_level))) { stop(paste0(\"The dose level(s) \", dose, \" is/are not present in \", name(tSet), \" with the specified parameters.\")) }},\n        # summary.stat\n        \"summary.statNotChar\" = {if (!is.character(summary.stat)) { stop(\"The parameter summary.stat must be a string or character vector.\") }},\n        \"summary.statGt1\" = {if (length(summary.stat) > 1)  {stop(\"Please pick only one summary statistic\") }},\n        \"summary.statNotIn\" = {if (!(summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) { stop(paste0(\"The the statistic \", summary.stat, \" is not implemented in this package\")) }},\n        # sensitivity.measure\n        \"sensitivity.measureNotChar\" = {if (!is.character(sensitivity.measure)) { stop(\"The parameter sensitivty.measure must be a string or character vector.\") }},\n        \"sensitivity.measureGt1\" = {if (length(sensitivity.measure) > 1)  {stop(\"Please pick only one sensitivity measure\") }},\n        \"sensitivity.measureNotIn\" = {if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(tSet)), \"max_conc\"))) {\n          stop(sprintf(\"Invalid sensitivity measure for %s, choose among: %s\", name(tSet), paste0(colnames(sensitivityProfiles(tSet)), collapse = \", \")))}},\n        # viabilties checks\n        \"viabilitiesNotNum\" = {if (!is.null(viabilities)) { if (!all(vapply(viabilities, function(viability) { is(viability, \"numeric\") }, FUN.VALUE = logical(1) ))) { stop(\"Viability values must be numeric.\") }}},\n        \"viabilitiesNotMissing\" = {if (!is.null(concentrations)) { if (is.null(viabilities)) { stop(\"If you pass in an argument for concentrations, you must also pass in an argument for viabilities.\")}}},\n        \"viabilitiesDiffLenConc\" = {\n          if (!is.null(concentrations) && !is.null(viabilities)) {\n            if (length(viabilities) != length(concentrations)) {\n              stop(paste0(ifelse(is(viabilities, \"list\"), \"List\", \"Vector\"), \" of viabilities is \", length(viabilities), \"long, but concentrations is \", length(concentrations), \"long.\")) }}},\n        \"viabilitiesDiffLenDur\" = {\n          if (!is.null(duration) && !is.null(viabilities)) {\n            if (length(viabilities) != length(duration)) {\n              stop(paste0(ifelse(is(viabilities, \"list\"), \"List\", \"Vector\"), \" of viabilities is \", length(viabilities), \"long, but concentrations is \", length(duration), \"long.\")) }}},\n        # concentrations checks\n        \"concentrationsNotNum\" = {\n          if (!is.null(concentrations)) {\n            if (!all(vapply(concentrations, function(concentration) {\n              is(concentration, \"numeric\") }, FUN.VALUE = logical(1) ))) {\n              stop(\"Concentration values must be numeric.\") }\n          }},\n      )\n    )\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `paramErrorChecker` function and how does it handle different function names?",
        "answer": "The `paramErrorChecker` function is designed to perform error checking on function parameters. It takes the function name and its parameters as input, then uses a switch statement to match the correct parameter constraints to each function name. It creates a list of parameter checks specific to the given function and passes them to the `.checkParamsForErrors` function for actual error checking."
      },
      {
        "question": "How does the `.checkParamsForErrors` function handle the various parameter checks?",
        "answer": "The `.checkParamsForErrors` function uses a for loop to iterate through the list of parameter checks. For each check, it uses a switch statement to execute the corresponding error checking code. If an error condition is met, it stops execution and throws an error with a descriptive message. The function handles checks for various parameter types, including tSet, cell_lines, drugs, features, duration, dose, and others."
      },
      {
        "question": "What is the purpose of the `intersectParamChecks` and `intersectViabPlotParamChecks` variables in the `paramErrorChecker` function?",
        "answer": "The `intersectParamChecks` and `intersectViabPlotParamChecks` variables contain lists of common parameter checks that are used across multiple functions. These lists are combined with function-specific checks in the switch statement to create a comprehensive set of parameter checks for each function. This approach reduces code duplication and ensures consistent error checking across different functions that share common parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "paramErrorChecker <- function(funName, tSet, ...) {\n  intersectParamChecks <- c(\n    \"tSetNotIs\", \"tSetGt1\", \"cell_linesNotChar\",\"cell_linesNotIn\",\n    \"drugsNotChar\", \"durationNotChar\"\n  )\n  intersectViabPlotParamChecks <- c(\n    \"tSetGt1\", \"tSetsNotIs\", \"viabilitiesNotMissing\", \"viabilitiesNotNum\"\n  )\n\n  paramChecks <-\n    switch(funName,\n           \"drugPerturbationSig\" =\n             c(intersectParamChecks, \"mDataTypeNotChar\", \"mDataTypeNotIn\",\n               \"mDataTypeGt1\", \"featuresLt2\", \"doseLt2\",\n               \"doseNotCtl\"\n             ),\n           \"summarizeMolecularProfiles\" =\n             c(intersectParamChecks, \"mDataTypeGt1\", \"mDataTypeNotChar\",\n               \"mDataTypeNotIn\",\n               \"summary.statNotChar\", \"summary.statNotIn\", \"summary.statGt1\",\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\"\n             ),\n           # ... (other cases)\n    )\n\n  if (missing(tSet)) {\n    tSet <- NULL\n  }\n\n  # TODO: Implement .checkParamsForErrors function\n}",
        "complete": "paramErrorChecker <- function(funName, tSet, ...) {\n  intersectParamChecks <- c(\n    \"tSetNotIs\", \"tSetGt1\", \"cell_linesNotChar\",\"cell_linesNotIn\",\n    \"drugsNotChar\", \"durationNotChar\"\n  )\n  intersectViabPlotParamChecks <- c(\n    \"tSetGt1\", \"tSetsNotIs\", \"viabilitiesNotMissing\", \"viabilitiesNotNum\"\n  )\n\n  paramChecks <-\n    switch(funName,\n           \"drugPerturbationSig\" =\n             c(intersectParamChecks, \"mDataTypeNotChar\", \"mDataTypeNotIn\",\n               \"mDataTypeGt1\", \"featuresLt2\", \"doseLt2\",\n               \"doseNotCtl\"\n             ),\n           \"summarizeMolecularProfiles\" =\n             c(intersectParamChecks, \"mDataTypeGt1\", \"mDataTypeNotChar\",\n               \"mDataTypeNotIn\",\n               \"summary.statNotChar\", \"summary.statNotIn\", \"summary.statGt1\",\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\"\n             ),\n           \"summarizeSensitivityProfiles\" =\n             c(intersectParamChecks,\n               \"durationMissing\", \"durationNotIn\", \"cell_linesNotIn\",\n               \"drugNotInCellLine\", \"summary.statNotIn\", \"summaryStatNotChar\",\n               \"summary.statGt1\", \"sensitivty.measureGt1\",\n               \"sensitivity.measureNotChar\"\n              ),\n           \"drugTimeResponseCurve\" =\n             c(intersectViabPlotParamChecks, 'tSetsHaveViab',\n               'viabilitiesDiffLenDur', \"durationNotChar\", \"drugsGt2\",\n               \"cell_linesGt2\"\n               ),\n           \"subsetTo\" =\n             c(\"tSetGt1\", \"tSetNotIs\", \"cell_linesNotChar\", \"cell_linesNotIn\",\n               \"drugsNotChar\", \"drugsNotIn\", \"featuresNotChar\", \"featuresNotIn\"\n               )\n    )\n\n  if (missing(tSet)) {\n    tSet <- NULL\n  }\n\n  .checkParamsForErrors(funName = funName, tSet = tSet,\n                        paramChecks = paramChecks, ...)\n}"
      },
      {
        "partial": ".checkParamsForErrors <- function(tSet, funName, paramChecks, ...) {\n  # Initialize variables\n  cell_lines <- concentrations <- dose <- drugs <- duration <- features <-\n    mDataType <- summary.stat <- sensitivity.measure <- tSets <- viabilities <- NULL\n\n  # Extract named arguments\n  argList <- list(...)\n  for (idx in seq_len(length(argList))) {\n    assign(names(argList)[idx], argList[[idx]])\n  }\n\n  if (is.null(mDataType) & !is.null(tSet)) {\n    mDataType <- names(molecularProfilesSlot(tSet))\n  }\n\n  # TODO: Implement parameter checks\n}",
        "complete": ".checkParamsForErrors <- function(tSet, funName, paramChecks, ...) {\n  # Initialize variables\n  cell_lines <- concentrations <- dose <- drugs <- duration <- features <-\n    mDataType <- summary.stat <- sensitivity.measure <- tSets <- viabilities <- NULL\n\n  # Extract named arguments\n  argList <- list(...)\n  for (idx in seq_len(length(argList))) {\n    assign(names(argList)[idx], argList[[idx]])\n  }\n\n  if (is.null(mDataType) & !is.null(tSet)) {\n    mDataType <- names(molecularProfilesSlot(tSet))\n  }\n\n  for (check in paramChecks) {\n    switch(\n      check,\n      \"tSetNotIs\" = if (!is(tSet, \"ToxicoSet\")) stop(paste0(name(tSet), \" is a \", class(tSet), \", not a ToxicoSet.\")),\n      \"tSetGt1\" = if (length(tSet) > 1) stop(\"You may only pass in one tSet.\"),\n      \"cell_linesNotChar\" = if (!is.character(unlist(cell_lines))) stop(\"cell_lines parameter must contain strings.\"),\n      \"cell_linesNotIn\" = if (all(!(cell_lines %in% sampleNames(tSet)))) stop(paste0(\"The cell line(s) \", paste(cell_lines[which(!(cell_lines %in% sampleNames(tSet)))], collapse = \", \"), \" is/are not present in \", name(tSet), \"with the specified parameters.\")),\n      \"drugsNotChar\" = if (!is.character(unlist(drugs))) stop(\"drugs parameter must contain strings.\"),\n      \"durationNotChar\" = if (!is.character(unlist(duration))) stop(\"duration parameter must contain strings.\"),\n      \"mDataTypeNotChar\" = if (!is.character(mDataType)) stop(\"mDataType must be a string.\"),\n      \"mDataTypeNotIn\" = if (!(mDataType %in% mDataNames(tSet))) stop(paste0(\"The molecular data type(s) \", paste(mDataType[which(!(mDataType %in% mDataNames(tSet)))], collapse = \", \" ), \" is/are not present in \", name(tSet), \".\")),\n      \"mDataTypeGt1\" = if (length(unlist(mDataType)) > 1) stop(\"Please only pass in one molecular data type.\"),\n      \"featuresLt2\" = if (length(fNames(tSet, mDataType)) < 2) stop(\"Must include at least 2 features to calculate summary statistics\"),\n      \"doseLt2\" = if (length(dose) < 2) stop(\"To fit a linear model we need at least two dose levels, please add another to the dose argument in the function call.\"),\n      \"doseNotCtl\" = if (!(\"Control\" %in% dose)) stop(\"You should not calculate summary statistics without including a control! Please add 'Control' to the dose argument vector.\")\n    )\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/src/foo_mrmr_surv.cpp",
    "language": "cpp",
    "content": "#include \"mrmr_surv.h\"\n\ndouble get_correlation(double data [],int namat[], int ind_x, int ind_y, int size){\n\t//compute correlation of two variables;\n\t//data: contains all data in a vector; variable-wise appended\n\t//ind_x: starting index of first variable in data\n\t//ind_y: starting index of second variable in data\n\t//size: number of samples for both variables\n\tdouble mean_data_x=0.0,mean_data_y=0.0;\n\tdouble correlation_nom=0.0,correlation_den_x=0.0,correlation_den_y=0.0;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif (namat[ind_x+i]==0 && namat[ind_y+i]==0 ) {\n\t\t\tmean_data_x+=data[ind_x+i];\n\t\t\tmean_data_y+=data[ind_y+i];\n\t\t}\n\t}\n\n\tmean_data_x=mean_data_x/size;\n\tmean_data_y=mean_data_y/size;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif(namat[ind_x+i]==0 && namat[ind_y+i]==0){\n\t\tcorrelation_nom+=(data[ind_x+i]-mean_data_x)*(data[ind_y+i]-mean_data_y);\n\t\tcorrelation_den_x+=(data[ind_x+i]-mean_data_x)*(data[ind_x+i]-mean_data_x);\n\t\tcorrelation_den_y+=(data[ind_y+i]-mean_data_y)*(data[ind_y+i]-mean_data_y);\n\t\t}\n\t}\n\treturn correlation_nom/(sqrt(correlation_den_x*correlation_den_y));\n}\n\n\nvoid build_mim_subset(double mim[],double data[], int namat [],int nvar,int nsample, int subset [],int size_subset){\n\t//compute mutual information matrix\n\t//mim:\t\t\tmatrix (stored as vector) in which the mi values will be stored\n\t//data:\t\t\tcontains all data in a vector; variable-wise appended\n\t//nvar:\t\t\tnumber of variables\n\t//nsample:\t\tnumber of samples in dataset\n\t//subset:\t\tindices of samples to be included in the bootstrapping data\n\t//size_subset:\tnumber of variables in the bootstrapped dataset\n\n\tdouble tmp;\n\tdouble *data_x;\n\tint *namat_x;\n\n\tnamat_x = (int*) R_alloc(nvar*size_subset, sizeof(int));\n\tdata_x = (double *) R_alloc(nvar*size_subset, sizeof(double));\n\n\tfor(unsigned int i=0; i< size_subset; ++i){\n\t\tfor(unsigned int j=0; j< nvar; ++j){\n\t\t\tdata_x[size_subset*j+i]=data[(subset[i])+nsample*j];\n\t\t\tnamat_x[size_subset*j+i]=namat[(subset[i])+nsample*j];\n\t\t}\n\t}\n\n\tfor(unsigned int i=0; i< nvar; ++i){\n\t\tmim[i*nvar+i]=0;\n\t\tfor(unsigned int j=i+1; j< nvar; ++j){\n\t\t\ttmp=get_correlation(data_x,namat_x,i*size_subset,j*size_subset,size_subset);\n\t\t\ttmp=tmp*tmp;\n\t\t\tif(tmp>0.999999){\n\t\t\t\ttmp=0.999999;\n\t\t\t}\n\t\t\tmim[j*nvar+i]= -0.5* log (1-tmp);\n\t\t\tmim[i*nvar+j]=mim[j*nvar+i];\n\t\t}\n\t}\n\t//delete [] namat_x;\n\t//delete [] data_x;\n\n}\n\ndouble returnConcordanceIndexC(int *msurv, int *ustrat, double *x2, int *cl2,\n\t\t\t\t\t   double *st, int *se, double *weights, int *strat, int *N, int *outx, int *lenS, int *lenU)\n{\n\n\tint lenUstrat = *lenU;\n\tint lenStrat = *lenS;\n\n\tdouble res_ch[lenStrat];\n\tdouble res_dh[lenStrat];\n\n\tdouble res_cIndex=0;\n\n\tint Ns_old = 0;\n\tint Ns = 0;\n\tfor(int s=0; s < lenUstrat; s++) {\n\t\tint ixs[lenStrat];\n\t\tfor(int i =0; i < lenStrat; i++){\n\t\t\tixs[i] = 0;\n\t\t\tif(strat[i] == ustrat[s]){\n\t\t\t\tixs[i] = 1;\n\t\t\t} else {\n\t\t\t\tixs[i] = 0;\n\t\t\t}\n\t\t}\n\t\tNs_old += Ns;\n\t\tNs = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tNs++;\n\t\t\t}\n\t\t}\n\t\tdouble xs[Ns];\n\t\tint c = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\txs[c] = x2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint cls[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tcls[c] = cl2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble sts[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tsts[c] = st[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint ses[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tses[c] = se[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble weightss[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tweightss[c] = weights[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble chs[Ns];\n\t\tdouble dhs[Ns];\n\t\tdouble uhs[Ns];\n\t\tdouble rphs[Ns];\n\t\tfor (int h=0; h < Ns; h++) {\n\t\t\tdouble chsj, dhsj, uhsj, rphsj = 0;\n\t\t\tfor (int j=0; j < Ns; j++) {\n\t\t\t\tdouble whj = weightss[h] * weightss[j];\n\t\t\t\tif((*msurv == 1 && (sts[h] < sts[j] && ses[h] == 1)) || (*msurv == 0 && cls[h] > cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] > xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t} else if (xs[h] < xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif((*msurv == 1 && (sts[h] > sts[j] && ses[j] == 1)) || (*msurv == 0 && cls[h] < cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] < xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse if (xs[h] > xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tchs[h] = chsj;\n\t\t\tdhs[h] = dhsj;\n\t\t\tuhs[h] = uhsj;\n\t\t\trphs[h] = rphsj;\n\t\t\tchsj = 0;\n\t\t\tdhsj = 0;\n\t\t\tuhsj = 0;\n\t\t\trphsj = 0;\n\t\t}\n\t\tfor(int i = 0; i < Ns; i++){\n\t\t\tint pos = i + Ns_old;\n\t\t\tres_ch [pos] =chs[i];\n\t\t\tres_dh [pos] =dhs[i];\n\t\t}\n\n\t}\n\n\tdouble tmp_ch=0, tmp_dh=0;\n\tfor(int s=0; s < lenStrat; s++) {\n\t\ttmp_ch+=res_ch[s];\n\t\ttmp_dh+=res_dh[s];\n\t}\n\n\tdouble n=*N;\n\ttmp_ch=(1/(n *(n - 1))) * tmp_ch;\n\ttmp_dh=(1/(n *(n - 1))) * tmp_dh;\n\tres_cIndex=tmp_ch/ (tmp_ch+tmp_dh);\n\n\t/// scale value to be in the intervall [-1,1] as correlation and square to be on same scale as mutual information [0,1]\n\n\tres_cIndex=2*res_cIndex-1;\n\tres_cIndex=res_cIndex*res_cIndex;\n\t//cout<<\"res cIndex \"<<res_cIndex<<endl;\n\treturn res_cIndex;\n}\n\n\nSEXP get_concordanceIndex_onevariable(SEXP Rmsurv, SEXP Rustrat, SEXP Rx2,SEXP Rcl2, SEXP Rst, SEXP Rse, SEXP Rweights, SEXP Rstrat, SEXP RN, SEXP Routx, SEXP RlenS, SEXP RlenU){\n\tdouble *res_cI , res_cIndex;\n\n\tdouble *x2, *st, *weights;\n\tint *msurv, *ustrat, *cl2, *se, *strat, *N, *outx, *lenS, *lenU;\n\n\tSEXP Rres;\n\n\tmsurv=INTEGER_POINTER(Rmsurv);\n\tustrat=INTEGER_POINTER(Rustrat);\n\tx2=NUMERIC_POINTER(Rx2);\n\tcl2=INTEGER_POINTER(Rcl2);\n\tst=NUMERIC_POINTER(Rst);\n\tse =INTEGER_POINTER(Rse);\n\tweights=NUMERIC_POINTER(Rweights);\n\tstrat=INTEGER_POINTER(Rstrat);\n\tN =INTEGER_POINTER(RN);\n\toutx=INTEGER_POINTER(Routx);\n\n\tlenS=INTEGER_POINTER(RlenS);\n\tlenU=INTEGER_POINTER(RlenU);\n\n\n\tPROTECT(Rres = NEW_NUMERIC(1));\n\tres_cI=NUMERIC_POINTER(Rres);\n\n\n\tres_cIndex=returnConcordanceIndexC(msurv, ustrat, x2, cl2,st, se, weights,strat, N, outx, lenS, lenU) ;\n\n\tres_cI[0]=res_cIndex;\n\n\tUNPROTECT(1);\n\n\treturn Rres;\n}\n\nextern \"C\" SEXP\nmrmr_cIndex(SEXP Rdata, SEXP Rnamat, SEXP RcIndex, SEXP Rnvar, SEXP Rnsample, SEXP Rthreshold){\n\tdouble *data, *cIndex;\n\tdouble *rel, *red, *res, *mim, score=1,*threshold, *res_final;\n\n\tconst int *nvar, *nsample;\n\tint *ind, *namat;\n\n\tunsigned int n, jmax=0;\n\tPROTECT(Rdata = AS_NUMERIC(Rdata));\n\tPROTECT(Rnamat = AS_INTEGER(Rnamat));\n\tPROTECT(RcIndex = AS_NUMERIC(RcIndex));\n\tPROTECT(Rnvar= AS_INTEGER(Rnvar));\n\tPROTECT(Rnsample= AS_INTEGER(Rnsample));\n\tPROTECT(Rthreshold= AS_NUMERIC(Rthreshold));\n\n\n\tdata = NUMERIC_POINTER(Rdata);\n\tnamat=INTEGER_POINTER(Rnamat);\n\tcIndex= NUMERIC_POINTER(RcIndex);\n\tnvar = INTEGER_POINTER(Rnvar);\n\tnsample = INTEGER_POINTER(Rnsample);\n\tthreshold = NUMERIC_POINTER(Rthreshold);\n\n\tn = *nvar;\n\n\t//new variables\n\tSEXP Rmim, Rres,Rred,Rrel,Rind,Rres_final;\n\n\tPROTECT(Rmim = NEW_NUMERIC(n*n));\n\tPROTECT(Rres = NEW_NUMERIC(n));\n\tPROTECT(Rres_final = NEW_NUMERIC(n));\n\tPROTECT(Rrel = NEW_NUMERIC(n));\n\tPROTECT(Rred = NEW_NUMERIC(n));\n\tPROTECT(Rind = NEW_INTEGER(*nsample));\n\n\tind = INTEGER_POINTER(Rind);\n\tmim = NUMERIC_POINTER(Rmim);\n\tres = NUMERIC_POINTER(Rres);\n\trel = NUMERIC_POINTER(Rrel);\n\tred = NUMERIC_POINTER(Rred);\n\tres_final = NUMERIC_POINTER(Rres_final);\n\n\n\tfor(unsigned int i=0;i < *nsample; ++i){\n\t\tind[i]=i;\n\t}\n\n\tbuild_mim_subset(mim, data, namat, n, *nsample, ind, *nsample);\n\n\n\tfor( unsigned int i=0; i< n; ++i ){\n\t\t\tres[i]=*threshold;\n\t\t\tres_final[i]=*threshold;\n\t}\n\n\t\t//init rel and red and select first\n\t\tfor( unsigned int j=0; j<n; ++j ) {\n\t\t\trel[j]=cIndex[j];\n\t\t\tred[j]=0;\n\t\t\tif( rel[j] > rel[jmax])\n\t\t\t\tjmax = j;\n\t\t}\n\n\t\tscore = rel[jmax];\n\t\tif( res[jmax] < score ) {\n\t\t\tres[jmax] = score;\n\t\t}\n\n\t\t//select others\n\t\tfor(unsigned int k=1; k < n+1; k++ ) {\n\t\t\tjmax = 0;\n\n\t\t\tfor(unsigned int j=1; j < n; ++j ) {\n\t\t\t\tif( (rel[j] - red[j]/k) > (rel[jmax] - red[jmax]/k) )\n\t\t\t\t\tjmax = j;\n\t\t\t}\n\t\t\tscore = (rel[jmax] - (red[jmax]/k));\n\t\t\tif( res[jmax] < score ) {\n\t\t\t\tres[jmax] = score;\n\t\t\t}\n\n\t\t\t//update rel and red\n\t\t\trel[jmax]=-1000;\n\t\t\tfor( int l=0; l<n; ++l )\n\t\t\t\tred[l] += mim[l*n+jmax];\n\n\t\t\t// stop criterion\n\t\t\tif( score < *threshold ) k=n;\n\n\t\t}\n\n\n\tfor(unsigned int i=0; i< n; ++i ) {\n\t\tres_final[i]=res[i];\n\t}\n\tUNPROTECT(12);\n\n\treturn Rres_final;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `get_correlation` function and how does it handle missing data?",
        "answer": "The `get_correlation` function calculates the correlation between two variables in a dataset. It handles missing data by using the `namat` array to check if both variables have valid values (namat[ind_x+i]==0 && namat[ind_y+i]==0) before including them in the correlation calculation. This ensures that only complete pairs of observations are used in the computation."
      },
      {
        "question": "How does the `build_mim_subset` function create a bootstrapped dataset, and what is the purpose of this operation?",
        "answer": "The `build_mim_subset` function creates a bootstrapped dataset by selecting a subset of samples from the original data. It uses the `subset` array to specify which samples to include, and creates new `data_x` and `namat_x` arrays containing only the selected samples. This operation is likely used for resampling techniques, such as bootstrap aggregating (bagging), to assess the stability of feature selection or to estimate the variability of the mutual information matrix."
      },
      {
        "question": "In the `returnConcordanceIndexC` function, what is the significance of the `outx` parameter, and how does it affect the calculation of the concordance index?",
        "answer": "The `outx` parameter in the `returnConcordanceIndexC` function determines how ties in the predictor variable (x) are handled. When `*outx == 1`, ties are considered as a separate category and contribute to the `uhsj` (undefined) count. When `*outx != 1`, ties are treated as discordant pairs and added to the `dhsj` (discordant) count. This affects the final calculation of the concordance index by influencing how tied predictor values are weighted in the overall measure of predictive discrimination."
      }
    ],
    "completion_tasks": [
      {
        "partial": "double get_correlation(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_data_x = 0.0, mean_data_y = 0.0;\n    double correlation_nom = 0.0, correlation_den_x = 0.0, correlation_den_y = 0.0;\n    int valid_samples = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x + i] == 0 && namat[ind_y + i] == 0) {\n            mean_data_x += data[ind_x + i];\n            mean_data_y += data[ind_y + i];\n            valid_samples++;\n        }\n    }\n\n    if (valid_samples == 0) return 0.0;\n\n    mean_data_x /= valid_samples;\n    mean_data_y /= valid_samples;\n\n    // Complete the function to calculate and return the correlation\n}",
        "complete": "double get_correlation(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_data_x = 0.0, mean_data_y = 0.0;\n    double correlation_nom = 0.0, correlation_den_x = 0.0, correlation_den_y = 0.0;\n    int valid_samples = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x + i] == 0 && namat[ind_y + i] == 0) {\n            mean_data_x += data[ind_x + i];\n            mean_data_y += data[ind_y + i];\n            valid_samples++;\n        }\n    }\n\n    if (valid_samples == 0) return 0.0;\n\n    mean_data_x /= valid_samples;\n    mean_data_y /= valid_samples;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x + i] == 0 && namat[ind_y + i] == 0) {\n            double dx = data[ind_x + i] - mean_data_x;\n            double dy = data[ind_y + i] - mean_data_y;\n            correlation_nom += dx * dy;\n            correlation_den_x += dx * dx;\n            correlation_den_y += dy * dy;\n        }\n    }\n\n    double denominator = sqrt(correlation_den_x * correlation_den_y);\n    return denominator == 0 ? 0 : correlation_nom / denominator;\n}"
      },
      {
        "partial": "void build_mim_subset(double mim[], double data[], int namat[], int nvar, int nsample, int subset[], int size_subset) {\n    double *data_x = (double *)R_alloc(nvar * size_subset, sizeof(double));\n    int *namat_x = (int *)R_alloc(nvar * size_subset, sizeof(int));\n\n    for (int i = 0; i < size_subset; ++i) {\n        for (int j = 0; j < nvar; ++j) {\n            data_x[size_subset * j + i] = data[subset[i] + nsample * j];\n            namat_x[size_subset * j + i] = namat[subset[i] + nsample * j];\n        }\n    }\n\n    // Complete the function to build the mutual information matrix\n}",
        "complete": "void build_mim_subset(double mim[], double data[], int namat[], int nvar, int nsample, int subset[], int size_subset) {\n    double *data_x = (double *)R_alloc(nvar * size_subset, sizeof(double));\n    int *namat_x = (int *)R_alloc(nvar * size_subset, sizeof(int));\n\n    for (int i = 0; i < size_subset; ++i) {\n        for (int j = 0; j < nvar; ++j) {\n            data_x[size_subset * j + i] = data[subset[i] + nsample * j];\n            namat_x[size_subset * j + i] = namat[subset[i] + nsample * j];\n        }\n    }\n\n    for (int i = 0; i < nvar; ++i) {\n        mim[i * nvar + i] = 0;\n        for (int j = i + 1; j < nvar; ++j) {\n            double tmp = get_correlation(data_x, namat_x, i * size_subset, j * size_subset, size_subset);\n            tmp = tmp * tmp;\n            if (tmp > 0.999999) {\n                tmp = 0.999999;\n            }\n            mim[j * nvar + i] = mim[i * nvar + j] = -0.5 * log(1 - tmp);\n        }\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/getsurv2.R",
    "language": "R",
    "content": "`getsurv2` <-\nfunction(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n\twhich.est <- match.arg(which.est)\n\tswitch(which.est,\n\t\"lower\"={\n\t\tsurvres <- as.matrix(sf$lower)\t\n\t},\n\t\"point\"={\n\t\tsurvres <- as.matrix(sf$surv)\n\t},\n\t\"upper\"={\n\t\tsurvres <- as.matrix(sf$upper)\n\t})\n\tif(time >= max(sf$time)) {\n\t\ttime.ix <- length(sf$time)\n\t} else { time.ix <- order(sf$time <= time)[1] - 1 }\n\tif(time.ix == 0) {\n\t\tres <- rep(1, ncol(sf$surv[time.ix, ]))\n\t\tnames(res) <- dimnames(sf$surv)[[2]]\n\t} else { res <- survres[time.ix, ] }\t\n\treturn(res)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getsurv2` function and what are its main parameters?",
        "answer": "The `getsurv2` function is designed to extract survival estimates from a survival object (sf) at a specified time point. Its main parameters are:\n1. sf: A survival object containing survival estimates.\n2. time: The time point at which to extract the survival estimate.\n3. which.est: Specifies which estimate to return ('point', 'lower', or 'upper')."
      },
      {
        "question": "How does the function handle cases where the requested time is greater than or equal to the maximum time in the survival object?",
        "answer": "When the requested time is greater than or equal to the maximum time in the survival object, the function sets `time.ix` to the length of `sf$time`. This effectively returns the survival estimate for the last available time point in the data."
      },
      {
        "question": "What does the function return when the calculated `time.ix` is 0, and why is this case handled separately?",
        "answer": "When `time.ix` is 0, the function returns a vector of 1s with length equal to the number of columns in `sf$surv`. This case is handled separately because it represents a time point before the first event in the survival data, where the survival probability is assumed to be 1 for all subjects. The names of the returned vector are set to match the column names of `sf$surv`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  survres <- switch(which.est,\n    \"lower\" = as.matrix(sf$lower),\n    \"point\" = as.matrix(sf$surv),\n    \"upper\" = as.matrix(sf$upper))\n  \n  # Complete the function to handle time comparison and return the result\n}",
        "complete": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  survres <- switch(which.est,\n    \"lower\" = as.matrix(sf$lower),\n    \"point\" = as.matrix(sf$surv),\n    \"upper\" = as.matrix(sf$upper))\n  \n  time.ix <- if(time >= max(sf$time)) length(sf$time) else order(sf$time <= time)[1] - 1\n  \n  if(time.ix == 0) {\n    res <- rep(1, ncol(sf$surv))\n    names(res) <- dimnames(sf$surv)[[2]]\n  } else {\n    res <- survres[time.ix, ]\n  }\n  \n  return(res)\n}"
      },
      {
        "partial": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  # Implement the function to select the appropriate survival estimate\n  # and handle time comparison\n}",
        "complete": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  survres <- switch(which.est,\n    lower = as.matrix(sf$lower),\n    point = as.matrix(sf$surv),\n    upper = as.matrix(sf$upper))\n  \n  time.ix <- if(time >= max(sf$time)) length(sf$time) else which.max(sf$time > time) - 1\n  \n  res <- if(time.ix == 0) {\n    setNames(rep(1, ncol(sf$surv)), dimnames(sf$surv)[[2]])\n  } else {\n    survres[time.ix, ]\n  }\n  \n  res\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/tamr13.R",
    "language": "R",
    "content": "#' @title Function to compute the risk scores of the tamoxifen resistance\n#' signature (TAMR13)\n#'\n#' @description\n#' This function computes signature scores from gene expression values\n#'   following the algorithm used for the Tamoxifen Resistance signature (TAMR13).\n#'\n#' @usage\n#' tamr13(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes\n#'   in columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to\n#' force the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low\n#'   risk (not implemented, the function will return NA values).\n#'\n#' @references\n#' Loi S, Haibe-Kains B, Desmedt C, Wirapati P, Lallemand F, Tutt AM, Gillet C,\n#'   Ellis P, Ryder K, Reid JF, Daidone MG, Pierotti MA, Berns EMJJ, Jansen MPHM,\n#'   Foekens JA, Delorenzi M, Bontempi G, Piccart MJ and Sotiriou C (2008)\n#'   \"Predicting prognosis using molecular profiling in estrogen receptor-\n#'   positive breast cancer treated with tamoxifen\", BMC Genomics, 9(1):239\n#'\n#' @seealso\n#' [genefu::gene76]\n#'\n#' @examples\n#' # load TAMR13 signature\n#' data(sig.tamr13)\n#' # load VDX dataset\n#' data(vdxs)\n#' # compute relapse score\n#' tamr13.vdxs <- tamr13(data=data.vdxs, annot=annot.vdxs, do.mapping=FALSE)\n#' summary(tamr13.vdxs$score)\n#'\n#' @md\n#' @export\n#' @name tamr13\ntamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\trest <- resc <- NULL\n\tfor(i in 1:length(sig.tamr13)) {\n\t\trest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n\t\tresc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n\t}\n\tdimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n\tnames(resc) <- names(sig.tamr13)\n\tres <- drop(rest %*% resc)\n\triskt <- rep(NA, length(res))\n\tnames(riskt) <- names(res)\n\tres <- list(\"score\"=res, \"risk\"=riskt)\n\n\treturn (res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `tamr13` function and what does it return?",
        "answer": "The `tamr13` function computes risk scores for the tamoxifen resistance signature (TAMR13) from gene expression data. It returns a list containing two items: 'score' (continuous signature scores) and 'risk' (binary risk classification, currently not implemented and returns NA values)."
      },
      {
        "question": "How does the function handle gene mapping, and what parameter controls this behavior?",
        "answer": "The function can perform gene mapping through Entrez Gene IDs if needed. This behavior is controlled by the `do.mapping` parameter. When `do.mapping` is TRUE, the function maps probes to genes, keeping the most variant probe for each gene in case of ambiguities. The `mapping` parameter can be used to force a specific mapping instead of selecting probes based on variance."
      },
      {
        "question": "Explain the main computation performed in the `tamr13` function and how it uses the `sig.tamr13` object.",
        "answer": "The main computation in `tamr13` involves iterating through the `sig.tamr13` object, which likely contains multiple gene signatures. For each signature, it calls the `sig.score` function to compute individual scores, storing them in `rest`. It also extracts coefficients from `sig.tamr13` attributes into `resc`. Finally, it computes the overall score by matrix multiplication of `rest` and `resc`, resulting in a weighted sum of individual signature scores."
      }
    ],
    "completion_tasks": [
      {
        "partial": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  for(i in 1:length(sig.tamr13)) {\n    rest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n    resc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n  }\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  # Complete the function by adding code to create the 'riskt' vector and return the result\n}",
        "complete": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  for(i in 1:length(sig.tamr13)) {\n    rest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n    resc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n  }\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  riskt <- rep(NA, length(res))\n  names(riskt) <- names(res)\n  list(score=res, risk=riskt)\n}"
      },
      {
        "partial": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  # Add code to calculate rest and resc\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  riskt <- rep(NA, length(res))\n  names(riskt) <- names(res)\n  list(score=res, risk=riskt)\n}",
        "complete": "tamr13 <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  rest <- resc <- NULL\n  for(i in 1:length(sig.tamr13)) {\n    rest <- cbind(rest, sig.score(x=sig.tamr13[[i]], data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score)\n    resc <- c(resc, attributes(sig.tamr13[[i]])$coefficient)\n  }\n  dimnames(rest) <- list(dimnames(data)[[1]], names(sig.tamr13))\n  names(resc) <- names(sig.tamr13)\n  res <- drop(rest %*% resc)\n  riskt <- rep(NA, length(res))\n  names(riskt) <- names(res)\n  list(score=res, risk=riskt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/fisherz.R",
    "language": "R",
    "content": "'fisherz' <-\nfunction(x, inv=FALSE, eps=1e-16) {\n\t\n\tmyfoo <- function(x, inv, eps) {\n\t\tif(is.na(x)) { return(NA) }\n\t\tif(!inv) {\n\t\t\tif((1 - abs(x)) < eps) { x <- ifelse(x < 0, -Inf, Inf) }\n\t\t\telse { x <- (log(1 + x) - log(1 - x)) / 2 }\n\t\t}\n\t\telse {\n\t\t\tif(is.infinite(x) || x > (1 / eps)) { x <- ifelse(x < 0, -1, 1) }\n\t\t\telse { x <- (exp(2 * x) - 1) / (exp(2 * x) + 1) }\n\t\t}\n\t\treturn(x)\n\t}\n\t\n\tif(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n\tif(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n\treturn(myfoo(x=x, inv=inv, eps=eps))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'fisherz' function and what transformation does it perform?",
        "answer": "The 'fisherz' function performs Fisher's z-transformation on input values. When 'inv=FALSE' (default), it applies the forward transformation: z = 0.5 * ln((1+r)/(1-r)), where r is the input correlation coefficient. When 'inv=TRUE', it applies the inverse transformation: r = (exp(2z) - 1) / (exp(2z) + 1). This transformation is useful in statistical analysis, particularly for normalizing correlation coefficients."
      },
      {
        "question": "How does the function handle different input types (scalar, vector, matrix, data frame)?",
        "answer": "The function handles different input types as follows:\n1. For matrices or data frames, it applies the transformation to each element using 'apply' with MARGIN=c(1, 2).\n2. For vectors, it uses 'sapply' to apply the transformation to each element.\n3. For scalar inputs, it directly calls the inner 'myfoo' function.\nThis design allows the function to work flexibly with various input types, applying the transformation element-wise as needed."
      },
      {
        "question": "What is the purpose of the 'eps' parameter in the 'fisherz' function, and how is it used?",
        "answer": "The 'eps' parameter (default value 1e-16) is used to handle numerical precision issues and edge cases:\n1. In the forward transformation, if |x| is very close to 1 (within 'eps'), it returns Inf or -Inf to avoid division by zero.\n2. In the inverse transformation, if |x| is very large (> 1/eps), it returns 1 or -1 to avoid overflow.\nThis ensures numerical stability and handles extreme values appropriately in the transformation process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    if(!inv) {\n      # Complete the transformation for non-inverse case\n    }\n    else {\n      # Complete the transformation for inverse case\n    }\n    return(x)\n  }\n  \n  # Complete the function to handle different input types\n}",
        "complete": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    if(!inv) {\n      if((1 - abs(x)) < eps) { x <- ifelse(x < 0, -Inf, Inf) }\n      else { x <- (log(1 + x) - log(1 - x)) / 2 }\n    }\n    else {\n      if(is.infinite(x) || x > (1 / eps)) { x <- ifelse(x < 0, -1, 1) }\n      else { x <- (exp(2 * x) - 1) / (exp(2 * x) + 1) }\n    }\n    return(x)\n  }\n  \n  if(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n  if(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n  return(myfoo(x=x, inv=inv, eps=eps))\n}"
      },
      {
        "partial": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    # Complete the transformation logic here\n  }\n  \n  if(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n  if(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n  return(myfoo(x=x, inv=inv, eps=eps))\n}",
        "complete": "fisherz <- function(x, inv=FALSE, eps=1e-16) {\n  myfoo <- function(x, inv, eps) {\n    if(is.na(x)) { return(NA) }\n    if(!inv) {\n      if((1 - abs(x)) < eps) { x <- ifelse(x < 0, -Inf, Inf) }\n      else { x <- (log(1 + x) - log(1 - x)) / 2 }\n    }\n    else {\n      if(is.infinite(x) || x > (1 / eps)) { x <- ifelse(x < 0, -1, 1) }\n      else { x <- (exp(2 * x) - 1) / (exp(2 * x) + 1) }\n    }\n    return(x)\n  }\n  \n  if(is.matrix(x) || is.data.frame(x)) { return(apply(X=x, MARGIN=c(1, 2), FUN=myfoo, inv=inv, eps=eps)) }\n  if(is.vector(x)) { return(sapply(X=x, FUN=myfoo, inv=inv, eps=eps)) }\n  return(myfoo(x=x, inv=inv, eps=eps))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/tdrocc.R",
    "language": "R",
    "content": "`tdrocc` <-\nfunction(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n\t#require(survivalROC)\t\n\tdata <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n\tcc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n   if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n   if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n   x2 <- x[cc.ix]\n   surv.time2 <- surv.time[cc.ix]\n   surv.event2 <- surv.event[cc.ix]\n    \n   if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }\n   if(is.na(cutpts)) {\n       ux <- unique(sort(x2))\n       delta <- min(diff(ux))/2\n       cutpts <- c(ux - delta, ux[length(ux)] + delta)\n   }\n   myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n   \n\tif(all(time < surv.time2[surv.event2 == 1])) { return(list(\"spec\"=NA, \"sens\"=NA, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=NA, \"AUC\"=NA, \"data\"=data)) }\n    \n\trocco <- survivalROC::survivalROC(Stime=surv.time2, status=surv.event2, marker=x2, predict.time=time, cut.values=cutpts, entry=surv.entry, span=span, lambda=lambda, ...)\n\tres <- list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts)\n\t\n\treturn(list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=rocco$Survival, \"AUC\"=rocco$AUC, \"data\"=data))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `tdrocc` function and what are its main input parameters?",
        "answer": "The `tdrocc` function is designed to calculate time-dependent receiver operating characteristic (ROC) curves for survival data. Its main input parameters are:\n- `x`: The marker or predictor variable\n- `surv.time`: Survival time\n- `surv.event`: Survival event indicator (0 or 1)\n- `time`: The time point at which to evaluate the ROC curve\n- `cutpts`: Cut points for the marker variable (optional)\n- `na.rm`: Whether to remove NA values\n- `span` and `lambda`: Parameters for smoothing (passed to survivalROC)"
      },
      {
        "question": "How does the function handle missing values and what condition must the survival event variable meet?",
        "answer": "The function handles missing values by:\n1. Checking for complete cases using `complete.cases()`\n2. If `na.rm=FALSE` and NA values are present, it stops with an error\n3. If `na.rm=TRUE`, it removes cases with NA values and optionally prints a message about the number of removed cases\n\nThe survival event variable must take only values 0 or 1. This is checked with the condition:\n`if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }`"
      },
      {
        "question": "What does the function return and under what condition does it return early with NA values?",
        "answer": "The function returns a list containing:\n- `spec`: Specificity values\n- `sens`: Sensitivity values\n- `rule`: A function for classification based on a threshold\n- `cuts`: Cut points used\n- `time`: Time point evaluated\n- `survival`: Survival probability\n- `AUC`: Area Under the Curve\n- `data`: Original input data\n\nThe function returns early with NA values for spec, sens, and AUC if:\n`all(time < surv.time2[surv.event2 == 1])`\nThis condition checks if the specified time point is earlier than all observed event times, in which case the ROC curve cannot be calculated."
      }
    ],
    "completion_tasks": [
      {
        "partial": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  cc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n  \n  x2 <- x[cc.ix]\n  surv.time2 <- surv.time[cc.ix]\n  surv.event2 <- surv.event[cc.ix]\n  \n  if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }\n  if(is.na(cutpts)) {\n    ux <- unique(sort(x2))\n    delta <- min(diff(ux))/2\n    cutpts <- c(ux - delta, ux[length(ux)] + delta)\n  }\n  myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n  \n  # Complete the function from here\n}",
        "complete": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  cc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n  \n  x2 <- x[cc.ix]\n  surv.time2 <- surv.time[cc.ix]\n  surv.event2 <- surv.event[cc.ix]\n  \n  if(!all(sort(unique(surv.event2)) == c(0, 1))) { stop(\"survival event variable must take values 0 or 1\") }\n  if(is.na(cutpts)) {\n    ux <- unique(sort(x2))\n    delta <- min(diff(ux))/2\n    cutpts <- c(ux - delta, ux[length(ux)] + delta)\n  }\n  myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n  \n  if(all(time < surv.time2[surv.event2 == 1])) { return(list(\"spec\"=NA, \"sens\"=NA, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=NA, \"AUC\"=NA, \"data\"=data)) }\n  \n  rocco <- survivalROC::survivalROC(Stime=surv.time2, status=surv.event2, marker=x2, predict.time=time, cut.values=cutpts, entry=surv.entry, span=span, lambda=lambda, ...)\n  \n  return(list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=rocco$Survival, \"AUC\"=rocco$AUC, \"data\"=data))\n}"
      },
      {
        "partial": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  # Add input data validation and preprocessing here\n  \n  myrule <- function(x, thresh) { return(ifelse(x > thresh, 1, 0)) }\n  \n  if(all(time < surv.time2[surv.event2 == 1])) {\n    # Return early if condition is met\n  }\n  \n  # Add survivalROC calculation here\n  \n  # Return the result\n}",
        "complete": "tdrocc <- function(x, surv.time, surv.event, surv.entry=NULL, time, cutpts=NA, na.rm=FALSE, verbose=FALSE, span=0, lambda=0, ...) {\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  cc.ix <- complete.cases(x, surv.time, surv.event, surv.entry)\n  if (!all(cc.ix) && !na.rm) stop(\"NA values are present!\")\n  if(verbose) message(sprintf(\"%i cases are removed due to NA values\", sum(!cc.ix)))\n  \n  x2 <- x[cc.ix]\n  surv.time2 <- surv.time[cc.ix]\n  surv.event2 <- surv.event[cc.ix]\n  \n  if(!all(sort(unique(surv.event2)) == c(0, 1))) stop(\"survival event variable must take values 0 or 1\")\n  if(is.na(cutpts)) {\n    ux <- unique(sort(x2))\n    delta <- min(diff(ux))/2\n    cutpts <- c(ux - delta, ux[length(ux)] + delta)\n  }\n  \n  myrule <- function(x, thresh) ifelse(x > thresh, 1, 0)\n  \n  if(all(time < surv.time2[surv.event2 == 1])) {\n    return(list(\"spec\"=NA, \"sens\"=NA, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=NA, \"AUC\"=NA, \"data\"=data))\n  }\n  \n  rocco <- survivalROC::survivalROC(Stime=surv.time2, status=surv.event2, marker=x2, predict.time=time, cut.values=cutpts, entry=surv.entry, span=span, lambda=lambda, ...)\n  \n  list(\"spec\"=1-rocco$FP, \"sens\"=rocco$TP, \"rule\"=myrule, \"cuts\"=cutpts, \"time\"=time, \"survival\"=rocco$Survival, \"AUC\"=rocco$AUC, \"data\"=data)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcAngiogenic.R",
    "language": "R",
    "content": "#' @title Function to compute the subtype scores and risk classifications\n#'   for the angiogenic molecular subtype in ovarian cancer\n#'\n#' @description\n#' This function computes subtype scores and risk classifications from\n#'   gene expression values following the algorithm developed by Bentink,\n#'   Haibe-Kains et al. to identify the angiogenic molecular subtype in\n#'   ovarian cancer.\n#'\n#' @usage\n#' ovcAngiogenic(data, annot, hgs,\n#' gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"),\n#' do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param hgs vector of booleans with TRUE represents the ovarian cancer\n#'   patients who have a high grade, late stage, serous tumor, FALSE otherwise. This is particularly important for properly rescaling the data. If hgs is missing, all the patients will be used to rescale the subtype score.\n#' @param gmap character string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#' - subtype: data frame reporting the subtype score, maximum likelihood\n#'   classification and corresponding subtype probabilities.\n#'\n#' @references\n#' Bentink S, Haibe-Kains B, Risch T, Fan J-B, Hirsch MS, Holton K, Rubio R,\n#'   April C, Chen J, Wickham-Garcia E, Liu J, Culhane AC, Drapkin R, Quackenbush\n#'   JF, Matulonis UA (2012) \"Angiogenic mRNA and microRNA Gene Expression\n#'   Signature Predicts a Novel Subtype of Serous Ovarian Cancer\", PloS one,\n#'   7(2):e30269\n#'\n#' @seealso\n#' [genefu::sigOvcAngiogenic]\n#'\n#' @examples\n#' # load the ovcAngiogenic signature\n#' \n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \n#'   \"entrezgene\"\n#' \n#' # compute relapse score\n#' ovcAngiogenic.nkis <- ovcAngiogenic(data=data.nkis, annot=annot.nkis,\n#'   gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcAngiogenic.nkis$risk)\n#'\n#' @md\n#' @export\novcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\",\n    \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE,\n    verbose=FALSE)\n{\n    # load gene signatures if needed\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcAngiogenic[order(abs(sigOvcAngiogenic[ ,\"weight\"]), \n            decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcAngiogenic), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcAngiogenic[gix, ,drop=FALSE]\n    }\n\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    ## rescale only with the high grade, late stage, serous (hgs) patients\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    ## rescale the whole dataset\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n    emclust.ts <- mclust::estep(modelName=\"E\", data=pscore, parameters=modelOvcAngiogenic)\n    dimnames(emclust.ts$z) <- list(names(pscore), c(\"Angiogenic.proba\", \"nonAngiogenic.proba\"))\n    class.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n    names(class.ts) <- names(pscore)\n    sbt.ts <- class.ts\n    sbt.ts[class.ts == 1] <- \"Angiogenic\"\n    sbt.ts[class.ts == 2] <- \"nonAngiogenic\"\n    sbts <- data.frame(\"subtype.score\"=pscore, \"subtype\"=sbt.ts, emclust.ts$z)\n    prisk <- as.numeric(sbts[ ,\"subtype\"] == \"Angiogenic\")\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `ovcAngiogenic` function?",
        "answer": "The `ovcAngiogenic` function computes subtype scores and risk classifications for the angiogenic molecular subtype in ovarian cancer. It uses gene expression data and follows the algorithm developed by Bentink, Haibe-Kains et al. The function returns continuous signature scores, binary risk classifications, and subtype probabilities."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs mapping through Entrez Gene IDs. It selects the most variant probe for each gene in case of ambiguities. The function uses the `geneid.map` function to map gene IDs between the input data and the signature. It also creates a `myprobe` data frame that shows the correspondence between original probes, gene maps, and new probes."
      },
      {
        "question": "How are the subtype scores calculated and rescaled in the `ovcAngiogenic` function?",
        "answer": "The function calculates subtype scores using the `sig.score` function from the genefu package. It then rescales these scores using only high-grade, late-stage, serous (hgs) patients with the `rescale` function. The rescaled scores are transformed to a range of -1 to 1. Finally, it uses the Mclust package to perform clustering and calculate subtype probabilities based on these rescaled scores."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        # Add mapping logic here\n    } else {\n        # Add non-mapping logic here\n    }\n\n    # Add score calculation and classification logic here\n\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}",
        "complete": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcAngiogenic[order(abs(sigOvcAngiogenic[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcAngiogenic), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcAngiogenic[gix, ,drop=FALSE]\n    }\n\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n    emclust.ts <- mclust::estep(modelName=\"E\", data=pscore, parameters=modelOvcAngiogenic)\n    dimnames(emclust.ts$z) <- list(names(pscore), c(\"Angiogenic.proba\", \"nonAngiogenic.proba\"))\n    class.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n    names(class.ts) <- names(pscore)\n    sbt.ts <- class.ts\n    sbt.ts[class.ts == 1] <- \"Angiogenic\"\n    sbt.ts[class.ts == 2] <- \"nonAngiogenic\"\n    sbts <- data.frame(\"subtype.score\"=pscore, \"subtype\"=sbt.ts, emclust.ts$z)\n    prisk <- as.numeric(sbts[ ,\"subtype\"] == \"Angiogenic\")\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}"
      },
      {
        "partial": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    # Load required data\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n\n    # Perform mapping or non-mapping logic\n\n    # Calculate scores and classify\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n\n    # Add classification logic here\n\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}",
        "complete": "ovcAngiogenic <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('modelOvcAngiogenic')) data(modelOvcAngiogenic, envir=environment())\n    if (!exists('sigOvcAngiogenic')) data(sigOvcAngiogenic, envir=environment())\n\n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) stop(\"gmap is not a column of annot!\")\n        if(verbose) message(\"the most variant probe is selected for each gene\")\n        sigt <- sigOvcAngiogenic[order(abs(sigOvcAngiogenic[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcAngiogenic), colnames(data))\n        if(length(gix) < 2) stop(\"data do not contain enough gene from the ovcTCGA signature!\")\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcAngiogenic))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcAngiogenic[gix, ,drop=FALSE]\n    }\n\n    ss <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=TRUE)$score\n    rr <- genefu::rescale(ss[hgs], q=0.05, na.rm=TRUE)\n    pscore <- ((ss - attributes(rr)$q1) / (attributes(rr)$q2 - attributes(rr)$q1) - 0.5) * 2\n    emclust.ts <- mclust::estep(modelName=\"E\", data=pscore, parameters=modelOvcAngiogenic)\n    dimnames(emclust.ts$z) <- list(names(pscore), c(\"Angiogenic.proba\", \"nonAngiogenic.proba\"))\n    class.ts <- mclust::map(emclust.ts$z, warn=FALSE)\n    names(class.ts) <- names(pscore)\n    sbt.ts <- ifelse(class.ts == 1, \"Angiogenic\", \"nonAngiogenic\")\n    sbts <- data.frame(\"subtype.score\"=pscore, \"subtype\"=sbt.ts, emclust.ts$z)\n    prisk <- as.numeric(sbts[ ,\"subtype\"] == \"Angiogenic\")\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe, \"subtype\"=sbts))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcCrijns.R",
    "language": "R",
    "content": "#' @title Function to compute the subtype scores and risk classifications\n#'   for the prognostic signature published by Crinjs et al.\n#'\n#' @description\n#' This function computes subtype scores and risk classifications from gene\n#' expression values using the weights published by Crijns et al.\n#'\n#' @usage\n#' ovcCrijns(data, annot, hgs,\n#'   gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"),\n#'   do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param hgs vector of booleans with TRUE represents the ovarian cancer\n#'   patients who have a high grade, late stage, serous tumor, FALSE otherwise.\n#'   This is particularly important for properly rescaling the data. If hgs is\n#'   missing, all the patients will be used to rescale the subtype score.\n#' @param gmap character string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for each\n#'   gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' Note that the original algorithm has not been implemented as it necessitates\n#'   refitting of the model weights in each new dataset. However the current\n#'   implementation should give similar results.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence.\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Crijns APG, Fehrmann RSN, de Jong S, Gerbens F, Meersma G J, Klip HG,\n#'   Hollema H, Hofstra RMW, te Meerman GJ, de Vries EGE, van der Zee AGJ (2009)\n#'   \"Survival-Related Profile, Pathways, and Transcription Factors in Ovarian\n#'   Cancer\" PLoS Medicine, 6(2):e1000024.\n#'\n#' @seealso\n#' [genefu::sigOvcCrijns]\n#'\n#' @examples\n#' # load the ovsCrijns signature\n#' data(sigOvcCrijns)\n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \n#'   \"entrezgene\"\n#' # compute relapse score\n#' ovcCrijns.nkis <- ovcCrijns(data=data.nkis, annot=annot.nkis, \n#'   gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcCrijns.nkis$risk)\n#'\n#' @md\n#' @export\n#' @name ovcCrijns\novcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\",\n    \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE)\n{\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    ## transform the gene expression in Z-scores\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `ovcCrijns` function and what are its key input parameters?",
        "answer": "The `ovcCrijns` function computes subtype scores and risk classifications for ovarian cancer patients based on gene expression data. Its key input parameters are:\n1. `data`: A matrix of gene expressions with samples in rows and probes in columns.\n2. `annot`: A matrix of annotations with a column named as specified in `gmap`.\n3. `hgs`: A vector of booleans indicating high-grade, late-stage, serous tumors.\n4. `gmap`: The biomaRt attribute to use for mapping (default options provided).\n5. `do.mapping`: A boolean indicating whether to perform mapping through Entrez Gene IDs.\n6. `verbose`: A boolean to control informative message output."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs the following steps:\n1. It checks if the specified `gmap` is a column in the `annot` matrix.\n2. It selects the most variant probe for each gene to handle ambiguities.\n3. It maps the gene IDs between the signature (`sigOvcCrijns`) and the input data.\n4. It updates the `data`, `annot`, and signature (`sigt`) objects to reflect the mapping.\n5. It creates a `mymapping` vector to track the number of mapped genes.\n6. It generates a `myprobe` data frame to store the mapping between original probes, gene IDs, and new probes.\nThis process ensures that the gene identifiers in the input data match those in the signature, allowing for accurate score computation."
      },
      {
        "question": "Explain the process of computing the subtype scores and risk classifications in the `ovcCrijns` function.",
        "answer": "The `ovcCrijns` function computes subtype scores and risk classifications as follows:\n1. It scales the gene expression data to Z-scores using the `scale()` function.\n2. It uses the `genefu::sig.score()` function to compute the subtype scores, passing:\n   - A data frame with probe IDs, Entrez Gene IDs, and coefficients from the signature.\n   - The scaled gene expression data.\n   - The annotation data.\n   - `do.mapping=FALSE` to prevent redundant mapping.\n   - `signed=FALSE` for unsigned scoring.\n3. It calculates risk classifications by comparing each score to the median score:\n   - Scores above the median are classified as high risk (1).\n   - Scores below or equal to the median are classified as low risk (0).\n4. Finally, it returns a list containing:\n   - `score`: The continuous signature scores.\n   - `risk`: The binary risk classifications.\n   - `mapping`: Information about the gene mapping process.\n   - `probe`: A data frame with probe mapping details."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        # Complete the else block\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        # Complete the do.mapping block\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcCrijns <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcCrijns')) data(sigOvcCrijns, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcCrijns[order(abs(sigOvcCrijns[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcCrijns), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcCrijns))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcCrijns[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/no.at.risk.R",
    "language": "R",
    "content": "`no.at.risk` <-\nfunction( formula.s, data.s, sub.s=\"all\", t.step, t.end ) {\n# Updated 6.6.11 to work from summary.surfvit\n\n    if( length(sub.s)==1 && sub.s==\"all\" ) sub.s <- rep(TRUE, nrow(data.s))\n    pos <- 1\n    envir = as.environment(pos)\n    assign(\"sub.s\", sub.s, envir = envir)\n\n    sf <- survfit( formula.s, data=data.s, subset=sub.s )\n    if (is.null(sf$strata))\n        sf$strata <- c(\"All\" = length(sf$time))\n    n.strata <- length(sf$strata)\n\n    t.pts <- seq(0, t.end, t.step)\n    sumsf <- summary(sf, times = t.pts, extend = TRUE)\n    tms <- with(sumsf, split(time, strata))\n    rsk <- with(sumsf, split(n.risk, strata))\n\n    nar <- do.call(\"rbind\", rsk)\n    nar <- data.frame(names(sf$strata), nar)\n    colnames(nar) <- c(\"risk.factor\", as.character(tms[[1]]))\n\n    remove(\"sub.s\", envir=.GlobalEnv)\n    nar\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `no.at.risk` function in this R code snippet?",
        "answer": "The `no.at.risk` function calculates and returns the number of subjects at risk over time for a survival analysis. It takes a survival formula, dataset, subset condition, time step, and end time as inputs. The function uses the `survfit` function to compute the survival fit, then summarizes the results at specified time points and formats the output as a data frame with risk factors and corresponding number at risk for each time point."
      },
      {
        "question": "How does the function handle the `sub.s` parameter, and what is its purpose?",
        "answer": "The `sub.s` parameter is used to specify a subset of the data for analysis. If `sub.s` is set to 'all' or not provided, the function creates a logical vector of `TRUE` values for all rows in the dataset. Otherwise, it uses the provided subset condition. The function then assigns this subset to the global environment temporarily for use in the `survfit` function call. After the calculation, it removes the `sub.s` variable from the global environment to avoid polluting the workspace."
      },
      {
        "question": "How does the function handle stratification in the survival analysis, and how is this reflected in the output?",
        "answer": "The function supports stratified survival analysis. If the `survfit` result contains strata, it processes each stratum separately. The output data frame (`nar`) includes a 'risk.factor' column that contains the names of the strata. Each row in the output represents a different stratum, with columns for each time point showing the number at risk. If there's no stratification, a single row with 'All' as the risk factor is returned."
      }
    ],
    "completion_tasks": [
      {
        "partial": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"sub.s\", sub.s, envir = envir)\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n  n.strata <- length(sf$strata)\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  # Complete the function to create and return the 'nar' data frame\n}",
        "complete": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"sub.s\", sub.s, envir = envir)\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n  n.strata <- length(sf$strata)\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  tms <- with(sumsf, split(time, strata))\n  rsk <- with(sumsf, split(n.risk, strata))\n\n  nar <- do.call(\"rbind\", rsk)\n  nar <- data.frame(names(sf$strata), nar)\n  colnames(nar) <- c(\"risk.factor\", as.character(tms[[1]]))\n\n  remove(\"sub.s\", envir=.GlobalEnv)\n  nar\n}"
      },
      {
        "partial": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  assign(\"sub.s\", sub.s, envir = as.environment(1))\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  # Complete the function to process the summary and return the result\n}",
        "complete": "no.at.risk <- function(formula.s, data.s, sub.s=\"all\", t.step, t.end) {\n  if(length(sub.s)==1 && sub.s==\"all\") sub.s <- rep(TRUE, nrow(data.s))\n  assign(\"sub.s\", sub.s, envir = as.environment(1))\n\n  sf <- survfit(formula.s, data=data.s, subset=sub.s)\n  if (is.null(sf$strata))\n    sf$strata <- c(\"All\" = length(sf$time))\n\n  t.pts <- seq(0, t.end, t.step)\n  sumsf <- summary(sf, times = t.pts, extend = TRUE)\n  tms <- with(sumsf, split(time, strata))\n  rsk <- with(sumsf, split(n.risk, strata))\n\n  nar <- data.frame(names(sf$strata), do.call(\"rbind\", rsk))\n  colnames(nar) <- c(\"risk.factor\", as.character(tms[[1]]))\n\n  remove(\"sub.s\", envir=.GlobalEnv)\n  nar\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ihc4.R",
    "language": "R",
    "content": "#' @name ihc4\n#' @title Function to compute the IHC4 prognostic score as published by\n#'   Paik et al. in 2004.\n#'\n#' @description\n#' This function computes the prognostic score based on four measured IHC markers\n#'   (ER, PGR, HER2, Ki-67), following the algorithm as published by Cuzick et al. 2011.\n#'   The user has the option to either obtain just the shrinkage-adjusted IHC4 score (IHC4)\n#'   or the overall score htat also combines the clinical score (IHC4+C)\n#'\n#' @usage\n#' ihc4(ER, PGR, HER2, Ki67,age,size,grade,node,ana,scoreWithClinical=FALSE, na.rm = FALSE)\n#'\n#' @param ER ER score between 0-10, calculated as (H-score/30).\n#' @param PGR Progesterone Receptor score between 0-10.\n#' @param HER2 Her2/neu status (0 or 1).\n#' @param Ki67 Ki67 score based on percentage of positively staining malignant cells.\n#' @param age patient age.\n#' @param size tumor size in cm.\n#' @param grade Histological grade, i.e. low (1), intermediate (2) and high (3) grade.\n#' @param node Nodal status.\n#' @param ana treatment with anastrozole.\n#' @param scoreWithClinical TRUE to get IHC4+C score, FALSE to get just the IHC4 score.\n#' @param na.rm TRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @return\n#' Shrinkage-adjusted IHC4 score or the Overall Prognostic Score based on IHC4+C\n#'   (IHC4+Clinical Score)\n#'\n#' @references\n#' Jack Cuzick, Mitch Dowsett, Silvia Pineda, Christopher Wale, Janine Salter, Emma Quinn,\n#'   Lila Zabaglo, Elizabeth Mallon, Andrew R. Green, Ian O. Ellis, Anthony Howell, Aman U.\n#'   Buzdar, and John F. Forbes (2011) \"Prognostic Value of a Combined Estrogen Receptor,\n#'   Progesterone Receptor, Ki-67, and Human Epidermal Growth Factor Receptor 2\n#'   Immunohistochemical Score and Comparison with the Genomic Health Recurrence Score\n#'   in Early Breast Cancer\", Journal of Clinical Oncologoy, 29(32):4273\u20134278.\n#'\n#' @examples\n#' # load NKI dataset\n#' data(nkis)\n#' # compute shrinkage-adjusted IHC4 score\n#' count<-nrow(demo.nkis)\n#' ihc4(ER=sample(x=1:10, size=count,replace=TRUE),PGR=sample(x=1:10, size=count,replace=TRUE),\n#' HER2=sample(x=0:1,size=count,replace=TRUE),Ki67=sample(x=1:100, size=count,replace=TRUE),\n#' scoreWithClinical=FALSE, na.rm=TRUE)\n#'\n#' # compute IHC4+C score\n#' ihc4(ER=sample(x=1:10, size=count,replace=TRUE),PGR=sample(x=1:10, size=count,replace=TRUE),\n#' HER2=sample(x=0:1,size=count,replace=TRUE),Ki67=sample(x=1:100, size=count,replace=TRUE),\n#' age=demo.nkis[,\"age\"],size=demo.nkis[ ,\"size\"],grade=demo.nkis[ ,\"grade\"],node=demo.nkis[ ,\"node\"],\n#' ana=sample(x=0:1,size=count,replace=TRUE), scoreWithClinical=TRUE, na.rm=TRUE)\n#'\n#' @md\n#' @export\nihc4 <- function(ER, PGR, HER2, Ki67,age,size,grade,node,ana,\n                 scoreWithClinical=FALSE,na.rm=FALSE)\n{\n    nn <- names(ER)\n    if(is.null(nn)) { nn <- paste(\"PATIENT\", 1:length(ER), sep=\".\") }\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n\n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]\n    PGR <- PGR[cc.ix]\n    HER2 <- HER2[cc.ix]\n\n    if(length(ER) != length(PGR) || length(ER) != length(HER2) || length(ER) != length(Ki67))\n    {stop(\"ER, PGR, HER2, and Ki67 scores must have the same length!\")}\n    if(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n\n    if(!all(is.element(PGR, c(\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\")))) {\n      stop(\"PGR scores must be between 0 and 10\")\n    }\n\n    if(!all(is.element(ER, c(\"0\", \"1\", \"2\",\"3\",\"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\")))) {\n      stop(\"ER scores must be between 0 and 10\")\n    }\n\n    if(!all(is.element(HER2, c(\"0\", \"1\")))) {\n      #if only \"0\" and \"1\" are available, map \"0\" -> \"1\" and \"1\" -> \"3\"\n      stop(\"her2 expression must be 0 or 1!\")\n    }\n\n    if(!is.numeric(Ki67)) {\n      stop(\"Ki67 must be numeric!\")\n    }\n\n    ihc4 <- 94.7 * ((0.586*HER2)-(0.100*ER)-(0.079*PGR)+(0.240*log(1 + 10 * Ki67)))\n\n    if(scoreWithClinical==FALSE)\n    {\n        names(ihc4) <- nn[cc.ix]\n        ihc4.score <- rep(NA, length(cc.ix))\n        names(ihc4.score) <- nn\n        ihc4.score[names(ihc4)] <- ihc4\n        return(\"score\"=ihc4.score)\n    }\n\n    if(scoreWithClinical==TRUE){\n        size <- size[cc.ix]\n        grade <- grade[cc.ix]\n        node <- node[cc.ix]\n\n        if(length(size) != length(grade) || length(grade) != length(node)) {\n          stop(\"size, grade and lymph node stage must have the same length!\")\n        }\n        if(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n        if(!all(is.element(grade, c(\"1\", \"2\", \"3\")))) {\n          stop(\"grade must be 1, 2 or 3!\")\n        }\n        if(!all(is.element(node, c(\"0\",\"1\", \"2\", \"3\")))) {\n          #if only \"0\" and \"1\" are available, map \"0\" -> \"1\" and \"1\" -> \"3\"\n          stop(\"lymph node stage must be 1, 2 or 3!\")\n        }\n        if(!is.numeric(size)) {\n          stop(\"tumor size (cm) must be numeric!\")\n        }\n\n        Ana<-ana\n        N_1to3<-ifelse(node>3,0,node)\n        N4<-ifelse(node==4,node,0)\n        Age<-ifelse(age>65,age,0) #Age above 65 yrs\n        T_1to2<-ifelse(grade<=2,grade,0)\n        T_2to3<-ifelse((grade==2 | grade==3),grade,0)\n        Tabove3<-ifelse(grade>3,grade,0)\n        G2<-ifelse(grade==2,grade,0)\n        G3<-ifelse(grade==3,grade,0)\n\n\n        ClinicalScore<- 100 * ((0.417 * N_1to3) + (1.566 * N4)\n                               + (0.930 * ((0.479*T_1to2)+(0.882*T_2to3)\n                                  +(1.838*Tabove3)+(0.559*G2)+(0.970*G3)+(0.130*Age)-(0.149*Ana))))\n\n        Overall<-ihc4+ClinicalScore\n\n        names(Overall) <- nn[cc.ix]\n        Overall.score <- rep(NA, length(cc.ix))\n        names(Overall.score) <- nn\n        Overall.score[names(Overall)] <- ihc4\n        return(\"score\"=Overall.score)\n    }\n  }",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ihc4` function and what are its main input parameters?",
        "answer": "The `ihc4` function computes the IHC4 prognostic score for breast cancer patients. It takes four main IHC markers as input: ER (Estrogen Receptor), PGR (Progesterone Receptor), HER2 (Human Epidermal Growth Factor Receptor 2), and Ki67 (a proliferation marker). Additional clinical parameters like age, tumor size, grade, nodal status, and treatment with anastrozole can be included for a combined IHC4+C score. The function can return either the shrinkage-adjusted IHC4 score or the overall IHC4+C score, depending on the `scoreWithClinical` parameter."
      },
      {
        "question": "How does the function handle missing values and what input validation does it perform?",
        "answer": "The function handles missing values using the `na.rm` parameter. If `na.rm` is set to FALSE and there are missing values, the function will stop with an error message. The function performs several input validations: it checks if ER, PGR, HER2, and Ki67 scores have the same length, ensures PGR and ER scores are between 0 and 10, verifies HER2 status is either 0 or 1, and confirms Ki67 is numeric. For clinical parameters, it checks if size, grade, and node have the same length, ensures grade is 1, 2, or 3, verifies node status is 0, 1, 2, or 3, and confirms tumor size is numeric."
      },
      {
        "question": "How is the IHC4 score calculated in the function, and what additional steps are taken if the clinical score is included?",
        "answer": "The IHC4 score is calculated using the formula: 94.7 * ((0.586*HER2) - (0.100*ER) - (0.079*PGR) + (0.240*log(1 + 10 * Ki67))). If `scoreWithClinical` is TRUE, the function calculates an additional Clinical Score using factors such as nodal status, tumor size, grade, age, and treatment with anastrozole. The Clinical Score is then added to the IHC4 score to produce the Overall Prognostic Score (IHC4+C). The function returns either the IHC4 score alone or the Overall Score, depending on the `scoreWithClinical` parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    nn <- names(ER)\n    if(is.null(nn)) { nn <- paste(\"PATIENT\", 1:length(ER), sep=\".\") }\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n\n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]\n    PGR <- PGR[cc.ix]\n    HER2 <- HER2[cc.ix]\n\n    # Add input validation here\n\n    ihc4 <- 94.7 * ((0.586*HER2)-(0.100*ER)-(0.079*PGR)+(0.240*log(1 + 10 * Ki67)))\n\n    if(!scoreWithClinical) {\n        # Return IHC4 score\n    } else {\n        # Calculate and return IHC4+C score\n    }\n}",
        "complete": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    nn <- names(ER)\n    if(is.null(nn)) { nn <- paste(\"PATIENT\", 1:length(ER), sep=\".\") }\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n\n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]\n    PGR <- PGR[cc.ix]\n    HER2 <- HER2[cc.ix]\n    Ki67 <- Ki67[cc.ix]\n\n    if(length(ER) != length(PGR) || length(ER) != length(HER2) || length(ER) != length(Ki67))\n        stop(\"ER, PGR, HER2, and Ki67 scores must have the same length!\")\n    if(!all(cc.ix) & !na.rm)  stop(\"NA values are present!\")\n    if(!all(ER %in% 0:10)) stop(\"ER scores must be between 0 and 10\")\n    if(!all(PGR %in% 0:10)) stop(\"PGR scores must be between 0 and 10\")\n    if(!all(HER2 %in% c(0, 1))) stop(\"HER2 expression must be 0 or 1!\")\n    if(!is.numeric(Ki67)) stop(\"Ki67 must be numeric!\")\n\n    ihc4 <- 94.7 * ((0.586*HER2)-(0.100*ER)-(0.079*PGR)+(0.240*log(1 + 10 * Ki67)))\n\n    if(!scoreWithClinical) {\n        ihc4.score <- rep(NA, length(cc.ix))\n        names(ihc4.score) <- nn\n        ihc4.score[names(ihc4)] <- ihc4\n        return(list(score=ihc4.score))\n    } else {\n        size <- size[cc.ix]\n        grade <- grade[cc.ix]\n        node <- node[cc.ix]\n        age <- age[cc.ix]\n        ana <- ana[cc.ix]\n\n        if(length(size) != length(grade) || length(grade) != length(node))\n            stop(\"size, grade and lymph node stage must have the same length!\")\n        if(!all(grade %in% 1:3)) stop(\"grade must be 1, 2 or 3!\")\n        if(!all(node %in% 0:3)) stop(\"lymph node stage must be 0, 1, 2 or 3!\")\n        if(!is.numeric(size)) stop(\"tumor size (cm) must be numeric!\")\n\n        ClinicalScore <- 100 * ((0.417 * pmin(node, 3)) + (1.566 * (node == 4)) +\n                                (0.930 * ((0.479 * (size <= 2)) + (0.882 * (size > 2 & size <= 3)) +\n                                           (1.838 * (size > 3)) + (0.559 * (grade == 2)) +\n                                           (0.970 * (grade == 3)) + (0.130 * (age > 65)) - (0.149 * ana))))\n\n        Overall <- ihc4 + ClinicalScore\n        Overall.score <- rep(NA, length(cc.ix))\n        names(Overall.score) <- nn\n        Overall.score[names(Overall)] <- Overall\n        return(list(score=Overall.score))\n    }\n}"
      },
      {
        "partial": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    # Input validation and preprocessing\n    \n    # Calculate IHC4 score\n    \n    if(!scoreWithClinical) {\n        # Return IHC4 score\n    } else {\n        # Calculate clinical score components\n        \n        # Calculate overall score\n        \n        # Return overall score\n    }\n}",
        "complete": "ihc4 <- function(ER, PGR, HER2, Ki67, age, size, grade, node, ana, scoreWithClinical=FALSE, na.rm=FALSE) {\n    nn <- names(ER) %||% paste(\"PATIENT\", seq_along(ER), sep=\".\")\n    names(ER) <- names(PGR) <- names(HER2) <- names(Ki67) <- nn\n    \n    cc.ix <- complete.cases(ER, PGR, HER2, Ki67)\n    ER <- ER[cc.ix]; PGR <- PGR[cc.ix]; HER2 <- HER2[cc.ix]; Ki67 <- Ki67[cc.ix]\n    \n    if(length(ER) != length(PGR) || length(ER) != length(HER2) || length(ER) != length(Ki67))\n        stop(\"ER, PGR, HER2, and Ki67 scores must have the same length!\")\n    if(!all(cc.ix) && !na.rm) stop(\"NA values are present!\")\n    if(!all(ER %in% 0:10) || !all(PGR %in% 0:10)) stop(\"ER and PGR scores must be between 0 and 10\")\n    if(!all(HER2 %in% c(0, 1))) stop(\"HER2 expression must be 0 or 1!\")\n    if(!is.numeric(Ki67)) stop(\"Ki67 must be numeric!\")\n    \n    ihc4 <- 94.7 * ((0.586*HER2) - (0.100*ER) - (0.079*PGR) + (0.240*log(1 + 10*Ki67)))\n    \n    if(!scoreWithClinical) {\n        score <- rep(NA_real_, length(cc.ix))\n        names(score) <- nn\n        score[names(ihc4)] <- ihc4\n        return(list(score=score))\n    } else {\n        size <- size[cc.ix]; grade <- grade[cc.ix]; node <- node[cc.ix]\n        age <- age[cc.ix]; ana <- ana[cc.ix]\n        \n        if(length(size) != length(grade) || length(grade) != length(node))\n            stop(\"size, grade and lymph node stage must have the same length!\")\n        if(!all(grade %in% 1:3)) stop(\"grade must be 1, 2 or 3!\")\n        if(!all(node %in% 0:4)) stop(\"lymph node stage must be 0, 1, 2, 3 or 4!\")\n        if(!is.numeric(size)) stop(\"tumor size (cm) must be numeric!\")\n        \n        clinical_score <- 100 * (0.417 * pmin(node, 3) + 1.566 * (node == 4) +\n                                  0.930 * (0.479 * (size <= 2) + 0.882 * (size > 2 & size <= 3) +\n                                           1.838 * (size > 3) + 0.559 * (grade == 2) +\n                                           0.970 * (grade == 3) + 0.130 * (age > 65) - 0.149 * ana))\n        \n        overall <- ihc4 + clinical_score\n        score <- rep(NA_real_, length(cc.ix))\n        names(score) <- nn\n        score[names(overall)] <- overall\n        return(list(score=score))\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/medianCtr.R",
    "language": "R",
    "content": "#' @title Center around the median\n#'\n#' @description\n#' Utility function called within the claudinLow classifier\n#'\n#' @usage\n#' medianCtr(x)\n#'\n#' @param x\t Matrix of numbers\n#'\n#' @return\n#' A matrix of median-centered numbers\n#'\n#' @references\n#' citation(\"claudinLow\")\n#'\n#' @seealso\n#' [genefu::claudinLow]\n#'\n#' @md\n#' @export\nmedianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  dimnames(x) <- annAll\n  return(x)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `medianCtr` function in the given R code snippet?",
        "answer": "The `medianCtr` function is a utility function used within the claudinLow classifier. Its purpose is to center a matrix of numbers around their respective medians. It takes a matrix as input, calculates the median of each row, subtracts these medians from their corresponding rows, and returns the median-centered matrix while preserving the original dimension names."
      },
      {
        "question": "How does the `medianCtr` function handle missing values (NA) in the input matrix?",
        "answer": "The `medianCtr` function handles missing values (NA) by using the `na.rm=TRUE` argument in the `median` function call within `apply`. This ensures that the median is calculated by ignoring any NA values in each row. However, it's important to note that the function does not remove or replace NA values in the final output; it only ignores them when calculating the median for centering."
      },
      {
        "question": "What is the significance of the `scale` function in the `medianCtr` implementation, and how is it used?",
        "answer": "The `scale` function is used in `medianCtr` to perform the median centering operation. It's applied with `center=medians` and `scale=FALSE` arguments. This means it subtracts the calculated row medians from each element in the corresponding row (centering) without performing any scaling. The `t` function is used before and after `scale` to transpose the matrix, allowing row-wise operations to be performed as column-wise operations, which is more efficient in R."
      }
    ],
    "completion_tasks": [
      {
        "partial": "medianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  # Complete the function\n}",
        "complete": "medianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  dimnames(x) <- annAll\n  return(x)\n}"
      },
      {
        "partial": "medianCtr <- function(x){\n  # Calculate medians\n  # Center the data\n  # Restore dimension names\n  # Return the result\n}",
        "complete": "medianCtr <- function(x){\n  annAll <- dimnames(x)\n  medians <- apply(x,1,median,na.rm=TRUE)\n  x <- t(scale(t(x),center=medians,scale=FALSE))\n  dimnames(x) <- annAll\n  return(x)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/balanced.hazard.ratio.R",
    "language": "R",
    "content": "`balanced.hazard.ratio` <-\nfunction(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...)\n{\n    #Balanced Hazard ratio\n    \n    if (missing(method.test))\n    {\n        method.test = \"logrank\"\n    }\n    if (missing(ties))\n    {\n        ties = \"breslow\"\n    }\n    if(!missing(weights))\n    {\n        if(length(weights) != length(x))\n        {\n            stop(\"bad length for parameter weights!\")\n        }\n    } else {\n        weights <- rep(1,  length(x))\n    }\n    if(!missing(strat)) {\n        if(length(strat) != length(x))\n        {\n            stop(\"bad length for parameter strat!\")\n        }\n        ## remove weights=0 because the coxph function does not deal with them properly\n        iix <- weights <= 0\n        if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n        weights[iix] <- NA\n    } else { \n        strat <- rep(1,  length(x))\n    }\n    \n    \n    #Duplicating the patients\n    cl = sort(unique(x))\n    \n    repTime = c()\n    repEvent = c()\n    repX = c()\n    repWeights = c()\n    repStrat = c()\n    \n    xOld = x\n    for(i in 1:(length(cl)))\n    {\n        x[xOld==cl[i]] = i*2\n    }\n    \n    for(i in 1:(length(cl)-1))\n    {\n        ind = c(which(x==i*2),which(x==(i+1)*2))\n        repTime = c(repTime,surv.time[ind])\n        repEvent = c(repEvent,surv.event[ind])\n        repX = c(repX,rep(i*2+1,length(ind)))\n        repWeights = c(repWeights,weights[ind])\n        repStrat = c(repStrat,strat[ind])\n    }\n    \n    #Compute the hazard ratio on the duplicated patients\n    BHr = hazard.ratio( x=c(x,repX), surv.time=c(surv.time,repTime), surv.event=c(surv.event,repEvent), weights=c(weights,repWeights), strat=c(strat,repStrat), alpha=alpha, method.test=method.test, ties=\"breslow\", ...)\n    \n    BHr$balanced.hazard.ratio = BHr$hazard.ratio\n    BHr$hazard.ratio = NULL\n    BHr$n = length(x)\n    BHr$data$x = x\n    BHr$data$surv.time = surv.time\n    BHr$data$surv.event = surv.event\n    \n    return(BHr)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `balanced.hazard.ratio` function and how does it differ from a standard hazard ratio calculation?",
        "answer": "The `balanced.hazard.ratio` function calculates a balanced hazard ratio, which is a modification of the standard hazard ratio. It works by duplicating patients and creating intermediate groups between existing groups. This approach helps to account for non-linear relationships between the predictor variable and the hazard, providing a more robust estimate of the overall effect across the range of the predictor. The function first prepares the data by duplicating and modifying it, then calculates the hazard ratio using the modified dataset."
      },
      {
        "question": "How does the function handle missing or incorrect input parameters?",
        "answer": "The function has several error handling mechanisms:\n1. It sets default values for `method.test` ('logrank') and `ties` ('breslow') if they are not provided.\n2. It checks if the length of `weights` and `strat` (if provided) match the length of `x`. If not, it stops execution with an error message.\n3. If `weights` are not provided, it creates a vector of 1s with the same length as `x`.\n4. If `strat` is not provided, it creates a vector of 1s with the same length as `x`.\n5. It issues a warning and discards samples with weights <= 0 when stratification is used.\nThese checks ensure that the function can handle various input scenarios and provide informative error messages when inputs are incorrect."
      },
      {
        "question": "Explain the process of duplicating patients in the `balanced.hazard.ratio` function. Why is this done and how does it affect the final calculation?",
        "answer": "The process of duplicating patients in the `balanced.hazard.ratio` function involves:\n1. Creating new groups between existing groups by assigning new values to `x`.\n2. Duplicating the survival times, events, and other relevant data for patients in adjacent groups.\n3. Creating a new group that sits between the two original groups.\n\nThis duplication is done to create a more continuous representation of the hazard across the range of the predictor variable. By introducing these intermediate groups, the function can capture non-linear relationships between the predictor and the hazard that might be missed by a standard hazard ratio calculation.\n\nThe final calculation uses this expanded dataset to compute the hazard ratio, which results in a 'balanced' hazard ratio that represents the average effect across the entire range of the predictor, rather than just comparing two distinct groups. This approach can provide a more robust and interpretable measure of the overall effect, especially when the relationship between the predictor and the hazard is not strictly linear."
      }
    ],
    "completion_tasks": [
      {
        "partial": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    if (missing(method.test)) method.test = \"logrank\"\n    if (missing(ties)) ties = \"breslow\"\n    if (!missing(weights)) {\n        if (length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n    } else {\n        weights <- rep(1, length(x))\n    }\n    if (!missing(strat)) {\n        if (length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n        iix <- weights <= 0\n        if (any(iix)) warning(\"samples with weight<=0 are discarded\")\n        weights[iix] <- NA\n    } else {\n        strat <- rep(1, length(x))\n    }\n    \n    # Complete the function to duplicate patients and compute balanced hazard ratio\n}",
        "complete": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    if (missing(method.test)) method.test = \"logrank\"\n    if (missing(ties)) ties = \"breslow\"\n    if (!missing(weights)) {\n        if (length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n    } else {\n        weights <- rep(1, length(x))\n    }\n    if (!missing(strat)) {\n        if (length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n        iix <- weights <= 0\n        if (any(iix)) warning(\"samples with weight<=0 are discarded\")\n        weights[iix] <- NA\n    } else {\n        strat <- rep(1, length(x))\n    }\n    \n    cl <- sort(unique(x))\n    x_old <- x\n    x[x_old %in% cl] <- match(x_old, cl) * 2\n    \n    rep_data <- lapply(1:(length(cl)-1), function(i) {\n        ind <- x %in% c(i*2, (i+1)*2)\n        list(\n            time = surv.time[ind],\n            event = surv.event[ind],\n            x = rep(i*2+1, sum(ind)),\n            weights = weights[ind],\n            strat = strat[ind]\n        )\n    })\n    \n    rep_data <- do.call(Map, c(list(c), rep_data))\n    \n    BHr <- hazard.ratio(\n        x = c(x, rep_data$x),\n        surv.time = c(surv.time, rep_data$time),\n        surv.event = c(surv.event, rep_data$event),\n        weights = c(weights, rep_data$weights),\n        strat = c(strat, rep_data$strat),\n        alpha = alpha,\n        method.test = method.test,\n        ties = \"breslow\",\n        ...\n    )\n    \n    BHr$balanced.hazard.ratio <- BHr$hazard.ratio\n    BHr$hazard.ratio <- NULL\n    BHr$n <- length(x)\n    BHr$data <- list(x = x, surv.time = surv.time, surv.event = surv.event)\n    \n    return(BHr)\n}"
      },
      {
        "partial": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    # Initialize method.test, ties, weights, and strat\n    \n    # Duplicate patients\n    cl <- sort(unique(x))\n    x_old <- x\n    x[x_old %in% cl] <- match(x_old, cl) * 2\n    \n    # Complete the function to create replicated data and compute balanced hazard ratio\n}",
        "complete": "balanced.hazard.ratio <- function(x, surv.time, surv.event, alpha=0.05, method.test = c(\"logrank\", \"likelihood.ratio\", \"wald\"), ties=c(\"efron\",\"breslow\",\"exact\"), weights, strat, ...) {\n    method.test <- if (missing(method.test)) \"logrank\" else match.arg(method.test)\n    ties <- if (missing(ties)) \"breslow\" else match.arg(ties)\n    weights <- if (missing(weights)) rep(1, length(x)) else {\n        if (length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n        weights\n    }\n    strat <- if (missing(strat)) rep(1, length(x)) else {\n        if (length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n        iix <- weights <= 0\n        if (any(iix)) warning(\"samples with weight<=0 are discarded\")\n        weights[iix] <- NA\n        strat\n    }\n    \n    cl <- sort(unique(x))\n    x_old <- x\n    x[x_old %in% cl] <- match(x_old, cl) * 2\n    \n    rep_data <- do.call(rbind, lapply(1:(length(cl)-1), function(i) {\n        ind <- x %in% c(i*2, (i+1)*2)\n        data.frame(\n            time = surv.time[ind],\n            event = surv.event[ind],\n            x = rep(i*2+1, sum(ind)),\n            weights = weights[ind],\n            strat = strat[ind]\n        )\n    }))\n    \n    BHr <- hazard.ratio(\n        x = c(x, rep_data$x),\n        surv.time = c(surv.time, rep_data$time),\n        surv.event = c(surv.event, rep_data$event),\n        weights = c(weights, rep_data$weights),\n        strat = c(strat, rep_data$strat),\n        alpha = alpha,\n        method.test = method.test,\n        ties = \"breslow\",\n        ...\n    )\n    \n    BHr$balanced.hazard.ratio <- BHr$hazard.ratio\n    BHr$hazard.ratio <- NULL\n    BHr$n <- length(x)\n    BHr$data <- list(x = x, surv.time = surv.time, surv.event = surv.event)\n    \n    return(BHr)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/data.R",
    "language": "R",
    "content": "#' @name claudinLowData\n#'\n#' @title claudinLowData for use in the claudinLow classifier. Data generously provided by Aleix Prat.\n#'\n#' @description Training and Testing Data for use with the Claudin-Low Classifier\n#'\n#' @usage data(claudinLowData)\n#'\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#'\n#' @format\n#'  - xd: Matrix of 807 features and 52 samples\n#'  - classes: factor to split samples\n#'  - nfeatures: number of features\n#'  - nsamples: number of samples\n#'  - fnames: names of features\n#'  - snames: names of samples\n#'\n#' @references Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso [genefu::claudinLow()]\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL\n\n#' @name expos\n#'\n#' @aliases data.expos annot.expos demo.expos\n#'\n#' @title Gene expression, annotations and clinical data from the International Genomics Consortium\n#'\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data from the expO dataset collected by the International Genomics Consortium ([](http://www.intgen.org/expo/)).\n#'\n#' @format expos is a dataset containing three matrices\n#'   - data.expos: Matrix containing gene expressions as measured by Affymetrix hgu133plus2 technology (single-channel, oligonucleotides)\n#'   - annot.expos: Matrix containing annotations of ffymetrix hgu133plus2 microarray platform\n#'   - demo.expos: Clinical information of the breast cancer patients whose tumors were hybridized\n#'\n#' @source [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109)\n#'\n#' @references\n#' International Genomics Consortium, http://www.intgen.org/research-services/biobanking-experience/expo/\n#' McCall MN, Bolstad BM, Irizarry RA. (2010) \"Frozen robust multiarray analysis (fRMA)\", Biostatistics, 11(2):242-253.\n#'\n#' @usage data(expos)\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL\n\n#' @name mod1\n#' @md\n#' @docType data\n#' @title Gene modules published in Desmedt et al. 2008\n#' @description List of seven gene modules published in Desmedt et a. 2008, i.e. ESR1 (estrogen receptor pathway), ERBB2 (her2/neu receptor pathway), AURKA (proliferation), STAT1 (immune response), PLAU (tumor invasion), VEGF (angogenesis) and CASP3 (apoptosis).\n#' @usage data(mod1)\n#' @details mod1 is a list of seven gene signatures, i.e. matrices with 3 columns containing the annotations and information related to the signatures themselves.\n#' @references Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend on the molecular subtypes\", Clinical Cancer Research, 14(16):5158--5165.\n#' @keywords data\nNULL\n\n#' @name mod2\n#' @md\n#' @docType data\n#' @title Gene modules published in Wirapati et al. 2008\n#' @description List of seven gene modules published in Wirapati et a. 2008, i.e. ESR1 (estrogen receptor pathway), ERBB2 (her2/neu receptor pathway) and AURKA (proliferation).\n#' @usage data(mod2)\n#' @details mod2 is a list of three gene signatures, i.e. matrices with 3 columns containing the annotations and information related to the signatures themselves.\n#' @source [http://breast-cancer-research.com/content/10/4/R65](http://breast-cancer-research.com/content/10/4/R65)\n#' @references Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' @keywords data\nNULL\n\n#' @name modelOvcAngiogenic\n#' @docType data\n#' @title Model used to classify ovarian tumors into Angiogenic and NonAngiogenic subtypes.\n#' @description Object containing the set of parameters for the mixture of Gaussians used as a model to classify ovarian tumors into Angiogenic and NonAngiogenic subtypes.\n#' @usage data(modelOvcAngiogenic)\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Bentink S, Haibe-Kains B, Risch T, Fan J-B, Hirsch MS, Holton K, Rubio R, April C, Chen J, Wickham-Garcia E, Liu J, Culhane AC, Drapkin R, Quackenbush JF, Matulonis UA (2012) \"Angiogenic mRNA and microRNA Gene Expression Signature Predicts a Novel Subtype of Serous Ovarian Cancer\", PloS one, 7(2):e30269\n#' @keywords data\nNULL\n\n#' @name nkis\n#' @md\n#' @aliases data.nkis annot.nkis demo.nkis\n#' @docType data\n#' @title Gene expression, annotations and clinical data from van de Vijver et al. 2002\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data as published in van de Vijver et al. 2002.\n#' @usage data(nkis)\n#' @details This dataset represent only partially the one published by van  de Vijver et al. in 2008. Indeed, only part of the patients (150) and gene expressions (922) in [`data.nkis`].\n#' @format nkis is a dataset containing three matrices:\n#'   - data.nkis: Matrix containing gene expressions as measured by Agilent technology (dual-channel, oligonucleotides)\n#'   - annot.nkis: Matrix containing annotations of Agilent microarray platform\n#'   - demon.nkis: Clinical information of the breast cancer patients whose tumors were hybridized\n#' @source [http://www.nature.com/nature/journal/v415/n6871/full/415530a.html](http://www.nature.com/nature/journal/v415/n6871/full/415530a.html)\n#' @references M. J. van de Vijver and Y. D. He and L. van't Veer and H. Dai and A. M. Hart and D. W. Voskuil and G. J. Schreiber and J. L. Peterse and C. Roberts and M. J. Marton and M. Parrish and D. Atsma and A. Witteveen and A. Glas and L. Delahaye and T. van der Velde and H. Bartelink and S. Rodenhuis and E. T. Rutgers and S. H. Friend and R. Bernards (2002) \"A Gene Expression Signature as a Predictor of Survival in Breast Cancer\", New England Journal of Medicine, 347(25):1999--2009\n#' @keywords data\nNULL\n\n#' @name pam50\n#' @md\n#' @aliases pam50.scale pam50.robust\n#' @docType data\n#' @title PAM50 classifier for identification of breast cancer molecular subtypes (Parker et al 2009)\n#' @description List of parameters defining the PAM50 classifier for identification of breast cancer molecular subtypes (Parker et al 2009).\n#' @usage\n#' data(pam50)\n#' data(pam50.scale)\n#' data(pam50.robust)\n#' @format List of parameters for PAM50:\n#'  - centroids: Gene expression centroids for each subtype.\n#'  - centroids.map: Mapping for centroids.\n#'  - method.cor: Method of correlation used to compute distance to the centroids.\n#'  - method.centroids: Method used to compute the centroids.\n#'  - std: Method of standardization for gene expressions (\"none\", \"scale\" or \"robust\")\n#'  - mins: Minimum number of samples within each cluster allowed during the fitting of the model.\n#' @details Three versions of the model are provided, each of ones differs by the gene expressions standardization method since it has an important impact on the subtype classification:\n#'   - pam50: Use of the official centroids without scaling of the gene expressions.\n#'   - pam50.scale: Use of the official centroids with traditional scaling of the gene expressions (see [`base::scale()`])\n#'   - pam50.robust: Use of the official centroids with robust scaling of the gene expressions (see [`genefu::rescale()`])\n#' The model `pam50.robust`` has been shown to reach the best concordance with the traditional clinical parameters (ER IHC, HER2 IHC/FISH and histological grade). However the use of this model is recommended only when the dataset is representative of a global population of breast cancer patients (no sampling bias, the 5 subtypes should be present).\n#' @source [http://jco.ascopubs.org/cgi/content/short/JCO.2008.18.1370v1](http://jco.ascopubs.org/cgi/content/short/JCO.2008.18.1370v1)\n#' @references Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, Samuel and Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, Christiane and He, Xiaping and Hu, Zhiyuan and Quackenbush, John F. and Stijleman, Inge J. and Palazzo, Juan and Marron, J.S. and Nobel, Andrew B. and Mardis, Elaine and Nielsen, Torsten O. and Ellis, Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009) \"Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\", Journal of Clinical Oncology, 27(8):1160--1167\n#' @keywords data\nNULL\n\n#' @name scmgene.robust\n#' @docType data\n#' @title Subtype Clustering Model using only ESR1, ERBB2 and AURKA genes for identification of breast cancer molecular subtypes\n#' @description List of parameters defining the Subtype Clustering Model as published in Wirapati et al 2009 and Desmedt et al 2008 but using single genes instead of gene modules.\n#' @usage data(scmgene.robust)\n#' @format List of parameters for SCMGENE:\n#'   - parameters: List of parameters for the mixture of three Gaussians (ER-/HER2-, HER2+ and ER+/HER2-) that define the Subtype Clustering Model. The structure is the same than for an [`mclust::Mclust`] object.\n#'   - cutoff.AURKA: Cutoff for AURKA module score in order to identify ER+/HER2- High Proliferation (aka Luminal B) tumors and ER+/HER2- Low Proliferation (aka Luminal A) tumors.\n#'   - mod: ESR1, ERBB2 and AURKA modules.\n#' @source [http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck](http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck)\n#' @references Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend on the molecular subtypes\", Clinical Cancer Research, 14(16):5158--5165.\n#' @keywords data\n#' @md\nNULL\n\n#' @name scmod1.robust\n#' @docType data\n#' @title Subtype Clustering Model using ESR1, ERBB2 and AURKA modules for identification of breast cancer molecular subtypes (Desmedt et al 2008)\n#' @description List of parameters defining the Subtype Clustering Model as published in Desmedt et al 2008.\n#' @usage data(scmod1.robust)\n#' @format List of parameters for SCMOD1:\n#'   - parameters: List of parameters for the mixture of three Gaussians (ER-/HER2-, HER2+ and ER+/HER2-) that define the Subtype Clustering Model. The structure is the same than for an [`mclust::Mclust()`] object.\n#'   - cutoff.AURKA: Cutoff for AURKA module score in order to identify ER+/HER2- High Proliferation (aka Luminal B) tumors and ER+/HER2- Low Proliferation (aka Luminal A) tumors.\n#'   - mod: ESR1, ERBB2 and AURKA modules.\n#' @source [http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck](http://clincancerres.aacrjournals.org/content/14/16/5158.abstract?ck=nck)\n#' @references Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend on the molecular subtypes\", _Clinical Cancer Research_, *14*(16):5158--5165.\n#' @keywords data\n#' @md\nNULL\n\n#' @name scmod2.robust\n#' @docType data\n#' @title Subtype Clustering Model using ESR1, ERBB2 and AURKA modules for identification of breast cancer molecular subtypes (Desmedt et al 2008)\n#' @description List of parameters defining the Subtype Clustering Model as published in Desmedt et al 2008.\n#' @usage data(scmod1.robust)\n#' @format List of parameters for SCMOD2:\n#'   - parameters: List of parameters for the mixture of three Gaussians (ER-/HER2-, HER2+ and ER+/HER2-) that define the Subtype Clustering Model. The structure is the same than for an [`mclust::Mclust`] object.\n#'   - cutoff.AURKA: Cutoff for AURKA module score in order to identify ER+/HER2- High Proliferation (aka Luminal B) tumors and ER+/HER2- Low Proliferation (aka Luminal A) tumors.\n#'   - mod: ESR1, ERBB2 and AURKA modules.\n#' @source [http://breast-cancer-research.com/content/10/4/R65k](http://breast-cancer-research.com/content/10/4/R65k)\n#' @references Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' @md\nNULL\n\n#' @name sig.endoPredict\n#' @docType data\n#' @title Signature used to compute the endoPredict signature as published by Filipits et al 2011\n#' @description List of 11 genes included in the endoPredict signature. The EntrezGene.ID allows for mapping and the mapping to affy probes is already provided.\n#' @usage data(sig.endoPredict)\n#' @format `sig.endoPredict` is a matrix with 5 columns containing the annotations and information related to the signature itself (including a mapping to Affymetrix HGU platform).\n#' @references Filipits, M., Rudas, M., Jakesz, R., Dubsky, P., Fitzal, F., Singer, C. F., et al. (2011). \"A new molecular predictor of distant recurrence in ER-positive, HER2-negative breast cancer adds independent information to conventional clinical risk factors.\" \\emph{Clinical Cancer Research}, \\bold{17}(18):6012--6020.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.gene70\n#' @docType data\n#' @title Signature used to compute the 70 genes prognosis profile (GENE70) as published by van't Veer et al. 2002\n#' @description List of 70 agilent probe ids representing 56 unique genes included in the GENE70 signature. The EntrezGene.ID allows for mapping and the \"average.good.prognosis.profile\" values allows for signature computation.\n#' @usage data(sig.gene70)\n#' @format sig.gene70 is a matrix with 9 columns containing the annotations and information related to the signature itself.\n#' @source [http://www.nature.com/nature/journal/v415/n6871/full/415530a.html](http://www.nature.com/nature/journal/v415/n6871/full/415530a.html)\n#' @references L. J. van't Veer and H. Dai and M. J. van de Vijver and Y. D. He and A. A. Hart and M. Mao and H. L. Peterse and K. van der Kooy and M. J. Marton and A. T. Witteveen and G. J. Schreiber and R. M. Kerkhiven and C. Roberts and P. S. Linsley and R. Bernards and S. H. Friend (2002) \"Gene Expression Profiling Predicts Clinical Outcome of Breast Cancer\", Nature, 415:530--536.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.gene76\n#' @docType data\n#' @title Signature used to compute the Relapse Score (GENE76) as published in Wang et al. 2005\n#' @description List of 76 affymetrix hgu133a probesets representing 60 unique genes included in the GENE76 signature. The EntrezGene.ID allows for mapping and the coefficient allows for signature computation.\n#' @usage data(sig.gene76)\n#' @format `sig.gene70` is a matrix with 10 columns containing the annotations and information related to the signature itself.\n#' @source [http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(05)17947-1/abstract](http://www.thelancet.com/journals/lancet/article/PIIS0140-6736(05)17947-1/abstract)\n#' @references Y. Wang and J. G. Klijn and Y. Zhang and A. M. Sieuwerts and M. P. Look and F. Yang and D. Talantov and M. Timmermans and M. E. Meijer-van Gelder and J. Yu and T. Jatkoe and E. M. Berns and D. Atkins and J. A. Foekens (2005) \"Gene-Expression Profiles to Predict Distant Metastasis of Lymph-Node-Negative Primary Breast Cancer\", Lancet, 365(9460):671--679.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.genius\n#' @docType data\n#' @title Gene Expression progNostic Index Using Subtypes (GENIUS) as published by Haibe-Kains et al. 2010.\n#' @description List of three gene signatures which compose the Gene Expression progNostic Index Using Subtypes (GENIUS) as published by Haibe-Kains et al. 2009. GENIUSM1, GENIUSM2 and GENIUSM3  are the ER-/HER2-, HER2+ and ER+/HER2- subtype signatures respectively.\n#' @format `sig.genius` is a list a three subtype signatures.\n#' @references Haibe-Kains B, Desmedt C, Rothe F, Sotiriou C and Bontempi G (2010) \"A fuzzy gene expression-based computational approach improves breast cancer prognostication\", Genome Biology, 11(2):R18\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.ggi\n#' @docType data\n#' @title Gene expression Grade Index (GGI) as published in Sotiriou et al. 2006\n#' @description List of 128 affymetrix hgu133a probesets representing 97 unique genes included in the GGI signature. The \"EntrezGene.ID\" column allows for mapping and \"grade\" defines the up-regulation of the expressions either in histological grade 1 or 3.\n#' @usage data(sig.ggi)\n#' @format sig.ggi is a matrix with 9 columns containing the annotations and information related to the signature itself.\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Sotiriou C, Wirapati P, Loi S, Harris A, Bergh J, Smeds J, Farmer P, Praz V, Haibe-Kains B, Lallemand F, Buyse M, Piccart MJ and Delorenzi M (2006) \"Gene expression profiling in breast cancer: Understanding the molecular basis of histologic grade to improve prognosis\", Journal of National Cancer Institute, 98:262--272\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.oncotypedx\n#' @docType data\n#' @title Signature used to compute the OncotypeDX signature as published by Paik et al 2004\n#' @description List of 21 genes included in the OncotypeDX signature. The EntrezGene.ID allows for mapping and the mapping to affy probes is already provided.\n#' @usage data(sig.oncotypedx)\n#' @references S. Paik, S. Shak, G. Tang, C. Kim, J. Bakker, M. Cronin, F. L. Baehner, M. G. Walker, D. Watson, T. Park, W. Hiller, E. R. Fisher, D. L. Wickerham, J. Bryant, and N. Wolmark (2004) \"A Multigene Assay to Predict Recurrence of Tamoxifen-Treated, Node-Negative Breast Cancer\", New England Journal of Medicine, 351(27):2817--2826.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.pik3cags\n#' @docType data\n#' @title Gene expression Grade Index (GGI) as published in Sotiriou et al. 2006\n#' @description List of 278 affymetrix hgu133a probesets representing 236 unique genes included in the PIK3CA-GS signature. The \"EntrezGene.ID\" column allows for mapping and \"coefficient\" refers to to the direction of association with PIK3CA mutation.\n#' @usage data(sig.pik3cags)\n#' @format sig.pik3cags is a matrix with 3 columns containing the annotations and information related to the signature itself.\n#' @source [http://www.pnas.org/content/107/22/10208/suppl/DCSupplemental](http://www.pnas.org/content/107/22/10208/suppl/DCSupplemental)\n#' @references Loi S, Haibe-Kains B, Majjaj S, Lallemand F, Durbecq V, Larsimont D, Gonzalez-Angulo AM, Pusztai L, Symmans FW, Bardelli A, Ellis P, Tutt AN, Gillett CE, Hennessy BT., Mills GB, Phillips WA, Piccart MJ, Speed TP, McArthur GA, Sotiriou C (2010) \"PIK3CA mutations associated with gene signature of low mTORC1 signaling and better outcomes in estrogen receptor-positive breast cancer\", Proceedings of the National Academy of Sciences, 107(22):10208--10213\n#' @keywords data\n#' @md\nNULL\n\n#' @name sig.tamr13\n#' @docType data\n#' @title Tamoxifen Resistance signature composed of 13 gene clusters (TAMR13) as published by Loi et al. 2008.\n#' @description List of 13 clusters of genes (and annotations) and their corresponding coefficient as an additional attribute.\n#' @usage data(sig.tamr13)\n#' @format sig.tamr13 is a list a 13 clusters of genes with their corresponding coefficient.\n#' @references Loi S, Haibe-Kains B, Desmedt C, Wirapati P, Lallemand F, Tutt AM, Gillet C, Ellis P, Ryder K, Reid JF, Daidone MG, Pierotti MA, Berns EMJJ, Jansen MPHM, Foekens JA, Delorenzi M, Bontempi G, Piccart MJ and Sotiriou C (2008) \"Predicting prognosis using molecular profiling in estrogen receptor-positive breast cancer treated with tamoxifen\", BMC Genomics, 9(1):239\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcAngiogenic\n#' @title sigOvcAngiogenic dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Bentink S, Haibe-Kains B, Risch T, Fan J-B, Hirsch MS, Holton K, Rubio R, April C, Chen J, Wickham-Garcia E, Liu J, Culhane AC, Drapkin R, Quackenbush JF, Matulonis UA (2012) \"Angiogenic mRNA and microRNA Gene Expression Signature Predicts a Novel Subtype of Serous Ovarian Cancer\", PloS one, 7(2):e30269\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcCrijns\n#' @title sigOvcCrijns dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Crijns APG, Fehrmann RSN, de Jong S, Gerbens F, Meersma G J, Klip HG, Hollema H, Hofstra RMW, te Meerman GJ, de Vries EGE, van der Zee AGJ (2009) \"Survival-Related Profile, Pathways, and Transcription Factors in Ovarian Cancer\" PLoS Medicine, 6(2):e1000024.\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcSpentzos\n#' @title sigOcvSpentzos dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Spentzos, D., Levine, D. A., Ramoni, M. F., Joseph, M., Gu, X., Boyd, J., et al. (2004). \"Gene expression signature with independent prognostic significance in epithelial ovarian cancer\". Journal of clinical oncology, 22(23), 4700--4710. doi:10.1200/JCO.2004.04.070\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcTCGA\n#' @title sigOvcTCGA dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Bell D, Berchuck A, Birrer M et al. (2011) \"Integrated genomic analyses of ovarian carcinoma\", Nature, 474(7353):609--615\n#' @keywords data\n#' @md\nNULL\n\n#' @name sigOvcYoshihara\n#' @title sigOvcYoshihara dataset\n#' @docType data\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#' @references Yoshihara K, Tajima A, Yahata T, Kodama S, Fujiwara H, Suzuki M, Onishi Y, Hatae M, Sueyoshi K, Fujiwara H, Kudo, Yoshiki, Kotera K, Masuzaki H, Tashiro H, Katabuchi H, Inoue I, Tanaka K (2010) \"Gene expression profile for predicting survival in advanced-stage serous ovarian cancer across two independent datasets\", PloS one, 5(3):e9615.\n#' @keywords data\n#' @md\nNULL\n\n#' @name ssp2003\n#' @aliases ssp2003.robust ssp2003.scale\n#' @title SSP2003 classifier for identification of breast cancer molecular subtypes (Sorlie et al 2003)\n#' @description List of parameters defining the SSP2003 classifier for identification of breast cancer molecular subtypes (Sorlie et al 2003).\n#' @usage\n#' data(ssp2003)\n#' data(ssp2003.robust)\n#' data(ssp2003.scale)\n#' @docType data\n#' @format List of parameters for SSP2003:\n#'   - centroids: Gene expression centroids for each subtype.\n#'   - centroids.map: Mapping for centroids.\n#'   - method.cor: Method of correlation used to compute distance to the centroids.\n#'   - method.centroids: Method used to compute the centroids.\n#'   - std: Method used to compute the centroids.\n#'   - mins: Minimum number of samples within each cluster allowed during the fitting of the model.\n#' @source [http://www.pnas.org/content/100/14/8418](http://www.pnas.org/content/100/14/8418)\n#' @references T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and A. Nobel and S. Deng and H. Johnsen and R. Pesich and S. Geister and J. Demeter and C. Perou and P. E. Lonning and P. O. Brown and A. L. Borresen-Dale and D. Botstein (2003) \"Repeated Observation of Breast Tumor Subtypes in Independent Gene Expression Data Sets\", Proceedings of the National Academy of Sciences, 1(14):8418--8423\n#' @keywords data\n#' @md\nNULL\n\n#' @name ssp2006\n#' @aliases ssp2006.robust ssp2006.scale\n#' @title SSP2006 classifier for identification of breast cancer molecular subtypes (Hu et al 2006)\n#' @description List of parameters defining the SSP2006 classifier for identification of breast cancer molecular subtypes (Hu et al 2006).\n#' @usage\n#' data(ssp2006)\n#' data(ssp2006.robust)\n#' data(ssp2006.scale)\n#' @format List of parameters for SSP2006:\n#'   - centroids: Gene expression centroids for each subtype.\n#'   - centroids.map: Mapping for centroids.\n#'   - method.cor: Method of correlation used to compute distance to the centroids.\n#'   - method.centroids: Method used to compute the centroids.\n#'   - std: Method of standardization for gene expressions.\n#'   - mins: Minimum number of samples within each cluster allowed during the fitting of the model.\n#' @details Three versions of the model are provided, each of ones differs by the gene expressions standardization method since it has an important impact on the subtype classification:\n#'   - ssp2006: Use of the official centroids without scaling of the gene expressions.\n#'   - ssp2006.scale: Use of the official centroids with traditional scaling of the gene expressions (see [`base::scale()`])\n#'   - ssp2006.robust: Use of the official centroids with robust scaling of the gene expressions (see [`genefu::rescale()`])\n#' The model `ssp2006.robust` has been shown to reach the best concordance with the traditional clinical parameters (ER IHC, HER2 IHC/FISH and histological grade). However the use of this model is recommended only when the dataset is representative of a global population of breast cancer patients (no sampling bias, the 5 subtypes should be present).\n#' @docType data\n#' @source [http://www.biomedcentral.com/1471-2164/7/96](http://www.biomedcentral.com/1471-2164/7/96)\n#' @references Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and Qaqish, Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and Dressler, Lynn and Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer, Lynda and Wu, Junyuan and Liu, Yudong and Nanda, Rita and Tretiakova, Maria and Orrico, Alejandra and Dreher, Donna and Palazzo, Juan and Perreard, Laurent and Nelson, Edward and Mone, Mary and Hansen, Heidi and Mullins, Michael and Quackenbush, John and Ellis, Matthew and Olopade, Olufunmilayo and Bernard, Philip and Perou, Charles (2006) \"The molecular portraits of breast tumors are conserved across microarray platforms\", _BMC Genomics_, *7*(96)\n#' @keywords data\n#' @md\nNULL\n\n#' @name vdxs\n#' @aliases data.vdxs annot.vdxs demo.vdxs\n#' @docType data\n#' @title Gene expression, annotations and clinical data from Wang et al. 2005 and Minn et al 2007\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data as published in Wang et al. 2005 and Minn et al 2007.\n#' @format `vdxs` is a dataset containing three matrices:\n#'   - data.vdxs: Matrix containing gene expressions as measured by Affymetrix hgu133a technology (single-channel, oligonucleotides)\n#'   - annot.vdxs: Matrix containing annotations of ffymetrix hgu133a microarray platform\n#'   - demo.vdxs: Clinical information of the breast cancer patients whose tumors were hybridized\n#' @details This dataset represent only partially the one published by Wang et al. 2005 and Minn et al 2007. Indeed only part of the patients (150) and gene expressions (966) are contained in `data.vdxs`.\n#' @source\n#' [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2034](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2034)\n#'\n#' [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5327](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE5327)\n#' @references\n#' Y. Wang and J. G. Klijn and Y. Zhang and A. M. Sieuwerts and M. P. Look and F. Yang and D. Talantov and M. Timmermans and M. E. Meijer-van Gelder and J. Yu and T. Jatkoe and E. M. Berns and D. Atkins and J. A. Foekens (2005) \"Gene-Expression Profiles to Predict Distant Metastasis of Lymph-Node-Negative Primary Breast Cancer\", _Lancet_, *365*:671--679\n#'\n#' Minn, Andy J. and Gupta, Gaorav P. and Padua, David and Bos, Paula and Nguyen, Don X. and Nuyten, Dimitry and Kreike, Bas and Zhang, Yi and Wang, Yixin and Ishwaran, Hemant and Foekens, John A. and van de Vijver, Marc and Massague, Joan (2007) \"Lung metastasis genes couple breast tumor size and metastatic spread\", _Proceedings of the National Academy of Sciences_, *104*(16):6740--6745\n#' @keywords data\n#' @md\nNULL\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'claudinLowData' dataset in this code?",
        "answer": "The 'claudinLowData' dataset is used for training and testing the Claudin-Low classifier. It contains gene expression data for 807 features across 52 samples, along with class labels and other metadata. This data is used to identify and characterize the claudin-low intrinsic subtype of breast cancer."
      },
      {
        "question": "How many versions of the PAM50 model are provided in this code, and how do they differ?",
        "answer": "Three versions of the PAM50 model are provided: pam50, pam50.scale, and pam50.robust. They differ in the method used for standardizing gene expressions. pam50 uses no scaling, pam50.scale uses traditional scaling, and pam50.robust uses robust scaling. The robust version is recommended for datasets representative of a global population of breast cancer patients, as it has shown the best concordance with traditional clinical parameters."
      },
      {
        "question": "What is the structure of the 'sig.genius' dataset and what does it represent?",
        "answer": "The 'sig.genius' dataset is a list of three gene signatures that compose the Gene Expression progNostic Index Using Subtypes (GENIUS). These signatures, GENIUSM1, GENIUSM2, and GENIUSM3, correspond to the ER-/HER2-, HER2+, and ER+/HER2- subtype signatures respectively. This dataset represents a computational approach to improve breast cancer prognostication using fuzzy gene expression-based methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @name claudinLowData\n#'\n#' @title claudinLowData for use in the claudinLow classifier. Data generously provided by Aleix Prat.\n#'\n#' @description Training and Testing Data for use with the Claudin-Low Classifier\n#'\n#' @usage data(claudinLowData)\n#'\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#'\n#' @format\n#'  - xd: Matrix of 807 features and 52 samples\n#'  - classes: factor to split samples\n#'  - nfeatures: number of features\n#'  - nsamples: number of samples\n#'  - fnames: names of features\n#'  - snames: names of samples\n#'\n#' @references Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso [genefu::claudinLow()]\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL",
        "complete": "#' @name claudinLowData\n#'\n#' @title claudinLowData for use in the claudinLow classifier. Data generously provided by Aleix Prat.\n#'\n#' @description Training and Testing Data for use with the Claudin-Low Classifier\n#'\n#' @usage data(claudinLowData)\n#'\n#' @source [http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1](http://jnci.oxfordjournals.org/cgi/content/full/98/4/262/DC1)\n#'\n#' @format\n#'  - xd: Matrix of 807 features and 52 samples\n#'  - classes: factor to split samples\n#'  - nfeatures: number of features\n#'  - nsamples: number of samples\n#'  - fnames: names of features\n#'  - snames: names of samples\n#'\n#' @references Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular characterization of the claudin-low intrinsic subtype of breast cancer\", Breast Cancer Research, 12(5):R68\n#'\n#' @seealso [genefu::claudinLow()]\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL"
      },
      {
        "partial": "#' @name expos\n#'\n#' @aliases data.expos annot.expos demo.expos\n#'\n#' @title Gene expression, annotations and clinical data from the International Genomics Consortium\n#'\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data from the expO dataset collected by the International Genomics Consortium ([](http://www.intgen.org/expo/)).\n#'\n#' @format expos is a dataset containing three matrices\n#'   - data.expos: Matrix containing gene expressions as measured by Affymetrix hgu133plus2 technology (single-channel, oligonucleotides)\n#'   - annot.expos: Matrix containing annotations of ffymetrix hgu133plus2 microarray platform\n#'   - demo.expos: Clinical information of the breast cancer patients whose tumors were hybridized\n#'\n#' @source [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109)\n#'\n#' @references\n#' International Genomics Consortium, http://www.intgen.org/research-services/biobanking-experience/expo/\n#' McCall MN, Bolstad BM, Irizarry RA. (2010) \"Frozen robust multiarray analysis (fRMA)\", Biostatistics, 11(2):242-253.\n#'\n#' @usage data(expos)\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL",
        "complete": "#' @name expos\n#'\n#' @aliases data.expos annot.expos demo.expos\n#'\n#' @title Gene expression, annotations and clinical data from the International Genomics Consortium\n#'\n#' @description This dataset contains (part of) the gene expression, annotations and clinical data from the expO dataset collected by the International Genomics Consortium ([](http://www.intgen.org/expo/)).\n#'\n#' @format expos is a dataset containing three matrices\n#'   - data.expos: Matrix containing gene expressions as measured by Affymetrix hgu133plus2 technology (single-channel, oligonucleotides)\n#'   - annot.expos: Matrix containing annotations of ffymetrix hgu133plus2 microarray platform\n#'   - demo.expos: Clinical information of the breast cancer patients whose tumors were hybridized\n#'\n#' @source [http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109](http://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE2109)\n#'\n#' @references\n#' International Genomics Consortium, http://www.intgen.org/research-services/biobanking-experience/expo/\n#' McCall MN, Bolstad BM, Irizarry RA. (2010) \"Frozen robust multiarray analysis (fRMA)\", Biostatistics, 11(2):242-253.\n#'\n#' @usage data(expos)\n#'\n#' @md\n#' @docType data\n#' @keywords data\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/methods-subsetTo.R",
    "language": "R",
    "content": "#' @include ToxicoSet-accessors.R\nNULL\n\n#'`[`\n#'\n#' @examples\n#' tSet <- TGGATESsmall[sampleNames(TGGATESsmall), treatmentNames(TGGATESsmall)[seq_len(3)]]\n#'\n#'@param x tSet\n#'@param i Cell lines to keep in tSet\n#'@param j Drugs to keep in tSet\n#'@param ... further arguments\n#'@param drop A boolean flag of whether to drop single dimensions or not\n#'@return Returns the subsetted tSet\n#'@export\nsetMethod(`[`, \"ToxicoSet\", function(x, i, j, ..., drop = FALSE){\n    if(is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n    }\n    else if(is.numeric(i) && is.numeric(j) &&\n            (as.integer(i)==i) && (as.integer(j)==j)) {\n        return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n})\n\n#### subsetTo ####\n\n## FIXED? TODO:: Subset function breaks if it doesnt find cell line in sensitivity info\n#' A function to subset a ToxicoSet to data containing only specified drugs, cells and genes\n#'\n#' This is the prefered method of subsetting a ToxicoSet. This function allows\n#' abstraction of the data to the level of biologically relevant objects: drugs\n#' and cells. The function will automatically go through all of the\n#' combined data in the ToxicoSet and ensure only the requested radiations\n#' and cell lines are found in any of the slots. This allows quickly picking out\n#' all the experiments for a radiation or cell of interest, as well removes the need\n#' to keep track of all the metadata conventions between different datasets.\n#'\n#' @examples\n#' TGGATESDrugNames  <- treatmentNames(TGGATESsmall)\n#' TGGATESCells <- sampleNames(TGGATESsmall)\n#' tSet <- subsetTo(TGGATESsmall,drugs = TGGATESDrugNames[1],\n#'   cells = TGGATESCells[1], duration = \"2\")\n#'\n#' @param object A \\code{ToxicoSet} to be subsetted\n#' @param cell_lines A list or vector of cell names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all cells will be left in\n#'   the dataset.\n#' @param drugs A list or vector of drug names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all drugs will be left in\n#'   the dataset.\n#' @param features A list or vector of feature names as used in the dataset from\n#'   which the object will be subsetted. If left blank that all features will\n#'   be left in.\n#' @param molecular.data.cells A list or vector of cell names to keep in the\n#'   molecular data\n#' @param duration A \\code{list} or \\code{vector} of the experimental durations\n#'   to include in the subset as strings. Defaults to all durations if parameter\n#'   is not specified.\n#' @param ... Other arguments passed to other functions within the package\n#'\n#' @return A ToxicoSet with only the selected drugs and cells\n#'\n#' @importFrom CoreGx .unionList .message .warning .error\n#' @export\n## TODO:: Include dose parmater to subset on\nsubsetTo <- function(object, cell_lines = NULL,\n                     drugs=NULL,\n                     molecular.data.cells=NULL,\n                     duration=NULL,\n                     features=NULL,\n                     ...\n) {\n    ## TODO:: Remove this or add it to the function parameters?\n    drop = FALSE\n\n    ####\n    # PARSING ARGUMENTS\n    ####\n    adArgs = list(...)\n    if (\"exps\" %in% names(adArgs)) {\n        exps <- adArgs[[\"exps\"]]\n        if(is(exps, \"data.frame\")) {\n            exps2 <- exps[[name(object)]]\n            names(exps2) <- rownames(exps)\n            exps <- exps2\n        } else{\n            exps <- exps[[name(object)]]\n        }\n    }else {\n        exps <- NULL\n    }\n\n    if (\"dose\" %in% names(adArgs)) {\n        ## TODO:: Add subsetting on dose\n        stop(\"Due to the structure of tSets, subsetting on dose can only be done on\n            specific slots - not on the entire tSet\")\n    }\n\n    ## MISSING VALUE HANDLING FOR PARAMETERS\n    # Get named list of default values for missing parameters\n    argDefaultList <-\n        paramMissingHandler(funName = \"subsetTo\", tSet = object,\n                            drugs = drugs, cell_lines = cell_lines,\n                            features = features, duration = duration)\n    # Assign any missing parameter default values to function environment\n    if (length(argDefaultList) > 0) {\n        for (idx in seq_along(argDefaultList)) {\n            assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n        }\n    }\n\n    # ERROR HANDLING FOR PARAMETERS\n    paramErrorChecker(funName = \"subsetTo\", tSet = object,\n                      cell_lines = cell_lines,\n                      drugs = drugs, features = features,\n                      duration = duration)\n\n    ##TODO:: Add a value to tSet which indicates the experimental design!\n    ##FIXME:: Don't hard code object names!\n    if (name(object) %in% c(\"drugMatrix_rat\", \"EMEXP2458\")) {\n        if (!('DMSO' %in% drugs)) {\n            drugs <- c(drugs, 'DMSO')\n        }\n    }\n\n    ######\n    # SUBSETTING MOLECULAR PROFILES SLOT\n    ######\n    ### TODO:: implement strict subsetting at this level!!!!\n\n    ### the function missing does not work as expected in the context below, because the arguments are passed to the anonymous\n    ### function in lapply, so it does not recognize them as missing\n    molecularProfilesSlot(object) <-\n        lapply(molecularProfilesSlot(object),\n               function(SE, cell_lines, drugs, molecular.data.cells, duration, features){\n\n                   if (!is.null(features)) {\n                       SE <- SE[which(rownames(SummarizedExperiment::rowData(SE)) %in% features), ]\n                   }\n\n                   ##FIXME:: Why is are all these if conditions being checked against length? Just use grepl?\n                   molecular.data.type <-\n                       ifelse(\n                           length(grep(\"rna\", S4Vectors::metadata(SE)$annotation) > 0),\n                           \"rna\",\n                           S4Vectors::metadata(SE)$annotation\n                       )\n\n                   if (length(grep(molecular.data.type, names(molecular.data.cells))) > 0) {\n                       cell_lines <- molecular.data.cells[[molecular.data.type]]\n                   }\n                   column_indices <- NULL\n\n                   if (length(cell_lines) == 0 && length(drugs) == 0) {\n                       column_indices <- seq_len(ncol(SE))\n                   }\n                   if (length(cell_lines) == 0 && datasetType(object) == \"sensitivity\") {\n                       column_indices <- seq_len(ncol(SE))\n                   }\n\n                   # Selecting indices which match the cells argument\n                   cell_line_index <- NULL\n                   if (length(cell_lines) != 0) {\n                       if (!all(cell_lines %in% sampleNames(object))) {\n                           stop(\"Some of the cell names passed to function did not match to names\n          in the ToxicoSet. Please ensure you are using cell names as\n          returned by the cellNames function\")\n                       }\n                       cell_line_index <- which(SummarizedExperiment::colData(SE)[[\"sampleid\"]] %in% cell_lines)\n                   }\n\n                   # Selecting indexes which match drugs arguement\n                   drugs_index <- NULL\n                   if (datasetType(object) == \"perturbation\" || datasetType(object) == \"both\") {\n                       if (length(drugs) != 0) {\n                           if (!all(drugs %in% treatmentNames(object))){\n                               stop(\"Some of the drug names passed to function did not match to names in the ToxicoSet Please ensure you are using drug names as returned by the drugNames function\")\n                           }\n                           drugs_index <- which(SummarizedExperiment::colData(SE)[[\"treatmentid\"]] %in% drugs)\n                       }\n                   }\n\n                   if (length(drugs_index) != 0 && length(cell_line_index) != 0) {\n                       if (length(intersect(drugs_index, cell_line_index)) == 0) {\n                           stop(\"This Drug - Cell Line combination was not tested together.\")\n                       }\n                       column_indices <- intersect(drugs_index, cell_line_index)\n                   } else {\n                       if (length(drugs_index) != 0) {\n                           column_indices <- drugs_index\n                       }\n                       if (length(cell_line_index) != 0) {\n                           column_indices <- cell_line_index\n                       }\n                   }\n\n                   # LOGIC TO SUBSET BASED ON DURATION\n                   ## TODO:: Determine if this works for other SE data types\n                   if (!is.null(duration)){\n                       if (all(!(duration %in% unique(SummarizedExperiment::colData(SE[, column_indices])$duration)))) {\n                           # Error when other parameters are passed in\n                           if ( !is.null(cell_lines) | !is.null(drugs) | !is.null(molecular.data.cells)) {\n                               stop(paste0(\n                                   \"There are no molecular profiles with duration of \",\n                                   duration, \" in the tSet with the selected parameters.\"\n                               ))\n                           } else { # Error when no other parameters are passed in\n                               stop(paste0(\n                                   \"There are no molecular profiles with duration of \",\n                                   duration, \" in the tSet.\"\n                               ))\n                           }\n                       }\n                       duration_indices <- which(SummarizedExperiment::colData(SE)$duration %in% duration)\n                       column_indices <- intersect(column_indices, duration_indices)\n                   }\n\n                   row_indices <- seq_len(nrow(SummarizedExperiment::assay(SE, 1)))\n\n                   # Final SE\n                   SE <- SE[row_indices, column_indices]\n                   return(SE)\n\n               }, cell_lines = cell_lines,\n               drugs = drugs,\n               molecular.data.cells = molecular.data.cells,\n               duration = duration,\n               features = features)\n\n\n    ######\n    # SUBSET SENSITIVITY SLOT\n    ######\n    # Logic if any \"...\" arguments are passed to subsetTo\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") & length(exps) != 0) {\n        sensitivityInfo(object) <- sensitivityInfo(object)[exps, , drop=drop]\n        rownames(sensitivityInfo(object)) <- names(exps)\n        if (length(sensitivityRaw(object)) > 0) {\n            sensitivityRaw(object) <- sensitivityRaw(object)[exps, , , drop=drop]\n            dimnames(sensitivityRaw(object))[[1]] <- names(exps)\n        }\n        sensitivityProfiles(object) <- sensitivityProfiles(object)[exps, , drop=drop]\n        rownames(sensitivityProfiles(object)) <- names(exps)\n\n        sensNumber(object) <- .summarizeSensitivityNumbers(object)\n    }\n    # Logic if drug or cell parameters are passed to subsetTo\n    else if (\n        (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") &\n        (length(drugs) != 0 | length(cell_lines) != 0 | !is.null(duration) )\n    ) {\n\n        drugs_index <- which(sensitivityInfo(object)[, \"treatmentid\"] %in% drugs)\n        cell_line_index <- which(sensitivityInfo(object)[,\"sampleid\"] %in% cell_lines)\n        if (length(drugs_index) !=0 & length(cell_line_index) !=0 ) {\n            if (length(intersect(drugs_index, cell_line_index)) == 0) {\n                stop(\"This Drug - Cell Line combination was not tested together.\")\n            }\n            row_indices <- intersect(drugs_index, cell_line_index)\n        } else {\n            if(length(drugs_index)!=0 & length(cell_lines)==0) {\n                row_indices <- drugs_index\n            } else {\n                if(length(cell_line_index)!=0 & length(drugs)==0){\n                    row_indices <- cell_line_index\n                } else {\n                    # Includes all rows if cell or drug arguments are absent\n                    row_indices <- seq_len(nrow(sensitivityInfo(object)))\n                }\n            }\n        }\n        # LOGIC TO SUBSET BASED ON DURATION\n        if(!is.null(duration)){\n            if(all(!(duration %in% unique(sensitivityInfo(object)[row_indices,]$duration_h)))) {\n                # Error when other parameters are passed in\n                if(!is.null(cell_lines) | !is.null(drugs) | !is.null(molecular.data.cells)) {\n                    stop(paste0(\n                        ## TODO:: Is sample the correct way to refer to one treatment/duration combination in TGx experiments?\n                        \"There are no samples with duration of \",\n                        duration, \" in the tSet with the selected parameters.\"\n                    ))\n                } else { # Error when no other parameters are passed in\n                    stop(paste0(\n                        \"There are no samples with duration of \",\n                        duration, \" in the tSet\"\n                    ))\n                }\n            }\n            duration_indices <- which(sensitivityInfo(object)$duration_h %in% duration)\n            row_indices <- intersect(row_indices, duration_indices)\n        }\n        sensItemNames <- names(treatmentResponse(object))\n        sensitivityVals <-\n            lapply(sensItemNames, function(sensItemName, drop){\n                if (sensItemName == \"n\") {\n                    sensItem <- treatmentResponse(object)[[sensItemName]]\n                    if (!is.null(cell_lines)) {\n                        sensItem[which(rownames(sensItem) %in% cell_lines),\n                                 which(colnames(sensItem) %in% drugs), drop = drop]\n                    } else {\n                        sensItem[ , which(colnames(sensItem) %in% drugs), drop = drop]\n                    }\n                } else {\n                    sensItem <- treatmentResponse(object)[[sensItemName]]\n                    if (length(dim(sensItem)) == 3) {\n                        sensItem[row_indices, , , drop = drop]\n                    } else {\n                        sensItem[row_indices, , drop = drop]\n                    }\n                }\n            }, drop = drop)\n        names(sensitivityVals) <- sensItemNames\n        treatmentResponse(object) <- sensitivityVals\n    }\n\n    #####\n    # SUBSET DRUG SLOT\n    #####\n    if (length(drugs) == 0) {\n        if (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\"){\n            drugs <- unique(sensitivityInfo(object)[[\"treatmentid\"]])\n        }\n        if(datasetType(object) == \"perturbation\" | datasetType(object) == \"both\"){\n            drugs <- union(drugs, na.omit(.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(SummarizedExperiment::colData(SE)[[\"treatmentid\"]])}))))\n        }\n    }\n\n    #####\n    # SUBSET CELLS SLOT\n    #####\n    if (length(cell_lines) == 0) {\n        cell_lines <- union(cell_lines, na.omit(.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(SummarizedExperiment::colData(SE)[[\"sampleid\"]])}))))\n        if (datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\"){\n            cell_lines <- union(cell_lines, sensitivityInfo(object)[[\"sampleid\"]])\n        }\n    }\n    #####\n    # ASSIGN SUBSETS BACK TO TOXICOSET OBJECT\n    #####\n    treatmentInfo(object) <- treatmentInfo(object)[drugs , , drop=drop]\n    sampleInfo(object) <- sampleInfo(object)[cell_lines , , drop=drop]\n    curation(object)$treatment <- curation(object)$treatment[drugs , , drop=drop]\n    curation(object)$sample <- curation(object)$sample[cell_lines , , drop=drop]\n    curation(object)$tissue <- curation(object)$tissue[cell_lines , , drop=drop]\n    return(object)\n}\n\n#\n# END SUBSET TO FUNCTION\n#\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `subsetTo` function in the given code snippet?",
        "answer": "The `subsetTo` function is designed to subset a ToxicoSet object to data containing only specified drugs, cells, and genes. It allows for abstraction of the data to the level of biologically relevant objects (drugs and cells) and automatically ensures that only the requested treatments and cell lines are found in all slots of the ToxicoSet. This function is the preferred method of subsetting a ToxicoSet, as it simplifies the process of selecting experiments for specific treatments or cells of interest."
      },
      {
        "question": "How does the `subsetTo` function handle missing parameters?",
        "answer": "The `subsetTo` function uses a `paramMissingHandler` function to handle missing parameters. It creates a named list of default values for any missing parameters. If there are any missing parameters, their default values are assigned to the function environment using a loop. This approach allows the function to work with partial input while providing sensible defaults for unspecified parameters."
      },
      {
        "question": "What is the purpose of the `[` method defined for the ToxicoSet class in this code snippet?",
        "answer": "The `[` method defined for the ToxicoSet class provides a convenient way to subset a ToxicoSet object using either character or numeric indices. If character indices are provided for both cells and drugs, it calls the `subsetTo` function with those parameters. If numeric indices are provided (and they are integers), it translates those indices to the corresponding cell and drug names before calling `subsetTo`. This method allows for more intuitive subsetting of ToxicoSet objects, similar to how one might subset a matrix or data frame."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(`[`, \"ToxicoSet\", function(x, i, j, ..., drop = FALSE){\n    if(is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n    }\n    else if(is.numeric(i) && is.numeric(j) &&\n            (as.integer(i)==i) && (as.integer(j)==j)) {\n        return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n})",
        "complete": "setMethod(`[`, \"ToxicoSet\", function(x, i, j, ..., drop = FALSE){\n    if(is.character(i) && is.character(j)) {\n        return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n    }\n    else if(is.numeric(i) && is.numeric(j) &&\n            (as.integer(i)==i) && (as.integer(j)==j)) {\n        return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],\n                        molecular.data.cells=sampleNames(x)[i]))\n    }\n    else {\n        stop(\"Invalid input types for subsetting ToxicoSet\")\n    }\n})"
      },
      {
        "partial": "subsetTo <- function(object, cell_lines = NULL, drugs = NULL, molecular.data.cells = NULL,\n                     duration = NULL, features = NULL, ...) {\n    # Parameter handling and error checking\n    argDefaultList <- paramMissingHandler(funName = \"subsetTo\", tSet = object,\n                                         drugs = drugs, cell_lines = cell_lines,\n                                         features = features, duration = duration)\n    if (length(argDefaultList) > 0) {\n        for (idx in seq_along(argDefaultList)) {\n            assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n        }\n    }\n    paramErrorChecker(funName = \"subsetTo\", tSet = object, cell_lines = cell_lines,\n                      drugs = drugs, features = features, duration = duration)\n\n    # Subset molecular profiles\n    molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object),\n        function(SE, cell_lines, drugs, molecular.data.cells, duration, features) {\n            # Subset logic here\n        }, cell_lines = cell_lines, drugs = drugs, molecular.data.cells = molecular.data.cells,\n           duration = duration, features = features)\n\n    # Subset sensitivity slot\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") &&\n        (length(drugs) != 0 | length(cell_lines) != 0 | !is.null(duration))) {\n        # Sensitivity subsetting logic here\n    }\n\n    # Subset drug and cell slots\n    # Logic for subsetting drug and cell slots here\n\n    return(object)\n}",
        "complete": "subsetTo <- function(object, cell_lines = NULL, drugs = NULL, molecular.data.cells = NULL,\n                     duration = NULL, features = NULL, ...) {\n    # Parameter handling and error checking\n    argDefaultList <- paramMissingHandler(funName = \"subsetTo\", tSet = object,\n                                         drugs = drugs, cell_lines = cell_lines,\n                                         features = features, duration = duration)\n    if (length(argDefaultList) > 0) {\n        for (idx in seq_along(argDefaultList)) {\n            assign(names(argDefaultList)[idx], argDefaultList[[idx]])\n        }\n    }\n    paramErrorChecker(funName = \"subsetTo\", tSet = object, cell_lines = cell_lines,\n                      drugs = drugs, features = features, duration = duration)\n\n    # Subset molecular profiles\n    molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object),\n        function(SE, cell_lines, drugs, molecular.data.cells, duration, features) {\n            if (!is.null(features)) {\n                SE <- SE[which(rownames(SummarizedExperiment::rowData(SE)) %in% features), ]\n            }\n            column_indices <- getColumnIndices(SE, cell_lines, drugs, molecular.data.cells, duration)\n            SE <- SE[, column_indices]\n            return(SE)\n        }, cell_lines = cell_lines, drugs = drugs, molecular.data.cells = molecular.data.cells,\n           duration = duration, features = features)\n\n    # Subset sensitivity slot\n    if ((datasetType(object) == \"sensitivity\" | datasetType(object) == \"both\") &&\n        (length(drugs) != 0 | length(cell_lines) != 0 | !is.null(duration))) {\n        row_indices <- getSensitivityIndices(object, drugs, cell_lines, duration)\n        treatmentResponse(object) <- subsetSensitivityData(object, row_indices)\n    }\n\n    # Subset drug and cell slots\n    drugs <- if (length(drugs) == 0) getUniqueDrugs(object) else drugs\n    cell_lines <- if (length(cell_lines) == 0) getUniqueCells(object) else cell_lines\n\n    # Update object slots\n    treatmentInfo(object) <- treatmentInfo(object)[drugs, , drop = FALSE]\n    sampleInfo(object) <- sampleInfo(object)[cell_lines, , drop = FALSE]\n    curation(object)$treatment <- curation(object)$treatment[drugs, , drop = FALSE]\n    curation(object)$sample <- curation(object)$sample[cell_lines, , drop = FALSE]\n    curation(object)$tissue <- curation(object)$tissue[cell_lines, , drop = FALSE]\n\n    return(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeICn.R",
    "language": "R",
    "content": "#' Computes the ICn for any n in 0-100 for a Drug Dose Viability Curve\n#'\n#' Returns the ICn for any given nth percentile when given concentration and viability as input, normalized by the concentration\n#' range of the experiment. A Hill Slope is first fit to the data, and the ICn is inferred from the fitted curve. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known.\n#'\n#' @examples\n#' dose <- c(\"0.0025\",\"0.008\",\"0.025\",\"0.08\",\"0.25\",\"0.8\",\"2.53\",\"8\")\n#' viability <- c(\"108.67\",\"111\",\"102.16\",\"100.27\",\"90\",\"87\",\"74\",\"57\")\n#' computeIC50(dose, viability)\n#' computeICn(dose, viability, n=10)\n#'\n#' @param concentration `vector` is a vector of drug concentrations.\n#' @param viability `vector` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param Hill_fit `list or vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param n `numeric` The percentile concentration to compute. If viability_as_pct set, assumed to be percentage, otherwise\n#' assumed to be a decimal value.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(ICn) should be returned instead of ICn.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf passed in as decimal.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return a numeric value for the concentration of the nth precentile viability reduction\n#' @export\ncomputeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n\n    Hill_fit <- logLogisticRegression(conc = concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration,\n                               Hill_fit=Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  } else if (!missing(Hill_fit)){\n\n    cleanData <- sanitizeInput(conc = concentration,\n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n  } else {\n\n    stop(\"Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.\")\n\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n\n\n  n <- 1 - n\n\n  if (n < pars[2] || n > 1) {\n    return(NA_real_)\n  } else if (n == pars[2]) {\n\n    return(Inf)\n\n  } else if (n == 1) {\n\n    return(ifelse(conc_as_log, -Inf, 0))\n\n  } else {\n\n    return(ifelse(conc_as_log,\n                  log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n                  10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n\n  }\n\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeICn` function and what are its main input parameters?",
        "answer": "The `computeICn` function computes the ICn (Inhibitory Concentration) for any nth percentile of a Drug Dose Viability Curve. It takes the following main input parameters: `concentration` (a vector of drug concentrations), `viability` (a vector of corresponding viability values), `Hill_fit` (optional, parameters of a Hill Slope), `n` (the percentile concentration to compute), and several boolean flags for input format and processing options."
      },
      {
        "question": "How does the function handle different input formats for concentration and viability data?",
        "answer": "The function uses boolean flags to handle different input formats: `conc_as_log` determines if the concentration data is in log10 format or not, `viability_as_pct` specifies if viability is given as a percentage or decimal, and `trunc` indicates whether to truncate viability data between 0 and 1. These flags allow the function to adapt to various input formats and ensure correct processing of the data."
      },
      {
        "question": "What is the significance of the Hill Slope parameters in this function, and how are they obtained if not provided?",
        "answer": "The Hill Slope parameters are crucial for calculating the ICn value. If not provided directly through the `Hill_fit` parameter, the function calls `logLogisticRegression` to fit a Hill Slope to the given concentration and viability data. The Hill Slope parameters describe the shape of the dose-response curve and are used in the final calculation of the ICn value. This allows the function to work with either raw data or pre-calculated Hill Slope parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration,\n                               Hill_fit=Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration,\n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  # Complete the function from here\n}",
        "complete": "computeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration,\n                               Hill_fit=Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration,\n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  if(viability_as_pct) n <- n/100\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) return(NA_real_)\n  if (n == pars[2]) return(Inf)\n  if (n == 1) return(ifelse(conc_as_log, -Inf, 0))\n  \n  return(ifelse(conc_as_log,\n                 log10(10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1])),\n                 10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1])))\n}"
      },
      {
        "partial": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  # Complete the function from here\n}",
        "complete": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)) {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  if(viability_as_pct) n <- n/100\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) return(NA_real_)\n  if (n == pars[2]) return(Inf)\n  if (n == 1) return(ifelse(conc_as_log, -Inf, 0))\n  \n  ifelse(conc_as_log,\n         log10(10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1])),\n         10^pars[3] * ((n-1)/(pars[2]-n))^(1/pars[1]))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/columnsAsCharacter.R",
    "language": "R",
    "content": "# Change all factor columns of tSet data.frames to character\n#\n#\n# @importFrom maggitr %<>%\n# @importFrom dplyr mutate_if\ncolumnsAsCharacter <- function(tSet) {\n    for (mDataType in names(molecularProfilesSlot(tSet))) {\n        phenoInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n\n        featureInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    if (length(sensitivityInfo(tSet)) > 0) {\n        sensitivityInfo(tSet) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    sampleInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    treatmentInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    return(tSet)\n}\n\n## TODO:: Determine how to use list.filter to recursively return all data.frames in a nested lsit\n## TODO:: Write a recursive functions to return all non-list elements in a nested list\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `columnsAsCharacter` function in the given code snippet?",
        "answer": "The main purpose of the `columnsAsCharacter` function is to convert all factor columns in various data frames within a `tSet` object to character type. It iterates through different components of the `tSet` (molecular profiles, sensitivity info, sample info, and treatment info) and applies the conversion to each."
      },
      {
        "question": "What R packages and functions are being used in this code snippet, and what are their roles?",
        "answer": "The code uses functions from several R packages: 1) `maggitr` for the `%<>%` operator (pipe and assignment), 2) `dplyr` for `mutate_if` to conditionally mutate columns, 3) `tibble` for `rownames_to_column` and `column_to_rownames` to manipulate row names. These functions are used to efficiently manipulate data frames, convert factor columns to character, and handle row names during the process."
      },
      {
        "question": "How does the function handle the case where `sensitivityInfo(tSet)` is empty?",
        "answer": "The function checks the length of `sensitivityInfo(tSet)` before processing it. If the length is greater than 0, it applies the factor-to-character conversion. If `sensitivityInfo(tSet)` is empty, this step is skipped. This conditional processing prevents errors when working with `tSet` objects that may not have sensitivity information."
      }
    ],
    "completion_tasks": [
      {
        "partial": "columnsAsCharacter <- function(tSet) {\n    for (mDataType in names(molecularProfilesSlot(tSet))) {\n        phenoInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n\n        featureInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    # Complete the function by adding code to handle sensitivityInfo, sampleInfo, and treatmentInfo\n    # Return the modified tSet\n}",
        "complete": "columnsAsCharacter <- function(tSet) {\n    for (mDataType in names(molecularProfilesSlot(tSet))) {\n        phenoInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n\n        featureInfo(tSet, mDataType) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    if (length(sensitivityInfo(tSet)) > 0) {\n        sensitivityInfo(tSet) %<>%\n            tibble::rownames_to_column() %>%\n            dplyr::mutate_if(is.factor, as.character) %>%\n            tibble::column_to_rownames()\n    }\n    sampleInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    treatmentInfo(tSet) %<>%\n        tibble::rownames_to_column() %>%\n        dplyr::mutate_if(is.factor, as.character) %>%\n        tibble::column_to_rownames()\n\n    return(tSet)\n}"
      },
      {
        "partial": "# Function to recursively process nested lists\nprocessNestedList <- function(lst) {\n    # Implement the recursive function to process nested lists\n    # Convert factors to characters in data frames\n    # Return the processed list\n}",
        "complete": "# Function to recursively process nested lists\nprocessNestedList <- function(lst) {\n    if (is.data.frame(lst)) {\n        return(dplyr::mutate_if(lst, is.factor, as.character))\n    } else if (is.list(lst)) {\n        return(lapply(lst, processNestedList))\n    } else {\n        return(lst)\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/SpearmanCI.R",
    "language": "R",
    "content": "#' @title Function to compute the confidence interval for the Spearman \n#'   correelation coefficient\n#'\n#' @description\n#' This function enables to compute the confidence interval for the Spearman \n#'   correelation coefficient using the Fischer Z transformation.\n#'\n#' @usage\n#' spearmanCI(x, n, alpha = 0.05)\n#'\n#' @param x\tSpearman correlation coefficient rho.\n#' @param n\tthe sample size used to compute the Spearman rho.\n#' @param alpha\talpha level for confidence interval.\n#'\n#' @return\n#' A vector containing the lower, upper values for the confidence interval \n#'   and p-value for Spearman rho\n#'\n#' @examples\n#' spearmanCI(x=0.2, n=100, alpha=0.05)\n#'\n#' @md\n#' @export\nspearmanCI <- \nfunction (x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- zz - qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    ll <- survcomp::fisherz(ll / sqrt((n-3)/1.06), inv=TRUE)\n    uu <- zz + qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    uu <- survcomp::fisherz(uu / sqrt((n-3)/1.06), inv=TRUE)\n    pp <- pnorm(q=zz, lower.tail=x < 0)\n    res <- c(\"lower\"=ll, \"upper\"=uu, \"p.value\"=pp)\n    return(res)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `spearmanCI` function and what are its main input parameters?",
        "answer": "The `spearmanCI` function computes the confidence interval for the Spearman correlation coefficient using the Fischer Z transformation. Its main input parameters are: `x` (the Spearman correlation coefficient rho), `n` (the sample size used to compute the Spearman rho), and `alpha` (the alpha level for the confidence interval, defaulting to 0.05)."
      },
      {
        "question": "How does the function calculate the lower and upper bounds of the confidence interval?",
        "answer": "The function calculates the lower and upper bounds of the confidence interval using the following steps: 1) It applies the Fisher Z transformation to the input correlation coefficient. 2) It calculates the standard error of the transformed value. 3) It computes the lower and upper bounds using the normal distribution quantile function (`qnorm`). 4) Finally, it applies the inverse Fisher Z transformation to convert the bounds back to the correlation scale."
      },
      {
        "question": "What does the function return, and how is the p-value calculated?",
        "answer": "The function returns a vector containing three values: the lower bound of the confidence interval, the upper bound of the confidence interval, and the p-value for the Spearman rho. The p-value is calculated using the `pnorm` function, which computes the cumulative probability of the standard normal distribution. The direction of the test (lower.tail argument) is determined by whether the input correlation coefficient is negative or positive."
      }
    ],
    "completion_tasks": [
      {
        "partial": "spearmanCI <- function(x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- zz - qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    ll <- survcomp::fisherz(ll / sqrt((n-3)/1.06), inv=TRUE)\n    # Complete the function by calculating uu and pp\n    # Then return the result as a vector\n}",
        "complete": "spearmanCI <- function(x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- zz - qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    ll <- survcomp::fisherz(ll / sqrt((n-3)/1.06), inv=TRUE)\n    uu <- zz + qnorm(p=alpha / 2, lower.tail=FALSE) * zz.se\n    uu <- survcomp::fisherz(uu / sqrt((n-3)/1.06), inv=TRUE)\n    pp <- pnorm(q=zz, lower.tail=x < 0)\n    c(lower=ll, upper=uu, p.value=pp)\n}"
      },
      {
        "partial": "spearmanCI <- function(x, n, alpha=0.05) {\n    # Calculate zz and zz.se\n    # Then compute ll, uu, and pp\n    # Finally, return the result as a named vector\n}",
        "complete": "spearmanCI <- function(x, n, alpha=0.05) {\n    zz <- sqrt((n-3)/1.06) * survcomp::fisherz(x)\n    zz.se <- 1/sqrt(n - 3)\n    ll <- survcomp::fisherz((zz - qnorm(1-alpha/2) * zz.se) / sqrt((n-3)/1.06), inv=TRUE)\n    uu <- survcomp::fisherz((zz + qnorm(1-alpha/2) * zz.se) / sqrt((n-3)/1.06), inv=TRUE)\n    pp <- pnorm(zz, lower.tail=x < 0)\n    c(lower=ll, upper=uu, p.value=pp)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/rorS.R",
    "language": "R",
    "content": "#' @title Function to compute the rorS signature as published by Parker\n#'   et al 2009\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene\n#'   expression values following the algorithm used for the rorS signature as\n#'   published by Parker et al 2009.\n#'\n#' @usage\n#' rorS(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and\n#'   probes in columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping\tTRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise. Note that for Affymetrix HGU datasets, the\n#'   mapping is not necessary.\n#' @param mapping\tMatrix with columns \"EntrezGene.ID\" and \"probe\" used to\n#'   force the mapping such that the probes are not selected based on their\n#'   variance.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwis.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung,\n#'   Samuel and Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron,\n#'   Christiane and He, Xiaping and Hu, Zhiyuan and Quackenbush, John F. and\n#'   Stijleman, Inge J. and Palazzo, Juan and Marron, J.S. and Nobel,\n#'   Andrew B. and Mardis, Elaine and Nielsen, Torsten O. and Ellis,\n#'   Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009) \"Supervised\n#'   Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\", Journal of\n#'   Clinical Oncology, 27(8):1160-1167\n#'\n#' @examples\n#' # load NKI dataset\n#' data(vdxs)\n#' data(pam50)\n#'\n#' # compute relapse score\n#' rs.vdxs <- rorS(data=data.vdxs, annot=annot.vdxs, do.mapping=TRUE)\n#'\n#' @md\n#' @export\n#' @name rorS\nrorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n  ## PAM50 classification\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  ## ROR-S\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  ## rescale between 0 and 100\n  rs <- (rs.unscaled - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) / (quantile(rs.unscaled, probs=0.975, na.rm=TRUE) - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) * 100\n  rs[!is.na(rs) & rs < 0] <- 0\n  rs[!is.na(rs) & rs > 100] <- 100\n  rsrisk[rs < 29] <- \"Low\"\n  rsrisk[rs >= 29 & rs < 53] <- \"Intermediate\"\n  rsrisk[rs >= 53] <- \"High\"\n  rsrisk <- factor(rsrisk, levels=c(\"Low\", \"Intermediate\", \"High\"))\n\n\treturn(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=sbts$centroids.map))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'rorS' function and what does it return?",
        "answer": "The 'rorS' function computes the rorS signature scores and risk classifications from gene expression values, as published by Parker et al. 2009. It returns a list containing: 'score' (continuous signature scores), 'risk' (binary risk classification), 'mapping' (mapping used if necessary), and 'probe' (correspondence between gene list and gene expression data if mapping is performed)."
      },
      {
        "question": "How does the function handle the rescaling of the risk scores?",
        "answer": "The function rescales the risk scores between 0 and 100 using the following steps: 1) It calculates the 2.5th and 97.5th percentiles of the unscaled scores. 2) It applies the formula: (unscaled_score - 2.5th_percentile) / (97.5th_percentile - 2.5th_percentile) * 100. 3) Any resulting scores below 0 are set to 0, and any scores above 100 are set to 100."
      },
      {
        "question": "How are the risk categories determined in the 'rorS' function?",
        "answer": "The risk categories are determined based on the rescaled risk scores as follows: 1) Scores below 29 are classified as 'Low' risk. 2) Scores between 29 (inclusive) and 53 are classified as 'Intermediate' risk. 3) Scores of 53 and above are classified as 'High' risk. The risk categories are then converted to a factor with levels 'Low', 'Intermediate', and 'High'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  # Complete the function by rescaling rs.unscaled and setting rsrisk\n}",
        "complete": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  rs <- (rs.unscaled - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) / (quantile(rs.unscaled, probs=0.975, na.rm=TRUE) - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) * 100\n  rs[!is.na(rs) & rs < 0] <- 0\n  rs[!is.na(rs) & rs > 100] <- 100\n  rsrisk[rs < 29] <- \"Low\"\n  rsrisk[rs >= 29 & rs < 53] <- \"Intermediate\"\n  rsrisk[rs >= 53] <- \"High\"\n  rsrisk <- factor(rsrisk, levels=c(\"Low\", \"Intermediate\", \"High\"))\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=sbts$centroids.map))\n}"
      },
      {
        "partial": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  # Load pam50 data if not exists\n  # Predict intrinsic clusters\n  # Calculate mymapping\n  # Initialize rs.unscaled, rs, and rsrisk\n  # Calculate rst\n  # Assign rst to rs.unscaled\n  # Rescale rs between 0 and 100\n  # Assign risk categories\n  # Return results\n}",
        "complete": "rorS <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  if (!exists('pam50')) data(pam50, envir=environment())\n  sbts <- intrinsic.cluster.predict(sbt.model=pam50, data=data, annot=annot, do.mapping=do.mapping, verbose=FALSE)\n  mymapping <- c(\"mapped\"=nrow(sbts$centroids.map), \"total\"=nrow(pam50$centroids.map))\n  rs.unscaled <- rs <- rsrisk <- rep(NA, nrow(data))\n  names(rs.unscaled) <- names(rs) <- names(rsrisk) <- rownames(data)\n  rst <- 0.05 * sbts$cor[ , \"Basal\"] + 0.12 * sbts$cor[ , \"Her2\"] - 0.34 * sbts$cor[ , \"LumA\"] + 0.23 * sbts$cor[ , \"LumB\"]\n  rs.unscaled[names(rst)] <- rst\n  rs <- (rs.unscaled - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) / (quantile(rs.unscaled, probs=0.975, na.rm=TRUE) - quantile(rs.unscaled, probs=0.025, na.rm=TRUE)) * 100\n  rs[!is.na(rs) & rs < 0] <- 0\n  rs[!is.na(rs) & rs > 100] <- 100\n  rsrisk[rs < 29] <- \"Low\"\n  rsrisk[rs >= 29 & rs < 53] <- \"Intermediate\"\n  rsrisk[rs >= 53] <- \"High\"\n  rsrisk <- factor(rsrisk, levels=c(\"Low\", \"Intermediate\", \"High\"))\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=sbts$centroids.map))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/km.coxph.plot.R",
    "language": "R",
    "content": "'km.coxph.plot' <-\nfunction(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n\n\tif (missing(sub.title)) { sub.title <- NULL }\n\tif (missing(leg.text)) { leg.text <- NULL }\n  if (missing(weight.s)) { weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s))) }\n  ## weights should be > 0\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"weight.s\", weight.s, envir = envir)\n  weighted <- length(sort(unique(weight.s))) > 1\n\n\tng <- length(leg.text)\n    old.mar <- par(\"mar\")\n    on.exit( par( mar = old.mar ) )\n    .xaxt=\"s\"\n    .xlab=x.label\n    if (show.n.risk) {\n        par(mar = old.mar + c(ng,8,3,0))\n        .xaxt=\"n\"\n        .xlab = \"\"\n    }\n\n    plot(survfit(formula.s, data=data.s, weights=weight.s), xaxt=.xaxt, col=.col, lty=.lty, lwd=.lwd, xlab=.xlab, ylab=y.label, ... )\n    title(main.title)\n\n    if (!missing(v.line) && !is.null(v.line)) { abline(v=v.line, lty=3, col=\"purple\") }\n    if (!missing(h.line) && !is.null(h.line)) { abline(h=h.line, lty=3, col=\"purple\") }\n\n    if (!is.null(leg.text)) { legend(x=leg.pos, xjust=0, yjust=1, legend=leg.text, col=.col, lty=.lty, lwd=.lwd, cex=0.9, bg=\"white\", inset=leg.inset, bty=leg.bty) }\n    if (!is.null(sub.title)) { mtext(sub.title, line=-4, outer=TRUE) }\n    if (missing(o.text)) {\n\t\t  sdf <- summary(survival::coxph(formula.s, data=data.s, weights=weight.s))\n\t    if(verbose) { print(sdf) }\n        p.val <- sdf$sctest[\"pvalue\"]\n        o.text <- sprintf(\"Logrank P = %.1E\", p.val)\n    }\n    if (is.null(o.text)) { o.text <- FALSE }\n    text(0,0, o.text, cex=0.85, pos=4)\n\n    if (show.n.risk) {\n        usr.xy <- par( \"usr\" )\n        nrisk <- no.at.risk(formula.s=formula.s, data.s=data.s, sub.s=\"all\", t.step=n.risk.step, t.end=floor(usr.xy[2]) )\n        at.loc <- seq(0, usr.xy[2], n.risk.step)\n        axis(1, at=at.loc)\n        mtext(x.label, side=1, line=2)\n        mtext(\"No. At Risk\", side=1, line=3, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex, font=2)\n        #nrsk.lbs <- sapply( strsplit(levels(nrisk[,1]),\"=\"), FUN=function(x) x[2] )\n        #if( any(is.na(nrsk.lbs)) ) nrsk.lbs <- leg.text\n        for( i in 1:nrow(nrisk) ) {\n            mtext(leg.text[i], side=1, line=3+i, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex)\n            mtext(nrisk[i,-1], side=1, at=at.loc, line=3+i, adj=1, cex=n.risk.cex)\n       }\n    }\n\n    if( exists(\"weight.s\", envir=.GlobalEnv) ) remove(\"weight.s\", envir=.GlobalEnv)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'km.coxph.plot' function and what are its main components?",
        "answer": "The 'km.coxph.plot' function is designed to create a Kaplan-Meier survival plot with Cox proportional hazards model analysis. Its main components include:\n1. Plotting the survival curve using survfit()\n2. Adding title, labels, and legend\n3. Displaying the logrank p-value\n4. Optionally showing the number of subjects at risk\n5. Handling weighted data\n6. Customizing plot appearance (colors, line types, etc.)"
      },
      {
        "question": "How does the function handle weighted data, and what precautions are taken regarding weights?",
        "answer": "The function handles weighted data by:\n1. Accepting a 'weight.s' parameter for weights\n2. Filtering out data points with NA or non-positive weights\n3. Assigning weights to the environment\n4. Checking if the data is weighted by comparing unique weight values\n5. Using weights in the survfit() and coxph() functions\n\nPrecautions:\n- Weights must be positive\n- NA weights are removed along with corresponding data points\n- The function cleans up the 'weight.s' variable from the global environment after use"
      },
      {
        "question": "How does the function implement the 'show.n.risk' feature, and what adjustments are made to the plot layout to accommodate this?",
        "answer": "The 'show.n.risk' feature is implemented as follows:\n1. If enabled, the plot margins are adjusted to make room for the risk information\n2. The x-axis is initially hidden, and the x-label is cleared\n3. After the main plot, the function calculates the number at risk using no.at.risk()\n4. It then adds a new x-axis with custom tick locations\n5. Labels for each group and their corresponding risk numbers are added below the plot\n\nAdjustments to the plot layout:\n- Margins are increased using par(mar = old.mar + c(ng,8,3,0))\n- The x-axis label is moved to a lower position\n- Additional lines of text are added below the x-axis for the risk information"
      }
    ],
    "completion_tasks": [
      {
        "partial": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  if (missing(sub.title)) { sub.title <- NULL }\n  if (missing(leg.text)) { leg.text <- NULL }\n  if (missing(weight.s)) { weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s))) }\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"weight.s\", weight.s, envir = envir)\n  weighted <- length(sort(unique(weight.s))) > 1\n\n  ng <- length(leg.text)\n  old.mar <- par(\"mar\")\n  on.exit( par( mar = old.mar ) )\n  .xaxt=\"s\"\n  .xlab=x.label\n  if (show.n.risk) {\n    par(mar = old.mar + c(ng,8,3,0))\n    .xaxt=\"n\"\n    .xlab = \"\"\n  }\n\n  # TODO: Complete the plot function and add additional features\n\n}",
        "complete": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  if (missing(sub.title)) { sub.title <- NULL }\n  if (missing(leg.text)) { leg.text <- NULL }\n  if (missing(weight.s)) { weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s))) }\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  pos <- 1\n  envir = as.environment(pos)\n  assign(\"weight.s\", weight.s, envir = envir)\n  weighted <- length(sort(unique(weight.s))) > 1\n\n  ng <- length(leg.text)\n  old.mar <- par(\"mar\")\n  on.exit( par( mar = old.mar ) )\n  .xaxt=\"s\"\n  .xlab=x.label\n  if (show.n.risk) {\n    par(mar = old.mar + c(ng,8,3,0))\n    .xaxt=\"n\"\n    .xlab = \"\"\n  }\n\n  plot(survfit(formula.s, data=data.s, weights=weight.s), xaxt=.xaxt, col=.col, lty=.lty, lwd=.lwd, xlab=.xlab, ylab=y.label, ... )\n  title(main.title)\n\n  if (!missing(v.line) && !is.null(v.line)) { abline(v=v.line, lty=3, col=\"purple\") }\n  if (!missing(h.line) && !is.null(h.line)) { abline(h=h.line, lty=3, col=\"purple\") }\n\n  if (!is.null(leg.text)) { legend(x=leg.pos, xjust=0, yjust=1, legend=leg.text, col=.col, lty=.lty, lwd=.lwd, cex=0.9, bg=\"white\", inset=leg.inset, bty=leg.bty) }\n  if (!is.null(sub.title)) { mtext(sub.title, line=-4, outer=TRUE) }\n  if (missing(o.text)) {\n    sdf <- summary(survival::coxph(formula.s, data=data.s, weights=weight.s))\n    if(verbose) { print(sdf) }\n    p.val <- sdf$sctest[\"pvalue\"]\n    o.text <- sprintf(\"Logrank P = %.1E\", p.val)\n  }\n  if (is.null(o.text)) { o.text <- FALSE }\n  text(0,0, o.text, cex=0.85, pos=4)\n\n  if (show.n.risk) {\n    usr.xy <- par( \"usr\" )\n    nrisk <- no.at.risk(formula.s=formula.s, data.s=data.s, sub.s=\"all\", t.step=n.risk.step, t.end=floor(usr.xy[2]) )\n    at.loc <- seq(0, usr.xy[2], n.risk.step)\n    axis(1, at=at.loc)\n    mtext(x.label, side=1, line=2)\n    mtext(\"No. At Risk\", side=1, line=3, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex, font=2)\n    for( i in 1:nrow(nrisk) ) {\n      mtext(leg.text[i], side=1, line=3+i, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex)\n      mtext(nrisk[i,-1], side=1, at=at.loc, line=3+i, adj=1, cex=n.risk.cex)\n    }\n  }\n\n  if( exists(\"weight.s\", envir=.GlobalEnv) ) remove(\"weight.s\", envir=.GlobalEnv)\n}"
      },
      {
        "partial": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  # TODO: Implement data preprocessing and parameter initialization\n\n  # TODO: Set up plot parameters\n\n  # TODO: Create the main plot\n\n  # TODO: Add additional plot elements (lines, legend, text)\n\n  # TODO: Handle 'show.n.risk' functionality\n\n  # TODO: Clean up global environment\n}",
        "complete": "km.coxph.plot <- function(formula.s, data.s, weight.s, x.label, y.label, main.title, sub.title, leg.text, leg.pos=\"bottomright\", leg.bty=\"o\", leg.inset=0.05, o.text, v.line, h.line, .col=1:4, .lty=1, .lwd=1, show.n.risk=FALSE, n.risk.step, n.risk.cex=0.85, verbose=TRUE, ...) {\n  if (missing(sub.title)) sub.title <- NULL\n  if (missing(leg.text)) leg.text <- NULL\n  if (missing(weight.s)) weight.s <- array(1, dim=nrow(data.s), dimnames=list(rownames(data.s)))\n  data.s <- data.s[!is.na(weight.s) & weight.s > 0, , drop=FALSE]\n  weight.s <- weight.s[!is.na(weight.s) & weight.s > 0]\n  assign(\"weight.s\", weight.s, envir = as.environment(1))\n\n  ng <- length(leg.text)\n  old.mar <- par(\"mar\")\n  on.exit(par(mar = old.mar))\n  .xaxt <- if(show.n.risk) {\n    par(mar = old.mar + c(ng,8,3,0))\n    \"n\"\n  } else \"s\"\n  .xlab <- if(show.n.risk) \"\" else x.label\n\n  plot(survfit(formula.s, data=data.s, weights=weight.s), xaxt=.xaxt, col=.col, lty=.lty, lwd=.lwd, xlab=.xlab, ylab=y.label, ...)\n  title(main.title)\n\n  if (!missing(v.line) && !is.null(v.line)) abline(v=v.line, lty=3, col=\"purple\")\n  if (!missing(h.line) && !is.null(h.line)) abline(h=h.line, lty=3, col=\"purple\")\n\n  if (!is.null(leg.text)) legend(x=leg.pos, legend=leg.text, col=.col, lty=.lty, lwd=.lwd, cex=0.9, bg=\"white\", inset=leg.inset, bty=leg.bty)\n  if (!is.null(sub.title)) mtext(sub.title, line=-4, outer=TRUE)\n\n  if (missing(o.text)) {\n    sdf <- summary(survival::coxph(formula.s, data=data.s, weights=weight.s))\n    if(verbose) print(sdf)\n    p.val <- sdf$sctest[\"pvalue\"]\n    o.text <- sprintf(\"Logrank P = %.1E\", p.val)\n  }\n  if (!is.null(o.text)) text(0, 0, o.text, cex=0.85, pos=4)\n\n  if (show.n.risk) {\n    usr.xy <- par(\"usr\")\n    nrisk <- no.at.risk(formula.s=formula.s, data.s=data.s, sub.s=\"all\", t.step=n.risk.step, t.end=floor(usr.xy[2]))\n    at.loc <- seq(0, usr.xy[2], n.risk.step)\n    axis(1, at=at.loc)\n    mtext(x.label, side=1, line=2)\n    mtext(\"No. At Risk\", side=1, line=3, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex, font=2)\n    for(i in 1:nrow(nrisk)) {\n      mtext(leg.text[i], side=1, line=3+i, at=-0.5*n.risk.step, adj=1, cex=n.risk.cex)\n      mtext(nrisk[i,-1], side=1, at=at.loc, line=3+i, adj=1, cex=n.risk.cex)\n    }\n  }\n\n  if(exists(\"weight.s\", envir=.GlobalEnv)) remove(\"weight.s\", envir=.GlobalEnv)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/drugGeneResponseCurve.R",
    "language": "R",
    "content": "#' Compares gene expression for a specificed set of features over specific\n#'   drug dosages vs time\n#'\n#' This function generates a plot visualizing the relationship between gene\n#'   expression, time and dose level for the selected tSet. The plot is generated\n#'   with ggplot2 and can be customized using ggplot plot + function() syntax.\n#'\n#' @examples\n#'\n#' if (interactive()) {\n#'   drugGeneResponseCurve(TGGATESsmall, dose = c(\"Control\", \"Low\", \"Middle\"),\n#'   mDataTypes=\"rna\", drug = treatmentNames(TGGATESsmall)[1],\n#'   duration = c(\"2\", \"8\", \"24\"), features = \"ENSG00000002726_at\")\n#' }\n#'\n#' @param tSet \\code{ToxicoSet} A ToxicoSet to be plotted in this graph. Currently\n#'   only a single tSet is supported.\n#' @param dose \\code{character} A vector of dose levels to be included in the\n#'   plot. Default to include all dose levels available for a drug. If you specify\n#'   more than two features you may only pass in up to two dose levels.\n#' @param mDataTypes \\code{vector} A vector specifying the molecular data types to\n#'   include in this plot. Defaults to the first mDataType if not specified.ex\n#'   This release version only accepts one mDataType, more to be added in\n#'   forthcoming releases.\n#' @param features \\code{character} A vector of feature names to include in the\n#'   plot. If you specify more than two dose levels, you may only pass in up to\n#'   two features.\n#' @param drug \\code{character} A drug name to include in this plot.\n#'   See treatmentNames(tSet) for a list of options.\n#' @param duration \\code{character} A vector of durations to include in the plot.\n#' @param summarize_replicates \\code{logical} If TRUE will average viability\n#'   across replicates for each unique drug-dose-duration combination.\n#' @param cell_lines \\code{character} A vector of cell lines to include in the\n#'   plot.\n#' @param line_width \\code{numeric} A number specifying the thickness of lines\n#'   in the plot, as passed to size in geom_line(). Defaults to 1.\n#' @param point_size \\code{numeric} A number specifying how large points should\n#'   be in the plot, as passed to size in geom_point(). Defaults to 2.5.\n#' @param ggplot_args \\code{list} A list of ggplot2 functions which can be\n#'   called using the plot + function() syntax. This allows arbitrary\n#'   customization of the plot including changing the title, axis labels,\n#'   colours, etc. Please see the included examples for basic usage or ggplot2\n#'   documentation for advanced customization.\n#' @param verbose \\code{boolean} Should warning messages about the data passed\n#'   in be printed?\n#'\n#' @return Plot of the viabilities for each drug vs time of exposure\n#'\n#' @importFrom data.table data.table melt.data.table `:=`\n#' @import ggplot2\n#' @importFrom tibble as_tibble\n#'\n#' @export\ndrugGeneResponseCurve <- function(\n  tSet,\n  duration = NULL,\n  cell_lines = NULL,\n  mDataTypes = NULL,\n  features = NULL,\n  dose = NULL,\n  drug = NULL,\n  summarize_replicates = TRUE,\n  line_width = 1,\n  point_size = 2.5,\n  ggplot_args = NULL,\n  verbose=TRUE\n) {\n\n  # Place tSet in a list if not already\n  if (!is(tSet, \"list\")) { tSet <- list(tSet) }\n\n  ## Tempary warnings until function is finished\n  if (length(tSet) > 1) { stop(\"This function currently only supports one tSet per plot...\")}\n  if (length(drug) > 1) { stop(\"This function currently only supports one drug per plot...\")}\n  if (length(mDataTypes) > 1) {stop(\"This function currently only supports one molecular data type per plot...\")}\n  if (length(features) > 2) { if (length(dose) > 2) { stop(\"To plot more than one feature, please specify only up to two dose levels...\")}}\n  if (length(dose) > 2) { if (length(features) > 2) { stop(\"To plot more than one dose level, please specify up to two molecular feature...\")}}\n\n  # Deal with controls (i.e., treated with DMSO)\n  if (any(vapply(tSet, function(tSet) { name(tSet) %in% c(\"drugMatrix_rat\", \"EMEXP2458\")}, FUN.VALUE = logical(1)))) {\n    drug <- c(\"DMSO\", drug)\n  }\n\n  ## TODO:: Generalize this to work with multiple data types\n  if (missing(mDataTypes)) { mDataTypes <- names(molecularProfilesSlot(tSet[[1]])) }\n\n  if (is.null(features)) {\n    features <- lapply(tSet, function(tSet) {\n      rownames(featureInfo(tSet, \"rna\"))[seq_len(5)]\n    })\n  }\n\n  if (missing(cell_lines)) {cell_lines <- unique(phenoInfo(tSet[[1]], mDataTypes[1])$sampleid)}\n  if (length(cell_lines) > 1) { stop(\"Only one cell type per plot is currently supported...\")}\n\n  # Places features in list if not already\n  if (!is(features, \"list\")) {\n    features <- list(features)\n  }\n  names(features) <- vapply(tSet, function(x) name(x), FUN.VALUE = character(1))\n\n  # Subsetting the tSet based on parameter arguments\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({ToxicoGx::subsetTo(tSet, mDataType = mDataTypes, drugs = drug,\n                       duration = duration, features = unique(unlist(features)), cell_lines=unique(cell_lines))})\n  })\n\n  # Get only the dose levels available for that drug\n  dose <- intersect(dose, as.character(unique(phenoInfo(tSet[[1]], \"rna\")$dose_level)))\n\n  # Gather the plot data\n  plotData <- lapply(tSet, function(tSet) {\n    m <- lapply(mDataTypes, function(mDataType) {\n      mProf <- molecularProfiles(tSet, mDataType)\n      list(\n        \"data\" = data.table(\n          mProf,\n          keep.rownames = TRUE\n        ),\n        \"pInfo\" = data.table(as.data.frame(phenoInfo(tSet, mDataType)))\n      )\n    })\n    names(m) <- mDataTypes; m\n  })\n  names(plotData) <-  vapply(tSet, function(x) name(x), FUN.VALUE = character(1))\n\n  #### Assembling the plot data ####\n  d <- plotData[[1]][[1]]$data\n  pInfo <- plotData[[1]][[1]]$pInfo\n  data <- melt.data.table(d, id.vars = 1, variable.factor = FALSE)\n  colnames(data) <- c(\"feature\", \"samplename\", \"expression\")\n  if (is.numeric(data$samplename)) data[, samplename := as.character(samplename)]\n  if (is.numeric(pInfo$samplename)) pInfo[, samplename := as.character(samplename)]\n  fInfo <- data.table(as.data.frame(featureInfo(tSet[[1]], \"rna\")))\n  colnames(fInfo)[2] <- \"feature\"\n\n  plotData <- merge(data, pInfo[, .(samplename, individual_id,\n                                    treatmentid, dose_level, duration)],\n                    by = \"samplename\")\n  plotData <- merge(plotData, fInfo[, .(Symbol, feature)], by = \"feature\")\n  plotData[, dose_level := as.factor(dose_level)]\n  plotData[dose_level == \"Control\",\n                       expression := mean(expression),\n                       by = .(dose_level, duration, Symbol)]\n  max_rep <- max(plotData[dose_level != 'Control', unique(individual_id)])\n  plotData <- plotData[individual_id %in% seq_len(max_rep), .SD, by = .(dose_level, duration)]\n  plotData <- plotData[dose_level %in% dose, .SD]\n\n  #### Rendering the plot ####\n  if (summarize_replicates == FALSE) {\n    plot <- ggplot(as_tibble(plotData),\n           aes(x = as.numeric(duration),\n               y = expression,\n               color = factor(dose_level, levels=dose),\n               linetype = as.factor(individual_id),\n               shape = Symbol,\n               group = interaction(dose_level, individual_id, Symbol))) +\n      geom_line(size = line_width) +\n      geom_point(size = point_size)\n  } else {\n    plotData <- plotData[, expression := mean(expression), by = .(dose_level, duration, Symbol)][individual_id == 1]\n    plot <- ggplot(plotData,\n                   aes(as.numeric(duration),\n                       expression,\n                       color = factor(dose_level, levels=c(\"Control\", \"Low\", \"Middle\", \"High\")))) +\n      geom_line(aes(linetype = Symbol), size = line_width) +\n      geom_point(size = point_size)\n  }\n  plot <- plot +\n    labs(\n      title = paste0(\"Drug Gene Response Curve for \", paste(drug, collapse = \" & \"), \" in \", paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n      color = \"Dose Level\",\n      linetype = \"Replicate\",\n      shape = \"Feature\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    ) + xlab(\"Duration (hrs)\") +\n    ylab(\"Expression\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  if (!is.null(ggplot_args)) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `drugGeneResponseCurve` function, and what type of visualization does it generate?",
        "answer": "The `drugGeneResponseCurve` function generates a plot visualizing the relationship between gene expression, time, and dose level for a selected ToxicoSet. It creates a line plot using ggplot2 to show how gene expression changes over time for different drug dosages."
      },
      {
        "question": "How does the function handle multiple features and dose levels? What are the limitations?",
        "answer": "The function can handle multiple features and dose levels, but with limitations. If more than two features are specified, only up to two dose levels can be plotted. Conversely, if more than two dose levels are specified, only up to two features can be plotted. These limitations are enforced through error checks at the beginning of the function."
      },
      {
        "question": "What is the purpose of the `summarize_replicates` parameter, and how does it affect the plot generation?",
        "answer": "The `summarize_replicates` parameter determines whether to average viability across replicates for each unique drug-dose-duration combination. If set to TRUE (default), the function calculates the mean expression for each combination and plots a single line per dose level and feature. If FALSE, it plots individual lines for each replicate, using different line types to distinguish between replicates."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugGeneResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, mDataTypes = NULL, features = NULL, dose = NULL, drug = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, ggplot_args = NULL, verbose=TRUE) {\n  if (!is(tSet, \"list\")) { tSet <- list(tSet) }\n  if (length(tSet) > 1 || length(drug) > 1 || length(mDataTypes) > 1) { stop(\"Unsupported input\") }\n  if (length(features) > 2 && length(dose) > 2) { stop(\"Too many features or doses\") }\n  \n  # TODO: Implement the rest of the function\n}",
        "complete": "drugGeneResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, mDataTypes = NULL, features = NULL, dose = NULL, drug = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, ggplot_args = NULL, verbose=TRUE) {\n  if (!is(tSet, \"list\")) { tSet <- list(tSet) }\n  if (length(tSet) > 1 || length(drug) > 1 || length(mDataTypes) > 1) { stop(\"Unsupported input\") }\n  if (length(features) > 2 && length(dose) > 2) { stop(\"Too many features or doses\") }\n  \n  if (missing(mDataTypes)) { mDataTypes <- names(molecularProfilesSlot(tSet[[1]])) }\n  if (is.null(features)) { features <- list(rownames(featureInfo(tSet[[1]], \"rna\"))[1:5]) }\n  if (missing(cell_lines)) { cell_lines <- unique(phenoInfo(tSet[[1]], mDataTypes[1])$sampleid) }\n  if (length(cell_lines) > 1) { stop(\"Only one cell type per plot is currently supported\") }\n  \n  tSet <- lapply(tSet, function(ts) suppressWarnings(ToxicoGx::subsetTo(ts, mDataType = mDataTypes, drugs = drug, duration = duration, features = unique(unlist(features)), cell_lines = unique(cell_lines))))\n  \n  plotData <- lapply(tSet, function(ts) {\n    m <- lapply(mDataTypes, function(mdt) {\n      mProf <- molecularProfiles(ts, mdt)\n      list(\"data\" = data.table(mProf, keep.rownames = TRUE),\n           \"pInfo\" = data.table(as.data.frame(phenoInfo(ts, mdt))))\n    })\n    names(m) <- mDataTypes\n    m\n  })\n  \n  d <- plotData[[1]][[1]]$data\n  pInfo <- plotData[[1]][[1]]$pInfo\n  data <- melt.data.table(d, id.vars = 1, variable.factor = FALSE)\n  colnames(data) <- c(\"feature\", \"samplename\", \"expression\")\n  fInfo <- data.table(as.data.frame(featureInfo(tSet[[1]], \"rna\")))\n  colnames(fInfo)[2] <- \"feature\"\n  \n  plotData <- merge(data, pInfo[, .(samplename, individual_id, treatmentid, dose_level, duration)], by = \"samplename\")\n  plotData <- merge(plotData, fInfo[, .(Symbol, feature)], by = \"feature\")\n  plotData[, dose_level := as.factor(dose_level)]\n  plotData[dose_level == \"Control\", expression := mean(expression), by = .(dose_level, duration, Symbol)]\n  \n  if (summarize_replicates) {\n    plotData <- plotData[, .(expression = mean(expression)), by = .(dose_level, duration, Symbol)]\n  }\n  \n  ggplot(plotData, aes(x = as.numeric(duration), y = expression, color = dose_level)) +\n    geom_line(aes(linetype = Symbol), size = line_width) +\n    geom_point(size = point_size) +\n    labs(title = paste(\"Drug Gene Response Curve for\", drug, \"in\", cell_lines),\n         color = \"Dose Level\", linetype = \"Feature\") +\n    theme(plot.title = element_text(hjust = 0.5, size = 14)) +\n    xlab(\"Duration (hrs)\") + ylab(\"Expression\") +\n    scale_x_continuous(breaks = as.numeric(duration), labels = duration) +\n    if (!is.null(ggplot_args)) ggplot_args else list()\n}"
      },
      {
        "partial": "plotData <- function(tSet, mDataTypes, drug, duration, features, cell_lines) {\n  # TODO: Implement the function to process and prepare plot data\n}",
        "complete": "plotData <- function(tSet, mDataTypes, drug, duration, features, cell_lines) {\n  tSet <- suppressWarnings(ToxicoGx::subsetTo(tSet, mDataType = mDataTypes, drugs = drug, duration = duration, features = features, cell_lines = cell_lines))\n  \n  mProf <- molecularProfiles(tSet, mDataTypes)\n  pInfo <- phenoInfo(tSet, mDataTypes)\n  fInfo <- featureInfo(tSet, mDataTypes)\n  \n  data <- data.table(mProf, keep.rownames = TRUE)\n  data <- melt.data.table(data, id.vars = 1, variable.factor = FALSE)\n  colnames(data) <- c(\"feature\", \"samplename\", \"expression\")\n  \n  pInfo <- data.table(as.data.frame(pInfo))\n  fInfo <- data.table(as.data.frame(fInfo))\n  colnames(fInfo)[2] <- \"feature\"\n  \n  plotData <- merge(data, pInfo[, .(samplename, individual_id, treatmentid, dose_level, duration)], by = \"samplename\")\n  plotData <- merge(plotData, fInfo[, .(Symbol, feature)], by = \"feature\")\n  plotData[, dose_level := as.factor(dose_level)]\n  plotData[dose_level == \"Control\", expression := mean(expression), by = .(dose_level, duration, Symbol)]\n  \n  return(plotData)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/src/survcomp_init.cpp",
    "language": "cpp",
    "content": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\n/* FIXME:\n Check these declarations against the C/Fortran source code.\n */\n\n/* .C calls */\nextern \"C\" {\n\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n\n    /* .Call calls */\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n\n    static const R_CallMethodDef CallEntries[] = {\n        {\"get_concordanceIndex_onevariable\", (DL_FUNC) &get_concordanceIndex_onevariable, 12},\n        {\"mrmr_cIndex\",                      (DL_FUNC) &mrmr_cIndex,                       6},\n        {\"mrmr_cIndex_ensemble_remove\",      (DL_FUNC) &mrmr_cIndex_ensemble_remove,      21},\n        {NULL, NULL, 0}\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        R_registerRoutines(dll, CEntries, CallEntries, NULL, NULL);\n        R_useDynamicSymbols(dll, FALSE);\n        R_forceSymbols(dll, TRUE);\n    }\n\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'extern \"C\"' block in this code snippet?",
        "answer": "The 'extern \"C\"' block is used to prevent name mangling of the enclosed function declarations. This ensures that the C++ compiler treats these functions as having C linkage, which is necessary for proper interaction with R's C interface. It allows R to correctly locate and call these functions from the compiled library."
      },
      {
        "question": "How are R_CMethodDef and R_CallMethodDef structures used in this code, and what is their significance?",
        "answer": "R_CMethodDef and R_CallMethodDef structures are used to register C and Call methods, respectively, that can be called from R. The CEntries array registers the 'concordanceIndexC' function as a .C method, while the CallEntries array registers 'get_concordanceIndex_onevariable', 'mrmr_cIndex', and 'mrmr_cIndex_ensemble_remove' as .Call methods. These structures provide R with the necessary information to locate and invoke these functions, including their names, addresses, and the number of arguments they expect."
      },
      {
        "question": "What is the role of the R_init_survcomp function, and why is it important?",
        "answer": "The R_init_survcomp function is an initialization function for the 'survcomp' R package. It is automatically called when the package is loaded. Its primary roles are: 1) Registering the C and Call routines using R_registerRoutines, which improves performance and safety by explicitly declaring the functions and their interfaces. 2) Disabling dynamic symbol lookup with R_useDynamicSymbols(dll, FALSE), which can improve security. 3) Enforcing use of registered symbols with R_forceSymbols(dll, TRUE), which ensures that only explicitly registered functions can be called from R."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n\n    static const R_CallMethodDef CallEntries[] = {\n        // TODO: Complete the CallEntries array\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        // TODO: Complete the R_init_survcomp function\n    }\n\n}",
        "complete": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n\n    static const R_CallMethodDef CallEntries[] = {\n        {\"get_concordanceIndex_onevariable\", (DL_FUNC) &get_concordanceIndex_onevariable, 12},\n        {\"mrmr_cIndex\",                      (DL_FUNC) &mrmr_cIndex,                       6},\n        {\"mrmr_cIndex_ensemble_remove\",      (DL_FUNC) &mrmr_cIndex_ensemble_remove,      21},\n        {NULL, NULL, 0}\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        R_registerRoutines(dll, CEntries, CallEntries, NULL, NULL);\n        R_useDynamicSymbols(dll, FALSE);\n        R_forceSymbols(dll, TRUE);\n    }\n\n}"
      },
      {
        "partial": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    // Function declarations\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    // TODO: Define CEntries and CallEntries arrays\n\n    // TODO: Implement R_init_survcomp function\n\n}",
        "complete": "#include <R.h>\n#include <Rinternals.h>\n#include <stdlib.h> // for NULL\n#include <R_ext/Rdynload.h>\n\nextern \"C\" {\n\n    // Function declarations\n    void concordanceIndexC(void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *, void *);\n    SEXP get_concordanceIndex_onevariable(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n    SEXP mrmr_cIndex_ensemble_remove(SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP, SEXP);\n\n    static const R_CMethodDef CEntries[] = {\n        {\"concordanceIndexC\", (DL_FUNC) &concordanceIndexC, 16},\n        {NULL, NULL, 0}\n    };\n\n    static const R_CallMethodDef CallEntries[] = {\n        {\"get_concordanceIndex_onevariable\", (DL_FUNC) &get_concordanceIndex_onevariable, 12},\n        {\"mrmr_cIndex\",                      (DL_FUNC) &mrmr_cIndex,                       6},\n        {\"mrmr_cIndex_ensemble_remove\",      (DL_FUNC) &mrmr_cIndex_ensemble_remove,      21},\n        {NULL, NULL, 0}\n    };\n\n    void R_init_survcomp(DllInfo *dll)\n    {\n        R_registerRoutines(dll, CEntries, CallEntries, NULL, NULL);\n        R_useDynamicSymbols(dll, FALSE);\n        R_forceSymbols(dll, TRUE);\n    }\n\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/LogLogisticRegression.R",
    "language": "R",
    "content": "#' Fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points (c, E) given by the user\n#' and returns a vector containing estimates for HS, E_inf, and EC50.\n#'\n#' By default, logLogisticRegression uses an L-BFGS algorithm to generate the fit. However, if\n#' this fails to converge to solution, logLogisticRegression samples lattice points throughout the parameter space.\n#' It then uses the lattice point with minimal least-squares residual as an initial guess for the optimal parameters,\n#' passes this guess to drm, and re-attempts the optimization. If this still fails, logLogisticRegression uses the\n#' PatternSearch algorithm to fit a log-logistic curve to the data.\n#'\n#' @examples\n#' dose <- c(\"0.0025\",\"0.008\",\"0.025\",\"0.08\",\"0.25\",\"0.8\",\"2.53\",\"8\")\n#' viability <- c(\"108.67\",\"111\",\"102.16\",\"100.27\",\"90\",\"87\",\"74\",\"57\")\n#' computeAUC(dose, viability)\n#'\n#' @param conc [vector] is a vector of drug concentrations.\n#' @param viability [vector] is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of the log_conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param density [vector] is a vector of length 3 whose components are the numbers of lattice points per unit\n#' length along the HS-, E_inf-, and base-10 logarithm of the EC50-dimensions of the parameter space, respectively.\n#' @param step [vector] is a vector of length 3 whose entries are the initial step sizes in the HS, E_inf, and\n#' base-10 logarithm of the EC50 dimensions, respectively, for the PatternSearch algorithm.\n#' @param precision is a positive real number such that when the ratio of current step size to initial step\n#' size falls below it, the PatternSearch algorithm terminates. A smaller value will cause LogisticPatternSearch\n#' to take longer to complete optimization, but will produce a more accurate estimate for the fitted parameters.\n#' @param lower_bounds [vector] is a vector of length 3 whose entries are the lower bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param upper_bounds [vector] is a vector of length 3 whose entries are the upper bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param scale is a positive real number specifying the shape parameter of the Cauchy distribution.\n#' @param family [character], if \"cauchy\", uses MLE under an assumption of Cauchy-distributed errors\n#' instead of sum-of-squared-residuals as the objective function for assessing goodness-of-fit of\n#' dose-response curves to the data. Otherwise, if \"normal\", uses MLE with a gaussian assumption of errors\n#' @param median_n If the viability points being fit were medians of measurements, they are expected to follow a median of \\code{family}\n#' distribution, which is in general quite different from the case of one measurement. Median_n is the number of measurements\n#' the median was taken of. If the measurements are means of values, then both the Normal and the Cauchy distributions are stable, so means of\n#' Cauchy or Normal distributed variables are still Cauchy and normal respectively.\n#' @param conc_as_log [logical], if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(EC50) should be returned instead of EC50.\n#' @param viability_as_pct [logical], if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf should be returned as a decimal rather than a percentage.\n#' @param trunc [logical], if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose [logical], if true, causes warnings thrown by the function to be printed.\n#' @return A vector containing estimates for HS, E_inf, and EC50\n#' @export\n#' @importFrom stats optim dcauchy dnorm pcauchy rcauchy rnorm pnorm integrate\nlogLogisticRegression <- function(conc,\n                                  viability,\n                                  density = c(2, 10, 2),\n                                  step = .5 / density,\n                                  precision = 0.05,\n                                  lower_bounds = c(0, 0, -6),\n                                  upper_bounds = c(4, 1, 6),\n                                  scale = 0.07,\n                                  family = c(\"normal\", \"Cauchy\"),\n                                  median_n = 1,\n                                  conc_as_log = FALSE,\n                                  viability_as_pct = TRUE,\n                                  trunc = TRUE,\n                                  verbose = FALSE) {\n\n  family <- match.arg(family)\n\n  if (prod(is.finite(step)) != 1) {\n    message(step)\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  if (prod(is.finite(precision)) != 1) {\n    message(precision)\n    stop(\"Precision value is not a real number.\")\n  }\n\n  if (prod(is.finite(lower_bounds)) != 1) {\n    message(lower_bounds)\n    stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(upper_bounds)) != 1) {\n    message(upper_bounds)\n    stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(density)) != 1) {\n    message(density)\n    stop(\"Density vector contains elements which are not real numbers.\")\n  }\n\n  if (is.finite(scale) == FALSE) {\n    message(scale)\n    stop(\"Scale is not a real number.\")\n  }\n\n  if (is.character(family) == FALSE) {\n    message(family)\n    stop(\"Cauchy flag is not a string.\")\n  }\n\n  if (length(density) != 3){\n    stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  }\n\n  if (!median_n==as.integer(median_n)){\n    stop(\"There can only be a integral number of samples to take a median of. Check your setting of median_n parameter, it is not an integer\")\n  }\n\n\n  if (min(upper_bounds - lower_bounds) < 0) {\n    message(rbind(lower_bounds, upper_bounds))\n    stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  }\n\n\n\n  if (min(density) <= 0) {\n    message(density)\n    stop(\"Lattice point density vector contains negative values.\")\n  }\n\n  if (precision <= 0) {\n    message(precision)\n    stop(\"Negative precision value.\")\n  }\n\n  if (min(step) <= 0) {\n    message(step)\n    stop(\"Step vector contains nonpositive numbers.\")\n  }\n\n  if (scale <= 0) {\n    message(scale)\n    stop(\"Scale parameter is a nonpositive number.\")\n  }\n\n  cleanData  <- sanitizeInput(conc=conc,\n                              viability=viability,\n                              conc_as_log = conc_as_log,\n                              viability_as_pct = viability_as_pct,\n                              trunc = trunc,\n                              verbose=verbose)\n\n  log_conc <- cleanData[[\"log_conc\"]]\n  viability <- cleanData[[\"viability\"]]\n\n\n  #ATTEMPT TO REFINE GUESS WITH L-BFGS OPTIMIZATION\n  gritty_guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n                    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n                    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3]))\n  guess <- tryCatch(optim(par=gritty_guess,#par = sieve_guess,\n                          fn = function(x) {.residual(log_conc,\n                                                      viability,\n                                                      pars = x,\n                                                      n = median_n,\n                                                      scale = scale,\n                                                      family = family,\n                                                      trunc = trunc)\n                          },\n                          lower = lower_bounds,\n                          upper = upper_bounds,\n                          method = \"L-BFGS-B\",\n  ),\n  error = function(e) {\n    list(\"par\"=gritty_guess, \"convergence\"=-1)\n  })\n  failed = guess[[\"convergence\"]] != 0\n  guess <- guess[[\"par\"]]\n\n  guess_residual <- .residual(log_conc,\n                              viability,\n                              pars = guess,\n                              n = median_n,\n                              scale = scale,\n                              family = family,\n                              trunc = trunc)\n\n\n  #GENERATE INITIAL GUESS BY OBJECTIVE FUNCTION EVALUATION AT LATTICE POINTS\n  gritty_guess_residual <- .residual(log_conc,\n                                     viability,\n                                     pars = gritty_guess,\n                                     n = median_n,\n                                     scale = scale,\n                                     family = family,\n                                     trunc = trunc)\n\n\n  #CHECK SUCCESS OF L-BFGS OPTIMIZAITON AND RE-OPTIMIZE WITH A PATTERN SEARCH IF NECESSARY\n  if (failed || any(is.na(guess)) || guess_residual >= gritty_guess_residual) {\n    #GENERATE INITIAL GUESS BY OBJECTIVE FUNCTION EVALUATION AT LATTICE POINTS\n    sieve_guess <- .meshEval(log_conc,\n                             viability,\n                             lower_bounds = lower_bounds,\n                             upper_bounds = upper_bounds,\n                             density = density,\n                             n=median_n,\n                             scale = scale,\n                             family = family,\n                             trunc = trunc)\n\n    sieve_guess_residual <- .residual(log_conc,\n                                      viability,\n                                      pars = sieve_guess,\n                                      n = median_n,\n                                      scale = scale,\n                                      family = family,\n                                      trunc = trunc)\n\n    guess <- sieve_guess\n    guess_residual <- sieve_guess_residual\n    span <- 1\n\n    while (span > precision) {\n      neighbours <- rbind(guess, guess, guess, guess, guess, guess)\n      neighbour_residuals <- matrix(NA, nrow=1, ncol=6)\n      neighbours[1, 1] <- pmin(neighbours[1, 1] + span * step[1], upper_bounds[1])\n      neighbours[2, 1] <- pmax(neighbours[2, 1] - span * step[1], lower_bounds[1])\n      neighbours[3, 2] <- pmin(neighbours[3, 2] + span * step[2], upper_bounds[2])\n      neighbours[4, 2] <- pmax(neighbours[4, 2] - span * step[2], lower_bounds[2])\n      neighbours[5, 3] <- pmin(neighbours[5, 3] + span * step[3], upper_bounds[3])\n      neighbours[6, 3] <- pmax(neighbours[6, 3] - span * step[3], lower_bounds[3])\n\n      for (i in seq_len(nrow(neighbours))) {\n        neighbour_residuals[i] <- .residual(log_conc,\n                                            viability,\n                                            pars = neighbours[i, ],\n                                            n = median_n,\n                                            scale = scale,\n                                            family = family,\n                                            trunc = trunc)\n      }\n\n      if (min(neighbour_residuals) < guess_residual) {\n        guess <- neighbours[which.min(neighbour_residuals), ]\n        guess_residual <- min(neighbour_residuals)\n      } else {\n        span <- span / 2\n      }\n    }\n  }\n\n  return(list(\"HS\" = guess[1],\n              \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n              \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logLogisticRegression` function and what type of data does it process?",
        "answer": "The `logLogisticRegression` function fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points. It processes concentration (c) and viability (E) data provided by the user, and returns estimates for the Hill Slope (HS), E_inf (efficacy at infinite concentration), and EC50 (concentration for 50% effect)."
      },
      {
        "question": "How does the function handle optimization failures, and what alternative methods does it employ?",
        "answer": "The function first attempts to use an L-BFGS algorithm for optimization. If this fails to converge, it samples lattice points throughout the parameter space and uses the point with minimal least-squares residual as an initial guess. It then tries optimization again using this guess. If this still fails, the function switches to using a PatternSearch algorithm to fit the log-logistic curve to the data."
      },
      {
        "question": "What is the significance of the `family` parameter in the `logLogisticRegression` function, and how does it affect the curve fitting process?",
        "answer": "The `family` parameter determines the error distribution assumption for the curve fitting process. If set to 'cauchy', the function uses Maximum Likelihood Estimation (MLE) under an assumption of Cauchy-distributed errors. If set to 'normal', it uses MLE with a Gaussian assumption of errors. This choice affects the objective function used for assessing the goodness-of-fit of the dose-response curves to the data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 2), step = .5 / density, precision = 0.05, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = FALSE) {\n  family <- match.arg(family)\n\n  # Input validation checks\n  if (prod(is.finite(step)) != 1) {\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  # ... (other input validations)\n\n  cleanData <- sanitizeInput(conc, viability, conc_as_log, viability_as_pct, trunc, verbose)\n  log_conc <- cleanData[['log_conc']]\n  viability <- cleanData[['viability']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Attempt L-BFGS optimization\n  guess <- tryCatch(\n    optim(\n      par = gritty_guess,\n      fn = function(x) {\n        .residual(log_conc, viability, pars = x, n = median_n, scale = scale, family = family, trunc = trunc)\n      },\n      lower = lower_bounds,\n      upper = upper_bounds,\n      method = \"L-BFGS-B\"\n    ),\n    error = function(e) {\n      list(\"par\" = gritty_guess, \"convergence\" = -1)\n    }\n  )\n\n  failed <- guess[['convergence']] != 0\n  guess <- guess[['par']]\n  guess_residual <- .residual(log_conc, viability, pars = guess, n = median_n, scale = scale, family = family, trunc = trunc)\n\n  # TODO: Implement fallback methods if L-BFGS optimization fails\n\n  return(list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  ))\n}",
        "complete": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 2), step = .5 / density, precision = 0.05, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = FALSE) {\n  family <- match.arg(family)\n\n  # Input validation checks\n  if (prod(is.finite(step)) != 1) {\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  # ... (other input validations)\n\n  cleanData <- sanitizeInput(conc, viability, conc_as_log, viability_as_pct, trunc, verbose)\n  log_conc <- cleanData[['log_conc']]\n  viability <- cleanData[['viability']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Attempt L-BFGS optimization\n  guess <- tryCatch(\n    optim(\n      par = gritty_guess,\n      fn = function(x) {\n        .residual(log_conc, viability, pars = x, n = median_n, scale = scale, family = family, trunc = trunc)\n      },\n      lower = lower_bounds,\n      upper = upper_bounds,\n      method = \"L-BFGS-B\"\n    ),\n    error = function(e) {\n      list(\"par\" = gritty_guess, \"convergence\" = -1)\n    }\n  )\n\n  failed <- guess[['convergence']] != 0\n  guess <- guess[['par']]\n  guess_residual <- .residual(log_conc, viability, pars = guess, n = median_n, scale = scale, family = family, trunc = trunc)\n\n  # Fallback methods if L-BFGS optimization fails\n  if (failed || any(is.na(guess)) || guess_residual >= .residual(log_conc, viability, pars = gritty_guess, n = median_n, scale = scale, family = family, trunc = trunc)) {\n    # Generate initial guess by objective function evaluation at lattice points\n    sieve_guess <- .meshEval(log_conc, viability, lower_bounds, upper_bounds, density, n = median_n, scale = scale, family = family, trunc = trunc)\n    guess <- sieve_guess\n    guess_residual <- .residual(log_conc, viability, pars = sieve_guess, n = median_n, scale = scale, family = family, trunc = trunc)\n    \n    # Pattern search optimization\n    span <- 1\n    while (span > precision) {\n      neighbours <- rbind(guess, guess, guess, guess, guess, guess)\n      neighbour_residuals <- numeric(6)\n      \n      for (i in 1:3) {\n        neighbours[2*i-1, i] <- pmin(neighbours[2*i-1, i] + span * step[i], upper_bounds[i])\n        neighbours[2*i, i] <- pmax(neighbours[2*i, i] - span * step[i], lower_bounds[i])\n      }\n      \n      for (i in 1:6) {\n        neighbour_residuals[i] <- .residual(log_conc, viability, pars = neighbours[i,], n = median_n, scale = scale, family = family, trunc = trunc)\n      }\n      \n      if (min(neighbour_residuals) < guess_residual) {\n        guess <- neighbours[which.min(neighbour_residuals),]\n        guess_residual <- min(neighbour_residuals)\n      } else {\n        span <- span / 2\n      }\n    }\n  }\n\n  return(list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  ))\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability, conc_as_log, viability_as_pct, trunc, verbose) {\n  # TODO: Implement input sanitization\n  # Convert concentration to log scale if necessary\n  # Convert viability to decimal if necessary\n  # Truncate viability values if required\n  # Handle missing or invalid data\n  # Return a list with 'log_conc' and 'viability'\n}",
        "complete": "sanitizeInput <- function(conc, viability, conc_as_log, viability_as_pct, trunc, verbose) {\n  # Convert inputs to numeric\n  conc <- as.numeric(conc)\n  viability <- as.numeric(viability)\n\n  # Check for missing or invalid data\n  valid_indices <- !is.na(conc) & !is.na(viability) & conc > 0\n  if (sum(!valid_indices) > 0 && verbose) {\n    warning(paste(sum(!valid_indices), \"invalid or missing data points were removed.\"))\n  }\n  conc <- conc[valid_indices]\n  viability <- viability[valid_indices]\n\n  # Convert concentration to log scale if necessary\n  log_conc <- if (conc_as_log) conc else log10(conc)\n\n  # Convert viability to decimal if necessary\n  if (viability_as_pct) {\n    viability <- viability / 100\n  }\n\n  # Truncate viability values if required\n  if (trunc) {\n    viability <- pmin(pmax(viability, 0), 1)\n  }\n\n  # Check for sufficient data points\n  if (length(log_conc) < 4) {\n    stop(\"At least 4 valid data points are required for curve fitting.\")\n  }\n\n  return(list(log_conc = log_conc, viability = viability))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/zzz.R",
    "language": "R",
    "content": "# Package Start-up Functions\n\n.onAttach <- function(libname, pkgname) {\n\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\nToxicoGx package brought to you by:\n\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\nFor more of our work visit bhklab.ca!\n\nLike ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\n        \"\n        )\n        # Prevent repeated messages when loading multiple lab packages\n        options(bhklab.startup_=FALSE)\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.onAttach` function in this R package code?",
        "answer": "The `.onAttach` function is a special function in R packages that runs when the package is attached (loaded). In this case, it displays a startup message with ASCII art and information about the ToxicoGx package, but only if the R session is interactive and the startup message hasn't been shown before."
      },
      {
        "question": "How does the code ensure that the startup message is only displayed once per R session?",
        "answer": "The code uses the `options()` function to set a flag `bhklab.startup_` to FALSE after displaying the message. Before showing the message, it checks if this option is NULL. This ensures the message is only shown once per session, even if multiple packages from the same lab are loaded."
      },
      {
        "question": "What is the purpose of the `on.exit(options(oldOpts))` line in the code?",
        "answer": "The `on.exit(options(oldOpts))` line ensures that the original options are restored when the function exits, even if an error occurs. This is good practice as it prevents the function from having unintended side effects on the global options settings."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"ToxicoGx package brought to you by BHKLab\\n\\nFor more of our work visit bhklab.ca!\\n\\nLike ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\"\n        )\n        # Complete the function\n    }\n}",
        "complete": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"ToxicoGx package brought to you by BHKLab\\n\\nFor more of our work visit bhklab.ca!\\n\\nLike ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\"\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}"
      },
      {
        "partial": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        # Set options and create on.exit handler\n\n        packageStartupMessage(\n        # Add ASCII art and message here\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}",
        "complete": ".onAttach <- function(libname, pkgname) {\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\n        ToxicoGx package brought to you by:\n\n        \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n        \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n        \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n        \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n        \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n        \\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\n        For more of our work visit bhklab.ca!\n\n        Like ToxicoGx? Check out our companion web-app at ToxicoDB.ca.\n        \"\n        )\n        options(bhklab.startup_=FALSE)\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/compute.proto.cor.meta.R",
    "language": "R",
    "content": "#' @title Function to compute correlations to prototypes in a\n#'   meta-analytical framework\n#'\n#' @description\n#' This function computes meta-estimate of correlation coefficients between a set of genes\n#'   and a set of prototypes from a list of gene expression datasets.\n#'\n#' @usage\n#' compute.proto.cor.meta(datas, proto, method = c(\"pearson\", \"spearman\"))\n#'\n#' @param datas List of datasets. Each dataset is a matrix of gene expressions with samples\n#'   in rows and probes in columns, dimnames being properly defined. All the datasets must have the same probes.\n#' @param proto\tNames of prototypes (e.g. their EntrezGene ID).\n#' @param method Estimator for correlation coefficient, can be either pearson or spearman\n#'\n#' @return\n#' A list with items:\n#' -cor Matrix of meta-estimate of correlation coefficients with probes in rows and prototypes in columns.\n#' -cor.n Number of samples used to compute meta-estimate of correlation coefficients.\n#'\n#' @seealso\n#' [genefu::map.datasets]\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # load NKI dataset\n#' data(nkis)\n#' # reduce datasets\n#' ginter <- intersect(annot.vdxs[ ,\"EntrezGene.ID\"], annot.nkis[ ,\"EntrezGene.ID\"])\n#' ginter <- ginter[!is.na(ginter)][1:30]\n#' myx <- unique(c(match(ginter, annot.vdxs[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.vdxs), size=20)))\n#' data2.vdxs <- data.vdxs[ ,myx]\n#' annot2.vdxs <- annot.vdxs[myx, ]\n#' myx <- unique(c(match(ginter, annot.nkis[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.nkis), size=20)))\n#' data2.nkis <- data.nkis[ ,myx]\n#' annot2.nkis <- annot.nkis[myx, ]\n#' # mapping of datasets\n#' datas <- list(\"VDX\"=data2.vdxs,\"NKI\"=data2.nkis)\n#' annots <- list(\"VDX\"=annot2.vdxs, \"NKI\"=annot2.nkis)\n#' datas.mapped <- map.datasets(datas=datas, annots=annots, do.mapping=TRUE)\n#' # define some prototypes\n#' protos <- paste(\"geneid\", ginter[1:3], sep=\".\")\n#' # compute meta-estimate of correlation coefficients to the three prototype genes\n#' probecor <- compute.proto.cor.meta(datas=datas.mapped$datas, proto=protos,\n#'   method=\"pearson\")\n#' str(probecor)\n#'\n#' @md\n#' @importFrom survcomp combine.est fisherz\n#' @export\ncompute.proto.cor.meta <-\nfunction(datas, proto, method=c(\"pearson\", \"spearman\")) {\n\tif(!is.list(datas)) {\n\t\tif(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n\t\tdatasp <- datas[ , proto, drop=FALSE]\n\t\tdatas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n\t\tmycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n\t\tmycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n\t\tfor(i in 1:length(proto)) {\n\t\t\tmycor[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y, m) { xx <- NA; if(sum(complete.cases(x, y)) > 1) xx <- cor(x=x, y=y, method=m, use=\"complete.obs\"); return(xx); }, y=datasp[ , i], m=method)\n\t\t\tmycorn[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y) { return(sum(complete.cases(x, y))) }, y=datasp[ , i])\n\t\t}\n\t} else {\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\t\tdatast <- datasp <- NULL\n\t\tfor(k in 1:length(datas)) {\n\t\t\tif(nc != ncol(datas[[k]]) | !all(dimnames(datas[[k]])[[2]] == ncn)) { stop(\"all the datasets have not the same variables (columns)\") }\n\t\t\tif(!all(is.element(proto, dimnames(datas[[k]])[[2]]))) { stop(\"some prototypes are not in the dataset!\") }\n\t\t\tdatasp <- c(datasp, list(datas[[k]][ , proto, drop=FALSE]))\n\t\t\tdatast <- c(datast, list(datas[[k]][ , !is.element(dimnames(datas[[k]])[[2]], proto)]))\n\t\t}\n\t\tnames(datasp) <- names(datast) <- names(datas)\n\t\tdatas <- datast\n\t\trm(datast)\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\n\t\tmycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n\t\tmycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n\t\tfor(i in 1:length(proto)) {\n\t\t\tfor(j in 1:nc) {\n\t\t\t\tmycorz <- mycorz.se <- NULL\n\t\t\t\tnnt <- 0\n\t\t\t\tfor(k in 1:length(datas)) {\n\t\t\t\t\tnn <- sum(complete.cases(datas[[k]][ , j], datasp[[k]][ , i]))\n\t\t\t\t\tif(nn > 3) {\n\t\t\t\t\t\tmycorz <- c(mycorz, fisherz(cor(x=datas[[k]][ , j], y=datasp[[k]][ , i], method=method, use=\"complete.obs\"), inv=FALSE))\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, 1/sqrt(nn - 3))\n\t\t\t\t\t\tnnt <- nnt + nn\n\t\t\t\t\t} else {\n\t\t\t\t\t\tmycorz <- c(mycorz, NA)\n\t\t\t\t\t\tmycorz.se <- c(mycorz.se, NA)\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tmycor[j, i] <- fisherz(combine.est(x=mycorz,x.se=mycorz.se,na.rm=TRUE)$estimate, inv=TRUE)\n\t\t\t\tmycorn[j, i] <- nnt\n\t\t\t}\n\t\t}\n\t}\n\treturn(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `compute.proto.cor.meta` function?",
        "answer": "The main purpose of the `compute.proto.cor.meta` function is to compute meta-estimates of correlation coefficients between a set of genes and a set of prototypes from a list of gene expression datasets. It can handle both single and multiple datasets, and supports Pearson and Spearman correlation methods."
      },
      {
        "question": "How does the function handle multiple datasets differently from a single dataset?",
        "answer": "For a single dataset, the function directly computes correlations between prototypes and other genes. For multiple datasets, it first ensures all datasets have the same variables, then computes correlations for each dataset separately. It then combines these correlations using Fisher's z-transformation and meta-analysis techniques to produce a single meta-estimate of correlation for each gene-prototype pair."
      },
      {
        "question": "What are the key components of the function's return value?",
        "answer": "The function returns a list with two main components: 1) 'cor': a matrix of meta-estimates of correlation coefficients with probes in rows and prototypes in columns, and 2) 'cor.n': a matrix with the number of samples used to compute each meta-estimate of correlation coefficient, having the same dimensions as the 'cor' matrix."
      }
    ],
    "completion_tasks": [
      {
        "partial": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    if(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n    datasp <- datas[ , proto, drop=FALSE]\n    datas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n    mycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    mycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    for(i in 1:length(proto)) {\n      # Complete the code here\n    }\n  } else {\n    # Complete the code for list input\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
        "complete": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    if(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n    datasp <- datas[ , proto, drop=FALSE]\n    datas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n    mycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    mycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    for(i in 1:length(proto)) {\n      mycor[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y, m) {\n        xx <- NA\n        if(sum(complete.cases(x, y)) > 1) xx <- cor(x=x, y=y, method=m, use=\"complete.obs\")\n        return(xx)\n      }, y=datasp[ , i], m=method)\n      mycorn[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y) sum(complete.cases(x, y)), y=datasp[ , i])\n    }\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    datasp <- lapply(datas, function(d) d[ , proto, drop=FALSE])\n    datas <- lapply(datas, function(d) d[ , !is.element(dimnames(d)[[2]], proto)])\n    mycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    mycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    for(i in 1:length(proto)) {\n      for(j in 1:nc) {\n        mycorz <- mycorz.se <- numeric(length(datas))\n        nnt <- 0\n        for(k in seq_along(datas)) {\n          nn <- sum(complete.cases(datas[[k]][ , j], datasp[[k]][ , i]))\n          if(nn > 3) {\n            mycorz[k] <- fisherz(cor(x=datas[[k]][ , j], y=datasp[[k]][ , i], method=method, use=\"complete.obs\"), inv=FALSE)\n            mycorz.se[k] <- 1/sqrt(nn - 3)\n            nnt <- nnt + nn\n          }\n        }\n        mycor[j, i] <- fisherz(combine.est(x=mycorz, x.se=mycorz.se, na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[j, i] <- nnt\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}"
      },
      {
        "partial": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    # Complete the code for non-list input\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    datasp <- lapply(datas, function(d) d[ , proto, drop=FALSE])\n    datas <- lapply(datas, function(d) d[ , !is.element(dimnames(d)[[2]], proto)])\n    mycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    mycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    for(i in 1:length(proto)) {\n      for(j in 1:nc) {\n        # Complete the code here\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}",
        "complete": "compute.proto.cor.meta <- function(datas, proto, method=c(\"pearson\", \"spearman\")) {\n  if(!is.list(datas)) {\n    if(!all(is.element(proto, dimnames(datas)[[2]]))) { stop(\"prototypes are not in the dataset!\") }\n    datasp <- datas[ , proto, drop=FALSE]\n    datas <- datas[ , !is.element(dimnames(datas)[[2]], proto)]\n    mycor <- matrix(NA, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    mycorn <- matrix(0, ncol=length(proto), nrow=ncol(datas), dimnames=list(dimnames(datas)[[2]], proto))\n    for(i in 1:length(proto)) {\n      mycor[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y, m) {\n        xx <- NA\n        if(sum(complete.cases(x, y)) > 1) xx <- cor(x=x, y=y, method=m, use=\"complete.obs\")\n        return(xx)\n      }, y=datasp[ , i], m=method)\n      mycorn[ , i] <- apply(X=datas, MARGIN=2, FUN=function(x, y) sum(complete.cases(x, y)), y=datasp[ , i])\n    }\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    datasp <- lapply(datas, function(d) d[ , proto, drop=FALSE])\n    datas <- lapply(datas, function(d) d[ , !is.element(dimnames(d)[[2]], proto)])\n    mycor <- matrix(NA, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    mycorn <- matrix(0, nrow=nc, ncol=length(proto), dimnames=list(ncn, proto))\n    for(i in 1:length(proto)) {\n      for(j in 1:nc) {\n        mycorz <- mycorz.se <- numeric(length(datas))\n        nnt <- 0\n        for(k in seq_along(datas)) {\n          nn <- sum(complete.cases(datas[[k]][ , j], datasp[[k]][ , i]))\n          if(nn > 3) {\n            mycorz[k] <- fisherz(cor(x=datas[[k]][ , j], y=datasp[[k]][ , i], method=method, use=\"complete.obs\"), inv=FALSE)\n            mycorz.se[k] <- 1/sqrt(nn - 3)\n            nnt <- nnt + nn\n          }\n        }\n        mycor[j, i] <- fisherz(combine.est(x=mycorz, x.se=mycorz.se, na.rm=TRUE)$estimate, inv=TRUE)\n        mycorn[j, i] <- nnt\n      }\n    }\n  }\n  return(list(\"cor\"=mycor, \"cor.n\"=mycorn))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/logpl.R",
    "language": "R",
    "content": "`logpl` <-\nfunction(pred, surv.time, surv.event, strata, na.rm=FALSE, verbose=FALSE) {\n\n\t##############\n\t#internal function\n\t##############\n\t\n\tlogpl1 <- function(pred, surv.time, surv.event, verbose=FALSE) {\t\n\t\n\t\tn <- length(pred)\n\t\tr <- rank(surv.time)\n\t\tita <- pred\n\t\tepita <- exp(ita)\n\t\td <- rep(0, n)\n\t\tdono <- rep(0, n)\n\t\tfor(i in 1:n) {\n\t\t\td[i] <- sum(surv.event[r == r[i]])\n\t\t\tdono[i] <- sum(epita[r >= r[i]])\n\t\t}\n\t\trisk <- d/dono\n\t\trisk1 <- d/dono^{\t2}\n\t\tculrisk1 <- culrisk <- rep(0, n)\n\t\tfor(i in 1:n) {\n\t\t\tculrisk[i] <- sum(unique(risk[r <= r[i]]))\n\t\t\tculrisk1[i] <- sum(unique(risk1[r <= r[i]]))\n\t\t}\n\t\tlik <- sum((ita - log(dono)) * surv.event)\n\t\tres <- c(lik, sum(surv.event))\n\t\tnames(res) <- c(\"logpl\", \"event\")\n\t\treturn(res)\n\t}\n\t\n\t##############\n\t\n\t## remove NA values\n\tif(missing(strata)) { strata <- rep(1, length(pred)) } \n\tcc.ix <- complete.cases(surv.time, surv.event, pred, strata)\n\tsurv.time <- surv.time[cc.ix]\n\tsurv.event <- surv.event[cc.ix]\n\tpred <- pred[cc.ix]\n\tstrata <- strata[cc.ix]\n    n <- sum(cc.ix)\n    if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n    if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n    ss <- unique(strata)\n    if(length(ss) < 2) {\n    \tres <- logpl1(surv.time=surv.time, surv.event=surv.event, pred=pred, verbose=verbose)\n    }\n    else {\n    \tres1 <- 0\n    \tres2 <- 0\n    \tfor(i in 1:length(ss)) {\n    \t\tmyx <- strata == ss[i]\n    \t\trr <- logpl1(surv.time=surv.time[myx], surv.event=surv.event[myx], pred=pred[myx], verbose=verbose)\n    \t\tres1 <- res1 + rr[1]\n    \t\tres2 <- res2 + rr[2]\n    \t}\n    \tres <- c(res1, res2)\n    \tnames(res) <- c(\"logpl\", \"event\")\n    }\n    return(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logpl` function and what are its main input parameters?",
        "answer": "The `logpl` function calculates the log partial likelihood for survival data. Its main input parameters are:\n- `pred`: Predicted values or covariates\n- `surv.time`: Survival times\n- `surv.event`: Event indicators (0 for censored, 1 for event)\n- `strata`: Optional stratification variable\n- `na.rm`: Boolean to remove NA values\n- `verbose`: Boolean to print additional information"
      },
      {
        "question": "How does the function handle stratified data?",
        "answer": "The function handles stratified data by:\n1. Checking if a `strata` variable is provided\n2. If there's only one stratum, it calls `logpl1` for the entire dataset\n3. If there are multiple strata:\n   - It iterates through each unique stratum\n   - Calls `logpl1` for each stratum separately\n   - Sums up the results (log partial likelihood and number of events)\n4. This allows for different baseline hazards in each stratum while combining the overall likelihood"
      },
      {
        "question": "What is the purpose of the internal `logpl1` function and how does it calculate the log partial likelihood?",
        "answer": "The `logpl1` function calculates the log partial likelihood for a single stratum. It does this by:\n1. Ranking survival times\n2. Calculating risk sets and risk sums for each time point\n3. Computing cumulative risks\n4. Calculating the log partial likelihood as: sum((ita - log(dono)) * surv.event)\nWhere:\n- `ita` is the linear predictor (pred)\n- `dono` is the sum of exp(ita) for subjects at risk\n- `surv.event` is the event indicator\n\nIt returns both the log partial likelihood and the number of events."
      }
    ],
    "completion_tasks": [
      {
        "partial": "logpl1 <- function(pred, surv.time, surv.event, verbose=FALSE) {\n    n <- length(pred)\n    r <- rank(surv.time)\n    ita <- pred\n    epita <- exp(ita)\n    d <- rep(0, n)\n    dono <- rep(0, n)\n    for(i in 1:n) {\n        d[i] <- sum(surv.event[r == r[i]])\n        dono[i] <- sum(epita[r >= r[i]])\n    }\n    risk <- d/dono\n    risk1 <- d/dono^2\n    culrisk1 <- culrisk <- rep(0, n)\n    for(i in 1:n) {\n        culrisk[i] <- sum(unique(risk[r <= r[i]]))\n        culrisk1[i] <- sum(unique(risk1[r <= r[i]]))\n    }\n    lik <- sum((ita - log(dono)) * surv.event)\n    res <- c(lik, sum(surv.event))\n    names(res) <- c(\"logpl\", \"event\")\n    return(res)\n}",
        "complete": "logpl1 <- function(pred, surv.time, surv.event, verbose=FALSE) {\n    n <- length(pred)\n    r <- rank(surv.time)\n    epita <- exp(pred)\n    d <- tabulate(r, nbins=n)\n    dono <- rev(cumsum(rev(epita)))\n    risk <- d/dono\n    risk1 <- d/dono^2\n    culrisk <- cumsum(risk)\n    culrisk1 <- cumsum(risk1)\n    lik <- sum((pred - log(dono)) * surv.event)\n    res <- c(lik, sum(surv.event))\n    names(res) <- c(\"logpl\", \"event\")\n    return(res)\n}"
      },
      {
        "partial": "logpl <- function(pred, surv.time, surv.event, strata, na.rm=FALSE, verbose=FALSE) {\n    if(missing(strata)) { strata <- rep(1, length(pred)) } \n    cc.ix <- complete.cases(surv.time, surv.event, pred, strata)\n    surv.time <- surv.time[cc.ix]\n    surv.event <- surv.event[cc.ix]\n    pred <- pred[cc.ix]\n    strata <- strata[cc.ix]\n    n <- sum(cc.ix)\n    if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n    if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n    ss <- unique(strata)\n    if(length(ss) < 2) {\n        res <- logpl1(surv.time=surv.time, surv.event=surv.event, pred=pred, verbose=verbose)\n    }\n    else {\n        # Complete this part\n    }\n    return(res)\n}",
        "complete": "logpl <- function(pred, surv.time, surv.event, strata, na.rm=FALSE, verbose=FALSE) {\n    if(missing(strata)) { strata <- rep(1, length(pred)) } \n    cc.ix <- complete.cases(surv.time, surv.event, pred, strata)\n    surv.time <- surv.time[cc.ix]\n    surv.event <- surv.event[cc.ix]\n    pred <- pred[cc.ix]\n    strata <- strata[cc.ix]\n    n <- sum(cc.ix)\n    if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n    if(verbose) { message(sprintf(\"%i cases are removed due to NA values\",as.integer(sum(!cc.ix)))) }\n    \n    ss <- unique(strata)\n    if(length(ss) < 2) {\n        res <- logpl1(surv.time=surv.time, surv.event=surv.event, pred=pred, verbose=verbose)\n    }\n    else {\n        res <- sapply(ss, function(s) {\n            myx <- strata == s\n            logpl1(surv.time=surv.time[myx], surv.event=surv.event[myx], pred=pred[myx], verbose=verbose)\n        })\n        res <- c(sum(res[1,]), sum(res[2,]))\n        names(res) <- c(\"logpl\", \"event\")\n    }\n    return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/computeDrugSensitivity.R",
    "language": "R",
    "content": "#' @importFrom BiocParallel bpvec\n.calculateSensitivitiesStar <-\n  function (tSets = list(), exps=NULL,\n            cap=NA, na.rm=TRUE, area.type=c(\"Fitted\",\"Actual\"), nthread=1) {\n\n    # Set multicore options\n    op <- options()\n    options(mc.cores=nthread)\n    on.exit(options(op))\n\n    if (missing(area.type)) {\n      area.type <- \"Fitted\"\n    }\n    if (is.null(exps)) {\n      stop(\"expriments is empty!\")\n    }\n    for (study in names(tSets)) {\n      sensitivityProfiles(tSets[[study]])$auc_recomputed_star <- NA\n    }\n    if (!is.na(cap)) {\n      trunc <- TRUE\n    }else{\n      trunc <- FALSE\n    }\n\n    for(i in seq_len(nrow(exps))) {\n      ranges <- list()\n      for (study in names(tSets)) {\n        ranges[[study]] <- as.numeric(sensitivityRaw(tSets[[study]])[exps[i,study], ,\"Dose\"])\n      }\n      ranges <- .getCommonConcentrationRange(ranges)\n      names(ranges) <- names(tSets)\n      for(study in names(tSets)) {\n        myx <- as.numeric(sensitivityRaw(tSets[[study]])[exps[i, study],,\"Dose\"]) %in% ranges[[study]]\n        sensitivityRaw(tSets[[study]])[exps[i,study],!myx, ] <- NA\n\n      }\n    }\n    for(study in names(tSets)){\n\n      auc_recomputed_star <- unlist(bpvec(rownames(sensitivityRaw(tSets[[study]])),\n                                          function(experiment, exps, study, dataset, area.type){\n        if(!experiment %in% exps[,study]){return(NA_real_)}\n        return(computeAUC(concentration=as.numeric(dataset[experiment,,1]),\n                          viability=as.numeric(dataset[experiment,,2]),\n                          trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE, area.type=area.type)/100)\n\n\n      }, exps = exps, study = study, dataset = sensitivityRaw(tSets[[study]]), area.type=area.type))\n\n      sensitivityProfiles(tSets[[study]])$auc_recomputed_star <- auc_recomputed_star\n    }\n    return(tSets)\n  }\n\n## This function computes AUC for the whole raw sensitivity data of a pset\n.calculateFromRaw <- function(raw.sensitivity, cap=NA, nthread=1,\n                              family=c(\"normal\", \"Cauchy\"), scale = 0.07,\n                              n = 1) {\n  # Set multicore options\n  op <- options()\n  options(mc.cores=nthread)\n  on.exit(options(op))\n\n  family <- match.arg(family)\n\n  AUC <- vector(length=dim(raw.sensitivity)[1])\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n  IC50 <- vector(length=dim(raw.sensitivity)[1])\n  names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n  if (!is.na(cap)) {trunc <- TRUE}else{trunc <- FALSE}\n\n  if (nthread == 1){\n    pars <- lapply(names(AUC), function(exp, raw.sensitivity, family, scale, n) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else{\n        logLogisticRegression(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Viability\"], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE, family=family, scale=scale, median_n=n)\n      }\n    },raw.sensitivity=raw.sensitivity, family = family, scale = scale, n = n)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(lapply(names(pars), function(exp,raw.sensitivity, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeAUC(concentration=raw.sensitivity[exp, , \"Dose\"], Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    },raw.sensitivity=raw.sensitivity, pars=pars))\n    IC50 <- unlist(lapply(names(pars), function(exp, pars) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeIC50(Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    }, pars=pars))\n  } else {\n    pars <- BiocParallel::bplapply(names(AUC), function(exp, raw.sensitivity, family, scale, n, trunc) {\n      if(length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 | all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n        NA\n      } else{\n        logLogisticRegression(raw.sensitivity[exp, , \"Dose\"], raw.sensitivity[exp, , \"Viability\"], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE, family=family, scale=scale, median_n=n)\n      }\n    },raw.sensitivity=raw.sensitivity, family = family, scale = scale, n = n, trunc = trunc)\n    names(pars) <- dimnames(raw.sensitivity)[[1]]\n    AUC <- unlist(BiocParallel::bplapply(names(pars), function(exp, raw.sensitivity, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeAUC(concentration=raw.sensitivity[exp, , \"Dose\"], Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    },raw.sensitivity=raw.sensitivity, pars=pars, trunc = trunc))\n    IC50 <- unlist(BiocParallel::bplapply(names(pars), function(exp, pars, trunc) {\n      if(any(is.na(pars[[exp]]))) {\n        NA\n      } else{\n        computeIC50(Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n      }\n    }, pars=pars, trunc = trunc))\n  }\n\n  names(AUC) <- dimnames(raw.sensitivity)[[1]]\n  names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n\n  return(list(\"AUC\"=AUC, \"IC50\"=IC50, \"pars\"=pars))\n}\n\n\n## This function computes intersected concentration range between a list of concentration ranges\n.getCommonConcentrationRange <- function(doses)\n{\n  min.dose <- 0\n  max.dose <- 10^100\n  for(i in seq_along(doses))\n  {\n    min.dose <- max(min.dose, min(as.numeric(doses[[i]]), na.rm = TRUE), na.rm = TRUE)\n    max.dose <- min(max.dose, max(as.numeric(doses[[i]]), na.rm = TRUE), na.rm = TRUE)\n  }\n\n  common.ranges <- list()\n  for(i in seq_along(doses))\n  {\n    common.ranges[[i]] <- doses[[i]][\n      which.min(abs(as.numeric(doses[[i]])-min.dose)):max(\n        which(abs(as.numeric(doses[[i]]) - max.dose)==min(abs(as.numeric(doses[[i]]) - max.dose), na.rm=TRUE)))]\n  }\n  return(common.ranges)\n}\n\n## predict viability from concentration data and curve parameters\n.Hill<-function(x, pars) {\n  return(pars[2] + (1 - pars[2]) / (1 + (10 ^ x / 10 ^ pars[3]) ^ pars[1]))\n}\n\n## calculate residual of fit\n## FIXME:: Why is this different from CoreGx?\n## FIXME:: Is this the same as PharmacoGx?\n.residual <- function(x, y, n, pars, scale = 0.07, family = c(\"normal\", \"Cauchy\"), trunc = FALSE) {\n  family <- match.arg(family)\n  Cauchy_flag = (family == \"Cauchy\") # Why?!\n  if (Cauchy_flag == FALSE) {\n    diffs <- .Hill(x, pars)-y\n    if (trunc == FALSE) {\n      return(sum(-log(CoreGx::.dmednnormals(diffs, n, scale))))\n    } else {\n      down_truncated <- abs(y) >= 1\n      up_truncated <- abs(y) <= 0\n\n      # For up truncated, integrate the cauchy dist up until -diff because anything less gets truncated to 0, and thus the residual is -diff, and the prob\n      # function becomes discrete\n      # For down_truncated, 1-cdf(diffs) = cdf(-diffs)\n\n      return(sum(-log(CoreGx::.dmednnormals(diffs[!(down_truncated | up_truncated)], n, scale))) + sum(-log(CoreGx::.edmednnormals(-diffs[up_truncated | down_truncated], n, scale))))\n\n    }\n\n  } else {\n    diffs <- .Hill(x, pars)-y\n    if (trunc == FALSE) {\n      return(sum(-log(CoreGx::.dmedncauchys(diffs, n, scale))))\n    } else {\n      down_truncated <- abs(y) >= 1\n      up_truncated <- abs(y) <= 0\n\n      # For up truncated, integrate the cauchy dist up until -diff because anything less gets truncated to 0, and thus the residual is -diff, and the prob\n      # function becomes discrete\n      # For down_truncated, 1-cdf(diffs) = cdf(-diffs)\n\n      return(sum(-log(CoreGx::.dmedncauchys(diffs[!(down_truncated | up_truncated)], n, scale))) + sum(-log(CoreGx::.edmedncauchys(-diffs[up_truncated | down_truncated], n, scale))))\n    }\n  }\n}\n\n## generate an initial guess for dose-response curve parameters by evaluating\n## the residuals at different lattice points of the search space\n## FIXME:: Why is this different from CoreGx?\n## FIXME:: Is this the same as PharmacoGx?\n.meshEval<-function(log_conc,\n                    viability,\n                    lower_bounds = c(0, 0, -6),\n                    upper_bounds = c(4, 1, 6),\n                    density = c(2, 10, 2),\n                    scale = 0.07,\n                    n = 1,\n                    family = c(\"normal\", \"Cauchy\"),\n                    trunc = FALSE) {\n  family <- match.arg(family)\n  guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n             pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n             pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3]))\n  guess_residual<- .residual(log_conc,\n                             viability,\n                             pars = guess,\n                             n=n,\n                             scale = scale,\n                             family = family,\n                             trunc = trunc)\n  for (i in seq(from = lower_bounds[1], to = upper_bounds[1], by = 1 / density[1])) {\n    for (j in seq(from = lower_bounds[2], to = upper_bounds[2], by = 1 / density[2])) {\n      for (k in seq(from = lower_bounds[3], to = upper_bounds[3], by = 1 / density[3])) {\n        test_guess_residual <- .residual(log_conc,\n                                         viability,\n                                         pars = c(i, j, k),\n                                         n=n,\n                                         scale = scale,\n                                         family = family,\n                                         trunc = trunc)\n        if(!is.finite(test_guess_residual)){\n          warning(paste0(\" Test Guess Residual is: \", test_guess_residual, \"\\n Other Pars: log_conc: \", paste(log_conc, collapse=\", \"), \"\\n Viability: \", paste(viability, collapse=\", \"), \"\\n Scale: \", scale, \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \", i, \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n        }\n        if(!length(test_guess_residual)){\n          warning(paste0(\" Test Guess Residual is: \", test_guess_residual, \"\\n Other Pars: log_conc: \", paste(log_conc, collapse=\", \"), \"\\n Viability: \", paste(viability, collapse=\", \"), \"\\n Scale: \", scale, \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \", i, \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n        }\n        if (test_guess_residual < guess_residual) {\n          guess <- c(i, j, k)\n          guess_residual <- test_guess_residual\n        }\n      }\n    }\n  }\n  return(guess)\n}\n\n######## TODO ADD computationg from  being passed in params\n#  Fits dose-response curves to data given by the user\n#  and returns the AUC of the fitted curve, normalized to the length of the concentration range.\n#\n#  @param concentration [vector] is a vector of drug concentrations.\n#\n#  @param viability [vector] is a vector whose entries are the viability values observed in the presence of the\n#  drug concentrations whose logarithms are in the corresponding entries of the log_conc, expressed as percentages\n#  of viability in the absence of any drug.\n#\n#  @param trunc [logical], if true, causes viability data to be truncated to lie between 0 and 1 before\n#  curve-fitting is performed.\n.computeAUCUnderFittedCurve <- function(concentration, viability, trunc=TRUE, verbose=FALSE) {\n\n  log_conc <- concentration\n  #FIT CURVE AND CALCULATE IC50\n  pars <- unlist(logLogisticRegression(log_conc,\n                                       viability,\n                                       conc_as_log = TRUE,\n                                       viability_as_pct = FALSE,\n                                       trunc = trunc))\n  x <- CoreGx::.getSupportVec(log_conc)\n  return(1 - trapz(x, .Hill(x, pars)) / (log_conc[length(log_conc)] - log_conc[1]))\n}\n#This function is being used in computeSlope\n.optimizeRegression <- function(x, y, x0 = -3, y0 = 100)\n{\n  beta1 = (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n  return(beta1)\n}\n\nupdateMaxConc <- function(tSet) {\n  sensitivityInfo(tSet)$max.conc <- apply(sensitivityRaw(tSet)[,,\"Dose\"], 1, max, na.rm=TRUE)\n  return(tSet)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.calculateSensitivitiesStar` function and how does it handle parallel processing?",
        "answer": "The `.calculateSensitivitiesStar` function calculates sensitivity profiles for drug experiments across multiple studies. It handles parallel processing by using the `bpvec` function from the BiocParallel package. The number of threads for parallel processing is controlled by the `nthread` parameter, which is used to set the `mc.cores` option. The function also ensures that the original options are restored using `on.exit(options(op))`."
      },
      {
        "question": "How does the `.calculateFromRaw` function differ in its approach to parallel processing compared to `.calculateSensitivitiesStar`?",
        "answer": "The `.calculateFromRaw` function uses a conditional approach to parallel processing. If `nthread` is 1, it uses regular `lapply` for sequential processing. If `nthread` is greater than 1, it switches to `BiocParallel::bplapply` for parallel processing. This allows the function to adapt its behavior based on the available computational resources and user preference."
      },
      {
        "question": "What is the purpose of the `.getCommonConcentrationRange` function and how does it work?",
        "answer": "The `.getCommonConcentrationRange` function finds the overlapping concentration range across multiple studies. It iterates through the provided dose ranges to find the maximum of the minimum doses and the minimum of the maximum doses. Then, it creates a list of common ranges for each study, including only the doses that fall within this intersected range. This ensures that sensitivity calculations are performed on a consistent concentration range across all studies."
      }
    ],
    "completion_tasks": [
      {
        "partial": "updateMaxConc <- function(tSet) {\n  sensitivityInfo(tSet)$max.conc <- apply(sensitivityRaw(tSet)[,,'Dose'], 1, max, na.rm=TRUE)\n  return(tSet)\n}",
        "complete": "updateMaxConc <- function(tSet) {\n  sensitivityInfo(tSet)$max.conc <- apply(sensitivityRaw(tSet)[,,'Dose'], 1, max, na.rm=TRUE)\n  return(tSet)\n}"
      },
      {
        "partial": ".optimizeRegression <- function(x, y, x0 = -3, y0 = 100) {\n  beta1 = (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n  return(beta1)\n}",
        "complete": ".optimizeRegression <- function(x, y, x0 = -3, y0 = 100) {\n  beta1 = (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n  return(beta1)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/cvpl.R",
    "language": "R",
    "content": "`cvpl` <-\nfunction(x, surv.time, surv.event, strata, nfold=1, setseed, na.rm=FALSE, verbose=FALSE) {\n\tx <- as.data.frame(x)\n\tif(is.null(dimnames(x))) { dimnames(x) <- list(names(surv.time), \"x\") }\n\tif(missing(strata)) { strata <- rep(1, length(surv.time)) }\n\t## remove NA values\n\tcc.ix <- complete.cases(x, surv.time, surv.event, strata)\n\tsurv.time <- surv.time[cc.ix]\n\tsurv.event <- surv.event[cc.ix]\n\tx <- x[cc.ix, ,drop=FALSE]\n\tstrata <- strata[cc.ix]\n\tnr <- sum(cc.ix)\n\tif (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n\tif(verbose) { message(sprintf(\"%i cases (%i cases are removed due to NA values)\", nr, sum(!cc.ix))) }\n\n\t## k-fold cross-validation\n\tif(nfold == 1) {\n\t\tk <- 1\n\t\tnfold <- nr\n\t} else { k <- floor(nr / nfold) }\n\n\t## set the random seed to use the same data partition\n\t## for the cross-validation\n\tif (!missing(setseed)) {\n\t\tset.seed(setseed)\n\t}\n\tsmpl <- sample(nr)\n\tres.cvpl <- 0\n\tconv <- pl <- NULL\n\tdd <- data.frame(\"stime\"=surv.time, \"sevent\"=surv.event, \"strat\"=strata, x)\n\tfor (i in 1:nfold) {\n\t\t#index of samples to hold out\n\t\tif(i == nfold) { s.ix <- smpl[c(((i - 1) * k + 1):nr)] } else { s.ix <- smpl[c(((i - 1) * k + 1):(i * k))] }\n\t\t## convergence ?\n\t\t#lwa <- options(\"warn\")$warn\n\t\t#options(\"warn\"=2)\n\t\tff <- sprintf(\"Surv(stime, sevent) ~ strata(strat) + %s\", paste(dimnames(dd)[[2]][4:ncol(dd)], collapse=\" + \"))\n\t\ttry(m <- coxph(formula=formula(ff), data=dd[-s.ix, , drop=FALSE]))\n\t\t#options(\"warn\"=lwa)\n\t\tif (class(m) != \"try-error\") {\n\t\t\tconv <- c(conv, TRUE)\n\t\t\tli <- m$loglik[2]\n\t\t\tmypred <- predict(object=m, newdata=dd)\n\t\t\tl <- logpl(surv.time=dd[ , \"stime\"], surv.event=dd[ , \"sevent\"], pred=mypred, strata=dd[ , \"strat\"])[1]\n\t\t} else {\n\t\t\tconv <- c(conv, FALSE)\n\t\t\tl <- NA\n\t\t\tli <- NA\n\t\t}\n\n\t\tres.cvpl <- res.cvpl + (li - l)\n\t\tpl <- c(pl, (li - l) / length(s.ix)) # dividing by the number of events instead?\n\t}\n\tres.cvpl <- res.cvpl / nr # dividing by the number of events instead?\n\tnames(conv) <- names(pl) <- paste(rep(\"split\", nfold), 1:nfold, sep=\".\")\n\treturn (list(\"cvpl\"=res.cvpl, \"pl\"=pl, \"convergence\"=conv, \"n\"=nr))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cvpl` function and what are its main input parameters?",
        "answer": "The `cvpl` function performs cross-validated partial likelihood for Cox proportional hazards models. Its main input parameters are:\n- `x`: A data frame of predictor variables\n- `surv.time`: Survival time vector\n- `surv.event`: Event indicator vector\n- `strata`: Stratification variable (optional)\n- `nfold`: Number of folds for cross-validation (default is 1)\n- `setseed`: Seed for random number generator (optional)\n- `na.rm`: Boolean to remove NA values (default is FALSE)\n- `verbose`: Boolean to print additional information (default is FALSE)"
      },
      {
        "question": "How does the function handle missing values (NA) in the input data?",
        "answer": "The function handles missing values as follows:\n1. It identifies complete cases using `complete.cases()` function.\n2. If `na.rm` is FALSE (default) and NA values are present, it stops execution with an error message.\n3. If `na.rm` is TRUE, it removes rows with NA values from all input data (x, surv.time, surv.event, strata).\n4. If `verbose` is TRUE, it prints a message indicating the number of cases removed due to NA values."
      },
      {
        "question": "Explain the cross-validation process implemented in this function.",
        "answer": "The cross-validation process in this function works as follows:\n1. If `nfold` is 1, it performs leave-one-out cross-validation (LOOCV).\n2. Otherwise, it divides the data into `nfold` subsets.\n3. For each fold:\n   a. It holds out one subset as the test set.\n   b. Fits a Cox proportional hazards model on the remaining data.\n   c. Calculates the partial likelihood for the held-out subset.\n4. It computes the cross-validated partial likelihood (CVPL) by summing the differences between the model log-likelihood and the partial likelihood for each fold, then dividing by the total number of samples.\n5. The function returns the CVPL, individual partial likelihoods for each fold, convergence status, and the number of samples used."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cvpl <- function(x, surv.time, surv.event, strata, nfold=1, setseed, na.rm=FALSE, verbose=FALSE) {\n  x <- as.data.frame(x)\n  if(is.null(dimnames(x))) { dimnames(x) <- list(names(surv.time), \"x\") }\n  if(missing(strata)) { strata <- rep(1, length(surv.time)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, strata)\n  surv.time <- surv.time[cc.ix]\n  surv.event <- surv.event[cc.ix]\n  x <- x[cc.ix, ,drop=FALSE]\n  strata <- strata[cc.ix]\n  nr <- sum(cc.ix)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases (%i cases are removed due to NA values)\", nr, sum(!cc.ix))) }\n\n  if(nfold == 1) {\n    k <- 1\n    nfold <- nr\n  } else { k <- floor(nr / nfold) }\n\n  if (!missing(setseed)) {\n    set.seed(setseed)\n  }\n  smpl <- sample(nr)\n  res.cvpl <- 0\n  conv <- pl <- NULL\n  dd <- data.frame(\"stime\"=surv.time, \"sevent\"=surv.event, \"strat\"=strata, x)\n\n  # Complete the function from here\n}",
        "complete": "cvpl <- function(x, surv.time, surv.event, strata, nfold=1, setseed, na.rm=FALSE, verbose=FALSE) {\n  x <- as.data.frame(x)\n  if(is.null(dimnames(x))) { dimnames(x) <- list(names(surv.time), \"x\") }\n  if(missing(strata)) { strata <- rep(1, length(surv.time)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, strata)\n  surv.time <- surv.time[cc.ix]\n  surv.event <- surv.event[cc.ix]\n  x <- x[cc.ix, ,drop=FALSE]\n  strata <- strata[cc.ix]\n  nr <- sum(cc.ix)\n  if (!all(cc.ix) && !na.rm) { stop(\"NA values are present!\") }\n  if(verbose) { message(sprintf(\"%i cases (%i cases are removed due to NA values)\", nr, sum(!cc.ix))) }\n\n  if(nfold == 1) {\n    k <- 1\n    nfold <- nr\n  } else { k <- floor(nr / nfold) }\n\n  if (!missing(setseed)) {\n    set.seed(setseed)\n  }\n  smpl <- sample(nr)\n  res.cvpl <- 0\n  conv <- pl <- NULL\n  dd <- data.frame(\"stime\"=surv.time, \"sevent\"=surv.event, \"strat\"=strata, x)\n\n  for (i in 1:nfold) {\n    s.ix <- if(i == nfold) smpl[((i - 1) * k + 1):nr] else smpl[((i - 1) * k + 1):(i * k)]\n    ff <- sprintf(\"Surv(stime, sevent) ~ strata(strat) + %s\", paste(names(dd)[4:ncol(dd)], collapse=\" + \"))\n    m <- try(coxph(formula=formula(ff), data=dd[-s.ix, , drop=FALSE]))\n    if (!inherits(m, \"try-error\")) {\n      conv <- c(conv, TRUE)\n      li <- m$loglik[2]\n      mypred <- predict(object=m, newdata=dd)\n      l <- logpl(surv.time=dd[, \"stime\"], surv.event=dd[, \"sevent\"], pred=mypred, strata=dd[, \"strat\"])[1]\n    } else {\n      conv <- c(conv, FALSE)\n      l <- li <- NA\n    }\n    res.cvpl <- res.cvpl + (li - l)\n    pl <- c(pl, (li - l) / length(s.ix))\n  }\n  res.cvpl <- res.cvpl / nr\n  names(conv) <- names(pl) <- paste(\"split\", 1:nfold, sep=\".\")\n  list(cvpl=res.cvpl, pl=pl, convergence=conv, n=nr)\n}"
      },
      {
        "partial": "logpl <- function(surv.time, surv.event, pred, strata) {\n  # Implement the logpl function here\n}",
        "complete": "logpl <- function(surv.time, surv.event, pred, strata) {\n  ord <- order(surv.time)\n  surv.time <- surv.time[ord]\n  surv.event <- surv.event[ord]\n  pred <- pred[ord]\n  strata <- strata[ord]\n  ustrata <- unique(strata)\n  res <- 0\n  for (s in ustrata) {\n    ix <- which(strata == s)\n    ti <- surv.time[ix]\n    di <- surv.event[ix]\n    ri <- pred[ix]\n    ixe <- which(di == 1)\n    res <- res + sum(ri[ixe] - log(cumsum(exp(rev(ri)))[length(ri) - ixe + 1]))\n  }\n  c(res, length(ustrata))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/score2proba.R",
    "language": "R",
    "content": "`score2proba` <-\nfunction(data.tr, score, yr, method=c(\"cox\", \"prodlim\"), conf.int=0.95, which.est=c(\"point\", \"lower\", \"upper\")) {\n\tmethod <- match.arg(method)\n\twhich.est <- match.arg(which.est)\n\tcc.ix <- complete.cases(score)\n\tscore2 <- score[cc.ix]\n\tpred <- rep(NA, length(score))\n\tnames(pred) <- names(score)\n\tswitch(method,\n\t\"cox\"={\n\t\tpredm <- coxph(Surv(time, event) ~ score, data=data.tr)\n\t\tsf <- survfit(predm, newdata=data.frame(\"score\"=score2), conf.int=conf.int)\n\t\tpred[cc.ix] <- getsurv2(sf=sf, time=yr, which.est=which.est)\n\t},\n\t\"prodlim\"={\n\t\t#require(prodlim)\n\t\t#require(KernSmooth)\n\t\tif(which.est != \"point\") { stop(\"not implemented yet!\") }\n\t\tpredm <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr, conf.int=conf.int)\n\t\tpred[cc.ix] <- unlist(predict(predm, newdata=data.frame(\"score\"=score2), times=yr))\n\t})\n\treturn(pred)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `score2proba` function in R?",
        "answer": "The `score2proba` function is designed to convert risk scores into survival probabilities at a specified time point. It uses either Cox proportional hazards model or product-limit estimation method to calculate these probabilities based on the input data, scores, and specified time point."
      },
      {
        "question": "How does the function handle missing data in the input score vector?",
        "answer": "The function handles missing data by using `complete.cases(score)` to identify non-missing values. It performs calculations only on the complete cases and then assigns the results back to the original vector, preserving NA values for missing data points. This ensures that the output vector has the same length and structure as the input score vector."
      },
      {
        "question": "What is the difference between the 'cox' and 'prodlim' methods in this function?",
        "answer": "The 'cox' method uses the Cox proportional hazards model (coxph) and survfit functions to estimate survival probabilities. It allows for point estimates as well as lower and upper confidence intervals. The 'prodlim' method uses the product-limit estimation (prodlim function) and currently only supports point estimates. The 'prodlim' method also uses kernel smoothing for its calculations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "score2proba <- function(data.tr, score, yr, method=c(\"cox\", \"prodlim\"), conf.int=0.95, which.est=c(\"point\", \"lower\", \"upper\")) {\n  method <- match.arg(method)\n  which.est <- match.arg(which.est)\n  cc.ix <- complete.cases(score)\n  score2 <- score[cc.ix]\n  pred <- rep(NA, length(score))\n  names(pred) <- names(score)\n  switch(method,\n    \"cox\"={\n      # Complete the cox method implementation\n    },\n    \"prodlim\"={\n      # Complete the prodlim method implementation\n    })\n  return(pred)\n}",
        "complete": "score2proba <- function(data.tr, score, yr, method=c(\"cox\", \"prodlim\"), conf.int=0.95, which.est=c(\"point\", \"lower\", \"upper\")) {\n  method <- match.arg(method)\n  which.est <- match.arg(which.est)\n  cc.ix <- complete.cases(score)\n  score2 <- score[cc.ix]\n  pred <- rep(NA, length(score))\n  names(pred) <- names(score)\n  switch(method,\n    \"cox\"={\n      predm <- coxph(Surv(time, event) ~ score, data=data.tr)\n      sf <- survfit(predm, newdata=data.frame(\"score\"=score2), conf.int=conf.int)\n      pred[cc.ix] <- getsurv2(sf=sf, time=yr, which.est=which.est)\n    },\n    \"prodlim\"={\n      if(which.est != \"point\") stop(\"not implemented yet!\")\n      predm <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr, conf.int=conf.int)\n      pred[cc.ix] <- unlist(predict(predm, newdata=data.frame(\"score\"=score2), times=yr))\n    })\n  return(pred)\n}"
      },
      {
        "partial": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  # Complete the function implementation\n}",
        "complete": "getsurv2 <- function(sf, time, which.est=c(\"point\", \"lower\", \"upper\")) {\n  which.est <- match.arg(which.est)\n  ix <- sapply(time, function(t) which.min(abs(sf$time - t)))\n  switch(which.est,\n    \"point\" = sf$surv[ix],\n    \"lower\" = sf$lower[ix],\n    \"upper\" = sf$upper[ix])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/stab.fs.ranking.R",
    "language": "R",
    "content": "#' @title Function to quantify stability of feature ranking\n#'\n#' @description\n#' This function computes several indexes to quantify feature ranking\n#'   stability for several number of selected features. This is usually\n#'   estimated through perturbation of the original dataset by generating\n#'   multiple sets of selected features.\n#'\n#' @usage\n#' stab.fs.ranking(fsets, sizes, N, method = c(\"kuncheva\", \"davis\"), ...)\n#'\n#' @param fsets\tlist or matrix of sets of selected features (in rows),\n#'   each ranking must have the same size.\n#' @param sizes\tNumber of top-ranked features for which the stability\n#'   index must be computed.\n#' @param N\ttotal number of features on which feature selection is performed\n#' @param method\tstability index (see details section).\n#' @param ...\tadditional parameters passed to stability index (penalty\n#'   that is a numeric for Davis' stability index, see details section).\n#'\n#' @details\n#' Stability indices may use different parameters. In this version only the\n#'   Davis index requires an additional parameter that is penalty, a numeric\n#'   value used as penalty term.\n#' Kuncheva index (kuncheva) lays in \\[-1, 1\\], An index of -1 means no\n#'   intersection between sets of selected features, +1 means that all the\n#'   same features are always selected and 0 is the expected stability of a\n#'   random feature selection.\n#' Davis index (davis) lays in \\[0,1\\], With a penalty term equal to 0, an index\n#'   of 0 means no intersection between sets of selected features and +1 means\n#'   that all the same features are always selected. A penalty of 1 is usually\n#'   used so that a feature selection performed with no or all features has a\n#'   Davis stability index equals to 0. None estimate of the expected Davis\n#'   stability index of a random feature selection was published.\n#'\n#' @return\n#' A vector of numeric that are stability indices for each size of the sets\n#'   of selected features given the rankings.\n#'\n#' @references\n#' Davis CA, Gerick F, Hintermair V, Friedel CC, Fundel K, Kuffner R,\n#'   Zimmer R (2006) \"Reliable gene signatures for microarray classification:\n#'  assessment of stability and performance\", Bioinformatics, 22(19):356-2363.\n#' Kuncheva LI (2007) \"A stability index for feature selection\", AIAP'07:\n#'   Proceedings of the 25th conference on Proceedings of the 25th IASTED\n#'   International Multi-Conference, pages 390-395.\n#'\n#' @seealso\n#' [genefu::stab.fs]\n#'\n#' @examples\n#' # 100 random selection of 50 features from a set of 10,000 features\n#' fsets <- lapply(as.list(1:100), function(x, size=50, N=10000) {\n#'   return(sample(1:N, size, replace=FALSE))} )\n#' names(fsets) <- paste(\"fsel\", 1:length(fsets), sep=\".\")\n#'\n#' # Kuncheva index\n#' stab.fs.ranking(fsets=fsets, sizes=c(1, 10, 20, 30, 40, 50),\n#'   N=10000, method=\"kuncheva\")\n#' # close to 0 as expected for a random feature selection\n#'\n#' # Davis index\n#' stab.fs.ranking(fsets=fsets, sizes=c(1, 10, 20, 30, 40, 50),\n#'   N=10000, method=\"davis\", penalty=1)\n#'\n#' @md\n#' @export\nstab.fs.ranking <-\nfunction(fsets, sizes, N, method=c(\"kuncheva\", \"davis\"), ...) {\n\n\t####################\n\t## internal functions\n\t####################\n\n\tkuncheva.stab.ranking <- function(fsets, N, x) {\n\t\tss <- x\n\t\tfsets <- fsets[ , 1:ss, drop=FALSE]\n\t\tkk <- nrow(fsets)\n\t\tKI <- function(f1, f2, ss, NN) {\n\t\t\t#if(length(f1) != length(f2)) { stop(\"length of the two sets of selected features must be identical!\") }\n\t\t\t#ss <- length(f1)\n\t\t\tif(ss == NN) { return(NA) }\n\t\t\trr <- length(intersect(f1, f2))\n\t\t\tki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n\t\t\treturn(ki.est)\n\t\t}\n\n\t\tstab.res <- 0\n\t\tfor(i in 1:(kk - 1)) {\n\t\t\tfor(j in (i + 1):kk) {\n\t\t\t\tstab.res <- stab.res + KI(f1=fsets[i, ], f2=fsets[j, ], ss=ss, NN=N)\n\t\t\t}\n\t\t}\n\t\treturn((2 * stab.res) / (kk * (kk - 1)))\n\t}\n\n\tdavis.stab.ranking <- function(fsets, N, x, penalty=1) {\n\t\tss <- x\n\t\tfsets <- fsets[ , 1:ss, drop=FALSE]\n\t\tkk <- nrow(fsets)\n\t\tstab.res <- sum(sort(table(fsets), decreasing=TRUE)[1:ss]) / (kk * ss)\n\t\treturn(stab.res - penalty * (ss / N))\n\t}\n\n\t####################\n\n\tmethod <- match.arg(method)\n\tif(is.list(fsets)) { ## transform list into matrix\n\t\tNn <- unique(unlist(lapply(fsets, length)))\n\t\tif(length(Nn) > 1) { stop(\"length of sets of selected features must be identical!\") }\n\t\tnam <- names(fsets)\n\t\tfsets <- t(sapply(X=1:length(fsets), FUN=function(y, x) { return(y[[x]]) }, y=fsets))\n\t\tdimnames(fsets) <- list(nam, paste(\"rank\", 1:Nn, sep=\".\"))\n\t} else { Nn <- ncol(fsets) }\n\tif(missing(N)) { N <- Nn }\n\tif(missing(sizes)) { sizes <- 1:Nn }\n\tsizes <- sizes[sizes <= Nn]\n\n\tswitch(method,\n\t\t\"kuncheva\"={\n\t\t\tstab <- unlist(sapply(X=sizes, FUN=kuncheva.stab.ranking, fsets=fsets, N=N))\n\t\t},\n\t\t\"davis\"={\n\t\t\t stab <- unlist(sapply(X=sizes, FUN=davis.stab.ranking, fsets=fsets, N=N, ...))\n\t\t})\n\t\tnames(stab) <- paste(\"size\", sizes, sep=\".\")\n\t\treturn(stab)\n}\n\n## k <- 100; fsets <- NULL; for(i in 1:k) { fsets <- c(fsets, list(sample(paste(\"feature\", 1:1000, sep=\".\")))) }; names(fsets) <- paste(\"rand\", 1:k, sep=\".\")\n## stab.fs.ranking(fsets=fsets, sizes=1:10, method=\"kuncheva\", penalty=1)",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `stab.fs.ranking` function and what are its main input parameters?",
        "answer": "The `stab.fs.ranking` function quantifies the stability of feature ranking. It computes stability indexes for several numbers of selected features, typically estimated through perturbation of the original dataset. The main input parameters are:\n- `fsets`: A list or matrix of sets of selected features\n- `sizes`: Number of top-ranked features for which the stability index must be computed\n- `N`: Total number of features on which feature selection is performed\n- `method`: Stability index to use (either 'kuncheva' or 'davis')"
      },
      {
        "question": "Explain the difference between the Kuncheva index and the Davis index as implemented in this function.",
        "answer": "The Kuncheva and Davis indices are two different methods for measuring feature ranking stability:\n\n1. Kuncheva index:\n   - Range: [-1, 1]\n   - -1 means no intersection between sets of selected features\n   - +1 means all the same features are always selected\n   - 0 is the expected stability of a random feature selection\n\n2. Davis index:\n   - Range: [0, 1]\n   - With a penalty term of 0, 0 means no intersection between sets of selected features\n   - +1 means all the same features are always selected\n   - Typically uses a penalty of 1 so that feature selection with no or all features has a Davis stability index of 0\n   - No published estimate of the expected Davis stability index for random feature selection"
      },
      {
        "question": "How does the function handle different input types for the `fsets` parameter, and what internal data transformation does it perform?",
        "answer": "The function handles different input types for the `fsets` parameter as follows:\n\n1. If `fsets` is a list:\n   - It checks if all sets have the same length\n   - Transforms the list into a matrix using `t(sapply(...))`\n   - Preserves the names of the list as row names in the matrix\n   - Creates column names as 'rank.1', 'rank.2', etc.\n\n2. If `fsets` is already a matrix:\n   - It uses the matrix as-is\n\nIn both cases, the function determines `Nn` (number of features in each set) and uses it to set `N` (total number of features) if not provided. The resulting matrix is then used in the stability index calculations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "stab.fs.ranking <- function(fsets, sizes, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  method <- match.arg(method)\n  if(is.list(fsets)) {\n    Nn <- unique(unlist(lapply(fsets, length)))\n    if(length(Nn) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    nam <- names(fsets)\n    fsets <- t(sapply(X=1:length(fsets), FUN=function(y, x) { return(y[[x]]) }, y=fsets))\n    dimnames(fsets) <- list(nam, paste(\"rank\", 1:Nn, sep=\".\"))\n  } else { Nn <- ncol(fsets) }\n  if(missing(N)) { N <- Nn }\n  if(missing(sizes)) { sizes <- 1:Nn }\n  sizes <- sizes[sizes <= Nn]\n\n  # Complete the function here\n}",
        "complete": "stab.fs.ranking <- function(fsets, sizes, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  method <- match.arg(method)\n  if(is.list(fsets)) {\n    Nn <- unique(unlist(lapply(fsets, length)))\n    if(length(Nn) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    nam <- names(fsets)\n    fsets <- t(sapply(X=1:length(fsets), FUN=function(y, x) { return(y[[x]]) }, y=fsets))\n    dimnames(fsets) <- list(nam, paste(\"rank\", 1:Nn, sep=\".\"))\n  } else { Nn <- ncol(fsets) }\n  if(missing(N)) { N <- Nn }\n  if(missing(sizes)) { sizes <- 1:Nn }\n  sizes <- sizes[sizes <= Nn]\n\n  stab <- switch(method,\n    \"kuncheva\" = sapply(sizes, kuncheva.stab.ranking, fsets=fsets, N=N),\n    \"davis\" = sapply(sizes, davis.stab.ranking, fsets=fsets, N=N, ...)\n  )\n  names(stab) <- paste(\"size\", sizes, sep=\".\")\n  return(stab)\n}"
      },
      {
        "partial": "kuncheva.stab.ranking <- function(fsets, N, x) {\n  ss <- x\n  fsets <- fsets[, 1:ss, drop=FALSE]\n  kk <- nrow(fsets)\n  KI <- function(f1, f2, ss, NN) {\n    if(ss == NN) { return(NA) }\n    rr <- length(intersect(f1, f2))\n    ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n    return(ki.est)\n  }\n\n  # Complete the function here\n}",
        "complete": "kuncheva.stab.ranking <- function(fsets, N, x) {\n  ss <- x\n  fsets <- fsets[, 1:ss, drop=FALSE]\n  kk <- nrow(fsets)\n  KI <- function(f1, f2, ss, NN) {\n    if(ss == NN) { return(NA) }\n    rr <- length(intersect(f1, f2))\n    ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n    return(ki.est)\n  }\n\n  stab.res <- sum(sapply(1:(kk-1), function(i) {\n    sapply((i+1):kk, function(j) KI(fsets[i,], fsets[j,], ss, N))\n  }))\n  return((2 * stab.res) / (kk * (kk - 1)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/ToxicoSet-accessors.R",
    "language": "R",
    "content": "#' @include ToxicoSet-class.R\nNULL\n\n# Navigating this file:\n# - Slot section names start with ----\n# - Method section names start with ==\n#\n# As a result, you can use Ctrl + f to find the slot or method you are looking\n# for quickly, assuming you know its name.\n#\n# For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n# method, while Ctrl + f '---- molecularProfiles' would take you to the slot\n# section.\n\n\n#### CoreGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n#' @name ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_accessors(class_='CoreSet')\n#' @eval CoreGx:::.parseToRoxygen(\"@examples data({data_})\", data_=.local_data)\nNULL\n\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ==============\n## ---- drug slot\n## --------------\n\n\n##\n## == drugInfo\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo\n#' @aliases drugInfo\n#' @export\ndrugInfo <- function(...) treatmentInfo(...)\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo<-\n#' @aliases drugInfo<-\n#' @export\n`drugInfo<-` <- function(..., value) `treatmentInfo<-`(..., value=value)\n\n\n\n##\n## == drugNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames\n#' @aliases drugNames\n#' @export\ndrugNames <- function(...) treatmentNames(...)\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames<-\n#' @aliases drugNames<-\n#' @export\n`drugNames<-` <- function(..., value) `treatmentNames<-`(..., value=value)\n\n\n## ====================\n## ---- annotation slot\n## --------------------\n\n\n##\n## == annotation\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation\nsetMethod('annotation', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation<-\nsetReplaceMethod(\"annotation\", signature(\"ToxicoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == dateCreated\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated\nsetMethod('dateCreated', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated<-\nsetReplaceMethod('dateCreated', signature(object=\"ToxicoSet\", value=\"character\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## === name\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name\nsetMethod('name', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name<-\nsetReplaceMethod('name', signature(\"ToxicoSet\"), function(object, value) {\n    object <- callNextMethod(object, value=value)\n    return(invisible(object))\n})\n\n## ==============\n## ---- sample slot\n## --------------\n\n\n##\n## == sampleInfo\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleInfo(class_=.local_class,\n#' sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleInfo\n#' @importFrom CoreGx cellInfo\n#' @export\nsetMethod(\"sampleInfo\", \"ToxicoSet\", function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleInfo(class_=.local_class,\n#' data_=.local_data, sample_=\"cell\")\n#' @importMethodsFrom CoreGx sampleInfo<-\n#' @importFrom CoreGx cellInfo<-\n#' @export\nsetReplaceMethod(\"sampleInfo\", signature(object=\"ToxicoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\n\n##\n## == sampleNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames\nsetMethod(\"sampleNames\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames<-\nsetReplaceMethod(\"sampleNames\", signature(object=\"ToxicoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n\n## ------------------\n## ---- curation slot\n\n\n##\n## == curation\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_curation(class_=.local_class,\n#' data_=.local_data, details_=\"Contains three `data.frame`s, 'cell' with\n#' cell-line ids and 'tissue' with tissue ids and 'drug' with drug ids.\")\n#' @importMethodsFrom CoreGx curation\nsetMethod('curation', signature(object=\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_curation(class_=.local_class,\n#' data_=.local_data, details_=\"For a `ToxicoSet` object the slot should\n#' contain tissue, cell-line and drug id `data.frame`s.\")\n#' @importMethodsFrom CoreGx curation<-\nsetReplaceMethod(\"curation\", signature(object=\"ToxicoSet\", value=\"list\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ----------------------\n## ---- datasetType slot\n\n\n#\n# == datasetType\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType\nsetMethod(\"datasetType\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType<-\nsetReplaceMethod(\"datasetType\", signature(object=\"ToxicoSet\",\n    value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ---------------------------\n## ---- molecularProfiles slot\n\n\n##\n## == molecularProfiles\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles\nsetMethod(molecularProfiles, \"ToxicoSet\", function(object, mDataType, assay)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles<-\nsetReplaceMethod(\"molecularProfiles\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\", assay=\"character\", value=\"matrix\"),\n    function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\nsetReplaceMethod(\"molecularProfiles\",\n    signature(object=\"ToxicoSet\", mDataType =\"character\", assay=\"missing\",\n        value=\"matrix\"), function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\n\n\n##\n## == featureInfo\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_featureInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx featureInfo\nsetMethod(featureInfo, \"ToxicoSet\", function(object, mDataType) {\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_featureInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx featureInfo<-\nsetReplaceMethod(\"featureInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\",value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"featureInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\",value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n\n##\n## == phenoInfo\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo\nsetMethod('phenoInfo', signature(object='ToxicoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo<-\nsetReplaceMethod(\"phenoInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\", value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"phenoInfo\", signature(object=\"ToxicoSet\",\n    mDataType =\"character\", value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == fNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames\nsetMethod('fNames', signature(object='ToxicoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames<-\nsetReplaceMethod('fNames', signature(object='ToxicoSet', mDataType='character',\n    value='character'), function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == mDataNames\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames\nsetMethod(\"mDataNames\", \"ToxicoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames<-\nsetReplaceMethod(\"mDataNames\", \"ToxicoSet\", function(object, value){\n    callNextMethod(object=object, value=value)\n})\n\n\n\n##\n## == molecularProfilesSlot\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot\nsetMethod(\"molecularProfilesSlot\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot<-\nsetReplaceMethod(\"molecularProfilesSlot\", signature(\"ToxicoSet\", \"list_OR_MAE\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n# ---------------------\n## ---- sensitivity slot\n\n\n##\n## == sensitivityInfo\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo\nsetMethod('sensitivityInfo', signature(\"ToxicoSet\"),\n    function(object, dimension, ...)\n{\n    callNextMethod(object=object, dimension=dimension, ...)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo<-\nsetReplaceMethod(\"sensitivityInfo\", signature(object=\"ToxicoSet\",\n    value=\"data.frame\"), function(object, dimension, ..., value)\n{\n    callNextMethod(object=object, dimension=dimension, ..., value=value)\n})\n\n\n##\n## == sensitvityMeasures\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityMeasures(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityMeasures\nsetMethod('sensitivityMeasures', signature(object=\"ToxicoSet\"),\n    function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitityMeasures(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('sensitivityMeasures',\n    signature(object='ToxicoSet', value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensitivityProfiles\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles\nsetMethod('sensitivityProfiles', signature(object=\"ToxicoSet\"), function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles<-\nsetReplaceMethod(\"sensitivityProfiles\",\n    signature(object=\"ToxicoSet\", value=\"data.frame\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n#\n# == sensitivityRaw\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw\nsetMethod(\"sensitivityRaw\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw<-\nsetReplaceMethod('sensitivityRaw', signature(\"ToxicoSet\", \"array\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n#\n# == treatmentResponse\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentResponse(class_=.local_class,\n#'   data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentResponse\nsetMethod(\"treatmentResponse\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n\n\n#' @rdname ToxicoSet-accessors\n#' @importMethodsFrom CoreGx treatmentResponse<-\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentResponse(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('treatmentResponse', signature(object='ToxicoSet',\n    value='list_OR_LongTable'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensNumber\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber\nsetMethod('sensNumber', \"ToxicoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber<-\nsetReplaceMethod('sensNumber', signature(object=\"ToxicoSet\", value=\"matrix\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n## ======================\n## ---- perturbation slot\n\n\n##\n## == pertNumber\n\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber\nsetMethod('pertNumber', signature(object='ToxicoSet'), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname ToxicoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber<-\nsetReplaceMethod('pertNumber', signature(object='ToxicoSet', value=\"array\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drugInfo` and `drugInfo<-` functions in the ToxicoSet class?",
        "answer": "The `drugInfo` and `drugInfo<-` functions are accessor methods for the `drug` slot in the ToxicoSet class. They are aliases for the `treatmentInfo` and `treatmentInfo<-` methods, respectively, allowing users to get and set drug information in a ToxicoSet object."
      },
      {
        "question": "How does the `molecularProfiles` method handle different types of molecular data in a ToxicoSet object?",
        "answer": "The `molecularProfiles` method for ToxicoSet objects takes two arguments: `mDataType` and `assay`. This allows users to retrieve specific molecular profile data based on the type of molecular data (e.g., RNA, DNA) and the specific assay used. The method is flexible to accommodate various types of molecular data stored in the ToxicoSet object."
      },
      {
        "question": "What is the purpose of the `curation` slot in the ToxicoSet class, and what kind of data does it contain?",
        "answer": "The `curation` slot in the ToxicoSet class contains three data frames: 'cell' with cell-line IDs, 'tissue' with tissue IDs, and 'drug' with drug IDs. This slot is used to store curated information about the samples, tissues, and drugs used in the toxicogenomic dataset, providing a structured way to access and manage this metadata."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('annotation', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"ToxicoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})",
        "complete": "setMethod('annotation', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"ToxicoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\nsetMethod('dateCreated', signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod('dateCreated', signature(object=\"ToxicoSet\", value=\"character\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})"
      },
      {
        "partial": "setMethod(\"sampleInfo\", \"ToxicoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"ToxicoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})",
        "complete": "setMethod(\"sampleInfo\", \"ToxicoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"ToxicoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\nsetMethod(\"sampleNames\", signature(\"ToxicoSet\"), function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleNames\", signature(object=\"ToxicoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/hr.comp.R",
    "language": "R",
    "content": "`hr.comp` <-\nfunction(hr1, hr2) {\n\tif(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n\tn <- hr1$n\n\tx1 <- hr1$data$x\n\tx2 <- hr2$data$x\n\tbeta1 <- hr1$coef\n\tbeta2 <- hr2$coef\n\tse1 <- hr1$se\n\tse2 <- hr2$se\n\tr <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n\tif(abs(r) < 1) {\n\t\tt.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hr.comp` function and what are its main inputs?",
        "answer": "The `hr.comp` function is designed to compare two hazard ratios. It takes two inputs, `hr1` and `hr2`, which are expected to be objects containing hazard ratio data. The function checks if the hazard ratios are computed from the same number of samples, calculates a test statistic, and returns a p-value along with the exponential of the coefficients for both hazard ratios."
      },
      {
        "question": "How does the function handle the case when the correlation between `x1` and `x2` is perfect (1 or -1)?",
        "answer": "When the absolute value of the correlation coefficient `r` between `x1` and `x2` is 1 (perfect correlation), the function sets the p-value (`diff.ci.p`) to 1 instead of calculating the test statistic. This is likely because perfect correlation would lead to division by zero in the t-statistic calculation, so this case is handled separately."
      },
      {
        "question": "What statistical test is being performed in this function, and how is the test statistic calculated?",
        "answer": "The function is performing a t-test to compare two hazard ratios. The test statistic is calculated as: t.stat = (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2), where beta1 and beta2 are the coefficients, se1 and se2 are the standard errors, and r is the Spearman correlation between x1 and x2. This test statistic accounts for the correlation between the two hazard ratios."
      }
    ],
    "completion_tasks": [
      {
        "partial": "hr.comp <- function(hr1, hr2) {\n  if(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  # Complete the return statement\n}",
        "complete": "hr.comp <- function(hr1, hr2) {\n  if(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      },
      {
        "partial": "hr.comp <- function(hr1, hr2) {\n  # Add input validation\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  # Complete the function\n}",
        "complete": "hr.comp <- function(hr1, hr2) {\n  if(hr1$n != hr2$n) { stop(\"the hazard ratios are computed from different number of samples!\") }\n  n <- hr1$n\n  x1 <- hr1$data$x\n  x2 <- hr2$data$x\n  beta1 <- hr1$coef\n  beta2 <- hr2$coef\n  se1 <- hr1$se\n  se2 <- hr2$se\n  r <- cor(x1, x2, method=\"spearman\", use=\"complete.obs\")\n  if(abs(r) < 1) {\n    t.stat <- (beta1 - beta2) / sqrt(se1^2 + se2^2 - 2 * r * se1 * se2)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"hr1\"=exp(beta1), \"hr2\"=exp(beta2)))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/readArray.R",
    "language": "R",
    "content": "#' @title Overlap two datasets\n#'\n#' @description\n#' Formatting function to read arrays and format for use in the claudinLow classifier.\n#'\n#' @usage\n#' readArray(dataFile,designFile=NA,hr=1,impute=TRUE,method=\"mean\")\n#'\n#' @param dataFile file with matrix to be read.\n#' @param designFile Design of file.\n#' @param hr Header rows as Present (2) or Absent (1).\n#' @param impute whether data will be imputed or not.\n#' @param method Default method is \"mean\".\n#'\n#' @return\n#' A list\n#'\n#' @references\n#' citation(\"claudinLow\")\n#'\n#' @seealso\n#' [genefu::claudinLow]\n#'\n#' @md\n#' @importFrom impute impute.knn\n#' @export\nreadArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE,\n    method=\"mean\")\n{\n\n  headerRows <- hr\n\n  x<-read.table(dataFile,sep=\"\\t\",header=FALSE,fill=TRUE,stringsAsFactors=FALSE)\n\n  if(headerRows==1){\n    sampleNames<-as.vector(t(x[1,-1]))\n    x<-x[-1,]\n    classes<-NULL\n    ids<-x[,1]\n    xd<-x[,-1]\n    xd<-apply(xd,2,as.numeric)\n    xd<-collapseIDs(xd,ids,method)\n  }else{\n    sampleNames<-as.vector(t(x[1,-1]))\n    x<-x[-1,]\n\n    classes<-x[1:(headerRows-1),]\n    dimnames(classes)[[1]]<-classes[,1]\n    classes<-classes[,-1]\n    classes[classes==\"\"]<-NA\n    classes<-t(classes)\n    rownames(classes)<-sampleNames\n    classes<-as.data.frame(classes)\n\n    xd<-x[(-1:-(headerRows-1)),]\n    ids<-as.vector(t(xd[,1]))\n    xd<-xd[,-1]\n    xd<-apply(xd,2,as.numeric)\n    xd<-collapseIDs(xd,ids,method)\n  }\n\n  features<- dim(xd)[1]\n  samples<- dim(xd)[2]\n  geneNames<-rownames(xd)\n  xd<-apply(xd,2,as.numeric)\n  rownames(xd)<-geneNames\n  colnames(xd)<-sampleNames\n\n  if(!is.na(designFile)){\n    x<-read.table(designFile,sep=\"\\t\", header=TRUE, row.names=1, fill=TRUE,\n                  stringsAsFactors=FALSE)\n    xd<-xd[,sort.list(colnames(xd))]\n    xd<-xd[,colnames(xd) %in% rownames(x)]\n    x<-x[rownames(x) %in% colnames(xd),]\n    x<-x[sort.list(rownames(x)),]\n    classes<-as.data.frame(x)\n  }\n\n  if(sum(apply(xd,2,is.na))>0 & impute){\n    #library(impute)\n    allAnn<-dimnames(xd)\n    data.imputed<-impute.knn(as.matrix(xd))$data\n    xd<-data.imputed[1:features,]\n    dimnames(xd)<-allAnn\n  }\n\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'readArray' function and what are its main parameters?",
        "answer": "The 'readArray' function is designed to read and format data arrays for use in the claudinLow classifier. Its main parameters are:\n1. dataFile: the file containing the matrix to be read\n2. designFile: the design of the file (optional)\n3. hr: header rows, can be 1 (absent) or 2 (present)\n4. impute: whether data should be imputed (default is TRUE)\n5. method: method for collapsing IDs (default is 'mean')"
      },
      {
        "question": "How does the function handle different header row configurations (hr=1 vs hr=2)?",
        "answer": "When hr=1 (header rows absent):\n- It treats the first row as sample names\n- There are no class labels\n- IDs are taken from the first column\n\nWhen hr=2 (header rows present):\n- The first row is still treated as sample names\n- Class labels are extracted from the rows between the sample names and data\n- IDs are taken from the first column of the data portion\n\nIn both cases, the function processes the data accordingly, extracting relevant information and formatting it for further use."
      },
      {
        "question": "What does the function do when imputation is required, and what external package does it use for this purpose?",
        "answer": "When imputation is required (i.e., there are NA values in the data and impute=TRUE), the function uses the impute.knn function from the 'impute' package to perform k-nearest neighbor imputation. The process is as follows:\n1. It checks if there are any NA values in the data\n2. If imputation is needed, it saves the original dimension names\n3. It calls impute.knn on the data matrix\n4. It extracts the imputed data and restores the original dimension names\n\nThis ensures that missing values are filled in using a reliable imputation method, maintaining the integrity of the dataset for further analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  headerRows <- hr\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  if(headerRows == 1) {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- NULL\n    ids <- x[,1]\n    xd <- x[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  } else {\n    # Complete the code for headerRows != 1\n  }\n\n  # Complete the rest of the function\n}",
        "complete": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  headerRows <- hr\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  if(headerRows == 1) {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- NULL\n    ids <- x[,1]\n    xd <- x[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  } else {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- x[1:(headerRows-1),]\n    dimnames(classes)[[1]] <- classes[,1]\n    classes <- classes[,-1]\n    classes[classes==\"\"] <- NA\n    classes <- t(classes)\n    rownames(classes) <- sampleNames\n    classes <- as.data.frame(classes)\n    xd <- x[(-1:-(headerRows-1)),]\n    ids <- as.vector(t(xd[,1]))\n    xd <- xd[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  }\n\n  features <- dim(xd)[1]\n  samples <- dim(xd)[2]\n  geneNames <- rownames(xd)\n  xd <- apply(xd, 2, as.numeric)\n  rownames(xd) <- geneNames\n  colnames(xd) <- sampleNames\n\n  if(!is.na(designFile)) {\n    x <- read.table(designFile, sep=\"\\t\", header=TRUE, row.names=1, fill=TRUE, stringsAsFactors=FALSE)\n    xd <- xd[,sort.list(colnames(xd))]\n    xd <- xd[,colnames(xd) %in% rownames(x)]\n    x <- x[rownames(x) %in% colnames(xd),]\n    x <- x[sort.list(rownames(x)),]\n    classes <- as.data.frame(x)\n  }\n\n  if(sum(apply(xd, 2, is.na)) > 0 & impute) {\n    allAnn <- dimnames(xd)\n    data.imputed <- impute.knn(as.matrix(xd))$data\n    xd <- data.imputed[1:features,]\n    dimnames(xd) <- allAnn\n  }\n\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}"
      },
      {
        "partial": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  # Read the data file\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  # Process the data based on header rows\n  if(hr == 1) {\n    # Complete the code for hr == 1\n  } else {\n    # Complete the code for hr != 1\n  }\n\n  # Process design file if provided\n  if(!is.na(designFile)) {\n    # Complete the code for processing design file\n  }\n\n  # Impute data if required\n  if(sum(apply(xd, 2, is.na)) > 0 & impute) {\n    # Complete the code for imputation\n  }\n\n  # Return the result\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}",
        "complete": "readArray <- function(dataFile, designFile=NA, hr=1, impute=TRUE, method=\"mean\") {\n  x <- read.table(dataFile, sep=\"\\t\", header=FALSE, fill=TRUE, stringsAsFactors=FALSE)\n\n  if(hr == 1) {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- NULL\n    ids <- x[,1]\n    xd <- x[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  } else {\n    sampleNames <- as.vector(t(x[1,-1]))\n    x <- x[-1,]\n    classes <- x[1:(hr-1),]\n    dimnames(classes)[[1]] <- classes[,1]\n    classes <- classes[,-1]\n    classes[classes==\"\"] <- NA\n    classes <- t(classes)\n    rownames(classes) <- sampleNames\n    classes <- as.data.frame(classes)\n    xd <- x[(-1:-(hr-1)),]\n    ids <- as.vector(t(xd[,1]))\n    xd <- xd[,-1]\n    xd <- apply(xd, 2, as.numeric)\n    xd <- collapseIDs(xd, ids, method)\n  }\n\n  features <- dim(xd)[1]\n  samples <- dim(xd)[2]\n  geneNames <- rownames(xd)\n  xd <- apply(xd, 2, as.numeric)\n  rownames(xd) <- geneNames\n  colnames(xd) <- sampleNames\n\n  if(!is.na(designFile)) {\n    x <- read.table(designFile, sep=\"\\t\", header=TRUE, row.names=1, fill=TRUE, stringsAsFactors=FALSE)\n    xd <- xd[,sort.list(colnames(xd))]\n    xd <- xd[,colnames(xd) %in% rownames(x)]\n    x <- x[rownames(x) %in% colnames(xd),]\n    x <- x[sort.list(rownames(x)),]\n    classes <- as.data.frame(x)\n  }\n\n  if(sum(apply(xd, 2, is.na)) > 0 & impute) {\n    allAnn <- dimnames(xd)\n    data.imputed <- impute.knn(as.matrix(xd))$data\n    xd <- data.imputed[1:features,]\n    dimnames(xd) <- allAnn\n  }\n\n  return(list(xd=xd, classes=classes, nfeatures=features, nsamples=samples, fnames=geneNames, snames=sampleNames))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/allGenerics.R",
    "language": "R",
    "content": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) setGeneric('methods-computeLimmaDiffExpr.R'))",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `setGeneric` function in this code snippet?",
        "answer": "The `setGeneric` function is used to define a new generic method called 'computeLimmaDiffExpr'. It creates a function that can have multiple implementations (methods) for different classes of objects. This allows for method dispatch based on the class of the 'object' argument."
      },
      {
        "question": "What does the `...` argument in the function definition represent?",
        "answer": "The `...` (ellipsis) in the function definition allows for additional arguments to be passed to the method. As stated in the comment, it allows new parameters to be added to this generic method without changing its signature. This provides flexibility for different implementations to accept various parameters."
      },
      {
        "question": "What is the significance of the `@export` tag in the Roxygen comments?",
        "answer": "The `@export` tag in the Roxygen comments indicates that this function should be made publicly available when the package is built. It tells the package building system to include this function in the package's namespace, making it accessible to users who import the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) ",
        "complete": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) standardGeneric('computeLimmaDiffExpr'))"
      },
      {
        "partial": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    ",
        "complete": "#' Generic method for performing differential expression analysis on an S4 object\n#'   using the limma package\n#'\n#' @param object [`S4`] An S4 object to conduct differential expression analysis\n#'   on.\n#' @param ... Allow new parameters to be added to this generic.\n#'\n#' @return To be defined by the method implementation.\n#'\n#' @export\nsetGeneric('computeLimmaDiffExpr',\n    function(object, ...) standardGeneric('computeLimmaDiffExpr'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/bimod.R",
    "language": "R",
    "content": "#' @name bimod\n#' @title Function to identify bimodality for gene expression or signature score\n#'\n#' @description\n#' This function fits a mixture of two Gaussians to identify bimodality.\n#' Useful to identify ER of HER2 status of breast tumors using\n#' ESR1 and ERBB2 expressions respectively.\n#'\n#' @usage\n#' bimod(x, data, annot, do.mapping = FALSE, mapping, model = c(\"E\", \"V\"),\n#' do.scale = TRUE, verbose = FALSE, ...)\n#'\n#' @param x Matrix containing the gene(s) in the gene list in rows and at least three columns:\n#'   \"probe\", \"EntrezGene.ID\" and \"coefficient\" standing for the name of the probe,\n#'   the NCBI Entrez Gene id and the coefficient giving the direction and the strength\n#'   of the association of each gene in the gene list.\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'   dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in case of\n#'   ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the\n#'   mapping such that the probes are not selected based on their variance.\n#' @param model Model name used in Mclust.\n#' @param do.scale TRUE if the gene expressions or signature scores must be rescaled (see rescale), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#' @param ... Additional parameters to pass to sig.score.\n#'\n#' @return\n#' A list with items:\n#' - status: Status being 0 or 1.\n#' - status1.proba: Probability p to be of status 1, the probability to\n#'   be of status 0 being 1-p.\n#' - gaussians: Matrix of parameters fitted in the mixture of two\n#'   Gaussians. Matrix of NA values if EM algorithm did not converge.\n#' - BIC: Values (gene expressions or signature scores) used to identify bimodality.\n#' - BI: Bimodality Index (BI) as defined by Wang et al., 2009.\n#' - x: Values (gene expressions or signature scores) used to identify bimodality\n#'\n#' @references\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi M, Piccart M,\n#'   and Sotiriou C (2008) \"Biological processes associated with breast cancer clinical outcome depend\n#'   on the molecular subtypes\", Clinical Cancer Research, 14(16):5158\u20135165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt C, Ignatiadis M,\n#'   Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M (2008) \"Meta-analysis of\n#'   Gene-Expression Profiles in Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing\n#'   and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#' Fraley C and Raftery E (2002) \"Model-Based Clustering, Discriminant Analysis, and Density Estimation\",\n#'   Journal of American Statistical Asscoiation, 97(458):611\u2013631.\n#' Wang J, Wen S, Symmans FW, Pusztai L and Coombes KR (2009) \"The bimodality index: a criterion for\n#'   discovering and ranking bimodal signatures from cancer gene expression profiling data\", Cancer\n#'   Informatics, 7:199\u2013216.\n#'\n#' @seealso\n#' [mclust::Mclust]\n#'\n#' @examples\n#' # load NKI data\n#' data(nkis)\n#' # load gene modules from Desmedt et al. 2008\n#' data(mod1)\n#' # retrieve esr1 affy probe and Entrez Gene id\n#' esr1 <- mod1$ESR1[1, ,drop=FALSE]\n#' # computation of signature scores\n#' esr1.bimod <- bimod(x=esr1, data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   model=\"V\", verbose=TRUE)\n#' table(\"ER.IHC\"=demo.nkis[ ,\"er\"], \"ER.GE\"=esr1.bimod$status)\n#'\n#' @md\n#' @importFrom mclust Mclust\n#' @export\nbimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      #How many Gaussians?\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      #Only 2 Gaussians\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) { ## EM algorithm did not converge\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      res[1, ] <- rr2$parameters$mean\n      res[2, ] <- rr2$parameters$variance$sigmasq\n      res[3, ] <- rr2$parameters$pro\n\n      ## bimodality index (BI)\n      smd <- abs(res[1, 2] - res[1, 1]) / sqrt((res[2, 2] + res[2, 1]) / 2)\n      bi <- sqrt(res[3, 2] * (1 - res[3, 2])) * smd\n\n      #classification\n      mystatus[cc.ix] <- as.numeric(rr2$classification == 2)\n      mystatus.proba[cc.ix] <- rr2$z[ , 2, drop=TRUE]\n      return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=mybic,  \"BI\"=bi, \"x\"=dd))\n    } else { stop(\"Not enough data!\") }\n  }",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `bimod` function in this code snippet?",
        "answer": "The `bimod` function is designed to identify bimodality in gene expression or signature scores. It fits a mixture of two Gaussians to the data, which is particularly useful for identifying ER or HER2 status of breast tumors using ESR1 and ERBB2 expressions, respectively. The function performs clustering using the Mclust algorithm and returns various statistics including the bimodal status, probabilities, fitted Gaussian parameters, and a Bimodality Index (BI)."
      },
      {
        "question": "How does the function handle cases where there might be more or fewer than two Gaussians in the data?",
        "answer": "The function first attempts to fit 1 to 10 Gaussians using Mclust and selects the best model based on BIC (Bayesian Information Criterion). However, it then forces a fit of exactly two Gaussians, regardless of the optimal number found. If the optimal number is not 2, it issues a warning message. This approach ensures that the function always attempts to identify bimodality, even if the data might be better described by a different number of clusters."
      },
      {
        "question": "What is the significance of the `do.scale` parameter in the `bimod` function?",
        "answer": "The `do.scale` parameter, when set to TRUE (which is the default), rescales the gene expressions or signature scores using the `rescale` function. The rescaling is performed with a quantile of 0.05 and then shifted and scaled to range from -1 to 1. This preprocessing step can help normalize the data and potentially improve the performance of the clustering algorithm by putting all variables on a comparable scale. Users can set `do.scale=FALSE` if they prefer to work with the original, unscaled data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) {\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      # Complete the function here\n    } else { stop(\"Not enough data!\") }\n  }",
        "complete": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) {\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      res[1, ] <- rr2$parameters$mean\n      res[2, ] <- rr2$parameters$variance$sigmasq\n      res[3, ] <- rr2$parameters$pro\n\n      smd <- abs(res[1, 2] - res[1, 1]) / sqrt((res[2, 2] + res[2, 1]) / 2)\n      bi <- sqrt(res[3, 2] * (1 - res[3, 2])) * smd\n\n      mystatus[cc.ix] <- as.numeric(rr2$classification == 2)\n      mystatus.proba[cc.ix] <- rr2$z[ , 2, drop=TRUE]\n      return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=mybic, \"BI\"=bi, \"x\"=dd))\n    } else { stop(\"Not enough data!\") }\n  }"
      },
      {
        "partial": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      # Add code here to fit Gaussian mixture models and calculate bimodality index\n    } else { stop(\"Not enough data!\") }\n  }",
        "complete": "bimod <- function(x, data, annot, do.mapping=FALSE, mapping, model=c(\"E\", \"V\"),\n    do.scale=TRUE, verbose=FALSE, ...)\n{\n    model <- match.arg(model)\n    dd <- sig.score(x=x, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=verbose, ...)$score\n    if(do.scale) { dd <- (rescale(x=dd, q=0.05, na.rm=TRUE) - 0.5) * 2 }\n    cc.ix <- complete.cases(dd)\n\n    mystatus <- mystatus.proba <- rep(NA, nrow(data))\n    names(mystatus) <- names(mystatus.proba) <- dimnames(data)[[1]]\n    res <- matrix(NA, nrow=3, ncol=2, dimnames=list(c(\"mean\", \"variance\", \"proportion\"), paste(\"cluster\", 1:2, sep=\".\")))\n    mybic <- matrix(NA, nrow=10, ncol=1, dimnames=list(1:10, model))\n\n    if(sum(cc.ix) >= 10) {\n      rr <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=1:10)\n      oo <- order(rr$BIC, decreasing=TRUE)[1]\n      if(oo != 2) { warning(sprintf(\"%i is the most likely number of Gaussians!\", oo)) }\n      mybic <- rr$BIC\n\n      rr2 <- mclust::Mclust(data=dd[cc.ix], modelNames=model, G=2)\n      if(is.null(rr2[[1]])) {\n        return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=rr$BIC, \"x\"=dd))\n      }\n      res[1, ] <- rr2$parameters$mean\n      res[2, ] <- rr2$parameters$variance$sigmasq\n      res[3, ] <- rr2$parameters$pro\n\n      smd <- abs(res[1, 2] - res[1, 1]) / sqrt((res[2, 2] + res[2, 1]) / 2)\n      bi <- sqrt(res[3, 2] * (1 - res[3, 2])) * smd\n\n      mystatus[cc.ix] <- as.numeric(rr2$classification == 2)\n      mystatus.proba[cc.ix] <- rr2$z[ , 2, drop=TRUE]\n      return(list(\"status\"=mystatus, \"status1.proba\"=mystatus.proba, \"gaussians\"=res, \"BIC\"=mybic, \"BI\"=bi, \"x\"=dd))\n    } else { stop(\"Not enough data!\") }\n  }"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/cindex.comp.meta.R",
    "language": "R",
    "content": "`cindex.comp.meta` <-\nfunction(list.cindex1, list.cindex2, hetero=FALSE) {\n\n\tif(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n\teps <- 1E-15\n\t\n\tn <- 0\n\tx1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n\tfor(i in 1:length(list.cindex1)) {\n\t\tnn <- list.cindex1[[i]]$n\n\t\tif(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n\t\tif(nn > 3) {\n\t\t\tn <- n + nn\n\t\t\tx1 <- c(x1, list.cindex1[[i]]$c.index)\n\t\t\tx1.se <- c(x1.se, list.cindex1[[i]]$se)\n\t\t\tx2 <- c(x2, list.cindex2[[i]]$c.index)\n\t\t\tx2.se <- c(x2.se, list.cindex2[[i]]$se)\n\t\t\tcort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n\t\t\t## since r is the spearman correlation coefficient and not the Pearson's one, we should apply a correction factor (see http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient for details)\n\t\t\tcorz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n\t\t\tcorz.se <- c(corz.se, 1 / sqrt(nn - 3))\n\t\t} else {\n\t\t\tcorz <- c(corz, NA)\n\t\t\tcorz.se <- c(corz.se, NA)\n\t\t}\n\t}\n\tx1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n\tx2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n\tif(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n\t## same concordance indices\t\n\t\treturn(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n\t}\n\trz <- combine.est(x=corz, x.se=corz.se, na.rm=TRUE, hetero=hetero)$estimate\n\t## since r is the spearman correlation coefficient and not the Pearson's one, we should apply a correction factor (see http://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient for details)\n\trz <- rz / (sqrt((n - 3) / 1.06))\n\tr <- fisherz(rz, inv=TRUE)\n\tif((1 - abs(r)) > eps) {\n\t\tt.stat <- (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se)\n\t\tdiff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n\t} else { diff.ci.p <- 1 }\n\treturn(list(\"p.value\"=diff.ci.p, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cindex.comp.meta` function and what are its main input parameters?",
        "answer": "The `cindex.comp.meta` function is designed to compare two lists of concordance indices and perform a meta-analysis. Its main input parameters are `list.cindex1` and `list.cindex2`, which are lists containing concordance indices, and an optional `hetero` parameter to specify whether heterogeneity should be considered in the analysis."
      },
      {
        "question": "How does the function handle the correlation between the two sets of concordance indices, and why is a correction factor applied?",
        "answer": "The function calculates the Spearman correlation coefficient between the data of the two concordance indices. A correction factor is applied to the Fisher's z-transformed correlation because Spearman's correlation is used instead of Pearson's. The correction factor is sqrt((nn - 3) / 1.06), where nn is the number of samples. This adjustment is made to account for the differences between Spearman's and Pearson's correlations in the Fisher's z-transformation."
      },
      {
        "question": "What statistical test is performed to compare the two sets of concordance indices, and under what condition is this test not applied?",
        "answer": "The function performs a t-test to compare the two sets of concordance indices. The test statistic is calculated as (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se), where r is the estimated correlation between the indices. However, if the absolute value of the correlation (r) is very close to 1 (within a small epsilon value), the test is not applied, and a p-value of 1 is returned instead. This is likely to avoid numerical instability when the correlation is extremely high."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      n <- n + nn\n      x1 <- c(x1, list.cindex1[[i]]$c.index)\n      x1.se <- c(x1.se, list.cindex1[[i]]$se)\n      x2 <- c(x2, list.cindex2[[i]]$c.index)\n      x2.se <- c(x2.se, list.cindex2[[i]]$se)\n      cort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n      corz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n      corz.se <- c(corz.se, 1 / sqrt(nn - 3))\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  # Complete the function\n}",
        "complete": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      n <- n + nn\n      x1 <- c(x1, list.cindex1[[i]]$c.index)\n      x1.se <- c(x1.se, list.cindex1[[i]]$se)\n      x2 <- c(x2, list.cindex2[[i]]$c.index)\n      x2.se <- c(x2.se, list.cindex2[[i]]$se)\n      cort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n      corz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n      corz.se <- c(corz.se, 1 / sqrt(nn - 3))\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  x1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n  x2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n  if(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n    return(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n  }\n  rz <- combine.est(x=corz, x.se=corz.se, na.rm=TRUE, hetero=hetero)$estimate\n  rz <- rz / (sqrt((n - 3) / 1.06))\n  r <- fisherz(rz, inv=TRUE)\n  if((1 - abs(r)) > eps) {\n    t.stat <- (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n}"
      },
      {
        "partial": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      # Complete the loop body\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  x1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n  x2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n  if(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n    return(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n  }\n  # Complete the function\n}",
        "complete": "cindex.comp.meta <- function(list.cindex1, list.cindex2, hetero=FALSE) {\n  if(length(list.cindex1) != length(list.cindex2)) { stop(\"the number of concordance indices is not the same!\") }\n  eps <- 1E-15\n  n <- 0\n  x1 <- x1.se <- x2 <- x2.se <- corz <- corz.se <- NULL\n  for(i in 1:length(list.cindex1)) {\n    nn <- list.cindex1[[i]]$n\n    if(nn != list.cindex2[[i]]$n) { stop(\"the number of samples to compute the concordance indices is not the same!\") }\n    if(nn > 3) {\n      n <- n + nn\n      x1 <- c(x1, list.cindex1[[i]]$c.index)\n      x1.se <- c(x1.se, list.cindex1[[i]]$se)\n      x2 <- c(x2, list.cindex2[[i]]$c.index)\n      x2.se <- c(x2.se, list.cindex2[[i]]$se)\n      cort <- cor(list.cindex1[[i]]$data$x, list.cindex2[[i]]$data$x, method=\"spearman\", use=\"complete.obs\")\n      corz <- c(corz, sqrt((nn - 3) / 1.06) * fisherz(cort, inv=FALSE))\n      corz.se <- c(corz.se, 1 / sqrt(nn - 3))\n    } else {\n      corz <- c(corz, NA)\n      corz.se <- c(corz.se, NA)\n    }\n  }\n  x1.meta <- combine.est(x=x1, x.se=x1.se, hetero=hetero, na.rm=TRUE)\n  x2.meta <- combine.est(x=x2, x.se=x2.se, hetero=hetero, na.rm=TRUE)\n  if(x1.meta$estimate == x2.meta$estimate && x1.meta$se == x2.meta$se) {\n    return(list(\"p.value\"=1, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n  }\n  rz <- combine.est(x=corz, x.se=corz.se, na.rm=TRUE, hetero=hetero)$estimate\n  rz <- rz / (sqrt((n - 3) / 1.06))\n  r <- fisherz(rz, inv=TRUE)\n  if((1 - abs(r)) > eps) {\n    t.stat <- (x1.meta$estimate - x2.meta$estimate) / sqrt(x1.meta$se^2 + x2.meta$se^2 - 2 * r * x1.meta$se * x2.meta$se)\n    diff.ci.p <- pt(q=t.stat, df=n - 1, lower.tail=FALSE)\n  } else { diff.ci.p <- 1 }\n  return(list(\"p.value\"=diff.ci.p, \"cindex1\"=x1.meta$estimate, \"cindex2\"=x2.meta$estimate))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/cordiff.dep.R",
    "language": "R",
    "content": "#' @title Function to estimate whether two dependent correlations differ\n#'\n#' @description\n#' This function tests for statistical differences between two dependent correlations\n#'   using the formula provided on page 56 of Cohen & Cohen (1983). The function returns \n#'   a t-value, the DF and the p-value.\n#'\n#' @usage\n#' cordiff.dep(r.x1y, r.x2y, r.x1x2, n,\n#'   alternative = c(\"two.sided\", \"less\", \"greater\"))\n#'\n#' @param r.x1y\tThe correlation between x1 and y where y is typically your outcome variable.\n#' @param r.x2y\tThe correlation between x2 and y where y is typically your outcome variable.\n#' @param r.x1x2 The correlation between x1 and x2 (the correlation between your two predictors).\n#' @param n The sample size.\n#' @param alternative A character string specifying the alternative hypothesis, must be\n#'   one of \"two.sided\" default), \"greater\" or \"less\". You can specify just the initial letter.\n#'\n#' @details\n#' This function is inspired from the cordif.dep.\n#'\n#' @return\n#' Vector of three values: t statistics, degree of freedom, and p-value.\n#'\n#' @references\n#' Cohen, J. & Cohen, P. (1983) \"Applied multiple regression/correlation analysis for the\n#'   behavioral sciences (2nd Ed.)\" Hillsdale, nJ: Lawrence Erlbaum Associates.\n#'\n#' @seealso\n#' [stats::cor], [stats::t.test], [genefu::compareProtoCor]\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # retrieve ESR1, AURKA and MKI67 gene expressions\n#' x1 <- data.vdxs[ ,\"208079_s_at\"]\n#' x2 <- data.vdxs[ ,\"205225_at\"]\n#' y <- data.vdxs[ ,\"212022_s_at\"]\n#' # is MKI67 significantly more correlated to AURKA than ESR1?\n#' cc.ix <- complete.cases(x1, x2, y)\n#' cordiff.dep(r.x1y=abs(cor(x=x1[cc.ix], y=y[cc.ix], use=\"everything\",\n#'   method=\"pearson\")), r.x2y=abs(cor(x=x2[cc.ix], y=y[cc.ix],\n#'   use=\"everything\", method=\"pearson\")), r.x1x2=abs(cor(x=x1[cc.ix],\n#'   y=x2[cc.ix], use=\"everything\", method=\"pearson\")), n=sum(cc.ix),\n#'   alternative=\"greater\")\n#'\n#' @md\n#' @export\ncordiff.dep <-\nfunction(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n\talternative <- match.arg(alternative)\n\trbar <- (r.x1y + r.x2y)/2\n\tbarRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n\ttvalue.num <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2)))\n\ttvalue.den <- sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n\tt.value <- tvalue.num / tvalue.den\n\tDF <- n - 3\n\tswitch(alternative,\n\t\"greater\"={\n\t\tp.value <- pt(t.value, DF, lower.tail=FALSE)\n\t},\n\t\"less\"={\n\t\tp.value <- 1 - pt(t.value, DF, lower.tail=FALSE)\n\t},\n\t\"two.sided\"={\n\t\tp.value <- (1 - pt(abs(t.value), DF)) * 2\n\t})\n\tOUT <- c(t.value, DF, p.value)\n\tnames(OUT) <- c(\"t.value\",  \"DF\",  \"p.value\")\n\t\n\treturn(OUT)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cordiff.dep` function and what statistical test does it perform?",
        "answer": "The `cordiff.dep` function is designed to estimate whether two dependent correlations differ significantly. It performs a statistical test to compare two correlations that share a common variable, based on the formula from Cohen & Cohen (1983). The function calculates a t-value, degrees of freedom, and a p-value to determine if there's a significant difference between the two correlations."
      },
      {
        "question": "How does the function handle different alternative hypotheses, and what are the available options?",
        "answer": "The function handles different alternative hypotheses using the `alternative` parameter and the `switch` statement. The available options are 'two.sided' (default), 'less', and 'greater'. For 'greater', it calculates the upper-tail probability; for 'less', it calculates the lower-tail probability; and for 'two.sided', it calculates the two-tailed probability. The p-value calculation is adjusted accordingly using the `pt` function with the appropriate tail specification."
      },
      {
        "question": "What are the key components of the t-value calculation in the `cordiff.dep` function, and how are they derived?",
        "answer": "The t-value calculation in the `cordiff.dep` function involves several key components:\n1. `rbar`: The average of the two correlations (r.x1y and r.x2y).\n2. `barRbar`: A complex term involving all three correlations.\n3. `tvalue.num`: The numerator of the t-value, which includes the difference between correlations and a term with r.x1x2.\n4. `tvalue.den`: The denominator of the t-value, involving `barRbar` and `rbar`.\nThese components are derived from the formula provided by Cohen & Cohen (1983) and are combined to calculate the final t-value, which is then used to determine the statistical significance of the difference between the two dependent correlations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  rbar <- (r.x1y + r.x2y)/2\n  barRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n  tvalue.num <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2)))\n  tvalue.den <- sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n  t.value <- tvalue.num / tvalue.den\n  DF <- n - 3\n  # Complete the function by adding the switch statement for p-value calculation\n}",
        "complete": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  rbar <- (r.x1y + r.x2y)/2\n  barRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n  tvalue.num <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2)))\n  tvalue.den <- sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n  t.value <- tvalue.num / tvalue.den\n  DF <- n - 3\n  p.value <- switch(alternative,\n    \"greater\" = pt(t.value, DF, lower.tail=FALSE),\n    \"less\" = pt(t.value, DF, lower.tail=TRUE),\n    \"two.sided\" = 2 * pt(-abs(t.value), DF)\n  )\n  c(t.value = t.value, DF = DF, p.value = p.value)\n}"
      },
      {
        "partial": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  # Calculate rbar and barRbar\n  # Calculate t-value numerator and denominator\n  # Calculate t-value and degrees of freedom\n  # Calculate p-value based on alternative hypothesis\n  # Return results\n}",
        "complete": "cordiff.dep <- function(r.x1y, r.x2y, r.x1x2, n, alternative=c(\"two.sided\", \"less\", \"greater\")) {\n  alternative <- match.arg(alternative)\n  rbar <- (r.x1y + r.x2y)/2\n  barRbar <- 1 - r.x1y^2 - r.x2y^2 - r.x1x2^2 + 2 * r.x1y * r.x2y * r.x1x2\n  t.value <- ((r.x1y - r.x2y) * sqrt((n - 1) * (1 + r.x1x2))) /\n              sqrt(((2 * ((n - 1)/(n - 3))) * barRbar + ((rbar^2)) * (1 - r.x1x2)^3))\n  DF <- n - 3\n  p.value <- switch(alternative,\n    \"greater\" = pt(t.value, DF, lower.tail=FALSE),\n    \"less\" = pt(t.value, DF, lower.tail=TRUE),\n    \"two.sided\" = 2 * pt(-abs(t.value), DF)\n  )\n  c(t.value = t.value, DF = DF, p.value = p.value)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/updateObject-methods.R",
    "language": "R",
    "content": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    names(curation(tSet)) <- gsub(\"drug\", \"treatment\", names(curation(tSet)))\n    if (\"treatment\" %in% names(curation(tSet))) {\n        colnames(curation(tSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(tSet)$treatment))\n    }\n    validObject(tSet)\n    return(tSet)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `updateObject` method for the `ToxicoSet` class?",
        "answer": "The `updateObject` method is used to update the class structure of a `ToxicoSet` object after changes in its structure or API. It converts the object to a `CoreGx` object, updates it, then converts it back to a `ToxicoSet` object with updated curation data."
      },
      {
        "question": "How does the method handle the renaming of 'drug' to 'treatment' in the curation data?",
        "answer": "The method uses the `gsub` function to replace 'drug' with 'treatment' in the names of the curation data. It does this by applying `gsub(\"drug\", \"treatment\", names(curation(tSet)))` to update the column names."
      },
      {
        "question": "What is the significance of the `validObject(tSet)` call at the end of the method?",
        "answer": "The `validObject(tSet)` call is used to ensure that the updated `ToxicoSet` object is valid according to its class definition. This helps maintain data integrity and catches any potential errors introduced during the update process before returning the object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    # Complete the function\n})",
        "complete": "#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    names(curation(tSet)) <- gsub(\"drug\", \"treatment\", names(curation(tSet)))\n    if (\"treatment\" %in% names(curation(tSet))) {\n        colnames(curation(tSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(tSet)$treatment))\n    }\n    validObject(tSet)\n    return(tSet)\n})"
      },
      {
        "partial": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    # Complete the function body\n})",
        "complete": "#' @include ToxicoSet-accessors.R\nNULL\n\n#' Update the ToxicoSet class after changes in it struture or API\n#'\n#' @param object A `ToxicoSet` object to update the class structure for.\n#'\n#' @return `ToxicoSet` with update class structure.\n#'\n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"ToxicoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    tSet <- as(cSet, \"ToxicoSet\")\n    names(curation(tSet)) <- gsub(\"drug\", \"treatment\", names(curation(tSet)))\n    if (\"treatment\" %in% names(curation(tSet))) {\n        colnames(curation(tSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(tSet)$treatment))\n    }\n    validObject(tSet)\n    return(tSet)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/gene76.R",
    "language": "R",
    "content": "#' @title Function to compute the Relapse Score as published by Wang et al. 2005\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene\n#'   expression values following the algorithm used for the Relapse Score (GENE76) as\n#'   published by Wang et al. 2005.\n#'\n#' @usage\n#' gene76(data, er)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param er Vector containing the estrogen receptor (ER) status of breast cancer patients in\n#'   the dataset.\n#'\n#'\n#' @return\n#' A list with items:\n#' - score Continuous signature scores\n#' - risk Binary risk classification, 1 being high risk and 0 being low risk.\n#'\n#' @references\n#' Y. Wang and J. G. Klijn and Y. Zhang and A. M. Sieuwerts and M. P. Look and F.\n#'   Yang and D. Talantov and M. Timmermans and M. E. Meijer-van Gelder and J. Yu and T.\n#'   Jatkoe and E. M. Berns and D. Atkins and J. A. Foekens (2005) \"Gene-Expression\n#'   Profiles to Predict Distant Metastasis of Lymph-Node-Negative Primary Breast Cancer\",\n#'   Lancet, 365(9460):671\u2013679.\n#'\n#' @seealso\n#' [genefu::ggi]\n#'\n#' @examples\n#' # load GENE76 signature\n#' data(sig.gene76)\n#' # load VDX dataset\n#' data(vdxs)\n#' # compute relapse score\n#' rs.vdxs <- gene76(data=data.vdxs, er=demo.vdxs[ ,\"er\"])\n#' table(rs.vdxs$risk)\n#'\n#' @md\n#' @export\n#' @name gene76\ngene76 <- function(data, er) {\n\n\tif (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n\t\n\tA <- 313.5\n\tB <- 280\n\n\tscore <- NULL\n\tfor(i in 1:nrow(data)) {\n\t\tif(is.na(er[i])) { score <- c(score, NA) }\n\t\telse {\n\t\t\tif(er[i] == 1) {\n\t\t\t\tscore <- c(score, A + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[ ,\"er\"] == 1]] * sig.gene76[sig.gene76[ ,\"er\"] == 1,\"std.cox.coefficient\"]))\n\t\t\t}\n\t\t\telse {\n\t\t\t\tscore <- c(score, B + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[ ,\"er\"] == 0]] * sig.gene76[sig.gene76[ ,\"er\"] == 0,\"std.cox.coefficient\"]))\n\t\t\t}\n\t\t}\n\t}\n\tnames(score) <- dimnames(data)[[1]]\n\trisk <- ifelse(score >= 0, 1, 0)\n\n\treturn(list(\"score\"=score, \"risk\"=risk))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'gene76' function and what are its input parameters?",
        "answer": "The 'gene76' function computes the Relapse Score (GENE76) for breast cancer patients based on gene expression data. It takes two input parameters: 'data', which is a matrix of gene expressions with samples in rows and probes in columns, and 'er', which is a vector containing the estrogen receptor (ER) status of breast cancer patients in the dataset."
      },
      {
        "question": "How does the function handle different ER statuses when calculating the score?",
        "answer": "The function uses different calculations based on the ER status. If er[i] == 1 (ER-positive), it uses the formula: A + sum(data[i, ER-positive genes] * coefficients). If er[i] == 0 (ER-negative), it uses: B + sum(data[i, ER-negative genes] * coefficients). A is set to 313.5 and B to 280. This approach allows for tailored scoring based on the patient's ER status."
      },
      {
        "question": "What does the function return, and how is the risk classification determined?",
        "answer": "The function returns a list with two items: 'score' (continuous signature scores) and 'risk' (binary risk classification). The risk classification is determined by the score: if the score is greater than or equal to 0, the risk is classified as high (1), otherwise it's classified as low (0). This binary classification provides a simple risk assessment based on the computed score."
      }
    ],
    "completion_tasks": [
      {
        "partial": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  score <- NULL\n  for(i in 1:nrow(data)) {\n    if(is.na(er[i])) { score <- c(score, NA) }\n    else {\n      # Complete the code here\n    }\n  }\n  names(score) <- dimnames(data)[[1]]\n  risk <- ifelse(score >= 0, 1, 0)\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}",
        "complete": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  score <- NULL\n  for(i in 1:nrow(data)) {\n    if(is.na(er[i])) { score <- c(score, NA) }\n    else {\n      if(er[i] == 1) {\n        score <- c(score, A + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 1]] * sig.gene76[sig.gene76[,\"er\"] == 1,\"std.cox.coefficient\"]))\n      } else {\n        score <- c(score, B + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 0]] * sig.gene76[sig.gene76[,\"er\"] == 0,\"std.cox.coefficient\"]))\n      }\n    }\n  }\n  names(score) <- dimnames(data)[[1]]\n  risk <- ifelse(score >= 0, 1, 0)\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}"
      },
      {
        "partial": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  # Complete the code here to calculate score and risk\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}",
        "complete": "gene76 <- function(data, er) {\n  if (!exists('sig.gene76')) data(sig.gene76, envir=environment())\n  \n  A <- 313.5\n  B <- 280\n\n  score <- sapply(1:nrow(data), function(i) {\n    if(is.na(er[i])) return(NA)\n    if(er[i] == 1) {\n      return(A + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 1]] * sig.gene76[sig.gene76[,\"er\"] == 1,\"std.cox.coefficient\"]))\n    } else {\n      return(B + sum(data[i,dimnames(sig.gene76)[[1]][sig.gene76[,\"er\"] == 0]] * sig.gene76[sig.gene76[,\"er\"] == 0,\"std.cox.coefficient\"]))\n    }\n  })\n  names(score) <- dimnames(data)[[1]]\n  risk <- ifelse(score >= 0, 1, 0)\n\n  return(list(\"score\"=score, \"risk\"=risk))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/pik3cags.R",
    "language": "R",
    "content": "#' @title Function to compute the PIK3CA gene signature (PIK3CA-GS)\n#'\n#' @description\n#' This function computes signature scores from gene expression values\n#'   following the algorithm used for the PIK3CA gene signature (PIK3CA-GS).\n#'\n#' @usage\n#' pik3cags(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force\n#'   the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' Vector of signature scores for PIK3CA-GS\n#'\n#' @references\n#' Loi S, Haibe-Kains B, Majjaj S, Lallemand F, Durbecq V, Larsimont D,\n#'   Gonzalez-Angulo AM, Pusztai L, Symmans FW, Bardelli A, Ellis P, Tutt AN,\n#'   Gillett CE, Hennessy BT., Mills GB, Phillips WA, Piccart MJ, Speed TP,\n#'   McArthur GA, Sotiriou C (2010) \"PIK3CA mutations associated with gene\n#'   signature of low mTORC1 signaling and better outcomes in estrogen\n#'   receptor-positive breast cancer\", Proceedings of the National Academy of\n#'   Sciences, 107(22):10208-10213\n#'\n#' @seealso\n#' [genefu::gene76]\n#'\n#' @examples\n#' # load GGI signature\n#' data(sig.pik3cags)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' pik3cags.nkis <- pik3cags(data=data.nkis, annot=annot.nkis, do.mapping=TRUE)\n#' head(pik3cags.nkis)\n#'\n#' @md\n#' @export\n#' @name pik3cags\npik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\tres <- sig.score(x=pik3cags.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score\n\n\treturn (res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'pik3cags' function and what does it return?",
        "answer": "The 'pik3cags' function computes the PIK3CA gene signature (PIK3CA-GS) scores from gene expression values. It returns a vector of signature scores for PIK3CA-GS based on the input gene expression data and annotations."
      },
      {
        "question": "How does the function handle gene mapping, and what parameter controls this behavior?",
        "answer": "The function can perform gene mapping through Entrez Gene IDs if needed. This behavior is controlled by the 'do.mapping' parameter. When set to TRUE, the function maps probes to genes, keeping the most variant probe for each gene in case of ambiguities. The 'mapping' parameter can be used to force a specific mapping instead of selecting probes based on variance."
      },
      {
        "question": "What is the significance of the 'sig.pik3cags' data in this function, and how is it used?",
        "answer": "The 'sig.pik3cags' data contains the PIK3CA gene signature information. If it doesn't exist in the current environment, the function loads it. It then extracts the probe, EntrezGene.ID, and coefficient information from 'sig.pik3cags' to create 'pik3cags.gl', which is used as input for the 'sig.score' function to compute the final signature scores."
      }
    ],
    "completion_tasks": [
      {
        "partial": "pik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\t# Complete the function by calling sig.score and returning the result\n\n}",
        "complete": "pik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\tres <- sig.score(x=pik3cags.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score\n\n\treturn(res)\n}"
      },
      {
        "partial": "#' @title Function to compute the PIK3CA gene signature (PIK3CA-GS)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force\n#'   the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' Vector of signature scores for PIK3CA-GS\n#'\n#' @export\n#' @name pik3cags\n# Complete the function definition and body",
        "complete": "#' @title Function to compute the PIK3CA gene signature (PIK3CA-GS)\n#'\n#' @param data Matrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force\n#'   the mapping such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' Vector of signature scores for PIK3CA-GS\n#'\n#' @export\n#' @name pik3cags\npik3cags <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\tif (!exists('sig.pik3cags')) data(sig.pik3cags, envir=environment())\n\tpik3cags.gl <- sig.pik3cags[ ,c(\"probe\", \"EntrezGene.ID\", \"coefficient\")]\n\tres <- sig.score(x=pik3cags.gl, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, signed=TRUE, verbose=verbose)$score\n\treturn(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/write.m.file.R",
    "language": "R",
    "content": "#' @title Function to write a 'csv' file containing gene lists \n#'   (aka gene signatures)\n#'\n#' @description\n#' This function allows for writing a 'csv' file containing gene signatures. \n#'   Each gene signature is composed of at least four columns: \"gene.list\" is \n#'   the name of the signature on the first line and empty fields below, \n#'   \"probes\" are the probe names, \"EntrezGene.ID\" are the EntrezGene IDs \n#'   and \"coefficient\" are the coefficients of each probe.\n#'\n#' @usage\n#' write.m.file(obj, file, ...)\n#'\n#' @param obj\tList of gene signatures.\n#' @param file Filename of the 'csv' file.\n#' @param ...\tAdditional parameters for read.csv function.\n#'\n#' @return\n#' None.\n#'\n#' @examples\n#' # load gene modules published by Demsedt et al 2009\n#' data(mod1)\n#' # write these gene modules in a 'csv' file\n#' # Not run: write.m.file(obj=mod1, file=\"desmedt2009_genemodules.csv\")\n#' \n#' @md\n#' @export\nwrite.m.file <-\nfunction(obj, file, ...) {\n\tlcn <- dimnames(obj[[1]])[[2]]\n\tc1 <- c2 <- NULL\n\tfor (i in 1:length(obj)) {\n\t\tct <- names(obj)[i]\n\t\ttt <- NULL\n\t\tfor(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n\t\tcolnames(tt) <- colnames(obj[[i]])\n\t\tc1 <- c(c1, ct, rep(\"\", nrow(tt)))\n\t\tc2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n\t}\n\tdimnames(c2)[[1]] <- 1:nrow(c2)\n\tres <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n\tdimnames(res)[[2]] <- c(\"gene.list\", lcn)\n\tdimnames(res)[[1]] <- 1:nrow(res)\n\twrite.table(res, file=file, row.names=FALSE, sep=\",\", ...)\n\tinvisible(res)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'write.m.file' function and what are its main parameters?",
        "answer": "The 'write.m.file' function is designed to write gene signatures (gene lists) to a CSV file. Its main parameters are 'obj' (a list of gene signatures) and 'file' (the filename for the output CSV). The function organizes the gene signatures into a specific format with columns for gene list names, probes, EntrezGene IDs, and coefficients."
      },
      {
        "question": "How does the function handle multiple gene signatures in the input 'obj' parameter?",
        "answer": "The function iterates through each gene signature in the 'obj' list using a nested loop structure. For each signature, it extracts the name (stored in 'ct') and the data (stored in 'tt'). It then combines this information into two main components: 'c1' for gene list names and 'c2' for the actual gene data. This process ensures that all signatures from the input are properly formatted and included in the output CSV file."
      },
      {
        "question": "What is the significance of the 'invisible(res)' statement at the end of the function?",
        "answer": "The 'invisible(res)' statement at the end of the function returns the formatted data ('res') invisibly. This means that the function will not print the result to the console when called, but the result can still be assigned to a variable if needed. This is a common practice in R for functions that primarily perform side effects (like writing to a file) but may still need to return their result for potential further use or inspection."
      }
    ],
    "completion_tasks": [
      {
        "partial": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    for(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  dimnames(c2)[[1]] <- 1:nrow(c2)\n  res <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n  dimnames(res)[[2]] <- c(\"gene.list\", lcn)\n  dimnames(res)[[1]] <- 1:nrow(res)\n  # Complete the function by writing the result to a file and returning it invisibly\n}",
        "complete": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    for(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  dimnames(c2)[[1]] <- 1:nrow(c2)\n  res <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n  dimnames(res)[[2]] <- c(\"gene.list\", lcn)\n  dimnames(res)[[1]] <- 1:nrow(res)\n  write.table(res, file=file, row.names=FALSE, sep=\",\", ...)\n  invisible(res)\n}"
      },
      {
        "partial": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    # Complete the nested loop to process each column of obj[[i]]\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  # Complete the function by finalizing the result and writing it to a file\n}",
        "complete": "write.m.file <- function(obj, file, ...) {\n  lcn <- dimnames(obj[[1]])[[2]]\n  c1 <- c2 <- NULL\n  for (i in 1:length(obj)) {\n    ct <- names(obj)[i]\n    tt <- NULL\n    for(j in 1:ncol(obj[[i]])) { tt <- cbind(tt, as.character(obj[[i]][ ,j]))}\n    colnames(tt) <- colnames(obj[[i]])\n    c1 <- c(c1, ct, rep(\"\", nrow(tt)))\n    c2 <- rbind(c2, tt, rep(\"\", ncol(tt)))\n  }\n  dimnames(c2)[[1]] <- 1:nrow(c2)\n  res <- cbind(c1, c2)[-length(c1), ,drop=FALSE]\n  dimnames(res)[[2]] <- c(\"gene.list\", lcn)\n  dimnames(res)[[1]] <- 1:nrow(res)\n  write.table(res, file=file, row.names=FALSE, sep=\",\", ...)\n  invisible(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/ibsc.comp.R",
    "language": "R",
    "content": "`ibsc.comp` <-\nfunction(bsc1, bsc2, time) {\n\tif((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) { stop(\"bsc1, bsc2 and time must have the same length!\") }\n\tcc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n\tbsc1 <- bsc1[cc.ix]\n\tbsc2 <- bsc2[cc.ix]\n\ttime <- time[cc.ix]\n\tdiffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n\tibsc1 <- sum(diffs * bsc1) / max(time)\n\tibsc2 <- sum(diffs * bsc2) / max(time)\n\trr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n\treturn(list(\"p.value\"=rr$p.value, \"ibsc1\"=ibsc1, \"ibsc2\"=ibsc2))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ibsc.comp` function and what are its input parameters?",
        "answer": "The `ibsc.comp` function is designed to compare two sets of BSC (possibly 'Balanced Scorecard') values over time. It takes three input parameters: `bsc1` and `bsc2` (two sets of BSC values to be compared) and `time` (the corresponding time points for the BSC values). The function calculates integrated BSC values for both sets and performs a Wilcoxon signed-rank test to compare them."
      },
      {
        "question": "How does the function handle missing or duplicate data in the input?",
        "answer": "The function handles missing or duplicate data by using the `complete.cases()` function to remove any rows with missing values across all three inputs (bsc1, bsc2, and time). It also removes duplicate time points using `!duplicated(time)`. This ensures that only complete and unique data points are used in the calculations, which is crucial for accurate results in time series analysis."
      },
      {
        "question": "What statistical test is performed in this function, and what does the result represent?",
        "answer": "The function performs a Wilcoxon signed-rank test using the `wilcox.test()` function. This is a non-parametric test used to compare two related samples. In this case, it's comparing `bsc1` and `bsc2` with the alternative hypothesis that `bsc1` is less than `bsc2` (indicated by `alternative=\"less\"`). The test is paired (indicated by `paired=TRUE`) and uses a normal approximation instead of exact p-values (indicated by `exact=FALSE`). The resulting p-value represents the probability of observing such a difference between `bsc1` and `bsc2` by chance, assuming the null hypothesis is true."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ibsc.comp <- function(bsc1, bsc2, time) {\n  if((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) {\n    stop(\"bsc1, bsc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n  ibsc1 <- sum(diffs * bsc1) / max(time)\n  ibsc2 <- sum(diffs * bsc2) / max(time)\n  # Complete the function by adding the Wilcoxon test and return statement\n}",
        "complete": "ibsc.comp <- function(bsc1, bsc2, time) {\n  if((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) {\n    stop(\"bsc1, bsc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n  ibsc1 <- sum(diffs * bsc1) / max(time)\n  ibsc2 <- sum(diffs * bsc2) / max(time)\n  rr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"ibsc1\"=ibsc1, \"ibsc2\"=ibsc2))\n}"
      },
      {
        "partial": "ibsc.comp <- function(bsc1, bsc2, time) {\n  # Add input validation\n  \n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  # Calculate diffs and ibsc values\n  \n  rr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n  # Return the results\n}",
        "complete": "ibsc.comp <- function(bsc1, bsc2, time) {\n  if((length(bsc1) + length(bsc2) + length(time)) != 3 * length(time)) {\n    stop(\"bsc1, bsc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(bsc1, bsc2, time) & !duplicated(time)\n  bsc1 <- bsc1[cc.ix]\n  bsc2 <- bsc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  ibsc1 <- sum(diffs * bsc1) / max(time)\n  ibsc2 <- sum(diffs * bsc2) / max(time)\n  rr <- wilcox.test(x=bsc1, y=bsc2, alternative=\"less\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"ibsc1\"=ibsc1, \"ibsc2\"=ibsc2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/rankGeneDrugPerturbation.R",
    "language": "R",
    "content": "# Rank genes based on drug effect\n#\n# A helper function called from within `drugPerturbationSig`. This is intended\n#   for developer use only; if you aren't debugging the package, this should\n#   not be used.\n#\n# @param data: gene expression data matrix\n# @param drug: single or vector of drug(s) of interest; if a vector of drugs is\n#    provided, they will be considered as being the same drug and will be\n#    jointly analyszed\n# @param drug.id: drug used in each experiment\n# @param drug.concentration: drug concentration used in each experiment\n# @param type: cell or tissue type for each experiment\n# @param xp: type of experiment (perturbation or control)\n# @param batch: experiment batches\n# @param duration: The duration of the experiment, in a consistent unit\n# @param single.type: Should the statitsics be computed for each cell/tissue\n#   type separately?\n# @param nthread: number of parallel threads (bound to the maximum number of\n#   cores available)\n#\n# @return [list] of \\code{data.frame}s with the statistics for each gene, for\n#   each type\n#\n# @keywords internal\n# @export\nrankGeneDrugPerturbation <-\n  function(data, drug, drug.id, drug.concentration, type, xp, batch, duration,\n           single.type=FALSE, nthread=1, verbose=FALSE) {\n\n    if (nthread != 1) {\n      availcore <- parallel::detectCores()\n      if (missing(nthread) || nthread < 1 || nthread > availcore) {\n        nthread <- availcore\n      }\n      else{\n      }\n    }\n\n    #### DIMENSIONALITY CHECK\n    if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n      stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n    }\n    names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n    if (!all(complete.cases(type, xp, batch, duration))) {\n      stop(\"type, batch, duration and xp should not contain missing values!\")\n    }\n\n    # Returns a matrix of NAs if there is no viability values for the requested dose levels\n    if (length(unique(xp)) < 2 ) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      res <- c(NULL, list(rest))\n      names(res) <- list(\"all\"=type)\n      return(res)\n    }\n\n    res.type <- NULL\n\n    ## build input matrix\n    inpumat <- NULL\n\n    ## for each batch/vehicle of perturbations+controls (test within each batch/vehicle to avoid batch effect)\n    ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n    names(ubatch) <- paste0(\"batch\", ubatch)\n\n\n    for (bb in seq_len(length(ubatch))) {\n      ## identify the perturbations and corresponding control experiments\n      xpix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"perturbation\"]\n\n      ctrlix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"control\"]\n\n      if (all(!is.na(c(xpix, ctrlix))) && length(xpix) > 0 && length(ctrlix) > 0) {\n        if (!all(is.element(ctrlix, rownames(data)))) {\n          stop(\"data for some control experiments are missing!\")\n        }\n        if (verbose) {\n          cat(sprintf(\"type %s: batch %i/%i -> %i vs %i\\n\", utype[bb], bb, length(ubatch), length(xpix), length(ctrlix)))\n        }\n        ## transformation of drug concentrations values - decision made to keep in \u00b5m\n        conc <- drug.concentration # / 10^6\n        inpumat <- rbind(inpumat, data.frame(\"treated\"=c(rep(1, length(xpix)), rep(0, length(ctrlix))), \"type\"=c(type[xpix], type[ctrlix]), \"batch\"=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"), \"concentration\"=c(conc[xpix], conc[ctrlix]), \"duration\"= c(duration[xpix], duration[ctrlix])))\n      }\n    }\n\n\n    inpumat[ , \"type\"] <- factor(inpumat[ , \"type\"], ordered=FALSE)\n    inpumat[ , \"batch\"] <- factor(inpumat[ , \"batch\"], ordered=FALSE)\n\n    if (nrow(inpumat) < 3 || length(sort(unique(inpumat[ , \"concentration\"]))) < 2){ #|| length(unique(inpumat[ , \"duration\"])) < 2) {\n      ## not enough experiments in drug list\n      warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n      return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n    }\n\n    res <- NULL\n    utype <- sort(unique(as.character(inpumat[ , \"type\"])))\n    ltype <- list(\"all\"=utype)\n\n    if(single.type) {\n      ltype <- c(ltype, as.list(utype))\n      names(ltype)[-1] <- utype\n    }\n\n    for(ll in seq_along(ltype)) {\n\n      ## select the type of cell line/tissue of interest\n      inpumat2 <- inpumat[!is.na(inpumat[ , \"type\"]) & is.element(inpumat[ , \"type\"], ltype[[ll]]), , drop=FALSE]\n      inpumat2 <- inpumat2[complete.cases(inpumat2), , drop=FALSE]\n\n      if (nrow(inpumat2) < 3 || length(sort(unique(inpumat2[ , \"concentration\"]))) < 2) {\n        ## not enough experiments in data\n        nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n        rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n\n      } else {\n        ## test perturbation vs control\n        if(nthread > 1) {\n          ## parallel threads\n          splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n          ##TODO:: Can we reimplement this without using length?\n          splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n          mcres <- BiocParallel::bplapply(splitix, function(x, data, inpumat) {\n            res <- t(apply(data[rownames(inpumat), x, drop=FALSE], 2, ToxicoGx::geneDrugPerturbation, concentration=inpumat[ , \"concentration\"], type=inpumat[ , \"type\"], batch=inpumat[ , \"batch\"], duration=inpumat[,\"duration\"]))\n            return(res)\n          }, data=data, inpumat=inpumat2)\n          rest <- do.call(rbind, mcres)\n        } else {\n          rest <- t(apply(data[rownames(inpumat2), , drop=FALSE], 2, ToxicoGx::geneDrugPerturbation, concentration=inpumat2[ , \"concentration\"], type=inpumat2[ , \"type\"], batch=inpumat2[ , \"batch\"], duration=inpumat2[,\"duration\"]))\n        }\n      }\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      res <- c(res, list(rest))\n    }\n    names(res) <- names(ltype)\n    return(res)\n  }\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rankGeneDrugPerturbation` function and what are its main input parameters?",
        "answer": "The `rankGeneDrugPerturbation` function is a helper function used to rank genes based on drug effect. It's intended for developer use within the `drugPerturbationSig` function. The main input parameters include: 'data' (gene expression data matrix), 'drug' (single or vector of drugs of interest), 'drug.id' (drug used in each experiment), 'drug.concentration' (concentration used in each experiment), 'type' (cell or tissue type for each experiment), 'xp' (type of experiment: perturbation or control), 'batch' (experiment batches), and 'duration' (experiment duration)."
      },
      {
        "question": "How does the function handle parallel processing, and what condition determines if parallel processing will be used?",
        "answer": "The function handles parallel processing using the 'nthread' parameter. If 'nthread' is not equal to 1, the function will use parallel processing. It detects the available cores using `parallel::detectCores()` and sets 'nthread' to the number of available cores if the provided value is invalid. Parallel processing is implemented using `BiocParallel::bplapply` to apply the `geneDrugPerturbation` function across subsets of the data."
      },
      {
        "question": "What conditions might cause the function to return a matrix of NAs, and how does it handle different cell/tissue types?",
        "answer": "The function returns a matrix of NAs if there are fewer than two unique experiment types (perturbation and control) in the 'xp' parameter. It handles different cell/tissue types through the 'single.type' parameter. If 'single.type' is TRUE, it computes statistics for each cell/tissue type separately. The function creates a list of types to process, either containing all types combined ('all') or individual types. It then processes each type or combination of types separately, returning results for each in the final output list."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n\n  # Dimensionality check\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) {\n    stop(\"type, batch, duration and xp should not contain missing values!\")\n  }\n\n  # TODO: Implement the rest of the function\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n\n  # Dimensionality check\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) {\n    stop(\"type, batch, duration and xp should not contain missing values!\")\n  }\n\n  if (length(unique(xp)) < 2) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    rest <- cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n    return(list(all=rest))\n  }\n\n  inpumat <- NULL\n  ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n  names(ubatch) <- paste0(\"batch\", ubatch)\n\n  for (bb in seq_along(ubatch)) {\n    xpix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"perturbation\"]\n    ctrlix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"control\"]\n    if (all(!is.na(c(xpix, ctrlix))) && length(xpix) > 0 && length(ctrlix) > 0) {\n      if (!all(is.element(ctrlix, rownames(data)))) stop(\"data for some control experiments are missing!\")\n      conc <- drug.concentration\n      inpumat <- rbind(inpumat, data.frame(treated=c(rep(1, length(xpix)), rep(0, length(ctrlix))),\n                                          type=c(type[xpix], type[ctrlix]),\n                                          batch=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"),\n                                          concentration=c(conc[xpix], conc[ctrlix]),\n                                          duration=c(duration[xpix], duration[ctrlix])))\n    }\n  }\n\n  inpumat$type <- factor(inpumat$type, ordered=FALSE)\n  inpumat$batch <- factor(inpumat$batch, ordered=FALSE)\n\n  if (nrow(inpumat) < 3 || length(unique(inpumat$concentration)) < 2) {\n    warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n\n  utype <- sort(unique(as.character(inpumat$type)))\n  ltype <- list(\"all\"=utype)\n  if(single.type) ltype <- c(ltype, as.list(utype))\n\n  res <- lapply(ltype, function(types) {\n    inpumat2 <- inpumat[!is.na(inpumat$type) & inpumat$type %in% types, ]\n    inpumat2 <- inpumat2[complete.cases(inpumat2), ]\n\n    if (nrow(inpumat2) < 3 || length(unique(inpumat2$concentration)) < 2) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    } else {\n      rest <- if(nthread > 1) {\n        splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n        mcres <- BiocParallel::bplapply(splitix, function(x, data, inpumat) {\n          t(apply(data[rownames(inpumat), x, drop=FALSE], 2, ToxicoGx::geneDrugPerturbation,\n                  concentration=inpumat$concentration, type=inpumat$type,\n                  batch=inpumat$batch, duration=inpumat$duration))\n        }, data=data, inpumat=inpumat2)\n        do.call(rbind, mcres)\n      } else {\n        t(apply(data[rownames(inpumat2), , drop=FALSE], 2, ToxicoGx::geneDrugPerturbation,\n                concentration=inpumat2$concentration, type=inpumat2$type,\n                batch=inpumat2$batch, duration=inpumat2$duration))\n      }\n    }\n    cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n  })\n\n  names(res) <- names(ltype)\n  return(res)\n}"
      },
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  # TODO: Implement input validation and preprocessing\n\n  res <- NULL\n  utype <- sort(unique(as.character(inpumat[, \"type\"])))\n  ltype <- list(\"all\"=utype)\n\n  if(single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  for(ll in seq_along(ltype)) {\n    # TODO: Implement the main analysis loop\n  }\n\n  names(res) <- names(ltype)\n  return(res)\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) nthread <- availcore\n  }\n\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) stop(\"type, batch, duration and xp should not contain missing values!\")\n\n  if (length(unique(xp)) < 2) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    return(list(all=cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))))\n  }\n\n  inpumat <- NULL\n  ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n  names(ubatch) <- paste0(\"batch\", ubatch)\n\n  for (bb in ubatch) {\n    xpix <- which(complete.cases(batch, xp) & batch == bb & xp == \"perturbation\")\n    ctrlix <- which(complete.cases(batch, xp) & batch == bb & xp == \"control\")\n    if (length(xpix) > 0 && length(ctrlix) > 0) {\n      if (!all(ctrlix %in% seq_len(nrow(data)))) stop(\"data for some control experiments are missing!\")\n      if (verbose) cat(sprintf(\"type %s: batch %i/%i -> %i vs %i\\n\", utype[bb], bb, length(ubatch), length(xpix), length(ctrlix)))\n      inpumat <- rbind(inpumat, data.frame(treated=c(rep(1, length(xpix)), rep(0, length(ctrlix))),\n                                          type=c(type[xpix], type[ctrlix]),\n                                          batch=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"),\n                                          concentration=c(drug.concentration[xpix], drug.concentration[ctrlix]),\n                                          duration=c(duration[xpix], duration[ctrlix])))\n    }\n  }\n\n  inpumat$type <- factor(inpumat$type, ordered=FALSE)\n  inpumat$batch <- factor(inpumat$batch, ordered=FALSE)\n\n  if (nrow(inpumat) < 3 || length(unique(inpumat$concentration)) < 2) {\n    warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n\n  utype <- sort(unique(as.character(inpumat$type)))\n  ltype <- list(\"all\"=utype)\n  if(single.type) ltype <- c(ltype, as.list(utype))\n\n  res <- lapply(ltype, function(types) {\n    inpumat2 <- inpumat[inpumat$type %in% types, ]\n    inpumat2 <- inpumat2[complete.cases(inpumat2), ]\n\n    if (nrow(inpumat2) < 3 || length(unique(inpumat2$concentration)) < 2) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    } else {\n      rest <- if(nthread > 1) {\n        splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n        do.call(rbind, BiocParallel::bplapply(splitix, function(x, data, inpumat) {\n          t(apply(data[rownames(inpumat), x, drop=FALSE], 2, ToxicoGx::geneDrugPerturbation,\n                  concentration=inpumat$concentration, type=inpumat$type,\n                  batch=inpumat$"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/mrmr.cindex.ensemble.R",
    "language": "R",
    "content": "`mrmr.cindex.ensemble` <-\nfunction(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n\tnvar<-ncol(x)\n\tnsample<-nrow(x)\n\n\tvec_ensemble<-.Call(.C_mrmr_cIndex_ensemble_remove,data.matrix(x),as.integer(is.na(x)),maxparents,ncol(x),nrow(x),1,1,nboot,maxnsol,-1000,as.integer(as.logical(TRUE)),as.integer(sort(unique(strat))),as.integer(cl ),as.double(surv.time),as.integer(surv.event),as.double(weights),as.integer(strat),as.integer(sum(weights)),as.integer(as.logical(outx)),as.integer(length(strat)),as.integer(length(sort(unique(strat)))))\n\n\tvec_ensemble[2:vec_ensemble[1]+1]<-vec_ensemble[2:vec_ensemble[1]+1]-1\n\n\tmodels.equiv <- .extract.all.parents(x,vec_ensemble,maxparents,1)\n\tmodels.equiv[1,]<-rep(\"T\",ncol(models.equiv))\n\n\treturn(models.equiv)\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mrmr.cindex.ensemble` function and what are its main input parameters?",
        "answer": "The `mrmr.cindex.ensemble` function appears to be a statistical analysis tool for survival data. Its main purpose is likely to perform ensemble-based feature selection using the minimum redundancy maximum relevance (mRMR) criterion and concordance index (C-index) for survival analysis. The main input parameters include:\n- `x`: A matrix of predictor variables\n- `surv.time`: Survival time data\n- `surv.event`: Survival event indicator\n- `cl`: Likely cluster information for parallel processing\n- `weights`: Sample weights\n- `strat`: Stratification variable\n- `maxparents`: Maximum number of parent variables\n- `maxnsol`: Maximum number of solutions\n- `nboot`: Number of bootstrap iterations"
      },
      {
        "question": "Explain the significance of the `.Call` function in this code and what it's doing.",
        "answer": "The `.Call` function in this code is used to interface with compiled C code, specifically calling a function named `.C_mrmr_cIndex_ensemble_remove`. This approach is often used in R for performance-critical operations. The function is likely implemented in C for speed and efficiency, especially when dealing with large datasets or complex computations. The `.Call` function allows passing R objects directly to C code and receiving results back, enabling seamless integration of compiled code within the R environment. In this case, it's performing the core computational task of the ensemble method, processing the input data, and returning results that are then further processed in R."
      },
      {
        "question": "What does the function return, and how is this return value processed before being output?",
        "answer": "The function returns a matrix called `models.equiv`, which likely represents equivalent models or feature sets identified by the ensemble method. The processing of the return value involves several steps:\n1. The results from the C function call are stored in `vec_ensemble`.\n2. An index adjustment is made to `vec_ensemble` by subtracting 1 from a subset of its elements.\n3. The `.extract.all.parents` function is called with `x`, `vec_ensemble`, `maxparents`, and 1 as arguments to generate `models.equiv`.\n4. The first row of `models.equiv` is set to a vector of 'T' values.\n5. Finally, the processed `models.equiv` matrix is returned.\n\nThis processing suggests that the function is converting the raw output from the C function into a more interpretable format, possibly indicating selected features or model structures in a binary (True/False) matrix form."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  nvar <- ncol(x)\n  nsample <- nrow(x)\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  # Complete the function by adjusting vec_ensemble and extracting models\n\n}",
        "complete": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  nvar <- ncol(x)\n  nsample <- nrow(x)\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  vec_ensemble[2:vec_ensemble[1]+1] <- vec_ensemble[2:vec_ensemble[1]+1] - 1\n\n  models.equiv <- .extract.all.parents(x, vec_ensemble, maxparents, 1)\n  models.equiv[1,] <- rep(\"T\", ncol(models.equiv))\n\n  return(models.equiv)\n}"
      },
      {
        "partial": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  # Add necessary variable initializations\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  # Complete the function by adjusting vec_ensemble and extracting models\n\n}",
        "complete": "mrmr.cindex.ensemble <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), maxparents, maxnsol, nboot=200, na.rm=FALSE) {\n\n  nvar <- ncol(x)\n  nsample <- nrow(x)\n\n  vec_ensemble <- .Call(.C_mrmr_cIndex_ensemble_remove, data.matrix(x), as.integer(is.na(x)), maxparents, ncol(x), nrow(x), 1, 1, nboot, maxnsol, -1000, as.integer(as.logical(TRUE)), as.integer(sort(unique(strat))), as.integer(cl), as.double(surv.time), as.integer(surv.event), as.double(weights), as.integer(strat), as.integer(sum(weights)), as.integer(as.logical(outx)), as.integer(length(strat)), as.integer(length(sort(unique(strat)))))\n\n  vec_ensemble[2:vec_ensemble[1]+1] <- vec_ensemble[2:vec_ensemble[1]+1] - 1\n\n  models.equiv <- .extract.all.parents(x, vec_ensemble, maxparents, 1)\n  models.equiv[1,] <- rep(\"T\", ncol(models.equiv))\n\n  return(models.equiv)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassAssignmentMethods.R",
    "language": "R",
    "content": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `test_that()` function in this code snippet?",
        "answer": "The `test_that()` function is used to define a unit test in R. It takes two arguments: a description of the test (as a string) and a code block containing the test expectations. In this case, it's testing that 'multiplication works'."
      },
      {
        "question": "What does the `expect_equal()` function do in this test?",
        "answer": "The `expect_equal()` function is an expectation function from the testthat package in R. It compares two values for equality. In this case, it's checking if the result of `2 * 2` is equal to `4`. If the comparison is true, the test passes; if not, it fails."
      },
      {
        "question": "How would you modify this test to check if division works correctly?",
        "answer": "To check if division works correctly, you could modify the test as follows:\n\ntest_that(\"division works\", {\n  expect_equal(8 / 2, 4)\n})\n\nThis test would verify that 8 divided by 2 equals 4, thus testing the division operation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, ___)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      },
      {
        "partial": "test_that(___, {\n  expect_equal(2 * 2, 4)\n})",
        "complete": "test_that(\"multiplication works\", {\n  expect_equal(2 * 2, 4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/D.index.R",
    "language": "R",
    "content": "`D.index` <-\nfunction(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n\t\n\tstrata <- survival::strata\n\t#require(SuppDists)\n\tmethod.test <- match.arg(method.test)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n\t\t## remove weights=0 because the coxph function does not deal with them properly\n\t\tiix <- weights <= 0\n\t\tif(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n\t\tweights[iix] <- NA\n\t} else { weights <- rep(1,  length(x)) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1,  length(x)) }\n\tcc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n\tif(sum(cc.ix) < 3) {\n\t## not enough observations\n\t\tdata <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n\t\treturn(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n\t}\n\tif(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n\tsx <- x[cc.ix]\n\too <- order(sx, decreasing=FALSE)\n\tsx <- sx[oo]\n\tstime <- surv.time[cc.ix][oo]\n\tsevent <- surv.event[cc.ix][oo]\n\tsweights <- weights[cc.ix][oo]\n\tsstrat <- strat[cc.ix][oo]\n\tkap <- sqrt(8/pi)\n\tz <- kap^-1 * SuppDists::normOrder(N=length(sx))\n\t#ties?\n\tdup <- duplicated(sx)\n\tif(any(dup)) {\n\t\tudup <- unique(sx[dup])\n\t\tfor(i in 1:length(udup)) { z[sx == udup[i]] <- mean(z[sx == udup[i]]) }\n\t}\n\tz2 <- x\n\tz2[!cc.ix] <- NA\n\tz2[cc.ix] <- z[match(1:length(sx), oo)]\n\tdata <- list(\"x\"=x, \"z\"=z2, \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n\t#fit the cox model\n\toptions(warn=2)\n\trr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + z, weights=sweights, ...))\n\toptions(warn=0)\n\tif(class(rr) == \"try-error\") {\n\t\tres <- list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n\t} else {\n\t\tdicoef <- rr$coefficients\n\t\tdise <- sqrt(drop(rr$var))\n\t\tnames(dicoef) <- names(dise) <- NULL\n\t\tmystat <- NA\n\t\tswitch(method.test,\n\t\t\"logrank\"={\n\t\t\tmystat <- rr$score\n\t\t},\n\t\t\"likelihood.ratio\"={\n\t\t\tmysat <- 2 * (rr$loglik[2] - rr$loglik[1])\n\t\t},\n\t\t\"wald\"={\n\t\t\tmystats <- rr$wald.test\n\t\t\t##(hrcoef / hrse)^2\n\t\t})\n\t\tmypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n\t\tres <- list(\"d.index\"=exp(dicoef), \"coef\"=dicoef, \"se\"=dise, \"lower\"=exp(dicoef - qnorm(alpha / 2, lower.tail=FALSE) * dise), \"upper\"=exp(dicoef + qnorm(alpha / 2, lower.tail=FALSE) * dise), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n\t}\n\n\treturn(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `D.index` function and what are its main input parameters?",
        "answer": "The `D.index` function is used to calculate a D-index (discrimination index) for survival analysis. Its main input parameters are:\n- `x`: The predictor variable\n- `surv.time`: Survival time\n- `surv.event`: Event indicator\n- `weights`: Optional weights for each observation\n- `strat`: Optional stratification variable\n- `alpha`: Significance level for confidence intervals (default 0.05)\n- `method.test`: Test method ('logrank', 'likelihood.ratio', or 'wald')\n- `na.rm`: Whether to remove NA values"
      },
      {
        "question": "How does the function handle missing data and what happens if there are fewer than 3 complete cases?",
        "answer": "The function handles missing data as follows:\n1. It checks for complete cases using `complete.cases()` function.\n2. If `na.rm=FALSE` (default) and there are any NA values, it stops with an error.\n3. If there are fewer than 3 complete cases, it returns a list with NA values for most results, including the calculated indices.\n4. The function only proceeds with calculations if there are at least 3 complete cases.\n5. If `na.rm=TRUE`, it will remove NA values and continue with the analysis using the remaining complete cases."
      },
      {
        "question": "Explain the purpose of the `z` variable in the function and how it's calculated.",
        "answer": "The `z` variable in the function represents a normalized rank transformation of the input variable `x`. It's calculated as follows:\n1. The input `x` is ordered.\n2. `z` is initially calculated using `SuppDists::normOrder(N=length(sx))`, which returns the expected values of standard normal order statistics.\n3. This is then scaled by `kap^-1`, where `kap = sqrt(8/pi)`.\n4. For tied values in `x`, the corresponding `z` values are averaged.\n5. The purpose of this transformation is to create a normalized version of the input variable, which is then used in the Cox proportional hazards model to estimate the D-index."
      }
    ],
    "completion_tasks": [
      {
        "partial": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  strata <- survival::strata\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  # Complete the function here\n}",
        "complete": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  strata <- survival::strata\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  kap <- sqrt(8/pi)\n  z <- kap^-1 * SuppDists::normOrder(N=length(sx))\n  dup <- duplicated(sx)\n  if(any(dup)) {\n    udup <- unique(sx[dup])\n    for(i in seq_along(udup)) { z[sx == udup[i]] <- mean(z[sx == udup[i]]) }\n  }\n  z2 <- x\n  z2[!cc.ix] <- NA\n  z2[cc.ix] <- z[match(seq_along(sx), oo)]\n  data <- list(\"x\"=x, \"z\"=z2, \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + z, weights=sweights, ...))\n  options(warn=0)\n  if(inherits(rr, \"try-error\")) {\n    return(list(\"d.index\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  dicoef <- rr$coefficients\n  dise <- sqrt(drop(rr$var))\n  names(dicoef) <- names(dise) <- NULL\n  mystat <- switch(method.test,\n    \"logrank\" = rr$score,\n    \"likelihood.ratio\" = 2 * diff(rr$loglik),\n    \"wald\" = rr$wald.test\n  )\n  mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n  ci <- exp(dicoef + c(-1, 1) * qnorm(1 - alpha/2) * dise)\n  list(\"d.index\"=exp(dicoef), \"coef\"=dicoef, \"se\"=dise, \"lower\"=ci[1], \"upper\"=ci[2], \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n}"
      },
      {
        "partial": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  # Add necessary imports and argument checks\n  \n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    # Handle case with insufficient observations\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  \n  # Prepare data for Cox model\n  \n  # Fit Cox model and calculate statistics\n  \n  # Return results\n}",
        "complete": "D.index <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  strata <- survival::strata\n  method.test <- match.arg(method.test)\n  \n  if(missing(weights)) weights <- rep(1, length(x))\n  else if(length(weights) != length(x)) stop(\"bad length for parameter weights!\")\n  \n  if(missing(strat)) strat <- rep(1, length(x))\n  else if(length(strat) != length(x)) stop(\"bad length for parameter strat!\")\n  \n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(x=x, z=rep(NA, length(x)), surv.time=surv.time, surv.event=surv.event, weights=weights, strat=strat)\n    return(list(d.index=NA, coef=NA, se=NA, lower=NA, upper=NA, p.value=NA, n=sum(cc.ix), coxm=NA, data=data))\n  }\n  if(any(!cc.ix) & !na.rm) stop(\"NA values are present!\")\n  \n  sx <- x[cc.ix]\n  oo <- order(sx)\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  \n  z <- sqrt(8/pi)^-1 * SuppDists::normOrder(N=length(sx))\n  z[duplicated(sx)] <- ave(z, sx, FUN = mean)[duplicated(sx)]\n  \n  z2 <- x\n  z2[cc.ix] <- z[order(oo)]\n  z2[!cc.ix] <- NA\n  \n  data <- list(x=x, z=z2, surv.time=surv.time, surv.event=surv.event, weights=weights, strat=strat)\n  \n  rr <- tryCatch({\n    coxph(Surv(stime, sevent) ~ strata(sstrat) + z, weights=sweights, ...)\n  }, error = function(e) NULL)\n  \n  if(is.null(rr)) {\n    return(list(d.index=NA, coef=NA, se=NA, lower=NA, upper=NA, p.value=NA, n=sum(cc.ix), coxm=NA, data=data))\n  }\n  \n  dicoef <- rr$coefficients\n  dise <- sqrt(drop(rr$var))\n  mystat <- switch(method.test,\n    logrank = rr$score,\n    likelihood.ratio = 2 * diff(rr$loglik),\n    wald = rr$wald.test\n  )\n  mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n  ci <- exp(dicoef + c(-1, 1) * qnorm(1 - alpha/2) * dise)\n  \n  list(d.index=exp(dicoef), coef=dicoef, se=dise, lower=ci[1], upper=ci[2],\n       p.value=mypp, n=rr$n, coxm=rr, data=data)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/zzz.R",
    "language": "R",
    "content": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", pkgname, libname)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .onLoad function in R package development?",
        "answer": "The .onLoad function is a special function in R package development that is automatically called when the package is loaded. It is used to perform initialization tasks, such as loading dynamic libraries or setting up package-specific environments. In this case, it's used to load a dynamic library named 'survcomp' using the library.dynam function."
      },
      {
        "question": "What are the parameters 'libname' and 'pkgname' in the .onLoad function, and how are they used?",
        "answer": "The parameters 'libname' and 'pkgname' are automatically provided by R when the package is loaded. 'libname' is the path where the package is installed, and 'pkgname' is the name of the package. In this function, they are used as arguments to the library.dynam function to correctly locate and load the dynamic library associated with the package."
      },
      {
        "question": "What does the library.dynam function do in this context, and why is it important?",
        "answer": "The library.dynam function is used to load compiled code (typically C or Fortran) that is part of the R package. It's important because it allows the package to include and use efficient, compiled code alongside R functions. In this case, it's loading a library named 'survcomp', which likely contains compiled functions that are essential for the package's functionality."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onLoad <- function(libname, pkgname)\n{\n  # Load the dynamic library for the package\n  \n}",
        "complete": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", pkgname, libname)\n}"
      },
      {
        "partial": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", _, _)\n}",
        "complete": ".onLoad <- function(libname, pkgname)\n{\n  library.dynam(\"survcomp\", pkgname, libname)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/oncotypedx.R",
    "language": "R",
    "content": "#' @title Function to compute the OncotypeDX signature as published by\n#'   Paik et al. in 2004.\n#'\n#' @description\n#' This function computes signature scores and risk classifications from\n#'   gene expression values following the algorithm used for the OncotypeDX\n#'   signature as published by Paik et al. 2004.\n#'\n#' @usage\n#' oncotypedx(data, annot, do.mapping = FALSE, mapping, do.scaling=TRUE,\n#'   verbose = FALSE)\n#'\n#' @param data Matrix of gene expressions with samples in rows and\n#'   probes in columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must\n#'   be performed (in case of ambiguities, the most variant probe is kept\n#'   for each gene), FALSE otherwise. Note that for Affymetrix HGU\n#'   datasets, the mapping is not necessary.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used\n#'   to force the mapping such that the probes are not selected based on\n#'   their variance.\n#' @param do.scaling Should the data be scaled?\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' Note that for Affymetrix HGU datasets, the mapping is not necessary.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' S. Paik, S. Shak, G. Tang, C. Kim, J. Bakker, M. Cronin, F. L. Baehner,\n#'   M. G. Walker, D. Watson, T. Park, W. Hiller, E. R. Fisher, D. L. Wickerham,\n#'   J. Bryant, and N. Wolmark (2004) \"A Multigene Assay to Predict Recurrence\n#'   of Tamoxifen-Treated, Node-Negative Breast Cancer\", New England Journal\n#'   of Medicine, 351(27):2817-2826.\n#'\n#' @examples\n#' # load GENE70 signature\n#' data(sig.oncotypedx)\n#' # load NKI dataset\n#' data(nkis)\n#' # compute relapse score\n#' rs.nkis <- oncotypedx(data=data.nkis, annot=annot.nkis, do.mapping=TRUE)\n#' table(rs.nkis$risk)\n#'\n#' @md\n#' @export\noncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE,\n                       verbose=FALSE) {\n\t\n\tif (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n\n\t## the reference genes are not taken into account due to their absence from most platforms\n\tsig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n\tdimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n\tgt <- nrow(sig2)\n\tif(do.mapping) { ## not an affy HGU platform\n\t\tgid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid1) <- dimnames(sig2)[[1]]\n\t\tgid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\t\t## mqpping\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tgm <- length(rr$geneid2)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tif(length(rr$geneid1) != gt) { ## some genes are missing\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n\t\t}\n\t\tgid1 <- rr$geneid2\n\t\tgid2 <- rr$geneid1\n\t\tdata <- rr$data1\n\t\tmyprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n\t\t## change the names of probes in the data\n\t\tdimnames(data)[[2]] <- names(gid2) <- names(gid1)\n\t} else {\n\t\tmyprobe <- NA\n\t\tdata <- data[ ,intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])]\n\t\tgm <- ncol(data)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tif(nrow(sig2) != ncol(data)) { ## some genes are missing\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\", ncol(data), gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=myprobe))\n\t\t}\n\t}\n\t## rename gene names by the gene symbols\n\tdimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n\n\tif (do.scaling) {\n\t\t## scaling between 0 and 15\n\t\tdata <- apply(data, 2, function(x) { xx <- (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE)); return(xx * 15) })\n\t} else if (max(data, na.rm=TRUE) > 20 || min(data, na.rm=TRUE) < -5) {\n\t\t## check that each gene expression lies approximately in [0, 15]\n\t\twarning(\"The max and min values of your data suggest it is not already scaled...\n\t\t\tIf it is please ignore this message, otherwise set `do.scaling=TRUE` to scale in the function call.\"\n\t\t\t)\n\t}\n\n\t## OcotypeDX recurrence score\n\t## GRB7 group score = 0.9 * GRB7 + 0.1 * HER2 if result < 8, then result = 8\n\t## ER group score = (0.8 * ER + 1.2 * PGR + BCL2 + SCUBE2) / 4\n\t## proliferation group score = ( survivin + KI67 + MYBL2 + CCNB1 + STK15) / 5 if result < 6.5, then result = 6.5\n\t## invasion group score = (CTSL2 + MMP11) / 2\n\t## RSU = + 0.47 * GRB7 group score - 0.34 * ER group score + 1.04 * proliferation group score + 0.10 * invasion group score + 0.05 * CD68 - 0.08 GSTM1 - 0.07 * BAG1\n\n\tcc.ix <- complete.cases(data)\n\trs <- rs.unscaled <- rsrisk <- NULL\n\tfor (i in 1:nrow(data)) {\n\t\tif(cc.ix[i]) {\n\t\t\tgrb7.gs <- 0.9 * data[i, \"GRB7\"] + 0.1 * data[i, \"ERBB2\"]\n\t\t\tif (grb7.gs < 8) { grb7.gs <- 8 }\n\n\t\t\ter.gs <- (0.8 * data[i, \"ESR1\"] + 1.2 * data[i, \"PGR\"] + data[i, \"BCL2\"] + data[i, \"SCUBE2\"]) / 4\n\n\t\t\tproliferation.gs <- (data[i, \"BIRC5\"] + data[i, \"MKI67\"] + data[i, \"MYBL2\"] + data[i, \"CCNB1\"] + data[i, \"AURKA\"]) / 5\n\t\t\tif (proliferation.gs < 6.5) { proliferation.gs <- 6.5 }\n\n\t\t\tinvasion.gs <- (data[i, \"CTSL2\"] + data[i, \"MMP11\"]) / 2\n\n\t\t\trsu <- 0.47 * (grb7.gs) - 0.34 * (er.gs) + 1.04 * (proliferation.gs) + 0.1 * (invasion.gs) + 0.05 * data[i, \"CD68\"] - 0.08 * data[i, \"GSTM1\"] - 0.07 * data[i, \"BAG1\"]\n\t\t\t## rescale the score\n\t\t\trsu2 <- rsu\n\t\t\tif(rsu >= 0 & rsu <= 100) { rsu <- 20 * (rsu - 6.7) }\n\t\t\tif(rsu < 0) { rsu <- 0 }\n\t\t\tif(rsu > 100) { rsu <- 100 }\n\t\t\t## use of the official curoffs\n\t\t\tif(rsu < 18) { rsr <- 0 }\n\t\t\tif(rsu >= 18 & rsu < 31) { rsr <- 0.5 }\n\t\t\tif(rsu >= 31) { rsr <- 1 }\n\t\t}\n\t\telse { rsu <- rsr <- rsu2 <- NA }\n\t\trs.unscaled <- c(rs.unscaled, rsu2)\n\t\trs <- c(rs, rsu)\n\t\trsrisk <- c(rsrisk, rsr)\n\t}\n\tnames(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n\treturn(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  # TODO: Implement mapping logic\n  \n  # TODO: Implement scaling and score calculation\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
        "complete": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(length(rr$geneid1) != gt) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n  } else {\n    myprobe <- NA\n    data <- data[ ,intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(nrow(sig2) != ncol(data)) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", ncol(data), gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=myprobe))\n    }\n  }\n  \n  dimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n  \n  if (do.scaling) {\n    data <- apply(data, 2, function(x) { xx <- (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE)); return(xx * 15) })\n  } else if (max(data, na.rm=TRUE) > 20 || min(data, na.rm=TRUE) < -5) {\n    warning(\"The max and min values of your data suggest it is not already scaled...\\nIf it is please ignore this message, otherwise set `do.scaling=TRUE` to scale in the function call.\")\n  }\n  \n  cc.ix <- complete.cases(data)\n  rs <- rs.unscaled <- rsrisk <- NULL\n  for (i in 1:nrow(data)) {\n    if(cc.ix[i]) {\n      grb7.gs <- max(0.9 * data[i, \"GRB7\"] + 0.1 * data[i, \"ERBB2\"], 8)\n      er.gs <- (0.8 * data[i, \"ESR1\"] + 1.2 * data[i, \"PGR\"] + data[i, \"BCL2\"] + data[i, \"SCUBE2\"]) / 4\n      proliferation.gs <- max((data[i, \"BIRC5\"] + data[i, \"MKI67\"] + data[i, \"MYBL2\"] + data[i, \"CCNB1\"] + data[i, \"AURKA\"]) / 5, 6.5)\n      invasion.gs <- (data[i, \"CTSL2\"] + data[i, \"MMP11\"]) / 2\n      rsu <- 0.47 * grb7.gs - 0.34 * er.gs + 1.04 * proliferation.gs + 0.1 * invasion.gs + 0.05 * data[i, \"CD68\"] - 0.08 * data[i, \"GSTM1\"] - 0.07 * data[i, \"BAG1\"]\n      rsu2 <- rsu\n      rsu <- max(min(20 * (rsu - 6.7), 100), 0)\n      rsr <- cut(rsu, breaks=c(-Inf, 18, 31, Inf), labels=c(0, 0.5, 1))\n    } else { rsu <- rsr <- rsu2 <- NA }\n    rs.unscaled <- c(rs.unscaled, rsu2)\n    rs <- c(rs, rsu)\n    rsrisk <- c(rsrisk, rsr)\n  }\n  names(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  # TODO: Implement mapping logic\n  \n  dimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n  \n  # TODO: Implement scaling logic\n  \n  # TODO: Implement score calculation\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
        "complete": "oncotypedx <- function(data, annot, do.mapping=FALSE, mapping, do.scaling=TRUE, verbose=FALSE) {\n  if (!exists('sig.oncotypedx')) data(sig.oncotypedx, envir=environment())\n  sig2 <- sig.oncotypedx[sig.oncotypedx[ , \"group\"] != \"reference\",  , drop=FALSE]\n  dimnames(sig2)[[1]] <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(length(rr$geneid1) != gt) {\n      return(list(\"score\"=rep(NA, nrow(data)), \"risk\"=rep(NA, nrow(data)), \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(rr$geneid2), \"EntrezGene.ID\"=rr$geneid2, \"new.probe\"=names(rr$geneid1))\n    dimnames(data)[[2]] <- names(rr$geneid1) <- names(rr$geneid2)\n  } else {\n    data <- data[ ,intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])]\n    mymapping <- c(\"mapped\"=ncol(data), \"total\"=gt)\n    myprobe <- NA\n    if(nrow(sig2) != ncol(data)) {\n      return(list(\"score\"=rep(NA, nrow(data)), \"risk\"=rep(NA, nrow(data)), \"mapping\"=mymapping, \"probe\"=myprobe))\n    }\n  }\n  \n  dimnames(data)[[2]] <- dimnames(sig2)[[1]] <- sig2[ , \"symbol\"]\n  \n  if (do.scaling) {\n    data <- apply(data, 2, function(x) { (x - min(x, na.rm=TRUE)) / (max(x, na.rm=TRUE) - min(x, na.rm=TRUE)) * 15 })\n  } else if (max(data, na.rm=TRUE) > 20 || min(data, na.rm=TRUE) < -5) {\n    warning(\"Data may not be scaled. Consider setting do.scaling=TRUE.\")\n  }\n  \n  cc.ix <- complete.cases(data)\n  rs <- rsrisk <- numeric(nrow(data))\n  for (i in which(cc.ix)) {\n    grb7.gs <- max(0.9 * data[i, \"GRB7\"] + 0.1 * data[i, \"ERBB2\"], 8)\n    er.gs <- (0.8 * data[i, \"ESR1\"] + 1.2 * data[i, \"PGR\"] + data[i, \"BCL2\"] + data[i, \"SCUBE2\"]) / 4\n    proliferation.gs <- max((data[i, \"BIRC5\"] + data[i, \"MKI67\"] + data[i, \"MYBL2\"] + data[i, \"CCNB1\"] + data[i, \"AURKA\"]) / 5, 6.5)\n    invasion.gs <- (data[i, \"CTSL2\"] + data[i, \"MMP11\"]) / 2\n    rsu <- 0.47 * grb7.gs - 0.34 * er.gs + 1.04 * proliferation.gs + 0.1 * invasion.gs + 0.05 * data[i, \"CD68\"] - 0.08 * data[i, \"GSTM1\"] - 0.07 * data[i, \"BAG1\"]\n    rs[i] <- max(min(20 * (rsu - 6.7), 100), 0)\n    rsrisk[i] <- cut(rs[i], breaks=c(-Inf, 18, 31, Inf), labels=c(0, 0.5, 1))\n  }\n  names(rs) <- names(rsrisk) <- rownames(data)\n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/extract.all.parents.R",
    "language": "R",
    "content": "'.extract.all.parents'  <-\nfunction(data,res.main,maxparents,predn) {\n### function taking the output of the regrnet.ensemble method and returns a matrix\n### containing one equivalent model in each column. The target variable is in the first row.\n### res.main: \toutput of regrnet.ensemble\n### maxparents:\tmaxparents parameter of the netinf method\n### predn:\tlist of target variables for which ensemble method was run.\n\n\tfinal <- NULL\n\tcnt_main <- 1\n\tfor(imain in 1:length(predn)){\n\t\tres.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n\t\tif(length(res.vec)>3){\n\t\t\tcnt_main <- cnt_main+2*res.main[cnt_main]+1\n\t\t\tnsol <- sum(res.vec==0)\n\t\t\tres <- matrix(0,ncol=nsol,nrow=(maxparents+1))\n\n\t\t\tval <- res.vec[2:(res.vec[1]+1)]\n\t\t\tind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n\t\t\tres[1,1:ind[1]] <- rep(val[1],ind[1])\n\t\t\tnvar <- length(val)\n\t\t\tlevel <- 2\n\t\t\tcnt <- 1\n\t\t\tnelem <- 0\n\t\t\tsum_old <- sum(ind[1])\n\t\t\tlast_level <- FALSE\n\t\t\ti <- 2\n\t\t\tind2 <- 1\n\n\t\t\twhile(i<=nvar && !last_level){\n\t\t\t\tif(ind[i]!=0){\n\t\t\t\t\tif(ind[i]>1){\n\t\t\t\t\t\ttmp <- res[,(ind2+1):nsol]\n\t\t\t\t\t\tres[level,ind2] <- val[i]\n\t\t\t\t\t\tfor(j in 1:level){\n\t\t\t\t\t\t\tres[j,(ind2+1):(ind2+ind[i]-1)] <- rep(res[j,ind2],(ind[i]-1))\n\t\t\t\t\t\t}\n\t\t\t\t\t\tif((nsol-(ind2+ind[i]-1))>0){\n\t\t\t\t\t\t\tres <- cbind(res[,1:(ind2+ind[i]-1)],tmp[,1:(nsol-(ind2+ind[i]-1))])\n\t\t\t\t\t\t}else{\n\t\t\t\t\t\t\tres <- res[,1:(ind2+ind[i]-1)]\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t\tres[level,ind2:(ind2-1+ind[i])] <- rep(val[i],ind[i])\n\t\t\t\t\tind2 <- ind2+ind[i]\n\t\t\t\t}else{\n\t\t\t\t\tres[level,ind2] <- val[i]\n\t\t\t\t\tind2 <- ind2+1\n\t\t\t\t}\n\t\t\t\tnelem <- nelem+ind[i]\n\t\t\t\tif(cnt==sum_old){\n\t\t\t\t\tsum_old <- nelem\n\t\t\t\t\tif(nelem==0){\n\t\t\t\t\t\tlast_level <- TRUE\n\t\t\t\t\t}\n\t\t\t\t\tnelem <- 0\n\t\t\t\t\tcnt <- 1\n\t\t\t\t\tlevel <- level+1\n\t\t\t\t\tind2 <- 1\n\t\t\t\t}\n\t\t\t\telse if(cnt< sum_old){\n\t\t\t\t\tcnt <- cnt+1\n\t\t\t\t}\n\t\t\t\ti <- i+1\n\t\t\t}\n\t\t\tfinal <- cbind(final,res)\n\t\t}\n\t}\n\tdimension <- dim(final)\n\tfinal <- colnames(data)[final]\n\tdim(final) <- dimension\n\treturn(final)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the '.extract.all.parents' function and what are its main input parameters?",
        "answer": "The '.extract.all.parents' function is designed to process the output of the regrnet.ensemble method and return a matrix containing one equivalent model in each column, with the target variable in the first row. The main input parameters are:\n1. data: The original dataset (implied by the use of colnames(data))\n2. res.main: The output of the regrnet.ensemble method\n3. maxparents: The maxparents parameter of the netinf method\n4. predn: A list of target variables for which the ensemble method was run"
      },
      {
        "question": "Explain the significance of the nested loop structure in the function and how it processes the input data.",
        "answer": "The nested loop structure in the function is crucial for processing the complex input data structure:\n1. The outer loop (for(imain in 1:length(predn))) iterates over each target variable in the predn list.\n2. Within this loop, there's a while loop that processes the data for each target variable, constructing the result matrix (res).\n3. The while loop handles different levels of parent variables, filling the res matrix column by column.\n4. It uses multiple counters and conditions (like cnt, ind2, last_level) to keep track of the current position and state in the data processing.\nThis structure allows the function to handle varying numbers of parent variables and solutions for each target variable, creating a flexible and comprehensive output."
      },
      {
        "question": "How does the function handle the conversion of numeric indices to column names in the final output, and why is this step important?",
        "answer": "The function handles the conversion of numeric indices to column names in the final output through these steps:\n1. It first processes all the data using numeric indices, storing the results in the 'final' matrix.\n2. After all processing is complete, it captures the dimensions of the 'final' matrix.\n3. It then uses these numeric indices to select the corresponding column names from the original data: final <- colnames(data)[final]\n4. Finally, it restores the original dimensions: dim(final) <- dimension\n\nThis step is important because:\n1. It converts the numeric representation of variables to their actual names, making the output more interpretable and usable.\n2. It maintains the connection between the processed results and the original dataset.\n3. It allows users to easily identify which variables are parents for each target variable in the resulting models.\n4. It enhances the readability and usability of the output for further analysis or model interpretation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      nvar <- length(val)\n      level <- 2\n      cnt <- 1\n      nelem <- 0\n      sum_old <- sum(ind[1])\n      last_level <- FALSE\n      i <- 2\n      ind2 <- 1\n\n      while(i <= nvar && !last_level) {\n        # Complete the while loop logic here\n      }\n      final <- cbind(final, res)\n    }\n  }\n  dimension <- dim(final)\n  final <- colnames(data)[final]\n  dim(final) <- dimension\n  return(final)\n}",
        "complete": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      nvar <- length(val)\n      level <- 2\n      cnt <- 1\n      nelem <- 0\n      sum_old <- sum(ind[1])\n      last_level <- FALSE\n      i <- 2\n      ind2 <- 1\n\n      while(i <= nvar && !last_level) {\n        if(ind[i] != 0) {\n          if(ind[i] > 1) {\n            tmp <- res[,(ind2+1):nsol]\n            res[level,ind2] <- val[i]\n            for(j in 1:level) {\n              res[j,(ind2+1):(ind2+ind[i]-1)] <- rep(res[j,ind2],(ind[i]-1))\n            }\n            if((nsol-(ind2+ind[i]-1)) > 0) {\n              res <- cbind(res[,1:(ind2+ind[i]-1)], tmp[,1:(nsol-(ind2+ind[i]-1))])\n            } else {\n              res <- res[,1:(ind2+ind[i]-1)]\n            }\n          }\n          res[level,ind2:(ind2-1+ind[i])] <- rep(val[i],ind[i])\n          ind2 <- ind2 + ind[i]\n        } else {\n          res[level,ind2] <- val[i]\n          ind2 <- ind2 + 1\n        }\n        nelem <- nelem + ind[i]\n        if(cnt == sum_old) {\n          sum_old <- nelem\n          if(nelem == 0) {\n            last_level <- TRUE\n          }\n          nelem <- 0\n          cnt <- 1\n          level <- level + 1\n          ind2 <- 1\n        } else if(cnt < sum_old) {\n          cnt <- cnt + 1\n        }\n        i <- i + 1\n      }\n      final <- cbind(final, res)\n    }\n  }\n  dimension <- dim(final)\n  final <- colnames(data)[final]\n  dim(final) <- dimension\n  return(final)\n}"
      },
      {
        "partial": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      # Complete the rest of the function here\n    }\n  }\n  # Final processing steps\n}",
        "complete": "'.extract.all.parents' <- function(data, res.main, maxparents, predn) {\n  final <- NULL\n  cnt_main <- 1\n  for(imain in 1:length(predn)) {\n    res.vec <- res.main[cnt_main:(cnt_main+2*res.main[cnt_main])]\n    if(length(res.vec) > 3) {\n      cnt_main <- cnt_main + 2*res.main[cnt_main] + 1\n      nsol <- sum(res.vec == 0)\n      res <- matrix(0, ncol=nsol, nrow=(maxparents+1))\n      val <- res.vec[2:(res.vec[1]+1)]\n      ind <- res.vec[(res.vec[1]+2):(2*res.vec[1]+1)]\n      res[1,1:ind[1]] <- rep(val[1], ind[1])\n      nvar <- length(val)\n      level <- 2\n      cnt <- 1\n      nelem <- 0\n      sum_old <- sum(ind[1])\n      last_level <- FALSE\n      i <- 2\n      ind2 <- 1\n\n      while(i <= nvar && !last_level) {\n        if(ind[i] != 0) {\n          if(ind[i] > 1) {\n            tmp <- res[,(ind2+1):nsol]\n            res[level,ind2] <- val[i]\n            for(j in 1:level) {\n              res[j,(ind2+1):(ind2+ind[i]-1)] <- rep(res[j,ind2],(ind[i]-1))\n            }\n            if((nsol-(ind2+ind[i]-1)) > 0) {\n              res <- cbind(res[,1:(ind2+ind[i]-1)], tmp[,1:(nsol-(ind2+ind[i]-1))])\n            } else {\n              res <- res[,1:(ind2+ind[i]-1)]\n            }\n          }\n          res[level,ind2:(ind2-1+ind[i])] <- rep(val[i],ind[i])\n          ind2 <- ind2 + ind[i]\n        } else {\n          res[level,ind2] <- val[i]\n          ind2 <- ind2 + 1\n        }\n        nelem <- nelem + ind[i]\n        if(cnt == sum_old) {\n          sum_old <- nelem\n          if(nelem == 0) {\n            last_level <- TRUE\n          }\n          nelem <- 0\n          cnt <- 1\n          level <- level + 1\n          ind2 <- 1\n        } else if(cnt < sum_old) {\n          cnt <- cnt + 1\n        }\n        i <- i + 1\n      }\n      final <- cbind(final, res)\n    }\n  }\n  dimension <- dim(final)\n  final <- colnames(data)[final]\n  dim(final) <- dimension\n  return(final)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/mrmr.cindex.R",
    "language": "R",
    "content": "`mrmr.cindex` <-\nfunction(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\tnvar<-ncol(x)\n\tnsample<-nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) { stop(\"bad length for parameter weights!\") }\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) { stop(\"weights must be a number between 0 and 1!\") }\n\t} else { weights <- rep(1, nsample) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1, nsample) }\n\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) { stop(\"binary classes and survival data are missing!\") }\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) { stop(\"choose binary classes or survival data but not both!\") }\n\n\n\tres_cIndex<-rep(0,nvar)\n\n\t\tmsurv <- FALSE\n\t\tif(missing(cl)) { ## survival data\n\t\t\tmsurv <- TRUE\n\t\t\tcl <- rep(0, nsample)\n\t\t} else { surv.time <- surv.event <- rep(0, nsample) } ## binary classes\n\n\n\t#### get cIndex for each variable in dataset with surv.time, surv.event ####\n\tfor(ivar in 1:nvar){\n\t\tis.correct <- TRUE\n\t\txx <- x[ ,ivar]\n\t\tcc.ix <- complete.cases(xx, surv.time, surv.event, cl, weights, strat)\n\n\t\tif(sum(cc.ix) < 3) {\n\t\t\t## not enough observations\n\t\t\tif(msurv) { data <- list(\"x\"=xx, \"surv.time\"=surv.time, \"surv.event\"=surv.event) } else { data  <- list(\"x\"=xx, \"cl\"=cl) }\n\t\t\treturn(list(\"c.index\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"data\"=data, \"comppairs\"=NA))\n\t\t}\n\n\t\tif(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n\n\t\t# remove samples whose the weight is equal to 0 to speed up the computation of the concordance index\n\t\tcc.ix <- cc.ix & weights != 0\n\t\tx2 <- xx[cc.ix]\n\t\tcl2 <- cl[cc.ix]\n\t\tst <- surv.time[cc.ix]\n\t\tse <- surv.event[cc.ix]\n\t\tif(msurv && sum(se) == 0) {\n\t\t\twarning(paste(\"\\nno events, the concordance index cannot be computed for variable \", colnames(x)[ivar],\" !\"))\n\t\t\tres_cIndex[ivar]<-NA\n\t\t\tis.correct<-FALSE\n\t\t}\n\t\tif(!msurv && length(unique(cl2)) == 1) {\n\t\t\twarning(paste(\"\\nonly one class, the concordance index cannot be computed for variable\", colnames(x)[ivar],\" !\"))\n\t\t\tres_cIndex[ivar]<-NA\n\t\t\tis.correct<-FALSE\n\t\t}\n\t\tweights <- weights[cc.ix]\n\t\tstrat <- strat[cc.ix]\n\t\tstrat <- as.numeric(as.factor(strat))\n\t\tustrat <- sort(unique(strat)) ## to check later\n\t\tN <- sum(weights) ##length(x2)\n\t\tif(N <= 1) {\n\t\t\twarning(paste(\"\\nweights of observations are too small (sum should be > 1), the concordance index cannot be computed for variable\", colnames(x)[ivar],\" !\"))\n\t\t\tres_cIndex[ivar]<-NA\n\t\t\tis.correct<-FALSE\n\t\t}\n\n\t\tlenS <- length(strat)\n\t\tlenU <- length(ustrat)\n\t\tif(is.correct){\n\t\t\tres_cIndex[ivar] <- .Call(.C_get_concordanceIndex_onevariable, as.integer(as.logical(msurv)), as.integer(ustrat), as.double(x2),as.integer(cl2), as.double(st), as.integer(se), as.double(weights), as.integer(strat),as.integer(N), as.integer(as.logical(outx)), as.integer(lenS), as.integer(lenU))\n\t\t}\n\t}\n\n\tres_mrmr_cIndex <- .Call(.C_mrmr_cIndex, data.matrix(x),as.integer(is.na(x)),as.double(res_cIndex),ncol(x), nrow(x), -1000)\n\tnames(res_mrmr_cIndex)<-colnames(x)\n\treturn(res_mrmr_cIndex)\n\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mrmr.cindex` function and what are its main input parameters?",
        "answer": "The `mrmr.cindex` function calculates the concordance index for each variable in a dataset, either for survival data or binary classification. Its main input parameters are:\n- `x`: A matrix of predictor variables\n- `surv.time` and `surv.event`: Survival time and event indicators for survival analysis\n- `cl`: Binary class labels for classification\n- `weights`: Optional observation weights\n- `strat`: Optional stratification variable\n- Additional parameters for controlling the analysis method and output"
      },
      {
        "question": "How does the function handle missing data and what options are available for dealing with it?",
        "answer": "The function handles missing data in several ways:\n1. It checks for complete cases using `complete.cases()` function.\n2. If there are fewer than 3 complete observations, it returns NA values for the results.\n3. The `na.rm` parameter controls whether to stop execution when NA values are present (default) or to remove them.\n4. If `na.rm=FALSE` and NA values are present, the function stops with an error message.\n5. The function also checks for and handles cases where weights are 0, effectively removing those observations from the analysis."
      },
      {
        "question": "What is the significance of the `.Call` function used in this code and what does it do?",
        "answer": "The `.Call` function is used to invoke compiled C code from within R, which can significantly improve performance for computationally intensive tasks. In this function, it's used twice:\n1. To calculate the concordance index for each variable using the C function `get_concordanceIndex_onevariable`.\n2. To perform the MRMR (Minimum Redundancy Maximum Relevance) calculation using the C function `mrmr_cIndex`.\nThese C functions likely implement efficient algorithms for these calculations, allowing the R function to handle large datasets more quickly than if implemented purely in R."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\tnvar <- ncol(x)\n\tnsample <- nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) { stop(\"bad length for parameter weights!\") }\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) { stop(\"weights must be a number between 0 and 1!\") }\n\t} else { weights <- rep(1, nsample) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1, nsample) }\n\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) { stop(\"binary classes and survival data are missing!\") }\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) { stop(\"choose binary classes or survival data but not both!\") }\n\n\tres_cIndex <- rep(0, nvar)\n\tmsurv <- FALSE\n\tif(missing(cl)) {\n\t\tmsurv <- TRUE\n\t\tcl <- rep(0, nsample)\n\t} else { surv.time <- surv.event <- rep(0, nsample) }\n\n\t# TODO: Implement the main loop for calculating cIndex\n\n\t# TODO: Calculate mrmr_cIndex\n\n\treturn(res_mrmr_cIndex)\n}",
        "complete": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\tnvar <- ncol(x)\n\tnsample <- nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) { stop(\"bad length for parameter weights!\") }\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) { stop(\"weights must be a number between 0 and 1!\") }\n\t} else { weights <- rep(1, nsample) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) { stop(\"bad length for parameter strat!\") }\n\t} else { strat <- rep(1, nsample) }\n\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) { stop(\"binary classes and survival data are missing!\") }\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) { stop(\"choose binary classes or survival data but not both!\") }\n\n\tres_cIndex <- rep(0, nvar)\n\tmsurv <- FALSE\n\tif(missing(cl)) {\n\t\tmsurv <- TRUE\n\t\tcl <- rep(0, nsample)\n\t} else { surv.time <- surv.event <- rep(0, nsample) }\n\n\tfor(ivar in 1:nvar) {\n\t\txx <- x[, ivar]\n\t\tcc.ix <- complete.cases(xx, surv.time, surv.event, cl, weights, strat)\n\t\tif(sum(cc.ix) < 3 || any(!cc.ix) & !na.rm) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tcontinue\n\t\t}\n\t\tcc.ix <- cc.ix & weights != 0\n\t\tx2 <- xx[cc.ix]\n\t\tcl2 <- cl[cc.ix]\n\t\tst <- surv.time[cc.ix]\n\t\tse <- surv.event[cc.ix]\n\t\tweights <- weights[cc.ix]\n\t\tstrat <- as.numeric(as.factor(strat[cc.ix]))\n\t\tustrat <- sort(unique(strat))\n\t\tN <- sum(weights)\n\t\tif(N <= 1 || (msurv && sum(se) == 0) || (!msurv && length(unique(cl2)) == 1)) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tcontinue\n\t\t}\n\t\tres_cIndex[ivar] <- .Call(.C_get_concordanceIndex_onevariable, as.integer(msurv), as.integer(ustrat), as.double(x2), as.integer(cl2), as.double(st), as.integer(se), as.double(weights), as.integer(strat), as.integer(N), as.integer(outx), as.integer(length(strat)), as.integer(length(ustrat)))\n\t}\n\n\tres_mrmr_cIndex <- .Call(.C_mrmr_cIndex, data.matrix(x), as.integer(is.na(x)), as.double(res_cIndex), ncol(x), nrow(x), -1000)\n\tnames(res_mrmr_cIndex) <- colnames(x)\n\treturn(res_mrmr_cIndex)\n}"
      },
      {
        "partial": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\n\t# TODO: Initialize variables and perform input validation\n\n\t# TODO: Handle missing cl or survival data\n\n\tfor(ivar in 1:nvar) {\n\t\t# TODO: Extract and process data for current variable\n\n\t\t# TODO: Check for valid data and compute cIndex\n\n\t\t# TODO: Call C function to compute concordance index\n\t}\n\n\t# TODO: Compute mrmr_cIndex using C function\n\n\treturn(res_mrmr_cIndex)\n}",
        "complete": "mrmr.cindex <- function(x, surv.time, surv.event, cl, weights, comppairs=10, strat, alpha=0.05, outx=TRUE, method=c(\"conservative\", \"noether\", \"nam\"), alternative=c(\"two.sided\", \"less\", \"greater\"), na.rm=FALSE) {\n\tnvar <- ncol(x)\n\tnsample <- nrow(x)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != nsample) stop(\"bad length for parameter weights!\")\n\t\tif(min(weights, na.rm=TRUE) < 0 && max(weights, na.rm=TRUE) > 1) stop(\"weights must be a number between 0 and 1!\")\n\t} else weights <- rep(1, nsample)\n\tif(!missing(strat)) {\n\t\tif(length(strat) != nsample) stop(\"bad length for parameter strat!\")\n\t} else strat <- rep(1, nsample)\n\tif(missing(cl) && (missing(surv.time) || missing(surv.event))) stop(\"binary classes and survival data are missing!\")\n\tif(!missing(cl) && (!missing(surv.time) || !missing(surv.event))) stop(\"choose binary classes or survival data but not both!\")\n\n\tres_cIndex <- rep(0, nvar)\n\tmsurv <- missing(cl)\n\tif(msurv) {\n\t\tcl <- rep(0, nsample)\n\t} else surv.time <- surv.event <- rep(0, nsample)\n\n\tfor(ivar in 1:nvar) {\n\t\txx <- x[, ivar]\n\t\tcc.ix <- complete.cases(xx, surv.time, surv.event, cl, weights, strat) & weights != 0\n\t\tif(sum(cc.ix) < 3 || (any(!cc.ix) && !na.rm)) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tnext\n\t\t}\n\t\tx2 <- xx[cc.ix]\n\t\tcl2 <- cl[cc.ix]\n\t\tst <- surv.time[cc.ix]\n\t\tse <- surv.event[cc.ix]\n\t\tweights <- weights[cc.ix]\n\t\tstrat <- as.numeric(as.factor(strat[cc.ix]))\n\t\tustrat <- sort(unique(strat))\n\t\tN <- sum(weights)\n\t\tif(N <= 1 || (msurv && sum(se) == 0) || (!msurv && length(unique(cl2)) == 1)) {\n\t\t\tres_cIndex[ivar] <- NA\n\t\t\tnext\n\t\t}\n\t\tres_cIndex[ivar] <- .Call(.C_get_concordanceIndex_onevariable, as.integer(msurv), as.integer(ustrat), as.double(x2), as.integer(cl2), as.double(st), as.integer(se), as.double(weights), as.integer(strat), as.integer(N), as.integer(outx), as.integer(length(strat)), as.integer(length(ustrat)))\n\t}\n\n\tres_mrmr_cIndex <- .Call(.C_mrmr_cIndex, data.matrix(x), as.integer(is.na(x)), as.double(res_cIndex), nvar, nsample, -1000)\n\tnames(res_mrmr_cIndex) <- colnames(x)\n\treturn(res_mrmr_cIndex)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/globals.R",
    "language": "R",
    "content": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', 'duration'))",
    "qa_pairs": [
      {
        "question": "What is the purpose of using utils::globalVariables() in this code snippet?",
        "answer": "The utils::globalVariables() function is used to declare global variables for dplyr and data.table column names. This prevents R CMD check from generating NOTE messages about undefined global variables when these column names are used in non-standard evaluation contexts within dplyr or data.table operations."
      },
      {
        "question": "How many and what types of variables are being declared as global in this code?",
        "answer": "The code declares 21 global variables. These include a mix of column names (e.g., 'sampleid', 'treatmentid', 'Symbol'), function names (e.g., 'read.csv'), and special symbols (e.g., '.'). The variables represent various aspects of data analysis, including sample information, treatment levels, and statistical measures."
      },
      {
        "question": "Why might a programmer choose to use globalVariables() instead of explicitly defining these variables in their code?",
        "answer": "A programmer might choose to use globalVariables() for several reasons: 1) It allows the use of these variables in dplyr or data.table operations without R CMD check warnings. 2) It keeps the code cleaner by avoiding the need to repeatedly define or pass these variables. 3) It's particularly useful when working with large datasets where column names are used frequently in data manipulation operations. However, it's important to use this approach judiciously, as overuse of global variables can make code harder to understand and maintain."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', # Complete the list",
        "complete": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', 'duration'))"
      },
      {
        "partial": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(# Add the column names here))",
        "complete": "# Declaring global variables for dplyr and data.table column names\nutils::globalVariables(c(\"sampleid\",\"treatmentid\",'read.csv','samplename','.',\n                         'Symbol', 'feature', 'Control', 'Low', 'Middle',\n                         'High', 'verbose', 'dose_level', 'individual_id',\n                         'duration_h', 'viability', '.SD', 'durations',\n                         'tSetName', '.intern', 'controlLevels', 'treatmentLevels',\n                         'dose', 'compound', 'duration'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/hazard.ratio.R",
    "language": "R",
    "content": "`hazard.ratio` <-\nfunction(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n\tmethod.test <- match.arg(method.test)\n\tif(!missing(weights)) {\n\t\tif(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n\t} else { weights <- rep(1,  length(x)) }\n\tif(!missing(strat)) {\n\t\tif(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n\t\t## remove weights=0 because the coxph function does not deal with them properly\n\t\tiix <- weights <= 0\n\t\tif(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n\t\tweights[iix] <- NA\n\t} else { strat <- rep(1,  length(x)) }\n\tcc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n\tif(sum(cc.ix) < 3) {\n\t## not enough observations\n\t\tdata <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n\t\treturn(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n\t}\n\tif(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n\tsx <- x[cc.ix]\n\too <- order(sx, decreasing=FALSE)\n\tsx <- sx[oo]\n\tstime <- surv.time[cc.ix][oo]\n\tsevent <- surv.event[cc.ix][oo]\n\tsweights <- weights[cc.ix][oo]\n\tsstrat <- strat[cc.ix][oo]\n\tdata <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n\t#fit the cox model\n\toptions(warn=2)\n\trr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n\toptions(warn=0)\n\tif(class(rr) == \"try-error\") {\n\t\tres <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n\t} else {\n\t\thrcoef <- rr$coefficients\n\t\thrse <- sqrt(drop(rr$var))\n\t\tnames(hrcoef) <- names(hrse) <- NULL\n\t\tmystat <- NA\n\t\tswitch(method.test,\n\t\t\"logrank\"={\n\t\t\tmystat <- rr$score\n\t\t},\n\t\t\"likelihood.ratio\"={\n\t\t\tmystat <- 2 * (rr$loglik[2] - rr$loglik[1])\n\t\t},\n\t\t\"wald\"={\n\t\t\tmystat <- rr$wald.test\n\t\t\t##(hrcoef / hrse)^2\n\t\t})\n\t\tmypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n\t\tres <- list(\"hazard.ratio\"=exp(hrcoef), \"coef\"=hrcoef, \"se\"=hrse, \"lower\"=exp(hrcoef - qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"upper\"=exp(hrcoef + qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n\t}\n\n\treturn(res)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `hazard.ratio` function and what are its main input parameters?",
        "answer": "The `hazard.ratio` function is used to calculate the hazard ratio in survival analysis. Its main input parameters are:\n- `x`: The predictor variable\n- `surv.time`: Survival time\n- `surv.event`: Event indicator\n- `weights`: Optional weights for each observation\n- `strat`: Optional stratification variable\n- `alpha`: Significance level for confidence interval calculation (default 0.05)\n- `method.test`: Method for calculating the test statistic (options: 'logrank', 'likelihood.ratio', 'wald')\n- `na.rm`: Boolean indicating whether to remove NA values"
      },
      {
        "question": "How does the function handle missing data and what happens if there are fewer than 3 complete cases?",
        "answer": "The function handles missing data as follows:\n1. It checks for complete cases using `complete.cases()` function.\n2. If `na.rm=FALSE` (default) and there are any incomplete cases, it stops with an error.\n3. If there are fewer than 3 complete cases, it returns a list with NA values for all results, including the hazard ratio, coefficient, standard error, confidence interval bounds, and p-value.\n4. The function also includes a warning if any weights are less than or equal to zero, as these observations are discarded."
      },
      {
        "question": "What statistical methods are used in this function and how is the final result calculated?",
        "answer": "The function uses Cox proportional hazards regression for survival analysis. Key statistical methods include:\n1. Fitting a Cox model using `coxph()` function.\n2. Calculating the test statistic based on the chosen method (logrank, likelihood ratio, or Wald test).\n3. Computing the p-value using the chi-square distribution.\n4. Calculating the hazard ratio as exp(coefficient).\n5. Determining confidence intervals using the normal distribution.\n\nThe final result is a list containing:\n- Hazard ratio\n- Coefficient and standard error\n- Lower and upper confidence interval bounds\n- P-value\n- Number of observations\n- The fitted Cox model\n- Input data"
      }
    ],
    "completion_tasks": [
      {
        "partial": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  # Complete the function here\n}",
        "complete": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx, decreasing=FALSE)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n  options(warn=0)\n  if(class(rr) == \"try-error\") {\n    res <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n  } else {\n    hrcoef <- rr$coefficients\n    hrse <- sqrt(drop(rr$var))\n    names(hrcoef) <- names(hrse) <- NULL\n    mystat <- switch(method.test,\n      \"logrank\" = rr$score,\n      \"likelihood.ratio\" = 2 * (rr$loglik[2] - rr$loglik[1]),\n      \"wald\" = rr$wald.test\n    )\n    mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n    res <- list(\"hazard.ratio\"=exp(hrcoef), \"coef\"=hrcoef, \"se\"=hrse, \"lower\"=exp(hrcoef - qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"upper\"=exp(hrcoef + qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n  }\n  return(res)\n}"
      },
      {
        "partial": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx, decreasing=FALSE)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n  options(warn=0)\n  if(class(rr) == \"try-error\") {\n    res <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n  } else {\n    # Complete the function here\n  }\n  return(res)\n}",
        "complete": "hazard.ratio <- function(x, surv.time, surv.event, weights, strat, alpha=0.05, method.test=c(\"logrank\", \"likelihood.ratio\", \"wald\"), na.rm=FALSE, ...) {\n  method.test <- match.arg(method.test)\n  if(!missing(weights)) {\n    if(length(weights) != length(x)) { stop(\"bad length for parameter weights!\") }\n  } else { weights <- rep(1, length(x)) }\n  if(!missing(strat)) {\n    if(length(strat) != length(x)) { stop(\"bad length for parameter strat!\") }\n    iix <- weights <= 0\n    if(any(iix)) { warning(\"samples with weight<=0 are discarded\") }\n    weights[iix] <- NA\n  } else { strat <- rep(1, length(x)) }\n  cc.ix <- complete.cases(x, surv.time, surv.event, weights, strat)\n  if(sum(cc.ix) < 3) {\n    data <- list(\"x\"=x, \"z\"=rep(NA, length(x)), \"surv.time\"=surv.time, \"surv.event\"=surv.event, \"weights\"=weights, \"strat\"=strat)\n    return(list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data))\n  }\n  if(any(!cc.ix) & !na.rm) { stop(\"NA values are present!\") }\n  sx <- x[cc.ix]\n  oo <- order(sx, decreasing=FALSE)\n  sx <- sx[oo]\n  stime <- surv.time[cc.ix][oo]\n  sevent <- surv.event[cc.ix][oo]\n  sweights <- weights[cc.ix][oo]\n  sstrat <- strat[cc.ix][oo]\n  data <- list(\"x\"=x, \"surv.time\"=surv.time, \"surv.event\"=surv.event)\n  options(warn=2)\n  rr <- try(coxph(Surv(stime, sevent) ~ strata(sstrat) + sx, weights=sweights, ...))\n  options(warn=0)\n  if(class(rr) == \"try-error\") {\n    res <- list(\"hazard.ratio\"=NA, \"coef\"=NA, \"se\"=NA, \"lower\"=NA, \"upper\"=NA, \"p.value\"=NA, \"n\"=sum(cc.ix), \"coxm\"=NA, \"data\"=data)\n  } else {\n    hrcoef <- rr$coefficients\n    hrse <- sqrt(drop(rr$var))\n    names(hrcoef) <- names(hrse) <- NULL\n    mystat <- switch(method.test,\n      \"logrank\" = rr$score,\n      \"likelihood.ratio\" = 2 * (rr$loglik[2] - rr$loglik[1]),\n      \"wald\" = rr$wald.test\n    )\n    mypp <- pchisq(mystat, df=1, lower.tail=FALSE)\n    res <- list(\"hazard.ratio\"=exp(hrcoef), \"coef\"=hrcoef, \"se\"=hrse, \"lower\"=exp(hrcoef - qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"upper\"=exp(hrcoef + qnorm(alpha / 2, lower.tail=FALSE) * hrse), \"p.value\"=mypp, \"n\"=rr$n, \"coxm\"=rr, \"data\"=data)\n  }\n  return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/subtype.cluster.R",
    "language": "R",
    "content": "#' @title Function to fit the Subtype Clustering Model\n#'\n#' @description\n#' This function fits the Subtype Clustering Model as published in Desmedt\n#'   et al. 2008 and Wiarapati et al. 2008. This model is actually a mixture\n#'   of three Gaussians with equal shape, volume and variance (see EEI model\n#'   in Mclust). This model is adapted to breast cancer and uses ESR1, ERBB2\n#'   and AURKA dimensions to identify the molecular subtypes, i.e. ER-/HER2-,\n#'   HER2+ and ER+/HER2- (Low and High Prolif).\n#'\n#' @usage\n#' subtype.cluster(module.ESR1, module.ERBB2, module.AURKA, data, annot,\n#'   do.mapping = FALSE, mapping, do.scale = TRUE, rescale.q = 0.05,\n#'   model.name = \"EEI\", do.BIC = FALSE, plot = FALSE, filen, verbose = FALSE)\n#'\n#' @param module.ESR1\tMatrix containing the ESR1-related gene(s) in\n#'   rows and at least three columns: \"probe\", \"EntrezGene.ID\" and\n#'   \"coefficient\" standing for the name of the probe, the NCBI Entrez\n#'   Gene id and the coefficient giving the direction and the strength of\n#'   the association of each gene in the gene list.\n#' @param module.ERBB2\tIdem for ERBB2.\n#' @param module.AURKA\tIdem for AURKA.\n#' @param data\tMatrix of gene expressions with samples in rows and probes\n#'   in columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with at least one column named\n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping\tTRUE if the mapping through Entrez Gene ids must\n#'   be performed (in case of ambiguities, the most variant probe is kept\n#'   for each gene), FALSE otherwise.\n#' @param mapping\t**DEPRECATED** Matrix with columns \"EntrezGene.ID\" and\n#'   \"probe\" used to force the mapping such that the probes are not selected\n#'   based on their variance.\n#' @param do.scale TRUE if the ESR1, ERBB2 and AURKA (module) scores must be\n#'   rescaled (see rescale), FALSE otherwise.\n#' @param rescale.q\tProportion of expected outliers for rescaling the gene expressions.\n#' @param do.BIC\tTRUE if the Bayesian Information Criterion must be computed\n#'   for number of clusters ranging from 1 to 10, FALSE otherwise.\n#' @param model.name Name of the model used to fit the mixture of Gaussians\n#'   with the Mclust from the mclust package; default is \"EEI\" for fitting a\n#'   mixture of Gaussians with diagonal variance, equal volume, equal shape\n#'   and identical orientation.\n#' @param plot TRUE if the patients and their corresponding subtypes must\n#'   be plotted, FALSE otherwise.\n#' @param filen Name of the csv file where the subtype clustering model must\n#' be stored.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - model: Subtype Clustering Model (mixture of three Gaussians),\n#'   like scmgene.robust, scmod1.robust and scmod2.robust when this function\n#'   is used on expO dataset (International Genomics Consortium) with the gene\n#'   modules published in the two references cited below.\n#' - BIC: Bayesian Information Criterion for the Subtype Clustering Model\n#'   with number of clusters ranging from 1 to 10.\n#' - subtype: Subtypes identified by the Subtype Clustering Model. Subtypes\n#'   can be either \"ER-/HER2-\", \"HER2+\" or \"ER+/HER2-\".\n#' - subtype.proba: Probabilities to belong to each subtype estimated by\n#'   the Subtype Clustering Model.\n#' - subtype2: Subtypes identified by the Subtype Clustering Model using\n#'   AURKA to discriminate low and high proliferative tumors. Subtypes can\n#'   be either \"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\" or \"ER+/HER2- Low Prolif\".\n#' - subtype.proba2: Probabilities to belong to each subtype (including\n#'   discrimination between lowly and highly proliferative ER+/HER2- tumors,\n#'   see subtype2) estimated by the Subtype Clustering Model.\n#' - module.scores: Matrix containing ESR1, ERBB2 and AURKA module scores.\n#'\n#' @references\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G,\n#'   Delorenzi M, Piccart M, and Sotiriou C (2008) \"Biological processes\n#'   associated with breast cancer clinical outcome depend on the molecular\n#'   subtypes\", Clinical Cancer Research, 14(16):5158-5165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B,\n#'   Desmedt C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ\n#'   and Delorenzi M (2008) \"Meta-analysis of Gene-Expression Profiles in\n#'   Breast Cancer: Toward a Unified Understanding of Breast Cancer Sub-typing\n#'   and Prognosis Signatures\", Breast Cancer Research, 10(4):R65.\n#'\n#' @seealso\n#' [genefu::subtype.cluster.predict], [genefu::intrinsic.cluster],\n#' [genefu::intrinsic.cluster.predict], [genefu::scmod1.robust],\n#' [genefu::scmod2.robust]\n#'\n#' @examples\n#' # example without gene mapping\n#' # load expO data\n#' data(expos)\n#' # load gene modules\n#' data(mod1)\n#' # fit a Subtype Clustering Model\n#' scmod1.expos <- subtype.cluster(module.ESR1=mod1$ESR1, module.ERBB2=mod1$ERBB2,\n#'   module.AURKA=mod1$AURKA, data=data.expos, annot=annot.expos, do.mapping=FALSE,\n#'   do.scale=TRUE, plot=TRUE, verbose=TRUE)\n#' str(scmod1.expos, max.level=1)\n#' table(scmod1.expos$subtype2)\n#'\n#' # example with gene mapping\n#' # load NKI data\n#' data(nkis)\n#' # load gene modules\n#' data(mod1)\n#' # fit a Subtype Clustering Model\n#' scmod1.nkis <- subtype.cluster(module.ESR1=mod1$ESR1, module.ERBB2=mod1$ERBB2,\n#'   module.AURKA=mod1$AURKA, data=data.nkis, annot=annot.nkis, do.mapping=TRUE,\n#'   do.scale=TRUE, plot=TRUE, verbose=TRUE)\n#' str(scmod1.nkis, max.level=1)\n#' table(scmod1.nkis$subtype2)\n#'\n#' @md\n#' @import mclust\n#' @import graphics\n#' @export\nsubtype.cluster <- function(module.ESR1, module.ERBB2, module.AURKA, data,\n\tannot, do.mapping=FALSE, mapping, do.scale=TRUE, rescale.q=0.05,\n\tmodel.name=\"EEI\", do.BIC=FALSE, plot=FALSE, filen, verbose=FALSE)\n{\n\tif(missing(data) || missing(annot)) { stop(\"data, and annot parameters must be specified\") }\n\n\tsbtn <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2-\")\n\tsbtn2 <- c(\"ER-/HER2-\", \"HER2+\", \"ER+/HER2- High Prolif\", \"ER+/HER2- Low Prolif\")\n\tsig.esr1 <- sig.score(x=module.ESR1, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsig.erbb2 <- sig.score(x=module.ERBB2, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tsig.aurka <- sig.score(x=module.AURKA, data=data, annot=annot, do.mapping=do.mapping, mapping=mapping, verbose=FALSE)\n\tdd <- cbind(\"ESR1\"=sig.esr1$score, \"ERBB2\"=sig.erbb2$score, \"AURKA\"=sig.aurka$score)\n\trnn <- rownames(dd)\n\tm.mod <- list(\"ESR1\"=cbind(\"probe\"=as.character(sig.esr1$probe[ ,\"new.probe\"]), \"EntrezGene.ID\"=as.character(sig.esr1$probe[ ,\"EntrezGene.ID\"]), \"coefficient\"=module.ESR1[match(sig.esr1$probe[ ,\"probe\"], module.ESR1[ ,\"probe\"]), \"coefficient\"]), \"ERBB2\"=cbind(\"probe\"=as.character(sig.erbb2$probe[ ,\"new.probe\"]), \"EntrezGene.ID\"=as.character(sig.erbb2$probe[ ,\"EntrezGene.ID\"]), \"coefficient\"=module.ERBB2[match(sig.erbb2$probe[ ,\"probe\"], module.ERBB2[ ,\"probe\"]), \"coefficient\"]), \"AURKA\"=cbind(\"probe\"=as.character(sig.aurka$probe[ ,\"new.probe\"]), \"EntrezGene.ID\"=as.character(sig.aurka$probe[ ,\"EntrezGene.ID\"]), \"coefficient\"=module.AURKA[match(sig.aurka$probe[ ,\"probe\"], module.AURKA[ ,\"probe\"]), \"coefficient\"]))\n\tif(do.scale) {\n\t\t## the rescaling needs a large sample size!!!\n\t\t## necessary if we want to validate the classifier using a different dataset\n\t\t## the estimation of survival probabilities depends on the scale of the score\n\t\tdd <- apply(dd, 2, function(x) { return((rescale(x, q=rescale.q, na.rm=TRUE) - 0.5) * 2) })\n\t\trownames(dd) <- rnn\n\t} else { rescale.q <- NA }\n\trownames(dd) <- rownames(data)\n\tdd2 <- dd\n\n\tcc.ix <- complete.cases(dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE])\n\tdd <- dd[cc.ix, , drop=FALSE]\n\tif(all(!cc.ix)) { stop(\"None of ESR1 and ERBB2 genes are present!\") }\n\n\tif(do.BIC) {\n\t\t## save the BIC values for the all the methods and a number of clusters from 1 to 10\n\t\tcluster.bic <- mclust::Mclust(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=model.name, G=1:10)$BIC\n\t} else { cluster.bic <- NA }\n\n\t#identify the 3 subtypes\n\trr3 <- mclust::Mclust(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], modelNames=model.name, G=3)\n\t#redefine classification to be coherent with subtypes\n\tuclass <- sort(unique(rr3$classification))\n\tuclass <- uclass[!is.na(uclass)]\n\tif(length(uclass) != 3) { stop(\"less than 3 subtypes are identified!\") }\n\tmm <- NULL\n\tfor(i in 1:length(uclass)) {\n\t\tmm <- c(mm, median(dd[rr3$classification == uclass[i],\"ERBB2\"], na.rm=TRUE) )\n\t}\n\tnclass <-  uclass[order(mm, decreasing=TRUE)[1]]\n\tmm <- NULL\n\tfor(i in 1:length(uclass[-nclass])) {\n\t\tmm <- c(mm, median(dd[rr3$classification == uclass[-nclass][i],\"ESR1\"], na.rm=TRUE))\n\t}\n\tnclass <- c(uclass[-nclass][order(mm, decreasing=TRUE)[2]], nclass, uclass[-nclass][order(mm, decreasing=TRUE)[1]])\n\t#nclass contains the new order\n\trr3$z <- rr3$z[ ,nclass, drop=FALSE]\n\tncl <- rr3$classification\n\tfor(i in 1:length(uclass)) {\n\t\tncl[rr3$classification == nclass[i]] <- i\n\t}\n\trr3$classification <- ncl\n\trr3$parameters$pro <- rr3$parameters$pro[nclass]\n\trr3$parameters$mean <- rr3$parameters$mean[ , nclass, drop=FALSE]\n\trr3$parameters$variance$sigma <- rr3$parameters$variance$sigma[ , , nclass, drop=FALSE]\n\n\tif(plot) {\n\t\tif(do.scale) {\n\t\t\tmyxlim <- myylim <- c(-2, 2)\n\t\t} else {\n\t\t\tmyxlim <- range(dd[ , \"ESR1\"])\n\t\t\tmyylim <- range(dd[ , \"ERBB2\"])\n\t\t}\n\t\t## plot the mixture of Gaussians of the model\n\t\txx <- mclust:::grid1(50, range=myxlim)\n\t\tyy <- mclust:::grid1(50, range=myylim)\n\t\txxyy <- mclust:::grid2(xx,yy)\n\t\t#density\n\t\txyDens <- mclust::dens(modelName = rr3$modelName, data = xxyy, parameters = rr3$parameters)\n\t\txyDens <- matrix(xyDens, nrow = length(xx), ncol = length(yy))\n\t\tpar(pty = \"s\")\n\t\tzz <- xyDens\n\t\t#plot\n\t\tpersp(x = xx, y = yy, z = zz, xlim=myxlim, ylim=myylim, theta=-25, phi=30, expand=0.5, xlab=\"ESR1\", ylab=\"ERBB2\", zlab=\"Density\", col=\"darkgrey\", ticktype=\"detailed\")\n\t}\n\n\t## use the previously computed model to fit a new model in a supervised manner\n\tmyclass <- mclust::unmap(rr3$classification)\n\tdimnames(myclass)[[1]] <- dimnames(dd)[[1]]\n\tmclust.tr <- mclust::mstep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], z=myclass)\n\tdimnames(mclust.tr$z) <- dimnames(myclass)\n\temclust.tr <- mclust::estep(modelName=model.name, data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], parameters=mclust.tr$parameters)\n\tdimnames(emclust.tr$z) <- dimnames(myclass)\n\tclass.tr <- mclust::map(emclust.tr$z, warn=FALSE)\n\tnames(class.tr) <- dimnames(dd)[[1]]\n\tdimnames(mclust.tr$parameters$mean)[[2]] <- names(mclust.tr$parameters$pro) <- dimnames(mclust.tr$z)[[2]]\n\n\t## subtypes\n\tsbt <- rep(NA, nrow(data))\n\tnames(sbt) <- dimnames(data)[[1]]\n\tsbt[names(class.tr)] <- sbtn[class.tr]\n\tsbt.proba <- matrix(NA, nrow(data), ncol=ncol(emclust.tr$z), dimnames=list(dimnames(data)[[1]], sbtn))\n\tsbt.proba[dimnames(emclust.tr$z)[[1]], ] <- emclust.tr$z\n\t## discriminate between luminal A and B using AURKA\n\t## since proliferation is a continuum we fit a Gaussian using AURKA expression of the ER+/HER2- tumors\n\ttt <- mclust::Mclust(dd2[complete.cases(sbt, dd2[ , \"AURKA\"]) & sbt == sbtn[3], \"AURKA\"], modelNames=\"E\", G=1)\n\tgauss.prolif <- c(\"mean\"=tt$parameters$mean, \"sigma\"=tt$parameters$variance$sigmasq)\n\tsbt2 <- sbt\n\tsbt2[sbt == sbtn[3]] <- NA\n\t## probability that tumor is highly proliferative\n\tpprolif <- pnorm(q=dd2[ , \"AURKA\"], mean=gauss.prolif[\"mean\"], sd=gauss.prolif[\"sigma\"], lower.tail=TRUE)\n\t## high proliferation\n\tsbt2[sbt == sbtn[3] & pprolif >= 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[3]\n\t## low proliferation\n\tsbt2[sbt == sbtn[3] & pprolif < 0.5 & complete.cases(sbt, pprolif)] <- sbtn2[4]\n\t## subtype probabilities for luminal B and A\n\tsbt.proba2 <- matrix(NA, nrow(data), ncol=ncol(emclust.tr$z) + 1, dimnames=list(dimnames(data)[[1]], sbtn2))\n\ttt <- sbt.proba[ , sbtn[3]]\n\ttt2 <- pprolif\n\ttt <- cbind(tt * tt2, tt * (1 - tt2))\n\tcolnames(tt) <- sbtn2[3:4]\n\tsbt.proba2[ , sbtn2[1:2]] <- sbt.proba[ , sbtn[1:2]]\n\tsbt.proba2[ , sbtn2[3:4]] <- tt[ , sbtn2[3:4]]\n\n\tif(plot) {\n\t\t## plot the clusters\n\t\tmclust::mclust2Dplot(data=dd[ , c(\"ESR1\", \"ERBB2\"), drop=FALSE], what=\"classification\", classification=class.tr, parameters=mclust.tr$parameters, colors=c(\"darkred\", \"darkgreen\", \"darkblue\"), xlim=myxlim, ylim=myylim)\n\t\tlegend(x=\"topleft\", col=c(\"darkred\", \"darkgreen\", \"darkblue\"), legend=sbtn, pch=mclust::mclust.options(\"classPlotSymbols\")[1:length(uclass)], bty=\"n\")\n\t\t## plot the clusters with luminals A and B\n\t\tmycol <- mypch <- sbt2\n\t\tmycol[sbt2 == sbtn2[1]] <- \"darkred\"\n\t\tmycol[sbt2 == sbtn2[2]] <- \"darkgreen\"\n\t\tmycol[sbt2 == sbtn2[3]] <- \"darkorange\"\n\t\tmycol[sbt2 == sbtn2[4]] <- \"darkviolet\"\n\t\tmypch[sbt2 == sbtn2[1]] <- 17\n\t\tmypch[sbt2 == sbtn2[2]] <- 0\n\t\tmypch[sbt2 == sbtn2[3] | sbt2 == sbtn2[4]] <- 10\n\t\tmypch <- as.numeric(mypch)\n\t\tnames(mycol) <- names(mypch) <- names(sbt2)\n\t\tplot(x=dd[ , \"ESR1\"], y=dd[ , \"ERBB2\"], xlim=myxlim, ylim=myylim, xlab=\"ESR1\", ylab=\"ERBB2\", col=mycol[dimnames(dd)[[1]]], pch=mypch[dimnames(dd)[[1]]])\n\t\tlegend(x=\"topleft\", col=c(\"darkred\", \"darkgreen\", \"darkorange\", \"darkviolet\"), legend=sbtn2, pch=c(17, 0, 10, 10), bty=\"n\")\n\t\t## display the three circles representing the Gaussians\n\t\tfor(kk in 1:3) { mclust::mvn2plot(mu=mclust.tr$parameters$mean[ ,kk], sigma=mclust.tr$parameters$variance$sigma[ , , kk]) }\n\t}\n\n\tif(!missing(filen)) {\n\t\t#save model parameters in a csv file for reuse\n\t\twrite(x=sprintf(\"# Benjamin Haibe-Kains. All rights reserved.\"), file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# model.name: %s\", model.name), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmymean <- t(mclust.tr$parameters$mean)\n\t\tif(is.null(dimnames(mymean)[[1]])) { dimnames(mymean)[[1]] <- 1:nrow(mymean) }\n\t\tfor(i in 1:nrow(mymean)) { write(x=sprintf(\"# mean.%s: %g %g\", dimnames(mymean)[[1]][i], mymean[i,1], mymean[i,2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\")) }\n\t\tmysigma <- diag(mysigma <- mclust.tr$parameters$variance$sigma[ , ,1])\n\t\twrite(x=sprintf(\"# sigma: %g %g\", mysigma[1], mysigma[2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmypro <- mclust.tr$parameters$pro\n\t\twrite(x=sprintf(\"# pro: %g %g %g\", mypro[1], mypro[2], mypro[3]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmyscale <- mclust.tr$parameters$variance$scale\n\t\twrite(x=sprintf(\"# scale: %g\", myscale), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\tmyshape <- mclust.tr$parameters$variance$shape\n\t\twrite(x=sprintf(\"# shape: %g %g\", myshape[1], myshape[2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# gaussian.AURKA.mean: %g\", gauss.prolif[1]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# gaussian.AURKA.sigma: %g\", gauss.prolif[2]), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(x=sprintf(\"# rescale.q: %g\", rescale.q), append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite(paste(\"\\\"\", c(\"module\", dimnames(m.mod[[1]])[[2]]), \"\\\"\", collapse=\",\", sep=\"\"), sep=\"\", append=TRUE, file=paste(filen, \"csv\", sep=\".\"))\n\t\twrite.m.file(m.mod, file=paste(filen, \"csv\", sep=\".\"), col.names=FALSE, append=TRUE)\n\t}\n\n\treturn(list(\"model\"=c(mclust.tr[\"parameters\"], list(\"gaussian.AURKA\"=gauss.prolif), list(\"rescale.q\"=rescale.q), list(\"mod\"=m.mod)), \"BIC\"=cluster.bic, \"subtype\"=sbt, \"subtype.proba\"=sbt.proba, \"subtype2\"=sbt2, \"subtype.proba2\"=sbt.proba2,  \"module.scores\"=dd2))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `subtype.cluster` function?",
        "answer": "The main purpose of the `subtype.cluster` function is to fit a Subtype Clustering Model for breast cancer molecular subtypes. It uses a mixture of three Gaussians with equal shape, volume, and variance (EEI model in Mclust) to identify molecular subtypes based on ESR1, ERBB2, and AURKA gene expression dimensions."
      },
      {
        "question": "How does the function handle the discrimination between luminal A and B subtypes?",
        "answer": "The function discriminates between luminal A and B subtypes using the AURKA expression of ER+/HER2- tumors. It fits a Gaussian distribution to the AURKA expression and calculates the probability of a tumor being highly proliferative. Tumors with a probability >= 0.5 are classified as 'ER+/HER2- High Prolif' (luminal B), while those < 0.5 are classified as 'ER+/HER2- Low Prolif' (luminal A)."
      },
      {
        "question": "What are the key components returned by the `subtype.cluster` function?",
        "answer": "The function returns a list containing: 1) the fitted model parameters, 2) Bayesian Information Criterion (BIC) values if requested, 3) subtype classifications, 4) subtype probabilities, 5) refined subtype classifications including luminal A/B distinction, 6) refined subtype probabilities, and 7) module scores for ESR1, ERBB2, and AURKA."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/ovcYoshihara.R",
    "language": "R",
    "content": "#' @title Function to compute the subtype scores and risk classifications for\n#'   the prognostic signature published by Yoshihara et al.\n#'\n#' @description\n#' This function computes subtype scores and risk classifications from gene\n#'   expression values following the algorithm developed by Yoshihara et al,\n#'   for prognosis in ovarian cancer.\n#'\n#' @usage\n#' ovcYoshihara(data, annot, hgs,\n#'   gmap = c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"),\n#'   do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in\n#'   columns, dimnames being properly defined.\n#' @param annot\tMatrix of annotations with one column named as gmap, dimnames\n#'   being properly defined.\n#' @param hgs vector of booleans with TRUE represents the ovarian cancer\n#'   patients who have a high grade, late stage, serous tumor, FALSE otherwise.\n#'   This is particularly important for properly rescaling the data. If hgs is\n#'   missing, all the patients will be used to rescale the subtype score.\n#' @param gmap character string containing the biomaRt attribute to use for\n#'   mapping if do.mapping=TRUE\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be\n#'   performed (in case of ambiguities, the most variant probe is kept for\n#'   each gene), FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores.\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence\n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Yoshihara K, Tajima A, Yahata T, Kodama S, Fujiwara H, Suzuki M, Onishi Y,\n#'   Hatae M, Sueyoshi K, Fujiwara H, Kudo, Yoshiki, Kotera K, Masuzaki H,\n#'   Tashiro H, Katabuchi H, Inoue I, Tanaka K (2010) \"Gene expression profile\n#'   for predicting survival in advanced-stage serous ovarian cancer across two\n#'   independent datasets\", PloS one, 5(3):e9615.\n#'\n#' @seealso\n#' [genefu::sigOvcYoshihara]\n#'\n#' @examples\n#' # load the ovcYoshihara signature\n#' data(sigOvcYoshihara)\n#' # load NKI dataset\n#' data(nkis)\n#' colnames(annot.nkis)[is.element(colnames(annot.nkis), \"EntrezGene.ID\")] <- \"entrezgene\"\n#' # compute relapse score\n#' ovcYoshihara.nkis <- ovcYoshihara(data=data.nkis,\n#'   annot=annot.nkis, gmap=\"entrezgene\", do.mapping=TRUE)\n#' table(ovcYoshihara.nkis$risk)\n#'\n#' @md\n#' @export\novcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\",\n    \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"),\n    do.mapping=FALSE, verbose=FALSE)\n{\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    ## transform the gene expression in Z-scores\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n\tnames(prisk) <- names(pscore) <- rownames(data)\n\treturn (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `ovcYoshihara` function, and what are its main inputs and outputs?",
        "answer": "The `ovcYoshihara` function computes subtype scores and risk classifications for ovarian cancer prognosis based on gene expression data. Its main inputs are: 'data' (gene expression matrix), 'annot' (gene annotation matrix), and 'hgs' (vector indicating high-grade serous tumors). The function returns a list containing: 'score' (continuous signature scores), 'risk' (binary risk classification), 'mapping' (gene mapping information), and 'probe' (probe-to-gene correspondence if mapping is performed)."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if `do.mapping` is set to TRUE?",
        "answer": "When `do.mapping` is set to TRUE, the function performs gene mapping using the specified `gmap` attribute. It selects the most variant probe for each gene in case of ambiguities. The mapping process involves: 1) Ordering the signature genes by absolute weight, 2) Removing duplicates, 3) Mapping gene IDs between the signature and input data, 4) Updating the data, annotation, and signature matrices accordingly. The function also creates a 'myprobe' data frame to store the mapping information and updates the 'mymapping' vector with the number of mapped and total genes."
      },
      {
        "question": "How are the final subtype scores and risk classifications calculated in the `ovcYoshihara` function?",
        "answer": "The final calculations in the `ovcYoshihara` function involve these steps: 1) The gene expression data is transformed into Z-scores using the `scale` function. 2) The subtype scores are computed using the `genefu::sig.score` function, which combines the scaled expression data with the signature weights. 3) Risk classifications are determined by comparing each sample's score to the median score across all samples. Scores above the median are classified as high risk (1), while those below are low risk (0). The function returns both the continuous scores and binary risk classifications for each sample."
      }
    ],
    "completion_tasks": [
      {
        "partial": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        # Complete the else block\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        # Complete the do.mapping block\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    \n    # Complete the rest of the function\n}",
        "complete": "ovcYoshihara <- function(data, annot, hgs, gmap=c(\"entrezgene\", \"ensembl_gene_id\", \"hgnc_symbol\", \"unigene\", \"refseq_mrna\"), do.mapping=FALSE, verbose=FALSE) {\n    if (!exists('sigOvcYoshihara')) data(sigOvcYoshihara, envir=environment())\n    \n    gmap <- match.arg(gmap)\n    if(missing(hgs)) { hgs <- rep(TRUE, nrow(data)) }\n    if(do.mapping) {\n        if(!is.element(gmap, colnames(annot))) { stop(\"gmap is not a column of annot!\") }\n        if(verbose) { message(\"the most variant probe is selected for each gene\") }\n        sigt <- sigOvcYoshihara[order(abs(sigOvcYoshihara[ ,\"weight\"]), decreasing=FALSE), ,drop=FALSE]\n        sigt <- sigt[!duplicated(sigt[ ,gmap]), ,drop=FALSE]\n        gid2 <- sigt[ ,gmap]\n        names(gid2) <- rownames(sigt)\n        gid1 <- annot[ ,gmap]\n        names(gid1) <- colnames(data)\n        rr <- geneid.map(geneid1=gid1, data1=data, geneid2=gid2)\n        data <- rr$data1\n        annot <- annot[colnames(data), ,drop=FALSE]\n        sigt <- sigt[names(rr$geneid2), ,drop=FALSE]\n        pold <- colnames(data)\n        pold2 <- rownames(sigt)\n        colnames(data) <- rownames(annot) <- rownames(sigt) <- paste(\"geneid\", annot[ ,gmap], sep=\".\")\n        mymapping <- c(\"mapped\"=nrow(sigt), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=pold, \"gene.map\"=annot[ ,gmap], \"new.probe\"=pold2)\n    } else {\n        gix <- intersect(rownames(sigOvcYoshihara), colnames(data))\n        if(length(gix) < 2) { stop(\"data do not contain enough gene from the ovcTCGA signature!\") }\n        data <- data[ ,gix,drop=FALSE]\n        annot <- annot[gix, ,drop=FALSE]\n        mymapping <- c(\"mapped\"=length(gix), \"total\"=nrow(sigOvcYoshihara))\n        myprobe <- data.frame(\"probe\"=gix, \"gene.map\"=annot[ ,gmap], \"new.probe\"=gix)\n        sigt <- sigOvcYoshihara[gix, ,drop=FALSE]\n    }\n    data <- scale(data)\n    pscore <- genefu::sig.score(x=data.frame(\"probe\"=colnames(data), \"EntrezGene.ID\"=annot[ ,gmap], \"coefficient\"=sigt[ ,\"weight\"]), data=data, annot=annot, do.mapping=FALSE, signed=FALSE)$score\n    prisk <- as.numeric(pscore > median(pscore, na.rm=TRUE))\n    names(prisk) <- names(pscore) <- rownames(data)\n    return (list(\"score\"=pscore, \"risk\"=prisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/geneid.map.R",
    "language": "R",
    "content": "#' @title Function to find the common genes between two datasets or a dataset and\n#'   a gene list\n#'\n#' @description\n#' This function allows for fast mapping between two datasets or a dataset and a gene\n#'   list. The mapping process is performed using Entrez Gene id as reference. In case of\n#'   ambiguities (several probes representing the same gene), the most variant probe is\n#'   selected.\n#'\n#' @usage\n#' geneid.map(geneid1, data1, geneid2, data2, verbose = FALSE)\n#'\n#' @param geneid1 First vector of Entrez Gene ids. The name of the vector cells must\n#'   be the name of the probes in the dataset data1.\n#' @param data1\tFirst dataset with samples in rows and probes in columns. The dimnames\n#'   must be properly defined.\n#' @param geneid2 Second vector of Entrez Gene ids. The name of the vector cells must\n#'   be the name of the probes in the dataset data1 if it is not missing, proper names must be assigned otherwise.\n#' @param data2\tFirst dataset with samples in rows and probes in columns. The dimnames\n#'   must be properly defined. It may be missing.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#'\n#' @return\n#' A list with items:\n#' - geneid1 Mapped gene list from geneid1.\n#' - data1 Mapped dataset from data1.\n#' - geneid2 Mapped gene list from geneid2.\n#' - data2 Mapped dataset from data2.\n#'\n#' @note\n#' It is mandatory that the names of geneid1 and geneid2 must be the probe names\n#'   of the microarray platform.\n#'\n#' @examples\n#' # load NKI data\n#' data(nkis)\n#' nkis.gid <- annot.nkis[ ,\"EntrezGene.ID\"]\n#' names(nkis.gid) <- dimnames(annot.nkis)[[1]]\n#' # load GGI signature\n#' data(sig.ggi)\n#' ggi.gid <- sig.ggi[ ,\"EntrezGene.ID\"]\n#' names(ggi.gid) <- as.character(sig.ggi[ ,\"probe\"])\n#' # mapping through Entrez Gene ids of NKI and GGI signature\n#' res <- geneid.map(geneid1=nkis.gid, data1=data.nkis,\n#'   geneid2=ggi.gid, verbose=FALSE)\n#' str(res)\n#'\n#' @md\n#' @export\ngeneid.map <-\nfunction(geneid1, data1, geneid2, data2, verbose=FALSE) {\n\n\tnn <- names(geneid1)\n\tgeneid1 <- as.character(geneid1)\n\tnames(geneid1) <- nn\n\tnn <- names(geneid2)\n\tgeneid2 <- as.character(geneid2)\n\tnames(geneid2) <- nn\n\tif(is.null(names(geneid1))) { names(geneid1) <- dimnames(data1)[[2]] }\n\tif(!missing(data2) && is.null(names(geneid2))) { names(geneid2) <- dimnames(data2)[[2]] }\n\tif(!missing(data1) && !missing(geneid1) && !missing(geneid2)) {\n\t\t## remove probes without any measurements\n\t\tna.ix <- apply(data1, 2, function(x) { return(all(is.na(x))) })\n\t\tdata1 <- data1[ , !na.ix, drop=FALSE]\n\t\tgeneid1 <- geneid1[!na.ix]\n\t} else { stop(\"data1, geneid1 and geneid2 parameters are mandatory!\") }\n\tif(!missing(data2)) {\n\t\t## remove probes without any measurements\n\t\tna.ix <- apply(data2, 2, function(x) { return(all(is.na(x))) })\n\t\tdata2 <- data2[ , !na.ix, drop=FALSE]\n\t\tgeneid2 <- geneid2[!na.ix]\n\t} else { data2 <- NULL }\n\n\tgix1 <- !is.na(geneid1)\n\tgix2 <- !is.na(geneid2)\n\n\tgeneid.common <- intersect(geneid1[gix1], geneid2[gix2])\n\tif(length(geneid.common) == 0) {\n\t\twarning(\"no gene ids in common!\")\n\t\treturn(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n\t}\n\n\t## dataset1\n\t## probes corresponding to common gene ids\n\tgg <- names(geneid1)[is.element(geneid1, geneid.common)]\n\tgid <- geneid1[is.element(geneid1, geneid.common)]\n\t## duplicated gene ids\n\tgid.dupl <- unique(gid[duplicated(gid)])\n\tgg.dupl <- names(geneid1)[is.element(geneid1, gid.dupl)]\n\t## unique gene ids\n\tgid.uniq <- gid[!is.element(gid, gid.dupl)]\n\tgg.uniq <- names(geneid1)[is.element(geneid1, gid.uniq)]\n\t## data corresponding to unique gene ids\n\tdatat <- data1[ ,gg.uniq,drop=FALSE]\n\t## data for duplicated gene ids\n\tif(length(gid.dupl) > 0) {\n\t\tif(verbose) { message(\"\\ndataset1 duplicates...\") }\n\t\t## compute the standard deviation with a penalization on the number of missing values\n\t\t## this should avoid selecting the most variant probe with a lot of missing values\n\t\tpena <- apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n\t\tpena <- log((nrow(data1) + 1) / (pena + 1)) + 1\n\t\t#pena <- 1\n\t\tsdr <- drop(apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n\t\tmysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid1[gg.dupl], \"sd\"=sdr)\n\t\tmysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n\t\tmysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n\t\tdatat <- cbind(datat, data1[ , mysd[ , \"probe\"], drop=FALSE])\n\t}\n\tdata1 <- datat\n\tgeneid1 <- geneid1[dimnames(data1)[[2]]]\n\n\t#dataset2\n\tif(is.null(data2)) {\n\t\t#keep arbitrarily the first occurence of each duplicated geneid\n\t\tgeneid2 <- geneid2[!duplicated(geneid2) & is.element(geneid2, geneid.common)]\n\t}\n\telse {\n\t\t## probes corresponding to common gene ids\n\t\tgg <- names(geneid2)[is.element(geneid2, geneid.common)]\n\t\tgid <- geneid2[is.element(geneid2, geneid.common)]\n\t\t## duplicated gene ids\n\t\tgid.dupl <- unique(gid[duplicated(gid)])\n\t\tgg.dupl <- names(geneid2)[is.element(geneid2, gid.dupl)]\n\t\t## unique gene ids\n\t\tgid.uniq <- gid[!is.element(gid, gid.dupl)]\n\t\tgg.uniq <- names(geneid2)[is.element(geneid2, gid.uniq)]\n\t\t## data corresponding to unique gene ids\n\t\tdatat <- data2[ ,gg.uniq,drop=FALSE]\n\t\t## data for duplicated gene ids\n\t\tif(length(gid.dupl) > 0) {\n\t\t\tif(verbose) { message(\"\\ndataset2 duplicates...\") }\n\t\t\t## compute the standard deviation with a penalization on the number of missing values\n\t\t\t## this should avoid selecting the most variant probe with a lotof missing values\n\t\t\tpena <- apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n\t\t\tpena <- log((nrow(data2) + 1) / (pena + 1)) + 1\n\t\t\t#pena <- 1\n\t\t\tsdr <- drop(apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n\t\t\tmysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid2[gg.dupl], \"sd\"=sdr)\n\t\t\tmysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n\t\t\tmysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n\t\t\tdatat <- cbind(datat, data2[ , mysd[ , \"probe\"], drop=FALSE])\n\t\t}\n\t\tdata2 <- datat\n\t\tgeneid2 <- geneid2[dimnames(data2)[[2]]]\n\t}\n\n\t#same order for the two datasets\n\trix <- match(geneid2, geneid1)\n\tgeneid1 <- geneid1[rix]\n\tdata1 <- data1[ ,rix,drop=FALSE]\n\treturn(list(\"geneid1\"=geneid1, \"data1\"=data1, \"geneid2\"=geneid2, \"data2\"=data2))\n}",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `geneid.map` function, and how does it handle ambiguities when mapping genes?",
        "answer": "The primary purpose of the `geneid.map` function is to find common genes between two datasets or a dataset and a gene list, using Entrez Gene IDs as a reference. It handles ambiguities (multiple probes representing the same gene) by selecting the most variant probe. This is done by computing the standard deviation of each probe's measurements, with a penalization for missing values, and selecting the probe with the highest penalized standard deviation for each duplicated gene ID."
      },
      {
        "question": "How does the function handle missing data in the input datasets, and what is the significance of the `pena` variable in the code?",
        "answer": "The function handles missing data in several ways: 1) It removes probes with all NA measurements from both datasets. 2) When selecting the most variant probe for duplicated gene IDs, it uses a penalization factor (`pena`) to avoid selecting probes with many missing values. The `pena` variable is calculated as `log((nrow(data) + 1) / (number_of_NAs + 1)) + 1`. This penalization is then multiplied by the standard deviation of the probe's measurements, effectively reducing the 'variance score' of probes with more missing values, making them less likely to be selected as the representative probe for a gene."
      },
      {
        "question": "What are the key steps in the `geneid.map` function for ensuring that the output datasets have matching gene IDs?",
        "answer": "The key steps for ensuring matching gene IDs in the output are: 1) Finding the intersection of gene IDs between the two input datasets. 2) For each dataset, separating unique and duplicated gene IDs. 3) For duplicated gene IDs, selecting the most variant probe. 4) Combining the data for unique and selected duplicated gene IDs. 5) If `data2` is provided, ensuring both datasets have the same order of gene IDs by using `match(geneid2, geneid1)` to reorder `geneid1` and `data1`. This process ensures that the final output contains only common genes between the two datasets, with a single representative probe for each gene, and that the gene order matches between the two datasets."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  nn <- names(geneid1)\n  geneid1 <- as.character(geneid1)\n  names(geneid1) <- nn\n  nn <- names(geneid2)\n  geneid2 <- as.character(geneid2)\n  names(geneid2) <- nn\n  if(is.null(names(geneid1))) { names(geneid1) <- dimnames(data1)[[2]] }\n  if(!missing(data2) && is.null(names(geneid2))) { names(geneid2) <- dimnames(data2)[[2]] }\n  if(!missing(data1) && !missing(geneid1) && !missing(geneid2)) {\n    na.ix <- apply(data1, 2, function(x) { return(all(is.na(x))) })\n    data1 <- data1[ , !na.ix, drop=FALSE]\n    geneid1 <- geneid1[!na.ix]\n  } else { stop(\"data1, geneid1 and geneid2 parameters are mandatory!\") }\n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) { return(all(is.na(x))) })\n    data2 <- data2[ , !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else { data2 <- NULL }\n\n  gix1 <- !is.na(geneid1)\n  gix2 <- !is.na(geneid2)\n\n  geneid.common <- intersect(geneid1[gix1], geneid2[gix2])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n\n  # Complete the function to handle dataset1 and dataset2\n  # ...\n\n}",
        "complete": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  nn <- names(geneid1)\n  geneid1 <- as.character(geneid1)\n  names(geneid1) <- nn\n  nn <- names(geneid2)\n  geneid2 <- as.character(geneid2)\n  names(geneid2) <- nn\n  if(is.null(names(geneid1))) { names(geneid1) <- dimnames(data1)[[2]] }\n  if(!missing(data2) && is.null(names(geneid2))) { names(geneid2) <- dimnames(data2)[[2]] }\n  if(!missing(data1) && !missing(geneid1) && !missing(geneid2)) {\n    na.ix <- apply(data1, 2, function(x) { return(all(is.na(x))) })\n    data1 <- data1[ , !na.ix, drop=FALSE]\n    geneid1 <- geneid1[!na.ix]\n  } else { stop(\"data1, geneid1 and geneid2 parameters are mandatory!\") }\n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) { return(all(is.na(x))) })\n    data2 <- data2[ , !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else { data2 <- NULL }\n\n  gix1 <- !is.na(geneid1)\n  gix2 <- !is.na(geneid2)\n\n  geneid.common <- intersect(geneid1[gix1], geneid2[gix2])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n\n  gg <- names(geneid1)[is.element(geneid1, geneid.common)]\n  gid <- geneid1[is.element(geneid1, geneid.common)]\n  gid.dupl <- unique(gid[duplicated(gid)])\n  gg.dupl <- names(geneid1)[is.element(geneid1, gid.dupl)]\n  gid.uniq <- gid[!is.element(gid, gid.dupl)]\n  gg.uniq <- names(geneid1)[is.element(geneid1, gid.uniq)]\n  datat <- data1[ ,gg.uniq,drop=FALSE]\n  if(length(gid.dupl) > 0) {\n    if(verbose) { message(\"\\ndataset1 duplicates...\") }\n    pena <- apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n    pena <- log((nrow(data1) + 1) / (pena + 1)) + 1\n    sdr <- drop(apply(X=data1[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n    mysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid1[gg.dupl], \"sd\"=sdr)\n    mysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n    mysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n    datat <- cbind(datat, data1[ , mysd[ , \"probe\"], drop=FALSE])\n  }\n  data1 <- datat\n  geneid1 <- geneid1[dimnames(data1)[[2]]]\n\n  if(is.null(data2)) {\n    geneid2 <- geneid2[!duplicated(geneid2) & is.element(geneid2, geneid.common)]\n  } else {\n    gg <- names(geneid2)[is.element(geneid2, geneid.common)]\n    gid <- geneid2[is.element(geneid2, geneid.common)]\n    gid.dupl <- unique(gid[duplicated(gid)])\n    gg.dupl <- names(geneid2)[is.element(geneid2, gid.dupl)]\n    gid.uniq <- gid[!is.element(gid, gid.dupl)]\n    gg.uniq <- names(geneid2)[is.element(geneid2, gid.uniq)]\n    datat <- data2[ ,gg.uniq,drop=FALSE]\n    if(length(gid.dupl) > 0) {\n      if(verbose) { message(\"\\ndataset2 duplicates...\") }\n      pena <- apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=function(x) { return(sum(is.na(x))) })\n      pena <- log((nrow(data2) + 1) / (pena + 1)) + 1\n      sdr <- drop(apply(X=data2[ , gg.dupl, drop=FALSE], MARGIN=2, FUN=sd, na.rm=TRUE)) * pena\n      mysd <- cbind(\"probe\"=gg.dupl, \"gid\"=geneid2[gg.dupl], \"sd\"=sdr)\n      mysd <- mysd[order(as.numeric(mysd[ , \"sd\"]), decreasing=TRUE, na.last=TRUE), , drop=FALSE]\n      mysd <- mysd[!duplicated(mysd[ , \"gid\"]), , drop=FALSE]\n      datat <- cbind(datat, data2[ , mysd[ , \"probe\"], drop=FALSE])\n    }\n    data2 <- datat\n    geneid2 <- geneid2[dimnames(data2)[[2]]]\n  }\n\n  rix <- match(geneid2, geneid1)\n  geneid1 <- geneid1[rix]\n  data1 <- data1[ ,rix,drop=FALSE]\n  return(list(\"geneid1\"=geneid1, \"data1\"=data1, \"geneid2\"=geneid2, \"data2\"=data2))\n}"
      },
      {
        "partial": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  # Initialize and validate input\n  geneid1 <- as.character(geneid1)\n  geneid2 <- as.character(geneid2)\n  names(geneid1) <- names(geneid1) %||% dimnames(data1)[[2]]\n  if(!missing(data2)) names(geneid2) <- names(geneid2) %||% dimnames(data2)[[2]]\n  \n  # Check for mandatory parameters\n  if(missing(data1) || missing(geneid1) || missing(geneid2)) {\n    stop(\"data1, geneid1 and geneid2 parameters are mandatory!\")\n  }\n  \n  # Remove probes without measurements\n  na.ix <- apply(data1, 2, function(x) all(is.na(x)))\n  data1 <- data1[, !na.ix, drop=FALSE]\n  geneid1 <- geneid1[!na.ix]\n  \n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) all(is.na(x)))\n    data2 <- data2[, !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else {\n    data2 <- NULL\n  }\n  \n  # Find common gene ids\n  geneid.common <- intersect(geneid1[!is.na(geneid1)], geneid2[!is.na(geneid2)])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n  \n  # Process dataset1\n  # ...\n  \n  # Process dataset2\n  # ...\n  \n  # Align datasets\n  # ...\n  \n  # Return result\n  # ...\n}",
        "complete": "geneid.map <- function(geneid1, data1, geneid2, data2, verbose=FALSE) {\n  # Initialize and validate input\n  geneid1 <- as.character(geneid1)\n  geneid2 <- as.character(geneid2)\n  names(geneid1) <- names(geneid1) %||% dimnames(data1)[[2]]\n  if(!missing(data2)) names(geneid2) <- names(geneid2) %||% dimnames(data2)[[2]]\n  \n  # Check for mandatory parameters\n  if(missing(data1) || missing(geneid1) || missing(geneid2)) {\n    stop(\"data1, geneid1 and geneid2 parameters are mandatory!\")\n  }\n  \n  # Remove probes without measurements\n  na.ix <- apply(data1, 2, function(x) all(is.na(x)))\n  data1 <- data1[, !na.ix, drop=FALSE]\n  geneid1 <- geneid1[!na.ix]\n  \n  if(!missing(data2)) {\n    na.ix <- apply(data2, 2, function(x) all(is.na(x)))\n    data2 <- data2[, !na.ix, drop=FALSE]\n    geneid2 <- geneid2[!na.ix]\n  } else {\n    data2 <- NULL\n  }\n  \n  # Find common gene ids\n  geneid.common <- intersect(geneid1[!is.na(geneid1)], geneid2[!is.na(geneid2)])\n  if(length(geneid.common) == 0) {\n    warning(\"no gene ids in common!\")\n    return(list(\"geneid1\"=NA, \"data1\"=NA, \"geneid2\"=NA, \"data2\"=NA))\n  }\n  \n  # Process dataset1\n  process_dataset <- function(geneid, data, common_ids, verbose) {\n    gg <- names(geneid)[geneid %in% common_ids]\n    gid <- geneid[geneid %in% common_ids]\n    gid.dupl <- unique(gid[duplicated(gid)])\n    gid.uniq <- setdiff(gid, gid.dupl)\n    datat <- data[, names(geneid)[geneid %in% gid.uniq], drop=FALSE]\n    \n    if(length(gid.dupl) > 0) {\n      if(verbose) message(\"\\ndataset duplicates...\")\n      gg.dupl <- names(geneid)[geneid %in% gid.dupl]\n      pena <- apply(data[, gg.dupl, drop=FALSE], 2, function(x) sum(is.na(x)))\n      "
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/utilities.R",
    "language": "R",
    "content": "# tSet molecularProfiles from eSets to SEs\n#\n# Converts all ExpressionSet objects within the molecularProfiles slot of a\n#   ToxicoSet to SummarizedExperiments\n#\n# @param tSet \\code{S4} A ToxicoSet containing molecular data in ExpressionSets\n#\n# @return \\code{S4} A ToxicoSet containing molecular data in a SummarizedExperiments\n#\n#' @importFrom SummarizedExperiment assay assays assayNames\n#' @importClassesFrom SummarizedExperiment SummarizedExperiment Assays\n#' @importFrom Biobase exprs fData pData annotation protocolData assayData experimentData\n#' @importFrom S4Vectors SimpleList DataFrame\n#' @importFrom stats setNames\n#' @export\n#' @keywords internal\n#' @noRd\n.convertTsetMolecularProfilesToSE <- function(tSet) {\n\n  eSets <- molecularProfilesSlot(tSet) # Extract eSet data\n\n  molecularProfilesSlot(tSet) <-\n    lapply(eSets,\n           function(eSet){\n\n             # Build summarized experiment from eSet\n             SE <- SummarizedExperiment::SummarizedExperiment(\n               ## TODO:: Do we want to pass an environment for better memory efficiency?\n               assays=SimpleList(as.list(Biobase::assayData(eSet))\n               ),\n               # Switch rearrange columns so that IDs are first, probes second\n               rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                            rownames=rownames(Biobase::fData(eSet))\n               ),\n               colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                            rownames=rownames(Biobase::pData(eSet))\n               ),\n               metadata=list(\"experimentData\" = eSet@experimentData,\n                             \"annotation\" = Biobase::annotation(eSet),\n                             \"protocolData\" = Biobase::protocolData(eSet)\n               )\n             )\n             ## TODO:: Determine if this can be done in the SE constructor?\n             # Extract names from expression set\n             SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n             # Assign SE to tSet\n             mDataType <- Biobase::annotation(eSet)\n             molecularProfilesSlot(tSet)[[mDataType]] <- SE\n           })\n  setNames(molecularProfilesSlot(tSet), names(eSets))\n  tSet\n}\n\n# Validate tSet molecularProfiles Conversion\n#\n# Checks that all the information contained in an ExpressionSet molecularProfile\n#   was successfully tranferred to the SummarizedExperiment molecularProfile\n#\n# @param tSet_new \\code{S4} a tSet containing molecularProfiles as SummarizedExperiments\n# @param tSet_old \\code{S4} a tSet containing molecularProfiles as ExpressionSets\n#\n# @return \\code{message} Any slots which are not the same\n#\n##' @importFrom assertthat are_equal\n##' @importFrom SummarizedExperiment SummarizedExperiment Assays assay\n##'   assayNames assayNames<-\n##' @importFrom Biobase exprs fData pData annotation protocolData\n##'   assayDataElementNames experimentData assayData\n##' @keywords internal\n#.validateTsetMolecularProfilesToSEConversion <- function(tSet_old, tSet_new) {\n#\n#  # Testing that tSets are in correct order\n#  message(\"Checking is tSet structures are correct\")\n#\n#  if(!all(vapply(tSet_old@molecularProfiles,\n#                 function(x) { is(x, \"ExpressionSet\") },\n#                 FUN.VALUE = logical(1)))\n#  ) message(\"Old tSet doesn't contain ExpressionSet objects, maybe argument\n#            order is wrong?\")\n#\n#  if(!all(vapply(molecularProfilesSlot(tSet_new),\n#                 function(x) { is(x, \"SummarizedExperiment\") },\n#                 FUN.VALUE = logical(1)))\n#  ) message(\"New tSet doesn't contain SummarizedExperiment objects, maybe\n#            argument order is wrong?\")\n#\n#  # Comparing molecularProfiles slot data\n#  message(\"Checking molecularProfiles slots hold equivalent data.\")\n#\n#  for (i in seq_len(length(tSet_old@molecularProfiles))) {\n#    for (j in seq_along(assays(molecularProfilesSlot(tSet_new)[[i]]))) {\n#      if(!all(as.list(assayData(tSet_old@molecularProfiles[[i]]))[[j]] ==\n#              as.list(assays(molecularProfilesSlot(tSet_new)[[i]]))[[j]],\n#          na.rm = TRUE)\n#        ) message(\"The assay data is not equivalent\")\n#    }\n#  }\n#  ## TODO:: Rewrite this as an apply statement\n#  for (i in seq_len(length(tSet_old@molecularProfiles))) { # Have to compare like this due to NAs in data\n#    # Checking phenoData\n#    if(\n#      if (nrow(pData(tSet_old@molecularProfiles[[i]])) > 0) {\n#        !all(\n#          as(tSet_old@molecularProfiles[[i]]@phenoData, \"data.frame\") ==\n#            as.data.frame(molecularProfilesSlot(tSet_new)[[i]]@colData[\n#              seq_len(length(molecularProfilesSlot(tSet_new)[[i]]@colData) - 1)]),\n#          na.rm = TRUE)\n#      } else { FALSE }\n#    ) message(\"The phenoData is not equivalent\")\n#    # Checking featureData\n#    if(\n#      if (nrow(fData(tSet_old@molecularProfiles[[i]])) > 0) {\n#        !all(\n#          as(tSet_old@molecularProfiles[[i]]@featureData, \"data.frame\") ==\n#            as.data.frame(molecularProfilesSlot(tSet_new)[[i]]@elementMetadata[\n#              seq_len(length(molecularProfilesSlot(tSet_new)[[i]]@elementMetadata) - 1)]),\n#          na.rm=TRUE)\n#      } else { FALSE }\n#    ) message(\"The featureData is not equivalent\")\n#    # Checking protocolData\n#    if(\n#      !all(\n#        as(tSet_old@molecularProfiles[[i]]@protocolData, \"data.frame\") ==\n#          as(molecularProfilesSlot(tSet_new)[[i]]@metadata$protocolData, \"data.frame\"),\n#        na.rm = TRUE)\n#    ) message(\"The protocolData is not equivalent\")\n#  }\n#\n#  if(!assertthat::are_equal(\n#    lapply(tSet_old@molecularProfiles, function(x) { annotation(x) }),\n#    lapply(molecularProfilesSlot(tSet_new), function(x) { metadata(x)$annotation }))\n#  ) message(\"The annotation is not equivalent\")\n#\n#  if(!assertthat::are_equal(\n#    lapply(tSet_old@molecularProfiles, function(x) { experimentData(x) }),\n#    lapply(molecularProfilesSlot(tSet_new), function(x) { metadata(x)$experimentData }))\n#  ) message(\"The experimentData is not equivalent\")\n#\n#  # Comparing remainder of tSet slots; should not be affect by conversion\n#  message(\"Comparing remainder of tSet slots\")\n#\n#  if(!assertthat::are_equal(annotation(tSet_old), annotation(tSet_new)))\n#    message(\"The annotation slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(sampleInfo(tSet_old), sampleInfo(tSet_new)))\n#    message(\"The cell slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(treatmentInfo(tSet_old), treatmentInfo(tSet_new)))\n#    message(\"The drug slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(treatmentResponse(tSet_old), treatmentResponse(tSet_new)))\n#    message(\"The sensitivity slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(datasetType(tSet_old), datasetType(tSet_new)))\n#    message(\"The datasetType slots are not equivalent!\")\n#\n#  if(!assertthat::are_equal(tSet_old@perturbation, tSet_new@perturbation))\n#    message(\"The perturbation slots are not equivalent!\")\n#\n# if(!assertthat::are_equal(curation(tSet_old), curation(tSet_new)))\n#   message(\"The curation slots are not equivalent!\")\n#}\n\n# Utility function to resave all datasets after modifying converttSetMolecularProfiles\n#\n# Converts all example dastasets specificed as an argument from\n#   molecularProfiles as ExpressionSet to molecularProfiles as\n#   SummarizedExperiment and saves them in the data folder\n#\n# @param datasets \\code{character} A list of the example datasets to update\n#\n# @return \\code{none} Works by side effects alone to resave all example\n#   datasets in a package to have SummarizedExperiments for molecularProfiles\n#.resaveAllExampleDatasets <- function(datasets) {\n#  for (dataset in datasets) {\n#    dataDir <- paste0(grep('data', list.dirs(), value=TRUE))\n#    load(paste0(dataDir, '/', dataset, '_old.rda'))\n#    assign(dataset, .convertTsetMolecularProfilesToSE(get(dataset)))\n#    save(list=dataset, file=paste0(dataDir, '/', dataset, '.rda'), compress='xz')\n#  }\n#}\n#\n\n.eSetToSE <- function(eSet) {\n    # Build summarized experiment from eSet\n    SE <- SummarizedExperiment::SummarizedExperiment(\n        ## TODO:: Do we want to pass an environment for better memory efficiency?\n        assays=SimpleList(as.list(Biobase::assayData(eSet))\n        ),\n        # Switch rearrange columns so that IDs are first, probes second\n        rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                     rownames=rownames(Biobase::fData(eSet))\n        ),\n        colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                     rownames=rownames(Biobase::pData(eSet))\n        ),\n        metadata=list(\"experimentData\" = eSet@experimentData,\n                      \"annotation\" = Biobase::annotation(eSet),\n                      \"protocolData\" = Biobase::protocolData(eSet)\n        )\n    )\n    ## TODO:: Determine if this can be done in the SE constructor?\n    # Extract names from expression set\n    SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n    return(SE)\n}\n\n#.validateESetToSEConversions <- function(eSet, SE) {\n#        for (j in seq_along(assays(SE))) {\n#            if(!all(as.list(assayData(eSet))[[j]] ==\n#                    as.list(assays(SE))[[j]],\n#                    na.rm = TRUE)\n#            ) message(\"The assay data is not equivalent\")\n#        }\n#    ## TODO:: Rewrite this as an apply statement\n#        # Checking phenoData\n#        if(\n#            if (nrow(pData(eSet)) > 0) {\n#                !all(\n#                    as(eSet@phenoData, value=\"data.frame\") ==\n#                    as.data.frame(SE@colData[\n#                        seq_len(length(SE@colData) - 1)]),\n#                    na.rm = TRUE)\n#            } else { FALSE }\n#        ) message(\"The phenoData is not equivalent\")\n#        # Checking featureData\n#        if(\n#            if (nrow(fData(eSet)) > 0) {\n#                !all(\n#                    as(eSet@featureData, value=\"data.frame\") ==\n#                    as.data.frame(SE@elementMetadata[\n#                        seq_len(length(SE@elementMetadata) - 1)]),\n#                    na.rm=TRUE)\n#            } else { FALSE }\n#        ) message(\"The featureData is not equivalent\")\n#        # Checking protocolData\n#        if(\n#            !all(\n#                as(eSet@protocolData, value=\"data.frame\") ==\n#                as(SE@metadata$protocolData, value=\"data.frame\"),\n#                na.rm = TRUE)\n#        ) message(\"The protocolData is not equivalent\")\n#\n#    if(!assertthat::are_equal(\n#        annotation(eSet),\n#        metadata(SE)$annotation\n#    )) message(\"The annotation is not equivalent\")\n#\n#    if(!assertthat::are_equal(\n#        experimentData(eSet),\n#        metadata(SE)$experimentData\n#    )) message(\"The experimentData is not equivalent\")\n#}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `.convertTsetMolecularProfilesToSE` function?",
        "answer": "The main purpose of the `.convertTsetMolecularProfilesToSE` function is to convert all ExpressionSet objects within the molecularProfiles slot of a ToxicoSet to SummarizedExperiments. It takes a ToxicoSet containing molecular data in ExpressionSets as input and returns a ToxicoSet with molecular data in SummarizedExperiments."
      },
      {
        "question": "How does the function handle the conversion of assay data from ExpressionSet to SummarizedExperiment?",
        "answer": "The function converts assay data by using `SimpleList(as.list(Biobase::assayData(eSet)))` to create the assays for the SummarizedExperiment. This approach ensures that all assay data from the ExpressionSet is transferred to the new SummarizedExperiment structure."
      },
      {
        "question": "What additional metadata is preserved during the conversion process, and how is it stored in the resulting SummarizedExperiment?",
        "answer": "The function preserves additional metadata including experimentData, annotation, and protocolData from the ExpressionSet. These are stored in the metadata list of the SummarizedExperiment, with keys 'experimentData', 'annotation', and 'protocolData' respectively. This ensures that all relevant information from the original ExpressionSet is maintained in the new structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Convert ExpressionSet to SummarizedExperiment\n.eSetToSE <- function(eSet) {\n    SE <- SummarizedExperiment::SummarizedExperiment(\n        assays=SimpleList(as.list(Biobase::assayData(eSet))),\n        rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                     rownames=rownames(Biobase::fData(eSet))),\n        colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                     rownames=rownames(Biobase::pData(eSet))),\n        metadata=list(\"experimentData\" = eSet@experimentData,\n                      \"annotation\" = Biobase::annotation(eSet),\n                      \"protocolData\" = Biobase::protocolData(eSet))\n    )\n    # TODO: Complete the function\n    return(SE)\n}",
        "complete": "# Convert ExpressionSet to SummarizedExperiment\n.eSetToSE <- function(eSet) {\n    SE <- SummarizedExperiment::SummarizedExperiment(\n        assays=SimpleList(as.list(Biobase::assayData(eSet))),\n        rowData=S4Vectors::DataFrame(Biobase::fData(eSet),\n                                     rownames=rownames(Biobase::fData(eSet))),\n        colData=S4Vectors::DataFrame(Biobase::pData(eSet),\n                                     rownames=rownames(Biobase::pData(eSet))),\n        metadata=list(\"experimentData\" = eSet@experimentData,\n                      \"annotation\" = Biobase::annotation(eSet),\n                      \"protocolData\" = Biobase::protocolData(eSet))\n    )\n    SummarizedExperiment::assayNames(SE) <- Biobase::assayDataElementNames(eSet)\n    return(SE)\n}"
      },
      {
        "partial": "# Convert ToxicoSet molecularProfiles from eSets to SEs\n.convertTsetMolecularProfilesToSE <- function(tSet) {\n    eSets <- molecularProfilesSlot(tSet)\n    molecularProfilesSlot(tSet) <- lapply(eSets, function(eSet) {\n        SE <- .eSetToSE(eSet)\n        # TODO: Complete the function\n    })\n    # TODO: Complete the function\n    return(tSet)\n}",
        "complete": "# Convert ToxicoSet molecularProfiles from eSets to SEs\n.convertTsetMolecularProfilesToSE <- function(tSet) {\n    eSets <- molecularProfilesSlot(tSet)\n    molecularProfilesSlot(tSet) <- lapply(eSets, function(eSet) {\n        SE <- .eSetToSE(eSet)\n        mDataType <- Biobase::annotation(eSet)\n        molecularProfilesSlot(tSet)[[mDataType]] <- SE\n    })\n    setNames(molecularProfilesSlot(tSet), names(eSets))\n    return(tSet)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/tests/testthat/test-tSetClassAccessorMethods.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(ToxicoGx)\n\n#### Checking example tSet structure ####\ncontext(\"Testing TGGATESsmall object validity...\")\n\ntest_that(\"TGGATESsmall tSet has correct structure\", {\n    data(TGGATESsmall)\n    expect_error(checkTSetStructure(TGGATESsmall), NA)\n})\n\n#### tSet Acessor methods ####\ncontext(\"Testing tSet Class Accessor Methods...\")\n\n# @treatment Slot\ntest_that(\"@treatment slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(treatmentInfo(TGGATESsmall),\n        \"drugInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(treatmentNames(TGGATESsmall),\n        \"drugNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(treatmentInfo(TGGATESsmall), TGGATESsmall@treatment)\n    expect_equal(treatmentNames(TGGATESsmall), TGGATESsmall@treatment$treatmentid)\n})\n\n# @annotation Slot\ntest_that(\"@annotation slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(TGGATESsmall@annotation,\n        \"annotation.TGGATESsmall.rds\")\n    expect_equal_to_reference(name(TGGATESsmall), \"name.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(name(TGGATESsmall), TGGATESsmall@annotation$name)\n})\n\n# @molecularProfile Slot\ntest_that(\"@molecularProfiles slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(mDataNames(TGGATESsmall),\n        \"mDataNames.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(mDataNames(TGGATESsmall),\n        names(TGGATESsmall@molecularProfiles))\n\n    ## TODO:: Test this with incorrect tSet structure to determine if error\n    ##>messages print in the correct order\n    for (name in names(TGGATESsmall@molecularProfiles)) {\n        context(\"External validation...\")\n        expect_equal_to_reference(\n            molecularProfiles(TGGATESsmall, name)[, 1:100],\n            paste0(name, \".molecularProfiles.TGGATESsmall.rds\")\n        )\n        expect_equal_to_reference(featureInfo(TGGATESsmall, name),\n            paste0(name, \".featureInfo.TGGATESsmall.rds\"))\n        expect_equal_to_reference(fNames(TGGATESsmall, name),\n            paste0(name, \".fNames.TGGATESsmall.rds\"))\n        expect_equal_to_reference(phenoInfo(TGGATESsmall, name)[1:100, ],\n            paste0(name, \".phenoData.TGGATESsmall.rds\"))\n        context(\"Internal validation...\")\n        # expect_equal(molecularProfiles(TGGATESsmall, name),\n        #     assay(TGGATESsmall@molecularProfiles[[name]], 1))\n        expect_equal(featureInfo(TGGATESsmall, name),\n            rowData(TGGATESsmall@molecularProfiles[[name]]))\n        expect_equal(fNames(TGGATESsmall, name),\n            rownames(rowData(TGGATESsmall@molecularProfiles[[name]])))\n        expect_equal(phenoInfo(TGGATESsmall, name),\n            colData(TGGATESsmall@molecularProfiles[[name]]))\n    }\n})\n\n# @sample Slot\ntest_that(\"@sample slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(sampleInfo(TGGATESsmall),\n        \"cellInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(sampleNames(TGGATESsmall),\n        \"cellNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(sampleInfo(TGGATESsmall), TGGATESsmall@sample)\n    expect_equal(sampleNames(TGGATESsmall), TGGATESsmall@sample$sampleid)\n})\n\n# @sensitivty Slot\ntest_that(\"@treatmentResponse slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(sensitivityInfo(TGGATESsmall),\n        \"sensitivityInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(sensitivityProfiles(TGGATESsmall),\n        \"sensitivitProfiles.TGGATESsmall.rds\")\n    expect_equal_to_reference(sensitivityMeasures(TGGATESsmall),\n        \"sensitivityMeasures.TGGATESsmall.rds\")\n    expect_equal_to_reference(sensNumber(TGGATESsmall),\n        \"sensNumber.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(sensitivityInfo(TGGATESsmall), TGGATESsmall@treatmentResponse$info)\n    expect_equal(sensitivityProfiles(TGGATESsmall),\n        TGGATESsmall@treatmentResponse$profiles)\n    expect_equal(sensitivityMeasures(TGGATESsmall),\n        colnames(TGGATESsmall@treatmentResponse$profiles))\n    expect_equal(sensNumber(TGGATESsmall), TGGATESsmall@treatmentResponse$n)\n})\n\n# @perturbation Slot\ntest_that(\"@perturbation slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(TGGATESsmall@perturbation,\n        \"perturbation.TGGATESsmall.rds\")\n    expect_equal_to_reference(pertNumber(TGGATESsmall),\n        \"pertNumber.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(pertNumber(TGGATESsmall), TGGATESsmall@perturbation$n)\n})\n\n# @curation Slot\ntest_that(\"@curation slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(TGGATESsmall@curation,\n        \"curation.TGGATESsmall.rds\")\n})\n\n# subsetTo Method\ntest_that(\"subsetTo() class method produces expected results\", {\n    data(\"TGGATESsmall\")\n\n    ## TODO:: Add unit tests for `[` subset operator\n    ## TODO:: Change context() messages to be more informative when\n    ##>running devtools::test()\n    context(\"External validation...\")\n    expect_equal_to_reference(\n        subsetTo(TGGATESsmall, drugs = treatmentNames(TGGATESsmall)[1],\n            cell_lines=sampleNames(TGGATESsmall)[1]),\n        \"subsetTo.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    ## Tests that subsetting molecularProfiles on duration works\n    expect_equal(all(\n        sensitivityInfo(subsetTo(TGGATESsmall, duration = \"2\"))$duration_h\n            %in% \"2\"),\n        TRUE)\n    # Tests that relationship between sensitivity experiments and\n    #>molecularProfiles is preserved\n    #>(4 molecular Profiles / 1 sensitivity experiment)\n    for (name in names(molecularProfilesSlot(TGGATESsmall))) {\n        testthat::context(paste0(\"Testing subsetTo on molecularProfile for \",\n            name))\n        ## TODO:: Generalize duration arguement so that it uses the first\n        ##>unique duration value in tSet (replace \"8\" with this)\n        testthat::expect_equal(all(\n            SummarizedExperiment::colData(\n                ToxicoGx::subsetTo(TGGATESsmall, duration = \"8\"\n                    )@molecularProfiles[[name]])$duration %in% \"8\"),\n            TRUE)\n    }\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `checkTSetStructure` function in the given code, and how is it being used?",
        "answer": "The `checkTSetStructure` function is used to validate the structure of a ToxicoGx TSet object. In this code, it's being used within a test case to ensure that the TGGATESsmall dataset has the correct structure. The test expects that calling `checkTSetStructure(TGGATESsmall)` will not produce an error, which is checked using the `expect_error(checkTSetStructure(TGGATESsmall), NA)` assertion."
      },
      {
        "question": "How does the code validate the consistency between accessor methods and direct slot access for the TGGATESsmall object?",
        "answer": "The code validates consistency between accessor methods and direct slot access by comparing the results of accessor functions with the corresponding slot values. For example, in the '@treatment Slot' test, it checks if `treatmentInfo(TGGATESsmall)` equals `TGGATESsmall@treatment`, and if `treatmentNames(TGGATESsmall)` equals `TGGATESsmall@treatment$treatmentid`. This pattern is repeated for other slots like @annotation, @molecularProfiles, @sample, etc., ensuring that the accessor methods correctly retrieve the data stored in the object's slots."
      },
      {
        "question": "What is the purpose of the `subsetTo` method in the given code, and how is it being tested?",
        "answer": "The `subsetTo` method is used to create a subset of the TGGATESsmall dataset based on specified criteria. In the code, it's being tested in several ways:\n1. External validation: The result of subsetting by the first drug and cell line is compared to a reference file.\n2. Internal validation: It checks if subsetting by duration works correctly by verifying that all duration values in the subset are as expected.\n3. It tests if the relationship between sensitivity experiments and molecular profiles is preserved after subsetting.\n4. It also checks if subsetting works correctly for each molecular profile type in the dataset.\nThese tests ensure that the `subsetTo` method correctly filters the data while maintaining the integrity and relationships within the TSet object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"@treatment slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(treatmentInfo(TGGATESsmall),\n        \"drugInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(treatmentNames(TGGATESsmall),\n        \"drugNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(treatmentInfo(TGGATESsmall), TGGATESsmall@treatment)\n    expect_equal(treatmentNames(TGGATESsmall), TGGATESsmall@treatment$treatmentid)\n})",
        "complete": "test_that(\"@treatment slot accessors produce expected results\", {\n    data(\"TGGATESsmall\")\n    context(\"External validation...\")\n    expect_equal_to_reference(treatmentInfo(TGGATESsmall),\n        \"drugInfo.TGGATESsmall.rds\")\n    expect_equal_to_reference(treatmentNames(TGGATESsmall),\n        \"drugNames.TGGATESsmall.rds\")\n\n    context(\"Internal validation...\")\n    expect_equal(treatmentInfo(TGGATESsmall), TGGATESsmall@treatment)\n    expect_equal(treatmentNames(TGGATESsmall), TGGATESsmall@treatment$treatmentid)\n})"
      },
      {
        "partial": "test_that(\"subsetTo() class method produces expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(\n        subsetTo(TGGATESsmall, drugs = treatmentNames(TGGATESsmall)[1],\n            cell_lines=sampleNames(TGGATESsmall)[1]),\n        \"subsetTo.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(all(\n        sensitivityInfo(subsetTo(TGGATESsmall, duration = \"2\"))$duration_h\n            %in% \"2\"),\n        TRUE)\n    # Complete the test for molecularProfiles\n})",
        "complete": "test_that(\"subsetTo() class method produces expected results\", {\n    data(\"TGGATESsmall\")\n\n    context(\"External validation...\")\n    expect_equal_to_reference(\n        subsetTo(TGGATESsmall, drugs = treatmentNames(TGGATESsmall)[1],\n            cell_lines=sampleNames(TGGATESsmall)[1]),\n        \"subsetTo.TGGATESsmall.rds\")\n    context(\"Internal validation...\")\n    expect_equal(all(\n        sensitivityInfo(subsetTo(TGGATESsmall, duration = \"2\"))$duration_h\n            %in% \"2\"),\n        TRUE)\n    for (name in names(molecularProfilesSlot(TGGATESsmall))) {\n        testthat::context(paste0(\"Testing subsetTo on molecularProfile for \",\n            name))\n        testthat::expect_equal(all(\n            SummarizedExperiment::colData(\n                ToxicoGx::subsetTo(TGGATESsmall, duration = \"8\"\n                    )@molecularProfiles[[name]])$duration %in% \"8\"),\n            TRUE)\n    }\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/read.m.file.R",
    "language": "R",
    "content": "#' @title Function to read a 'csv' file containing gene lists (aka gene signatures)\n#'\n#' @description\n#' This function allows for reading a 'csv' file containing gene signatures. \n#'   Each gene signature is composed of at least four columns: \"gene.list\" is the name \n#'   of the signature on the first line and empty fields below, \"probes\" are the probe \n#'   names, \"EntrezGene.ID\" are the EntrezGene IDs and \"coefficient\" are the coefficients \n#'   of each probe.\n#'\n#' @usage\n#' read.m.file(file, ...)\n#'\n#' @param file\tFilename of the 'csv' file.\n#' @param ... Additional parameters for read.csv function.\n#'\n#' @return\n#' List of gene signatures.\n#'\n#' @seealso\n#' [genefu::mod1], [genefu::mod2], 'extdata/desmedt2008_genemodules.csv', 'extdata/haibekains2009_sig_genius.csv'\n#'\n#' @examples\n#' # read the seven gene modules as published in Desmedt et al 2008\n#' genemods <- read.m.file(system.file(\"extdata/desmedt2008_genemodules.csv\",\n#'   package = \"genefu\"))\n#' str(genemods, max.level=1)\n#' # read the three subtype signtaures from GENIUS\n#' geniusm <- read.m.file(system.file(\"extdata/haibekains2009_sig_genius.csv\",\n#'   package = \"genefu\"))\n#' str(geniusm, max.level=1)\n#' \n#' @md\n#' @export\nread.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\tix.l <- ix.delim[i] - 1\n\t\t\tnp <- c(np, as.character(obj.file[ix.f,1]))\n\t\t\tgroups <- c(groups, rep(i, ix.l - ix.f + 1))\n\t\t\tnpp <- rbind(npp, obj.file[ix.f:ix.l,2:ncol(obj.file)])\n\t\t\tix.f <- ix.l + 2\n\t\t}\n\t\tugroups <- unique(groups)\n\t\tobj <- NULL\n\t\tfor (j in 1:length(ugroups)) {\n\t\t\tobj <- c(obj, list(npp[groups == ugroups[j], ]))\n\t\t}\n\tnames(obj) <- np\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read.m.file` function and what type of data does it process?",
        "answer": "The `read.m.file` function is designed to read a CSV file containing gene signatures. It processes data where each gene signature is composed of at least four columns: 'gene.list' (signature name), 'probes' (probe names), 'EntrezGene.ID' (EntrezGene IDs), and 'coefficient' (coefficients of each probe). The function returns a list of gene signatures."
      },
      {
        "question": "How does the function handle multiple gene signatures within a single CSV file?",
        "answer": "The function handles multiple gene signatures by identifying non-empty entries in the first column as delimiters between signatures. It uses these delimiters to split the data into separate groups, each representing a distinct gene signature. The function then creates a list where each element corresponds to a different gene signature, with the signature names used as list element names."
      },
      {
        "question": "What is the significance of the `groups` and `ugroups` variables in the function, and how are they used?",
        "answer": "The `groups` variable is used to assign a group number to each row of data, indicating which gene signature it belongs to. The `ugroups` variable contains the unique group numbers. These variables are crucial for separating the data into distinct gene signatures. The function iterates through `ugroups` to create separate list elements for each unique gene signature, effectively organizing the data into a structured list output."
      }
    ],
    "completion_tasks": [
      {
        "partial": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\t# Complete the code here\n\t\t}\n\t\t# Complete the rest of the function\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}",
        "complete": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\tix.l <- ix.delim[i] - 1\n\t\t\tnp <- c(np, as.character(obj.file[ix.f,1]))\n\t\t\tgroups <- c(groups, rep(i, ix.l - ix.f + 1))\n\t\t\tnpp <- rbind(npp, obj.file[ix.f:ix.l,2:ncol(obj.file)])\n\t\t\tix.f <- ix.l + 2\n\t\t}\n\t\tugroups <- unique(groups)\n\t\tobj <- NULL\n\t\tfor (j in 1:length(ugroups)) {\n\t\t\tobj <- c(obj, list(npp[groups == ugroups[j], ]))\n\t\t}\n\t\tnames(obj) <- np\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}"
      },
      {
        "partial": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\t# Complete the code here\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}",
        "complete": "read.m.file <- function(file, ...) {\n\tobj.file <- read.csv(file, stringsAsFactors=FALSE, ...)\n\tif(sum(!is.na(obj.file[ ,1]) & obj.file[ ,1] != \"\") > 0) {\n\t\tix.delim <- c(which(obj.file[ ,1] != \"\")[-1]-1, nrow(obj.file) + 1)\n\t\tix.f <- ix.l <- 1\n\t\tgroups <- NULL\n\t\tnpp <- np <- NULL\n\t\tfor (i in 1:length(ix.delim)) {\n\t\t\tix.l <- ix.delim[i] - 1\n\t\t\tnp <- c(np, as.character(obj.file[ix.f,1]))\n\t\t\tgroups <- c(groups, rep(i, ix.l - ix.f + 1))\n\t\t\tnpp <- rbind(npp, obj.file[ix.f:ix.l,2:ncol(obj.file)])\n\t\t\tix.f <- ix.l + 2\n\t\t}\n\t\tugroups <- unique(groups)\n\t\tobj <- NULL\n\t\tfor (j in 1:length(ugroups)) {\n\t\t\tobj <- c(obj, list(npp[groups == ugroups[j], ]))\n\t\t}\n\t\tnames(obj) <- np\n\t} else { obj <- list(\"module\"=obj)}\n\treturn(obj)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/geneDrugPerturbation.R",
    "language": "R",
    "content": "#' Compute gene-drug associations\n#'\n#' Function computing gene-drug associations from perturbation data\n#'\n#' @examples\n#' ToxicoGx::drugPerturbationSig(tSet = TGGATESsmall,\n#'   mDataType=\"rna\",\n#'   cell_lines=\"Hepatocyte\",\n#'   duration=\"24\",\n#'   dose=c(\"Control\", \"Low\"),\n#'   drugs=c(\"Omeprazole\", \"Isoniazid\"),\n#'   returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n#'   verbose=FALSE)\n#'\n#' @param x [numeric] Vector of gene expression values\n#' @param concentration [numeric] Vector with drug concentrations/doses\n#' @param type [factor] Vector of factors specifying the cell lines or type types\n#' @param batch [factor] Vector of factors specifying the batch\n#' @param duration [character] Vector of measurement times (in hours)\n#' @param model [logical] Should the full linear model be returned? Default set to FALSE\n#'\n#' @return [numeric] Vector reporting the effect size (estimateof the coefficient of drug concentration), standard error (se), sample size (n), t statistic, and F statistics and its corresponding p-value\n#'\n#' @importFrom stats complete.cases median lm anova\n#'\n#' @keywords internal\n#' @export\ngeneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n\n  if (length(sort(unique(type))) > 1) {\n    ff0 <- sprintf(\"%s + type\", ff0)\n    ff <- sprintf(\"%s + type\", ff)\n  }\n  if (length(sort(unique(batch))) > 1) {\n    ff0 <- sprintf(\"%s + batch\", ff0)\n    ff <- sprintf(\"%s + batch\", ff)\n  }\n\n  ### add experiment duration if the vector consists of more than one different value\n\n  if(length(sort(unique(duration))) > 2){\n    ff0 <- sprintf(\"%s + duration\", ff0)\n    ff <- sprintf(\"%s + duration\", ff)\n  }\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    ## extract statistics\n    tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n  }\n  names(tt) <- nc\n  ## add tissue type/cell line statistics\n  if(length(sort(unique(type))) > 1) {\n    rr <- summary(mm0)\n    ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n  } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n  tt <- c(tt, ttype)\n  ## add model\n  if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n  return(tt)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `geneDrugPerturbation` function and what are its main input parameters?",
        "answer": "The `geneDrugPerturbation` function computes gene-drug associations from perturbation data. Its main input parameters are:\n- `x`: A numeric vector of gene expression values\n- `concentration`: A numeric vector of drug concentrations/doses\n- `type`: A factor vector specifying cell lines or types\n- `batch`: A factor vector specifying the batch\n- `duration`: A character vector of measurement times (in hours)\n- `model`: A logical value indicating whether to return the full linear model (default is FALSE)"
      },
      {
        "question": "How does the function handle different scenarios in terms of the number of unique concentrations, types, batches, and durations?",
        "answer": "The function adapts its behavior based on the input data:\n1. If there are fewer than 2 unique concentrations, it returns a warning and a vector of NAs.\n2. If there are more than 1 unique type, it adds 'type' to the linear model formula.\n3. If there are more than 1 unique batch, it adds 'batch' to the formula.\n4. If there are more than 2 unique durations, it adds 'duration' to the formula.\nThis approach allows the function to create an appropriate linear model based on the available data."
      },
      {
        "question": "What statistical measures does the `geneDrugPerturbation` function return, and how are they calculated?",
        "answer": "The function returns several statistical measures:\n1. Estimate: The coefficient of drug concentration from the linear model\n2. Standard Error (se): The standard error of the estimate\n3. Sample size (n): The number of complete cases in the data\n4. T-statistic: The t-value for the concentration coefficient\n5. F-statistic: From the ANOVA comparison of models with and without concentration\n6. P-value: The p-value from the F-test\n7. Type F-statistic and p-value: If multiple types are present\n\nThese are calculated using linear models (lm) and ANOVA, comparing a model with concentration to one without it. If requested, the full linear model is also returned."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  # Add type, batch, and duration to formulas if necessary\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    # Fit models and extract statistics\n  }\n\n  # Complete the function\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  if (length(sort(unique(type))) > 1) {\n    ff0 <- sprintf(\"%s + type\", ff0)\n    ff <- sprintf(\"%s + type\", ff)\n  }\n  if (length(sort(unique(batch))) > 1) {\n    ff0 <- sprintf(\"%s + batch\", ff0)\n    ff <- sprintf(\"%s + batch\", ff)\n  }\n  if(length(sort(unique(duration))) > 2){\n    ff0 <- sprintf(\"%s + duration\", ff0)\n    ff <- sprintf(\"%s + duration\", ff)\n  }\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n  }\n  names(tt) <- nc\n  if(length(sort(unique(type))) > 1) {\n    rr <- summary(mm0)\n    ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n  } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n  tt <- c(tt, ttype)\n  if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n  return(tt)\n}"
      },
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  # Add type, batch, and duration to formulas if necessary\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    # Extract statistics\n  }\n  names(tt) <- nc\n  # Add type statistics and model if necessary\n  return(tt)\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n  nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n  if (length(sort(unique(concentration))) < 2) {\n    warning(\"No drug concentrations tested\")\n    tt <- rep(NA, length(nc))\n    names(tt) <- nc\n    return(tt)\n  }\n  ff0 <- sprintf(\"x ~ 1\")\n  ff <- sprintf(\"%s + concentration\", ff0)\n\n  if (length(sort(unique(type))) > 1) {\n    ff0 <- sprintf(\"%s + type\", ff0)\n    ff <- sprintf(\"%s + type\", ff)\n  }\n  if (length(sort(unique(batch))) > 1) {\n    ff0 <- sprintf(\"%s + batch\", ff0)\n    ff <- sprintf(\"%s + batch\", ff)\n  }\n  if(length(sort(unique(duration))) > 2){\n    ff0 <- sprintf(\"%s + duration\", ff0)\n    ff <- sprintf(\"%s + duration\", ff)\n  }\n\n  dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n  nn <- sum(complete.cases(dd))\n  if(nn < 3) {\n    tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n  } else {\n    names(dd)[1]<-\"x\"\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm <- summary(mm)\n    tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n  }\n  names(tt) <- nc\n  if(length(sort(unique(type))) > 1) {\n    rr <- summary(mm0)\n    ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n  } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n  tt <- c(tt, ttype)\n  if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n  return(tt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/endoPredict.R",
    "language": "R",
    "content": "if(getRversion() >= \"2.15.1\")  utils::globalVariables(\"sig.endoPredict\")\n\n#' @name endoPredict\n#' @title Function to compute the endoPredict signature as published by Filipits et al 2011\n#'\n#' @description\n#' This function computes signature scores and risk classifications from gene expression\n#'   values following the algorithm used for the endoPredict signature as published by\n#'   Filipits et al 2011.\n#'\n#' @usage\n#' endoPredict(data, annot, do.mapping = FALSE, mapping, verbose = FALSE)\n#'\n#' @param data\tMatrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\",\n#'    dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed (in\n#'   case ofambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#'   Note that for Affymetrix HGU datasets, the mapping is not necessary.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to force the mapping\n#'   such that the probes are not selected based on their variance.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' The function works best if data have been noralized with MAS5. Note that for Affymetrix\n#'   HGU datasets, the mapping is not necessary.\n#'\n#' @return\n#' A list with items:\n#' -score Continuous signature scores\n#' -risk Binary risk classification, 1 being high risk and 0 being low risk.\n#' -mapping Mapping used if necessary.\n#' -probe If mapping is performed, this matrix contains the correspondence between the gene\n#' list (aka signature) and gene expression data.\n#'\n#' @references\n#' Filipits, M., Rudas, M., Jakesz, R., Dubsky, P., Fitzal, F., Singer, C. F., et al. (2011).\n#'   \"A new molecular predictor of distant recurrence in ER-positive, HER2-negative\n#'   breast cancer adds independent information to conventional clinical risk factors.\"\n#'   Clinical Cancer Research, 17(18):6012\u20136020.\n#'\n#' @examples\n#' # load GENE70 signature\n#' data(sig.endoPredict)\n#' # load NKI dataset\n#' data(vdxs)\n#' # compute relapse score\n#' rs.vdxs <- endoPredict(data=data.vdxs, annot=annot.vdxs, do.mapping=FALSE)\n#'\n#' @md\n#' @export\nendoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n\n\t## the reference genes are not taken into account due to their absence from most platforms\n  #sig2 <- sig.endoPredict\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n\trownames(sig2) <- sig2[ , \"probe.affy\"]\n\tgt <- nrow(sig2)\n\tif(do.mapping) { ## not an affy HGU platform\n\t\tgid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid1) <- dimnames(sig2)[[1]]\n\t\tgid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\t\t## mqpping\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tgm <- length(rr$geneid2)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t\tif(!all(is.element(sig2[sig2[ , \"group\"] == \"GOI\", \"EntrezGene.ID\"], rr$geneid1))) { ## if genes of interest are missing\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n\t\t\treturn(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n\t\t}\n\t\tgid1 <- rr$geneid2\n\t\tgid2 <- rr$geneid1\n\t\tdata <- rr$data1\n\t\tmyprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n\t\t## change the names of probes in the data\n\t\tcolnames(data) <- names(gid2) <- names(gid1)\n    sig2 <- sig2[colnames(data), , drop=FALSE]\n\t\tgm <- ncol(data)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t} else {\n\t\tmyprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n\t\tdata <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n\t\tgm <- ncol(data)\n\t\tmymapping <- c(\"mapped\"=gm, \"total\"=gt)\n\t}\n\t## rename gene names by the gene symbols\n\tcolnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n\n\tif(do.mapping) {\n    ## transform expressions so they match approximately the scale of Affymetrix data\n    data <- apply(data, 2, function(x) {\n      xx <- (x - quantile(x, probs=0.025, na.rm=TRUE)) / (quantile(x, probs=0.975, na.rm=TRUE) - quantile(x, probs=0.025, na.rm=TRUE))\n      return((xx * 8) + 6)\n    })\n    data[!is.na(data) & data < 1] <- 1\n    data[!is.na(data) & data > 15] <- 15\n  }\n\n  data <- (data - apply(data, 1, mean, na.rm=TRUE)) + log2(500)\n  ## apply transformation factor and offset\n  datat <- t(apply(data, 1, function(x, a, b) {\n    return((x - b) / a)\n  }, a=sig2[ , \"a\"], b=sig2[ , \"b\"]))\n  data <- matrix(NA, nrow=nrow(data), ncol=ncol(data), dimnames=dimnames(data))\n  data[rownames(datat), colnames(datat)] <- datat\n\n\trs <- rs.unscaled <- rsrisk <- rep(NA, nrow(data))\n  rs.unscaled <- drop((sig2[ , \"weight\"] %*% t(data)) - 2.63)\n  rs <- sapply(rs.unscaled, function(x) {\n    if(!is.na(x)) {\n      x <- 1.5 * x + 18.95\n      if(x < 0) {\n        x <- 0\n      } else {\n        if(x > 15) {\n          x <- 15\n        }\n      }\n    }\n    return(x)\n  })\n  rsrisk <- ifelse(rs >= 5, 1, 0)\n\tnames(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n\treturn(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'endoPredict' function and what are its main inputs?",
        "answer": "The 'endoPredict' function computes signature scores and risk classifications for the endoPredict signature in breast cancer. Its main inputs are 'data' (a matrix of gene expressions), 'annot' (a matrix of annotations), 'do.mapping' (a boolean for gene mapping), 'mapping' (an optional matrix for forced mapping), and 'verbose' (a boolean for printing messages)."
      },
      {
        "question": "How does the function handle gene mapping, and what happens if genes of interest are missing?",
        "answer": "If 'do.mapping' is TRUE, the function maps genes using Entrez Gene IDs. It removes missing and duplicated gene IDs from the gene list, then performs mapping using the 'geneid.map' function. If genes of interest are missing after mapping, the function returns NA values for scores and risks, along with mapping information."
      },
      {
        "question": "How is the final risk classification determined in the 'endoPredict' function?",
        "answer": "The function calculates an unscaled risk score (rs.unscaled) using gene weights and transformed expression data. This score is then scaled and bounded between 0 and 15. The final risk classification (rsrisk) is determined by comparing the scaled score (rs) to a threshold of 5. Scores >= 5 are classified as high risk (1), while scores < 5 are classified as low risk (0)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    # Complete the mapping process and data transformation\n  } else {\n    # Handle non-mapping case\n  }\n  \n  # Complete the function with score calculation and risk assessment\n}",
        "complete": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(!all(is.element(sig2[sig2[ , \"group\"] == \"GOI\", \"EntrezGene.ID\"], rr$geneid1))) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    colnames(data) <- names(gid2) <- names(gid1)\n    sig2 <- sig2[colnames(data), , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  } else {\n    myprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n    data <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  }\n  \n  colnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n  \n  if(do.mapping) {\n    data <- apply(data, 2, function(x) {\n      xx <- (x - quantile(x, probs=0.025, na.rm=TRUE)) / (quantile(x, probs=0.975, na.rm=TRUE) - quantile(x, probs=0.025, na.rm=TRUE))\n      return((xx * 8) + 6)\n    })\n    data[!is.na(data) & data < 1] <- 1\n    data[!is.na(data) & data > 15] <- 15\n  }\n  \n  data <- (data - apply(data, 1, mean, na.rm=TRUE)) + log2(500)\n  datat <- t(apply(data, 1, function(x, a, b) { return((x - b) / a) }, a=sig2[ , \"a\"], b=sig2[ , \"b\"]))\n  data <- matrix(NA, nrow=nrow(data), ncol=ncol(data), dimnames=dimnames(data))\n  data[rownames(datat), colnames(datat)] <- datat\n  \n  rs <- rs.unscaled <- rsrisk <- rep(NA, nrow(data))\n  rs.unscaled <- drop((sig2[ , \"weight\"] %*% t(data)) - 2.63)\n  rs <- sapply(rs.unscaled, function(x) {\n    if(!is.na(x)) {\n      x <- 1.5 * x + 18.95\n      if(x < 0) { x <- 0 } else if(x > 15) { x <- 15 }\n    }\n    return(x)\n  })\n  rsrisk <- ifelse(rs >= 5, 1, 0)\n  names(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      },
      {
        "partial": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    # Implement mapping logic\n  } else {\n    myprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n    data <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  }\n  \n  colnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n  \n  # Implement data transformation and score calculation\n}",
        "complete": "endoPredict <- function(data, annot, do.mapping=FALSE, mapping, verbose=FALSE) {\n  sig2 <- sig.endoPredict[sig.endoPredict[ , \"group\"] != \"REFERENCE\", , drop=FALSE]\n  rownames(sig2) <- sig2[ , \"probe.affy\"]\n  gt <- nrow(sig2)\n  \n  if(do.mapping) {\n    gid1 <- as.numeric(as.character(sig2[ ,\"EntrezGene.ID\"]))\n    names(gid1) <- dimnames(sig2)[[1]]\n    gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    gm <- length(rr$geneid2)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n    if(!all(is.element(sig2[sig2[ , \"group\"] == \"GOI\", \"EntrezGene.ID\"], rr$geneid1))) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      if(verbose) { message(sprintf(\"probe candidates: %i/%i\", gm, gt)) }\n      return(list(\"score\"=res, \"risk\"=res, \"mapping\"=mymapping, \"probe\"=NA))\n    }\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n    colnames(data) <- names(gid2) <- names(gid1)\n    sig2 <- sig2[colnames(data), , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  } else {\n    myprobe <- NA\n    nn <- intersect(dimnames(sig2)[[1]], dimnames(data)[[2]])\n    data <- data[ , nn]\n    sig2 <- sig2[nn, , drop=FALSE]\n    gm <- ncol(data)\n    mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n  }\n  \n  colnames(data) <- rownames(sig2) <- sig2[ , \"symbol\"]\n  \n  if(do.mapping) {\n    data <- apply(data, 2, function(x) {\n      xx <- (x - quantile(x, probs=0.025, na.rm=TRUE)) / (quantile(x, probs=0.975, na.rm=TRUE) - quantile(x, probs=0.025, na.rm=TRUE))\n      return((xx * 8) + 6)\n    })\n    data[!is.na(data) & data < 1] <- 1\n    data[!is.na(data) & data > 15] <- 15\n  }\n  \n  data <- (data - apply(data, 1, mean, na.rm=TRUE)) + log2(500)\n  datat <- t(apply(data, 1, function(x, a, b) { return((x - b) / a) }, a=sig2[ , \"a\"], b=sig2[ , \"b\"]))\n  data <- matrix(NA, nrow=nrow(data), ncol=ncol(data), dimnames=dimnames(data))\n  data[rownames(datat), colnames(datat)] <- datat\n  \n  rs <- rs.unscaled <- rsrisk <- rep(NA, nrow(data))\n  rs.unscaled <- drop((sig2[ , \"weight\"] %*% t(data)) - 2.63)\n  rs <- sapply(rs.unscaled, function(x) {\n    if(!is.na(x)) {\n      x <- 1.5 * x + 18.95\n      if(x < 0) { x <- 0 } else if(x > 15) { x <- 15 }\n    }\n    return(x)\n  })\n  rsrisk <- ifelse(rs >= 5, 1, 0)\n  names(rs) <- names(rs.unscaled) <- names(rsrisk) <- dimnames(data)[[1]]\n  \n  return(list(\"score\"=rs, \"risk\"=rsrisk, \"mapping\"=mymapping, \"probe\"=myprobe))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/rename.duplicate.R",
    "language": "R",
    "content": "#' @title Function to rename duplicated strings\n#'\n#' @description\n#' This function renames duplicated strings by adding their number of \n#'   occurrences at the end.\n#'\n#' @usage\n#' rename.duplicate(x, sep = \"_\", verbose = FALSE)\n#'\n#' @param x\tvector of strings.\n#' @param sep\ta character to be the separator between the number added at \n#'   the end and the string itself.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - new.x:\tnew strings (without duplicates).\n#' - duplicated.x: strings which were originally duplicated.\n#'\n#' @examples\n#' nn <- sample(letters[1:10], 30, replace=TRUE)\n#' table(nn)\n#' rename.duplicate(x=nn, verbose=TRUE)\n#'\n#' @md\n#' @export\nrename.duplicate <-\nfunction (x, sep=\"_\", verbose=FALSE) {\n\n\tx <- as.character(x)\n\tduplix <- duplicated(x)\n\tduplin <- x[duplix]\n\n\tix <- numeric(length=length(unique(duplin)))\n\tnames(ix) <- unique(duplin)\n\tretval <- numeric(length=length(duplin))\n\tfor(i in 1:length(duplin)) { retval[i] <- ix[duplin[i]] <- ix[duplin[i]] + 1 }\n\tretval <- retval + 1\n\tx[duplix] <- paste(duplin, retval, sep=sep)\n\n\tif (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\t\n\treturn (list(new.x=x, duplicated.x=duplin))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rename.duplicate` function, and what are its main parameters?",
        "answer": "The `rename.duplicate` function renames duplicated strings in a vector by adding their number of occurrences at the end. Its main parameters are:\n- `x`: the input vector of strings\n- `sep`: the separator character between the original string and the added number (default is '_')\n- `verbose`: a boolean flag to print informative messages (default is FALSE)"
      },
      {
        "question": "How does the function handle the renaming of duplicates, and what does it return?",
        "answer": "The function renames duplicates by appending a number to the end of each duplicated string, separated by the specified separator. It returns a list with two items:\n1. `new.x`: the new vector of strings with duplicates renamed\n2. `duplicated.x`: the original duplicated strings"
      },
      {
        "question": "Explain the purpose and functionality of the nested loop in the `rename.duplicate` function.",
        "answer": "The nested loop in the function serves to count the occurrences of each duplicated string and assign the appropriate number to append. Here's how it works:\n1. It initializes `ix` as a named numeric vector to keep track of counts for each unique duplicated string.\n2. It iterates through the duplicated strings, incrementing the count for each occurrence in `ix`.\n3. It stores these counts in `retval`, which is then used to create the new names by adding 1 (to start counting from 2) and pasting with the original string."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  ix <- numeric(length=length(unique(duplin)))\n  names(ix) <- unique(duplin)\n  retval <- numeric(length=length(duplin))\n  for(i in 1:length(duplin)) {\n    # Complete the loop body\n  }\n  retval <- retval + 1\n  x[duplix] <- paste(duplin, retval, sep=sep)\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}",
        "complete": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  ix <- numeric(length=length(unique(duplin)))\n  names(ix) <- unique(duplin)\n  retval <- numeric(length=length(duplin))\n  for(i in 1:length(duplin)) {\n    retval[i] <- ix[duplin[i]] <- ix[duplin[i]] + 1\n  }\n  retval <- retval + 1\n  x[duplix] <- paste(duplin, retval, sep=sep)\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}"
      },
      {
        "partial": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  # Complete the code to handle duplicates\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}",
        "complete": "rename.duplicate <- function(x, sep=\"_\", verbose=FALSE) {\n  x <- as.character(x)\n  duplix <- duplicated(x)\n  duplin <- x[duplix]\n\n  ix <- numeric(length=length(unique(duplin)))\n  names(ix) <- unique(duplin)\n  retval <- numeric(length=length(duplin))\n  for(i in 1:length(duplin)) {\n    retval[i] <- ix[duplin[i]] <- ix[duplin[i]] + 1\n  }\n  retval <- retval + 1\n  x[duplix] <- paste(duplin, retval, sep=sep)\n\n  if (verbose) { message(sprintf(\"%i duplicated names\", length(duplin))) }\n\n  return (list(new.x=x, duplicated.x=duplin))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/test.hetero.test.R",
    "language": "R",
    "content": "'test.hetero.test' <-\nfunction(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'test.hetero.test' function in R?",
        "answer": "The 'test.hetero.test' function performs a heterogeneity test on a set of p-values. It calculates a Q statistic and its corresponding p-value using a chi-square distribution. This test is often used in meta-analysis to assess whether the effect sizes across different studies are homogeneous or heterogeneous."
      },
      {
        "question": "How does the function handle missing values in the input vector 'p'?",
        "answer": "The function handles missing values in two ways: 1) If 'na.rm' is set to FALSE (default) and there are missing values, it stops execution with an error message. 2) If 'na.rm' is TRUE, it removes the missing values from both 'p' and 'weight' vectors using the '!is.na(p)' condition before performing calculations."
      },
      {
        "question": "What statistical transformation is applied to the p-values, and why?",
        "answer": "The p-values are transformed using the inverse normal cumulative distribution function (qnorm) with 'lower.tail=FALSE'. This transformation converts the p-values to z-scores. The resulting z-scores are then used to calculate the Q statistic, which follows a chi-square distribution under the null hypothesis of homogeneity."
      }
    ],
    "completion_tasks": [
      {
        "partial": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\t# Calculate Q and qpv here\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}",
        "complete": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}"
      },
      {
        "partial": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\t# Handle missing values and calculate z-scores here\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}",
        "complete": "function(p, weight, na.rm=FALSE) {\n\tk <- length(p);\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tcc.ix <- !is.na(p);\n\tif(!all(cc.ix) && !na.rm) { stop(\"missing values are present!\"); }\n\tp <- p[cc.ix];\n\tweight <- weight[cc.ix];\n\tz <- qnorm(p, lower.tail=FALSE);\n\tQ <- sum(weight * (z - mean(z))^2)\n\tqpv <- pchisq(Q, df=k-1, lower.tail=FALSE);\n\treturn(list(\"Q\"=Q, \"p.value\"=qpv));\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/map.datasets.R",
    "language": "R",
    "content": "#' @title Function to map a list of datasets through EntrezGene IDs in order to \n#'   get the union of the genes\n#'\n#' @description\n#' This function maps a list of datasets through EntrezGene IDs in order to get \n#'   the union of the genes.\n#'\n#' @usage\n#' map.datasets(datas, annots, do.mapping = FALSE, \n#'   mapping.coln = \"EntrezGene.ID\", mapping, verbose = FALSE)\n#'\n#' @param datas\tList of matrices of gene expressions with samples in rows and \n#'   probes in columns, dimnames being properly defined.\n#' @param annots\tList of matrices of annotations with at least one column named \n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping\tTRUE if the mapping through Entrez Gene ids must be \n#'   performed (in case of ambiguities, the most variant probe is kept for each \n#'   gene), FALSE otherwise.\n#' @param mapping.coln\tName of the column containing the biological annotation \n#'   to be used to map the different datasets, default is \"EntrezGene.ID\".\n#' @param mapping\tMatrix with columns \"EntrezGene.ID\" and \"probe.x\" used to \n#'   force the mapping such that the probes of platform x are not selected based on \n#'   their variance.\n#' @param verbose\tTRUE to print informative messages, FALSE otherwise.\n#'\n#' @details\n#' In case of several probes representing the same EntrezGene ID, the most \n#'   variant is selected if mapping is not specified. When a EntrezGene ID does not \n#'   exist in a specific dataset, NA values are introduced.\n#'\n#' @return\n#' A list with items:\n#' - datas: List of datasets (gene expression matrices)\n#' - annots: List of annotations (annotation matrices)\n#'\n#' @examples\n#' # load VDX dataset\n#' data(vdxs)\n#' # load NKI dataset\n#' data(nkis)\n#' # reduce datasets\n#' ginter <- intersect(annot.vdxs[ ,\"EntrezGene.ID\"], annot.nkis[ ,\"EntrezGene.ID\"])\n#' ginter <- ginter[!is.na(ginter)][1:30]\n#' myx <- unique(c(match(ginter, annot.vdxs[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.vdxs), size=20)))\n#' data2.vdxs <- data.vdxs[ ,myx]\n#' annot2.vdxs <- annot.vdxs[myx, ]\n#' myx <- unique(c(match(ginter, annot.nkis[ ,\"EntrezGene.ID\"]),\n#'   sample(x=1:nrow(annot.nkis), size=20)))\n#' data2.nkis <- data.nkis[ ,myx]\n#' annot2.nkis <- annot.nkis[myx, ]\n#' # mapping of datasets\n#' datas <- list(\"VDX\"=data2.vdxs,\"NKI\"=data2.nkis)\n#' annots <- list(\"VDX\"=annot2.vdxs, \"NKI\"=annot2.nkis)\n#' datas.mapped <- map.datasets(datas=datas, annots=annots, do.mapping=TRUE)\n#' str(datas.mapped, max.level=2)\n#'\n#' @md\n#' @export\nmap.datasets <-\nfunction(datas, annots, do.mapping=FALSE, mapping.coln=\"EntrezGene.ID\", mapping, verbose=FALSE) {\n\tif((length(datas) != length(annots)) || !all(names(datas) == names(annots))) { stop(\"discordance between lists of datasets and annotations!\") }\n\t## do the mapping (or not) and collect the set of unique features\n\tdatas2 <- annots2 <- comid <- NULL\n\tfor(k in 1:length(datas)) {\n\t\tif(verbose) { message(sprintf(\"%s\", names(datas)[k])) }\n\t\tif(do.mapping) {\n\t\t\tgid <- as.character(annots[[k]][ ,mapping.coln])\n\t\t\tnames(gid) <- dimnames(annots[[k]])[[1]]\n\t\t\tugid <- unique(gid)\n\t\t\tugid <- ugid[!is.na(ugid)]\n\t\t\tnames(ugid) <- paste(\"geneid\", ugid, sep=\".\")\n\t\t\trr <- geneid.map(geneid1=gid, data1=datas[[k]], geneid2=ugid, verbose=FALSE)\n\t\t\ttt <- rr$data1\n\t\t\t## update gene ids since only missing values may be present for some of them\n\t\t\tugid <- rr$geneid2\n\t\t\tdimnames(tt)[[2]] <- names(ugid)\n\t\t\tdatas2 <- c(datas2, list(tt))\n\t\t\ttt <- annots[[k]][names(rr$geneid1), , drop=FALSE]\n\t\t\tdimnames(tt)[[1]] <- names(ugid)\n\t\t\tannots2 <- c(annots2, list(tt))\n\t\t\tcomid <- unique(c(comid, names(ugid)))\n\t\t\trm(rr)\n\t\t\tgc()\n\t\t} else {\n\t\t\tdatas2 <- c(datas2, list(datas[[k]]))\n\t\t\tannots2 <- c(annots2, list(annots[[k]]))\n\t\t\tcomid <- unique(c(comid, dimnames(datas[[k]])[[2]]))\n\t\t}\n\t}\n\tnames(datas2) <- names(annots2) <- names(datas)\n\t#comid <- sort(comid)\n\t## put NA values for missing features\n\tfor(k in 1:length(datas)) {\n\t\ttt <- matrix(NA, nrow=nrow(datas2[[k]]), ncol=length(comid), dimnames=list(dimnames(datas2[[k]])[[1]], comid))\n\t\ttt[dimnames(datas2[[k]])[[1]], dimnames(datas2[[k]])[[2]]] <- datas2[[k]]\n\t\tdatas2[[k]] <- tt\n\t\ttt <- rbind(annots2[[k]], matrix(NA, nrow=length(comid) - nrow(annots2[[k]]), ncol=ncol(annots2[[k]]), dimnames=list(comid[!is.element(comid, dimnames(annots2[[k]])[[1]])], dimnames(annots2[[k]])[[2]])))\n\t\ttt <- tt[comid, , drop=FALSE]\n\t\tannots2[[k]] <- tt\n\t}\n\treturn(list(\"datas\"=datas2, \"annots\"=annots2))\n}",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `map.datasets` function?",
        "answer": "The main purpose of the `map.datasets` function is to map a list of datasets through EntrezGene IDs to get the union of genes across multiple datasets. It handles cases where different datasets might use different probes or identifiers for the same genes, allowing for standardization and comparison across datasets."
      },
      {
        "question": "How does the function handle cases where multiple probes represent the same EntrezGene ID?",
        "answer": "When multiple probes represent the same EntrezGene ID and the `do.mapping` parameter is set to TRUE, the function selects the most variant probe for each gene. This is done using the `geneid.map` function, which is called within the main loop of `map.datasets`. However, if a `mapping` parameter is provided, it can be used to force specific probe selections instead of using variance-based selection."
      },
      {
        "question": "What does the function return, and how does it handle missing data across datasets?",
        "answer": "The function returns a list with two items: 'datas' (a list of gene expression matrices) and 'annots' (a list of annotation matrices). For genes that exist in some datasets but not others, the function introduces NA values in the datasets where the gene is missing. This ensures that all returned datasets have the same set of genes (union of all datasets), allowing for easier comparison and analysis across different studies or platforms."
      }
    ],
    "completion_tasks": [
      {
        "partial": "map.datasets <- function(datas, annots, do.mapping=FALSE, mapping.coln=\"EntrezGene.ID\", mapping, verbose=FALSE) {\n  if((length(datas) != length(annots)) || !all(names(datas) == names(annots))) { stop(\"discordance between lists of datasets and annotations!\") }\n  datas2 <- annots2 <- comid <- NULL\n  for(k in 1:length(datas)) {\n    if(verbose) { message(sprintf(\"%s\", names(datas)[k])) }\n    if(do.mapping) {\n      gid <- as.character(annots[[k]][ ,mapping.coln])\n      names(gid) <- dimnames(annots[[k]])[[1]]\n      ugid <- unique(gid)\n      ugid <- ugid[!is.na(ugid)]\n      names(ugid) <- paste(\"geneid\", ugid, sep=\".\")\n      # Complete the mapping process here\n    } else {\n      datas2 <- c(datas2, list(datas[[k]]))\n      annots2 <- c(annots2, list(annots[[k]]))\n      comid <- unique(c(comid, dimnames(datas[[k]])[[2]]))\n    }\n  }\n  # Complete the function here\n}",
        "complete": "map.datasets <- function(datas, annots, do.mapping=FALSE, mapping.coln=\"EntrezGene.ID\", mapping, verbose=FALSE) {\n  if((length(datas) != length(annots)) || !all(names(datas) == names(annots))) { stop(\"discordance between lists of datasets and annotations!\") }\n  datas2 <- annots2 <- comid <- NULL\n  for(k in 1:length(datas)) {\n    if(verbose) { message(sprintf(\"%s\", names(datas)[k])) }\n    if(do.mapping) {\n      gid <- as.character(annots[[k]][ ,mapping.coln])\n      names(gid) <- dimnames(annots[[k]])[[1]]\n      ugid <- unique(gid)\n      ugid <- ugid[!is.na(ugid)]\n      names(ugid) <- paste(\"geneid\", ugid, sep=\".\")\n      rr <- geneid.map(geneid1=gid, data1=datas[[k]], geneid2=ugid, verbose=FALSE)\n      tt <- rr$data1\n      ugid <- rr$geneid2\n      dimnames(tt)[[2]] <- names(ugid)\n      datas2 <- c(datas2, list(tt))\n      tt <- annots[[k]][names(rr$geneid1), , drop=FALSE]\n      dimnames(tt)[[1]] <- names(ugid)\n      annots2 <- c(annots2, list(tt))\n      comid <- unique(c(comid, names(ugid)))\n      rm(rr)\n      gc()\n    } else {\n      datas2 <- c(datas2, list(datas[[k]]))\n      annots2 <- c(annots2, list(annots[[k]]))\n      comid <- unique(c(comid, dimnames(datas[[k]])[[2]]))\n    }\n  }\n  names(datas2) <- names(annots2) <- names(datas)\n  for(k in 1:length(datas)) {\n    tt <- matrix(NA, nrow=nrow(datas2[[k]]), ncol=length(comid), dimnames=list(dimnames(datas2[[k]])[[1]], comid))\n    tt[dimnames(datas2[[k]])[[1]], dimnames(datas2[[k]])[[2]]] <- datas2[[k]]\n    datas2[[k]] <- tt\n    tt <- rbind(annots2[[k]], matrix(NA, nrow=length(comid) - nrow(annots2[[k]]), ncol=ncol(annots2[[k]]), dimnames=list(comid[!is.element(comid, dimnames(annots2[[k]])[[1]])], dimnames(annots2[[k]])[[2]])))\n    tt <- tt[comid, , drop=FALSE]\n    annots2[[k]] <- tt\n  }\n  return(list(\"datas\"=datas2, \"annots\"=annots2))\n}"
      },
      {
        "partial": "geneid.map <- function(geneid1, data1, geneid2, verbose=FALSE) {\n  # Implement the geneid.map function here\n  # This function should map gene IDs and handle data accordingly\n  # Return a list with mapped data and gene IDs\n}",
        "complete": "geneid.map <- function(geneid1, data1, geneid2, verbose=FALSE) {\n  if(verbose) { message(\"mapping gene ids...\") }\n  ii <- match(geneid2, geneid1)\n  if(any(is.na(ii))) {\n    geneid2 <- geneid2[!is.na(ii)]\n    ii <- ii[!is.na(ii)]\n  }\n  data2 <- data1[, ii, drop=FALSE]\n  dimnames(data2)[[2]] <- names(geneid2)\n  if(verbose) { message(sprintf(\"\\t%d/%d features\", ncol(data2), length(geneid1))) }\n  return(list(data1=data2, geneid1=geneid1[ii], geneid2=geneid2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/molecular.subtyping.R",
    "language": "R",
    "content": "if(getRversion() >= \"2.15.1\")\n    utils::globalVariables(c(\"scmgene.robust\",\"scmod2.robust\",\"pam50.robust\",\n                             \"ssp2006.robust\",\"ssp2003.robust\",\"claudinLowData\"))\n\n#' @title Function to identify breast cancer molecular subtypes using\n#'   the Subtype Clustering Model\n#'\n#' @description\n#' This function identifies the breast cancer molecular subtypes using a Subtype\n#'   Clustering Model fitted by subtype.cluster.\n#'\n#' @usage\n#' molecular.subtyping(sbt.model = c(\"scmgene\", \"scmod1\", \"scmod2\",\n#'   \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"),\n#'   data, annot, do.mapping = FALSE, verbose = FALSE)\n#'\n#' @param sbt.model\tSubtyping classification model, can be either \"scmgene\", \"scmod1\",\n#'   \"scmod2\", \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\", or \"claudinLow\".\n#' @param data Matrix of gene expressions with samples in rows and probes in columns,\n#'   dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \"EntrezGene.ID\"\n#'   (for ssp, scm, AIMS, and claudinLow models) or \"Gene.Symbol\" (for the intClust\n#'   model), dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be performed\n#'   (in case of ambiguities, the most variant probe is kept for each gene), FALSE otherwise.\n#' @param verbose TRUE if informative messages should be displayed, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - subtype: Subtypes identified by the subtyping classification model.\n#' - subtype.proba: Probabilities to belong to each subtype estimated by the\n#'   subtyping classification model.\n#' - subtype.crisp: Crisp classes identified by the subtyping classification model.\n#'\n#' @references\n#' T. Sorlie and R. Tibshirani and J. Parker and T. Hastie and J. S. Marron and A.\n#'   Nobel and S. Deng and H. Johnsen and R. Pesich and S. Geister and J. Demeter and\n#'   C. Perou and P. E. Lonning and P. O. Brown and A. L. Borresen-Dale and D. Botstein\n#'   (2003) \"Repeated Observation of Breast Tumor Subtypes in Independent Gene\n#'   Expression Data Sets\", Proceedings of the National Academy of Sciences,\n#'   1(14):8418-8423\n#' Hu, Zhiyuan and Fan, Cheng and Oh, Daniel and Marron, JS and He, Xiaping and\n#'   Qaqish, Bahjat and Livasy, Chad and Carey, Lisa and Reynolds, Evangeline and\n#'   Dressler, Lynn and Nobel, Andrew and Parker, Joel and Ewend, Matthew and Sawyer,\n#'   Lynda and Wu, Junyuan and Liu, Yudong and Nanda, Rita and Tretiakova, Maria and\n#'   Orrico, Alejandra and Dreher, Donna and Palazzo, Juan and Perreard, Laurent and\n#'   Nelson, Edward and Mone, Mary and Hansen, Heidi and Mullins, Michael and\n#'   Quackenbush, John and Ellis, Matthew and Olopade, Olufunmilayo and Bernard,\n#'   Philip and Perou, Charles (2006) \"The molecular portraits of breast tumors are\n#'   conserved across microarray platforms\", BMC Genomics, 7(96)\n#' Parker, Joel S. and Mullins, Michael and Cheang, Maggie C.U. and Leung, Samuel and\n#'   Voduc, David and Vickery, Tammi and Davies, Sherri and Fauron, Christiane and He,\n#'   Xiaping and Hu, Zhiyuan and Quackenbush, John F. and Stijleman, Inge J. and Palazzo,\n#'   Juan and Marron, J.S. and Nobel, Andrew B. and Mardis, Elaine and Nielsen, Torsten O.\n#'   and Ellis, Matthew J. and Perou, Charles M. and Bernard, Philip S. (2009)\n#'   \"Supervised Risk Predictor of Breast Cancer Based on Intrinsic Subtypes\",\n#'   Journal of Clinical Oncology, 27(8):1160-1167\n#' Desmedt C, Haibe-Kains B, Wirapati P, Buyse M, Larsimont D, Bontempi G, Delorenzi\n#'   M, Piccart M, and Sotiriou C (2008) \"Biological processes associated with breast\n#'   cancer clinical outcome depend on the molecular subtypes\", Clinical Cancer\n#'   Research, 14(16):5158-5165.\n#' Wirapati P, Sotiriou C, Kunkel S, Farmer P, Pradervand S, Haibe-Kains B, Desmedt\n#'   C, Ignatiadis M, Sengstag T, Schutz F, Goldstein DR, Piccart MJ and Delorenzi M\n#'   (2008) \"Meta-analysis of Gene-Expression Profiles in Breast Cancer: Toward a\n#'   Unified Understanding of Breast Cancer Sub-typing and Prognosis Signatures\",\n#'   Breast Cancer Research, 10(4):R65.\n#' Haibe-Kains B, Desmedt C, Loi S, Culhane AC, Bontempi G, Quackenbush J, Sotiriou\n#'   C. (2012) \"A three-gene model to robustly identify breast cancer molecular\n#'   subtypes.\", J Natl Cancer Inst., 104(4):311-325.\n#' Curtis C, Shah SP, Chin SF, Turashvili G, Rueda OM, Dunning MJ, Speed D, Lynch AG,\n#'   Samarajiwa S, Yuan Y, Graf S, Ha G, Haffari G, Bashashati A, Russell R, McKinney\n#'   S; METABRIC Group, Langerod A, Green A, Provenzano E, Wishart G, Pinder S, Watson\n#'   P, Markowetz F, Murphy L, Ellis I, Purushotham A, Borresen-Dale AL, Brenton JD,\n#'   Tavare S, Caldas C, Aparicio S. (2012) \"The genomic and transcriptomic\n#'   architecture of 2,000 breast tumours reveals novel subgroups.\", Nature,\n#'   486(7403):346-352.\n#' \n#' Paquet ER, Hallett MT. (2015) \"Absolute assignment of breast cancer intrinsic\n#'   molecular subtype.\", J Natl Cancer Inst., 107(1):357.\n#' \n#' Aleix Prat, Joel S Parker, Olga Karginova, Cheng Fan, Chad Livasy, Jason I\n#'   Herschkowitz, Xiaping He, and Charles M. Perou (2010) \"Phenotypic and molecular\n#'   characterization of the claudin-low intrinsic subtype of breast cancer\", Breast\n#'   Cancer Research, 12(5):R68\n#'\n#' @seealso\n#' [genefu::subtype.cluster.predict], [genefu::intrinsic.cluster.predict]\n#'\n#' @examples\n#' ##### without mapping (affy hgu133a or plus2 only)\n#' # load VDX data\n#' data(vdxs)\n#' data(AIMSmodel)\n#' data(scmgene.robust)\n#'\n#' # Subtype Clustering Model fitted on EXPO and applied on VDX\n#' sbt.vdx.SCMGENE <- molecular.subtyping(sbt.model=\"scmgene\",\n#'   data=data.vdxs, annot=annot.vdxs, do.mapping=FALSE)\n#' table(sbt.vdx.SCMGENE$subtype)\n#'\n#' # Using the AIMS molecular subtyping algorithm\n#' sbt.vdxs.AIMS <- molecular.subtyping(sbt.model=\"AIMS\", data=data.vdxs,\n#'                                      annot=annot.vdxs, do.mapping=FALSE)\n#' table(sbt.vdxs.AIMS$subtype)\n#'\n#' # Using the IntClust molecular subtyping algorithm\n#' colnames(annot.vdxs)[3]<-\"Gene.Symbol\"\n#' sbt.vdxs.intClust <- molecular.subtyping(sbt.model=\"intClust\", data=data.vdxs,\n#'   annot=annot.vdxs, do.mapping=FALSE)\n#' table(sbt.vdxs.intClust$subtype)\n#'\n#' ##### with mapping\n#' # load NKI data\n#' data(nkis)\n#'\n#' # Subtype Clustering Model fitted on EXPO and applied on NKI\n#' sbt.nkis <- molecular.subtyping(sbt.model=\"scmgene\", data=data.nkis,\n#'   annot=annot.nkis, do.mapping=TRUE)\n#' table(sbt.nkis$subtype)\n#'\n#' ##### with mapping\n#' ## load vdxs data\n#' data(vdxs)\n#' data(claudinLowData)\n#'\n#' ## Claudin-Low classification of 150 VDXS samples\n#' sbt.vdxs.CL <- molecular.subtyping(sbt.model=\"claudinLow\", data=data.vdxs,\n#'   annot=annot.vdxs, do.mapping=TRUE)\n#' table(sbt.vdxs.CL$subtype)\n#'\n#' @md\n#' @export\nmolecular.subtyping <- function(sbt.model=c(\"scmgene\", \"scmod1\", \"scmod2\",\n  \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"), data, annot,\n  do.mapping=FALSE, verbose=FALSE)\n{\n\n  sbt.model <- match.arg(sbt.model)\n\n  ## convert SCM to SSP nomenclature\n  sbt.conv <- rbind(c(\"ER-/HER2-\", \"Basal\"),\n    c(\"HER2+\", \"Her2\"),\n    c(\"ER+/HER2- High Prolif\", \"LumB\"),\n    c(\"ER+/HER2- Low Prolif\", \"LumA\")\n  )\n  colnames(sbt.conv) <- c(\"SCM.nomenclature\", \"SSP.nomenclature\")\n\n  sbtn.ssp <- c(\"Basal\", \"Her2\", \"LumB\", \"LumA\", \"Normal\")\n  sbtn2.ssp <- c(\"Basal\", \"Her2\", \"Lums\", \"LumB\", \"LumA\", \"Normal\")\n\n  ## SCM family\n  if (sbt.model %in% c(\"scmgene\", \"scmod1\", \"scmod2\")) {\n    switch(sbt.model,\n      \"scmgene\" = {\n        sbts <- subtype.cluster.predict(sbt.model=scmgene.robust, data=data,\n          annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n      },\n      \"scmod1\" = {\n        sbts <- subtype.cluster.predict(sbt.model=scmod1.robust, data=data,\n          annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n      },\n      \"scmod2\" = {\n        sbts <- subtype.cluster.predict(sbt.model=scmod2.robust, data=data,\n          annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n      }\n    )\n    names(sbts) <- c(\"subtype\", \"subtype.proba\")\n    ## compute crisp classification\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n\n    ## reorder columns\n    #ss <- sbtn2.ssp[is.element(sbtn2.ssp, colnames(sbts$subtype.proba))]\n    #sbts$subtype.proba <- sbts$subtype.proba[ , ss, drop=FALSE]\n    #sbts$subtype.crisp <- sbts$subtype.crisp[ , ss, drop=FALSE]\n\n    ## set the proper names\n    names(sbts$subtype) <- rownames(sbts$subtype.proba) <- rownames(sbts$subtype.crisp)<- rownames(data)\n  }\n\n  ## SSP family\n  if (sbt.model %in% c(\"ssp2003\", \"ssp2006\", \"pam50\")) {\n    switch(sbt.model,\n      \"pam50\" = {\n        sbts <- intrinsic.cluster.predict(sbt.model=pam50.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n      },\n      \"ssp2006\" = {\n        sbts <- intrinsic.cluster.predict(sbt.model=ssp2006.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n      },\n      \"ssp2003\" = {\n        sbts <- intrinsic.cluster.predict(sbt.model=ssp2003.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n      }\n    )\n    sbts$subtype <- factor(as.character(sbts$subtype), levels=sbtn.ssp)\n    ## compute crisp classification\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n\n    ## merge LumA and LumB: #sum the probability for LumA and LumB to get the probability for Luminals in general\n    #lums.proba <- apply(sbts$subtype.proba[ , c(\"LumB\", \"LumA\"), drop=FALSE], 1, sum, na.rm=TRUE)\n    #sbts$subtype.proba <- cbind(sbts$subtype.proba, \"Lums\"=lums.proba)\n    #lums.crisp <- as.numeric(is.element(sbts$subtype, c(\"LumA\", \"LumB\")))\n    #sbts$subtype.crisp <- cbind(sbts$subtype.crisp, \"Lums\"=lums.crisp)\n\n    ## reorder columns\n    #ss <- sbtn2.ssp[is.element(sbtn2.ssp, colnames(sbts$subtype.proba))]\n    #sbts$subtype.proba <- sbts$subtype.proba[ , ss, drop=FALSE]\n    #sbts$subtype.crisp <- sbts$subtype.crisp[ , ss, drop=FALSE]\n\n    ## set the proper names\n    names(sbts$subtype) <- rownames(sbts$subtype.proba) <- rownames(sbts$subtype.crisp)<- rownames(data)\n  }\n\n  ## IntClust family\n  if (sbt.model %in% c(\"intClust\")) {\n    #message(\"Note: Need a Gene.Symbol column in the annotation object\")\n    sbts<-NULL\n    myx <- !is.na(annot[ , \"Gene.Symbol\"]) & !duplicated(annot[ , \"Gene.Symbol\"])\n    dd <- t(data[ , myx, drop=FALSE])\n    rownames(dd) <- annot[myx, \"Gene.Symbol\"]\n    ## remove patients with more than 80% missing values\n    rix <- apply(dd, 2, function (x, y) { return ((sum(is.na(x) / length(x))) > y) }, y=0.8)\n    cix <- apply(dd, 2, function (x, y) { return ((sum(is.na(x) / length(x))) > y) }, y=0.8)\n    dd <- dd[!rix, !cix, drop=FALSE]\n    features <- iC10::matchFeatures(Exp=dd, Exp.by.feat=\"Gene.Symbol\")\n    features <- iC10::normalizeFeatures(features, method=\"scale\")\n    res <- iC10::iC10(features)\n    ## compute crisp classification\n    crisp <- t(apply(res$posterior, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n    sbts$subtype <- array(NA, dim=nrow(data), dimnames=list(rownames(data)))\n    sbts$subtype[!rix] <- res$class\n    sbts$subtype.proba <- array(NA, dim=c(nrow(data), ncol(res$posterior)), dimnames=list(rownames(data), colnames(res$posterior)))\n    sbts$subtype.proba[!rix, ] <- res$posterior\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function (x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return (xx)\n    }))\n    ## set the proper colnames\n    colnames(sbts$subtype.proba) <- colnames(sbts$subtype.crisp) <- paste(\"iC\", colnames(sbts$subtype.proba), sep=\"\")\n    sbts$subtype <- paste(\"iC\", sbts$subtype, sep=\"\")\n    sbts$subtype <- factor(sbts$subtype, levels=colnames(sbts$subtype.proba))\n    ## set the proper rownames\n    names(sbts$subtype) <- rownames(sbts$subtype.proba) <- rownames(sbts$subtype.crisp)<- rownames(data)\n  }\n\n  ## AIMS classifier\n  if (sbt.model %in% c(\"AIMS\")) {\n      sbts <- AIMS::applyAIMS(eset=t(data), EntrezID=annot[ , \"EntrezGene.ID\"])[c(\"cl\", \"all.probs\")]\n      sbts$subtype <- sbts$cl\n      sbts$subtype.proba <- matrix(unlist(sbts$all.probs$`20`), ncol = 5, byrow = TRUE)\n      colnames(sbts$subtype.proba) <- colnames(sbts$all.probs$`20`)\n      rownames(sbts$subtype.proba) <- rownames(sbts$subtype)\n\n      ## compute crisp classification\n      sbts$subtype.crisp <- t(\n        apply(sbts$subtype.proba, 1, function (x) {\n        xx <- array(0, dim=length(x), dimnames=list(names(x)))\n        xx[which.max(x)] <- 1\n        return (xx)\n      })\n      )\n      sbts<-sbts[- which(names(sbts) %in% c(\"cl\",\"all.probs\"))]\n  }\n\n  ## CLAUDIN-LOW classifier\n  if (sbt.model %in% c(\"claudinLow\")) {\n    train<-claudinLowData\n    train$xd<- medianCtr(train$xd)\n\n    if(do.mapping) {\n      gid1 <- as.numeric(rownames(train$xd))\n      names(gid1) <- paste(\"geneid\", rownames(train$xd), sep=\".\")\n      gid2 <- as.numeric(as.character(annot[ ,\"EntrezGene.ID\"]))\n      names(gid2) <- colnames(data)\n\n      ## remove missing and duplicated geneids from the gene list\n      rm.ix <- is.na(gid1) | duplicated(gid1)\n      gid1 <- gid1[!rm.ix]\n      rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n      gt <- length(rr$geneid2)\n      if(is.na(rr$geneid1[1])) {\n        gm <- 0\n        #no gene ids in common\n        res <- rep(NA, nrow(data))\n        names(res) <- dimnames(data)[[1]]\n        gf <- c(\"mapped\"=0, \"total\"=gt)\n        if(verbose) { message(sprintf(\"probe candidates: 0/%i\", gt)) }\n        return(list(\"score\"=res, \"risk\"=res, \"mapping\"=gf, \"probe\"=NA))\n      }\n      gid1 <- rr$geneid2\n      gid2 <- rr$geneid1\n      data <- rr$data1\n      #mymapping <- c(\"mapped\"=gm, \"total\"=gt)\n      myprobe <- cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))\n      ## change the names of probes in the data\n      dimnames(data)[[2]] <- names(gid2) <- names(gid1)\n    }\n\n    test <- medianCtr(t(data)) #probes as rows, median-centered\n    #Run Classifier Call\n\ttrain2 <- train$xd\n\trownames(train2) <- paste(\"geneid\", rownames(train2), sep=\".\")\n    predout <- claudinLow(x=train2, classes=as.matrix(train$classes$Group,ncol=1), y=test)\n    sbts <- NULL\n    sbts$subtype <- factor(as.character(predout$predictions$Call))\n    colnames(predout$centroids) <- c(\"Claudin\",\"Others\")\n\n    ## apply the nearest centroid classifier to classify the samples again\n    ncor <- t(apply(X=data, MARGIN=1, FUN=function(x, y) {\n      rr <- array(NA, dim=ncol(y), dimnames=list(colnames(y)))\n      if (sum(complete.cases(x, y)) > 3) {\n        rr <- cor(x=x, y=y, method=\"spearman\", use=\"complete.obs\")\n      }\n      return (rr)\n    }, y=predout$centroids))\n\n    #Calculate posterior probability based on the correlationss\n   # nproba <- t(apply(X=ncor, MARGIN=1, FUN=function(x) { return(abs(x) / sum(abs(x), na.rm=TRUE)) }))\n\n    # negative correlations are truncated to zero since they have no meaning for subtypes identification\n    nproba <- t(apply(X=ncor, MARGIN=1, FUN=function (x) {\n      rr <- array(NA, dim=length(x), dimnames=list(names(x)))\n      x[!is.na(x) & x < 0] <- 0\n      if (!all(is.na(x))) {\n        rr <- x / sum(x, na.rm=TRUE)\n      }\n      return (rr)\n    }))\n\n    sbts$subtype.proba<-nproba\n\n    ## compute crisp classification - in this case, really based on the binary call from the CL classifier\n    #     sbts$subtype.crisp <- t(\n    #       apply(sbts$subtype.proba, 1, function (x) {\n    #         xx <- array(0, dim=length(x), dimnames=list(names(x)))\n    #         xx[which.max(x)] <- 1\n    #         return (xx)\n    #       })\n    #     )\n    #     colnames(sbts$subtype.crisp)<-c(\"Claudin\",\"Others\")\n\n    # In this case, really based on the binary call from the CL classifier. Use that for accuracy\n      CLsubtypes<-c(\"Claudin\",\"Others\")\n      sbts$subtype.crisp <- matrix(0, nrow=nrow(predout$predictions), ncol=2,dimnames=list(rownames(predout$predictions),CLsubtypes))\n      for(count in 1:nrow(predout$predictions))\n      {\n        if(predout$predictions$Call[count]==\"Others\")\n          sbts$subtype.crisp[count,2]<-1\n        else sbts$subtype.crisp[count,1]<-1\n      }\n  }\n  return (sbts)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `molecular.subtyping` function and what are its main input parameters?",
        "answer": "The `molecular.subtyping` function is used to identify breast cancer molecular subtypes using various subtyping classification models. Its main input parameters are: `sbt.model` (the subtyping classification model to use), `data` (a matrix of gene expressions), `annot` (a matrix of annotations), `do.mapping` (boolean indicating whether to perform mapping through Entrez Gene ids), and `verbose` (boolean for displaying informative messages)."
      },
      {
        "question": "How does the function handle different subtyping models, and what are some of the available options?",
        "answer": "The function uses a switch statement to handle different subtyping models. Available options include 'scmgene', 'scmod1', 'scmod2' (SCM family), 'pam50', 'ssp2006', 'ssp2003' (SSP family), 'intClust', 'AIMS', and 'claudinLow'. Each model uses a specific prediction method and may require different data processing steps."
      },
      {
        "question": "What are the main components of the function's return value, and what do they represent?",
        "answer": "The function returns a list with three main components: 'subtype' (identified subtypes), 'subtype.proba' (probabilities of belonging to each subtype), and 'subtype.crisp' (crisp classes identified by the model). These represent the classification results, providing both the final subtype assignment and the underlying probabilities for each sample."
      }
    ],
    "completion_tasks": [
      {
        "partial": "molecular.subtyping <- function(sbt.model=c(\"scmgene\", \"scmod1\", \"scmod2\", \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"), data, annot, do.mapping=FALSE, verbose=FALSE) {\n  sbt.model <- match.arg(sbt.model)\n\n  if (sbt.model %in% c(\"scmgene\", \"scmod1\", \"scmod2\")) {\n    # SCM family implementation\n  } else if (sbt.model %in% c(\"ssp2003\", \"ssp2006\", \"pam50\")) {\n    # SSP family implementation\n  } else if (sbt.model == \"intClust\") {\n    # IntClust implementation\n  } else if (sbt.model == \"AIMS\") {\n    # AIMS implementation\n  } else if (sbt.model == \"claudinLow\") {\n    # CLAUDIN-LOW implementation\n  }\n\n  # Return results\n}",
        "complete": "molecular.subtyping <- function(sbt.model=c(\"scmgene\", \"scmod1\", \"scmod2\", \"pam50\", \"ssp2006\", \"ssp2003\", \"intClust\", \"AIMS\",\"claudinLow\"), data, annot, do.mapping=FALSE, verbose=FALSE) {\n  sbt.model <- match.arg(sbt.model)\n\n  if (sbt.model %in% c(\"scmgene\", \"scmod1\", \"scmod2\")) {\n    sbts <- switch(sbt.model,\n      \"scmgene\" = subtype.cluster.predict(sbt.model=scmgene.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")],\n      \"scmod1\" = subtype.cluster.predict(sbt.model=scmod1.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")],\n      \"scmod2\" = subtype.cluster.predict(sbt.model=scmod2.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype2\", \"subtype.proba2\")]\n    )\n    names(sbts) <- c(\"subtype\", \"subtype.proba\")\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function(x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return(xx)\n    }))\n  } else if (sbt.model %in% c(\"ssp2003\", \"ssp2006\", \"pam50\")) {\n    sbts <- switch(sbt.model,\n      \"pam50\" = intrinsic.cluster.predict(sbt.model=pam50.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")],\n      \"ssp2006\" = intrinsic.cluster.predict(sbt.model=ssp2006.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")],\n      \"ssp2003\" = intrinsic.cluster.predict(sbt.model=ssp2003.robust, data=data, annot=annot, do.mapping=do.mapping)[c(\"subtype\", \"subtype.proba\")]\n    )\n    sbts$subtype <- factor(as.character(sbts$subtype), levels=c(\"Basal\", \"Her2\", \"LumB\", \"LumA\", \"Normal\"))\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function(x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return(xx)\n    }))\n  } else if (sbt.model == \"intClust\") {\n    # IntClust implementation (omitted for brevity)\n  } else if (sbt.model == \"AIMS\") {\n    sbts <- AIMS::applyAIMS(eset=t(data), EntrezID=annot[, \"EntrezGene.ID\"])[c(\"cl\", \"all.probs\")]\n    sbts$subtype <- sbts$cl\n    sbts$subtype.proba <- matrix(unlist(sbts$all.probs$`20`), ncol = 5, byrow = TRUE)\n    colnames(sbts$subtype.proba) <- colnames(sbts$all.probs$`20`)\n    rownames(sbts$subtype.proba) <- rownames(sbts$subtype)\n    sbts$subtype.crisp <- t(apply(sbts$subtype.proba, 1, function(x) {\n      xx <- array(0, dim=length(x), dimnames=list(names(x)))\n      xx[which.max(x)] <- 1\n      return(xx)\n    }))\n    sbts <- sbts[- which(names(sbts) %in% c(\"cl\",\"all.probs\"))]\n  } else if (sbt.model == \"claudinLow\") {\n    # CLAUDIN-LOW implementation (omitted for brevity)\n  }\n\n  return(sbts)\n}"
      },
      {
        "partial": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE) {\n  # Function implementation\n}",
        "complete": "subtype.cluster.predict <- function(sbt.model, data, annot, do.mapping=FALSE) {\n  if (do.mapping) {\n    mapped_data <- map_data(data, annot)\n  } else {\n    mapped_data <- data\n  }\n  \n  subtype_probabilities <- predict(sbt.model, newdata=mapped_data)\n  subtypes <- apply(subtype_probabilities, 1, which.max)\n  subtype_names <- colnames(subtype_probabilities)[subtypes]\n  \n  return(list(\n    subtype2 = factor(subtype_names, levels=colnames(subtype_probabilities)),\n    subtype.proba2 = subtype_probabilities\n  ))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/td.sens.spec.R",
    "language": "R",
    "content": "'td.sens.spec' <-\nfunction(cl, surv.time, surv.event, time, span=0, sampling=FALSE, na.rm=FALSE, ...) {\n\t#require(survivalROC)\n\t\n\tif((length(cl) + length(surv.time) + length(surv.event)) != (3 * length(cl))) { stop(\"paramaters cl, surv.time and surv.event must have the same length!\") }\n\tif(is.null(names(cl))) { names(cl) <- names(surv.time) <- names(surv.event) <- paste(\"X\", 1:length(cl), sep=\".\") }\n\tcc.ix <- complete.cases(cl, surv.time, surv.event)\n\tif(all(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tcl2 <- cl[cc.ix]\n\tucl2 <- sort(unique(cl2))\n\tif(length(ucl2) != 2) { stop(\"cl must be binary!\") }\n\too <- order(cl2, decreasing=FALSE)\n\tcl2 <- cl2[oo]\n\tst <- surv.time[cc.ix][oo]\n\tse <- surv.event[cc.ix][oo]\n\tmarker.fake <- 1:length(cl2)\n\tnames(marker.fake) <- names(cl2)\n\tmycutoff <- sum(cl2 == ucl2[1])\n\t\n\t##using the survival.C function\n\trr <- survivalROC::survivalROC.C(Stime=st, status=se, marker=marker.fake, predict.time=time, span=span,  ...)\n\t#rr <- survivalROC::survivalROC(Stime=st, status=se, marker=marker.fake, cut.values=mycutoff, predict.time=time, span=span, lambda=lambda, ...)\n\n\tsens.se <- spec.se <- NA\n\tif(sampling) {\n\t\t#require(bootstrap)\n\t\t\n\t\ttheta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\t\t\tcl <- cl[x]\n\t\t\too <- order(cl, decreasing=FALSE)\n\t\t\tcl <- cl[oo]\n\t\t\tsurv.time <- surv.time[x][oo]\n\t\t\tsurv.event <- surv.event[x][oo]\n\t\t\tucl <- sort(unique(cl))\n\t\t\tmarker.fake <- 1:length(cl)\n\t\t\tnames(marker.fake) <- names(cl)\n\t\t\tmycutoff <- sum(cl == ucl[1])\n\t\t\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake,  predict.time=time, span=span, ...)\n\t\t\t#rr <- survivalROC::survivalROC(Stime=surv.time, status=surv.event, marker=marker.fake, cut.values=mycutoff, predict.time=time, span=span, lambda=lambda, ...)\n\t\t\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)]))\n\t\t}\n\t\tmyx <- 1:length(cl2)\n\t\tmyx2 <- myx\n\t\tsens.values <- spec.values <- rep(NA, length(cl2))\n\t\tnames(sens.values) <- names(spec.values) <- names(cl2)\n\t\tfor(i in length(myx2):1) {\n\t\t\ttt <- theta.foo1(x=myx[-myx2[i]], cl=cl2, surv.time=st, surv.event=se, time=time, ...)\n\t\t\tsens.values[is.na(sens.values) & myx >= myx2[i]] <- tt$sens\n\t\t\tspec.values[is.na(spec.values) & myx >= myx2[i]] <- tt$spec\n\t\t}\n\t\t\n\t\ttheta.foo2 <- function(x, stat=c(\"sens\", \"spec\"), xdata) {\n\t\t\tstat <- match.arg(stat)\n\t\t\txdata <- unlist(xdata[stat])\n\t\t\treturn(xdata[-x])\t\n\t\t}\n\t\t\n\t\tsens.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"sens\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\n\t\tspec.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"spec\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\t\n\t}\n\t\n\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"sens.se\"=sens.se, \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)], \"spec.se\"=spec.se))\t\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'td.sens.spec' function and what are its main input parameters?",
        "answer": "The 'td.sens.spec' function calculates sensitivity and specificity for time-dependent ROC analysis in survival data. Its main input parameters are: 'cl' (a binary classifier), 'surv.time' (survival times), 'surv.event' (event indicators), and 'time' (the time point for ROC analysis). It also has optional parameters like 'span' for smoothing, 'sampling' for bootstrap estimation, and 'na.rm' for handling missing values."
      },
      {
        "question": "How does the function handle missing values and ensure data consistency?",
        "answer": "The function handles missing values and ensures data consistency by: 1) Checking if all input vectors have the same length. 2) Using 'complete.cases()' to identify and remove rows with missing values. 3) Stopping execution if all cases are incomplete and 'na.rm' is FALSE. 4) Verifying that the classifier 'cl' is binary. These checks help maintain data integrity and prevent errors in subsequent calculations."
      },
      {
        "question": "What is the purpose of the 'sampling' parameter, and how does it affect the function's output?",
        "answer": "The 'sampling' parameter enables bootstrap estimation of standard errors for sensitivity and specificity. When set to TRUE, the function uses jackknife resampling to calculate standard errors (sens.se and spec.se). This provides additional information about the variability of the estimates. The output list includes these standard errors when sampling is enabled, allowing for more comprehensive statistical inference."
      }
    ],
    "completion_tasks": [
      {
        "partial": "function(cl, surv.time, surv.event, time, span=0, sampling=FALSE, na.rm=FALSE, ...) {\n\tif((length(cl) + length(surv.time) + length(surv.event)) != (3 * length(cl))) { stop(\"paramaters cl, surv.time and surv.event must have the same length!\") }\n\tif(is.null(names(cl))) { names(cl) <- names(surv.time) <- names(surv.event) <- paste(\"X\", 1:length(cl), sep=\".\") }\n\tcc.ix <- complete.cases(cl, surv.time, surv.event)\n\tif(all(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tcl2 <- cl[cc.ix]\n\tucl2 <- sort(unique(cl2))\n\tif(length(ucl2) != 2) { stop(\"cl must be binary!\") }\n\too <- order(cl2, decreasing=FALSE)\n\tcl2 <- cl2[oo]\n\tst <- surv.time[cc.ix][oo]\n\tse <- surv.event[cc.ix][oo]\n\tmarker.fake <- 1:length(cl2)\n\tnames(marker.fake) <- names(cl2)\n\tmycutoff <- sum(cl2 == ucl2[1])\n\n\trr <- survivalROC::survivalROC.C(Stime=st, status=se, marker=marker.fake, predict.time=time, span=span, ...)\n\n\t# Complete the function by adding code for sampling and return statement\n}",
        "complete": "'td.sens.spec' <-\nfunction(cl, surv.time, surv.event, time, span=0, sampling=FALSE, na.rm=FALSE, ...) {\n\tif((length(cl) + length(surv.time) + length(surv.event)) != (3 * length(cl))) { stop(\"paramaters cl, surv.time and surv.event must have the same length!\") }\n\tif(is.null(names(cl))) { names(cl) <- names(surv.time) <- names(surv.event) <- paste(\"X\", 1:length(cl), sep=\".\") }\n\tcc.ix <- complete.cases(cl, surv.time, surv.event)\n\tif(all(!cc.ix) & !na.rm) { stop(\"missing values are present!\") }\n\tcl2 <- cl[cc.ix]\n\tucl2 <- sort(unique(cl2))\n\tif(length(ucl2) != 2) { stop(\"cl must be binary!\") }\n\too <- order(cl2, decreasing=FALSE)\n\tcl2 <- cl2[oo]\n\tst <- surv.time[cc.ix][oo]\n\tse <- surv.event[cc.ix][oo]\n\tmarker.fake <- 1:length(cl2)\n\tnames(marker.fake) <- names(cl2)\n\tmycutoff <- sum(cl2 == ucl2[1])\n\n\trr <- survivalROC::survivalROC.C(Stime=st, status=se, marker=marker.fake, predict.time=time, span=span, ...)\n\n\tsens.se <- spec.se <- NA\n\tif(sampling) {\n\t\ttheta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\t\t\tcl <- cl[x]\n\t\t\too <- order(cl, decreasing=FALSE)\n\t\t\tcl <- cl[oo]\n\t\t\tsurv.time <- surv.time[x][oo]\n\t\t\tsurv.event <- surv.event[x][oo]\n\t\t\tucl <- sort(unique(cl))\n\t\t\tmarker.fake <- 1:length(cl)\n\t\t\tnames(marker.fake) <- names(cl)\n\t\t\tmycutoff <- sum(cl == ucl[1])\n\t\t\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake, predict.time=time, span=span, ...)\n\t\t\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)]))\n\t\t}\n\t\tmyx <- 1:length(cl2)\n\t\tmyx2 <- myx\n\t\tsens.values <- spec.values <- rep(NA, length(cl2))\n\t\tnames(sens.values) <- names(spec.values) <- names(cl2)\n\t\tfor(i in length(myx2):1) {\n\t\t\ttt <- theta.foo1(x=myx[-myx2[i]], cl=cl2, surv.time=st, surv.event=se, time=time, ...)\n\t\t\tsens.values[is.na(sens.values) & myx >= myx2[i]] <- tt$sens\n\t\t\tspec.values[is.na(spec.values) & myx >= myx2[i]] <- tt$spec\n\t\t}\n\t\t\n\t\ttheta.foo2 <- function(x, stat=c(\"sens\", \"spec\"), xdata) {\n\t\t\tstat <- match.arg(stat)\n\t\t\txdata <- unlist(xdata[stat])\n\t\t\treturn(xdata[-x])\t\n\t\t}\n\t\t\n\t\tsens.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"sens\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\n\t\tspec.se <- bootstrap::jackknife(x=1:length(cl2), theta=theta.foo2, stat=\"spec\", xdata=list(\"sens\"=sens.values, \"spec\"=spec.values))$jack.se\t\n\t}\n\t\n\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"sens.se\"=sens.se, \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)], \"spec.se\"=spec.se))\t\n}"
      },
      {
        "partial": "theta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\tcl <- cl[x]\n\too <- order(cl, decreasing=FALSE)\n\tcl <- cl[oo]\n\tsurv.time <- surv.time[x][oo]\n\tsurv.event <- surv.event[x][oo]\n\tucl <- sort(unique(cl))\n\tmarker.fake <- 1:length(cl)\n\tnames(marker.fake) <- names(cl)\n\tmycutoff <- sum(cl == ucl[1])\n\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake, predict.time=time, span=span, ...)\n\t# Complete the function by adding the return statement\n}",
        "complete": "theta.foo1 <- function(x, cl, surv.time, surv.event, time, ...) {\n\tcl <- cl[x]\n\too <- order(cl, decreasing=FALSE)\n\tcl <- cl[oo]\n\tsurv.time <- surv.time[x][oo]\n\tsurv.event <- surv.event[x][oo]\n\tucl <- sort(unique(cl))\n\tmarker.fake <- 1:length(cl)\n\tnames(marker.fake) <- names(cl)\n\tmycutoff <- sum(cl == ucl[1])\n\trr <- survivalROC::survivalROC.C(Stime=surv.time, status=surv.event, marker=marker.fake, predict.time=time, span=span, ...)\n\treturn(list(\"sens\"=rr$TP[which(rr$cut.values == mycutoff)], \"spec\"=1-rr$FP[which(rr$cut.values == mycutoff)]))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/compute.pairw.cor.z.R",
    "language": "R",
    "content": "#' @title Function to compute the Z transformation of the pairwise\n#'   correlations for a list of datasets\n#'\n#' @description\n#' This function computes the Z transformation of the meta-estimate of pairwise correlation\n#'   coefficients for a set of genes from a list of gene expression datasets.\n#'\n#' @usage\n#' compute.pairw.cor.z(datas, method = c(\"pearson\"))\n#'\n#' @param datas List of datasets. Each dataset is a matrix of gene expressions with samples in\n#'   rows and probes in columns, dimnames being properly defined. All the datasets must have\n#'   the same probes.\n#' @param method Estimator for correlation coefficient, can be either pearson or spearman.\n#'\n#' @return\n#' A list with items:\n#' -z Z transformation of the meta-estimate of correlation coefficients.\n#' -se Standard error of the Z transformation of the meta-estimate of\n#'   correlation coefficients.\n#' -nn Number of samples used to compute the meta-estimate of correlation coefficients.\n#'\n#' @seealso\n#' [genefu::map.datasets], [genefu::compute.pairw.cor.meta], [genefu::compute.proto.cor.meta]\n#'\n#' @md\n#' @importFrom survcomp fisherz\n#' @export\ncompute.pairw.cor.z <-\nfunction(datas, method=c(\"pearson\")) {\n\tif(!is.list(datas)) {\n\t\tmycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n\t\tfor(i in 1:ncol(datas)) {\n\t\t\tfor(j in 1:i) {\n\t\t\t\tmycorz[i, j] <- mycorz[j, i] <- fisherz(cor(x=datas[ , i], y=datas[ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n\t\t\t\tmycorz.se[i, j] <- mycorz.se[j, i] <- 1/sqrt(sum(complete.cases(datas[ , c(i, j)])) - 3)\n\t\t\t}\n\t\t}\n\t} else {\n\t\tnc <- ncol(datas[[1]])\n\t\tncn <- dimnames(datas[[1]])[[2]]\n\t\tmycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn,  ncn,  names(datas)))\n\t\tmycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn,  ncn,  names(datas)))\n\t\tfor(k in 1:length(datas)) {\n\t\t\tif(nc != ncol(datas[[k]])) { stop(\"all the datasets have not the same number of columns!\") }\n\t\t\tfor(i in 1:ncol(datas[[k]])) {\n\t\t\t\tfor(j in 1:i) {\n\t\t\t\t\tnn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n\t\t\t\t\tmycorz[i, j, k] <- mycorz[j, i, k] <- fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n\t\t\t\t\tmycorz.se[i, j, k] <- mycorz.se[j, i, k] <- 1/sqrt(nn - 3)\n\t\t\t\t\tmycorn[i, j, k] <- mycorn[j, i, k] <- nn\n\t\t\t\t}\n\t\t\t}\n\t\t}\n\t}\n\treturn(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `compute.pairw.cor.z` function and what are its main inputs and outputs?",
        "answer": "The `compute.pairw.cor.z` function computes the Z transformation of the meta-estimate of pairwise correlation coefficients for a set of genes from a list of gene expression datasets. Its main inputs are: 1) `datas`: a list of datasets or a single dataset matrix, where each dataset has samples in rows and probes in columns, and 2) `method`: the correlation coefficient estimator (default is 'pearson'). The function returns a list with three items: 'z' (Z transformation of the meta-estimate of correlation coefficients), 'se' (standard error of the Z transformation), and 'nn' (number of samples used for computation)."
      },
      {
        "question": "How does the function handle the computation differently when `datas` is a single matrix versus a list of matrices?",
        "answer": "When `datas` is a single matrix, the function computes the pairwise correlations and their Z transformations directly on this matrix. It uses nested loops to calculate correlations between all pairs of columns. When `datas` is a list of matrices, the function computes pairwise correlations for each matrix in the list separately, storing results in 3D arrays. It also checks that all matrices in the list have the same number of columns. This approach allows for meta-analysis across multiple datasets."
      },
      {
        "question": "What is the significance of the `fisherz` function used in the correlation calculations, and how is the standard error computed?",
        "answer": "The `fisherz` function is used to apply Fisher's Z transformation to the computed correlation coefficients. This transformation helps to stabilize the variance of the correlation estimate and make it approximately normally distributed. The standard error for each Z-transformed correlation is computed as 1 / sqrt(n - 3), where n is the number of complete cases (non-missing pairs of observations) used in the correlation calculation. This formula is derived from the approximate variance of the Fisher Z transformation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    mycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n    for(i in 1:ncol(datas)) {\n      for(j in 1:i) {\n        # Complete the code here\n      }\n    }\n  } else {\n    # Complete the code for list input\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}",
        "complete": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    mycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n    for(i in 1:ncol(datas)) {\n      for(j in 1:i) {\n        mycorz[i, j] <- mycorz[j, i] <- fisherz(cor(x=datas[ , i], y=datas[ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n        mycorz.se[i, j] <- mycorz.se[j, i] <- 1/sqrt(sum(complete.cases(datas[ , c(i, j)])) - 3)\n      }\n    }\n    mycorn <- NULL\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    mycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    mycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    for(k in 1:length(datas)) {\n      if(nc != ncol(datas[[k]])) stop(\"all the datasets have not the same number of columns!\")\n      for(i in 1:nc) {\n        for(j in 1:i) {\n          nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n          mycorz[i, j, k] <- mycorz[j, i, k] <- fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n          mycorz.se[i, j, k] <- mycorz.se[j, i, k] <- 1/sqrt(nn - 3)\n          mycorn[i, j, k] <- mycorn[j, i, k] <- nn\n        }\n      }\n    }\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}"
      },
      {
        "partial": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    # Complete the code for non-list input\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    mycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    mycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    for(k in 1:length(datas)) {\n      if(nc != ncol(datas[[k]])) stop(\"all the datasets have not the same number of columns!\")\n      # Complete the nested loops and calculations here\n    }\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}",
        "complete": "compute.pairw.cor.z <- function(datas, method=c(\"pearson\")) {\n  if(!is.list(datas)) {\n    mycorz <- mycorz.se <- matrix(NA, nrow=ncol(datas), ncol=ncol(datas), dimnames=list(dimnames(datas)[[2]], dimnames(datas)[[2]]))\n    for(i in 1:ncol(datas)) {\n      for(j in 1:i) {\n        mycorz[i, j] <- mycorz[j, i] <- fisherz(cor(x=datas[ , i], y=datas[ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n        mycorz.se[i, j] <- mycorz.se[j, i] <- 1/sqrt(sum(complete.cases(datas[ , c(i, j)])) - 3)\n      }\n    }\n    mycorn <- NULL\n  } else {\n    nc <- ncol(datas[[1]])\n    ncn <- dimnames(datas[[1]])[[2]]\n    mycorz <- mycorz.se <- array(NA, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    mycorn <- array(0, dim=c(nc, nc, length(datas)), dimnames=list(ncn, ncn, names(datas)))\n    for(k in 1:length(datas)) {\n      if(nc != ncol(datas[[k]])) stop(\"all the datasets have not the same number of columns!\")\n      for(i in 1:nc) {\n        for(j in 1:i) {\n          nn <- sum(complete.cases(datas[[k]][ , c(i, j)]))\n          mycorz[i, j, k] <- mycorz[j, i, k] <- fisherz(cor(x=datas[[k]][ , i], y=datas[[k]][ , j], method=method, use=\"complete.obs\"), inv=FALSE)\n          mycorz.se[i, j, k] <- mycorz.se[j, i, k] <- 1/sqrt(nn - 3)\n          mycorn[i, j, k] <- mycorn[j, i, k] <- nn\n        }\n      }\n    }\n  }\n  return(list(\"z\"=mycorz, \"se\"=mycorz.se, \"nn\"=mycorn))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/sig.score.R",
    "language": "R",
    "content": "#' @title Function to compute signature scores as linear combination of \n#'   gene expressions\n#'\n#' @description\n#' This function computes a signature score from a gene list (aka gene \n#'   signature), i.e. a signed average as published in Sotiriou et al. 2006 \n#'   and Haibe-Kains et al. 2009.\n#'\n#' @usage\n#' sig.score(x, data, annot, do.mapping = FALSE, mapping, size = 0,\n#'   cutoff = NA, signed = TRUE, verbose = FALSE)\n#'\n#' @param x\tMatrix containing the gene(s) in the gene list in rows and at \n#'   least three columns: \"probe\", \"EntrezGene.ID\" and \"coefficient\" standing \n#'   for the name of the probe, the NCBI Entrez Gene id and the coefficient \n#'   giving the direction and the strength of the association of each gene \n#'   in the gene list.\n#' @param data Matrix of gene expressions with samples in rows and probes \n#'   in columns, dimnames being properly defined.\n#' @param annot Matrix of annotations with at least one column named \n#'   \"EntrezGene.ID\", dimnames being properly defined.\n#' @param do.mapping TRUE if the mapping through Entrez Gene ids must be \n#'   performed (in case of ambiguities, the most variant probe is kept for \n#'   each gene), FALSE otherwise.\n#' @param mapping Matrix with columns \"EntrezGene.ID\" and \"probe\" used to \n#'   force the mapping such that the probes are not selected based on their \n#'   variance.\n#' @param size Integer specifying the number of probes to be considered in \n#'   signature computation. The probes will be sorted by absolute value of \n#'   coefficients.\n#' @param cutoff Only the probes with coefficient greater than cutoff will\n#'    be considered in signature computation.\n#' @param signed TRUE if only the sign of the coefficient must be considered \n#'   in signature computation, FALSE otherwise.\n#' @param verbose TRUE to print informative messages, FALSE otherwise.\n#'\n#' @return\n#' A list with items:\n#' - score: Signature score.\n#' - mapping: Mapping used if necessary.\n#' - probe: If mapping is performed, this matrix contains the correspondence \n#'   between the gene list (aka signature) and gene expression data.\n#'\n#' @references\n#' Sotiriou C, Wirapati P, Loi S, Harris A, Bergh J, Smeds J, Farmer P, Praz \n#'   V, Haibe-Kains B, Lallemand F, Buyse M, Piccart MJ and Delorenzi M \n#'   (2006) \"Gene expression profiling in breast cancer: Understanding the \n#'   molecular basis of histologic grade to improve prognosis\", Journal of \n#'   National Cancer Institute, 98:262-272\n#' Haibe-Kains B (2009) \"Identification and Assessment of Gene Signatures \n#'   in Human Breast Cancer\", PhD thesis at Universite Libre de Bruxelles,\n#'   http://theses.ulb.ac.be/ETD-db/collection/available/ULBetd-02182009-083101/\n#'\n#' @examples\n#' # load NKI data\n#' data(nkis)\n#' # load GGI signature\n#' data(sig.ggi)\n#' # make of ggi signature a gene list\n#' ggi.gl <- cbind(sig.ggi[ ,c(\"probe\", \"EntrezGene.ID\")],\n#'   \"coefficient\"=ifelse(sig.ggi[ ,\"grade\"] == 1, -1, 1))\n#' # computation of signature scores\n#' ggi.score <- sig.score(x=ggi.gl, data=data.nkis, annot=annot.nkis,\n#'   do.mapping=TRUE, signed=TRUE, verbose=TRUE)\n#' str(ggi.score)\n#'\n#' @md\n#' @export\nsig.score <-\nfunction(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n\t\n\tif(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n\tx <- as.data.frame(x, stringsAsFactors=FALSE)\n\tif(nrow(x) == 0) { stop(\"empty gene list!\"); }\n\n\tmyprobe <- as.character(x[ ,\"probe\"])\n\tmygid <- as.character(x[ ,\"EntrezGene.ID\"])\n\tmycoef <- as.numeric(x[ ,\"coefficient\"])\n\tnames(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n\n\tnix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n\tmyprobe <- myprobe[nix]\n\tmygid <- mygid[nix]\n\tmycoef <- mycoef[nix]\n   \n   if(do.mapping) { ## mapping is requested\n\t\tgid1 <- mygid\n\t\tgid2 <- as.character(annot[ ,\"EntrezGene.ID\"])\n\t\tnames(gid2) <- dimnames(annot)[[1]]\n\t\t## remove missing and duplicated geneids from the gene list\n\t\trm.ix <- is.na(gid1) | duplicated(gid1)\n\t\tgid1 <- gid1[!rm.ix]\n\t\n\t\trr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n\t\tif(is.na(rr$geneid1[1])) {\n\t\t\t#no gene ids in common\n\t\t\tres <- rep(NA, nrow(data))\n\t\t\tnames(res) <- dimnames(data)[[1]]\n\t\t\tgf <- c(\"mapped\"=0, \"total\"=nrow(x))\n\t\t\tif(verbose) { message(sprintf(\"probe candidates: 0/%i\", nrow(x))) }\n\t\t\treturn(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=NA, \"EntrezGene.ID\"=NA, \"new.probe\"=NA)))\n\t\t}\n\t\tnix <- match(rr$geneid2, mygid)\n\t\tmyprobe <- myprobe[nix]\n\t\tmygid <- mygid[nix]\n\t\tmycoef <- mycoef[nix]\n\t\tgid1 <- rr$geneid2\n\t\tif(is.null(names(gid1))) { stop(\"problem with annotations!\") }\n\t\tgid2 <- rr$geneid1\n\t\tif(is.null(names(gid2))) { stop(\"problem with annotations!\") }\n\t\tdata <- rr$data1\n\t\n\t\t#change the names of probes in x and data\n\t\tnames(mycoef) <- names(mygid) <- mygid <- names(myprobe) <- myprobe <- as.character(gid1)\n\t\tdimnames(data)[[2]] <- as.character(gid2)\n\t} else { ## no mapping\n\t\tnix <- is.element(myprobe, dimnames(data)[[2]])\n\t\tmyprobe <- myprobe[nix]\n\t\tmygid <- mygid[nix]\n\t\tmycoef <- mycoef[nix]\n\t\tgid1 <- gid2 <- mygid\n\t\tdata <- data[ ,myprobe,drop=FALSE]\n\t}\n\tif(length(myprobe) == 0) {\n\t\tif(verbose) { message(sprintf(\"probe candidates: 0/%i\", size)) }\n\t\ttt <- rep(NA, nrow(data))\n\t\tnames(tt) <- dimnames(data)[[1]]\n\t\treturn(list(\"score\"=tt, \"mapping\"=c(\"mapped\"=0, \"total\"=nrow(x)), \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n\t}\n\t\n\tif(size == 0 || size > nrow(x)) { size <- length(myprobe) }\n\tnix <- 1:size\n\tmyprobe <- myprobe[nix]\n\tmygid <- mygid[nix]\n\tmycoef <- mycoef[nix]\n\tgid1 <- gid1[nix]\n\tgid2 <- gid2[nix]\n\tif(!is.na(cutoff)) {\n\t\tnix <- abs(mycoef) > cutoff\n\t\tmyprobe <- myprobe[nix]\n\t\tmygid <- mygid[nix]\n\t\tmycoef <- mycoef[nix]\n\t\tgid1 <- gid1[nix]\n\t\tgid2 <- gid2[nix]\n   }\n\tprobe.candp <- myprobe[mycoef >= 0]\n\tprobe.candn <- myprobe[mycoef < 0]\n\tgf <- length(myprobe)\n\n\tgf <- c(\"mapped\"=gf, \"total\"=nrow(x))\n\tif(verbose) { message(sprintf(\"probe candidates: %i/%i\",gf[1], gf[2])) }\n\n\tnprobe <- c(probe.candp, probe.candn)\n\tmyw <- c(\"p\"=length(probe.candp) / length(nprobe), \"n\"=length(probe.candn) / length(nprobe))\n\tres <- rep(0, nrow(data))\n\t\n\tif(signed) {\n\t\t## consider only the sign of the coefficients\n\t\tif(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n\t\tif(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n\t} else {\n\t\t## consider the exact value of the coefficients\n\t\tif(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candp]))) }\n\t\tif(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candn]))) }\n\t}\n\treturn(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'sig.score' function and what are its main input parameters?",
        "answer": "The 'sig.score' function computes a signature score from a gene list (gene signature) as a linear combination of gene expressions. Its main input parameters are:\n- x: Matrix containing gene information (probe, EntrezGene.ID, coefficient)\n- data: Matrix of gene expressions\n- annot: Matrix of annotations\n- do.mapping: Boolean indicating whether to perform mapping through Entrez Gene IDs\n- size: Number of probes to consider in signature computation\n- signed: Boolean indicating whether to consider only the sign of coefficients"
      },
      {
        "question": "How does the function handle gene mapping when 'do.mapping' is set to TRUE?",
        "answer": "When 'do.mapping' is TRUE, the function:\n1. Removes missing and duplicated gene IDs from the gene list\n2. Uses 'geneid.map' function to map gene IDs between the gene list and annotation data\n3. Updates the probe names, gene IDs, and coefficients based on the mapping results\n4. Modifies the data matrix to use the mapped gene IDs as column names\nIf no gene IDs are in common after mapping, the function returns NA scores with appropriate mapping information."
      },
      {
        "question": "How does the 'sig.score' function calculate the final signature score?",
        "answer": "The function calculates the signature score as follows:\n1. Separates probes into positive and negative groups based on their coefficients\n2. Calculates weights for positive and negative groups based on their sizes\n3. If 'signed' is TRUE, it uses only the sign of coefficients:\n   - For positive probes: (sum of expressions) / (number of non-NA values) * weight\n   - For negative probes: -(sum of expressions) / (number of non-NA values) * weight\n4. If 'signed' is FALSE, it uses the exact coefficient values:\n   - For positive probes: (sum of expressions * abs(coefficients)) / (sum of abs(coefficients) for non-NA values) * weight\n   - For negative probes: -(sum of expressions * abs(coefficients)) / (sum of abs(coefficients) for non-NA values) * weight\n5. The final score is the sum of positive and negative contributions"
      }
    ],
    "completion_tasks": [
      {
        "partial": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n\n  # TODO: Implement mapping logic\n\n  # TODO: Implement score calculation\n\n  # TODO: Return result\n}",
        "complete": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n   \n  if(do.mapping) {\n    gid1 <- mygid\n    gid2 <- as.character(annot[ ,\"EntrezGene.ID\"])\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    \n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    if(is.na(rr$geneid1[1])) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=nrow(x))\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", nrow(x))) }\n      return(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=NA, \"EntrezGene.ID\"=NA, \"new.probe\"=NA)))\n    }\n    nix <- match(rr$geneid2, mygid)\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    \n    names(mycoef) <- names(mygid) <- mygid <- names(myprobe) <- myprobe <- as.character(gid1)\n    dimnames(data)[[2]] <- as.character(gid2)\n  } else {\n    nix <- is.element(myprobe, dimnames(data)[[2]])\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- gid2 <- mygid\n    data <- data[ ,myprobe,drop=FALSE]\n  }\n\n  if(length(myprobe) == 0) {\n    if(verbose) { message(sprintf(\"probe candidates: 0/%i\", size)) }\n    tt <- rep(NA, nrow(data))\n    names(tt) <- dimnames(data)[[1]]\n    return(list(\"score\"=tt, \"mapping\"=c(\"mapped\"=0, \"total\"=nrow(x)), \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n  }\n  \n  if(size == 0 || size > nrow(x)) { size <- length(myprobe) }\n  nix <- 1:size\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n  gid1 <- gid1[nix]\n  gid2 <- gid2[nix]\n  if(!is.na(cutoff)) {\n    nix <- abs(mycoef) > cutoff\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- gid1[nix]\n    gid2 <- gid2[nix]\n  }\n  probe.candp <- myprobe[mycoef >= 0]\n  probe.candn <- myprobe[mycoef < 0]\n  gf <- length(myprobe)\n\n  gf <- c(\"mapped\"=gf, \"total\"=nrow(x))\n  if(verbose) { message(sprintf(\"probe candidates: %i/%i\",gf[1], gf[2])) }\n\n  nprobe <- c(probe.candp, probe.candn)\n  myw <- c(\"p\"=length(probe.candp) / length(nprobe), \"n\"=length(probe.candn) / length(nprobe))\n  res <- rep(0, nrow(data))\n  \n  if(signed) {\n    if(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n    if(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=sum, na.rm=TRUE) / apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x) { return(sum(!is.na(x))) })) }\n  } else {\n    if(length(probe.candp) > 0) { res <- myw[\"p\"] * (apply(X=data[ ,probe.candp,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candp]))) }\n    if(length(probe.candn) > 0) { res <- res - myw[\"n\"] * (apply(X=data[ ,probe.candn,drop=FALSE], MARGIN=1, FUN=function(x, y) { nix <- is.na(x); return(sum(x * y, na.rm=TRUE) / sum(y[!nix])) }, y=abs(mycoef[probe.candn]))) }\n  }\n  return(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=names(gid1), \"EntrezGene.ID\"=gid1, \"new.probe\"=names(gid2))))\n}"
      },
      {
        "partial": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  # Input validation\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  # Extract and sort probe information\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n\n  # TODO: Implement mapping logic if do.mapping is TRUE\n\n  # TODO: Apply size and cutoff filters\n\n  # TODO: Calculate scores\n\n  # TODO: Return results\n}",
        "complete": "sig.score <- function(x, data, annot, do.mapping=FALSE, mapping, size=0, cutoff=NA, signed=TRUE, verbose=FALSE) {\n  # Input validation\n  if(missing(data) || missing(annot)) { stop(\"data and annot parameters must be specified\") }\n  x <- as.data.frame(x, stringsAsFactors=FALSE)\n  if(nrow(x) == 0) { stop(\"empty gene list!\") }\n\n  # Extract and sort probe information\n  myprobe <- as.character(x[ ,\"probe\"])\n  mygid <- as.character(x[ ,\"EntrezGene.ID\"])\n  mycoef <- as.numeric(x[ ,\"coefficient\"])\n  names(mycoef) <- names(mygid) <- names(myprobe) <- myprobe\n  nix <- order(abs(mycoef), decreasing=TRUE, na.last=NA)\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n\n  # Mapping logic\n  if(do.mapping) {\n    gid1 <- mygid\n    gid2 <- as.character(annot[ ,\"EntrezGene.ID\"])\n    names(gid2) <- dimnames(annot)[[1]]\n    rm.ix <- is.na(gid1) | duplicated(gid1)\n    gid1 <- gid1[!rm.ix]\n    \n    rr <- geneid.map(geneid1=gid2, data1=data, geneid2=gid1, verbose=FALSE)\n    if(is.na(rr$geneid1[1])) {\n      res <- rep(NA, nrow(data))\n      names(res) <- dimnames(data)[[1]]\n      gf <- c(\"mapped\"=0, \"total\"=nrow(x))\n      if(verbose) { message(sprintf(\"probe candidates: 0/%i\", nrow(x))) }\n      return(list(\"score\"=res, \"mapping\"=gf, \"probe\"=cbind(\"probe\"=NA, \"EntrezGene.ID\"=NA, \"new.probe\"=NA)))\n    }\n    nix <- match(rr$geneid2, mygid)\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- rr$geneid2\n    gid2 <- rr$geneid1\n    data <- rr$data1\n    \n    names(mycoef) <- names(mygid) <- mygid <- names(myprobe) <- myprobe <- as.character(gid1)\n    dimnames(data)[[2]] <- as.character(gid2)\n  } else {\n    nix <- is.element(myprobe, dimnames(data)[[2]])\n    myprobe <- myprobe[nix]\n    mygid <- mygid[nix]\n    mycoef <- mycoef[nix]\n    gid1 <- gid2 <- mygid\n    data <- data[ ,myprobe,drop=FALSE]\n  }\n\n  # Apply size and cutoff filters\n  if(size == 0 || size > nrow(x)) { size <- length(myprobe) }\n  nix <- 1:size\n  myprobe <- myprobe[nix]\n  mygid <- mygid[nix]\n  mycoef <- mycoef[nix]\n  gid1 <- gid1["
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/src/foo_mrmr_ensemble_surv.cpp",
    "language": "cpp",
    "content": "#include \"foo_mrmr_ensemble_surv.h\"\n\ndouble get_correlation_ensemble(double data [],int namat[], int ind_x, int ind_y, int size){\n\t//compute correlation of two variables;\n\t//data: contains all data in a vector; variable-wise appended\n\t//ind_x: starting index of first variable in data\n\t//ind_y: starting index of second variable in data\n\t//size: number of samples for both variables\n\tdouble mean_data_x=0.0,mean_data_y=0.0;\n\tdouble correlation_nom=0.0,correlation_den_x=0.0,correlation_den_y=0.0;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif (namat[ind_x+i]==0 && namat[ind_y+i]==0 ) {\n\t\t\tmean_data_x+=data[ind_x+i];\n\t\t\tmean_data_y+=data[ind_y+i];\n\t\t}\n\t}\n\n\tmean_data_x=mean_data_x/size;\n\tmean_data_y=mean_data_y/size;\n\n\tfor( unsigned int i=0; i< size; ++i ) {\n\t\tif(namat[ind_x+i]==0 && namat[ind_y+i]==0){\n\t\tcorrelation_nom+=(data[ind_x+i]-mean_data_x)*(data[ind_y+i]-mean_data_y);\n\t\tcorrelation_den_x+=(data[ind_x+i]-mean_data_x)*(data[ind_x+i]-mean_data_x);\n\t\tcorrelation_den_y+=(data[ind_y+i]-mean_data_y)*(data[ind_y+i]-mean_data_y);\n\t\t}\n\t}\n\treturn correlation_nom/(sqrt(correlation_den_x*correlation_den_y));\n}\n\n\ndouble returnConcordanceIndexC(int *msurv, int *ustrat, double *x2, int *cl2,\n\t\t\t\t\t   double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU)\n{\n\n\tint lenUstrat = *lenU;\n\tint lenStrat = lenS;\n\n\tdouble res_ch[lenStrat];\n\tdouble res_dh[lenStrat];\n\n\tdouble res_cIndex=0;\n\n\tint Ns_old = 0;\n\tint Ns = 0;\n\tfor(int s=0; s < lenUstrat; s++) {\n\t\tint ixs[lenStrat];\n\t\tfor(int i =0; i < lenStrat; i++){\n\t\t\tixs[i] = 0;\n\t\t\tif(strat[i] == ustrat[s]){\n\t\t\t\tixs[i] = 1;\n\t\t\t} else {\n\t\t\t\tixs[i] = 0;\n\t\t\t}\n\t\t}\n\t\tNs_old += Ns;\n\t\tNs = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tNs++;\n\t\t\t}\n\t\t}\n\t\tdouble xs[Ns];\n\t\tint c = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\txs[c] = x2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint cls[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tcls[c] = cl2[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble sts[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tsts[c] = st[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tint ses[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tses[c] = se[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble weightss[Ns];\n\t\tc = 0;\n\t\tfor(int i=0; i < lenStrat; i++){\n\t\t\tif(ixs[i] == 1){\n\t\t\t\tweightss[c] = weights[i];\n\t\t\t\tc++;\n\t\t\t}\n\t\t}\n\t\tdouble chs[Ns];\n\t\tdouble dhs[Ns];\n\t\tdouble uhs[Ns];\n\t\tdouble rphs[Ns];\n\t\tfor (int h=0; h < Ns; h++) {\n\t\t\tdouble chsj, dhsj, uhsj, rphsj = 0;\n\t\t\tfor (int j=0; j < Ns; j++) {\n\t\t\t\tdouble whj = weightss[h] * weightss[j];\n\t\t\t\tif((*msurv == 1 && (sts[h] < sts[j] && ses[h] == 1)) || (*msurv == 0 && cls[h] > cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] > xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t} else if (xs[h] < xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t\tif((*msurv == 1 && (sts[h] > sts[j] && ses[j] == 1)) || (*msurv == 0 && cls[h] < cls[j])){\n\t\t\t\t\trphsj = rphsj + whj;\n\t\t\t\t\tif (xs[h] < xs[j]) {\n\t\t\t\t\t\tchsj = chsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse if (xs[h] > xs[j]) {\n\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t}\n\t\t\t\t\telse {\n\t\t\t\t\t\tif (*outx == 1) {\n\t\t\t\t\t\t\tuhsj = uhsj + whj;\n\t\t\t\t\t\t} else {\n\t\t\t\t\t\t\tdhsj = dhsj + whj;\n\t\t\t\t\t\t}\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}\n\t\t\tchs[h] = chsj;\n\t\t\tdhs[h] = dhsj;\n\t\t\tuhs[h] = uhsj;\n\t\t\trphs[h] = rphsj;\n\t\t\tchsj = 0;\n\t\t\tdhsj = 0;\n\t\t\tuhsj = 0;\n\t\t\trphsj = 0;\n\t\t}\n\t\tfor(int i = 0; i < Ns; i++){\n\t\t\tint pos = i + Ns_old;\n\t\t\tres_ch [pos] =chs[i];\n\t\t\tres_dh [pos] =dhs[i];\n\t\t}\n\n\t}\n\n\tdouble tmp_ch=0, tmp_dh=0;\n\tfor(int s=0; s < lenStrat; s++) {\n\t\ttmp_ch+=res_ch[s];\n\t\ttmp_dh+=res_dh[s];\n\t}\n\n\tdouble n=*N;\n\ttmp_ch=(1/(n *(n - 1))) * tmp_ch;\n\ttmp_dh=(1/(n *(n - 1))) * tmp_dh;\n\tres_cIndex=tmp_ch/ (tmp_ch+tmp_dh);\n\n\t/// scale value to be in the intervall [-1,1] as correlation and square to be on same scale as mutual information [0,1]\n\n\tres_cIndex=2*res_cIndex-1;\n\tres_cIndex=res_cIndex*res_cIndex;\n\treturn res_cIndex;\n}\n\n\nvoid build_mim_cIndex_subset(double mim[],double data[], int namat [],int nvar,int nsample, int subset [],int size_subset,int *msurv, int *ustrat, int *cl2, double *st, int *se, double *weights, int *strat, int *N, int *outx, int *lenU){\n\t//compute mutual information matrix\n\t//mim:\t\t\tmatrix (stored as vector) in which the mi values will be stored\n\t//data:\t\t\tcontains all data in a vector; variable-wise appended\n\t//nvar:\t\t\tnumber of variables\n\t//nsample:\t\tnumber of samples in dataset\n\t//subset:\t\tindices of samples to be included in the bootstrapping data\n\t//size_subset:\tnumber of variables in the bootstrapped dataset\n\n\tdouble tmp;\n\tdouble *data_x, *st_x, *weights_x;\n\tint *namat_x, *msurv_x, *ustrat_x, *cl2_x, *se_x, *strat_x;\n\n\tnamat_x = (int*) R_alloc(nvar*size_subset, sizeof(int));\n\tcl2_x = (int*) R_alloc(size_subset, sizeof(int));\n\tse_x = (int*) R_alloc(size_subset, sizeof(int));\n\tstrat_x = (int*) R_alloc(size_subset, sizeof(int));\n\n\tdata_x = (double *) R_alloc(nvar*size_subset, sizeof(double));\n\tst_x = (double *) R_alloc(size_subset, sizeof(double));\n\tweights_x = (double *) R_alloc(size_subset, sizeof(double));\n\n\tfor(unsigned int i=0; i< size_subset; ++i){\n\t\tfor(unsigned int j=0; j< (nvar-1); ++j){\n\t\t\tdata_x[size_subset*j+i]=data[(subset[i])+nsample*j];\n\t\t\tnamat_x[size_subset*j+i]=namat[(subset[i])+nsample*j];\n\t\t}\n\n\t\tcl2_x[i]=cl2[subset[i]];\n\t\tse_x[i]=se[subset[i]];\n\t\tstrat_x[i]=strat[subset[i]];\n\t\tst_x[i]=st[subset[i]];\n\t\tweights_x[i]=weights[subset[i]];\n\t}\n\tfor(unsigned int i=0; i< nvar-1; ++i){\n\t\tmim[(i+1)*(nvar)+(i+1)]=0;\n\t\tfor(unsigned int j=i+1; j< nvar-1; ++j){\n\t\t\ttmp=get_correlation_ensemble(data_x,namat_x,i*size_subset,j*size_subset,size_subset);\n\t\t\ttmp=tmp*tmp;\n\t\t\tif(tmp>0.999999){\n\t\t\t\ttmp=0.999999;\n\t\t\t}\n\t\t\tmim[(j+1)*(nvar)+i+1]= -0.5* log (1-tmp);\n\t\t\tmim[(i+1)*(nvar)+j+1]=mim[(j+1)*(nvar)+i+1];\n\t\t}\n\t}\n\n\tdouble *data_small;\n\tdata_small =(double*) R_alloc(size_subset, sizeof(double));\n\n\tfor(int j=0;j< nvar-1 ;++j){\n\t\tfor(int i=0;i< size_subset;++i){\n\t\t\tdata_small[i]=data_x[j* (nvar-1)+i];\n\t\t}\n\t\tmim[j+1]=returnConcordanceIndexC(msurv, ustrat, data_small, cl2_x,st_x, se_x, weights_x,strat_x, N, outx, size_subset, lenU) ;\n\t\tmim[(nvar)*(j+1)] = mim[j+1];\n\t}\n\n}\n\nvoid remove_childless_nodes( tree<int>& res, tree<double>&res_mean, int max_elements_tmp){\n\ttree<int>::pre_order_iterator it_tmp,it_back, it=res.begin();\n\ttree<double>::pre_order_iterator  it_mean_tmp,it_mean_back, it_mean=res_mean.begin();\n\ttree<int>::leaf_iterator li;\n\ttree<double>::leaf_iterator li_mean;\n\tbool found_child, multiple;\n\n\tint depth_max=0;\n\t//determine max depth\n\twhile(it!=res.end()){\n\t\tif(depth_max<res.depth(it)){\n\t\t\tdepth_max=res.depth(it);\n\t\t}\n\t\tit++;\n\t}\n\tit=res.begin();\n\twhile(it!=res.end() && max_elements_tmp<= (depth_max+1)) {\n\t\tif(res.depth(it)<=(max_elements_tmp-2) && res.number_of_children(it)==0){ //advance through the tree\n\t\t\tit_tmp=res.parent(it);it_mean_tmp=res_mean.parent(it_mean);\n\t\t\tfound_child=false; multiple=false;\n\t\t\twhile(!found_child && (it_tmp!=res.begin() || res.number_of_children(it_tmp)>1)){ //end loop if there is a node with more than one child or if back at top and number of children==1 for top node\n\t\t\t\tif(res.number_of_children(it_tmp)==1){\n\t\t\t\t\tit_back=it_tmp;it_mean_back=it_mean_tmp; //if this was the last level for which there is only one child\n\t\t\t\t\tmultiple=true;\n\t\t\t\t\tif(it_tmp!=res.begin()){\n\t\t\t\t\t\tit_tmp=res.parent(it_tmp); it_mean_tmp=res_mean.parent(it_mean_tmp);\n\t\t\t\t\t}else{//in case of having arrived at the top node\n\t\t\t\t\t\tres.erase(it_back); res_mean.erase(it_mean_back);\n\t\t\t\t\t\tfound_child=true;\n\t\t\t\t\t}\n\t\t\t\t}else{\n\t\t\t\t\tif(multiple){\n\t\t\t\t\t\tres.erase(it_back); res_mean.erase(it_mean_back);\n\t\t\t\t\t}else{\n\t\t\t\t\t\tres.erase(it); res_mean.erase(it_mean);\n\t\t\t\t\t}\n\t\t\t\t\tfound_child=true;\n\t\t\t\t}\n\t\t\t}\n\t\t\tit=it_tmp; it_mean=it_mean_tmp;\n\t\t}else{\n\t\t\t++it; ++it_mean;\n\t\t}\n\t}\n}\nvoid bootstrap_tree(tree<int>& res,tree<double>& res_mrmr, double data[],int namat[], int nsamples,int n, int rep_boot, int *msurv, int *ustrat, int *cl2,double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU){\n\tint  nsub, *prev_sel,nsamples_boot=nsamples,*to_remove;\n\ttree<int>::iterator li=res.begin_leaf(),li2;\n\ttree<double>::iterator li_mrmr=res_mrmr.begin_leaf(),li2_mrmr;\n\tdouble *mean, *sd;\n\tint cnt_leafs=0;\n\tint max_depth=res.depth(li),index;\n\twhile (li!=res.end()) {\n\t\tif(res.depth(li)==max_depth){\n\t\t\tcnt_leafs++;\n\t\t}\n\t\tli++;\n\t}\n\n\tli=res.begin_leaf();\n\n\tmean =(double*) R_alloc(cnt_leafs, sizeof(double));\n\tsd =(double*) R_alloc(cnt_leafs, sizeof(double));\n\tto_remove=(int*) R_alloc(cnt_leafs, sizeof(int));\n\n\tfor(int k=0;k<cnt_leafs;k++){\n\t\tmean[k]=0;sd[k]=0;\n\t}\n\n\tint target=*res.begin();\n\tint nto_remove=0;\n\n\tprev_sel=(int*) R_alloc(max_depth, sizeof(int));\n\n\tint k=0;\n\twhile (li!=res.end()) {\n\t\tif(res.depth(li)==max_depth){\n\t\t\tli2=li;\n\t\t\tprev_sel[max_depth-1]=*(li);\n\t\t\tli2=res.parent(li2);\n\t\t\tindex=max_depth-2;\n\t\t\twhile (li2!=res.begin()) {\n\t\t\t\tprev_sel[index]=*(li2);\n\t\t\t\tindex--;\n\t\t\t\tli2=res.parent(li2);\n\t\t\t}\n\t\t\tbootstrap_mrmr(mean[k], sd[k], data,namat,n, rep_boot,nsamples_boot,nsamples, target, prev_sel[max_depth-1], max_depth-1,prev_sel, msurv, ustrat, cl2,st, se, weights,strat, N, outx, lenS, lenU);\n\t\t\tk++;\n\t\t}\n\t\tli++;\n\t}\n\tdouble max_mrmr=-1000;\n\tint max_mrmr_ind=-1;\n\tfor(int k=0;k<cnt_leafs;k++){\n\t\tif(mean[k]>max_mrmr){\n\t\t\tmax_mrmr=mean[k];\n\t\t\tmax_mrmr_ind=k;\n\t\t}\n\t}\n\tfor(int k=0;k<cnt_leafs;k++){\n\t\tif(k!=max_mrmr_ind && (mean[k] < max_mrmr-sd[max_mrmr_ind])){\n\t\t\tto_remove[nto_remove]=k;\n\t\t\tnto_remove++;\n\t\t}\n\t}\n\n\tint cnt2=nto_remove;\n\tif(cnt2>0){\n\t\tli=res.begin_leaf(res.end());\n\t\tsort(to_remove,to_remove+cnt2);\n\t\tli_mrmr=res_mrmr.begin_leaf(res_mrmr.end());\n\t\tint cnt_back=cnt2;\n\t\twhile (cnt_leafs>=0 && cnt2>0) {\n\t\t\tli2=li;\n\t\t\tli2_mrmr=li_mrmr;\n\t\t\tli--;li_mrmr--;\n\n\t\t\twhile(res.depth(li)<max_depth && li2!=res.begin_leaf(res.begin())) {\n\t\t\t\tli--;li_mrmr--;\n\t\t\t}\n\n\t\t\tif(to_remove[cnt2-1]==cnt_leafs){\n\t\t\t\tres.erase(li2);res_mrmr.erase(li2_mrmr);\n\t\t\t\tcnt2--;\n\t\t\t}\n\t\t\tcnt_leafs--;\n\t\t}\n\t}\n\tremove_childless_nodes(res, res_mrmr,max_depth+1);\n\n}\nvoid bootstrap_mrmr(double &mean, double &sd, double data[],int namat[],int size, int rep_boot, int size_boot,int nsamples, int var_target, int var_interest, int nprev_sel,int* var_ind, int *msurv, int *ustrat, int *cl2,\n\t\t\t\t\tdouble *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU)\n{\n\t//mean\n\t//sd\n\t//data\n\t//size\n\t//rep_boot\n\t//size_boot\n\t//nsamples\n\t//var_target\n\t//var_interest\n\t//nprev_sel\n\t//var_ind\n\n\tint *ind;\n\tdouble *mim, *boot_val, *mat_info;\n\n\tind=(int*) R_alloc(size_boot, sizeof(int));\n\tboot_val =(double*) R_alloc(rep_boot, sizeof(double));\n\tmat_info =(double*) R_alloc(((size)*(size )), sizeof(double));\n\n\tfor(unsigned int k=0; k< rep_boot; ++k){\n\t\t//in total there will be rep_boot times the mrmr sampled\n\t\t//determine the subset of samples that should be used (in total size_boot samples will be selected)\n\t\tfor(unsigned int i=1;i<= size_boot;++i){\n\t\t\tind[i-1]=(int)unif_rand () %nsamples;\n\t\t}\n\t\t// compute mi matrix for the subset\n\t\tfor( unsigned int i=0; i< size ; ++i ){\n\t\t\tfor( unsigned int j=0; j< size ; ++j ){\n\t\t\t\tmat_info[i+(size )*j]=0;\n\t\t\t}\n\t\t}\n\n\t\tbuild_mim_cIndex_subset(mat_info, data, namat, (size) , nsamples, ind, size_boot ,msurv, ustrat, cl2,st, se, weights,strat, N, outx,  lenU);\n\t\tboot_val[k]=mrnet_onegene(mat_info, size, nprev_sel, var_ind, var_target, var_interest);\n\n\t}\n\n\t// determine mean and variance of bootstrapped values\n\tfor(unsigned int i=0;i< rep_boot;++i){\n\t\tif(boot_val[i]==boot_val[i]){\n\t\t\tmean+=boot_val[i];\n\t\t}\n\t}\n\tmean=mean/rep_boot;\n\n\tfor(unsigned int i=0;i< rep_boot;++i){\n\t\tif(boot_val[i]==boot_val[i]){\n\t\t\tsd+=(boot_val[i]-mean) * (boot_val[i]-mean);\n\t\t}\n\t}\n\tsd=sqrt(sd/rep_boot);\n\n}\n\n\ndouble mrnet_onegene(double mim [], int size, int nbvar,int *var_ind,int var_target, int var_interest){\n\t// mim:\t\t\tmutual information matrix\n\t// size:\t\ttotal number of variables\n\t// nbvar:\t\tnumber of previously selected variables (not the target)\n\t// var_ind:\t\tthe indices of the previously selected variables as vector\n\t// var_target:\tthe index of the target gene\n\t// var_interest: the variable for which the mrmr score with the target has to be computed; will be used for bootstrapping it\n\n\tunsigned int jmax;\n\tdouble rel, red,res;\n\tdouble max_val=-1000;\n\n\tjmax=var_target-1;\n\t//initialize the remaining entries to zero\n\tred=0;\n\t// the relevance for variable of interest with the target is simply its mutual information with it\n\trel=mim[(var_target-1)*size+var_interest-1];\n\tif(nbvar > 0){\n\t\t// in case other variables have been previously selected; compute their redundancy with the variable of interest\n\t\tfor(unsigned int j=0;j< nbvar; j++){\n\t\t\tred+=mim[(var_ind[j]-1)*size+var_interest-1];\n\t\t}\n\t\tres=rel-red/nbvar ;\n\t}else{\n\t\tres=rel;\n\t}\n\treturn res;\n}\n\nint power(int a, int b)\n{\n\tint c=a;\n\tfor (int n=b; n>1; n--) c*=a;\n\treturn c;\n}\nint verify_equivalentset_nparents (tree<int>& tr, tree<int>::pre_order_iterator it, tree<int>::pre_order_iterator end,tree<double>& tr_mrmr, int maxnsol){\n\tif(!tr.is_valid(it)) return 0;\n\n\tbool found=false;\n\tint number_elements_to_remove=0, cnt=1, index=0;\n\ttree<int>::leaf_iterator li=tr.begin_leaf(it), li_tmp=tr.begin_leaf(tr.begin()), li_tmp2=li_tmp;\n\ttree<double>::leaf_iterator li_mrmr=tr_mrmr.begin_leaf(tr_mrmr.begin()), li_mrmr2=li_mrmr;\n\tint depth=tr.depth(li);\n\ttree<int>::pre_order_iterator it2;\n\tint vec_old[depth+1];\n\tint mat_res [power((maxnsol+1),(depth))][depth+2];\n\tint number_leafs=power((maxnsol+1),(depth));\n\tint to_remove[number_leafs];\n\n\tfor (int k=0; k< number_leafs ; k++) {\n\t\tto_remove[k]=0;\n\t}\n\tint cnt2=0,cnt_leafs=0;\n\n\twhile( li!=tr.end_leaf(it) ){\n\t\tvec_old[0]=*(li);\n\t\tit2=li;\n\n\t\twhile(it2!=tr.begin()){\n\t\t\tit2=tr.parent(it2);\n\t\t\tvec_old[cnt]=*(it2);\n\t\t\tcnt++;\n\t\t}\n\n\t\tsort(vec_old,vec_old+depth+1);\n\t\tmat_res[index][depth+1]=0;\n\n\t\tfor(int k=0;k<=depth;k++){\n\t\t\tmat_res[index][k]=vec_old[k];\n\t\t\tmat_res[index][depth+1]+=vec_old[k]+power(2,k);\n\t\t}\n\t\tindex++;\n\t\tcnt=1;\n\t\tli++;\n\t\tcnt_leafs++;\n\t}\n\n\n\tindex=0;\n\tbool found1=false,found2;\n\n\tfor(int k=0;k<(cnt_leafs-1) && !found1;k++){\n\t\tfor(int j=k+1;j<(cnt_leafs);j++){\n\t\t\tfound2=false;\n\t\t\tif(mat_res[k][depth+1]==mat_res[j][depth+1]){\n\t\t\t\tfor(int i=0;i<=depth && !found2;i++){\n\t\t\t\t\tif(mat_res[j][i]!=mat_res[k][i]){\n\t\t\t\t\t\tfound2=true;\n\t\t\t\t\t}\n\t\t\t\t}\n\t\t\t}else{\n\t\t\t\tfound2=true;\n\t\t\t}\n\t\t\tif(!found2){\n\t\t\t\tnumber_elements_to_remove++;\n\t\t\t\tint tmp=j;\n\t\t\t\twhile(tmp>0){\n\t\t\t\t\tli_tmp++,li_mrmr++;\n\t\t\t\t\ttmp--;\n\t\t\t\t}\n\n\n\t\t\t\ttmp=k;\n\t\t\t\twhile(tmp>0){\n\t\t\t\t\tli_tmp2++,li_mrmr2++;\n\t\t\t\t\ttmp--;\n\t\t\t\t}\n\n\t\t\t\tif(*(li_mrmr)< *(li_mrmr2)){\n\t\t\t\t\tto_remove[cnt2]=j;\n\t\t\t\t}else {\n\t\t\t\t\tto_remove[cnt2]=k;\n\t\t\t\t}\n\t\t\t\tcnt2++;\n\t\t\t}\n\t\t}\n\t}\n\n\tif(cnt2>0){\n\t\tli=tr.begin_leaf(tr.end());\n\t\tsort(to_remove,to_remove+cnt2);\n\t\tli_mrmr=tr_mrmr.begin_leaf(tr_mrmr.end());\n\t\tint cnt_back=cnt2;\n\t\twhile (cnt_leafs>=0 && cnt2>0) {\n\t\t\tit2=li;li_mrmr2=li_mrmr;\n\t\t\tli--;li_mrmr--;\n\t\t\twhile(to_remove[cnt2-1]==to_remove[cnt2] && cnt2!=cnt_back){\n\t\t\t\tcnt2--;\n\t\t\t}\n\t\t\tif(to_remove[cnt2-1]==cnt_leafs){\n\t\t\t\ttr.erase(it2);\n\t\t\t\ttr_mrmr.erase(li_mrmr2);\n\t\t\t\tcnt2--;\n\t\t\t}\n\t\t\tcnt_leafs--;\n\t\t}\n\t}\n\treturn number_elements_to_remove;\n}\n\n\nvoid mrmr_ensemble_one_gene_remove (tree<int>& res, tree<int>::pre_order_iterator one, double data[], int namat[], int nsamples,int n , int max_elements, int predn , int rep_boot, int maxnsol, double threshold, int *msurv, int *ustrat, int *cl2,\n\t\t\tdouble *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU){\n\t//n\t\t\t\t\tnumber of variables\n\t//predn:\t\t\tindex of target node\n\n\t// nsub: the variables which have been previously selected + target; prev_sel=nsub-target\n\t// number of samples to use for bootstrapping is equal to total number of samples\n\n\tint  *nsub, *prev_sel,nsamples_boot=nsamples, tmp_val_max_ind, *prev_sel_tmp,*vec_sol_local,ndelete;\n\tdouble *vec_mean, *vec_sort, *vec_sd,  *vec_local_max_mean, *vec_local_max_sd,tmp_val_max, *mrmr_vec_sort,*vec_sol_local_mrmr;\n\tdouble *mat_info;\n\n\n\tint cnt=0, max_elements_tmp=1; //current depth in the tree\n\tint *ind;\n\tvec_mean =(double*) R_alloc(n, sizeof(double));\n\tvec_sd =(double*) R_alloc(n, sizeof(double));\n\tmrmr_vec_sort =(double*) R_alloc(n, sizeof(double));\n\tvec_local_max_mean =(double*) R_alloc(max_elements, sizeof(double));\n\tvec_local_max_sd =(double*) R_alloc(max_elements, sizeof(double));\n\tmat_info =(double*) R_alloc((n*n), sizeof(double));\n\tind=(int*) R_alloc(nsamples, sizeof(int));\n\n\tfor(unsigned int i=1;i<= nsamples;++i){\n\t\tind[i-1]=i-1;\n\t}\n\tfor( unsigned int i=0; i< n; ++i ){\n\t\tfor( unsigned int j=0; j< n; ++j ){\n\t\t\tmat_info[i+(n)*j]=0;\n\t\t}\n\t}\n\n\tbuild_mim_cIndex_subset(mat_info, data, namat, n, nsamples, ind, nsamples ,msurv, ustrat, cl2,st, se, weights,strat, N, outx,  lenU);\n\n\tfor(unsigned int k=0;k< max_elements ;++k){\n\t\tvec_local_max_mean[k]=-1000;\n\t}\n\n\tprev_sel=(int*) R_alloc(max_elements, sizeof(int));\n\tnsub=(int*) R_alloc(max_elements, sizeof(int));\n\n\ttree<int> res_tmp_new=res ;\n\ttree<int>::iterator it_local=res_tmp_new.begin(),it_local2=it_local;\n\n\tvec_sol_local=(int*) R_alloc(maxnsol, sizeof(int));\n\tvec_sol_local_mrmr=(double*) R_alloc(maxnsol, sizeof(double));\n\t//mrmr score should not be predicted for the target node\n\tvec_mean[predn-1]=-1000; vec_sd[predn-1]=-1000;\n\tprev_sel[0]=0; nsub[0]=predn;\n\n\ttree<double> res_mrmr;\n\ttree<double>::iterator top_mrmr;\n\n\n\ttop_mrmr=res_mrmr.begin();\n\tres_mrmr.insert(top_mrmr, predn);\n\ttree<double>::iterator it_mrmr_local=res_mrmr.begin(),it_mrmr_local2=it_mrmr_local;\n\tint target_depth=max_elements, max_depth=0;\n\tint max_depth_local=2;\n\n\twhile (res_tmp_new.depth(it_local)<target_depth && it_local!=res_tmp_new.end()) {\n\t\tmax_depth=res_tmp_new.depth(it_local);\n\t\twhile(it_local!=res_tmp_new.end()) {\n\t\t\tif(cnt!=0){\n\t\t\t\tit_local2=it_local; it_mrmr_local2=it_mrmr_local;\n\t\t\t\twhile(res_tmp_new.depth(it_local2)<max_depth){\n\t\t\t\t\tit_local2++;it_mrmr_local2++;\n\n\t\t\t\t}\n\t\t\t\twhile(it_local2!=res_tmp_new.begin()){\n\t\t\t\t\tnsub[res_tmp_new.depth(it_local2)]=*(it_local2);\n\n\t\t\t\t\tit_local2=res_tmp_new.parent(it_local2);\n\t\t\t\t\tit_mrmr_local2=res_mrmr.parent(it_mrmr_local2);\n\t\t\t\t}\n\t\t\t}\n\t\t\tfor (unsigned int i=0;i<=max_depth;++i){\n\t\t\t\tprev_sel[i]=nsub[i+1];\n\t\t\t}\n\n\t\t\t////////////\n\t\t\t// initialize vec_mean and vec_sd for bootstrapping to -1000 if variable is not supposed to be tested (target or prev selected) otherwise 0\n\t\t\t////////////\n\n\t\t\tfor(unsigned int k=0;k< n;++k){\n\t\t\t\tvec_mean[k]=0;vec_sd[k]=0;\n\t\t\t}\n\t\t\tfor(unsigned int k=0;k<=max(res_tmp_new.depth(it_local),max_depth) ;++k){\n\t\t\t\tvec_mean[nsub[k]-1]=-1000;\tvec_sd[nsub[k]-1]=-1000;\n\t\t\t}\n\n\t\t\tfor(unsigned int k=0;k< n;++k){\n\t\t\t\tif(vec_mean[k]!= (-1000)){\n\t\t\t\t\tvec_mean[k]=mrnet_onegene( mat_info, n,min(cnt,max_elements_tmp),prev_sel, nsub[0], (k+1)); vec_sd[k]=0;\n\n\t\t\t\t}\n\t\t\t\tmrmr_vec_sort[k]=vec_mean[k];\n\t\t\t}\n\n\t\t\tsort(mrmr_vec_sort,mrmr_vec_sort+n);\n\n\t\t\ttmp_val_max=mrmr_vec_sort[n-maxnsol-1];\n\t\t\tint cnt_loop_max=0;\n\n\t\t\twhile (res_tmp_new.depth(it_local)<max_depth) {\n\t\t\t\tit_local++;it_mrmr_local++;\n\t\t\t}\n\t\t\tit_local2=it_local;it_mrmr_local2=it_mrmr_local;\n\t\t\tit_local2++;it_mrmr_local2++;\n\n\n\t\t\tfor(int k=0;k<n;k++){\n\t\t\t\tif(vec_mean[k]>tmp_val_max){\n\t\t\t\t\tvec_sol_local[cnt_loop_max]=k+1;\n\t\t\t\t\tvec_sol_local_mrmr[cnt_loop_max]=vec_mean[k];\n\t\t\t\t\tcnt_loop_max++;\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tfor(int k=maxnsol-1;k>=0;k--){\n\t\t\t\tres_tmp_new.append_child(it_local,vec_sol_local[k]);\n\t\t\t\tres_mrmr.append_child(it_mrmr_local,vec_sol_local_mrmr[k]);\n\t\t\t}\n\n\t\t\tif(res_tmp_new.depth(it_local)>0){\n\t\t\t\tit_local=it_local2;it_mrmr_local=it_mrmr_local2;\n\t\t\t}else{\n\t\t\t\tit_local++;it_mrmr_local++;\n\t\t\t}\n\t\t\tcnt++;\n\t\t}\n\t\tcnt++; max_elements_tmp++;\n\t\tndelete= -1;\n\t\twhile (ndelete!=0 ) {\n\t\t\tndelete=verify_equivalentset_nparents (res_tmp_new, res_tmp_new.begin(),res_tmp_new.end(),res_mrmr, maxnsol);\n\t\t}\n\n\t\tremove_childless_nodes(res_tmp_new, res_mrmr,max_depth_local+1);\n\n\t\tit_local=res_tmp_new.begin_leaf();it_mrmr_local=res_mrmr.begin_leaf();\n\t\tmax_depth_local++;\n\t}\n\tres=res_tmp_new;\n\n\tbootstrap_tree(res,res_mrmr, data, namat,  nsamples, n, rep_boot, msurv, ustrat, cl2,st, se, weights,strat, N, outx, lenS, lenU);\n\n}\n\n\nextern \"C\" SEXP\nmrmr_cIndex_ensemble_remove( SEXP Rdata, SEXP Rnamat, SEXP Rmaxparents, SEXP Rnvar, SEXP Rnsample, SEXP Rpredn, SEXP Rnpredn, SEXP Rrep_boot, SEXP Rmaxnsol, SEXP Rthreshold, SEXP Rmsurv, SEXP Rustrat, SEXP Rcl2, SEXP Rst, SEXP Rse, SEXP Rweights, SEXP Rstrat, SEXP RN, SEXP Routx, SEXP RlenS, SEXP RlenU){\n\t// Rdata:\t\tdata should be passed as vector, variable-wise appended\n\t// Rmaxparents:\tnumber of maximum number of parents\n\t// Rnvar:\t\tnumber of variables in the dataset\n\t// Rnsample:\tnumber of samples in the dataset\n\t// Rpredn:\t\tvector of target genes to consider\n\t// Rnpredn:\t\tnumber of target genes (number of elements in Rpredn)\n\t// Rrep_boot:\thow many bootstrap iterations\n\t// Rmaxnsol:\tmaximum number of children for each node at each step\n\n\tdouble *data, *threshold;\n\tconst int* maxparents, * nvar, *nsample, *maxnsol;\n\n\tint *predn, *rep_boot,*res,*res_all,*res_all2, *namat;\n\tint vec_tmp;\n\tconst int *npredn;\n\n\n\tdouble  *st, *weights;\n\tint *msurv, *ustrat, *cl2, *se, *strat, *N, *outx, *lenS, *lenU;\n\n\tSEXP Rres;\n\n\t//srand (time(NULL));\n\tPROTECT(Rdata = AS_NUMERIC(Rdata));\n\tPROTECT(Rnamat = AS_INTEGER(Rnamat));\n\tPROTECT(Rmaxparents= AS_INTEGER(Rmaxparents));\n\tPROTECT(Rnvar= AS_INTEGER(Rnvar));\n\tPROTECT(Rnsample= AS_INTEGER(Rnsample));\n\tPROTECT(Rpredn = AS_INTEGER(Rpredn));\n\tPROTECT(Rnpredn = AS_INTEGER(Rnpredn));\n\tPROTECT(Rrep_boot = AS_INTEGER(Rrep_boot));\n\tPROTECT(Rmaxnsol= AS_INTEGER(Rmaxnsol));\n\tPROTECT(Rthreshold= AS_NUMERIC(Rthreshold));\n\n\tdata=NUMERIC_POINTER(Rdata);\n\tnamat=INTEGER_POINTER(Rnamat);\n\tmaxparents = INTEGER_POINTER(Rmaxparents);\n\tnvar= INTEGER_POINTER(Rnvar);\n\tnsample= INTEGER_POINTER(Rnsample);\n\tpredn= INTEGER_POINTER(Rpredn);\n\tnpredn= INTEGER_POINTER(Rnpredn);\n\trep_boot= INTEGER_POINTER(Rrep_boot);\n\tmaxnsol= INTEGER_POINTER(Rmaxnsol);\n\tthreshold = NUMERIC_POINTER(Rthreshold);\n\n\tmsurv=INTEGER_POINTER(Rmsurv);\n\tustrat=INTEGER_POINTER(Rustrat);\n\tcl2=INTEGER_POINTER(Rcl2);\n\tst=NUMERIC_POINTER(Rst);\n\tse =INTEGER_POINTER(Rse);\n\tweights=NUMERIC_POINTER(Rweights);\n\tstrat=INTEGER_POINTER(Rstrat);\n\tN =INTEGER_POINTER(RN);\n\toutx=INTEGER_POINTER(Routx);\n\n\tlenS=INTEGER_POINTER(RlenS);\n\tlenU=INTEGER_POINTER(RlenU);\n\n\ttree<int> res_tree;\n\ttree<int>::iterator top,one;\n\ttree<int>::breadth_first_queued_iterator it_final;\n\n\ttop=res_tree.begin();\n\tint length_res=0;\n\tint length_res_old;\n\tfor(unsigned int i=0;i< *npredn;++i){\n\t//\tstd::cout<<\"model for node \"<<predn[i]<< \" is being built!\"<<std::endl;\n\t\tone=res_tree.insert(top, predn[i]);\n\n\t\t//build ensemble tree\n\t\tmrmr_ensemble_one_gene_remove(res_tree, one, data,namat,*nsample,(*nvar+1),*maxparents,predn[i],*rep_boot, *maxnsol, *threshold, msurv, ustrat, cl2,st, se, weights,strat, N, outx, *lenS, lenU);\n\n\t\t////////////////////////\n\t\t//convert tree to vector\n\t\t////////////////////////\n\t\tint *tmp_nchildren,*res_tmp;\n\t\tres_tmp=new int [2*(res_tree.size())+1];\n\t\ttmp_nchildren= new int [(res_tree.size())];\n\n\t\tit_final=res_tree.begin_breadth_first();\n\t\tint cnt=1,cnt2=0;\n\n\t\tres_tmp[0]=res_tree.size();\n\t\tint rootdepth=res_tree.depth(it_final);\n\t\twhile(it_final!=res_tree.end_breadth_first()) {\n\t\t\tres_tmp[cnt]=*it_final;\n\t\t\ttmp_nchildren[cnt-1]=(res_tree.number_of_children(it_final));\n\t\t\tcnt++;\n\t\t\t++it_final;\n\t\t}\n\t\t////////////////\n\t\t//save in final result vector\n\t\t////////////////\n\t\tlength_res_old=length_res;\n\t\tlength_res+=2*(res_tree.size())+1;\n\t\tint *res_all, *res_old;\n\t\tint ind=0;\n\t\tres_all=new int[length_res];\n\t\tif(length_res_old>0){\n\t\t\tfor(unsigned int k=0;k<length_res_old;k++){\n\t\t\t\tres_all[k]=res_old[k];\n\t\t\t}\n\t\t}\n\n\t\tfor(unsigned int k=0;k<=res_tree.size();k++){\n\t\t\tres_all[length_res_old+k]=res_tmp[k];\n\t\t}\n\t\tfor(unsigned int k=0;k<res_tree.size();k++){\n\t\t\tres_all[length_res_old+k+res_tree.size()+1]=tmp_nchildren[k];\n\t\t}\n\n\t\tdelete [] res_old;\n\t\tres_old=new int[length_res];\n\t\tfor(unsigned int k=0;k<length_res;k++){\n\t\t\tres_old[k]=res_all[k];\n\t\t}\n\n\t\tdelete [] res_all;\n\t\tif(i==(*npredn-1)){\n\n\t\t\tPROTECT(Rres = NEW_INTEGER(length_res));\n\t\t\tres = INTEGER_POINTER(Rres);\n\t\t\tfor(unsigned int k=0;k<length_res;k++){\n\t\t\t\tres[k]=res_old[k];\n\t\t\t}\n\t\t\tdelete [] res_old;\n\t\t}\n\n\t\t////////////////\n\t\t//erase old tree\n\t\t////////////////\n\n\t\tdelete [] tmp_nchildren;\n\t\tdelete [] res_tmp;\n\t\tres_tree.erase(res_tree.begin());\n\t}\n\tUNPROTECT(11);\n\n\treturn Rres;\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the get_correlation_ensemble function?",
        "answer": "The get_correlation_ensemble function computes the correlation between two variables in a dataset. It takes as input the data array, an array indicating missing values, indices of the two variables, and the number of samples. It calculates the means of the variables, then computes the correlation using the formula for Pearson correlation coefficient."
      },
      {
        "question": "How does the returnConcordanceIndexC function handle stratification?",
        "answer": "The returnConcordanceIndexC function handles stratification by iterating through each unique stratum (ustrat). For each stratum, it creates subsets of the data (xs, cls, sts, ses, weightss) containing only the samples belonging to that stratum. It then computes the concordance index for each stratum separately and combines the results."
      },
      {
        "question": "What is the purpose of the bootstrap_tree function and how does it work?",
        "answer": "The bootstrap_tree function performs bootstrap resampling to assess the stability of the tree structure. It iterates through the leaf nodes of the tree, computes bootstrap estimates of the mean and standard deviation of the MRMR scores for each leaf, and then prunes branches of the tree that are not statistically significant based on these bootstrap estimates. This helps to reduce overfitting and identify the most robust features in the model."
      }
    ],
    "completion_tasks": [
      {
        "partial": "double get_correlation_ensemble(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_x = 0, mean_y = 0, nom = 0, den_x = 0, den_y = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x+i] == 0 && namat[ind_y+i] == 0) {\n            mean_x += data[ind_x+i];\n            mean_y += data[ind_y+i];\n        }\n    }\n\n    mean_x /= size;\n    mean_y /= size;\n\n    // Complete the function by calculating correlation\n}",
        "complete": "double get_correlation_ensemble(double data[], int namat[], int ind_x, int ind_y, int size) {\n    double mean_x = 0, mean_y = 0, nom = 0, den_x = 0, den_y = 0;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x+i] == 0 && namat[ind_y+i] == 0) {\n            mean_x += data[ind_x+i];\n            mean_y += data[ind_y+i];\n        }\n    }\n\n    mean_x /= size;\n    mean_y /= size;\n\n    for (int i = 0; i < size; ++i) {\n        if (namat[ind_x+i] == 0 && namat[ind_y+i] == 0) {\n            double dx = data[ind_x+i] - mean_x;\n            double dy = data[ind_y+i] - mean_y;\n            nom += dx * dy;\n            den_x += dx * dx;\n            den_y += dy * dy;\n        }\n    }\n\n    return nom / sqrt(den_x * den_y);\n}"
      },
      {
        "partial": "void bootstrap_mrmr(double &mean, double &sd, double data[], int namat[], int size, int rep_boot, int size_boot, int nsamples, int var_target, int var_interest, int nprev_sel, int* var_ind, int *msurv, int *ustrat, int *cl2, double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU) {\n    int *ind = (int*) R_alloc(size_boot, sizeof(int));\n    double *boot_val = (double*) R_alloc(rep_boot, sizeof(double));\n    double *mat_info = (double*) R_alloc((size * size), sizeof(double));\n\n    for (int k = 0; k < rep_boot; ++k) {\n        // Select random samples\n        for (int i = 0; i < size_boot; ++i) {\n            ind[i] = (int)unif_rand() % nsamples;\n        }\n\n        // Initialize mat_info\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < size; ++j) {\n                mat_info[i + size * j] = 0;\n            }\n        }\n\n        // Complete the function by computing mrmr score\n    }\n\n    // Calculate mean and standard deviation\n}",
        "complete": "void bootstrap_mrmr(double &mean, double &sd, double data[], int namat[], int size, int rep_boot, int size_boot, int nsamples, int var_target, int var_interest, int nprev_sel, int* var_ind, int *msurv, int *ustrat, int *cl2, double *st, int *se, double *weights, int *strat, int *N, int *outx, int lenS, int *lenU) {\n    int *ind = (int*) R_alloc(size_boot, sizeof(int));\n    double *boot_val = (double*) R_alloc(rep_boot, sizeof(double));\n    double *mat_info = (double*) R_alloc((size * size), sizeof(double));\n\n    for (int k = 0; k < rep_boot; ++k) {\n        // Select random samples\n        for (int i = 0; i < size_boot; ++i) {\n            ind[i] = (int)unif_rand() % nsamples;\n        }\n\n        // Initialize mat_info\n        for (int i = 0; i < size; ++i) {\n            for (int j = 0; j < size; ++j) {\n                mat_info[i + size * j] = 0;\n            }\n        }\n\n        build_mim_cIndex_subset(mat_info, data, namat, size, nsamples, ind, size_boot, msurv, ustrat, cl2, st, se, weights, strat, N, outx, lenU);\n        boot_val[k] = mrnet_onegene(mat_info, size, nprev_sel, var_ind, var_target, var_interest);\n    }\n\n    // Calculate mean and standard deviation\n    mean = 0;\n    for (int i = 0; i < rep_boot; ++i) {\n        if (!isnan(boot_val[i])) {\n            mean += boot_val[i];\n        }\n    }\n    mean /= rep_boot;\n\n    sd = 0;\n    for (int i = 0; i < rep_boot; ++i) {\n        if (!isnan(boot_val[i])) {\n            sd += (boot_val[i] - mean) * (boot_val[i] - mean);\n        }\n    }\n    sd = sqrt(sd / rep_boot);\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/iauc.comp.R",
    "language": "R",
    "content": "`iauc.comp` <-\nfunction(auc1, auc2, time) {\n\tif((length(auc1) + length(auc2) + length(time)) != 3 * length(time)) { stop(\"auc1, auc2 and time must have the same length!\") }\n\tcc.ix <- complete.cases(auc1, auc2, time)\n\tauc1 <- auc1[cc.ix]\n\tauc2 <- auc2[cc.ix]\n\ttime <- time[cc.ix]\n\tdiffs <- c(time[1], time[2:length(time)] - time[1:(length(time) - 1)])\n\tiauc1 <- sum(diffs * auc1) / max(time)\n\tiauc2 <- sum(diffs * auc2) / max(time)\n\trr <- wilcox.test(x=auc1, y=auc2, alternative=\"greater\", paired=TRUE, exact=FALSE)\n\treturn(list(\"p.value\"=rr$p.value, \"iauc1\"=iauc1, \"iauc2\"=iauc2))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `iauc.comp` function and what are its input parameters?",
        "answer": "The `iauc.comp` function compares two sets of AUC (Area Under the Curve) values over time. It takes three input parameters: `auc1` and `auc2` (two vectors of AUC values to be compared) and `time` (a vector of corresponding time points). The function calculates the integrated AUC for both sets and performs a statistical test to compare them."
      },
      {
        "question": "How does the function handle missing or incomplete data in the input vectors?",
        "answer": "The function handles missing or incomplete data by using the `complete.cases()` function. It creates an index `cc.ix` of complete cases across all three input vectors (auc1, auc2, and time). Then, it subsets all three vectors using this index, effectively removing any rows with missing data before performing calculations."
      },
      {
        "question": "What statistical test is used to compare the AUC values, and what does the function return?",
        "answer": "The function uses the Wilcoxon signed-rank test (implemented through `wilcox.test()`) to compare the AUC values. It's a paired test with the alternative hypothesis that `auc1` is greater than `auc2`. The function returns a list containing three elements: the p-value from the Wilcoxon test, the integrated AUC for the first set (iauc1), and the integrated AUC for the second set (iauc2)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "iauc.comp <- function(auc1, auc2, time) {\n  if((length(auc1) + length(auc2) + length(time)) != 3 * length(time)) {\n    stop(\"auc1, auc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(auc1, auc2, time)\n  auc1 <- auc1[cc.ix]\n  auc2 <- auc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  iauc1 <- sum(diffs * auc1) / max(time)\n  iauc2 <- sum(diffs * auc2) / max(time)\n  # Complete the function by adding the Wilcoxon test and return statement\n}",
        "complete": "iauc.comp <- function(auc1, auc2, time) {\n  if((length(auc1) + length(auc2) + length(time)) != 3 * length(time)) {\n    stop(\"auc1, auc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(auc1, auc2, time)\n  auc1 <- auc1[cc.ix]\n  auc2 <- auc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  iauc1 <- sum(diffs * auc1) / max(time)\n  iauc2 <- sum(diffs * auc2) / max(time)\n  rr <- wilcox.test(x=auc1, y=auc2, alternative=\"greater\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"iauc1\"=iauc1, \"iauc2\"=iauc2))\n}"
      },
      {
        "partial": "iauc.comp <- function(auc1, auc2, time) {\n  # Add input validation\n  # Filter out incomplete cases\n  # Calculate time differences\n  # Calculate integrated AUCs\n  # Perform Wilcoxon test\n  # Return results\n}",
        "complete": "iauc.comp <- function(auc1, auc2, time) {\n  if(length(auc1) != length(auc2) || length(auc1) != length(time)) {\n    stop(\"auc1, auc2 and time must have the same length!\")\n  }\n  cc.ix <- complete.cases(auc1, auc2, time)\n  auc1 <- auc1[cc.ix]\n  auc2 <- auc2[cc.ix]\n  time <- time[cc.ix]\n  diffs <- c(time[1], diff(time))\n  iauc1 <- sum(diffs * auc1) / max(time)\n  iauc2 <- sum(diffs * auc2) / max(time)\n  rr <- wilcox.test(x=auc1, y=auc2, alternative=\"greater\", paired=TRUE, exact=FALSE)\n  return(list(\"p.value\"=rr$p.value, \"iauc1\"=iauc1, \"iauc2\"=iauc2))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/class-signature.R",
    "language": "R",
    "content": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.ToxicoSig <- setClass('ToxicoSig', slots=list(\n            Arguments = \"list\",\n            tSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n#' ToxicoSig Constructor\n#'\n#' A user friendly constructor to create ToxicoSig class objects. This function\n#'   is implemented as an internal and should only be called for development purposes\n#'\n#' @param Data `array`` An array contiaining the data for constructing the ToxicoSig object\n#' @param tSetName `character(1)` The name of the tSet used in the constructor\n#' @param DateCreated `date` The data at time of running the constructor\n#' @param SigType `character`A string of the experiment type\n#' @param SessionInfo `sessionInfo`The current session info\n#' @param Call `character(1)` A string\n#' @param Arguments `list` A list of arguments passed to the constructor\n#'\n#' @return `object` A new ToxicoSig object\n#'\n#' @keywords internal\n#' @export\nToxicoSig <- function(Data=array(NA, dim=c(0,0,0)), tSetName='', DateCreated=date(), SigType='sensitivity', SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n  return(.ToxicoSig(Data, Arguments = Arguments, tSetName=tSetName, DateCreated=DateCreated, SigType=SigType, SessionInfo=SessionInfo, Call=Call))}\n\n#' Show ToxicoGx Signatures\n#'\n#' @examples\n#' data(TGGATESsmall)\n#' drug.perturbation <- drugPerturbationSig(TGGATESsmall, mDataType=\"rna\", nthread = 1, duration = \"2\",\n#'      drugs = head(treatmentNames(TGGATESsmall)), features = fNames(TGGATESsmall, \"rna\")[seq_len(2)])\n#' drug.perturbation\n#'\n#' @param object \\code{ToxicoSig}\n#'\n#' @return Prints the ToxicoGx Signatures object to the output stream, and returns invisible NULL.\n#'\n#' @export\nsetMethod(\"show\", signature=signature(object='ToxicoSig'),\n          function(object) {\n            cat('ToxicoSet Name: ', attr(object, 'PSetName'), \"\\n\")\n            cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n            cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n            cat(\"Number of Drugs: \", dim(object)[[2]], \"\\n\")\n            cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n          })\n\n#' Show the Annotations of a signature object\n#'\n#' This funtion prints out the information about the call used to compute the drug signatures, and the session info\n#' for the session in which the computation was done. Useful for determining the exact conditions used to generate signatures.\n#'\n#' @examples\n#' data(TGGATESsmall)\n#' drug.perturbation <- drugPerturbationSig(TGGATESsmall, mDataType=\"rna\", nthread=1, duration = \"2\",\n#'      drugs = head(treatmentNames(TGGATESsmall)), features = fNames(TGGATESsmall, \"rna\")[seq_len(2)])\n#' showSigAnnot(drug.perturbation)\n#'\n#' @param Sigs An object of the \\code{ToxicoSig} Class, as returned by \\code{drugPerturbationSig}\n#'\n#' @return Prints the ToxicoGx Signatures annotations to the output stream, and returns invisible NULL.\n#'\n#' @export\nshowSigAnnot <- function(Sigs){\n\n  print(Sigs@Call)\n  print(Sigs@SessionInfo)\n  return(invisible(NULL))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ToxicoSig` class and its constructor function?",
        "answer": "The `ToxicoSig` class is designed to store and manage toxicogenomic signature data. Its constructor function creates a new `ToxicoSig` object with specified attributes such as data, tSet name, creation date, signature type, session info, and call information. This allows for organized storage and manipulation of toxicogenomic data within the R environment."
      },
      {
        "question": "How does the `show` method for the `ToxicoSig` class work, and what information does it display?",
        "answer": "The `show` method for the `ToxicoSig` class is implemented using `setMethod`. When called on a `ToxicoSig` object, it displays key information about the object, including the ToxicoSet name, signature type, creation date, number of drugs, and number of genes/probes. This provides a quick summary of the object's contents without revealing the entire dataset."
      },
      {
        "question": "What is the purpose of the `showSigAnnot` function, and how does it differ from the `show` method?",
        "answer": "The `showSigAnnot` function is designed to display detailed annotations of a `ToxicoSig` object. Unlike the `show` method, which provides a summary, `showSigAnnot` prints out the specific call used to compute the drug signatures and the session information in which the computation was done. This function is useful for determining the exact conditions used to generate signatures, aiding in reproducibility and debugging."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.ToxicoSig <- setClass('ToxicoSig', slots=list(\n            Arguments = \"list\",\n            tSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\nToxicoSig <- function(Data=array(NA, dim=c(0,0,0)), tSetName='', DateCreated=date(), SigType='sensitivity', SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n  # Complete the function body\n}",
        "complete": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.ToxicoSig <- setClass('ToxicoSig', slots=list(\n            Arguments = \"list\",\n            tSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\nToxicoSig <- function(Data=array(NA, dim=c(0,0,0)), tSetName='', DateCreated=date(), SigType='sensitivity', SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n  return(.ToxicoSig(Data, Arguments = Arguments, tSetName=tSetName, DateCreated=DateCreated, SigType=SigType, SessionInfo=SessionInfo, Call=Call))\n}"
      },
      {
        "partial": "setMethod(\"show\", signature=signature(object='ToxicoSig'),\n          function(object) {\n            # Complete the function body\n          })",
        "complete": "setMethod(\"show\", signature=signature(object='ToxicoSig'),\n          function(object) {\n            cat('ToxicoSet Name: ', attr(object, 'PSetName'), \"\\n\")\n            cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n            cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n            cat(\"Number of Drugs: \", dim(object)[[2]], \"\\n\")\n            cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n          })"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/censor.time.R",
    "language": "R",
    "content": "`censor.time` <-\nfunction(surv.time, surv.event, time.cens=0) {\n\tstc <- surv.time\n   sec <- surv.event\n   cc.ix <- complete.cases(stc, sec)\n   if(time.cens != 0) { \n   \tstc[cc.ix][surv.time[cc.ix] > time.cens] <- time.cens\n   \tsec[cc.ix][surv.time[cc.ix] > time.cens] <- 0\n   }\n   return(list(\"surv.time.cens\"=stc, \"surv.event.cens\"=sec))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `censor.time` function in R?",
        "answer": "The `censor.time` function is used to apply right censoring to survival data. It takes survival times and event indicators as input, and optionally a censoring time. If a censoring time is provided, it modifies the survival times and event indicators for observations that exceed this time, effectively censoring the data at that point."
      },
      {
        "question": "How does the function handle missing data in the input vectors?",
        "answer": "The function uses `complete.cases(stc, sec)` to identify rows where both survival time and event indicator are non-missing. This creates an index `cc.ix` which is then used to apply censoring only to complete cases, effectively ignoring any rows with missing data in either input vector."
      },
      {
        "question": "What does the function return, and in what format?",
        "answer": "The function returns a list containing two elements: 'surv.time.cens' and 'surv.event.cens'. These are the censored versions of the input survival times and event indicators, respectively. The return format allows easy access to both censored vectors while keeping them associated."
      }
    ],
    "completion_tasks": [
      {
        "partial": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  stc <- surv.time\n  sec <- surv.event\n  cc.ix <- complete.cases(stc, sec)\n  if(time.cens != 0) {\n    # Complete the censoring logic here\n  }\n  return(list(\"surv.time.cens\"=stc, \"surv.event.cens\"=sec))\n}",
        "complete": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  stc <- surv.time\n  sec <- surv.event\n  cc.ix <- complete.cases(stc, sec)\n  if(time.cens != 0) {\n    cens_idx <- cc.ix & surv.time > time.cens\n    stc[cens_idx] <- time.cens\n    sec[cens_idx] <- 0\n  }\n  return(list(\"surv.time.cens\"=stc, \"surv.event.cens\"=sec))\n}"
      },
      {
        "partial": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  # Initialize variables and handle censoring\n  # Return the censored data\n}",
        "complete": "censor.time <- function(surv.time, surv.event, time.cens=0) {\n  stc <- surv.time\n  sec <- surv.event\n  if(time.cens != 0) {\n    cc.ix <- complete.cases(stc, sec)\n    cens_idx <- which(cc.ix & surv.time > time.cens)\n    stc[cens_idx] <- time.cens\n    sec[cens_idx] <- 0\n  }\n  list(surv.time.cens=stc, surv.event.cens=sec)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/sbrier.score2proba.R",
    "language": "R",
    "content": "`sbrier.score2proba` <-\nfunction(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n\t## require(ipred)\n\tmethod <- match.arg(method)\n\t## remove missing values and sort the data for the test set\n\tcc.ix <- complete.cases(data.ts)\n\tot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n\tdata.ts <- data.ts[ot, ,drop=FALSE]\n\tsurv.time.ts <- data.ts$time\n\tsurv.event.ts <- data.ts$event\n\tscore.ts <- data.ts$score\n\tbtime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n\tutime <- unique(surv.time.ts[surv.event.ts == 1])\n\tbsc <- rep(NA, length(btime))\n\tswitch(method,\n\t\"cox\"={\n\t\t##require(survival)\n\t\t## fit the cox model for the training set\n\t\tcoxm <- survival::coxph(Surv(time, event) ~ score, data=data.tr)\n\t\t## compute survival probabilities using the cox model fitted on the training set and the score from the test set\n\t\t#sf <- survfit(coxm, newdata=data.ts)\n\t\tdd <- data.frame(\"score\"=score.ts)\n\t\tsf <- survfit(coxm, newdata=dd)\n\t\tfor(i in 1:length(utime)) {\n\t\t\tmypred <- getsurv2(sf=sf, time=utime[i])\n\t\t\tbsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n\t\t}\t\n\t},\n\t\"prodlim\"={\n\t\t#require(KernSmooth)\n\t\tprodlim.m <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr)\n\t\tlpred <- predict(prodlim.m, newdata=data.ts, times=utime)\n\t\tnames(lpred) <- dimnames(data.ts)[[1]]\n\t\tbsc <- rep(NA, length(btime))\n\t\tfor(i in 1:length(utime)) {\n\t\t\tmypred <- unlist(lapply(lpred, function(x, ix) { return(x[[ix]]) }, ix=i))\n\t\t\tbsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n\t\t}\n\t})\n\tif(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n\tdiffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n\tbsc.int <- sum(diffs * bsc)/max(btime)\n\treturn(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `sbrier.score2proba` function in this R code?",
        "answer": "The `sbrier.score2proba` function calculates the Brier score for survival data using either Cox proportional hazards model or product-limit estimation. It takes training and test datasets as input, computes survival probabilities, and returns time-dependent Brier scores along with an integrated Brier score."
      },
      {
        "question": "How does the function handle missing values in the test dataset?",
        "answer": "The function handles missing values in the test dataset by first removing incomplete cases using `complete.cases(data.ts)`. It then sorts the remaining data by time and adjusts the indices accordingly. This ensures that only complete cases are used in the subsequent calculations."
      },
      {
        "question": "What are the two methods available for calculating survival probabilities, and how are they implemented?",
        "answer": "The two methods available are 'cox' and 'prodlim'. For the 'cox' method, it fits a Cox proportional hazards model using the training data and computes survival probabilities for the test set using this model. For the 'prodlim' method, it uses product-limit estimation to calculate survival probabilities. The choice between these methods is made using the `method` argument and implemented through a `switch` statement."
      }
    ],
    "completion_tasks": [
      {
        "partial": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  # Complete the function body here\n  \n  if(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n  diffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n  bsc.int <- sum(diffs * bsc)/max(btime)\n  return(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}",
        "complete": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  switch(method,\n    \"cox\"={\n      coxm <- survival::coxph(Surv(time, event) ~ score, data=data.tr)\n      dd <- data.frame(\"score\"=score.ts)\n      sf <- survfit(coxm, newdata=dd)\n      for(i in 1:length(utime)) {\n        mypred <- getsurv2(sf=sf, time=utime[i])\n        bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n      }\n    },\n    \"prodlim\"={\n      prodlim.m <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr)\n      lpred <- predict(prodlim.m, newdata=data.ts, times=utime)\n      names(lpred) <- dimnames(data.ts)[[1]]\n      for(i in 1:length(utime)) {\n        mypred <- unlist(lapply(lpred, function(x, ix) { return(x[[ix]]) }, ix=i))\n        bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n      }\n    }\n  )\n  \n  if(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n  diffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n  bsc.int <- sum(diffs * bsc)/max(btime)\n  return(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}"
      },
      {
        "partial": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  if(method == \"cox\") {\n    # Complete the cox method here\n  } else {\n    # Complete the prodlim method here\n  }\n  \n  # Complete the final calculations here\n}",
        "complete": "sbrier.score2proba <- function(data.tr, data.ts, method=c(\"cox\", \"prodlim\")) {\n  method <- match.arg(method)\n  cc.ix <- complete.cases(data.ts)\n  ot <- order(data.ts$time)[1:(length(cc.ix)-sum(!cc.ix))]\n  data.ts <- data.ts[ot, ,drop=FALSE]\n  surv.time.ts <- data.ts$time\n  surv.event.ts <- data.ts$event\n  score.ts <- data.ts$score\n  btime <- surv.time.ts[surv.time.ts >= 0 & surv.time.ts <= max(surv.time.ts, na.rm=TRUE)]\n  utime <- unique(surv.time.ts[surv.event.ts == 1])\n  bsc <- rep(NA, length(btime))\n  \n  if(method == \"cox\") {\n    coxm <- survival::coxph(Surv(time, event) ~ score, data=data.tr)\n    dd <- data.frame(\"score\"=score.ts)\n    sf <- survfit(coxm, newdata=dd)\n    for(i in 1:length(utime)) {\n      mypred <- getsurv2(sf=sf, time=utime[i])\n      bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n    }\n  } else {\n    prodlim.m <- prodlim::prodlim(Surv(time, event) ~ score, data=data.tr)\n    lpred <- predict(prodlim.m, newdata=data.ts, times=utime)\n    names(lpred) <- dimnames(data.ts)[[1]]\n    for(i in 1:length(utime)) {\n      mypred <- unlist(lapply(lpred, function(x, ix) { return(x[[ix]]) }, ix=i))\n      bsc[is.na(bsc) & btime <= utime[i]] <- ipred::sbrier(obj=Surv(surv.time.ts, surv.event.ts), pred=mypred, btime=utime[i])\n    }\n  }\n  \n  if(sum(is.na(bsc)) > 0) { bsc[is.na(bsc)] <- bsc[ min(which(is.na(bsc)))-1] } \n  diffs <- c(btime[1], btime[2:length(btime)] - btime[1:(length(btime) - 1)])\n  bsc.int <- sum(diffs * bsc)/max(btime)\n  return(list(\"time\"=btime, \"bsc\"=bsc, \"bsc.integrated\"=bsc.int))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/ToxicoGx.git",
    "file": "../../../../repos/ToxicoGx/R/drugTimeResponseCurve.R",
    "language": "R",
    "content": "#' Compares viabilities at a given dose over different experimental durations\n#'\n#' This function generates a plot visualizing the relationship between gene\n#'   expression, time and dose level for the selected tSet. The plot is generated\n#'   with ggplot2 and can be customized using ggplot plot + function() syntax.\n#'\n#' @examples\n#'   library(ggplot2)\n#'\n#'   # Default settings\n#'   plot <- drugTimeResponseCurve(TGGATESsmall, cell_lines = \"Hepatocyte\",\n#'   dose = c(\"Control\", \"Low\", \"Middle\"), drugs = treatmentNames(TGGATESsmall)[6],\n#'   duration = c(\"2\", \"8\", \"24\"))\n#'\n#'   # Customize title, x/y labels, x/y limits, colour palette and define\n#'   # custom ticks for x axis using the function argument ggplot2_args\n#'   customizations <- list(labs(title= 'My Custom Title', ylab = 'The y-axis'),\n#'                          xlim(c(2, 24)), ylim(c(99,105)),\n#'                          scale_color_brewer(palette=\"Set1\"),\n#'                          scale_x_continuous(breaks=c(2, 8, 24),\n#'                            labels = c(\"Two\", \"Eight\", \"Twenty-Four\"))\n#'                          )\n#'\n#'    if(interactive()) {\n#'       drugTimeResponseCurve(TGGATESsmall, cell_lines = \"Hepatocyte\",\n#'         dose = c(\"Control\", \"Low\", \"Middle\"),\n#'         drugs = treatmentNames(TGGATESsmall)[6], duration = c(\"2\", \"8\", \"24\"),\n#'         ggplot_args = customizations)\n#'    }\n#'\n#'    # Customize the plot using standard ggplot2 syntax\n#'    if(interactive()) {\n#'       plot + labs(title= 'My Custom Title', ylab = 'The y-axis') +\n#'         xlim(c(2, 24)) + ylim(c(99,105)) + scale_color_brewer(palette=\"Set1\")\n#'    }\n#'\n#' @param tSet \\code{ToxicoSet} A ToxicoSet to be plotted in\n#'   this figure\n#' @param dose \\code{character} A vector of dose levels to be included in the\n#'   plot. Default to include all dose levels available for a drug. Must include\n#'   at minimum two dose levels, one of witch is \"Control\".\n#' @param drugs \\code{character} A drugs or pair of drugs to be plotted.\n#' @param duration \\code{character} A vector of durations to include in the plot.\n#' @param summarize_replicates \\code{logical} If TRUE will average viability\n#'   across replicates for each unique drug-dose-duration combination.\n#' @param cell_lines \\code{character} A vector of cell lines to include in the\n#'   plot.\n#' @param line_width \\code{numeric} A number specifying the thickness of lines\n#'   in the plot, as passed to size in geom_line(). Defaults to 1.\n#' @param point_size \\code{numeric} A number specifying how large points should\n#'   be in the plot, as passed to size in geom_point(). Defaults to 2.5.\n#' @param verbose \\code{boolean} Should warning messages about the data passed\n#'   in be printed?\n#' @param ggplot_args \\code{list} A list of ggplot2 functions which can be\n#'   called using the plot + function() syntax. This allows arbitrary\n#'   customization of the plot including changing the title, axis labels,\n#'   colours, etc. Please see the included examples for basic usage or ggplot2\n#'   documentation for advanced customization. Alternatively, you could assign\n#'   the return value to a variable and add the customization yourself using\n#'   plot + function().\n#'\n#' @return Plot of the viabilities for each drugs vs time of exposure\n#'\n#' @import ggplot2\n#' @importFrom magrittr %<>%\n#' @importFrom dplyr %>% filter group_by mutate\n#' @importFrom tidyr gather\n#'\n#' @export\ndrugTimeResponseCurve <- function(\n  tSet,\n  duration = NULL,\n  cell_lines = NULL,\n  dose = NULL,\n  drugs = NULL,\n  summarize_replicates = TRUE,\n  line_width = 1,\n  point_size = 2.5,\n  verbose=TRUE,\n  ggplot_args=NULL\n) {\n  # Place tSet in a list if not already\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n\n  paramErrorChecker(\"drugTimeResponseCurve\",\n                    tSets = tSet, drugs = drugs, duration = duration,\n                    cell_lines = cell_lines, dose = dose)\n\n  ## TODO:: Throw warning if a dose level or time point is not available for\n    # a specific drug\n\n  # Subsetting the tSet based on parameter arguments\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs,\n                               duration = duration, cells = cell_lines)})\n  })\n\n  # Gather data for the plot\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  for (data in plotData) {\n    if (summarize_replicates) {\n      data %<>% group_by(dose_level, duration_h) %>% mutate(viability = mean(viability))\n      plot <- ggplot(as_tibble(data) %>% filter(replicate == 1),\n                     aes(as.numeric(duration_h), viability, color = dose_level)) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    } else {\n      plot <- ggplot(as_tibble(data), aes(as.numeric(duration_h), viability,\n                                          color = dose_level,\n                                          shape = as.factor(replicate),\n                                          linetype = as.factor(replicate))) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    }\n  }\n\n  plot <- plot + labs(\n    title = paste0(\"Drug Response Curve for \",\n                   paste(drugs, collapse = \" & \"), \" in \",\n                   paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n    color = \"Dose Level\",\n    shape = \"Replicate\"\n  ) +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    ) +\n    xlab(\"Duration (hrs)\") +\n    ylab(\"Viability (%)\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  # Pass in any additional ggplot2 customizations\n  if (!(is.null(ggplot_args))) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `drugTimeResponseCurve` function, and what type of visualization does it generate?",
        "answer": "The `drugTimeResponseCurve` function generates a plot that visualizes the relationship between gene expression, time, and dose level for a selected ToxicoSet (tSet). It creates a line plot using ggplot2 to show how viability changes over different experimental durations for various dose levels of a drug or pair of drugs."
      },
      {
        "question": "How does the function handle data for multiple replicates when `summarize_replicates` is set to TRUE?",
        "answer": "When `summarize_replicates` is set to TRUE, the function averages the viability across replicates for each unique drug-dose-duration combination. It does this by grouping the data by dose_level and duration_h, then calculating the mean viability. The plot is then generated using only the first replicate's data points, as they now represent the average values."
      },
      {
        "question": "What customization options does the function provide for the generated plot, and how can users apply these customizations?",
        "answer": "The function provides several customization options: 1) Users can adjust line width and point size using the `line_width` and `point_size` parameters. 2) The `ggplot_args` parameter allows passing a list of ggplot2 functions for further customization (e.g., changing title, axis labels, colors). 3) Users can also customize the plot after it's generated using standard ggplot2 syntax by assigning the return value to a variable and adding customizations with the + operator."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  # Complete the function by adding the plotting logic\n}",
        "complete": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  for (data in plotData) {\n    if (summarize_replicates) {\n      data %<>% group_by(dose_level, duration_h) %>% mutate(viability = mean(viability))\n      plot <- ggplot(as_tibble(data) %>% filter(replicate == 1),\n                     aes(as.numeric(duration_h), viability, color = dose_level)) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    } else {\n      plot <- ggplot(as_tibble(data), aes(as.numeric(duration_h), viability,\n                                          color = dose_level,\n                                          shape = as.factor(replicate),\n                                          linetype = as.factor(replicate))) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    }\n  }\n\n  plot <- plot + labs(\n    title = paste0(\"Drug Response Curve for \",\n                   paste(drugs, collapse = \" & \"), \" in \",\n                   paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n    color = \"Dose Level\",\n    shape = \"Replicate\"\n  ) +\n    theme(\n      plot.title = element_text(hjust = 0.5, size = 14)\n    ) +\n    xlab(\"Duration (hrs)\") +\n    ylab(\"Viability (%)\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  if (!(is.null(ggplot_args))) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}"
      },
      {
        "partial": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  # Place tSet in a list if not already\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  # Subsetting the tSet based on parameter arguments\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  # Gather data for the plot\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  # Complete the function by adding the plotting logic and returning the plot\n}",
        "complete": "drugTimeResponseCurve <- function(tSet, duration = NULL, cell_lines = NULL, dose = NULL, drugs = NULL, summarize_replicates = TRUE, line_width = 1, point_size = 2.5, verbose=TRUE, ggplot_args=NULL) {\n  if (!is(tSet, \"list\")) {\n    tSet <- list(tSet)\n  }\n\n  paramErrorChecker(\"drugTimeResponseCurve\", tSets = tSet, drugs = drugs, duration = duration, cell_lines = cell_lines, dose = dose)\n\n  tSet <- lapply(tSet, function(tSet) {\n    suppressWarnings({subsetTo(tSet, mDataType = \"rna\", drugs = drugs, duration = duration, cells = cell_lines)})\n  })\n\n  plotData <- lapply(tSet, function(tSet) {\n    sInfo <- sensitivityInfo(tSet)[, seq_len(4)]\n    sValues <- sensitivityRaw(tSet)[,,2]\n    plotData <- cbind(sInfo, sValues)\n    cols <- c('Low', 'Middle', 'High')\n    colnames(plotData)[which(colnames(plotData) %in% c('doses1', 'doses2', 'doses3'))] <-\n      cols[which(c('doses1', 'doses2', 'doses3') %in% colnames(plotData))]\n    plotData %<>% gather('dose_level', 'viability', Control, Low, Middle, High)\n  })\n\n  for (data in plotData) {\n    if (summarize_replicates) {\n      data %<>% group_by(dose_level, duration_h) %>% mutate(viability = mean(viability))\n      plot <- ggplot(as_tibble(data) %>% filter(replicate == 1),\n                     aes(as.numeric(duration_h), viability, color = dose_level)) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    } else {\n      plot <- ggplot(as_tibble(data), aes(as.numeric(duration_h), viability,\n                                          color = dose_level,\n                                          shape = as.factor(replicate),\n                                          linetype = as.factor(replicate))) +\n        geom_point(size = point_size) +\n        geom_line(size = line_width)\n    }\n  }\n\n  plot <- plot + labs(\n    title = paste0(\"Drug Response Curve for \",\n                   paste(drugs, collapse = \" & \"), \" in \",\n                   paste(cell_lines, collapse = \" & \"), collapse = \" & \"),\n    color = \"Dose Level\",\n    shape = \"Replicate\"\n  ) +\n    theme(plot.title = element_text(hjust = 0.5, size = 14)) +\n    xlab(\"Duration (hrs)\") +\n    ylab(\"Viability (%)\") +\n    scale_x_continuous(breaks=as.numeric(duration), labels = duration)\n\n  if (!(is.null(ggplot_args))) {\n    plot <- plot + ggplot_args\n  }\n  plot\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/stab.fs.R",
    "language": "R",
    "content": "#' @title Function to quantify stability of feature selection\n#'\n#' @description\n#' This function computes several indexes to quantify feature selection\n#'   stability. This is usually estimated through perturbation of the original\n#'   dataset by generating multiple sets of selected features.\n#'\n#' @usage\n#' stab.fs(fsets, N, method = c(\"kuncheva\", \"davis\"), ...)\n#'\n#' @param fsets\tlist of sets of selected features, each set of selected\n#'   features may have different size.\n#' @param N\ttotal number of features on which feature selection is performed.\n#' @param method\tstability index (see details section).\n#' @param ...\tadditional parameters passed to stability index (penalty\n#'   that is a numeric for Davis' stability index, see details section).\n#'\n#' @details\n#' Stability indices may use different parameters. In this version only the\n#'   Davis index requires an additional parameter that is penalty, a numeric\n#'   value used as penalty term.\n#' Kuncheva index (kuncheva) lays in \\[-1, 1\\], An index of -1 means no\n#'   intersection between sets of selected features, +1 means that all the\n#'   same features are always selected and 0 is the expected stability of a\n#'   random feature selection.\n#' Davis index (davis) lays in \\[0,1\\], With a penalty term equal to 0, an\n#'   index of 0 means no intersection between sets of selected features\n#'   and +1 means that all the same features are always selected. A penalty\n#'   of 1 is usually used so that a feature selection performed with no or\n#'   all features has a Davis stability index equals to 0. None estimate of\n#'   the expected Davis stability index of a random feature selection was\n#'   published.\n#'\n#' @return\n#' A numeric that is the stability index.\n#'\n#' @references\n#' Davis CA, Gerick F, Hintermair V, Friedel CC, Fundel K, Kuffner R, Zimmer R\n#'   (2006) \"Reliable gene signatures for microarray classification: assessment\n#'   of stability and performance\", Bioinformatics, 22(19):356-2363.\n#' Kuncheva LI (2007) \"A stability index for feature selection\", AIAP'07:\n#'   Proceedings of the 25th conference on Proceedings of the 25th IASTED\n#'   International Multi-Conference, pages 390-395.\n#'\n#' @seealso\n#' [genefu::stab.fs.ranking]\n#'\n#' @examples\n#' set.seed(54321)\n#' # 100 random selection of 50 features from a set of 10,000 features\n#' fsets <- lapply(as.list(1:100), function(x, size=50, N=10000) {\n#'   return(sample(1:N, size, replace=FALSE))} )\n#' names(fsets) <- paste(\"fsel\", 1:length(fsets), sep=\".\")\n#'\n#' # Kuncheva index\n#' stab.fs(fsets=fsets, N=10000, method=\"kuncheva\")\n#' # close to 0 as expected for a random feature selection\n#'\n#' # Davis index\n#' stab.fs(fsets=fsets, N=10000, method=\"davis\", penalty=1)\n#'\n#' @md\n#' @export\nstab.fs <-\nfunction(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n\n\t####################\n\t## internal functions\n\t####################\n\n\tkuncheva.stab <- function(fsets, N) {\n\t\tkk <- length(fsets)\n\t\tKI <- function(f1, f2, ss, NN) {\n\t\t\t#if(length(f1) != length(f2)) { stop(\"length of the two sets of selected features must be identical!\") }\n\t\t\t#ss <- length(f1)\n\t\t\tif(ss == NN) { return(NA) }\n\t\t\trr <- length(intersect(f1, f2))\n\t\t\tki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n\t\t\treturn(ki.est)\n\t\t}\n\t\tss <- unique(unlist(lapply(fsets, length)))\n\t\tif(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n\t\tstab.res <- 0\n\t\tfor(i in 1:(kk - 1)) {\n\t\t\tfor(j in (i + 1):kk) {\n\t\t\t\tstab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n\t\t\t}\n\t\t}\n\t\treturn((2 * stab.res) / (kk * (kk - 1)))\n\t}\n\n\tdavis.stab <- function(fsets, N, penalty=1) {\n\t\tkk <- length(fsets)\n\t\tss <- unique(unlist(lapply(fsets, length)))\n\t\tif(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n\t\tstab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n\t\treturn(stab.res - penalty * (ss / N))\n\t}\n\n\t####################\n\n\tmethod <- match.arg(method)\n\tif(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n\tswitch(method,\n\t\t\"kuncheva\"={\n\t\t\tstab <- kuncheva.stab(fsets=fsets, N=N)\n\t\t},\n\t\t\"davis\"={\n\t\t\t stab <- davis.stab(fsets=fsets, N=N, ...)\n\t\t})\n\t\treturn(stab)\n}\n\n## k <- 1000; fsets <- NULL; for(i in 1:k) { fsets <- c(fsets, list(sample(paste(\"feature\", 1:10000, sep=\".\"), 200))) }; names(fsets) <- paste(\"rand\", 1:k, sep=\".\")\n## kuncheva.stab(fsets=fsets, N=10000)\n## davis.stab(fsets=fsets, N=10000, penalty=1)",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `stab.fs` function and what are the two stability indices it can calculate?",
        "answer": "The `stab.fs` function quantifies the stability of feature selection. It can calculate two stability indices: Kuncheva index and Davis index. The Kuncheva index ranges from -1 to 1, where -1 indicates no intersection between sets of selected features, 1 means the same features are always selected, and 0 is the expected stability of random feature selection. The Davis index ranges from 0 to 1, where 0 (with penalty term 0) means no intersection between sets, and 1 means the same features are always selected."
      },
      {
        "question": "How does the function handle different lengths of feature sets for the Kuncheva index calculation?",
        "answer": "The function checks if all feature sets have the same length using `ss <- unique(unlist(lapply(fsets, length)))`. If `length(ss) > 1`, it throws an error with the message 'length of sets of selected features must be identical!'. This ensures that the Kuncheva index is only calculated for feature sets of equal length, which is a requirement for this stability measure."
      },
      {
        "question": "What is the significance of the `penalty` parameter in the Davis stability index calculation?",
        "answer": "The `penalty` parameter in the Davis stability index calculation is used to adjust the index based on the number of selected features relative to the total number of features. It's subtracted from the initial stability calculation as `penalty * (ss / N)`, where `ss` is the number of selected features and `N` is the total number of features. A penalty of 1 is typically used to ensure that feature selections with no features or all features result in a Davis stability index of 0. This penalization helps to discourage trivial feature selections."
      }
    ],
    "completion_tasks": [
      {
        "partial": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    kk <- length(fsets)\n    KI <- function(f1, f2, ss, NN) {\n      if(ss == NN) { return(NA) }\n      rr <- length(intersect(f1, f2))\n      ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n      return(ki.est)\n    }\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- 0\n    for(i in 1:(kk - 1)) {\n      for(j in (i + 1):kk) {\n        stab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n      }\n    }\n    return((2 * stab.res) / (kk * (kk - 1)))\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    # Complete this function\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}",
        "complete": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    kk <- length(fsets)\n    KI <- function(f1, f2, ss, NN) {\n      if(ss == NN) { return(NA) }\n      rr <- length(intersect(f1, f2))\n      ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n      return(ki.est)\n    }\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- 0\n    for(i in 1:(kk - 1)) {\n      for(j in (i + 1):kk) {\n        stab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n      }\n    }\n    return((2 * stab.res) / (kk * (kk - 1)))\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    kk <- length(fsets)\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n    return(stab.res - penalty * (ss / N))\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}"
      },
      {
        "partial": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    # Complete this function\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    kk <- length(fsets)\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n    return(stab.res - penalty * (ss / N))\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}",
        "complete": "stab.fs <- function(fsets, N, method=c(\"kuncheva\", \"davis\"), ...) {\n  kuncheva.stab <- function(fsets, N) {\n    kk <- length(fsets)\n    KI <- function(f1, f2, ss, NN) {\n      if(ss == NN) { return(NA) }\n      rr <- length(intersect(f1, f2))\n      ki.est <- (rr - (ss^2 / NN)) / (ss - (ss^2 / NN))\n      return(ki.est)\n    }\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- 0\n    for(i in 1:(kk - 1)) {\n      for(j in (i + 1):kk) {\n        stab.res <- stab.res + KI(f1=fsets[[i]], f2=fsets[[j]], ss=ss, NN=N)\n      }\n    }\n    return((2 * stab.res) / (kk * (kk - 1)))\n  }\n\n  davis.stab <- function(fsets, N, penalty=1) {\n    kk <- length(fsets)\n    ss <- unique(unlist(lapply(fsets, length)))\n    if(length(ss) > 1) { stop(\"length of sets of selected features must be identical!\") }\n    stab.res <- sum(sort(table(unlist(fsets)), decreasing=TRUE)[1:ss]) / (kk * ss)\n    return(stab.res - penalty * (ss / N))\n  }\n\n  method <- match.arg(method)\n  if(!is.list(fsets)) { stop(\"fsets must be a list of sets of selected features!\") }\n  switch(method,\n    \"kuncheva\"={\n      stab <- kuncheva.stab(fsets=fsets, N=N)\n    },\n    \"davis\"={\n      stab <- davis.stab(fsets=fsets, N=N, ...)\n    })\n  return(stab)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/survcomp.git",
    "file": "../../../../repos/survcomp/R/combine.test.R",
    "language": "R",
    "content": "'combine.test' <-\nfunction(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n\tif(hetero) { stop(\"function to deal with heterogeneity is not implemented yet!\") }\n\tmethod <- match.arg(method)\n\tna.ix <- is.na(p)\n\tif(any(na.ix) && !na.rm) { stop(\"missing values are present!\") }\n\tif(all(na.ix)) { return(NA) } ## all p-values are missing\n\tp <- p[!na.ix]\n\tk <- length(p)\n\tif(k == 1) { return(p) }\n\tif(missing(weight)) { weight <- rep(1, k); }\n\tswitch(method,  \n\t\"fisher\"={\n\t\tcp <- pchisq(-2 * sum(log(p)), df=2*k, lower.tail=FALSE)\n\t}, \n\t\"z.transform\"={\n\t\tz <- qnorm(p, lower.tail=FALSE)\n\t\tcp <- pnorm(sum(weight * z) / sqrt(sum(weight^2)), lower.tail=FALSE)\n\t}, \n\t\"logit\"={\n\t\ttt <- (- sum(log(p / (1 - p)))) / sqrt(k * pi^2 * (5 * k + 2) / (3 * (5 * k + 4)))\n\t\tcp <- pt(tt,df=5*k+4, lower.tail=FALSE)\n\t})\n\treturn(cp)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'combine.test' function in R, and what are the different methods it supports for combining p-values?",
        "answer": "The 'combine.test' function in R is designed to combine multiple p-values into a single p-value. It supports three methods for combining p-values: Fisher's method, Z-transform method, and logit method. These methods are specified using the 'method' parameter, which can be set to 'fisher', 'z.transform', or 'logit'."
      },
      {
        "question": "How does the function handle missing values (NA) in the input p-values?",
        "answer": "The function handles missing values (NA) in the input p-values as follows: 1) If any NA values are present and na.rm=FALSE (default), the function stops with an error message. 2) If na.rm=TRUE, the function removes NA values before processing. 3) If all p-values are NA, the function returns NA. 4) The function uses logical indexing to exclude NA values from further calculations."
      },
      {
        "question": "Explain the implementation of the Z-transform method in the 'combine.test' function.",
        "answer": "The Z-transform method in the 'combine.test' function is implemented as follows: 1) It converts p-values to Z-scores using the inverse normal distribution (qnorm function). 2) It calculates a weighted sum of these Z-scores. 3) It divides this sum by the square root of the sum of squared weights. 4) Finally, it converts the result back to a p-value using the standard normal distribution (pnorm function). This method allows for weighted combination of p-values, with weights specified by the 'weight' parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  if(hetero) { stop(\"function to deal with heterogeneity is not implemented yet!\") }\n  method <- match.arg(method)\n  na.ix <- is.na(p)\n  if(any(na.ix) && !na.rm) { stop(\"missing values are present!\") }\n  if(all(na.ix)) { return(NA) }\n  p <- p[!na.ix]\n  k <- length(p)\n  if(k == 1) { return(p) }\n  if(missing(weight)) { weight <- rep(1, k) }\n  switch(method,\n    \"fisher\"={\n      # Complete the Fisher's method calculation\n    },\n    \"z.transform\"={\n      # Complete the Z-transform method calculation\n    },\n    \"logit\"={\n      # Complete the logit method calculation\n    })\n  return(cp)\n}",
        "complete": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  if(hetero) { stop(\"function to deal with heterogeneity is not implemented yet!\") }\n  method <- match.arg(method)\n  na.ix <- is.na(p)\n  if(any(na.ix) && !na.rm) { stop(\"missing values are present!\") }\n  if(all(na.ix)) { return(NA) }\n  p <- p[!na.ix]\n  k <- length(p)\n  if(k == 1) { return(p) }\n  if(missing(weight)) { weight <- rep(1, k) }\n  switch(method,\n    \"fisher\"={\n      cp <- pchisq(-2 * sum(log(p)), df=2*k, lower.tail=FALSE)\n    },\n    \"z.transform\"={\n      z <- qnorm(p, lower.tail=FALSE)\n      cp <- pnorm(sum(weight * z) / sqrt(sum(weight^2)), lower.tail=FALSE)\n    },\n    \"logit\"={\n      tt <- (-sum(log(p / (1 - p)))) / sqrt(k * pi^2 * (5 * k + 2) / (3 * (5 * k + 4)))\n      cp <- pt(tt, df=5*k+4, lower.tail=FALSE)\n    })\n  return(cp)\n}"
      },
      {
        "partial": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  # Add input validation and preprocessing\n  \n  # Implement the chosen method\n  cp <- switch(method,\n    \"fisher\"={\n      # Fisher's method\n    },\n    \"z.transform\"={\n      # Z-transform method\n    },\n    \"logit\"={\n      # Logit method\n    })\n  \n  return(cp)\n}",
        "complete": "combine.test <- function(p, weight, method=c(\"fisher\", \"z.transform\", \"logit\"), hetero=FALSE, na.rm=FALSE) {\n  if(hetero) stop(\"function to deal with heterogeneity is not implemented yet!\")\n  method <- match.arg(method)\n  na.ix <- is.na(p)\n  if(any(na.ix) && !na.rm) stop(\"missing values are present!\")\n  if(all(na.ix)) return(NA)\n  p <- p[!na.ix]\n  k <- length(p)\n  if(k == 1) return(p)\n  if(missing(weight)) weight <- rep(1, k)\n  \n  cp <- switch(method,\n    \"fisher\" = pchisq(-2 * sum(log(p)), df=2*k, lower.tail=FALSE),\n    \"z.transform\" = {\n      z <- qnorm(p, lower.tail=FALSE)\n      pnorm(sum(weight * z) / sqrt(sum(weight^2)), lower.tail=FALSE)\n    },\n    \"logit\" = {\n      tt <- (-sum(log(p / (1 - p)))) / sqrt(k * pi^2 * (5 * k + 2) / (3 * (5 * k + 4)))\n      pt(tt, df=5*k+4, lower.tail=FALSE)\n    })\n  \n  return(cp)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/genefu.git",
    "file": "../../../../repos/genefu/R/npi.R",
    "language": "R",
    "content": "#' @title Function to compute the Nottingham Prognostic Index\n#'\n#' @description\n#' This function computes the Nottingham Prognostic Index (NPI) as published\n#'   in Galeat et al, 1992. NPI is a clinical index shown to be highly prognostic \n#'   in breast cancer.\n#'\n#' @usage\n#' npi(size, grade, node, na.rm = FALSE)\n#'\n#' @param size tumor size in cm.\n#' @param grade\tHistological grade, i.e. low (1), intermediate (2) and high (3) grade.\n#' @param node Nodal status. If only binary nodal status (0/1) is available, \n#'   map 0 to 1 and 1 to 3.\n#' @param na.rm\tTRUE if missing values should be removed, FALSE otherwise.\n#'\n#' @details\n#' The risk prediction is either Good if score < 3.4, Intermediate \n#'   if 3.4 <= score <- 5.4, or Poor if score > 5.4.\n#'\n#' @return\n#' A list with items:\n#' - score: Continuous signature scores\n#' - risk: Binary risk classification, 1 being high risk and 0 being low risk.\n\n\n#' @references\n#' Galea MH, Blamey RW, Elston CE, and Ellis IO (1992) \"The nottingham \n#'   prognostic index in primary breast cancer\", Breast Cancer Reasearch \n#'   and Treatment, 22(3):207-219.\n#'\n#' @seealso\n#' [genefu::st.gallen]\n#'\n#' @examples\n#' # load NKI dataset\n#' data(nkis)\n#' # compute NPI score and risk classification\n#' npi(size=demo.nkis[ ,\"size\"], grade=demo.nkis[ ,\"grade\"],\n#'   node=ifelse(demo.nkis[ ,\"node\"] == 0, 1, 3), na.rm=TRUE)\n#' \n#' @md\n#' @export\nnpi <-\nfunction(size, grade, node, na.rm=FALSE) {\n\n\tnn <- names(size)\n\tif(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n\tcc.ix <- complete.cases(size, grade, node)\n\tif(all(!cc.ix)) {\n\t\ttt <- rep(NA, length(size))\n\t\tnames(tt) <- nn\n\t\treturn(list(\"score\"=tt, \"risk\"=tt))\n\t}\n\tsize <- size[cc.ix]\n\tgrade <- grade[cc.ix]\n\tnode <- node[cc.ix]\n\t\n\tif(length(size) != length(grade) || length(grade) != length(node)) {\n\t\tstop(\"size, grade and lymph node stage must have the same length!\")\n\t}\n\tif(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n\tif(!all(is.element(grade, c(\"1\", \"2\", \"3\")))) {\n\t\tstop(\"grade must be 1, 2 or 3!\")\n\t}\n\tif(!all(is.element(node, c(\"1\", \"2\", \"3\")))) {\n\t\t#if only \"0\" and \"1\" are available, map \"0\" -> \"1\" and \"1\" -> \"3\"\n\t\tstop(\"lymph node stage must be 1, 2 or 3!\")\n\t}\n\tif(!is.numeric(size)) {\n\t\tstop(\"tumor size (cm) must be numeric!\")\n\t}\n\t\n\tnpi <- 0.2 * size + grade + node\n\tnames(npi) <- nn[cc.ix]\n\t\n\tnpi.score <- rep(NA, length(cc.ix))\n\tnames(npi.score) <- nn\n\tnpi.score[names(npi)] <- npi\n\t\n\tnpi.c <- npi\n\tnpi.c[npi < 3.4] <- \"Good\"\n\tnpi.c[npi > 5.4] <- \"Poor\"\n\tnpi.c[npi >= 3.4 & npi <= 5.4] <- \"Intermediate\"\n\t\n\tnpi.classif <- rep(NA, length(cc.ix))\n\tnames(npi.classif) <- nn\n\tnpi.classif[names(npi.c)] <- npi.c\n\t#npi.classif <- as.factor(npi.classif)\n\t\n\treturn(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'npi' function and what does it calculate?",
        "answer": "The 'npi' function calculates the Nottingham Prognostic Index (NPI) for breast cancer prognosis. It computes a score based on tumor size, histological grade, and nodal status, and then classifies the risk as 'Good', 'Intermediate', or 'Poor' based on the calculated score."
      },
      {
        "question": "How does the function handle missing values and what input validation does it perform?",
        "answer": "The function checks for complete cases using 'complete.cases()'. If 'na.rm=FALSE' and NA values are present, it stops with an error. It also validates that grade and node are within the correct range (1, 2, or 3) and that size is numeric. If all values are NA, it returns NA for both score and risk."
      },
      {
        "question": "What is the formula used to calculate the NPI score, and how is the risk classification determined?",
        "answer": "The NPI score is calculated using the formula: 0.2 * size + grade + node. The risk classification is determined as follows: 'Good' if score < 3.4, 'Intermediate' if 3.4 <= score <= 5.4, and 'Poor' if score > 5.4. The function returns both the continuous score and the risk classification."
      }
    ],
    "completion_tasks": [
      {
        "partial": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  # Add input validation here\n  \n  npi <- 0.2 * size + grade + node\n  names(npi) <- nn[cc.ix]\n  \n  npi.score <- rep(NA, length(cc.ix))\n  names(npi.score) <- nn\n  npi.score[names(npi)] <- npi\n  \n  # Add risk classification logic here\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}",
        "complete": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  if(length(size) != length(grade) || length(grade) != length(node)) {\n    stop(\"size, grade and lymph node stage must have the same length!\")\n  }\n  if(!all(cc.ix) & !na.rm)  { stop(\"NA values are present!\") }\n  if(!all(is.element(grade, c(\"1\", \"2\", \"3\")))) {\n    stop(\"grade must be 1, 2 or 3!\")\n  }\n  if(!all(is.element(node, c(\"1\", \"2\", \"3\")))) {\n    stop(\"lymph node stage must be 1, 2 or 3!\")\n  }\n  if(!is.numeric(size)) {\n    stop(\"tumor size (cm) must be numeric!\")\n  }\n  \n  npi <- 0.2 * size + grade + node\n  names(npi) <- nn[cc.ix]\n  \n  npi.score <- rep(NA, length(cc.ix))\n  names(npi.score) <- nn\n  npi.score[names(npi)] <- npi\n  \n  npi.c <- cut(npi, breaks=c(-Inf, 3.4, 5.4, Inf), labels=c(\"Good\", \"Intermediate\", \"Poor\"))\n  \n  npi.classif <- rep(NA, length(cc.ix))\n  names(npi.classif) <- nn\n  npi.classif[names(npi.c)] <- npi.c\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}"
      },
      {
        "partial": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  # Add input validation here\n  \n  # Calculate NPI score\n  \n  # Classify risk\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.classif))\n}",
        "complete": "npi <- function(size, grade, node, na.rm=FALSE) {\n  nn <- names(size)\n  if(is.null(nn)) { nn <- paste(\"X\", 1:length(size), sep=\".\") }\n  cc.ix <- complete.cases(size, grade, node)\n  if(all(!cc.ix)) {\n    tt <- rep(NA, length(size))\n    names(tt) <- nn\n    return(list(\"score\"=tt, \"risk\"=tt))\n  }\n  size <- size[cc.ix]\n  grade <- grade[cc.ix]\n  node <- node[cc.ix]\n  \n  if(!all(cc.ix) & !na.rm) stop(\"NA values are present!\")\n  if(!all(grade %in% 1:3)) stop(\"grade must be 1, 2 or 3!\")\n  if(!all(node %in% 1:3)) stop(\"lymph node stage must be 1, 2 or 3!\")\n  if(!is.numeric(size)) stop(\"tumor size (cm) must be numeric!\")\n  \n  npi <- 0.2 * size + as.numeric(grade) + as.numeric(node)\n  \n  npi.score <- rep(NA, length(cc.ix))\n  names(npi.score) <- nn\n  npi.score[cc.ix] <- npi\n  \n  npi.classif <- cut(npi, breaks=c(-Inf, 3.4, 5.4, Inf), labels=c(\"Good\", \"Intermediate\", \"Poor\"))\n  npi.risk <- rep(NA, length(cc.ix))\n  names(npi.risk) <- nn\n  npi.risk[cc.ix] <- npi.classif\n  \n  return(list(\"score\"=npi.score, \"risk\"=npi.risk))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  }
]