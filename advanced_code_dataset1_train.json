[
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_unionList.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\n##TODO:: Migrate tests to CoreGx\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"union List works as union with arbitrary number of arguments\",{\n\texpect_equal(CoreGx::.unionList(1,2,3,4,2,2,1), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1,2), c(1,2))\n\texpect_equal(CoreGx::.unionList(list(c(1,2,3), c(2,3,4), c(1,1,1))), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1), c(1))\n\texpect_equal(CoreGx::.unionList(), NULL)\n})\n\ntest_that(\"CoreGx::.unionList unlists things and unions properly\", {\n\n\texpect_equal(CoreGx::.unionList(list(1,2,3,4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3), list(2,2,2,2,2,2,2)), c(1,2,3,4))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the CoreGx::.unionList function based on the test cases provided?",
        "answer": "The CoreGx::.unionList function is designed to perform a union operation on its input arguments. It can handle an arbitrary number of arguments, including lists, and returns a unique set of elements. The function removes duplicates and flattens nested lists, returning a single vector of unique elements."
      },
      {
        "question": "How does the CoreGx::.unionList function handle different types of input, as demonstrated in the test cases?",
        "answer": "The CoreGx::.unionList function can handle various types of input: 1) Individual arguments: It can take multiple separate arguments and return their union. 2) Lists: It can take a single list or multiple lists as arguments and return the union of all elements. 3) Nested lists: It can handle nested lists, flattening them before performing the union operation. 4) Empty input: When called with no arguments, it returns NULL."
      },
      {
        "question": "What testing framework and assertions are used in this code snippet, and what are they checking for?",
        "answer": "This code snippet uses the testthat framework for R, as evidenced by the 'test_that' function calls. The assertions use 'expect_equal' to check if the output of CoreGx::.unionList matches the expected result. The tests verify that the function correctly handles various input scenarios, including multiple arguments, lists, nested lists, and edge cases like single elements or empty input."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"union List works as union with arbitrary number of arguments\",{\n\texpect_equal(CoreGx::.unionList(1,2,3,4,2,2,1), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1,2), c(1,2))\n\texpect_equal(CoreGx::.unionList(list(c(1,2,3), c(2,3,4), c(1,1,1))), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1), c(1))\n\t# Complete the last expectation\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"union List works as union with arbitrary number of arguments\",{\n\texpect_equal(CoreGx::.unionList(1,2,3,4,2,2,1), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1,2), c(1,2))\n\texpect_equal(CoreGx::.unionList(list(c(1,2,3), c(2,3,4), c(1,1,1))), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(1), c(1))\n\texpect_equal(CoreGx::.unionList(), NULL)\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"CoreGx::.unionList unlists things and unions properly\", {\n\texpect_equal(CoreGx::.unionList(list(1,2,3,4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3)), c(1,2,3,4))\n\t# Complete the last expectation\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking CoreGx::.unionList function.\")\n\ntest_that(\"CoreGx::.unionList unlists things and unions properly\", {\n\texpect_equal(CoreGx::.unionList(list(1,2,3,4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3)), c(1,2,3,4))\n\texpect_equal(CoreGx::.unionList(list(1,2,3), list(4,3), list(2,2,2,2,2,2,2)), c(1,2,3,4))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/__init__.py",
    "language": "py",
    "content": "# read version from installed package\nfrom importlib.metadata import version\n__version__ = \"1.8.0\"\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the code snippet and how does it determine the version of the package?",
        "answer": "The purpose of this code snippet is to set the version of the package. It imports the 'version' function from 'importlib.metadata', but then directly assigns a string '1.8.0' to the '__version__' variable. This means the imported 'version' function is not actually used, and the version is hardcoded instead of being read dynamically from the installed package metadata."
      },
      {
        "question": "What potential issue exists in this code snippet regarding version management?",
        "answer": "The main issue in this snippet is that it imports the 'version' function from 'importlib.metadata', but doesn't use it. Instead, it hardcodes the version string '1.8.0'. This can lead to inconsistencies if the actual package version changes but this file isn't updated. It's generally better to use dynamic version retrieval to ensure the reported version always matches the installed package version."
      },
      {
        "question": "How could this code snippet be improved to better manage the package version?",
        "answer": "To improve this code snippet, you could use the imported 'version' function to dynamically retrieve the package version. This can be done by replacing the hardcoded version with: __version__ = version('your_package_name'). This ensures that the '__version__' variable always reflects the actual installed version of the package, reducing the risk of version inconsistencies and making maintenance easier."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from importlib.metadata import version\n__version__ = ",
        "complete": "from importlib.metadata import version\n__version__ = version('package_name')"
      },
      {
        "partial": "from importlib.metadata import version\n__version__ = ",
        "complete": "from importlib.metadata import version\n__version__ = '1.8.0'"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "importlib.metadata.version"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugSensitivity.R",
    "language": "R",
    "content": "#' Calcualte The Gene Drug Sensitivity\n#'\n#' TODO:: Write a description!\n#' @examples\n#' print(\"TODO::\")\n#'\n#' @param x A \\code{numeric} vector of gene expression values\n#' @param type A \\code{vector} of factors specifying the cell lines or type types\n#' @param batch A \\code{vector} of factors specifying the batch\n#' @param drugpheno A \\code{numeric} vector of drug sensitivity values (e.g.,\n#'   IC50 or AUC)\n# @param duration A \\code{numeric} vector of experiment duration in hours\n#' @param interaction.typexgene \\code{boolean} Should interaction between gene\n#'   expression and cell/type type be computed? Default set to FALSE\n#' @param model \\code{boolean} Should the full linear model be returned? Default\n#'   set to FALSE\n#' @param standardize \\code{character} One of 'SD', 'rescale' or 'none'\n#' @param verbose \\code{boolean} Should the function display messages?\n#'\n#' @return A \\code{vector} reporting the effect size (estimate of the coefficient\n#'   of drug concentration), standard error (se), sample size (n), t statistic,\n#'   and F statistics and its corresponding p-value.\n#'\n#' @importFrom stats sd complete.cases lm glm anova pf formula var\ngeneDrugSensitivity <- function(x, type, batch, drugpheno,\n                                interaction.typexgene=FALSE,\n                                model=FALSE,  standardize=c(\"SD\", \"rescale\", \"none\"), verbose=FALSE) {\n\n  ## NOTE:: The use of T/F warning from BiocCheck is a false positive on the string 'Pr(>F)'\n  standardize <- match.arg(standardize)\n\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  if(length(table(drugpheno)) > 2){\n     if(ncol(drugpheno)>1){\n      ##### FIX NAMES!!!\n      rest <- lapply(seq_len(ncol(drugpheno)), function(i){\n\n        est <- paste(\"estimate\", i, sep=\".\")\n        se <-  paste(\"se\", i, sep=\".\")\n        tstat <- paste(\"tstat\", i, sep=\".\")\n\n        rest <- rep(NA, 3)\n        names(rest) <- c(est, se, tstat)\n        return(rest)\n      })\n      rest <- do.call(c, rest)\n      rest <- c(rest, n=nn, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n      rest <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tstat\"=NA, \"fstat\"=NA, \"pvalue\"=NA, \"df\"=NA)\n    }\n  } else {\n    # rest <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"pvalue\"=NA)\n    if (is.factor(drugpheno[,1])){\n      rest <- c(\"estimate\"=NA_real_, \"se\"=NA_real_, \"n\"=nn, \"pvalue\"=NA_real_, df=NA_real_)\n    } else {\n      rest <- c(\"estimate\" = NA, \"se\" = NA , \"n\" = nn, \"tstat\" = NA , \"fstat\" = NA , \"pvalue\" = NA , \"df\" = NA )\n    }\n  }\n\n  if(nn < 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    ## not enough samples with complete information or no variation in gene expression\n    return(rest)\n  }\n\n  ## standardized coefficient in linear model\n  if(length(table(drugpheno)) > 2 & standardize!= \"none\") {\n    switch(standardize,\n      \"SD\" = drugpheno <- apply(drugpheno, 2, function(x){\n      return(x[ccix]/sd(as.numeric(x[ccix])))}) ,\n      \"rescale\" = drugpheno <- apply(drugpheno, 2, function(x){\n      return(.rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE))    })\n      )\n\n  }else{\n    drugpheno <- drugpheno[ccix,,drop=FALSE]\n  }\n  if(length(table(x)) > 2  & standardize!= \"none\"){\n    switch(standardize,\n      \"SD\" = xx <- x[ccix]/sd(as.numeric(x[ccix])) ,\n      \"rescale\" = xx <- .rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE)\n      )\n  }else{\n    xx <- x[ccix]\n  }\n  if(ncol(drugpheno)>1){\n    ff0 <- paste(\"cbind(\", paste(paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\"), collapse=\",\"), \")\", sep=\"\")\n  } else {\n    ff0 <- \"drugpheno.1\"\n  }\n\n  # ff1 <- sprintf(\"%s + x\", ff0)\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n  # , \"x\"=xx, \"type\"=type[ccix], \"batch\"=batch[ccix])\n\n  ## control for tissue type\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  ## control for batch\n  if(length(sort(unique(batch[ccix]))) > 1) {\n        dd <- cbind(dd, batch=batch[ccix])\n  }\n  ## control for duration\n  # if(length(sort(unique(duration))) > 1){\n  #   ff0 <- sprintf(\"%s + duration\", ff0)\n  #   ff <- sprintf(\"%s + duration\", ff)\n  # }\n\n  # if(is.factor(drugpheno[,1])){\n\n  #   drugpheno <- drugpheno[,1]\n\n  # } else {\n\n  #   drugpheno <- as.matrix(drugpheno)\n\n  # }\nif(any(unlist(lapply(drugpheno,is.factor)))){\n\n## Added default '' value to ww to fix function if it is passed verbose = FALSE\nww = ''\n\nrr0 <- tryCatch(try(glm(formula(drugpheno.1 ~ . - x), data=dd, model=FALSE, x=FALSE, y=FALSE, family=\"binomial\")),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Null model did not convrge\"\n        print(ww)\n        if(\"type\" %in% colnames(dd)) {\n          tt <- table(dd[,\"type\"])\n          print(tt)\n        }\n        return(ww)\n      }\n    })\n  rr1 <- tryCatch(try(glm(formula(drugpheno.1 ~ .), data=dd, model=FALSE, x=FALSE, y=FALSE, family=\"binomial\")),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Model did not converge\"\n        tt <- table(dd[,\"drugpheno.1\"])\n        print(ww)\n        print(tt)\n      }\n      return(ww)\n    })\n\n\n} else{\n\nrr0 <- tryCatch(try(lm(formula(paste(ff0, \"~ . -x\", sep=\" \")), data=dd)),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Null model did not converge\"\n        print(ww)\n        if(\"type\" %in% colnames(dd)) {\n          tt <- table(dd[,\"type\"])\n          print(tt)\n        }\n      return(ww)\n      }\n    })\n  rr1 <- tryCatch(try(lm(formula(paste(ff0, \"~ . \", sep=\" \")), data=dd)),\n    warning=function(w) {\n      if(verbose) {\n        ww <- \"Model did not converge\"\n        tt <- table(dd[,\"drugpheno.1\"])\n        print(ww)\n        print(tt)\n      }\n      return(ww)\n    })\n\n\n}\n\n\n  if (!is(rr0, \"try-error\") && !is(rr1, \"try-error\") & !is(rr0, \"character\") && !is(rr1, \"character\")) {\n    rr <- summary(rr1)\n\n    if(any(unlist(lapply(drugpheno,is.factor)))){\n      rrc <- stats::anova(rr0, rr1, test=\"Chisq\")\n      rest <- c(\"estimate\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Estimate\"], \"se\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Std. Error\"], \"n\"=nn, \"pvalue\"=rrc$'Pr(>Chi)'[2], \"df\"=rr1$df.residual)\n      names(rest) <- c(\"estimate\", \"se\", \"n\", \"pvalue\", \"df\")\n\n    } else {\n      if(ncol(drugpheno)>1){\n        rrc <- summary(stats::manova(rr1))\n        rest <- lapply(seq_len(ncol(drugpheno)), function(i) {\n          est <- paste(\"estimate\", i, sep=\".\")\n          se <-  paste(\"se\", i, sep=\".\")\n          tstat <- paste(\"tstat\", i, sep=\".\")\n          rest <- c(rr[[i]]$coefficients[grep(\"^x\", rownames(rr[[i]]$coefficients)), \"Estimate\"], rr[[i]]$coefficients[grep(\"^x\", rownames(rr[[i]]$coefficients)), \"Std. Error\"], rr[[i]]$coefficients[grep(\"^x\", rownames(rr[[i]]$coefficients)), \"t value\"])\n          names(rest) <- c(est, se, tstat)\n          return(rest)\n        })\n        rest <- do.call(c, rest)\n        rest <- c(rest,\"n\"=nn, \"fstat\"=rrc$stats[grep(\"^x\", rownames(rrc$stats)), \"approx F\"], \"pvalue\"=rrc$stats[grep(\"^x\", rownames(rrc$stats)), \"Pr(>F)\"])\n      } else {\n        rrc <- stats::anova(rr0, rr1, test = \"F\")\n        if(!length(rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Estimate\"])){\n          stop(\"A model failed to converge even with sufficient data. Please investigate further\")\n        }\n        rest <- c(\"estimate\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Estimate\"], \"se\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"Std. Error\"],\"n\"=nn, \"tstat\"=rr$coefficients[grep(\"^x\", rownames(rr$coefficients)), \"t value\"], \"fstat\"=rrc$F[2], \"pvalue\"=rrc$'Pr(>F)'[2], \"df\"=rr1$df.residual)\n        names(rest) <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\")\n      }\n    }\n\n\n#    rest <- c(\"estimate\"=rr$coefficients[\"x\", \"Estimate\"], \"se\"=rr$coefficients[\"x\", \"Std. Error\"], \"n\"=nn, \"tsat\"=rr$coefficients[\"x\", \"t value\"], \"fstat\"=rrc$F[2], \"pvalue\"=rrc$'Pr(>F)'[2])\n\n#   names(rest) <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n\n## add tissue type/cell line statistics\n#     if(length(sort(unique(type))) > 1) {\n#       rr <- summary(rr0)\n#       ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n#       names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n#     } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n#     rest <- c(rest, ttype)\n    ## add model\n    if(model) { rest <- list(\"stats\"=rest, \"model\"=rr1) }\n  }\n  return(rest)\n}\n\n## Helper Functions\n##TODO:: Add  function documentation\n#' @importFrom stats quantile\n.rescale <- function(x, na.rm=FALSE, q=0)\n{\n  if(q == 0) {\n    ma <- max(x, na.rm=na.rm)\n    mi <- min(x, na.rm=na.rm)\n  } else {\n    ma <- quantile(x, probs=1-(q/2), na.rm=na.rm)\n    mi <- quantile(x, probs=q/2, na.rm=na.rm)\n  }\n  xx <- (x - mi) / (ma - mi)\n  return(xx)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `geneDrugSensitivity` function and what are its key input parameters?",
        "answer": "The `geneDrugSensitivity` function calculates gene drug sensitivity. Its key input parameters are: `x` (gene expression values), `type` (cell lines or type types), `batch` (batch factors), `drugpheno` (drug sensitivity values), `interaction.typexgene` (boolean for interaction computation), `model` (boolean to return full linear model), `standardize` (method for standardization), and `verbose` (boolean for displaying messages)."
      },
      {
        "question": "How does the function handle different types of drug phenotype data, and what statistical models are used for analysis?",
        "answer": "The function handles both continuous and categorical drug phenotype data. For continuous data, it uses linear models (lm). For categorical data (factors), it uses generalized linear models (glm) with a binomial family. The function performs model comparisons using ANOVA for continuous data and Chi-square tests for categorical data to assess the significance of the gene expression effect."
      },
      {
        "question": "What is the purpose of the `.rescale` helper function in this code, and how is it used within the main function?",
        "answer": "The `.rescale` helper function is used to rescale numeric vectors. It's used in the `geneDrugSensitivity` function when the `standardize` parameter is set to 'rescale'. It scales the values of `x` (gene expression) and `drugpheno` to a range between 0 and 1, with optional quantile-based capping to handle outliers. This standardization helps in comparing effects across different scales of measurement."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneDrugSensitivity <- function(x, type, batch, drugpheno,\n                                interaction.typexgene=FALSE,\n                                model=FALSE,  standardize=c(\"SD\", \"rescale\", \"none\"), verbose=FALSE) {\n  standardize <- match.arg(standardize)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  \n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  # TODO: Complete the rest of the function\n}",
        "complete": "geneDrugSensitivity <- function(x, type, batch, drugpheno,\n                                interaction.typexgene=FALSE,\n                                model=FALSE,  standardize=c(\"SD\", \"rescale\", \"none\"), verbose=FALSE) {\n  standardize <- match.arg(standardize)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  \n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  if(nn < 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    return(c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tstat\"=NA, \"fstat\"=NA, \"pvalue\"=NA, \"df\"=NA))\n  }\n\n  if(standardize != \"none\") {\n    drugpheno <- apply(drugpheno, 2, function(x) {\n      if(standardize == \"SD\") x[ccix]/sd(as.numeric(x[ccix]))\n      else .rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE)\n    })\n    xx <- if(standardize == \"SD\") x[ccix]/sd(as.numeric(x[ccix]))\n          else .rescale(as.numeric(x[ccix]), q=0.05, na.rm=TRUE)\n  } else {\n    drugpheno <- drugpheno[ccix,,drop=FALSE]\n    xx <- x[ccix]\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n  if(length(unique(type[ccix])) > 1) dd$type <- type[ccix]\n  if(length(unique(batch[ccix])) > 1) dd$batch <- batch[ccix]\n\n  ff0 <- paste(\"cbind(\", paste(paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\"), collapse=\",\"), \")\")\n  \n  rr0 <- try(lm(formula(paste(ff0, \"~ . -x\", sep=\" \")), data=dd))\n  rr1 <- try(lm(formula(paste(ff0, \"~ . \", sep=\" \")), data=dd))\n\n  if (!inherits(rr0, \"try-error\") && !inherits(rr1, \"try-error\")) {\n    rr <- summary(rr1)\n    rrc <- anova(rr0, rr1)\n    rest <- c(\"estimate\"=rr$coefficients[\"x\", \"Estimate\"],\n              \"se\"=rr$coefficients[\"x\", \"Std. Error\"],\n              \"n\"=nn,\n              \"tstat\"=rr$coefficients[\"x\", \"t value\"],\n              \"fstat\"=rrc$F[2],\n              \"pvalue\"=rrc$`Pr(>F)`[2],\n              \"df\"=rr1$df.residual)\n    if(model) rest <- list(\"stats\"=rest, \"model\"=rr1)\n    return(rest)\n  }\n  return(c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tstat\"=NA, \"fstat\"=NA, \"pvalue\"=NA, \"df\"=NA))\n}"
      },
      {
        "partial": ".rescale <- function(x, na.rm=FALSE, q=0) {\n  # TODO: Implement the rescaling function\n}",
        "complete": ".rescale <- function(x, na.rm=FALSE, q=0) {\n  if(q == 0) {\n    ma <- max(x, na.rm=na.rm)\n    mi <- min(x, na.rm=na.rm)\n  } else {\n    ma <- quantile(x, probs=1-(q/2), na.rm=na.rm)\n    mi <- quantile(x, probs=q/2, na.rm=na.rm)\n  }\n  xx <- (x - mi) / (ma - mi)\n  return(xx)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeAUC_old.R",
    "language": "R",
    "content": "# Return AUC (Area Under the drug response curve) for an experiment of a pSet by taking\n# its concentration and viability as input.\n#\n# @param conc `numeric` A concentration range that the AUC should be computed for that range.\n# Concentration range by default considered as not logarithmic scaled.\n# @param viability `numeric` Viablities correspondant to the concentration range passed as first parameter.\n# The range of viablity values by definition should be between 0 and 100. But the viabalities greater than\n# 100 and lower than 0 are also accepted.\n# @param trunc [binary] A flag that identify if the viabality values should be truncated to be in the\n# range of (0,100)\n# @param verbose `logical(1)` If 'TRUE' the function will retrun warnings and other infomrative messages.\n# @import caTools\n#' @keywords internal\n#' @noRd\ncomputeAUC_old <- function(conc, viability,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc=TRUE,\n                       verbose=TRUE,\n                       area.type=c(\"Fitted\",\"Actual\")) {\n    cleanData <- sanitizeInput(conc, viability,\n                             conc_as_log=conc_as_log,\n                             viability_as_pct=viability_as_pct,\n                             trunc=trunc, verbose=verbose)\n    log_conc <- cleanData[[\"log_conc\"]]\n    viability <- cleanData[[\"viability\"]]\n\n#   ii <- which(concentration == 0)\n#   if(length(ii) > 0) {\n#     concentration <- concentration[-ii]\n#     viability <- viability[-ii]\n#   }\n\n  if(missing(area.type)){\n    area.type <- \"Fitted\"\n  }\n  if(length(conc) < 2){\n    return(NA)\n  }\n  if(area.type == \"Actual\"){\n    # if(trunc) {viability = pmin(as.numeric(viability), 100); viability = pmax(as.numeric(viability), 0)}\n    trapezoid.integral <- caTools::trapz(log10(as.numeric(conc) + 1) ,as.numeric(viability))\n    AUC <- round(1- (trapezoid.integral/trapz(log10(as.numeric(conc)), rep(100, length(viability)))), digits=2)\n  }else{\n    AUC <- .computeAUCUnderFittedCurve(conc, viability, trunc)\n  }\n  return (AUC)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC_old` function and what are its main input parameters?",
        "answer": "The `computeAUC_old` function calculates the Area Under the Curve (AUC) for a drug response experiment. Its main input parameters are: 'conc' (a numeric vector of concentration values), 'viability' (a numeric vector of corresponding viability values), 'conc_as_log' (boolean indicating if concentration is in log scale), 'viability_as_pct' (boolean indicating if viability is in percentage), 'trunc' (boolean for truncating viability values), 'verbose' (boolean for displaying messages), and 'area.type' (string specifying the type of AUC calculation)."
      },
      {
        "question": "How does the function handle data sanitization and what happens if the concentration vector has less than 2 elements?",
        "answer": "The function uses a `sanitizeInput` helper function to clean and prepare the input data. This function likely handles the conversion of concentration and viability values based on the `conc_as_log` and `viability_as_pct` parameters, and may also perform truncation if `trunc` is TRUE. If the concentration vector has less than 2 elements, the function returns NA, as seen in the line `if(length(conc) < 2){ return(NA) }`."
      },
      {
        "question": "What are the two methods of calculating AUC in this function and how do they differ?",
        "answer": "The function offers two methods for calculating AUC, specified by the 'area.type' parameter: 'Actual' and 'Fitted' (default). The 'Actual' method uses the trapezoidal rule (implemented with caTools::trapz) to calculate the area under the actual data points. The 'Fitted' method uses a helper function `.computeAUCUnderFittedCurve`, which likely fits a curve to the data points and then calculates the area under this fitted curve. The 'Actual' method works directly with the provided data, while the 'Fitted' method potentially smooths out noise in the data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC_old <- function(conc, viability,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc=TRUE,\n                       verbose=TRUE,\n                       area.type=c(\"Fitted\",\"Actual\")) {\n    cleanData <- sanitizeInput(conc, viability,\n                             conc_as_log=conc_as_log,\n                             viability_as_pct=viability_as_pct,\n                             trunc=trunc, verbose=verbose)\n    log_conc <- cleanData[[\"log_conc\"]]\n    viability <- cleanData[[\"viability\"]]\n\n    if(missing(area.type)){\n        area.type <- \"Fitted\"\n    }\n    if(length(conc) < 2){\n        return(NA)\n    }\n    # Complete the function here\n}",
        "complete": "computeAUC_old <- function(conc, viability,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc=TRUE,\n                       verbose=TRUE,\n                       area.type=c(\"Fitted\",\"Actual\")) {\n    cleanData <- sanitizeInput(conc, viability,\n                             conc_as_log=conc_as_log,\n                             viability_as_pct=viability_as_pct,\n                             trunc=trunc, verbose=verbose)\n    log_conc <- cleanData[[\"log_conc\"]]\n    viability <- cleanData[[\"viability\"]]\n\n    if(missing(area.type)){\n        area.type <- \"Fitted\"\n    }\n    if(length(conc) < 2){\n        return(NA)\n    }\n    if(area.type == \"Actual\"){\n        trapezoid.integral <- caTools::trapz(log10(as.numeric(conc) + 1), as.numeric(viability))\n        AUC <- round(1 - (trapezoid.integral / trapz(log10(as.numeric(conc)), rep(100, length(viability)))), digits=2)\n    } else {\n        AUC <- .computeAUCUnderFittedCurve(conc, viability, trunc)\n    }\n    return(AUC)\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability,\n                      conc_as_log = FALSE,\n                      viability_as_pct = TRUE,\n                      trunc = TRUE,\n                      verbose = TRUE) {\n    # Implement input sanitization logic here\n}",
        "complete": "sanitizeInput <- function(conc, viability,\n                      conc_as_log = FALSE,\n                      viability_as_pct = TRUE,\n                      trunc = TRUE,\n                      verbose = TRUE) {\n    if (!is.numeric(conc) || !is.numeric(viability)) {\n        stop(\"conc and viability must be numeric vectors\")\n    }\n    if (length(conc) != length(viability)) {\n        stop(\"conc and viability must have the same length\")\n    }\n    if (!conc_as_log) {\n        log_conc <- log10(conc)\n    } else {\n        log_conc <- conc\n    }\n    if (!viability_as_pct) {\n        viability <- viability * 100\n    }\n    if (trunc) {\n        viability <- pmin(pmax(viability, 0), 100)\n    }\n    if (verbose && any(viability < 0 | viability > 100)) {\n        warning(\"Some viability values are outside the range [0, 100]\")\n    }\n    return(list(log_conc = log_conc, viability = viability))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/GuideToPharm.R",
    "language": "R",
    "content": "#' Get data from the Guide to PHARMACOLOGY Database Web Services\n#'\n#' @param ids `character()` or `integer()` Identifiers to query the web\n#'   service with. If excluded, the entire record for the specified service\n#'   is returned.\n#' @param service `character(1)` Which Guide to PHARMACOLOGY web service\n#'   to query. Defaults to 'ligands'. Other options are 'targets', 'interactions',\n#'   'diseases' and 'references'.\n#' @param id_type `character(1)` What type of identifiers are in `ids`? Defaults\n#'   to 'name', for drug name. Other options are 'accession', which accepts\n#'   PubChem CIDs.\n#' @param ... Force subsequent parameters to be named. Not used.\n#'\n#' @return A `data.table` of query results.\n#'\n#' @details\n#' The API reference documentation can be found here:\n#' https://www.guidetopharmacology.org/webServices.jsp\n#'\n#' There is also a Python interface available for querying this API. See:\n#' https://github.com/samirelanduk/pygtop\n#'\n#' @importFrom data.table data.table as.data.table rbindlist setnames\n#' @importFrom jsonlite fromJSON\n#' @importFrom httr RETRY GET status_code\n#'\n#' @export\n# getGuideToPharm <- function(\n#     ids = character(),\n#     service = c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"),\n#     id_type = c(\"name\", \"accession\"),\n#     ...,\n# ){\n\n\n#     checkmate::assert_atomic(ids, any.missing = FALSE, min.len = 1)\n#     checkmate::assert_character(service, len = 1)\n#     checkmate::assert_character(id_type, len = 1)\n\n#     url <- httr2::url_parse(\"https://www.guidetopharmacology.org/services\")\n#     url$path <- .buildURL(url$path, service)\n\n#     opts <- list()\n\n#     opts[id_type] <- paste0(ids, collapse = \",\")\n\n# }\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getGuideToPharm` function and what are its main parameters?",
        "answer": "The `getGuideToPharm` function is designed to retrieve data from the Guide to PHARMACOLOGY Database Web Services. Its main parameters are: `ids` (identifiers to query the web service), `service` (which web service to query, defaulting to 'ligands'), and `id_type` (type of identifiers in `ids`, defaulting to 'name')."
      },
      {
        "question": "How does the function handle input validation for its parameters?",
        "answer": "The function uses the `checkmate` package for input validation. It checks that `ids` is an atomic vector with no missing values and a minimum length of 1, `service` is a character vector of length 1, and `id_type` is also a character vector of length 1. This ensures that the inputs meet the expected format and constraints before proceeding with the API request."
      },
      {
        "question": "What is the purpose of the `url` and `opts` variables in the function, and how are they constructed?",
        "answer": "The `url` variable is used to construct the base URL for the API request. It starts with 'https://www.guidetopharmacology.org/services' and is then modified using the `.buildURL` function to include the specified service. The `opts` variable is a list that will contain query parameters for the API request. It's populated with the `id_type` as the key and a comma-separated string of `ids` as the value, which will be used to form the query string in the final API request."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getGuideToPharm <- function(ids = character(), service = c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"), id_type = c(\"name\", \"accession\"), ...) {\n  checkmate::assert_atomic(ids, any.missing = FALSE, min.len = 1)\n  checkmate::assert_character(service, len = 1)\n  checkmate::assert_character(id_type, len = 1)\n\n  url <- httr2::url_parse(\"https://www.guidetopharmacology.org/services\")\n  url$path <- .buildURL(url$path, service)\n\n  opts <- list()\n  opts[id_type] <- paste0(ids, collapse = \",\")\n\n  # Complete the function by adding code to make the API request and process the response\n}",
        "complete": "getGuideToPharm <- function(ids = character(), service = c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"), id_type = c(\"name\", \"accession\"), ...) {\n  checkmate::assert_atomic(ids, any.missing = FALSE, min.len = 1)\n  checkmate::assert_character(service, len = 1)\n  checkmate::assert_character(id_type, len = 1)\n\n  url <- httr2::url_parse(\"https://www.guidetopharmacology.org/services\")\n  url$path <- .buildURL(url$path, service)\n\n  opts <- list()\n  opts[id_type] <- paste0(ids, collapse = \",\")\n\n  response <- httr::RETRY(\"GET\", httr2::url_build(url), query = opts)\n  if (httr::status_code(response) != 200) {\n    stop(\"API request failed with status code: \", httr::status_code(response))\n  }\n\n  content <- jsonlite::fromJSON(httr::content(response, \"text\", encoding = \"UTF-8\"))\n  result <- data.table::as.data.table(content)\n\n  return(result)\n}"
      },
      {
        "partial": ".buildURL <- function(base_path, service) {\n  # Complete the function to build the correct URL path based on the service\n}",
        "complete": ".buildURL <- function(base_path, service) {\n  service <- match.arg(service, c(\"ligands\", \"targets\", \"interactions\", \"diseases\", \"references\"))\n  return(file.path(base_path, service))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_summarizeSensitivityProfiles.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking summarizeSensitivityProfiles function.\")\n\ndata(\"GDSCsmall\")\n\n## FIXME:: No S4 method for summarizeSensitivityProfiles with class 'missing'\n#test_that(\"Summarize Sensitivity Profiles fails gracefully.\", {\n#  expect_error(summarizeSensitivityProfiles(), \"argument \\\"pSet\\\" is missing\")\n#})\n\n\ntest_that(\"Summarize Sensitivity Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall)\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equal(rownames(testSummary), treatmentNames(GDSCsmall))\n  expect_equivalent(is(testSummary, \"matrix\"), TRUE)\n})\n\ntest_that(\"summarizeSensitivityProfiles produces correct values.\",{\n\n  GDSCsmall2 <- subsetTo(GDSCsmall, drugs=\"AZD6482\")\n  testCells <- sensitivityProfiles(GDSCsmall2)[order(sensitivityInfo(GDSCsmall2)$sampleid),\"auc_recomputed\", drop=FALSE]\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"median\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {median(c(x,y))}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"mean\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {mean(c(x,y))}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"first\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {x}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"last\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary)<- NULL\n  expect_equivalent(testSummary, mapply(function(x,y) {y}, testCells[seq(1,18,2),], testCells[seq(1,18,2)+1,]))\n\n})\n\n\ntest_that(\"Summarize Sensitivity Profiles parameters work as expected\", {\n  expect_silent(summarizeSensitivityProfiles(GDSCsmall, verbose = FALSE))\n  expect_equal(ncol(summarizeSensitivityProfiles(GDSCsmall, fill.missing = FALSE)), length(unique(sensitivityInfo(GDSCsmall)$sampleid)))\n  expect_equal(ncol(summarizeSensitivityProfiles(GDSCsmall, fill.missing = TRUE)), length(sampleNames(GDSCsmall)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeSensitivityProfiles` function in this code, and how is it being tested?",
        "answer": "The `summarizeSensitivityProfiles` function is used to summarize sensitivity profiles from a PharmacoSet object. It's being tested for correct output dimensions, column and row names, class type, and various summary statistics (median, mean, first, last). The tests ensure that the function produces expected results for different input parameters and handles edge cases correctly."
      },
      {
        "question": "How does the code test different summary statistics in the `summarizeSensitivityProfiles` function?",
        "answer": "The code tests different summary statistics by calling `summarizeSensitivityProfiles` with various `summary.stat` parameters: 'median', 'mean', 'first', and 'last'. For each statistic, it compares the function's output to manually calculated expected values using `mapply` and the corresponding R functions (median, mean) or selection methods (first, last element). This ensures that each summary statistic option produces the correct result."
      },
      {
        "question": "What is the purpose of the `fill.missing` parameter in the `summarizeSensitivityProfiles` function, and how is it tested?",
        "answer": "The `fill.missing` parameter in `summarizeSensitivityProfiles` determines whether missing values should be filled in the output. When `fill.missing = TRUE`, the function includes all samples from the PharmacoSet, potentially filling in missing values. When `FALSE`, it only includes samples with available sensitivity data. The test compares the number of columns in the output when `fill.missing` is set to `TRUE` and `FALSE`, expecting more columns (all samples) when `TRUE` and fewer columns (only samples with data) when `FALSE`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Summarize Sensitivity Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall)\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equal(rownames(testSummary), treatmentNames(GDSCsmall))\n  # Complete the test by checking if testSummary is a matrix\n})",
        "complete": "test_that(\"Summarize Sensitivity Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall)\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equal(rownames(testSummary), treatmentNames(GDSCsmall))\n  expect_true(is.matrix(testSummary))\n})"
      },
      {
        "partial": "test_that(\"summarizeSensitivityProfiles produces correct values.\", {\n  GDSCsmall2 <- subsetTo(GDSCsmall, drugs=\"AZD6482\")\n  testCells <- sensitivityProfiles(GDSCsmall2)[order(sensitivityInfo(GDSCsmall2)$sampleid),\"auc_recomputed\", drop=FALSE]\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"median\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary) <- NULL\n  # Complete the test by comparing testSummary with the expected result\n})",
        "complete": "test_that(\"summarizeSensitivityProfiles produces correct values.\", {\n  GDSCsmall2 <- subsetTo(GDSCsmall, drugs=\"AZD6482\")\n  testCells <- sensitivityProfiles(GDSCsmall2)[order(sensitivityInfo(GDSCsmall2)$sampleid),\"auc_recomputed\", drop=FALSE]\n\n  testSummary <- summarizeSensitivityProfiles(GDSCsmall2, summary.stat = \"median\", fill.missing=FALSE)\n  testSummary <- testSummary[order(names(testSummary))]\n  names(testSummary) <- NULL\n  expect_equal(testSummary, sapply(seq(1, nrow(testCells), 2), function(i) median(testCells[i:(i+1), 1])))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/datasets.R",
    "language": "R",
    "content": "  #' Connectivity Map Example PharmacoSet\n#'\n#' A small example version of the Connectivity Map PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the Connectivity Map\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name CMAPsmall\n#' @usage data(CMAPsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL\n\n#' Genomics of Drug Sensitivity in Cancer Example PharmacoSet\n#'\n#' A small example version of the Genomics of Drug Sensitivity in Cancer Project\n#' PharmacoSet, used in the documentation examples. All credit for the data goes\n#' to the Genomics of Drug Sensitivity in Cancer Project group at the Sanger.This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Garnett et al. Systematic identification of genomic markers of drug sensitivity in cancer cells. Nature, 2012.\n#' \n#' @docType data\n#' @name GDSCsmall\n#' @usage data(GDSCsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL\n\n#' Cancer Cell Line Encyclopedia (CCLE) Example PharmacoSet\n#'\n#' A small example version of the CCLE PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the CCLE\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Barretina et al. The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity. Nature, 2012\n#' \n#' @docType data\n#' @name CCLEsmall\n#' @usage data(CCLEsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL\n\n#' HDAC Gene Signature\n#'\n#' A gene signature for HDAC inhibitors, as detailed by Glaser et al. The \n#' signature is mapped from the probe to gene level using\n#' \\code{probeGeneMapping}\n#' \n#' @references\n#' Glaser et al. Gene expression profiling of multiple histone deacetylase (HDAC) inhibitors: defining a common gene set produced by HDAC inhibition in T24 and MDA carcinoma cell lines. Molecular cancer therapeutics, 2003.\n#' \n#' @docType data\n#' @name HDAC_genes\n#' @usage data(HDAC_genes)\n#' @keywords datasets\n#' @format a 13x2 data.frame with gene identifiers in the first column and\n#'   direction change in the second\n#'\nNULL",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'NULL' statement at the end of each data object definition in this code snippet?",
        "answer": "The 'NULL' statement at the end of each data object definition is used as a placeholder. In R, when defining data objects using roxygen2 documentation, the 'NULL' statement is often used to indicate that no actual R code is being executed. It allows the documentation to be associated with the data object without defining any functionality."
      },
      {
        "question": "How are the dataset references formatted in the code, and what information do they provide?",
        "answer": "The dataset references are formatted using the '@references' roxygen2 tag. They provide citation information for the original source of the data, including the authors, title of the publication, journal name, and publication year. This gives proper credit to the original researchers and allows users to find more information about the dataset's origin and methodology."
      },
      {
        "question": "What is the significance of the '@format' tag in the documentation for these datasets?",
        "answer": "The '@format' tag in the documentation specifies the structure or type of the dataset. For the CMAPsmall, GDSCsmall, and CCLEsmall datasets, it indicates that they are 'PharmacoSet object' types. For the HDAC_genes dataset, it describes the format as 'a 13x2 data.frame with gene identifiers in the first column and direction change in the second'. This information helps users understand the structure and content of the data they are working with."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Connectivity Map Example PharmacoSet\n#'\n#' A small example version of the Connectivity Map PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the Connectivity Map\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name CMAPsmall\n#' @usage data(CMAPsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\n",
        "complete": "#' Connectivity Map Example PharmacoSet\n#'\n#' A small example version of the Connectivity Map PharmacoSet, used in the\n#' documentation examples. All credit for the data goes to the Connectivity Map\n#' group at the Broad Institute. This is not a full version of the dataset, most of\n#' of the dataset was removed to make runnable example code. For the full dataset,\n#' please download using the downloadPSet function. \n#' \n#' @references\n#' Lamb et al. The Connectivity Map: using gene-expression signatures to connect small molecules, genes, and disease. Science, 2006.\n#' \n#' @docType data\n#' @name CMAPsmall\n#' @usage data(CMAPsmall)\n#' @keywords datasets\n#' @format PharmacoSet object\n#'\nNULL"
      },
      {
        "partial": "#' HDAC Gene Signature\n#'\n#' A gene signature for HDAC inhibitors, as detailed by Glaser et al. The \n#' signature is mapped from the probe to gene level using\n#' \\code{probeGeneMapping}\n#' \n#' @references\n#' Glaser et al. Gene expression profiling of multiple histone deacetylase (HDAC) inhibitors: defining a common gene set produced by HDAC inhibition in T24 and MDA carcinoma cell lines. Molecular cancer therapeutics, 2003.\n#' \n#' @docType data\n#' @name HDAC_genes\n#' @usage data(HDAC_genes)\n#' @keywords datasets\n#' @format a 13x2 data.frame with gene identifiers in the first column and\n#'   direction change in the second\n#'",
        "complete": "#' HDAC Gene Signature\n#'\n#' A gene signature for HDAC inhibitors, as detailed by Glaser et al. The \n#' signature is mapped from the probe to gene level using\n#' \\code{probeGeneMapping}\n#' \n#' @references\n#' Glaser et al. Gene expression profiling of multiple histone deacetylase (HDAC) inhibitors: defining a common gene set produced by HDAC inhibition in T24 and MDA carcinoma cell lines. Molecular cancer therapeutics, 2003.\n#' \n#' @docType data\n#' @name HDAC_genes\n#' @usage data(HDAC_genes)\n#' @keywords datasets\n#' @format a 13x2 data.frame with gene identifiers in the first column and\n#'   direction change in the second\n#'\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/globals.R",
    "language": "R",
    "content": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `utils::globalVariables()` function in this code snippet, and why might it be necessary?",
        "answer": "The `utils::globalVariables()` function is used to declare global variables that are used in ggplot2 or dplyr operations. This is necessary to suppress R CMD check notes about undefined global variables, which can occur when using non-standard evaluation in these packages. It informs R that these variables will be defined later or are part of the data being manipulated."
      },
      {
        "question": "How many and what type of variables are being declared as global in this code?",
        "answer": "The code declares 11 global variables. They are all character strings representing column names or identifiers used in data manipulation or plotting. The variables include: 'X', 'Y', 'Cutoff', 'rn', 'treatmentid', 'sampleid', 'rowKey', 'colKey', 'drug_cell_rep', 'value', and 'max.conc'."
      },
      {
        "question": "Why is 'rn' included twice in the list of global variables, and what potential issue could this cause?",
        "answer": "The variable 'rn' is included twice in the list of global variables, which is likely an oversight or typo. This redundancy doesn't cause any functional issues as R will simply ignore the duplicate declaration. However, it's generally good practice to remove such duplicates to maintain clean and efficient code. It might also indicate that the code hasn't been thoroughly reviewed or cleaned up."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\",\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))",
        "complete": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))"
      },
      {
        "partial": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc'))",
        "complete": "# Define global variables for ggplot/dplyr columns\nutils::globalVariables(c(\"X\", \"Y\", \"Cutoff\", 'rn', \"treatmentid\", \"sampleid\", 'rn',\n    'rowKey', 'colKey', 'drug_cell_rep', 'value', 'max.conc',\n    'drug_cell_rep.x'))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_CancerTargetDiscovery.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n  \n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result4))\n  checkmate::expect_data_table(result4, min.cols = 1, min.rows = 1)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CTD` function in this test suite, and how is it being tested?",
        "answer": "The `mapCompound2CTD` function appears to map compound names to their corresponding entries in the Comparative Toxicogenomics Database (CTD). The test suite checks various aspects of this function:\n1. It tests the basic functionality with a single compound.\n2. It verifies different output formats (default, query_only, and raw).\n3. It checks the function's behavior with an invalid compound name.\nThe tests ensure that the function returns the expected data structure, contains specific fields like 'displayName' and 'PUBCHEM', and handles different input scenarios correctly."
      },
      {
        "question": "How does the test suite use the `checkmate` package, and what is its purpose in this context?",
        "answer": "The `checkmate` package is used in this test suite for assertion-based testing. It provides functions to check the structure and properties of objects returned by `mapCompound2CTD`. Specifically:\n1. `assert_list` is used to ensure the result is a list with at least one element.\n2. `assert_class` checks if an object is of a specific class (e.g., 'httr2_request').\n3. `expect_data_table` verifies that the result is a data table with minimum column and row requirements.\nThese assertions help ensure that the function returns data in the expected format and structure, enhancing the robustness of the tests."
      },
      {
        "question": "What are the different output formats tested for the `mapCompound2CTD` function, and how do they differ?",
        "answer": "The test suite checks three different output formats for the `mapCompound2CTD` function:\n1. Default format (no additional parameters): Returns a result with 'displayName' and 'PUBCHEM' fields.\n2. Query-only format (`query_only = TRUE`): Returns a list of 'httr2_request' objects, representing the API queries without executing them.\n3. Raw format (`raw = TRUE`): Returns a list of raw response data, including 'class' and 'displayName' fields.\nThese different formats allow users to choose between processed results, pre-execution queries, or raw API responses, depending on their specific needs for data analysis or further processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n  \n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  # Complete the assertions for result4\n})",
        "complete": "test_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n  \n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result4))\n  checkmate::expect_data_table(result4, min.cols = 1, min.rows = 1)\n})"
      },
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  # Add assertions for result\n\n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  # Add assertions for result2\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  # Add assertions for result3\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  # Add assertions for result4\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"mapCompound2CTD returns expected results\", {\n  # Test case 1: Single compound mapping\n  compounds <- c(\"Bax channel blocker\")\n  result <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result))\n  expect_true(\"PUBCHEM\" %in% names(result))\n\n  result2 <- mapCompound2CTD(compounds, nParallel = 1, query_only = TRUE)\n  checkmate::assert_list(result2, min.len = 1)\n  checkmate::assert_class(result2[[1]], \"httr2_request\")\n\n  result3 <- mapCompound2CTD(compounds, nParallel = 1, raw = TRUE)\n  checkmate::assert_list(result3, min.len = 1)\n  expect_equal(result3[[1]]$class, \"Compound\")\n  expect_equal(result3[[1]]$displayName, \"Bax channel blocker\")\n\n  # Test case 3: Invalid compound mapping\n  compounds <- c(\"Invalid Compound\")\n  result4 <- mapCompound2CTD(compounds, nParallel = 1)\n  expect_true(\"displayName\" %in% names(result4))\n  checkmate::expect_data_table(result4, min.cols = 1, min.rows = 1)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_computeIC50.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking computeIC50/ICn.\")\n\ntest_that(\"Function complains when given insensible input\",{\n\texpect_error(computeIC50(concentration = c(1, 2, 3),\n\t\tviability = c(50, 60, 70),\n\t\tHill_fit = c(1, 0, 0.1)), \"Please pass in only one\")\n\t# expect_silent(computeIC50(concentration = c(1, 2, 3),\n\t# \t\t# \tviability1 = c(50, 60, 70),\n\t# \tHill_fit2 = c(0.5, 0.2, 1)))\n\n\texpect_error(computeIC50(concentration = c(1, 2, 3, 5), viability = c(50, 60, 70)), \"is not of same length\") #should complain\n\texpect_error(computeIC50(concentration = c(-1, 2, 3),viability = c(50, 60, 70),conc_as_log = FALSE),\"'x_as_log' flag may be set incorrectly\") #should complain\n\t##TO-DO:: Add wanring strings to expect_warning call\n\texpect_error(computeIC50(concentration = c(NA, \"cat\", 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"real numbers\") #should complain\n\texpect_error(computeIC50(concentration = c(1, 2, Inf), viability = c(50, 60, 70)), \"real numbers, NA-values, and/or -Inf\") #should complain\n\texpect_warning(computeIC50(concentration = c(1, 2, 3),\n\t\tviability = c(.50, .60, .70),\n\t\tviability_as_pct = TRUE), \"as_pct\") #should complain\n\texpect_error(computeIC50()) #should complain\n})\n\ntest_that(\"Functions return right values\",{\n\texpect_equal(computeIC50(concentration = seq(-3,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n\texpect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n\texpect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.9,0), conc_as_log=TRUE, viability_as_pct=FALSE), NA_real_)\n\texpect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.5,0), conc_as_log=TRUE, viability_as_pct=FALSE), Inf)\n\texpect_equal(.Hill(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=.7, conc_as_log=TRUE, viability_as_pct=FALSE), c(1,0,0)), .3)\n\texpect_equal(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=0, conc_as_log=TRUE, viability_as_pct=FALSE), -Inf)\n})\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeIC50(concentration = c(1, 2, 3),\n    viability = c(50, 60, 70),\n    Hill_fit = c(1, 0, 0.1)), \"Please pass in only one\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, 3, 5), viability = c(50, 60, 70)), \"is not of same length\")\n  \n  expect_error(computeIC50(concentration = c(-1, 2, 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"'x_as_log' flag may be set incorrectly\")\n  \n  expect_error(computeIC50(concentration = c(NA, \"cat\", 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"real numbers\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, Inf), viability = c(50, 60, 70)), \"real numbers, NA-values, and/or -Inf\")\n  \n  expect_warning(computeIC50(concentration = c(1, 2, 3),\n    viability = c(.50, .60, .70),\n    viability_as_pct = TRUE), \"as_pct\")\n  \n  expect_error(computeIC50())\n})",
        "complete": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeIC50(concentration = c(1, 2, 3),\n    viability = c(50, 60, 70),\n    Hill_fit = c(1, 0, 0.1)), \"Please pass in only one\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, 3, 5), viability = c(50, 60, 70)), \"is not of same length\")\n  \n  expect_error(computeIC50(concentration = c(-1, 2, 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"'x_as_log' flag may be set incorrectly\")\n  \n  expect_error(computeIC50(concentration = c(NA, \"cat\", 3), viability = c(50, 60, 70), conc_as_log = FALSE), \"real numbers\")\n  \n  expect_error(computeIC50(concentration = c(1, 2, Inf), viability = c(50, 60, 70)), \"real numbers, NA-values, and/or -Inf\")\n  \n  expect_warning(computeIC50(concentration = c(1, 2, 3),\n    viability = c(.50, .60, .70),\n    viability_as_pct = TRUE), \"as_pct\")\n  \n  expect_error(computeIC50())\n})"
      },
      {
        "partial": "test_that(\"Functions return right values\", {\n  expect_equal(computeIC50(concentration = seq(-3,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.9,0), conc_as_log=TRUE, viability_as_pct=FALSE), NA_real_)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.5,0), conc_as_log=TRUE, viability_as_pct=FALSE), Inf)\n  expect_equal(.Hill(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=.7, conc_as_log=TRUE, viability_as_pct=FALSE), c(1,0,0)), .3)\n  expect_equal(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=0, conc_as_log=TRUE, viability_as_pct=FALSE), -Inf)\n})",
        "complete": "test_that(\"Functions return right values\", {\n  expect_equal(computeIC50(concentration = seq(-3,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.9,0), conc_as_log=TRUE, viability_as_pct=FALSE), NA_real_)\n  expect_equal(computeIC50(concentration = seq(1,3), Hill_fit=c(1,.5,0), conc_as_log=TRUE, viability_as_pct=FALSE), Inf)\n  expect_equal(.Hill(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=.7, conc_as_log=TRUE, viability_as_pct=FALSE), c(1,0,0)), .3)\n  expect_equal(computeICn(concentration = seq(1,3), Hill_fit=c(1,0,0), n=0, conc_as_log=TRUE, viability_as_pct=FALSE), -Inf)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_feature_extraction.py",
    "language": "py",
    "content": "from readii.loaders import (\n    loadDicomSITK, \n    loadRTSTRUCTSITK, \n    loadSegmentation,\n) \n\nfrom readii.feature_extraction import (\n    singleRadiomicFeatureExtraction,\n    radiomicFeatureExtraction,\n)\n\nimport pytest\nimport collections\nimport pandas as pd\nimport os \n\n@pytest.fixture\ndef nsclcCTImage():\n    nsclcCTPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n    return loadDicomSITK(nsclcCTPath)\n\n@pytest.fixture\ndef nsclcSEGImage():\n    nsclcSEGPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n    segDictionary = loadSegmentation(nsclcSEGPath, modality = 'SEG')\n    return segDictionary['Heart']\n\n@pytest.fixture\ndef lung4DCTImage():\n    lung4DCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n    return loadDicomSITK(lung4DCTPath)\n\n@pytest.fixture\ndef lung4DRTSTRUCTImage():\n    lung4DRTSTRUCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-47.35/1-1.dcm\"\n    lung4DCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n    segDictionary = loadSegmentation(lung4DRTSTRUCTPath, modality = 'RTSTRUCT',\n                                     baseImageDirPath = lung4DCTPath, roiNames = 'Tumor_c.*')\n    return segDictionary['Tumor_c40']\n\n@pytest.fixture\ndef pyradiomicsParamFilePath():\n    return \"src/readii/data/default_pyradiomics.yaml\"\n\n@pytest.fixture\ndef nsclcMetadataPath():\n    return \"tests/output/ct_to_seg_match_list_NSCLC_Radiogenomics.csv\"\n\n\ndef test_singleRadiomicFeatureExtraction_SEG(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath):\n    \"\"\"Test single image feature extraction with a CT and SEG\"\"\"\n\n    actual = singleRadiomicFeatureExtraction(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict, \\\n        \"Wrong return type, expect a collections.OrderedDict\"\n    assert len(actual) == 1353, \\\n        \"Wrong return size, check pyradiomics parameter file is correct\"\n    assert actual['diagnostics_Configuration_Settings']['label'] == 255, \\\n        \"Wrong label getting passed for ROI\"\n    assert actual['diagnostics_Image-original_Size'] == (26, 21, 20), \\\n        \"Cropped CT image is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == (26, 21, 20), \\\n        \"Cropped segmentation mask is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size'], \\\n        \"Cropped CT and segmentation mask dimensions do not match\"\n    assert actual['original_shape_MeshVolume'].tolist()== pytest.approx(1273.7916666666667), \\\n        \"Volume feature is incorrect\"\n\n\ndef test_singleRadiomicFeatureExtraction_RTSTRUCT(lung4DCTImage, lung4DRTSTRUCTImage, pyradiomicsParamFilePath):\n    \"\"\"Test single image feature extraction with a CT and RTSTRUCT\"\"\"\n\n    actual = singleRadiomicFeatureExtraction(lung4DCTImage, lung4DRTSTRUCTImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict, \\\n        \"Wrong return type, expect a collections.OrderedDict\"\n    assert len(actual) == 1353, \\\n        \"Wrong return size, check pyradiomics parameter file is correct\"\n    assert actual['diagnostics_Configuration_Settings']['label'] == 1, \\\n        \"Wrong label getting passed for ROI\"\n    assert actual['diagnostics_Image-original_Size'] == (51, 92, 28), \\\n        \"Cropped CT image is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == (51, 92, 28), \\\n        \"Cropped segmentation mask is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size'], \\\n        \"Cropped CT and segmentation mask dimensions do not match\"\n    assert actual['original_shape_MeshVolume'].tolist()== pytest.approx(66346.66666666667), \\\n        \"Volume feature is incorrect\"\n\n\ndef test_radiomicFeatureExtraction(nsclcMetadataPath):\n    \"\"\"Test full radiomicFeatureExtraction function with CT and SEG and default PyRadiomics \n       parameter file and no output\"\"\"\n    \n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath=\"tests/\",\n                                       roiNames = None)\n    assert isinstance(actual, pd.DataFrame), \\\n        \"Wrong return type, expect a pandas DataFrame\"\n    assert actual.shape[1] == 1365, \\\n        \"Wrong return size, should include image metadata, diagnostics, and pyradiomics features\"\n    assert actual['diagnostics_Configuration_Settings'][0]['label'] == 255, \\\n        \"Wrong label getting passed for ROI\"\n    assert actual['diagnostics_Image-original_Size'][0] == (26, 21, 20), \\\n        \"Cropped CT image is incorrect size\"\n    assert actual['diagnostics_Mask-original_Size'][0] == (26, 21, 20), \\\n        \"Cropped segmentation mask is incorrect size\"\n    assert actual['original_shape_MeshVolume'][0].tolist()== pytest.approx(1273.7916666666667), \\\n        \"Volume feature is incorrect\"\n\n\ndef test_radiomicFeatureExtraction_output(nsclcMetadataPath):\n    \"\"\"Test output creation from radiomic feature extraction\"\"\"\n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath = \"tests/\",\n                                       roiNames = None,\n                                       outputDirPath = \"tests/output/\")\n    assert os.path.exists(\"tests/output/features/radiomicfeatures_original_NSCLC_Radiogenomics.csv\")",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@pytest.fixture` decorators in this code, and how are they used in the test functions?",
        "answer": "The `@pytest.fixture` decorators are used to define reusable test data or objects. They create functions that can be used across multiple test functions without repeating code. In this snippet, fixtures are used to load and prepare image data (CT scans, segmentations) and file paths. Test functions can then use these fixtures as arguments, allowing pytest to automatically provide the required data for each test."
      },
      {
        "question": "How does the `singleRadiomicFeatureExtraction` function differ in its handling of SEG and RTSTRUCT image types, based on the test functions provided?",
        "answer": "The `singleRadiomicFeatureExtraction` function handles SEG and RTSTRUCT image types differently in terms of the label values and image dimensions:\n1. For SEG images, the label value is 255, while for RTSTRUCT images, it's 1.\n2. The cropped image sizes differ: SEG images are (26, 21, 20), while RTSTRUCT images are (51, 92, 28).\n3. The volume features (MeshVolume) also differ significantly between the two types.\nDespite these differences, both types return an OrderedDict with 1353 features, suggesting the function adapts its processing based on the input image type while maintaining a consistent output structure."
      },
      {
        "question": "What is the purpose of the `radiomicFeatureExtraction` function, and how does it differ from `singleRadiomicFeatureExtraction` in terms of input and output?",
        "answer": "The `radiomicFeatureExtraction` function is designed for batch processing of multiple images, unlike `singleRadiomicFeatureExtraction` which processes a single image-mask pair. Key differences include:\n1. Input: `radiomicFeatureExtraction` takes a metadata file path containing information about multiple images, while `singleRadiomicFeatureExtraction` takes individual image and mask objects.\n2. Output: `radiomicFeatureExtraction` returns a pandas DataFrame containing features for all processed images, whereas `singleRadiomicFeatureExtraction` returns an OrderedDict for a single image.\n3. Functionality: `radiomicFeatureExtraction` can process multiple ROIs, save output to a file, and includes additional metadata in its output.\n4. The `radiomicFeatureExtraction` function is more high-level and likely uses `singleRadiomicFeatureExtraction` internally for each image in the batch."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_singleRadiomicFeatureExtraction_SEG(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath):\n    actual = singleRadiomicFeatureExtraction(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict\n    assert len(actual) == 1353\n    assert actual['diagnostics_Configuration_Settings']['label'] == 255\n    assert actual['diagnostics_Image-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size']\n    # Complete the assertion for the 'original_shape_MeshVolume' feature",
        "complete": "def test_singleRadiomicFeatureExtraction_SEG(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath):\n    actual = singleRadiomicFeatureExtraction(nsclcCTImage, nsclcSEGImage, pyradiomicsParamFilePath)\n    assert type(actual) == collections.OrderedDict\n    assert len(actual) == 1353\n    assert actual['diagnostics_Configuration_Settings']['label'] == 255\n    assert actual['diagnostics_Image-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'] == actual['diagnostics_Image-original_Size']\n    assert actual['original_shape_MeshVolume'].tolist() == pytest.approx(1273.7916666666667)"
      },
      {
        "partial": "def test_radiomicFeatureExtraction(nsclcMetadataPath):\n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath=\"tests/\",\n                                       roiNames = None)\n    assert isinstance(actual, pd.DataFrame)\n    assert actual.shape[1] == 1365\n    assert actual['diagnostics_Configuration_Settings'][0]['label'] == 255\n    assert actual['diagnostics_Image-original_Size'][0] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'][0] == (26, 21, 20)\n    # Complete the assertion for the 'original_shape_MeshVolume' feature",
        "complete": "def test_radiomicFeatureExtraction(nsclcMetadataPath):\n    actual = radiomicFeatureExtraction(nsclcMetadataPath,\n                                       imageDirPath=\"tests/\",\n                                       roiNames = None)\n    assert isinstance(actual, pd.DataFrame)\n    assert actual.shape[1] == 1365\n    assert actual['diagnostics_Configuration_Settings'][0]['label'] == 255\n    assert actual['diagnostics_Image-original_Size'][0] == (26, 21, 20)\n    assert actual['diagnostics_Mask-original_Size'][0] == (26, 21, 20)\n    assert actual['original_shape_MeshVolume'][0].tolist() == pytest.approx(1273.7916666666667)"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest",
        "collections",
        "pandas",
        "os"
      ],
      "from_imports": [
        "readii.loaders.loadDicomSITK",
        "readii.feature_extraction.singleRadiomicFeatureExtraction"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/__init__.py",
    "language": "py",
    "content": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import *",
    "qa_pairs": [
      {
        "question": "What is the purpose of using asterisks (*) in the import statements of this Python code snippet?",
        "answer": "The asterisks (*) in the import statements are used for wildcard imports. This means that all public names (functions, classes, variables) defined in the specified modules are imported directly into the current namespace. For example, 'from .imageutils import *' imports all public names from the 'imageutils' module. While convenient, this practice is generally discouraged as it can lead to namespace pollution and make it harder to track where specific functions or classes are coming from."
      },
      {
        "question": "What does the dot (.) before each module name in the import statements indicate?",
        "answer": "The dot (.) before each module name in the import statements indicates relative imports. This means that the modules being imported are located in the same package as the current module. For instance, '.imageutils' refers to an 'imageutils.py' file (or an 'imageutils' directory with an __init__.py file) that is in the same directory as the current file. Relative imports are useful for organizing code within a package and make it easier to restructure the package without changing import statements."
      },
      {
        "question": "What potential issues might arise from using wildcard imports (*) as shown in this code snippet, and how could they be addressed?",
        "answer": "Using wildcard imports (*) can lead to several issues:\n1. Namespace pollution: It becomes unclear which names come from which modules, making the code harder to understand and maintain.\n2. Name conflicts: If two modules define the same name, the last import will silently overwrite previous ones.\n3. Reduced readability: It's not immediately clear what specific functions or classes are being used from each module.\n\nTo address these issues:\n1. Use specific imports instead, e.g., 'from .imageutils import specific_function, SpecificClass'.\n2. Use module aliases, e.g., 'import .imageutils as iu' and then use 'iu.function_name()'.\n3. If many names are needed, import the module and use the module name as a prefix, e.g., 'import .imageutils' and use 'imageutils.function_name()'.\n\nThese approaches improve code clarity, reduce the risk of conflicts, and make dependencies more explicit."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .",
        "complete": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import *"
      },
      {
        "partial": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import",
        "complete": "from .imageutils import *\nfrom .arrayutils import *\nfrom .crawl import *\nfrom .dicomutils import *\nfrom .args import *\nfrom .nnunet import *\nfrom .autopipeutils import *"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "imageutils.*",
        "arrayutils.*",
        "crawl.*",
        "dicomutils.*",
        "args.*",
        "nnunet.*",
        "autopipeutils.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_computeABC.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking computeABC.\")\n\ntest_that(\"Function complains when given insensible input\",{\n\t\n\texpect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, 2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tHill_fit1 = c(1, 0, 0.1),\n\t\tHill_fit2 = c(0.5, 0.2, 1)), \"Please pass in only one\")\n\t# expect_silent(computeABC(conc1 = c(1, 2, 3),\n\t# \tconc2 = c(1, 2, 3),\n\t# \tviability1 = c(50, 60, 70),\n\t# \tHill_fit2 = c(0.5, 0.2, 1)))\n\n\texpect_error(computeABC(conc1 = c(1, 2,3),\n\t\tconc2 = c(1, 2, 3, 4),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10)), \"is not of same length\") #should complain\n\texpect_error(computeABC(conc1 = c(-1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tconc_as_log = FALSE)) #should complain\n\t##TO-DO::Add warning string to expect_warning call\n\texpect_error(computeABC(conc1 = c(NA, \"cat\", 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tconc_as_log = FALSE)) #should complain\n\texpect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10),\n\t\tverbose = NA)) #should complain\n\texpect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70))) #should complain\n\texpect_error(computeABC(conc1 = c(1, 2, Inf),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(50, 60, 70),\n\t\tviability2 = c(40, 90, 10))) #should complain\n\t##TO-DO::Add warning string to expect_warning call\n\texpect_warning(expect_error(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, -2, 3),\n\t\tviability1 = c(.50, .60, .70),\n\t\tviability2 = c(.40, .90, .10),\n\t\tviability_as_pct = TRUE))) #should complain\n\texpect_warning(computeABC(conc1 = c(1, 2, 3),\n\t\tconc2 = c(1, 2, 3),\n\t\tviability1 = c(.50, .60, .70),\n\t\tviability2 = c(.40, .90, .10),\n\t\tviability_as_pct = TRUE)) #should complain\n\texpect_error(computeABC()) #should complain\n})\n\ntest_that(\"Function values make sense\",{\n\texpect_equal(computeABC(conc1=c(-1,0,1), conc2=c(-1,0,1), Hill_fit1=c(0,1,0), Hill_fit2=c(1,0,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0.5)\n\texpect_equal(computeABC(conc1=c(-1,0,1), conc2=c(-1,0,1), Hill_fit1=c(0,1,0), Hill_fit2=c(0,1,0), conc_as_log=TRUE, viability_as_pct=FALSE), 0)\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'computeABC' function based on the test cases provided?",
        "answer": "The 'computeABC' function appears to be designed to compute some measure (possibly Area Between Curves) using concentration and viability data from two different conditions. It takes inputs such as concentration values, viability values, and Hill fit parameters. The function seems to handle various input validations and edge cases, as evidenced by the numerous error checks in the test cases."
      },
      {
        "question": "What are some of the input validation checks performed by the 'computeABC' function?",
        "answer": "Based on the test cases, the 'computeABC' function performs several input validation checks, including: ensuring that only one set of parameters is passed, verifying that input vectors have the same length, checking for negative concentration values when 'conc_as_log' is FALSE, validating that inputs are numeric (not strings or NA), ensuring that 'verbose' is a logical value, checking for the presence of required parameters, and verifying that concentration values are finite (not Inf)."
      },
      {
        "question": "How does the 'computeABC' function handle viability values, and what potential issue is highlighted in the test cases?",
        "answer": "The 'computeABC' function appears to handle viability values in two ways, depending on the 'viability_as_pct' parameter. When 'viability_as_pct' is TRUE, the function expects viability values to be percentages (e.g., 50, 60, 70). However, the test cases highlight a potential issue where passing decimal values (e.g., 0.50, 0.60, 0.70) with 'viability_as_pct' set to TRUE results in a warning. This suggests that users need to be careful about the format of viability data they provide and ensure it matches the 'viability_as_pct' setting."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    Hill_fit1 = c(1, 0, 0.1),\n    Hill_fit2 = c(0.5, 0.2, 1)), \"Please pass in only one\")\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3, 4),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10)), \"is not of same length\")\n  \n  # Add more expect_error tests here\n})",
        "complete": "test_that(\"Function complains when given insensible input\", {\n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    Hill_fit1 = c(1, 0, 0.1),\n    Hill_fit2 = c(0.5, 0.2, 1)), \"Please pass in only one\")\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3, 4),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10)), \"is not of same length\")\n  \n  expect_error(computeABC(conc1 = c(-1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    conc_as_log = FALSE))\n  \n  expect_error(computeABC(conc1 = c(NA, \"cat\", 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    conc_as_log = FALSE))\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10),\n    verbose = NA))\n  \n  expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70)))\n  \n  expect_error(computeABC(conc1 = c(1, 2, Inf),\n    conc2 = c(1, -2, 3),\n    viability1 = c(50, 60, 70),\n    viability2 = c(40, 90, 10)))\n  \n  expect_warning(expect_error(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, -2, 3),\n    viability1 = c(.50, .60, .70),\n    viability2 = c(.40, .90, .10),\n    viability_as_pct = TRUE)))\n  \n  expect_warning(computeABC(conc1 = c(1, 2, 3),\n    conc2 = c(1, 2, 3),\n    viability1 = c(.50, .60, .70),\n    viability2 = c(.40, .90, .10),\n    viability_as_pct = TRUE))\n  \n  expect_error(computeABC())\n})"
      },
      {
        "partial": "test_that(\"Function values make sense\", {\n  # Test case 1\n  expect_equal(computeABC(conc1 = c(-1, 0, 1),\n                         conc2 = c(-1, 0, 1),\n                         Hill_fit1 = c(0, 1, 0),\n                         Hill_fit2 = c(1, 0, 0),\n                         conc_as_log = TRUE,\n                         viability_as_pct = FALSE),\n             0.5)\n  \n  # Add test case 2 here\n})",
        "complete": "test_that(\"Function values make sense\", {\n  # Test case 1\n  expect_equal(computeABC(conc1 = c(-1, 0, 1),\n                         conc2 = c(-1, 0, 1),\n                         Hill_fit1 = c(0, 1, 0),\n                         Hill_fit2 = c(1, 0, 0),\n                         conc_as_log = TRUE,\n                         viability_as_pct = FALSE),\n             0.5)\n  \n  # Test case 2\n  expect_equal(computeABC(conc1 = c(-1, 0, 1),\n                         conc2 = c(-1, 0, 1),\n                         Hill_fit1 = c(0, 1, 0),\n                         Hill_fit2 = c(0, 1, 0),\n                         conc_as_log = TRUE,\n                         viability_as_pct = FALSE),\n             0)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/writers.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport json\nimport csv\nimport pickle\nimport shutil\nfrom datetime import datetime, timezone\n\nimport h5py\nimport numpy as np\n\nimport SimpleITK as sitk\nimport nrrd\nfrom skimage.measure import regionprops\n\nfrom ..utils import image_to_array\n\n\nclass BaseWriter:\n    def __init__(self, root_directory, filename_format, create_dirs=True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        if create_dirs and not os.path.exists(self.root_directory):\n            os.makedirs(self.root_directory)\n\n    def put(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def _get_path_from_subject_id(self, subject_id, **kwargs):\n        now = datetime.now(timezone.utc)\n        date = now.strftime(\"%Y-%m-%d\")\n        time = now.strftime(\"%H%M%S\")\n        date_time = date + \"_\" + time\n        out_filename = self.filename_format.format(subject_id=subject_id,\n                                                   date=date,\n                                                   time=time,\n                                                   date_time=date_time,\n                                                   **kwargs)\n        out_path = pathlib.Path(self.root_directory, out_filename).as_posix()\n        out_dir = os.path.dirname(out_path)\n        if self.create_dirs and not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)  # create subdirectories if specified in filename_format\n\n        return out_path\n\n\nclass BaseSubjectWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        if os.path.exists(self.root_directory):\n            # delete the folder called {subject_id} that was made in the original BaseWriter / the one named {label_or_image}\n            if os.path.basename(os.path.dirname(self.root_directory)) == \"{subject_id}\":\n                shutil.rmtree(os.path.dirname(self.root_directory))\n            elif \"{label_or_image}{train_or_test}\" in os.path.basename(self.root_directory):\n                shutil.rmtree(self.root_directory)\n\n    def put(self, subject_id, \n            image, is_mask=False, \n            nnunet_info=None, \n            label_or_image: str = \"images\", \n            mask_label: str=\"\", \n            train_or_test: str = \"Tr\", **kwargs):\n        \n        if is_mask:\n            # remove illegal characters for Windows/Unix\n            badboys = '<>:\"/\\|?*'\n            for char in badboys: \n                mask_label = mask_label.replace(char, \"\")\n\n            # filename_format eh\n            self.filename_format = mask_label + \".nii.gz\"  # save the mask labels as their rtstruct names\n\n        if nnunet_info:\n            if label_or_image == \"labels\":\n                filename = f\"{subject_id}.nii.gz\"  # naming convention for labels\n            else:\n                filename = self.filename_format.format(subject_id=subject_id, modality_index=nnunet_info['modalities'][nnunet_info['current_modality']])  # naming convention for images\n            out_path = self._get_path_from_subject_id(filename, label_or_image=label_or_image, train_or_test=train_or_test)\n        else:\n            out_path = self._get_path_from_subject_id(self.filename_format, subject_id=subject_id)\n        sitk.WriteImage(image, out_path, self.compress)\n\n    def _get_path_from_subject_id(self, filename, **kwargs):\n        root_directory = self.root_directory.format(**kwargs)  # replace the {} with the kwargs passed in from .put() (above)\n        out_path = pathlib.Path(root_directory, filename).as_posix()\n        out_dir = os.path.dirname(out_path)\n        if self.create_dirs and not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)  # create subdirectories if specified in filename_format\n        return out_path\n\n\nclass ImageFileWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.compress = compress\n\n    def put(self, subject_id, image, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        sitk.WriteImage(image, out_path, self.compress)\n\n        \nclass SegNrrdWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.seg.nrrd\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        if compress:\n            self.compression_level = 9\n        else:\n            self.compression_level = 1\n\n    def put(self, subject_id, mask, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        labels = [k for k in mask.roi_names]\n        print(labels)\n\n        origin = mask.GetOrigin()\n        spacing = mask.GetSpacing()\n        #direction = mask.GetDirection()\n\n        space = \"left-posterior-superior\"  # everything is ITK read/write \n\n        # fix reverted somewhere.... :''''(\n        space_directions = [[spacing[0], 0., 0.],\n                            [0., spacing[1], 0.],\n                            [0., 0., spacing[2]]]\n        kinds = ['domain', 'domain', 'domain']\n        dims = 3\n\n        # permute axes to original orientations\n        if len(labels) > 1: \n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3, -4])\n\n            # add extra dimension to metadata\n            space_directions.insert(0, [float('nan'), float('nan'), float('nan')])\n            kinds.insert(0, 'vector')\n            dims += 1 \n        else:\n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3])\n        \n        # ensure proper conversion to array\n        assert mask.GetSize() == arr.shape[-3:]\n\n        segment_info = {}\n        for n, i in enumerate(labels):\n            try:\n                if len(labels) > 1:\n                    props = regionprops(arr[n])[0]\n                else:\n                    props = regionprops(arr)[0]\n                bbox = props[\"bbox\"]\n                bbox_segment = [bbox[0], bbox[3], bbox[1], bbox[4], bbox[2], bbox[5]]\n            except IndexError:  # mask is empty\n                assert arr[n].sum() == 0, \"Mask not empty but 'skimage.measure.regionprops' failed.\"\n                bbox_segment = [0, 0, 0, 0, 0, 0]\n\n            segment_info[f\"Segment{n}_Color\"] = list(np.random.random(3))\n            segment_info[f\"Segment{n}_ColorAutoGenerated\"] = '1'\n            segment_info[f\"Segment{n}_Extent\"] = bbox_segment\n            segment_info[f\"Segment{n}_ID\"] = str(n)\n            segment_info[f\"Segment{n}_Name\"] = i\n            segment_info[f\"Segment{n}_NameautoGenerated\"] = '0'\n        \n        header = {'dimension': dims,\n                  'space': space,\n                  'sizes': mask.GetSize(),\n                  'space directions': space_directions,\n                  'kinds': kinds,\n                  'endian': 'little',\n                  'space origin': origin,\n                  'roi_names': labels,\n                  **segment_info}\n        \n        nrrd.write(out_path, arr, header=header, compression_level=self.compression_level, **kwargs)\n\n\nclass NumpyWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.npy\", create_dirs=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n\n    def put(self, subject_id, image, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        if isinstance(image, sitk.Image):\n            array, *_ = image_to_array(image)  # TODO (Michal) optionally save the image geometry\n        np.save(out_path, array)\n\n\nclass HDF5Writer(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.h5\", create_dirs=True, save_geometry=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.save_geometry = save_geometry\n\n    def put(self, subject_id, images, metadata=None, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        with h5py.File(out_path, \"w\") as f:\n            if not isinstance(images, dict):\n                images = {\"image\": images}\n            for k, v in images.items():\n                array, origin, direction, spacing = image_to_array(v)\n                dataset = f.create_dataset(k, data=array)\n                dataset.attrs.create(\"subject_id\", subject_id)\n                if self.save_geometry:\n                    dataset.attrs.create(\"origin\", data=origin)\n                    dataset.attrs.create(\"direction\", data=direction)\n                    dataset.attrs.create(\"spacing\", data=spacing)\n            if metadata:\n                for k, attrs in metadata.items():\n                    for name, v in attrs:\n                        f[subject_id].attrs.create(name, data=v)\n\n\nclass MetadataWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.json\", create_dirs=True, remove_existing=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.file_format = os.path.splitext(filename_format)[1].lstrip(\".\")\n        self.remove_existing = remove_existing\n        if self.file_format not in [\"json\", \"csv\", \"pkl\"]:\n            raise ValueError(f\"File format {self.file_format} not supported. Supported formats: JSON (.json), CSV (.csv), Pickle (.pkl).\")\n\n        if self.file_format == \"csv\" and self.remove_existing:\n            out_path = pathlib.Path(self.root_directory, self.filename_format).as_posix()\n            if os.path.exists(out_path):\n                os.remove(out_path) # remove existing CSV instead of appending\n\n    def _put_json(self, out_path, **kwargs):\n        with open(out_path, \"w\") as f:\n            json.dump(kwargs, f)\n\n    def _put_csv(self, out_path, **kwargs):\n        with open(out_path, \"a+\") as f:\n            writer = csv.DictWriter(f, fieldnames=kwargs.keys())\n            pos = f.tell()\n            f.seek(0)\n            sample = \"\\n\".join([f.readline() for _ in range(2)])\n            if sample == \"\\n\" or not csv.Sniffer().has_header(sample):\n                writer.writeheader()\n            f.seek(pos)\n            writer.writerow(kwargs)\n\n    def _put_pickle(self, out_path, **kwargs):\n        with open(out_path, \"wb\") as f:\n            pickle.dump(kwargs, f)\n\n    def put(self, subject_id, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id)\n\n        if \"subject_id\" not in kwargs:\n            kwargs[\"subject_id\"] = subject_id\n\n        if self.file_format == \"json\":\n            self._put_json(out_path, **kwargs)\n        elif self.file_format == \"csv\":\n            self._put_csv(out_path, **kwargs)\n        elif self.file_format == \"pkl\":\n            self._put_pickle(out_path, **kwargs)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the BaseWriter class and how does it handle directory creation?",
        "answer": "The BaseWriter class is a base class for writing data to files. It initializes with a root directory and filename format. If create_dirs is True (default), it creates the root directory if it doesn't exist. The _get_path_from_subject_id method generates a full file path based on the subject ID and current date/time, creating any necessary subdirectories."
      },
      {
        "question": "How does the BaseSubjectWriter class differ from the BaseWriter class, and what specific functionality does it add?",
        "answer": "The BaseSubjectWriter class extends BaseWriter and adds functionality specific to writing subject data. It includes options for compression and handles special cases for mask labels. It also implements a put method that writes image data using SimpleITK, with support for different naming conventions based on whether it's writing labels or images for nnUNet."
      },
      {
        "question": "What is the purpose of the SegNrrdWriter class and how does it handle the writing of segmentation data?",
        "answer": "The SegNrrdWriter class is designed to write segmentation data in the NRRD format. It handles multiple labels, creates a header with segment information including color, extent, and name for each segment. It also manages the conversion of SimpleITK image data to numpy arrays, handles axis permutation, and writes the data using the nrrd library with specified compression levels."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class BaseSubjectWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        if os.path.exists(self.root_directory):\n            # TODO: Implement directory cleanup logic\n            pass\n\n    def put(self, subject_id, image, is_mask=False, nnunet_info=None, label_or_image: str = \"images\", mask_label: str=\"\", train_or_test: str = \"Tr\", **kwargs):\n        # TODO: Implement put method logic\n        pass\n\n    def _get_path_from_subject_id(self, filename, **kwargs):\n        # TODO: Implement path generation logic\n        pass",
        "complete": "class BaseSubjectWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.nii.gz\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        if os.path.exists(self.root_directory):\n            if os.path.basename(os.path.dirname(self.root_directory)) == \"{subject_id}\":\n                shutil.rmtree(os.path.dirname(self.root_directory))\n            elif \"{label_or_image}{train_or_test}\" in os.path.basename(self.root_directory):\n                shutil.rmtree(self.root_directory)\n\n    def put(self, subject_id, image, is_mask=False, nnunet_info=None, label_or_image: str = \"images\", mask_label: str=\"\", train_or_test: str = \"Tr\", **kwargs):\n        if is_mask:\n            mask_label = ''.join(c for c in mask_label if c not in '<>:\"/\\|?*')\n            self.filename_format = mask_label + \".nii.gz\"\n        if nnunet_info:\n            filename = f\"{subject_id}.nii.gz\" if label_or_image == \"labels\" else self.filename_format.format(subject_id=subject_id, modality_index=nnunet_info['modalities'][nnunet_info['current_modality']])\n            out_path = self._get_path_from_subject_id(filename, label_or_image=label_or_image, train_or_test=train_or_test)\n        else:\n            out_path = self._get_path_from_subject_id(self.filename_format, subject_id=subject_id)\n        sitk.WriteImage(image, out_path, self.compress)\n\n    def _get_path_from_subject_id(self, filename, **kwargs):\n        root_directory = self.root_directory.format(**kwargs)\n        out_path = pathlib.Path(root_directory, filename).as_posix()\n        out_dir = os.path.dirname(out_path)\n        if self.create_dirs and not os.path.exists(out_dir):\n            os.makedirs(out_dir, exist_ok=True)\n        return out_path"
      },
      {
        "partial": "class SegNrrdWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.seg.nrrd\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.compression_level = 9 if compress else 1\n\n    def put(self, subject_id, mask, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        labels = [k for k in mask.roi_names]\n        print(labels)\n\n        # TODO: Implement the rest of the put method\n        pass",
        "complete": "class SegNrrdWriter(BaseWriter):\n    def __init__(self, root_directory, filename_format=\"{subject_id}.seg.nrrd\", create_dirs=True, compress=True):\n        super().__init__(root_directory, filename_format, create_dirs)\n        self.compression_level = 9 if compress else 1\n\n    def put(self, subject_id, mask, **kwargs):\n        out_path = self._get_path_from_subject_id(subject_id, **kwargs)\n        labels = [k for k in mask.roi_names]\n        print(labels)\n\n        origin = mask.GetOrigin()\n        spacing = mask.GetSpacing()\n        space = \"left-posterior-superior\"\n        space_directions = [[spacing[0], 0., 0.], [0., spacing[1], 0.], [0., 0., spacing[2]]]\n        kinds = ['domain', 'domain', 'domain']\n        dims = 3\n\n        if len(labels) > 1:\n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3, -4])\n            space_directions.insert(0, [float('nan'), float('nan'), float('nan')])\n            kinds.insert(0, 'vector')\n            dims += 1\n        else:\n            arr = np.transpose(sitk.GetArrayFromImage(mask), [-1, -2, -3])\n\n        assert mask.GetSize() == arr.shape[-3:]\n\n        segment_info = {}\n        for n, i in enumerate(labels):\n            try:\n                props = regionprops(arr[n] if len(labels) > 1 else arr)[0]\n                bbox = props[\"bbox\"]\n                bbox_segment = [bbox[0], bbox[3], bbox[1], bbox[4], bbox[2], bbox[5]]\n            except IndexError:\n                assert arr[n].sum() == 0 if len(labels) > 1 else arr.sum() == 0, \"Mask not empty but 'skimage.measure.regionprops' failed.\"\n                bbox_segment = [0, 0, 0, 0, 0, 0]\n\n            segment_info[f\"Segment{n}_Color\"] = list(np.random.random(3))\n            segment_info[f\"Segment{n}_ColorAutoGenerated\"] = '1'\n            segment_info[f\"Segment{n}_Extent\"] = bbox_segment\n            segment_info[f\"Segment{n}_ID\"] = str(n)\n            segment_info[f\"Segment{n}_Name\"] = i\n            segment_info[f\"Segment{n}_NameautoGenerated\"] = '0'\n\n        header = {\n            'dimension': dims,\n            'space': space,\n            'sizes': mask.GetSize(),\n            'space directions': space_directions,\n            'kinds': kinds,\n            'endian': 'little',\n            'space origin': origin,\n            'roi_names': labels,\n            **segment_info\n        }\n\n        nrrd.write(out_path, arr, header=header, compression_level=self.compression_level, **kwargs)"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "json",
        "csv",
        "pickle",
        "shutil",
        "h5py",
        "numpy",
        "SimpleITK",
        "nrrd"
      ],
      "from_imports": [
        "datetime.datetime",
        "skimage.measure.regionprops",
        "utils.image_to_array"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/transforms/intensity.py",
    "language": "py",
    "content": "def clip(image, lower, upper):\n    return NotImplementedError\n\n\ndef window(image, window, level):\n    return NotImplementedError\n\n\ndef mean(image, mask=None, labels=None):\n    if mask is not None:\n        pass\n    return NotImplementedError\n\n\ndef var(image, mask=None, labels=None):\n    if mask is not None:\n        pass\n    return NotImplementedError\n\n\ndef standard_scale(image, dataset_mean=0., dataset_var=1.):\n    return (image - dataset_mean) / dataset_var\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'clip' function in the given code snippet, and what parameters does it take?",
        "answer": "The 'clip' function is intended to limit the values in an image to a specified range. It takes three parameters: 'image' (the input image), 'lower' (the lower bound of the range), and 'upper' (the upper bound of the range). However, the function is not implemented in the given code snippet and returns NotImplementedError."
      },
      {
        "question": "How does the 'standard_scale' function work, and what is its purpose in image processing?",
        "answer": "The 'standard_scale' function normalizes an image by subtracting a mean value and dividing by a standard deviation. It takes three parameters: 'image' (the input image), 'dataset_mean' (default 0), and 'dataset_var' (default 1). The function subtracts the dataset mean from each pixel value and then divides by the square root of the dataset variance. This process standardizes the image data, which can be useful for machine learning algorithms or for comparing images with different intensity ranges."
      },
      {
        "question": "What is the significance of the 'mask' and 'labels' parameters in the 'mean' and 'var' functions, and how might they be used if implemented?",
        "answer": "The 'mask' and 'labels' parameters in the 'mean' and 'var' functions suggest that these functions are designed to calculate mean and variance for specific regions or objects within an image. If implemented, the 'mask' parameter could be used to specify which pixels to include in the calculation, while 'labels' could be used to calculate statistics for different labeled regions separately. This functionality would be useful for analyzing specific parts of an image or for segmentation tasks. However, in the given code snippet, these functions are not fully implemented and return NotImplementedError."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def clip(image, lower, upper):\n    return np.clip(image, lower, upper)\n\ndef window(image, window, level):\n    return NotImplementedError\n\ndef mean(image, mask=None, labels=None):\n    if mask is not None:\n        return np.mean(image[mask])\n    return np.mean(image)",
        "complete": "def clip(image, lower, upper):\n    return np.clip(image, lower, upper)\n\ndef window(image, window, level):\n    lower = level - window / 2\n    upper = level + window / 2\n    return clip(image, lower, upper)\n\ndef mean(image, mask=None, labels=None):\n    if mask is not None:\n        return np.mean(image[mask])\n    return np.mean(image)"
      },
      {
        "partial": "def var(image, mask=None, labels=None):\n    if mask is not None:\n        return np.var(image[mask])\n    return np.var(image)\n\ndef standard_scale(image, dataset_mean=0., dataset_var=1.):\n    return NotImplementedError",
        "complete": "def var(image, mask=None, labels=None):\n    if mask is not None:\n        return np.var(image[mask])\n    return np.var(image)\n\ndef standard_scale(image, dataset_mean=0., dataset_var=1.):\n    return (image - dataset_mean) / np.sqrt(dataset_var)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_components.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport urllib.request as request\nfrom zipfile import ZipFile\n\nimport pytest\nimport SimpleITK as sitk\nimport pandas as pd\nfrom imgtools.autopipeline import AutoPipeline\nimport ast\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    output_path = pathlib.Path(curr_path, 'tests', 'temp').as_posix()\n    quebec_path = quebec_path.as_posix()\n    \n    #Dataset name\n    dataset_name  = os.path.basename(quebec_path)\n    imgtools_path = pathlib.Path(os.path.dirname(quebec_path), '.imgtools')\n\n    #Defining paths for autopipeline and dataset component\n    crawl_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.csv\").as_posix()\n    edge_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}_edges.csv\").as_posix()\n    # json_path =  pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.json\").as_posix()  # noqa: F841\n    \n    yield quebec_path, output_path, crawl_path, edge_path\n\n\n\n\n# @pytest.mark.parametrize(\"modalities\",[\"PT\", \"CT,RTSTRUCT\", \"CT,RTDOSE\", \"CT,PT,RTDOSE\", \"CT,RTSTRUCT,RTDOSE\", \"CT,RTSTRUCT,RTDOSE,PT\"])\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])#, \"CT,RTDOSE,PT\"])\nclass TestComponents:\n    \"\"\"\n    For testing the autopipeline components of the med-imagetools package\n    It has two methods:\n    test_pipeline:\n        1) Checks if there is any crawler and edge table output generated by autopipeline\n        2) Checks if for the test data, the lengths of the crawler and edge table matches the actual length of what should be ideally created\n        3) Checks if the length of component table(dataset.csv) is correct or not\n        4) Checks for every component, the shape of all different modalities matches or not\n\n    \"\"\"\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n    \n    def test_pipeline(self, modalities):\n        \"\"\"\n        Testing the Autopipeline for processing the DICOMS and saving it as nrrds\n        \"\"\"\n        n_jobs = 2\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        #Initialize pipeline for the current setting\n        pipeline = AutoPipeline(self.input_path, output_path_mod, modalities, n_jobs=n_jobs, spacing=(5,5,5), overwrite=True)\n        #Run for different modalities\n        comp_path = pathlib.Path(output_path_mod, \"dataset.csv\").as_posix()\n        pipeline.run()\n\n        #Check if the crawl and edges exist\n        assert os.path.exists(self.crawl_path) & os.path.exists(self.edge_path), \"There was no crawler output\"\n\n        #for the test example, there are 6 files and 4 connections\n        crawl_data = pd.read_csv(self.crawl_path, index_col=0)\n        edge_data = pd.read_csv(self.edge_path)\n        # this assert will fail....\n        assert (len(crawl_data) == 12) & (len(edge_data) == 10), \"There was an error in crawling or while making the edge table\"\n\n        #Check if the dataset.csv is having the correct number of components and has all the fields\n        comp_table = pd.read_csv(comp_path, index_col=0)\n        assert len(comp_table) == 2, \"There was some error in making components, check datagraph.parser\"\n\n        #Check the nrrd files\n        subject_id_list = list(comp_table.index)\n        output_streams = [(\"_\").join(cols.split(\"_\")[2:]) for cols in comp_table.columns if cols.split(\"_\")[0] == \"output\"]\n        for subject_id in subject_id_list:\n            shapes = []\n            for col in output_streams:\n                if 'RTSTRUCT' in col:\n                    filename = ast.literal_eval(comp_table.loc[subject_id]['metadata_RTSTRUCT_CT'])[0][0]\n                else:\n                    filename = col\n                \n                print(subject_id, col, filename)\n                path_mod = pathlib.Path(output_path_mod, subject_id, col, f\"{filename}.nii.gz\").as_posix()\n                # All modalities except RTSTRUCT should be of type torchIO.ScalarImage\n                temp_dicom = sitk.GetArrayFromImage(sitk.ReadImage(path_mod))\n                shapes.append(temp_dicom.shape)\n            A = [item == shapes[0] for item in shapes]\n            print(shapes)\n            assert all(A)\n    ",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dataset_path` fixture in this code, and how does it handle the dataset download?",
        "answer": "The `dataset_path` fixture is responsible for setting up the test dataset. It checks if the dataset already exists, and if not, it downloads and extracts it. The fixture performs the following steps:\n1. Determines the path for the dataset\n2. If the dataset doesn't exist, it creates the directory, downloads the zip file, extracts it, and removes the zip file\n3. If the dataset already exists, it skips the download\n4. Sets up paths for the autopipeline and dataset components\n5. Yields the necessary paths for testing"
      },
      {
        "question": "How does the `test_pipeline` method validate the output of the AutoPipeline process?",
        "answer": "The `test_pipeline` method validates the AutoPipeline output through several checks:\n1. It verifies the existence of crawler and edge table outputs\n2. It checks if the lengths of the crawler and edge table match the expected values (12 and 10 respectively)\n3. It ensures the dataset.csv file has the correct number of components (2 in this case)\n4. It checks that all modalities for each subject have the same shape by loading the NIFTI files and comparing their dimensions"
      },
      {
        "question": "What is the purpose of the `@pytest.mark.parametrize` decorator in this code, and how is it being used?",
        "answer": "The `@pytest.mark.parametrize` decorator is used to run the test method with different input parameters. In this code:\n1. It's applied to the `TestComponents` class\n2. It specifies different combinations of modalities to test: 'CT', 'CT,RTSTRUCT', and 'CT,RTSTRUCT,RTDOSE'\n3. This allows the `test_pipeline` method to be executed multiple times, once for each specified modality combination\n4. It helps in testing the AutoPipeline's behavior with various input modality configurations without duplicating test code"
      }
    ],
    "completion_tasks": [
      {
        "partial": "import os\nimport pathlib\nimport urllib.request as request\nfrom zipfile import ZipFile\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    \n    # Complete the function by adding the missing code here\n",
        "complete": "import os\nimport pathlib\nimport urllib.request as request\nfrom zipfile import ZipFile\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    output_path = pathlib.Path(curr_path, 'tests', 'temp').as_posix()\n    quebec_path = quebec_path.as_posix()\n    \n    dataset_name = os.path.basename(quebec_path)\n    imgtools_path = pathlib.Path(os.path.dirname(quebec_path), '.imgtools')\n\n    crawl_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.csv\").as_posix()\n    edge_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}_edges.csv\").as_posix()\n    \n    yield quebec_path, output_path, crawl_path, edge_path"
      },
      {
        "partial": "import pytest\nimport SimpleITK as sitk\nimport pandas as pd\nfrom imgtools.autopipeline import AutoPipeline\nimport ast\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])\nclass TestComponents:\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n    \n    def test_pipeline(self, modalities):\n        n_jobs = 2\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        pipeline = AutoPipeline(self.input_path, output_path_mod, modalities, n_jobs=n_jobs, spacing=(5,5,5), overwrite=True)\n        comp_path = pathlib.Path(output_path_mod, \"dataset.csv\").as_posix()\n        pipeline.run()\n\n        # Complete the test_pipeline method by adding the missing code here\n",
        "complete": "import pytest\nimport SimpleITK as sitk\nimport pandas as pd\nfrom imgtools.autopipeline import AutoPipeline\nimport ast\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])\nclass TestComponents:\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n    \n    def test_pipeline(self, modalities):\n        n_jobs = 2\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        pipeline = AutoPipeline(self.input_path, output_path_mod, modalities, n_jobs=n_jobs, spacing=(5,5,5), overwrite=True)\n        comp_path = pathlib.Path(output_path_mod, \"dataset.csv\").as_posix()\n        pipeline.run()\n\n        assert os.path.exists(self.crawl_path) & os.path.exists(self.edge_path), \"There was no crawler output\"\n\n        crawl_data = pd.read_csv(self.crawl_path, index_col=0)\n        edge_data = pd.read_csv(self.edge_path)\n        assert (len(crawl_data) == 12) & (len(edge_data) == 10), \"There was an error in crawling or while making the edge table\"\n\n        comp_table = pd.read_csv(comp_path, index_col=0)\n        assert len(comp_table) == 2, \"There was some error in making components, check datagraph.parser\"\n\n        subject_id_list = list(comp_table.index)\n        output_streams = [\"_\".join(cols.split(\"_\")[2:]) for cols in comp_table.columns if cols.split(\"_\")[0] == \"output\"]\n        for subject_id in subject_id_list:\n            shapes = []\n            for col in output_streams:\n                if 'RTSTRUCT' in col:\n                    filename = ast.literal_eval(comp_table.loc[subject_id]['metadata_RTSTRUCT_CT'])[0][0]\n                else:\n                    filename = col\n                \n                print(subject_id, col, filename)\n                path_mod = pathlib.Path(output_path_mod, subject_id, col, f\"{filename}.nii.gz\").as_posix()\n                temp_dicom = sitk.GetArrayFromImage(sitk.ReadImage(path_mod))\n                shapes.append(temp_dicom.shape)\n            A = [item == shapes[0] for item in shapes]\n            print(shapes)\n            assert all(A)"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "urllib.request",
        "pytest",
        "SimpleITK",
        "pandas",
        "ast"
      ],
      "from_imports": [
        "zipfile.ZipFile",
        "imgtools.autopipeline.AutoPipeline"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_PharmacoSet_utils.R",
    "language": "R",
    "content": "library(PharmacoGx)\nlibrary(testthat)\ndata(CCLEsmall)\n\n# --\ncontext(\"Testing PharmacoSet subset methods...\")\n\ntest_that('subsetByTreatment works...', {\n    expect_true({\n        treatments <- treatmentNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetByTreatment(CCLEsmall, treatments)\n        })\n        all(treatmentNames(CCLE_sub) %in% treatments)\n    })\n})\n\ntest_that('subsetBySample works...', {\n    expect_true({\n        samples <- sampleNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetBySample(CCLEsmall, samples)\n        })\n        all(sampleNames(CCLE_sub) %in% samples)\n    })\n})\n\ntest_that('subsetByFeature works...', {\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'subsetByTreatment' function in this code, and how is it being tested?",
        "answer": "The 'subsetByTreatment' function is used to create a subset of a PharmacoSet object based on specified treatments. In the test, it's being used to create a subset of the CCLEsmall dataset using the first 5 treatment names. The test checks if all treatment names in the resulting subset are among the specified treatments, verifying that the function correctly filters the dataset."
      },
      {
        "question": "How does the code handle potential warning messages during the subset operations?",
        "answer": "The code uses the 'suppressMessages' function to prevent any messages or warnings from being displayed during the subset operations. This is applied to both 'subsetByTreatment' and 'subsetBySample' function calls, ensuring that the test output remains clean and focused on the actual test results."
      },
      {
        "question": "What is the significance of using 'expect_true' in the test cases, and what condition is being checked?",
        "answer": "The 'expect_true' function is used to assert that a condition is true. In these test cases, it's checking if all elements in the subset (either treatments or samples) are present in the original selection. This verifies that the subsetting functions are correctly filtering the data without including any unintended elements. If the condition is true, the test passes; otherwise, it fails."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that('subsetByFeature works...', {\n    expect_true({\n        features <- featureNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetByFeature(CCLEsmall, features)\n        })\n        # Complete the condition here\n    })\n})",
        "complete": "test_that('subsetByFeature works...', {\n    expect_true({\n        features <- featureNames(CCLEsmall)[1:5]\n        suppressMessages({\n            CCLE_sub <- subsetByFeature(CCLEsmall, features)\n        })\n        all(featureNames(CCLE_sub) %in% features)\n    })\n})"
      },
      {
        "partial": "test_that('subsetByTreatment handles invalid input...', {\n    expect_error({\n        invalid_treatments <- c('InvalidDrug1', 'InvalidDrug2')\n        # Complete the function call here\n    }, 'None of the provided treatment names are in the PharmacoSet')\n})",
        "complete": "test_that('subsetByTreatment handles invalid input...', {\n    expect_error({\n        invalid_treatments <- c('InvalidDrug1', 'InvalidDrug2')\n        subsetByTreatment(CCLEsmall, invalid_treatments)\n    }, 'None of the provided treatment names are in the PharmacoSet')\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_bug_fixes.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    # Fixed Issue31\n    # For now, the fix is to return NA and a warning \n    expect_equal(result, NA_character_)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `annotatePubchemCompound` function in this test case, and what specific attribute is it trying to retrieve?",
        "answer": "The `annotatePubchemCompound` function is being used to retrieve information about a compound from PubChem. In this specific test case, it's attempting to fetch the CAS (Chemical Abstracts Service) number for the compound with PubChem CID '60838'."
      },
      {
        "question": "Why does the test expect the result to be `NA_character_`, and what does this suggest about the function's behavior?",
        "answer": "The test expects the result to be `NA_character_`, which suggests that the function is designed to return NA and issue a warning when it encounters an error or cannot retrieve the requested information. This behavior is mentioned in the comment as a fix for 'Issue31', indicating that it's a deliberate error handling strategy."
      },
      {
        "question": "What libraries are being imported at the beginning of this code snippet, and what might their purposes be in the context of this test?",
        "answer": "The code imports three libraries: AnnotationGx, testthat, and checkmate. AnnotationGx likely contains the `annotatePubchemCompound` function being tested. testthat is a popular R testing framework used to define and run the test case. checkmate might be used for additional assertion functions, although it's not explicitly used in this snippet."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    # TODO: Add expectation here\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    expect_equal(result, NA_character_)\n})"
      },
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    # TODO: Define cid and call annotatePubchemCompound\n\n    expect_equal(result, NA_character_)\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"AnnotationGx::annotatePubchemCompound error from CCLE pipeline\",{\n    cid <- \"60838\"\n    result <- annotatePubchemCompound(cid, \"CAS\")\n\n    expect_equal(result, NA_character_)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/__init__.py",
    "language": "py",
    "content": "from .common import *\nfrom .loaders import *\nfrom .writers import *\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of using asterisks (*) in the import statements of this Python code snippet?",
        "answer": "The asterisks (*) in the import statements are used for wildcard imports. This means that all public names (those not starting with an underscore) from the modules 'common', 'loaders', and 'writers' are imported into the current namespace. While convenient, this practice is generally discouraged as it can lead to namespace pollution and make it unclear where specific names are coming from."
      },
      {
        "question": "How might the use of relative imports (indicated by the dot before module names) affect the structure and organization of this Python project?",
        "answer": "The use of relative imports (indicated by the dot before module names) suggests that this code is part of a package structure. The dot notation means that the imported modules ('common', 'loaders', and 'writers') are in the same directory as the current module. This structure implies a well-organized project with related functionality grouped into separate modules within the same package, promoting modularity and easier maintenance."
      },
      {
        "question": "What potential issues or best practices should be considered when using wildcard imports as shown in this code snippet?",
        "answer": "When using wildcard imports as shown in this snippet, several issues and best practices should be considered: 1) Namespace pollution: All names from the imported modules are brought into the current namespace, which can lead to naming conflicts. 2) Reduced code readability: It becomes unclear where specific names originate from. 3) Potential performance impact: Importing everything can be slower than importing specific names. 4) Difficulty in static analysis: Tools may struggle to determine which names are available. Best practices include using specific imports (e.g., 'from module import specific_name') or using the module name as a prefix (e.g., 'import module' and then 'module.specific_name') to maintain clarity and avoid these issues."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .common import *\nfrom .loaders import *\nfrom .",
        "complete": "from .common import *\nfrom .loaders import *\nfrom .writers import *"
      },
      {
        "partial": "from .common import *\nfrom . import loaders\nfrom . import",
        "complete": "from .common import *\nfrom . import loaders\nfrom . import writers"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "common.*",
        "loaders.*",
        "writers.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CCLE/CCLE_sampleMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\n# get the first part of the name split on _\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1) \n\nusethis::use_data(CCLE_sampleMetadata, overwrite = TRUE)\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in the first line of the code snippet?",
        "answer": "The `system.file()` function is used to locate and construct the file path for a specific file within an R package. In this case, it's locating the file 'CCLE_Cell_lines_annotations_20181226.txt' in the 'extdata' directory of the 'AnnotationGx' package. This ensures that the correct file is accessed regardless of where the package is installed on the user's system."
      },
      {
        "question": "How does the code extract the first part of the CCLE_ID and what function is used to apply this operation to all rows?",
        "answer": "The code extracts the first part of the CCLE_ID by splitting the string on underscores using `strsplit()` and then selecting the first element of the resulting list. This operation is applied to all rows using `purrr::map_chr(1)`, which maps the function of selecting the first element (`1`) over all split strings and returns the result as a character vector."
      },
      {
        "question": "What is the purpose of the `usethis::use_data()` function at the end of the snippet?",
        "answer": "The `usethis::use_data()` function is used to save R objects (in this case, the `CCLE_sampleMetadata` data frame) as internal data in an R package. The `overwrite = TRUE` argument allows the function to overwrite any existing data with the same name. This is typically used when developing R packages to include datasets that can be easily loaded by users of the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\n# get the first part of the name split on _\nCCLE_sampleMetadata$CCLE_ID_parsed <- ",
        "complete": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\n# get the first part of the name split on _\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1)"
      },
      {
        "partial": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1) \n\n",
        "complete": "filePath <- system.file(\"extdata\", \"CCLE_Cell_lines_annotations_20181226.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCCLE_sampleMetadata <- rawdata[, c(\"CCLE_ID\", \"depMapID\", \"Name\")]\n\nCCLE_sampleMetadata$CCLE_ID_parsed <- strsplit(CCLE_sampleMetadata$CCLE_ID, \"_\") |> \n  purrr::map_chr(1) \n\nusethis::use_data(CCLE_sampleMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/transforms/spatial.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\nfrom typing import Sequence, Union\n\n\nINTERPOLATORS = {\n    \"linear\": sitk.sitkLinear,\n    \"nearest\": sitk.sitkNearestNeighbor,\n    \"bspline\": sitk.sitkBSpline,\n}\n\n\ndef resample(image: sitk.Image,\n             spacing: Union[Sequence[float], float],\n             interpolation: str = \"linear\",\n             anti_alias: bool = True,\n             anti_alias_sigma: float = 2.,\n             transform: sitk.Transform = None) -> sitk.Image:\n    \"\"\"Resample image to a given spacing.\n\n\n    Parameters\n    ----------\n    image\n        The image to be resampled.\n\n    spacing\n        The new image spacing. If float, assumes the same spacing in all directions.\n        Alternatively, a sequence of floats can be passed to specify spacing along\n        x, y and z dimensions. Passing 0 at any position will keep the original\n        spacing along that dimension (useful for in-plane resampling).\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `spacing < image.GetSpacing()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n\n    Returns\n    -------\n    out : sitk.Image or tuple of sitk.Image\n        The resampled image. If mask is given, also return resampled mask.\n\n    \"\"\"\n\n    try:\n        interpolator = INTERPOLATORS[interpolation]\n    except KeyError:\n        raise ValueError(f\"interpolator must be one of {list(INTERPOLATORS.keys())}, got {interpolation}.\")\n\n    original_spacing = np.array(image.GetSpacing())\n    original_size = np.array(image.GetSize())\n\n    if isinstance(spacing, (float, int)):\n        new_spacing = np.repeat(spacing, len(original_spacing)).astype(np.float64)\n    else:\n        spacing = np.asarray(spacing)\n        new_spacing = np.where(spacing == 0, original_spacing, spacing)\n    new_size = np.floor(original_size * original_spacing / new_spacing).astype(int)\n\n    rif = sitk.ResampleImageFilter()\n    rif.SetOutputOrigin(image.GetOrigin())\n    rif.SetOutputSpacing(new_spacing)\n    rif.SetOutputDirection(image.GetDirection())\n    rif.SetSize(new_size.tolist())\n\n    if transform is not None:\n        rif.SetTransform(transform)\n\n    downsample = new_spacing > original_spacing\n    if downsample.any() and anti_alias:\n        sigma = np.where(downsample, anti_alias_sigma, 1e-11)\n        image = sitk.SmoothingRecursiveGaussian(image, sigma)  # TODO implement better sigma computation\n\n    rif.SetInterpolator(interpolator)\n    resampled_image = rif.Execute(image)\n\n    return resampled_image\n\n\ndef resize(image, new_size, interpolation=\"linear\"):\n\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n    new_size = np.asarray(new_size)\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image, new_spacing, interpolation=interpolation)\n\n\ndef rotate(image, rotation_centre, angles, interpolation=\"linear\"):\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n    x_angle, y_angle, z_angle = angles\n\n    rotation = sitk.Euler3DTransform(\n        rotation_centre,\n        x_angle,      # the angle of rotation around the x-axis, in radians -> coronal rotation\n        y_angle,      # the angle of rotation around the y-axis, in radians -> saggittal rotation\n        z_angle,      # the angle of rotation around the z-axis, in radians -> axial rotation\n        (0., 0., 0.)  # optional translation (shift) of the image, here we don't want any translation\n    )\n    return resample(image, spacing=image.GetSpacing(), interpolation=interpolation, transform=rotation)\n\n\ndef crop(image, crop_centre, size):\n    \"\"\"\n    \n    Parameters\n    ----------\n    image : \n\n    crop_centre : \n\n    size : \n\n\n    Returns\n    -------\n    out : \n\n    \"\"\"\n    crop_centre = np.asarray(crop_centre, dtype=np.float64)\n    image_shape = np.array((image.GetSize()[::-1]), dtype=np.float64)\n\n    if isinstance(size, int):\n        size_lower = size_upper = np.array([size for _ in image.GetSize()])\n    elif isinstance(size, (tuple, list, np.ndarray)):\n        if isinstance(size[0], int):\n            size_lower = size_upper = np.asarray(size)\n        elif isinstance(size[0], (tuple, list, np.ndarray)):\n            size_lower = np.array([s[0] for s in size])\n            size_upper = np.array([s[1] for s in size])\n\n    if (crop_centre < 0).any() or (crop_centre > image_shape).any():\n        raise ValueError(f\"Crop centre outside image boundaries. Image shape = {image_shape}, crop centre = {crop_centre}\")\n\n    min_x, min_y, min_z = np.clip(np.floor((image_shape - size_lower) / 2).astype(np.int64), 0, image_shape)\n    max_x, max_y, max_z = np.clip(np.floor((image_shape + size_upper) / 2).astype(np.int64), 0, image_shape)\n\n    return image[min_x:max_x, min_y:max_y, min_z:max_z]\n\n\ndef constant_pad(image, size, cval=0.):\n    if isinstance(size, int):\n        size_lower = size_upper = [size for _ in image.GetSize()]\n    elif isinstance(size, (tuple, list, np.ndarray)):\n        if isinstance(size[0], int):\n            size_lower = size_upper = size\n        elif isinstance(size[0], (tuple, list, np.ndarray)):\n            size_lower = [s[0] for s in size]\n            size_upper = [s[1] for s in size]\n    else:\n        raise ValueError(f\"Size must be either int, sequence of int or sequence of sequences of ints, got {size}.\")\n    return sitk.ConstantPad(image, size_lower, size_upper, cval)\n\n\ndef centre_on_point(image, centre):\n    pass\n\n# def resize_by_cropping_or_padding(image, size, centre=None, cval=0.):\n#     original_size = np.array(image.GetSize())\n#     size = np.asarray(size)\n#     centre = np.asarray(centre) if centre is not None else original_size / 2 # XXX is there any benefit to not using floor div here?\n\n#     crop_dims = np.where(size < original_size)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `resample` function in this code, and what are its key parameters?",
        "answer": "The `resample` function is used to resample a SimpleITK image to a new spacing. Its key parameters are:\n1. `image`: The SimpleITK image to be resampled.\n2. `spacing`: The new image spacing, which can be a single float (for uniform spacing) or a sequence of floats for each dimension.\n3. `interpolation`: The interpolation method to use (linear, nearest, or bspline).\n4. `anti_alias`: A boolean to determine whether to apply Gaussian smoothing before downsampling.\n5. `transform`: An optional SimpleITK transform to apply during resampling."
      },
      {
        "question": "How does the `rotate` function work, and what does it return?",
        "answer": "The `rotate` function rotates a SimpleITK image around a specified center point. It works as follows:\n1. Converts the rotation center from index to physical coordinates.\n2. Creates a 3D Euler transform with the specified rotation angles (in radians) around x, y, and z axes.\n3. Calls the `resample` function with the original image spacing and the created rotation transform.\n4. Returns the rotated image as a new SimpleITK Image object."
      },
      {
        "question": "What is the purpose of the `crop` function, and how does it handle different input types for the `size` parameter?",
        "answer": "The `crop` function extracts a subregion of a SimpleITK image around a specified center point. It handles the `size` parameter in three ways:\n1. If `size` is an integer, it creates a cubic crop region with that size in all dimensions.\n2. If `size` is a sequence of integers, it creates a crop region with those dimensions.\n3. If `size` is a sequence of tuples/lists, it uses the first element of each tuple for the lower bound and the second for the upper bound of the crop region in each dimension.\nThe function returns the cropped image as a new SimpleITK Image object."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def resize(image, new_size, interpolation=\"linear\"):\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n    new_size = np.asarray(new_size)\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image, new_spacing, interpolation=interpolation)",
        "complete": "def resize(image, new_size, interpolation=\"linear\"):\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n    new_size = np.asarray(new_size)\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image, new_spacing, interpolation=interpolation)"
      },
      {
        "partial": "def rotate(image, rotation_centre, angles, interpolation=\"linear\"):\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n    x_angle, y_angle, z_angle = angles\n\n    rotation = sitk.Euler3DTransform(\n        rotation_centre,\n        x_angle,\n        y_angle,\n        z_angle,\n        (0., 0., 0.)\n    )\n    return resample(image, spacing=image.GetSpacing(), interpolation=interpolation, transform=rotation)",
        "complete": "def rotate(image, rotation_centre, angles, interpolation=\"linear\"):\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n    x_angle, y_angle, z_angle = angles\n\n    rotation = sitk.Euler3DTransform(\n        rotation_centre,\n        x_angle,\n        y_angle,\n        z_angle,\n        (0., 0., 0.)\n    )\n    return resample(image, spacing=image.GetSpacing(), interpolation=interpolation, transform=rotation)"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy"
      ],
      "from_imports": [
        "typing.Sequence"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/logLogisticRegression.R",
    "language": "R",
    "content": "#' Fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points (c, E) given by the user\n#' and returns a vector containing estimates for HS, E_inf, and EC50.\n#'\n#' By default, logLogisticRegression uses an L-BFGS algorithm to generate the fit. However, if\n#' this fails to converge to solution, logLogisticRegression samples lattice points throughout the parameter space.\n#' It then uses the lattice point with minimal least-squares residual as an initial guess for the optimal parameters,\n#' passes this guess to drm, and re-attempts the optimization. If this still fails, logLogisticRegression uses the\n#' PatternSearch algorithm to fit a log-logistic curve to the data.\n#'\n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeAUC(dose, viability)\n#'\n#' @param conc `numeric` is a vector of drug concentrations.\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of the log_conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells.\n#' @param density `numeric` is a vector of length 3 whose components are the numbers of lattice points per unit\n#' length along the HS-, E_inf-, and base-10 logarithm of the EC50-dimensions of the parameter space, respectively.\n#' @param step `numeric` is a vector of length 3 whose entries are the initial step sizes in the HS, E_inf, and\n#' base-10 logarithm of the EC50 dimensions, respectively, for the PatternSearch algorithm.\n#' @param precision is a positive real number such that when the ratio of current step size to initial step\n#' size falls below it, the PatternSearch algorithm terminates. A smaller value will cause LogisticPatternSearch\n#' to take longer to complete optimization, but will produce a more accurate estimate for the fitted parameters.\n#' @param lower_bounds `numeric` is a vector of length 3 whose entries are the lower bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param upper_bounds `numeric` is a vector of length 3 whose entries are the upper bounds on the HS, E_inf,\n#' and base-10 logarithm of the EC50 parameters, respectively.\n#' @param scale is a positive real number specifying the shape parameter of the Cauchy distribution.\n#' @param family `character`, if \"cauchy\", uses MLE under an assumption of Cauchy-distributed errors\n#' instead of sum-of-squared-residuals as the objective function for assessing goodness-of-fit of\n#' dose-response curves to the data. Otherwise, if \"normal\", uses MLE with a gaussian assumption of errors\n#' @param median_n If the viability points being fit were medians of measurements, they are expected to follow a median of \\code{family}\n#' distribution, which is in general quite different from the case of one measurement. Median_n is the number of measurements\n#' the median was taken of. If the measurements are means of values, then both the Normal and the Cauchy distributions are stable, so means of\n#' Cauchy or Normal distributed variables are still Cauchy and normal respectively.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(EC50) should be returned instead of EC50.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf should be returned as a decimal rather than a percentage.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return A list containing estimates for HS, E_inf, and EC50. It is annotated with the attribute Rsquared, which is the R^2 of the fit.\n#' Note that this is calculated using the values actually used for the fit, after truncation and any transform applied. With truncation, this will be\n#' different from the R^2 compared to the variance of the raw data. This also means that if all points were truncated down or up, there is no variance\n#' in the data, and the R^2 may be NaN.\n#'\n#' @export\n#'\n#' @importFrom CoreGx .meshEval .residual\n#' @importFrom stats optim dcauchy dnorm pcauchy rcauchy rnorm pnorm integrate\nlogLogisticRegression <- function(conc,\n                                  viability,\n                                  density = c(2, 10, 5),\n                                  step = .5 / density,\n                                  precision = 1e-4,\n                                  lower_bounds = c(0, 0, -6),\n                                  upper_bounds = c(4, 1, 6),\n                                  scale = 0.07,\n                                  family = c(\"normal\", \"Cauchy\"),\n                                  median_n = 1,\n                                  conc_as_log = FALSE,\n                                  viability_as_pct = TRUE,\n                                  trunc = TRUE,\n                                  verbose = TRUE) {\n  # guess <- .logLogisticRegressionRaw(conc, viability, density , step, precision, lower_bounds, upper_bounds, scale, Cauchy_flag, conc_as_log, viability_as_pct, trunc, verbose)\n\n\n# .logLogisticRegressionRaw <- function(conc,\n#                                   viability,\n#                                   density = c(2, 10, 2),\n#                                   step = .5 / density,\n#                                   precision = 0.05,\n#                                   lower_bounds = c(0, 0, -6),\n#                                   upper_bounds = c(4, 1, 6),\n#                                   scale = 0.07,\n#                                   Cauchy_flag = FALSE,\n#                                   conc_as_log = FALSE,\n#                                   viability_as_pct = TRUE,\n#                                   trunc = TRUE,\n#                                   verbose = FALSE) {\n  family <- match.arg(family)\n\n\n  if (prod(is.finite(step)) != 1) {\n    print(step)\n    stop(\"Step vector contains elements which are not positive real numbers.\")\n  }\n\n  if (prod(is.finite(precision)) != 1) {\n    print(precision)\n    stop(\"Precision value is not a real number.\")\n  }\n\n  if (prod(is.finite(lower_bounds)) != 1) {\n    print(lower_bounds)\n    stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(upper_bounds)) != 1) {\n    print(upper_bounds)\n    stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  }\n\n  if (prod(is.finite(density)) != 1) {\n    print(density)\n    stop(\"Density vector contains elements which are not real numbers.\")\n  }\n\n  if (is.finite(scale) == FALSE) {\n    print(scale)\n    stop(\"Scale is not a real number.\")\n  }\n\n  if (is.character(family) == FALSE) {\n    print(family)\n    stop(\"Cauchy flag is not a string.\")\n  }\n\n  if (length(density) != 3){\n    stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  }\n\n  if (!median_n==as.integer(median_n)){\n    stop(\"There can only be a integral number of samples to take a median of. Check your setting of median_n parameter, it is not an integer\")\n  }\n\n\n  if (min(upper_bounds - lower_bounds) < 0) {\n    print(rbind(lower_bounds, upper_bounds))\n    stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  }\n\n\n\n  if (min(density) <= 0) {\n    print(density)\n    stop(\"Lattice point density vector contains negative values.\")\n  }\n\n  if (precision <= 0) {\n    print(precision)\n    stop(\"Negative precision value.\")\n  }\n\n  if (min(step) <= 0) {\n    print(step)\n    stop(\"Step vector contains nonpositive numbers.\")\n  }\n\n  if (scale <= 0) {\n    print(scale)\n    stop(\"Scale parameter is a nonpositive number.\")\n  }\n\n\n\n\n\n  CoreGx::.sanitizeInput(x = conc,\n                         y = viability,\n                         x_as_log = conc_as_log,\n                         y_as_log = FALSE,\n                         y_as_pct = viability_as_pct,\n                         trunc = trunc,\n                         verbose = verbose)\n\n  cleanData <- CoreGx::.reformatData(x = conc,\n                               y = viability,\n                               x_to_log = !conc_as_log,\n                               y_to_log = FALSE,\n                               y_to_frac = viability_as_pct,\n                               trunc = trunc)\n\n  if (!(all(lower_bounds < upper_bounds))) {\n    if (verbose == 2) {\n      message(\"lower_bounds:\")\n      message(lower_bounds)\n      message(\"upper_bounds:\")\n      message(upper_bounds)\n    }\n    stop (\"All lower bounds must be less than the corresponding upper_bounds.\")\n  }\n\n\n  log_conc <- cleanData[[\"x\"]]\n  viability <- cleanData[[\"y\"]]\n\n\n  #ATTEMPT TO REFINE GUESS WITH L-BFGS OPTIMIZATION\n  # tryCatch(\n  gritty_guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n                    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n                    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3]))\n\n\n  guess <- CoreGx::.fitCurve(x = log_conc,\n                              y = viability,\n                              f = PharmacoGx:::.Hill,\n                              density = density,\n                              step = step,\n                              precision = precision,\n                              lower_bounds = lower_bounds,\n                              upper_bounds = upper_bounds,\n                              scale = scale,\n                              family = family,\n                              median_n = median_n,\n                              trunc = trunc,\n                              verbose = verbose,\n                              gritty_guess = gritty_guess,\n                              span = 1)\n\n  returnval <- list(\"HS\" = guess[1],\n              \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n              \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3]))\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logLogisticRegression` function and what type of data does it process?",
        "answer": "The `logLogisticRegression` function fits curves of the form E = E_inf + (1 - E_inf)/(1 + (c/EC50)^HS) to dose-response data points. It processes drug concentration (conc) and corresponding cell viability data, returning estimates for Hill Slope (HS), E_inf (efficacy at infinite concentration), and EC50 (concentration for 50% effect)."
      },
      {
        "question": "How does the function handle potential convergence issues during curve fitting?",
        "answer": "The function uses a multi-step approach to handle convergence issues: 1) It first attempts to fit using an L-BFGS algorithm. 2) If that fails, it samples lattice points in the parameter space and uses the best point as an initial guess for optimization. 3) If optimization still fails, it uses the PatternSearch algorithm. This approach ensures robustness in fitting curves to various types of dose-response data."
      },
      {
        "question": "What are the key parameters that control the behavior of the curve fitting process in `logLogisticRegression`, and how do they affect the function's operation?",
        "answer": "Key parameters include: 1) `density`: controls the number of lattice points sampled in each dimension. 2) `step` and `precision`: control the PatternSearch algorithm's behavior. 3) `lower_bounds` and `upper_bounds`: constrain the parameter space. 4) `scale`: shape parameter for Cauchy distribution. 5) `family`: determines the error distribution assumption (normal or Cauchy). 6) `median_n`: accounts for median measurements. These parameters allow fine-tuning of the fitting process for different types of data and desired levels of precision."
      }
    ],
    "completion_tasks": [
      {
        "partial": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  if (prod(is.finite(step)) != 1) stop(\"Step vector contains elements which are not positive real numbers.\")\n  if (!is.finite(precision)) stop(\"Precision value is not a real number.\")\n  if (prod(is.finite(lower_bounds)) != 1) stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(upper_bounds)) != 1) stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(density)) != 1) stop(\"Density vector contains elements which are not real numbers.\")\n  if (!is.finite(scale)) stop(\"Scale is not a real number.\")\n  if (!is.character(family)) stop(\"Family is not a string.\")\n  if (length(density) != 3) stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  if (!median_n == as.integer(median_n)) stop(\"median_n must be an integer.\")\n  if (min(upper_bounds - lower_bounds) < 0) stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  if (min(density) <= 0) stop(\"Lattice point density vector contains negative values.\")\n  if (precision <= 0) stop(\"Negative precision value.\")\n  if (min(step) <= 0) stop(\"Step vector contains nonpositive numbers.\")\n  if (scale <= 0) stop(\"Scale parameter is a nonpositive number.\")\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    # ... (parameters for .fitCurve)\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  )\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}",
        "complete": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  if (prod(is.finite(step)) != 1) stop(\"Step vector contains elements which are not positive real numbers.\")\n  if (!is.finite(precision)) stop(\"Precision value is not a real number.\")\n  if (prod(is.finite(lower_bounds)) != 1) stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(upper_bounds)) != 1) stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(density)) != 1) stop(\"Density vector contains elements which are not real numbers.\")\n  if (!is.finite(scale)) stop(\"Scale is not a real number.\")\n  if (!is.character(family)) stop(\"Family is not a string.\")\n  if (length(density) != 3) stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  if (!median_n == as.integer(median_n)) stop(\"median_n must be an integer.\")\n  if (min(upper_bounds - lower_bounds) < 0) stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  if (min(density) <= 0) stop(\"Lattice point density vector contains negative values.\")\n  if (precision <= 0) stop(\"Negative precision value.\")\n  if (min(step) <= 0) stop(\"Step vector contains nonpositive numbers.\")\n  if (scale <= 0) stop(\"Scale parameter is a nonpositive number.\")\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    x = log_conc,\n    y = viability,\n    f = PharmacoGx:::.Hill,\n    density = density,\n    step = step,\n    precision = precision,\n    lower_bounds = lower_bounds,\n    upper_bounds = upper_bounds,\n    scale = scale,\n    family = family,\n    median_n = median_n,\n    trunc = trunc,\n    verbose = verbose,\n    gritty_guess = gritty_guess,\n    span = 1\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  )\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}"
      },
      {
        "partial": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  # ... (input validation code)\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    # ... (parameters for .fitCurve)\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],\n    \"E_inf\" = ifelse(viability_as_pct, 100 * guess[2], guess[2]),\n    \"EC50\" = ifelse(conc_as_log, guess[3], 10 ^ guess[3])\n  )\n  attr(returnval, \"Rsquare\") <- attr(guess, \"Rsquare\")\n\n  return(returnval)\n}",
        "complete": "logLogisticRegression <- function(conc, viability, density = c(2, 10, 5), step = .5 / density, precision = 1e-4, lower_bounds = c(0, 0, -6), upper_bounds = c(4, 1, 6), scale = 0.07, family = c(\"normal\", \"Cauchy\"), median_n = 1, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  family <- match.arg(family)\n\n  # Input validation\n  if (prod(is.finite(step)) != 1) stop(\"Step vector contains elements which are not positive real numbers.\")\n  if (!is.finite(precision)) stop(\"Precision value is not a real number.\")\n  if (prod(is.finite(lower_bounds)) != 1) stop(\"Lower bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(upper_bounds)) != 1) stop(\"Upper bounds vector contains elements which are not real numbers.\")\n  if (prod(is.finite(density)) != 1) stop(\"Density vector contains elements which are not real numbers.\")\n  if (!is.finite(scale)) stop(\"Scale is not a real number.\")\n  if (!is.character(family)) stop(\"Family is not a string.\")\n  if (length(density) != 3) stop(\"Density parameter needs to have length of 3, for HS, Einf, EC50\")\n  if (!median_n == as.integer(median_n)) stop(\"median_n must be an integer.\")\n  if (min(upper_bounds - lower_bounds) < 0) stop(\"Upper bounds on parameters do not exceed lower bounds.\")\n  if (min(density) <= 0) stop(\"Lattice point density vector contains negative values.\")\n  if (precision <= 0) stop(\"Negative precision value.\")\n  if (min(step) <= 0) stop(\"Step vector contains nonpositive numbers.\")\n  if (scale <= 0) stop(\"Scale parameter is a nonpositive number.\")\n\n  # Data preprocessing\n  cleanData <- CoreGx::.reformatData(x = conc, y = viability, x_to_log = !conc_as_log, y_to_log = FALSE, y_to_frac = viability_as_pct, trunc = trunc)\n  log_conc <- cleanData[['x']]\n  viability <- cleanData[['y']]\n\n  # Initial guess\n  gritty_guess <- c(\n    pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n    pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n    pmin(pmax(log_conc[which.min(abs(viability - 1/2))], lower_bounds[3]), upper_bounds[3])\n  )\n\n  # Curve fitting\n  guess <- CoreGx::.fitCurve(\n    x = log_conc,\n    y = viability,\n    f = PharmacoGx:::.Hill,\n    density = density,\n    step = step,\n    precision = precision,\n    lower_bounds = lower_bounds,\n    upper_bounds = upper_bounds,\n    scale = scale,\n    family = family,\n    median_n = median_n,\n    trunc = trunc,\n    verbose = verbose,\n    gritty_guess = gritty_guess,\n    span = 1\n  )\n\n  # Prepare return value\n  returnval <- list(\n    \"HS\" = guess[1],"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/gCSI/gCSI_sampleMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]\n\nusethis::use_data(gCSI_sampleMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code snippet, and how is it being used?",
        "answer": "The `system.file()` function is used to locate and retrieve the file path of a specific file within an R package. In this code, it's being used to find the 'gCSI_sampleMap.txt' file in the 'extdata/gCSI' directory of the 'AnnotationGx' package. This allows the code to access package-specific data files regardless of where the package is installed on the user's system."
      },
      {
        "question": "Explain the purpose of the `data.table::fread()` function and the `check.names=T` parameter in this context.",
        "answer": "The `data.table::fread()` function is used to read the contents of the file specified by `filePath` into a data.table object called `rawdata`. It's a fast and efficient way to read large data files. The `check.names=T` parameter ensures that the column names in the resulting data.table are valid R variable names, converting any invalid characters or spaces to periods if necessary. This helps prevent issues when accessing columns later in the code."
      },
      {
        "question": "What is the purpose of the last line of code using `usethis::use_data()`, and what does the `overwrite = TRUE` argument do?",
        "answer": "The `usethis::use_data()` function is used to save R objects as internal data in a package. In this case, it's saving the `gCSI_sampleMetadata` object as a dataset within the current package. The `overwrite = TRUE` argument allows the function to overwrite any existing dataset with the same name. This is useful for updating the package's internal data or when running the code multiple times during development."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]\n\n# Complete the code to save gCSI_sampleMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]\n\nusethis::use_data(gCSI_sampleMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the file and extract specific columns\n\nfilePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\n\n# Add code here to read the file and extract columns",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_sampleMap.txt\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath, check.names=T)\ngCSI_sampleMetadata <- rawdata[,c(\"Characteristics.cell.line.\", \"Comment.ENA_SAMPLE.\")]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/AnnotationGx-package.R",
    "language": "R",
    "content": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end\nNULL\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `#' @keywords internal` line in the given code snippet?",
        "answer": "The `#' @keywords internal` line is a roxygen2 tag that marks the following object or function as internal to the package. This means it's not intended for direct use by end-users and won't be included in the package's public documentation."
      },
      {
        "question": "What does the `@importFrom data.table` directive do in this code, and why are there multiple instances of it?",
        "answer": "The `@importFrom data.table` directive is used to import specific functions from the data.table package. There are multiple instances because each one imports a different function or operator (like `:=`, `.BY`, `.EACHI`, etc.). This approach allows the package to use these data.table functions without needing to prefix them with `data.table::` every time they're used in the code."
      },
      {
        "question": "Why is there a `NULL` statement at the end of the code snippet?",
        "answer": "The `NULL` statement at the end of the code snippet is used as a placeholder. In R package development, this is a common practice when defining a package-level documentation file (usually named `packagename-package.R`). The `NULL` ensures that R doesn't try to evaluate any of the roxygen comments as actual code, while still allowing the roxygen2 package to process the documentation tags."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n## usethis namespace: end\nNULL",
        "complete": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end\nNULL"
      },
      {
        "partial": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end",
        "complete": "#' @keywords internal\n\"_PACKAGE\"\n\n## usethis namespace: start\n#' @importFrom data.table :=\n#' @importFrom data.table .BY\n#' @importFrom data.table .EACHI\n#' @importFrom data.table .GRP\n#' @importFrom data.table .I\n#' @importFrom data.table .N\n#' @importFrom data.table .NGRP\n#' @importFrom data.table .SD\n#' @importFrom data.table data.table\n## usethis namespace: end\nNULL"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/dataset_check.py",
    "language": "py",
    "content": "import pathlib\nimport os\nimport re\nimport pandas as pd\nimport torchio as tio\nimport pytest\nimport torch\nimport urllib.request as request\nfrom zipfile import ZipFile\n\nfrom typing import List\nfrom imgtools.io import Dataset\n\n@pytest.fixture(scope=\"session\")\ndef dataset_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    quebec_path = pathlib.Path(pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\").as_posix())\n    \n    if not os.path.exists(quebec_path):\n        pathlib.Path(quebec_path).mkdir(parents=True, exist_ok=True)\n        # Download QC dataset\n        print(\"Downloading the test dataset...\")\n        quebec_data_url = \"https://github.com/bhklab/tcia_samples/blob/main/Head-Neck-PET-CT.zip?raw=true\"\n        quebec_zip_path = pathlib.Path(quebec_path, \"Head-Neck-PET-CT.zip\").as_posix()\n        request.urlretrieve(quebec_data_url, quebec_zip_path)\n        with ZipFile(quebec_zip_path, 'r') as zipfile:\n            zipfile.extractall(quebec_path)\n        os.remove(quebec_zip_path)\n    else:\n        print(\"Data already downloaded...\")\n    output_path = pathlib.Path(curr_path, 'tests','temp').as_posix()\n    quebec_path = quebec_path.as_posix()\n    \n    #Dataset name\n    dataset_name  = os.path.basename(quebec_path)\n    imgtools_path = pathlib.Path(os.path.dirname(quebec_path), '.imgtools')\n\n    #Defining paths for autopipeline and dataset component\n    crawl_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.csv\").as_posix()\n    json_path =  pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}.json\").as_posix()  # noqa: F841\n    edge_path = pathlib.Path(imgtools_path, f\"imgtools_{dataset_name}_edges.csv\").as_posix()\n    assert os.path.exists(crawl_path) & os.path.exists(edge_path) & os.path.exists(json_path), \"There was no crawler output\"\n    \n    yield quebec_path, output_path, crawl_path, edge_path\n\nclass select_roi_names(tio.LabelTransform):\n    \"\"\"\n    Based on the given roi names, selects from the given set\n    \"\"\"\n    def __init__(\n            self,\n            roi_names: List[str] = None,\n            **kwargs\n            ) -> None:\n        super().__init__(**kwargs)\n        self.kwargs = kwargs\n        self.roi_names = roi_names\n    \n    def apply_transform(self, subject):\n        #list of roi_names\n        for image in self.get_images(subject):\n            #For only applying to labelmaps\n            metadata = subject[\"metadata_RTSTRUCT_CT\"]\n            patterns = self.roi_names\n            mask = torch.empty_like(image.data)[:len(patterns)]\n            for j,pat in enumerate(patterns):\n                k = []\n                for i,col in enumerate(metadata):\n                    if re.match(pat,col,flags=re.IGNORECASE):\n                        k.append(i)\n                if len(k)==0:\n                    mask[j] = mask[j]*0\n                else:  \n                    mask[j] = (image.data[k].sum(axis=0)>0)*1    \n            image.set_data(mask)\n        return subject\n    \n    def is_invertible(self):\n        return False\n\n#Defining for test_dataset method in Test_components class\ndef collate_fn(data):\n    \"\"\"\n       data: is a tio.subject with multiple columns\n             Need to return required data\n    \"\"\"\n    mod_names = [items for items in data[0].keys() if items.split(\"_\")[0]==\"mod\"]\n    temp_stack = {}\n    for names in mod_names:\n        temp_stack[names] = torch.stack(tuple(items[names].data for items in data))\n    return temp_stack\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"CT,RTSTRUCT\", \"CT,RTSTRUCT,RTDOSE\"])\nclass TestDataset:\n    \"\"\"\n    For testing the dataset components of the med-imagetools package\n    test_dataset:\n        1) Checks if the length of the dataset matches\n        2) Checks if the items in the subject object is correct and present\n        3) Checks if you are able to load it via load_nrrd and load_directly, and checks if the subjects generated matches\n        4) Checks if torch data loader can load the formed dataset and get atleast 1 iteration\n        5) Checks if the transforms are happening by checking the size\n    \"\"\"\n    @pytest.fixture(autouse=True)\n    def _get_path(self, dataset_path):\n        self.input_path, self.output_path, self.crawl_path, self.edge_path = dataset_path\n        print(dataset_path)\n\n    def test_dataset(self, modalities):\n        \"\"\"\n        Testing the Dataset class\n        \"\"\"\n        output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n        comp_path = pathlib.Path(output_path_mod).resolve().joinpath('dataset.csv').as_posix()\n        comp_table = pd.read_csv(comp_path, index_col=0)\n        print(comp_path, comp_table)\n        \n        #Loading from nrrd files\n        subjects_nrrd = Dataset.load_image(output_path_mod, ignore_multi=True)\n        #Loading files directly\n        # subjects_direct = Dataset.load_directly(self.input_path,modalities=modalities,ignore_multi=True)\n        \n        #The number of subjects is equal to the number of components which is 2 for this dataset\n        # assert len(subjects_nrrd) == len(subjects_direct) == 2, \"There was some error in generation of subject object\"\n        # assert subjects_nrrd[0].keys() == subjects_direct[0].keys()\n\n        # del subjects_direct\n        # To check if all metadata items present in the keys\n        # temp_nrrd = subjects_nrrd[0]\n        # columns_shdbe_present = set([col if col.split(\"_\")[0]==\"metadata\" else \"mod_\"+(\"_\").join(col.split(\"_\")[1:]) for col in list(comp_table.columns) if col.split(\"_\")[0] in [\"folder\",\"metadata\"]])\n        # print(columns_shdbe_present)\n        # assert set(temp_nrrd.keys()).issubset(columns_shdbe_present), \"Not all items present in dictionary, some fault in going through the different columns in a single component\"\n\n        transforms = tio.Compose([tio.Resample(4), tio.CropOrPad((96,96,40)), select_roi_names([\"larynx\"]), tio.OneHot()])\n\n        #Forming dataset and dataloader\n        test_set = tio.SubjectsDataset(subjects_nrrd, transform=transforms)\n        test_loader = torch.utils.data.DataLoader(test_set,batch_size=2,shuffle=True,collate_fn = collate_fn)\n\n        #Check test_set is correct\n        assert len(test_set)==2\n\n        #Get items from test loader\n        #If this function fails , there is some error in formation of test\n        data = next(iter(test_loader))\n        A = [item[1].shape == (2,1,96,96,40) if \"RTSTRUCT\" not in item[0] else item[1].shape == (2,2,96,96,40) for item in data.items()]\n        assert all(A), \"There is some problem in the transformation/the formation of subject object\"\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `dataset_path` fixture in this code, and how does it handle downloading and extracting the dataset?",
        "answer": "The `dataset_path` fixture is responsible for setting up the test dataset. It checks if the dataset already exists, and if not, it downloads and extracts it. The fixture performs the following steps:\n1. Determines the path for the Quebec dataset.\n2. If the dataset doesn't exist, it creates the directory, downloads the dataset from a GitHub URL, extracts the ZIP file, and removes the ZIP file after extraction.\n3. If the dataset already exists, it skips the download process.\n4. It then sets up paths for various components like crawl path, JSON path, and edge path.\n5. Finally, it yields the necessary paths for use in tests."
      },
      {
        "question": "Explain the purpose and functionality of the `select_roi_names` class in the given code snippet.",
        "answer": "The `select_roi_names` class is a custom transform that inherits from `tio.LabelTransform`. Its purpose is to select specific regions of interest (ROIs) from a given set of labels based on provided ROI names. Here's how it works:\n1. It takes a list of ROI names as input.\n2. In the `apply_transform` method, it iterates through the images in the subject.\n3. For each image, it uses the provided ROI names to create a mask.\n4. It matches the ROI names against the metadata columns using regex, ignoring case.\n5. If a match is found, it creates a binary mask for that ROI.\n6. The resulting mask contains only the selected ROIs, with each ROI on a separate channel.\n7. The transform is not invertible, as indicated by the `is_invertible` method returning False."
      },
      {
        "question": "What is the purpose of the `TestDataset` class, and how does it use pytest's parameterization to test different modality combinations?",
        "answer": "The `TestDataset` class is designed to test various aspects of the dataset components in the med-imagetools package. Its main features and functionalities are:\n1. It uses pytest's `@pytest.mark.parametrize` decorator to test three different modality combinations: 'CT', 'CT,RTSTRUCT', and 'CT,RTSTRUCT,RTDOSE'.\n2. The `test_dataset` method performs several checks:\n   a. Verifies if the dataset length matches the expected value.\n   b. Ensures that the correct items are present in the subject object.\n   c. Checks if the dataset can be loaded using both `load_nrrd` and `load_directly` methods.\n   d. Verifies if a PyTorch DataLoader can successfully load the dataset and perform at least one iteration.\n   e. Confirms that the transforms are applied correctly by checking the size of the transformed data.\n3. It uses a custom `collate_fn` to prepare the data for the DataLoader.\n4. The test applies a series of transforms to the dataset, including resampling, cropping/padding, selecting specific ROIs, and one-hot encoding.\n5. Finally, it asserts that the transformed data has the expected shape, ensuring that the entire pipeline works correctly for different modality combinations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class select_roi_names(tio.LabelTransform):\n    def __init__(self, roi_names: List[str] = None, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.kwargs = kwargs\n        self.roi_names = roi_names\n    \n    def apply_transform(self, subject):\n        for image in self.get_images(subject):\n            metadata = subject[\"metadata_RTSTRUCT_CT\"]\n            patterns = self.roi_names\n            mask = torch.empty_like(image.data)[:len(patterns)]\n            for j, pat in enumerate(patterns):\n                # TODO: Implement pattern matching and mask creation\n            image.set_data(mask)\n        return subject\n    \n    def is_invertible(self):\n        return False",
        "complete": "class select_roi_names(tio.LabelTransform):\n    def __init__(self, roi_names: List[str] = None, **kwargs) -> None:\n        super().__init__(**kwargs)\n        self.kwargs = kwargs\n        self.roi_names = roi_names\n    \n    def apply_transform(self, subject):\n        for image in self.get_images(subject):\n            metadata = subject[\"metadata_RTSTRUCT_CT\"]\n            patterns = self.roi_names\n            mask = torch.empty_like(image.data)[:len(patterns)]\n            for j, pat in enumerate(patterns):\n                k = [i for i, col in enumerate(metadata) if re.match(pat, col, flags=re.IGNORECASE)]\n                mask[j] = (image.data[k].sum(axis=0) > 0) * 1 if k else mask[j] * 0\n            image.set_data(mask)\n        return subject\n    \n    def is_invertible(self):\n        return False"
      },
      {
        "partial": "def test_dataset(self, modalities):\n    output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n    comp_path = pathlib.Path(output_path_mod).resolve().joinpath('dataset.csv').as_posix()\n    comp_table = pd.read_csv(comp_path, index_col=0)\n    \n    subjects_nrrd = Dataset.load_image(output_path_mod, ignore_multi=True)\n    \n    transforms = tio.Compose([tio.Resample(4), tio.CropOrPad((96,96,40)), select_roi_names([\"larynx\"]), tio.OneHot()])\n    \n    test_set = tio.SubjectsDataset(subjects_nrrd, transform=transforms)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=True, collate_fn=collate_fn)\n    \n    # TODO: Implement assertions to check the dataset and loader\n",
        "complete": "def test_dataset(self, modalities):\n    output_path_mod = pathlib.Path(self.output_path, str(\"temp_folder_\" + (\"_\").join(modalities.split(\",\")))).as_posix()\n    comp_path = pathlib.Path(output_path_mod).resolve().joinpath('dataset.csv').as_posix()\n    comp_table = pd.read_csv(comp_path, index_col=0)\n    \n    subjects_nrrd = Dataset.load_image(output_path_mod, ignore_multi=True)\n    \n    transforms = tio.Compose([tio.Resample(4), tio.CropOrPad((96,96,40)), select_roi_names([\"larynx\"]), tio.OneHot()])\n    \n    test_set = tio.SubjectsDataset(subjects_nrrd, transform=transforms)\n    test_loader = torch.utils.data.DataLoader(test_set, batch_size=2, shuffle=True, collate_fn=collate_fn)\n    \n    assert len(test_set) == 2, \"Incorrect number of subjects in the dataset\"\n    \n    data = next(iter(test_loader))\n    assert all(item[1].shape == (2,1,96,96,40) if \"RTSTRUCT\" not in item[0] else item[1].shape == (2,2,96,96,40) for item in data.items()), \"Incorrect shape after transformations\""
      }
    ],
    "dependencies": {
      "imports": [
        "pathlib",
        "os",
        "re",
        "pandas",
        "torchio",
        "pytest",
        "torch",
        "urllib.request"
      ],
      "from_imports": [
        "zipfile.ZipFile",
        "typing.List",
        "imgtools.io.Dataset"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/loaders.py",
    "language": "py",
    "content": "import os\nimport pydicom\nimport SimpleITK as sitk\n\nfrom imgtools.ops import StructureSetToSegmentation\nfrom imgtools.io import read_dicom_auto\n\nfrom typing import Optional\n\nfrom readii.utils import get_logger\n\n# Create a global logger instance\nlogger = get_logger()\n\ndef loadDicomSITK(imgDirPath: str) -> sitk.Image:\n    \"\"\"Read DICOM series as SimpleITK Image.\n\n    Parameters\n    ----------\n    img_path : str\n        Path to directory containing the DICOM series to load.\n\n    Returns\n    -------\n    sitk.Image\n        The loaded image.\n    \"\"\"\n    # Set up the reader for the DICOM series\n    logger.debug(f\"Loading DICOM series from directory: {imgDirPath}\")\n    reader = sitk.ImageSeriesReader()\n    dicomNames = reader.GetGDCMSeriesFileNames(imgDirPath)\n    reader.SetFileNames(dicomNames)\n    return reader.Execute()\n\n\ndef loadRTSTRUCTSITK(\n    rtstructPath: str, baseImageDirPath: str, roiNames: Optional[str] = None\n) -> dict:\n    \"\"\"Load RTSTRUCT into SimpleITK Image.\n\n    Parameters\n    ----------\n    rtstructPath : str\n        Path to the DICOM file containing the RTSTRUCT\n    baseImageDirPath : str\n        Path to the directory containing the DICOMS for the original image the segmentation\n        was created from (e.g. CT). This is required to load the RTSTRUCT.\n    roiNames : str\n        Identifier for which region(s) of interest to load from the total segmentation file\n\n    Returns\n    -------\n    dict\n        A dictionary of mask ROIs from the loaded RTSTRUCT image as a SimpleITK image objects.\n        The segmentation label is set to 1.\n    \"\"\"\n    # Set up segmentation loader\n    logger.debug(f\"Making mask using ROI names: {roiNames}\")\n    makeMask = StructureSetToSegmentation(roi_names=roiNames)\n\n    # Read in the base image (CT) and segmentation DICOMs into SITK Images\n    logger.debug(f\"Loading RTSTRUCT from directory: {rtstructPath}\")\n    baseImage = read_dicom_auto(baseImageDirPath)\n    segImage = read_dicom_auto(rtstructPath)\n\n    try:\n        # Get the individual ROI masks\n        segMasks = makeMask(\n            segImage,\n            baseImage.image,\n            existing_roi_indices={\"background\": 0},\n            ignore_missing_regex=False,\n        )\n    except ValueError:\n        return {}\n\n    # Get list of ROIs present in this rtstruct\n    loadedROINames = segMasks.raw_roi_names\n    # Initialize dictionary to store ROI names and images\n    roiStructs = {}\n    # Get each roi and its label and store in dictionary\n    for roi in loadedROINames:\n        # Get the mask for this ROI\n        roiMask = segMasks.get_label(name=roi)\n        # Store the ROI name and image\n        roiStructs[roi] = roiMask\n\n    return roiStructs\n\n\ndef loadSegmentation(\n    segImagePath: str,\n    modality: str,\n    baseImageDirPath: Optional[str] = None,\n    roiNames: Optional[str] = None,\n) -> dict:\n    \"\"\"Function to load a segmentation with the correct function.\n\n    Parameters\n    ----------\n    segImagePath : str\n        Path to the segmentation file to load\n    modality : str\n        Type of image that imgPath points to to load. If RTSTRUCT, must set baseImageDirPath\n    baseImageDirPath : str\n        Path to the directory containing the DICOMS for the original image the segmentation\n        was created from.\n    roiNames : str\n        Identifier for which region(s) of interest to load from the total segmentation file\n\n    Returns\n    -------\n    dict\n        A dictionary of each of the ROIs and their name in the segmentation image as sitk.Image objects.\n\n    Examples\n    --------\n    >>> segImages = loadSegmentation(\"/path/to/segmentation/1.dcm\", 'RTSTRUCT', '/path/to/CT', 'GTVp.*')\n    \"\"\"\n\n    if modality in [\"SEG\", \"seg\"]:\n        # Loading SEG requires directory containing file, not the actual file path\n        imgFolder, _ = os.path.split(segImagePath)\n        segHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n        roiName = segHeader.SegmentSequence[0].SegmentLabel\n        return {roiName: loadDicomSITK(imgFolder)}\n\n    elif modality in [\"RTSTRUCT\", \"rtstruct\"]:\n        if baseImageDirPath == None:\n            raise ValueError(\n                \"Missing path to original image segmentation was taken from. RTSTRUCT loader requires original image.\"\n            )\n        else:\n            return loadRTSTRUCTSITK(segImagePath, baseImageDirPath, roiNames)\n\n    else:\n        raise ValueError(\n            \"This segmentation modality is not supported. Must be one of RTSTRUCT or SEG\"\n        )\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `loadDicomSITK` function, and what type of object does it return?",
        "answer": "The `loadDicomSITK` function is used to read a DICOM series as a SimpleITK Image. It takes a directory path containing the DICOM series as input and returns a `sitk.Image` object representing the loaded image. The function uses `sitk.ImageSeriesReader()` to read the DICOM files in the specified directory."
      },
      {
        "question": "In the `loadRTSTRUCTSITK` function, what does the `roiNames` parameter do, and how does it affect the function's output?",
        "answer": "The `roiNames` parameter in the `loadRTSTRUCTSITK` function is an optional string that identifies which region(s) of interest to load from the total segmentation file. It is used as an argument for the `StructureSetToSegmentation` class to filter the ROIs. The function returns a dictionary where the keys are the ROI names, and the values are SimpleITK image objects representing the masks for each ROI. If `roiNames` is specified, only the matching ROIs will be included in the output dictionary."
      },
      {
        "question": "How does the `loadSegmentation` function handle different types of segmentation files, and what are its requirements for loading an RTSTRUCT file?",
        "answer": "The `loadSegmentation` function handles different types of segmentation files based on the `modality` parameter. For 'SEG' or 'seg' modalities, it loads the segmentation using `loadDicomSITK`. For 'RTSTRUCT' or 'rtstruct' modalities, it uses `loadRTSTRUCTSITK`. When loading an RTSTRUCT file, the function requires the `baseImageDirPath` parameter to be set, which should point to the directory containing the original image (e.g., CT) that the segmentation was created from. If `baseImageDirPath` is not provided for an RTSTRUCT file, the function raises a ValueError."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def loadDicomSITK(imgDirPath: str) -> sitk.Image:\n    \"\"\"Read DICOM series as SimpleITK Image.\"\"\"\n    reader = sitk.ImageSeriesReader()\n    dicomNames = reader.GetGDCMSeriesFileNames(imgDirPath)\n    reader.SetFileNames(dicomNames)\n    # Complete the function",
        "complete": "def loadDicomSITK(imgDirPath: str) -> sitk.Image:\n    \"\"\"Read DICOM series as SimpleITK Image.\"\"\"\n    reader = sitk.ImageSeriesReader()\n    dicomNames = reader.GetGDCMSeriesFileNames(imgDirPath)\n    reader.SetFileNames(dicomNames)\n    return reader.Execute()"
      },
      {
        "partial": "def loadSegmentation(segImagePath: str, modality: str, baseImageDirPath: Optional[str] = None, roiNames: Optional[str] = None) -> dict:\n    \"\"\"Function to load a segmentation with the correct function.\"\"\"\n    if modality.lower() == 'seg':\n        imgFolder, _ = os.path.split(segImagePath)\n        segHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n        roiName = segHeader.SegmentSequence[0].SegmentLabel\n        return {roiName: loadDicomSITK(imgFolder)}\n    elif modality.lower() == 'rtstruct':\n        # Complete the RTSTRUCT loading logic\n    else:\n        # Complete the error handling",
        "complete": "def loadSegmentation(segImagePath: str, modality: str, baseImageDirPath: Optional[str] = None, roiNames: Optional[str] = None) -> dict:\n    \"\"\"Function to load a segmentation with the correct function.\"\"\"\n    if modality.lower() == 'seg':\n        imgFolder, _ = os.path.split(segImagePath)\n        segHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n        roiName = segHeader.SegmentSequence[0].SegmentLabel\n        return {roiName: loadDicomSITK(imgFolder)}\n    elif modality.lower() == 'rtstruct':\n        if baseImageDirPath is None:\n            raise ValueError(\"Missing path to original image segmentation was taken from. RTSTRUCT loader requires original image.\")\n        return loadRTSTRUCTSITK(segImagePath, baseImageDirPath, roiNames)\n    else:\n        raise ValueError(\"This segmentation modality is not supported. Must be one of RTSTRUCT or SEG\")"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pydicom",
        "SimpleITK"
      ],
      "from_imports": [
        "imgtools.ops.StructureSetToSegmentation",
        "imgtools.io.read_dicom_auto",
        "typing.Optional",
        "readii.utils.get_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_negative_controls.py",
    "language": "py",
    "content": "import numpy as np\n\nfrom readii.image_processing import *\nfrom radiomics import imageoperations\nfrom readii.negative_controls import (\n    makeShuffleImage,\n    makeRandomImage,\n    makeRandomSampleFromDistributionImage,\n    negativeControlROIOnly,\n    negativeControlNonROIOnly,\n    applyNegativeControl\n)\n\n\nimport pytest\nimport collections\n\n\n@pytest.fixture\ndef nsclcCTImage():\n    nsclcCTPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n    return loadDicomSITK(nsclcCTPath)\n\n\n@pytest.fixture\ndef nsclcSEGImage():\n    nsclcSEGPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n    segDictionary = loadSegmentation(nsclcSEGPath, modality='SEG')\n    return segDictionary['Heart']\n\n\n@pytest.fixture()\ndef nsclcCTImageFolderPath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n\n\n@pytest.fixture()\ndef nsclcSEGFilePath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n\n\n@pytest.fixture()\ndef nsclcCropped(nsclcCTImage, nsclcSEGImage, nsclcCTImageFolderPath, nsclcSEGFilePath):\n    roiImage = flattenImage(nsclcSEGImage)\n    alignedROIImage = alignImages(nsclcCTImage, roiImage)\n    segmentationLabel = getROIVoxelLabel(alignedROIImage)\n\n    croppedCT, croppedROI = getCroppedImages(nsclcCTImage, alignedROIImage, segmentationLabel)\n\n    return croppedCT, croppedROI, segmentationLabel\n\n\n@pytest.fixture()\ndef randomSeed():\n    return 10\n\ndef test_makeShuffleImage(nsclcCTImage, randomSeed):\n    \" Test negative control to shuffle the whole image\"\n\n    shuffled_image = makeShuffleImage(nsclcCTImage, randomSeed)\n    original_pixels = sitk.GetArrayFromImage(nsclcCTImage)\n    shuffled_pixels = sitk.GetArrayFromImage(shuffled_image)\n\n    assert nsclcCTImage.GetSize() == shuffled_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == shuffled_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == shuffled_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_pixels), \\\n        \"Pixel values are not shuffled\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_pixels.flatten())), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    assert shuffled_pixels[0,0,0] == -987, \\\n        \"Random seed is not working for shuffled image, first voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[-1,-1,-1] == 10, \\\n        \"Random seed is not working for shuffled image, last voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[238,252,124] == -118, \\\n        \"Random seed is not working for shuffled image, central ROI voxel has wrong shuffled value. Random seed should be 10.\"\n\n\ndef test_makeRandomImage(nsclcCTImage, randomSeed):\n    \" Test negative control to randomize the pixels of the whole image\"\n\n    randomized_image = makeRandomImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_image)\n\n    assert nsclcCTImage.GetSize() == randomized_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    assert randomized_arr_image[0,0,0] == 2156, \\\n        \"Random seed is not working for random image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[-1,-1,-1] == 90, \\\n        \"Random seed is not working for random image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[238,252,124] == -840, \\\n        \"Random seed is not working for random image, central ROI voxel has wrong random value. Random seed should be 10.\"\n    \n\ndef test_makeRandomSampleFromDistributionImage(nsclcCTImage, randomSeed):\n    \" Test negative control to uniformly sample the pixels of the whole image from the images pixel distribution\"\n\n    randomized_sampled_image = makeRandomSampleFromDistributionImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_sampled_image)\n\n    assert nsclcCTImage.GetSize() == randomized_sampled_image.GetSize(), \\\n        \"Randomized sampled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_sampled_image.GetSpacing(), \\\n        \"Randomized sampled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_sampled_image.GetOrigin(), \\\n        \"Randomized sampled image origin not same as input image\"\n    assert isinstance(randomized_sampled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_arr_image, randomized_arr_image), \\\n        \"No voxel values have been changed to random.\"\n    assert np.all(np.isin(randomized_arr_image.flatten(), original_arr_image.flatten())), \\\n        \"Retuned object has values not sampled from original image\"\n    assert randomized_arr_image[0,0,0] == -1005, \\\n        \"Random seed is not working for randomized from distribution image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[-1,-1,-1] == 414, \\\n        \"Random seed is not working for randomized from distribution image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[238,252,124] == 49, \\\n        \"Random seed is not working for randomized from distribution image, central ROI voxel has wrong random value. Random seed should be 10.\"\n\n\n\ndef test_makeShuffleROI(nsclcCropped, randomSeed):\n    \" Test negative control to shuffle the roi of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    shuffled_roi_image = negativeControlROIOnly(croppedCT, croppedROI, \"shuffled\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    shuffled_roi_pixels = sitk.GetArrayFromImage(shuffled_roi_image)\n\n    assert croppedCT.GetSize() == shuffled_roi_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert croppedCT.GetSpacing() == shuffled_roi_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == shuffled_roi_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_roi_pixels), \\\n        \"No voxel values are being shuffled.\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_roi_pixels.flatten())), \\\n        \"Shuffled pixel values in ROI are different\"\n    assert shuffled_roi_pixels[0,0,0] == -740, \\\n        \"Voxel outside the ROI is being shuffled. Should just be the ROI voxels.\"\n    assert shuffled_roi_pixels[7,18,11] == -322, \\\n        \"Random seed is not working for shuffled ROI image, centre pixel has wrong value. Random seed should be 10.\"\n\n\ndef test_makeRandomROI(nsclcCropped, randomSeed):\n    \" Test negative control to randomize the pixels of the ROI of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    randomized_roi_image = negativeControlROIOnly(croppedCT, croppedROI, \"randomized\", randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(croppedCT)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n    assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_arr_image, randomized_roi_image), \\\n        \"No voxel values have been changed to random.\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    assert randomized_arr_image[0,0,0] == -740, \\\n        \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n    assert randomized_arr_image[7,18,11] == 1, \\\n        \"Random seed is not working for randomized ROI image, centre pixel has wrong value. Random seed should be 10.\"\n\n\ndef test_makeRandomSampleFromDistributionROI(nsclcCropped, randomSeed):\n    \" Test negative control to uniformly sample the pixels of the ROI of the image randomly\"\n\n    croppedCT, croppedROI, _= nsclcCropped\n\n    randomized_roi_image = negativeControlROIOnly(croppedCT, croppedROI, \"randomized_sampled\", randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(croppedCT)\n\n    randomized_roi_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n    assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n        \"Randomized ROI Sampled image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n        \"Randomized ROI Sampled image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n        \"Randomized ROI Sampled image origin not same as input image\"\n    assert isinstance(randomized_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_arr_image, randomized_roi_arr_image), \\\n        \"No voxel values have been changed to random.\"\n    assert np.all(np.isin(randomized_roi_arr_image.flatten(), original_arr_image.flatten())), \\\n        \"Retuned object has values not sampled from original image\"\n    assert randomized_roi_arr_image[0,0,0] == -740, \\\n        \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n    assert randomized_roi_arr_image[7,18,11] == -81, \\\n        \"Random seed is not working for randomized from distribution ROI image, centre pixel has wrong value. Random seed should be 10.\"\n    \n\ndef test_makeShuffleNonROI(nsclcCropped, randomSeed):\n    \" Test negative control to shuffle the pixels outside roi of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    shuffled_non_roi_image = negativeControlNonROIOnly(croppedCT, croppedROI, \"shuffled\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    shuffled_non_roi_pixels = sitk.GetArrayFromImage(shuffled_non_roi_image)\n\n    assert croppedCT.GetSize() == shuffled_non_roi_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert croppedCT.GetSpacing() == shuffled_non_roi_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == shuffled_non_roi_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_non_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_non_roi_pixels), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_non_roi_pixels.flatten())), \\\n        \"Shuffled pixel values outside ROI are different\"\n    assert shuffled_non_roi_pixels[0,0,0] == -840, \\\n        \"Random seed is not working for shuffled non-ROI image, first voxel has wrong shuffle value. Random seed should be 10.\"\n    assert shuffled_non_roi_pixels[-1,-1,-1] == -490, \\\n        \"Random seed is not working for shuffled non-ROI image, last voxel has wrong shuffle value. Random seed should be 10.\"\n    assert shuffled_non_roi_pixels[7,18,11] == -1, \\\n        \"ROI is getting shuffled when it shouldn't.\"\n\n\ndef test_makeRandomNonRoi(nsclcCropped, randomSeed):\n    \" Test negative control to randomize the pixels outside the ROI of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    randomized_non_roi_image = negativeControlNonROIOnly(croppedCT, croppedROI, \"randomized\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n    # Make ROI pixels = NaN to help check of min and max values in non-ROI pixels\n    original_roi_pixels = sitk.GetArrayFromImage(croppedROI)\n    roiMask = np.where(original_roi_pixels > 0, np.NaN, 1)\n    masked_randomized_non_roi_pixels = randomized_non_roi_pixels * roiMask\n    \n    assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_non_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert np.nanmax(masked_randomized_non_roi_pixels) <= np.max(original_pixels) and np.nanmin(masked_randomized_non_roi_pixels) >= np.min(original_pixels), \\\n        \"Pixel values outside ROI are not within the expected range\"\n    assert randomized_non_roi_pixels[0,0,0] == -124, \\\n        \"Random seed is not working for randomized non-ROI image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[-1,-1,-1] == 123, \\\n        \"Random seed is not working for randomized non-ROI image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[7,18,11] == -1, \\\n        \"ROI is getting randomized when it shouldn't.\"\n\n\ndef test_makeRandomNonRoiFromDistribution(nsclcCropped, randomSeed):\n    \" Test negative control to randomize the pixels outside the ROI of the image\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    randomized_non_roi_image = negativeControlNonROIOnly(croppedCT, croppedROI, \"randomized_sampled\", randomSeed)\n\n    original_pixels = sitk.GetArrayFromImage(croppedCT)\n    randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n    assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_non_roi_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, randomized_non_roi_pixels), \\\n        \"No voxel values have been changed to random.\"\n    assert np.all(np.isin(randomized_non_roi_pixels, original_pixels)), \\\n        \"Retuned object has values outside ROI not sampled from original image\"\n    assert randomized_non_roi_pixels[0,0,0] == -653, \\\n        \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[-1,-1,-1] == -805, \\\n        \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n    assert randomized_non_roi_pixels[7,18,11] == -1, \\\n        \"ROI is getting randomized when it shouldn't.\"\n    \n\n@pytest.mark.parametrize(\n    \"wrongNCType\",\n    [\n        \"shaken\",\n        \"stirred\",\n        \"\"\n    ]\n)\ndef test_negativeControlROIOnly_wrongNCType(nsclcCropped, wrongNCType):\n    \" Test passing wrong negative control type to negativeControlROIOnly\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    with pytest.raises(ValueError):\n        negativeControlROIOnly(croppedCT, croppedROI, wrongNCType, randomSeed=10)\n\n\n@pytest.mark.parametrize(\n    \"wrongNCType\",\n    [\n        \"shaken\",\n        \"stirred\",\n        \"\"\n    ]\n)\ndef test_negativeControlNonROIOnly_wrongNCType(nsclcCropped, wrongNCType):\n    \" Test passing wrong negative control type to negativeControlNonROIOnly\"\n\n    croppedCT, croppedROI, _ = nsclcCropped\n\n    with pytest.raises(ValueError):\n        negativeControlNonROIOnly(croppedCT, croppedROI, wrongNCType, randomSeed=10)\n\n\n# def test_noROILabel_shuffleROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to shuffleROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     shuffled_roi_image = negativeControlROIOnly(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     shuffled_roi_pixels = sitk.GetArrayFromImage(shuffled_roi_image)\n\n#     assert croppedCT.GetSize() == shuffled_roi_image.GetSize(), \\\n#         \"Shuffled image size not same as input image\"\n#     assert croppedCT.GetSpacing() == shuffled_roi_image.GetSpacing(), \\\n#         \"Shuffled image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == shuffled_roi_image.GetOrigin(), \\\n#         \"Shuffled image origin not same as input image\"\n#     assert isinstance(shuffled_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_pixels, shuffled_roi_pixels), \\\n#         \"No voxel values are being shuffled.\"\n#     assert np.array_equal(np.sort(original_pixels.flatten()),\n#                           np.sort(shuffled_roi_pixels.flatten())), \\\n#         \"Shuffled pixel values in ROI are different\"\n#     assert shuffled_roi_pixels[0,0,0] == -740, \\\n#         \"Voxel outside the ROI is being shuffled. Should just be the ROI voxels.\"\n#     assert shuffled_roi_pixels[7,18,11] == -100, \\\n#         \"Random seed is not working for shuffled ROI image, centre pixel has wrong value. Random seed should be 10.\"\n    \n\n# def test_noROILabel_randomROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_roi_image = makeRandomRoi(croppedCT, croppedROI, randomSeed=randomSeed)\n#     original_arr_image = sitk.GetArrayFromImage(croppedCT)\n#     minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n#     randomized_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n#     assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n#         \"Randomized image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n#         \"Randomized image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n#         \"Randomized image origin not same as input image\"\n#     assert isinstance(randomized_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_arr_image, randomized_roi_image), \\\n#         \"No voxel values have been changed to random.\"\n#     assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n#         \"Pixel values are not within the expected range\"\n#     assert randomized_arr_image[0,0,0] == -740, \\\n#         \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n#     assert randomized_arr_image[7,18,11] == -400, \\\n#         \"Random seed is not working for randomized ROI image, centre pixel has wrong value. Random seed should be 10.\"\n\n\n# def test_noROILabel_randomROIFromDist(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomFromRoiDistribution\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_roi_image = makeRandomFromRoiDistribution(croppedCT, croppedROI, randomSeed=randomSeed)\n#     original_arr_image = sitk.GetArrayFromImage(croppedCT)\n\n#     randomized_roi_arr_image = sitk.GetArrayFromImage(randomized_roi_image)\n\n#     assert croppedCT.GetSize() == randomized_roi_image.GetSize(), \\\n#         \"Randomized ROI Sampled image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_roi_image.GetSpacing(), \\\n#         \"Randomized ROI Sampled image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_roi_image.GetOrigin(), \\\n#         \"Randomized ROI Sampled image origin not same as input image\"\n#     assert isinstance(randomized_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_arr_image, randomized_roi_arr_image), \\\n#         \"No voxel values have been changed to random.\"\n#     assert np.all(np.isin(randomized_roi_arr_image.flatten(), original_arr_image.flatten())), \\\n#         \"Retuned object has values not sampled from original image\"\n#     assert randomized_roi_arr_image[0,0,0] == -740, \\\n#         \"Voxel outside the ROI is being randomized. Should just be the ROI voxels.\"\n#     assert randomized_roi_arr_image[7,18,11] == -5, \\\n#         \"Random seed is not working for randomized from distribution ROI image, centre pixel has wrong value. Random seed should be 10.\"\n    \n\n# def test_noROILabel_shuffleNonROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to shuffleNonROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     shuffled_non_roi_image = shuffleNonROI(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     shuffled_non_roi_pixels = sitk.GetArrayFromImage(shuffled_non_roi_image)\n\n#     assert croppedCT.GetSize() == shuffled_non_roi_image.GetSize(), \\\n#         \"Shuffled image size not same as input image\"\n#     assert croppedCT.GetSpacing() == shuffled_non_roi_image.GetSpacing(), \\\n#         \"Shuffled image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == shuffled_non_roi_image.GetOrigin(), \\\n#         \"Shuffled image origin not same as input image\"\n#     assert isinstance(shuffled_non_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_pixels, shuffled_non_roi_pixels), \\\n#         \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n#     assert np.array_equal(np.sort(original_pixels.flatten()),\n#                           np.sort(shuffled_non_roi_pixels.flatten())), \\\n#         \"Shuffled pixel values outside ROI are different\"\n#     assert shuffled_non_roi_pixels[0,0,0] == 54, \\\n#         \"Random seed is not working for shuffled non-ROI image, first voxel has wrong shuffle value. Random seed should be 10.\"\n#     assert shuffled_non_roi_pixels[-1,-1,-1] == -617, \\\n#         \"Random seed is not working for shuffled non-ROI image, last voxel has wrong shuffle value. Random seed should be 10.\"\n#     assert shuffled_non_roi_pixels[7,18,11] == -1, \\\n#         \"ROI is getting shuffled when it shouldn't.\"\n    \n    \n# def test_noROILabel_randomNonROI(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomNonROI\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_non_roi_image = makeRandomNonRoi(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n#     # Make ROI pixels = NaN to help check of min and max values in non-ROI pixels\n#     original_roi_pixels = sitk.GetArrayFromImage(croppedROI)\n#     roiMask = np.where(original_roi_pixels > 0, np.NaN, 1)\n#     masked_randomized_non_roi_pixels = randomized_non_roi_pixels * roiMask\n    \n#     assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n#         \"Randomized image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n#         \"Randomized image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n#         \"Randomized image origin not same as input image\"\n#     assert isinstance(randomized_non_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert np.nanmax(masked_randomized_non_roi_pixels) <= np.max(original_pixels) and np.nanmin(masked_randomized_non_roi_pixels) >= np.min(original_pixels), \\\n#         \"Pixel values outside ROI are not within the expected range\"\n#     assert randomized_non_roi_pixels[0,0,0] == -124, \\\n#         \"Random seed is not working for randomized non-ROI image, first voxel has wrong random value. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[-1,-1,-1] == 123, \\\n#         \"Random seed is not working for randomized non-ROI image, last voxel has wrong random value. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[7,18,11] == -1, \\\n#         \"ROI is getting randomized when it shouldn't.\"\n    \n\n# def test_noROILabel_randomNonROIFromDist(nsclcCropped, randomSeed):\n#     \" Test passing no roiLabel to makeRandomNonRoiFromDistribution\"\n\n#     croppedCT, croppedROI, _ = nsclcCropped\n\n#     randomized_non_roi_image = makeRandomNonRoiFromDistribution(croppedCT, croppedROI, randomSeed=randomSeed)\n\n#     original_pixels = sitk.GetArrayFromImage(croppedCT)\n#     randomized_non_roi_pixels = sitk.GetArrayFromImage(randomized_non_roi_image)\n\n#     assert croppedCT.GetSize() == randomized_non_roi_image.GetSize(), \\\n#         \"Randomized image size not same as input image\"\n#     assert croppedCT.GetSpacing() == randomized_non_roi_image.GetSpacing(), \\\n#         \"Randomized image spacing not same as input image\"\n#     assert croppedCT.GetOrigin() == randomized_non_roi_image.GetOrigin(), \\\n#         \"Randomized image origin not same as input image\"\n#     assert isinstance(randomized_non_roi_image, sitk.Image), \\\n#         \"Returned object is not a sitk.Image\"\n#     assert not np.array_equal(original_pixels, randomized_non_roi_pixels), \\\n#         \"No voxel values have been changed to random.\"\n#     assert np.all(np.isin(randomized_non_roi_pixels, original_pixels)), \\\n#         \"Retuned object has values outside ROI not sampled from original image\"\n#     assert randomized_non_roi_pixels[0,0,0] == -709, \\\n#         \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[-1,-1,-1] == -830, \\\n#         \"Random seed is not working for randomized from distribution non-ROI image. Random seed should be 10.\"\n#     assert randomized_non_roi_pixels[7,18,11] == -1, \\\n#         \"ROI is getting randomized when it shouldn't.\"",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `negativeControlROIOnly` function in this code, and how does it handle different types of negative controls?",
        "answer": "The `negativeControlROIOnly` function applies negative controls to the Region of Interest (ROI) in an image. It handles three types of negative controls: 'shuffled', 'randomized', and 'randomized_sampled'. For 'shuffled', it shuffles the pixel values within the ROI. For 'randomized', it replaces ROI pixels with random values within the image's range. For 'randomized_sampled', it samples new values for ROI pixels from the original image's distribution. The function preserves the image's size, spacing, and origin while only modifying the ROI pixels."
      },
      {
        "question": "How does the code ensure that the random seed is working correctly in the negative control functions?",
        "answer": "The code ensures the random seed is working correctly by checking specific pixel values after applying the negative controls. For example, in the `test_makeShuffleImage` function, it asserts that the first voxel should be -987, the last voxel should be 10, and a specific central ROI voxel should be -118 when the random seed is set to 10. Similar checks are performed in other test functions for different negative control types, verifying that the randomization or shuffling produces consistent results when the same seed is used."
      },
      {
        "question": "What is the purpose of the `applyNegativeControl` function, and how does it differ from the other negative control functions in the code?",
        "answer": "The `applyNegativeControl` function is not explicitly shown in the provided code snippet, but based on the context, it likely serves as a higher-level function that applies various negative controls to an image. It would differ from the other negative control functions (like `negativeControlROIOnly` and `negativeControlNonROIOnly`) by potentially offering a more flexible interface to apply different types of negative controls to either the ROI, non-ROI areas, or the entire image. This function would likely use the more specific negative control functions internally, providing a unified way to apply negative controls based on user-specified parameters."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_makeShuffleImage(nsclcCTImage, randomSeed):\n    \"Test negative control to shuffle the whole image\"\n\n    shuffled_image = makeShuffleImage(nsclcCTImage, randomSeed)\n    original_pixels = sitk.GetArrayFromImage(nsclcCTImage)\n    shuffled_pixels = sitk.GetArrayFromImage(shuffled_image)\n\n    assert nsclcCTImage.GetSize() == shuffled_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == shuffled_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == shuffled_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_pixels), \\\n        \"Pixel values are not shuffled\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_pixels.flatten())), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    # Add assertions for specific pixel values",
        "complete": "def test_makeShuffleImage(nsclcCTImage, randomSeed):\n    \"Test negative control to shuffle the whole image\"\n\n    shuffled_image = makeShuffleImage(nsclcCTImage, randomSeed)\n    original_pixels = sitk.GetArrayFromImage(nsclcCTImage)\n    shuffled_pixels = sitk.GetArrayFromImage(shuffled_image)\n\n    assert nsclcCTImage.GetSize() == shuffled_image.GetSize(), \\\n        \"Shuffled image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == shuffled_image.GetSpacing(), \\\n        \"Shuffled image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == shuffled_image.GetOrigin(), \\\n        \"Shuffled image origin not same as input image\"\n    assert isinstance(shuffled_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert not np.array_equal(original_pixels, shuffled_pixels), \\\n        \"Pixel values are not shuffled\"\n    assert np.array_equal(np.sort(original_pixels.flatten()),\n                          np.sort(shuffled_pixels.flatten())), \\\n        \"Shuffled image has different pixel values than original image. Should just be original pixels rearranged.\"\n    assert shuffled_pixels[0,0,0] == -987, \\\n        \"Random seed is not working for shuffled image, first voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[-1,-1,-1] == 10, \\\n        \"Random seed is not working for shuffled image, last voxel has wrong shuffled value. Random seed should be 10.\"\n    assert shuffled_pixels[238,252,124] == -118, \\\n        \"Random seed is not working for shuffled image, central ROI voxel has wrong shuffled value. Random seed should be 10.\""
      },
      {
        "partial": "def test_makeRandomImage(nsclcCTImage, randomSeed):\n    \"Test negative control to randomize the pixels of the whole image\"\n\n    randomized_image = makeRandomImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_image)\n\n    assert nsclcCTImage.GetSize() == randomized_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    # Add assertions for specific pixel values",
        "complete": "def test_makeRandomImage(nsclcCTImage, randomSeed):\n    \"Test negative control to randomize the pixels of the whole image\"\n\n    randomized_image = makeRandomImage(nsclcCTImage, randomSeed)\n    original_arr_image = sitk.GetArrayFromImage(nsclcCTImage)\n    minVoxelVal, maxVoxelVal = np.min(original_arr_image), np.max(original_arr_image)\n\n    randomized_arr_image = sitk.GetArrayFromImage(randomized_image)\n\n    assert nsclcCTImage.GetSize() == randomized_image.GetSize(), \\\n        \"Randomized image size not same as input image\"\n    assert nsclcCTImage.GetSpacing() == randomized_image.GetSpacing(), \\\n        \"Randomized image spacing not same as input image\"\n    assert nsclcCTImage.GetOrigin() == randomized_image.GetOrigin(), \\\n        \"Randomized image origin not same as input image\"\n    assert isinstance(randomized_image, sitk.Image), \\\n        \"Returned object is not a sitk.Image\"\n    assert randomized_arr_image.max() <= maxVoxelVal and randomized_arr_image.min() >= minVoxelVal, \\\n        \"Pixel values are not within the expected range\"\n    assert randomized_arr_image[0,0,0] == 2156, \\\n        \"Random seed is not working for random image, first voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[-1,-1,-1] == 90, \\\n        \"Random seed is not working for random image, last voxel has wrong random value. Random seed should be 10.\"\n    assert randomized_arr_image[238,252,124] == -840, \\\n        \"Random seed is not working for random image, central ROI voxel has wrong random value. Random seed should be 10.\""
      }
    ],
    "dependencies": {
      "imports": [
        "numpy",
        "pytest",
        "collections"
      ],
      "from_imports": [
        "readii.image_processing.*",
        "radiomics.imageoperations",
        "readii.negative_controls.makeShuffleImage"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/data/__init__.py",
    "language": "py",
    "content": "",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `isValidSudoku` function and what data structure does it use to check for validity?",
        "answer": "The `isValidSudoku` function checks if a given 9x9 Sudoku board is valid. It uses a set data structure to keep track of seen numbers in each row, column, and 3x3 sub-box. The function returns true if the board is valid and false otherwise."
      },
      {
        "question": "How does the function handle checking for duplicates in the 3x3 sub-boxes of the Sudoku board?",
        "answer": "The function uses integer division and modulo operations to map each cell to its corresponding 3x3 sub-box. The formula `(i//3)*3 + j//3` calculates the index of the sub-box (0-8) for each cell. This index is then used with the current number to create a unique key for the set, ensuring that duplicates within each 3x3 sub-box are detected."
      },
      {
        "question": "What is the time and space complexity of the `isValidSudoku` function?",
        "answer": "The time complexity of the `isValidSudoku` function is O(1) because it always iterates over a fixed 9x9 board, resulting in 81 operations. The space complexity is also O(1) because the set used to store seen numbers will have at most 3 * 81 = 243 elements (one for each cell in rows, columns, and sub-boxes), which is a constant regardless of input size."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def fibonacci(n):\n    if n <= 1:\n        return n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        # Complete the loop body\n    return b",
        "complete": "def fibonacci(n):\n    if n <= 1:\n        return n\n    a, b = 0, 1\n    for _ in range(2, n + 1):\n        a, b = b, a + b\n    return b"
      },
      {
        "partial": "def fibonacci(n):\n    # Implement the function using recursion with memoization\n    pass",
        "complete": "def fibonacci(n):\n    memo = {0: 0, 1: 1}\n    def fib(n):\n        if n not in memo:\n            memo[n] = fib(n-1) + fib(n-2)\n        return memo[n]\n    return fib(n)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/rankGeneDrugSensitivity.R",
    "language": "R",
    "content": "#' @importFrom stats complete.cases\n#' @importFrom stats p.adjust\n\n#################################################\n## Rank genes based on drug effect in the Connectivity Map\n##\n## inputs:\n##      - data: gene expression data matrix\n##            - drugpheno: sensititivity values fo thr drug of interest\n##            - type: cell or tissue type for each experiment\n##            - duration: experiment duration in hours\n##      - batch: experiment batches\n##            - single.type: Should the statitsics be computed for each cell/tissue type separately?\n##      - nthread: number of parallel threads (bound to the maximum number of cores available)\n##\n## outputs:\n## list of datafraes with the statistics for each gene, for each type\n##\n## Notes:    duration is not taken into account as only 4 perturbations lasted 12h, the other 6096 lasted 6h\n#################################################\n\nrankGeneDrugSensitivity <- function (data, drugpheno, type, batch,\n                                     single.type=FALSE, standardize = \"SD\",\n                                     nthread=1, verbose=FALSE,\n                                     modeling.method = c(\"anova\", \"pearson\"),\n                                     inference.method = c(\"analytic\", \"resampling\"), req_alpha = 0.05) {\n\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if ((missing(nthread) || nthread < 1 || nthread > availcore) && verbose) {\n      warning(\"nthread undefined, negative or larger than available cores. Resetting to maximum number of cores.\")\n      nthread <- availcore\n    }\n  }\n\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if(modeling.method == \"anova\" && inference.method == \"resampling\") {\n    stop(\"Resampling based inference for anova model is not yet implemented.\")\n  }\n\n  if(is.null(dim(drugpheno))){\n\n    drugpheno <- data.frame(drugpheno)\n\n  } else if(!is(drugpheno, \"data.frame\")) {\n    drugpheno <- as.data.frame(drugpheno)\n\n  }\n\n  if (missing(type) || all(is.na(type))) {\n    type <- array(\"other\", dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (missing(batch) || all(is.na(batch))) {\n    batch <- array(1, dim=nrow(data), dimnames=list(rownames(data)))\n  }\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, duration, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  res <- NULL\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n  res <- NULL\n  ccix <- complete.cases(data, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n\n  if(modeling.method == \"anova\"){\n    if(!any(unlist(lapply(drugpheno,is.factor)))){\n      if(ncol(drugpheno)>1){\n        ##### FIX NAMES!!! This is important\n        nc <- lapply(seq_len(ncol(drugpheno)), function(i){\n\n          est <- paste(\"estimate\", i, sep=\".\")\n          se <-  paste(\"se\", i, sep=\".\")\n          tstat <- paste(\"tstat\", i, sep=\".\")\n\n          nc <- c(est, se, tstat)\n          return(nc)\n\n        })\n        nc  <- c(nc, n=nn, \"fstat\"=NA, \"pvalue\"=NA, \"fdr\")\n      } else {\n        nc  <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\")\n      }\n    } else {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\")\n    }\n  } else if (modeling.method == \"pearson\") {\n    nc <- c(\"estimate\", \"n\", \"df\", \"significant\", \"pvalue\", \"lower\", \"upper\")\n  }\n\n  for (ll in seq_len(length(ltype))) {\n    iix <- !is.na(type) & is.element(type, ltype[[ll]])\n    # ccix <- complete.cases(data[iix, , drop=FALSE], drugpheno[iix,,drop=FALSE], type[iix], batch[iix]) ### HACK???\n\n    ccix <- rowSums(!is.na(data)) > 0 | rowSums(!is.na(drugpheno)) > 0 | is.na(type) | is.na(batch)\n    ccix <- ccix[iix]\n    # ccix <- !vapply(seq_len(NROW(data[iix,,drop=FALSE])), function(x) {\n    #   return(all(is.na(data[iix,,drop=FALSE][x,])) || all(is.na(drugpheno[iix,,drop=FALSE][x,])) || all(is.na(type[iix][x])) || all(is.na(batch[iix][x])))\n    # }, FUN.VALUE=logical(1))\n\n    if (sum(ccix) < 3) {\n      ## not enough experiments\n      rest <- list(matrix(NA, nrow=ncol(data), ncol=length(nc), dimnames=list(colnames(data), nc)))\n      res <- c(res, rest)\n    } else {\n      # splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n      # splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n      mcres <- parallel::mclapply(seq_len(ncol(data)), function(x, data, type, batch, drugpheno, standardize, modeling.method, inference.method, req_alpha) {\n        if(modeling.method == \"anova\"){\n          res <- t(apply(data[ , x, drop=FALSE], 2, geneDrugSensitivity, type=type, batch=batch, drugpheno=drugpheno, verbose=verbose, standardize=standardize))\n        } else if(modeling.method == \"pearson\") {\n          if(!is.character(data)){\n            res <- t(apply(data[ , x, drop=FALSE], 2, geneDrugSensitivityPCorr,\n                                                      type=type,\n                                                      batch=batch,\n                                                      drugpheno=drugpheno,\n                                                      verbose=verbose,\n                                                      test=inference.method,\n                                                      req_alpha = req_alpha))\n          } else {\n            res <- t(apply(data[ , x, drop=FALSE], 2, function(dataIn) {\n              geneDrugSensitivityPBCorr(as.factor(dataIn),\n                                                      type=type,\n                                                      batch=batch,\n                                                      drugpheno=drugpheno,\n                                                      verbose=verbose,\n                                                      test=inference.method,\n                                                      req_alpha = req_alpha)}))\n          }\n\n        }\n\n\n        return(res)\n      }, data=data[iix, , drop=FALSE],\n         type=type[iix], batch=batch[iix],\n         drugpheno=drugpheno[iix,,drop=FALSE],\n         standardize=standardize,\n         modeling.method = modeling.method,\n         inference.method = inference.method,\n         req_alpha = req_alpha, mc.cores = nthread, mc.preschedule = TRUE)\n      rest <- do.call(rbind, mcres)\n      rest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n      # rest <- rest[ , nc, drop=FALSE]\n      res <- c(res, list(rest))\n    }\n  }\n  names(res) <- names(ltype)\n  return(res)\n}\n\n## End\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `rankGeneDrugSensitivity` function, and what are its key inputs?",
        "answer": "The `rankGeneDrugSensitivity` function is designed to rank genes based on drug effect in the Connectivity Map. Its key inputs are:\n1. `data`: gene expression data matrix\n2. `drugpheno`: sensitivity values for the drug of interest\n3. `type`: cell or tissue type for each experiment\n4. `batch`: experiment batches\n5. `single.type`: boolean to determine if statistics should be computed for each cell/tissue type separately\n6. `nthread`: number of parallel threads for computation"
      },
      {
        "question": "How does the function handle different modeling and inference methods, and what happens if an invalid combination is chosen?",
        "answer": "The function supports two modeling methods ('anova' and 'pearson') and two inference methods ('analytic' and 'resampling'). These are selected using `match.arg()` to ensure valid inputs. If 'anova' modeling is combined with 'resampling' inference, the function will stop with an error message: 'Resampling based inference for anova model is not yet implemented.' This prevents users from selecting an unsupported combination of methods."
      },
      {
        "question": "How does the function handle parallel processing, and what precautions are taken to ensure proper use of available cores?",
        "answer": "The function uses parallel processing through the `parallel::mclapply()` function. It takes the following precautions:\n1. It checks the available cores using `parallel::detectCores()`.\n2. If `nthread` is undefined, negative, or larger than available cores, it issues a warning and resets `nthread` to the maximum number of cores.\n3. The `mclapply()` function is called with `mc.cores = nthread` to utilize the specified number of threads.\n4. If `nthread` is set to 1, the function will not attempt parallel processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rankGeneDrugSensitivity <- function (data, drugpheno, type, batch,\n                                     single.type=FALSE, standardize = \"SD\",\n                                     nthread=1, verbose=FALSE,\n                                     modeling.method = c(\"anova\", \"pearson\"),\n                                     inference.method = c(\"analytic\", \"resampling\"), req_alpha = 0.05) {\n  # Input validation\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if(modeling.method == \"anova\" && inference.method == \"resampling\") {\n    stop(\"Resampling based inference for anova model is not yet implemented.\")\n  }\n\n  # Prepare data\n  drugpheno <- if(is.null(dim(drugpheno))) data.frame(drugpheno) else as.data.frame(drugpheno)\n  type <- if(missing(type) || all(is.na(type))) array(\"other\", dim=nrow(data), dimnames=list(rownames(data))) else type\n  batch <- if(missing(batch) || all(is.na(batch))) array(1, dim=nrow(data), dimnames=list(rownames(data))) else batch\n\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  # Set up types\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  # Main processing\n  res <- lapply(ltype, function(current_type) {\n    # TODO: Implement main processing logic here\n  })\n\n  names(res) <- names(ltype)\n  return(res)\n}",
        "complete": "rankGeneDrugSensitivity <- function (data, drugpheno, type, batch,\n                                     single.type=FALSE, standardize = \"SD\",\n                                     nthread=1, verbose=FALSE,\n                                     modeling.method = c(\"anova\", \"pearson\"),\n                                     inference.method = c(\"analytic\", \"resampling\"), req_alpha = 0.05) {\n  # Input validation\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if(modeling.method == \"anova\" && inference.method == \"resampling\") {\n    stop(\"Resampling based inference for anova model is not yet implemented.\")\n  }\n\n  # Prepare data\n  drugpheno <- if(is.null(dim(drugpheno))) data.frame(drugpheno) else as.data.frame(drugpheno)\n  type <- if(missing(type) || all(is.na(type))) array(\"other\", dim=nrow(data), dimnames=list(rownames(data))) else type\n  batch <- if(missing(batch) || all(is.na(batch))) array(1, dim=nrow(data), dimnames=list(rownames(data))) else batch\n\n  if (any(c(nrow(drugpheno), length(type), length(batch)) != nrow(data))) {\n    stop(\"length of drugpheno, type, and batch should be equal to the number of rows of data!\")\n  }\n  rownames(drugpheno) <- names(type) <- names(batch) <- rownames(data)\n\n  # Set up types\n  utype <- sort(unique(as.character(type)))\n  ltype <- list(\"all\"=utype)\n  if (single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n\n  # Main processing\n  res <- lapply(ltype, function(current_type) {\n    iix <- !is.na(type) & is.element(type, current_type)\n    ccix <- complete.cases(data[iix, , drop=FALSE], drugpheno[iix, , drop=FALSE], type[iix], batch[iix])\n    \n    if (sum(ccix) < 3) {\n      return(matrix(NA, nrow=ncol(data), ncol=8, dimnames=list(colnames(data), c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\", \"df\", \"fdr\"))))\n    }\n    \n    mcres <- parallel::mclapply(seq_len(ncol(data)), function(x) {\n      if (modeling.method == \"anova\") {\n        geneDrugSensitivity(data[iix, x], type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix, , drop=FALSE], verbose=verbose, standardize=standardize)\n      } else {\n        geneDrugSensitivityPCorr(data[iix, x], type=type[iix], batch=batch[iix], drugpheno=drugpheno[iix, , drop=FALSE], verbose=verbose, test=inference.method, req_alpha=req_alpha)\n      }\n    }, mc.cores = nthread, mc.preschedule = TRUE)\n    \n    rest <- do.call(rbind, mcres)\n    rest <- cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n    return(rest)\n  })\n\n  names(res) <- names(ltype)\n  return(res)\n}"
      },
      {
        "partial": "geneDrugSensitivity <- function(geneExpr, type, batch, drugpheno, verbose=FALSE, standardize=\"SD\") {\n  # TODO: Implement function body\n}",
        "complete": "geneDrugSensitivity <- function(geneExpr, type, batch, drugpheno, verbose=FALSE, standardize=\"SD\") {\n  if (standardize == \"SD\") {\n    geneExpr <- scale(geneExpr)\n  }\n  \n  df <- data.frame(geneExpr=geneExpr, type=type, batch=batch, drugpheno=drugpheno)\n  model <- lm(geneExpr ~ type + batch + drugpheno, data=df)\n  \n  coef <- summary(model)$coefficients\n  drugpheno_coef <- coef[\"drugpheno\", ]\n  \n  result <- c(\n    estimate = drugpheno_coef[\"Estimate\"],\n    se = drugpheno_coef[\"Std. Error\"],\n    n = nrow(df),\n    tstat = drugpheno_coef[\"t value\"],\n    fstat = summary(model)$fstatistic[1],\n    pvalue = drugpheno_coef[\"Pr(>|t|)\"],\n    df = summary(model)$df[2]\n  )\n  \n  if (verbose) {\n    print(summary(model))\n  }\n  \n  return(result)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/datagraph.py",
    "language": "py",
    "content": "import os\nimport time\nimport pathlib\nfrom typing import List\nfrom functools import reduce\nimport numpy as np\nimport pandas as pd\n\n\nclass DataGraph:\n    '''\n    This class given the crawled dataset in the form of CSV file, deals with forming a graph on the full dataset, taking advantage of connections between different modalities. Based\n    on these connections, an edge table is made. This class also supports querying and for a given query, returns the file locations of the user-specified sub-dataset from the full dataset.\n    The graph is made based on the references made by different DICOMS. Different connections are given different edge type values, to make parsing easier. The edge types are as follows:-\n    1) edge_type:0 RTDOSE(key:ref_rt) -> RTSTRUCT(pair: series/instance)\n    2) edge_type:1 RTDOSE(key:ref_ct) -> CT(pair: series) \n    3) edge_type:2 RTSTRUCT(key:ref_ct) -> CT(pair: series) \n    4) edge_type:3 RTSTRUCT(key:ref_ct) -> PT(pair: series) \n    5) edge_type:4 CT(key:study) -> PT(pair: study) \n    6) edge_type:5 RTDOSE(key: ref_pl) -> RTPLAN(pair: instance)\n    7) edge_type:6 RTPLAN(key: ref_rs) -> RTSTRUCT(pair: series/instance)\n    8) edge_type:7 SEG(key:ref_ct) -> CT(pair: series)\n\n    Once the edge table is formed, one can query on the graph to get the desired results. For uniformity, the supported query is list of modalities to consider\n    For ex:\n    query = [\"CT\",\"RTDOSE\",\"RTSTRUCT\",\"PT], will return interconnected studies containing the listed DICOM modalities. The interconnected studies for example may look like \n    (RTDOSE->RTSTRUCT->CT<-PT<-RTSTRUCT)\n    '''\n    def __init__(self,\n                 path_crawl: str,\n                 edge_path: str = \"./patient_id_full_edges.csv\",\n                 visualize: bool = False) -> None:\n        '''\n        Parameters\n        ----------\n        path_crawl\n            The csv returned by the crawler\n\n        edge_path\n            This path denotes where the graph in the form of edge table is stored or to be stored\n        '''\n        self.df = pd.read_csv(path_crawl, index_col=0)\n        self.edge_path = edge_path\n        self.df_new = None\n        if os.path.exists(self.edge_path):\n            print(\"Edge table is already present. Loading the data...\")\n            self.df_edges = pd.read_csv(self.edge_path)\n        else:\n            print(\"Edge table not present. Forming the edge table based on the crawl data...\")\n            self.form_graph()\n        if visualize:\n            self.visualize_graph()\n    \n    def form_graph(self):\n        '''\n        Forms edge table based on the crawled data\n        '''\n        # enforce string type to all columns to prevent dtype merge errors for empty columns\n        for col in self.df:\n            self.df[col] = self.df[col].astype(str)\n        \n        #Get reference_rs information from RTDOSE-RTPLAN connections    \n        df_filter = pd.merge(self.df, self.df[[\"instance_uid\",\"reference_rs\"]].apply(lambda x: x.astype(str), axis=1), \n                             left_on=\"reference_pl\", \n                             right_on=\"instance_uid\", \n                             how=\"left\")\n        \n        df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_x\"] = df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_y\"].values\n        df_filter.drop(columns=[\"reference_rs_y\", \"instance_uid_y\"], inplace=True)\n        df_filter.rename(columns={\"reference_rs_x\":\"reference_rs\", \"instance_uid_x\":\"instance_uid\"}, inplace=True)\n        \n        # Remove entries with no RTDOSE reference, for extra check, such cases are mostprobably removed in the earlier step\n        df_filter = df_filter.loc[~((df_filter[\"modality\"] == \"RTDOSE\") & (df_filter[\"reference_ct\"].isna()) & (df_filter[\"reference_rs\"].isna()))]\n\n        # Get all study ids\n        # all_study = df_filter.study.unique()\n        start = time.time()\n\n        # Defining Master df to store all the Edge dataframes\n        # self.df_master = []\n\n        # for i in tqdm(range(len(all_study))):\n        #     self._form_edge_study(df_filter, all_study, i)\n\n        # df_edge_patient = form_edge_study(df,all_study,i)\n        \n        self.df_edges = self._form_edges(self.df)  # pd.concat(self.df_master, axis=0, ignore_index=True)\n        end = time.time()\n        print(f\"\\nTotal time taken: {end - start}\")\n\n\n        self.df_edges.loc[self.df_edges.study_x.isna(),\"study_x\"] = self.df_edges.loc[self.df_edges.study_x.isna(), \"study\"]\n        # dropping some columns\n        self.df_edges.drop(columns=[\"study_y\", \"patient_ID_y\", \"series_description_y\", \"study_description_y\", \"study\"],inplace=True)\n        self.df_edges.sort_values(by=\"patient_ID_x\", ascending=True)\n        print(f\"Saving edge table in {self.edge_path}\")\n        self.df_edges.to_csv(self.edge_path, index=False)\n\n    def visualize_graph(self):\n        \"\"\"\n        Generates visualization using Pyviz, a wrapper around visJS. The visualization can be found at datanet.html\n        \"\"\"\n        from pyvis.network import Network  # type: ignore (PyLance)\n        print(\"Generating visualizations...\")\n        data_net = Network(height='100%', width='100%', bgcolor='#222222', font_color='white')\n\n        sources = self.df_edges[\"series_y\"]\n        targets = self.df_edges[\"series_x\"]\n        name_src = self.df_edges[\"modality_y\"] \n        name_tar = self.df_edges[\"modality_x\"]\n        patient_id = self.df_edges[\"patient_ID_x\"]\n        reference_ct = self.df_edges[\"reference_ct_y\"]\n        reference_rs = self.df_edges[\"reference_rs_y\"]\n\n        data_zip = zip(sources,targets,name_src,name_tar,patient_id,reference_ct,reference_rs)\n\n        for i in data_zip:\n            data_net.add_node(i[0],i[2],title=i[2],group=i[4])\n            data_net.add_node(i[1],i[3],title=i[3],group=i[4])\n            data_net.add_edge(i[0],i[1])\n            node = data_net.get_node(i[0])\n            node[\"title\"] = \"<br>Patient_id: {}<br>Series: {}<br>reference_ct: {}<br>reference_rs: {}\".format(i[4],i[0],i[5],i[6])\n            node = data_net.get_node(i[1])\n            node[\"title\"] = \"<br>Patient_id: {}<br>Series: {}<br>reference_ct: {}<br>reference_rs: {}\".format(i[4],i[1],i[5],i[6])\n\n        neigbour_map = data_net.get_adj_list()\n        for node in data_net.nodes:\n            node[\"title\"] += \"<br>Number of connections: {}\".format(len(neigbour_map[node['id']])) \n            node[\"value\"] = len(neigbour_map[node['id']])\n\n        vis_path = pathlib.Path(os.path.dirname(self.edge_path),\"datanet.html\").as_posix()\n        data_net.show(vis_path)\n\n    def _form_edges(self, df):\n        '''\n        For a given study id forms edge table\n        '''\n\n        df_list = []\n\n        # Split into each modality\n        plan = df[df[\"modality\"] == \"RTPLAN\"]\n        dose = df[df[\"modality\"] == \"RTDOSE\"]\n        struct = df[df[\"modality\"] == \"RTSTRUCT\"]\n        seg = df[df[\"modality\"] == \"SEG\"]\n        ct = df[df[\"modality\"] == \"CT\"]\n        mr = df[df[\"modality\"] == \"MR\"]\n        pet = df[df[\"modality\"] == \"PT\"]\n\n        edge_types = np.arange(7)\n        for edge in edge_types:\n            if edge==0:    # FORMS RTDOSE->RTSTRUCT, can be formed on both series and instance uid\n                df_comb1    = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2    = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                # Cases where both series and instance_uid are the same for struct\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n\n            elif edge==1:  # FORMS RTDOSE->CT \n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==2:  # FORMS RTSTRUCT->CT on ref_ct to series\n                df_ct = pd.merge(ct, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n\n            elif edge==3:  # FORMS RTSTRUCT->PET on ref_ct to series\n                df_combined = pd.merge(pet, struct, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==4:  # FORMS PET->CT on study\n                df_combined = pd.merge(ct, pet, left_on=\"study\", right_on=\"study\")\n\n            elif edge==5:  # FORMS RTPLAN->RTDOSE on ref_pl\n                df_combined = pd.merge(plan, dose, left_on=\"instance_uid\", right_on=\"reference_pl\")\n\n            elif edge==7:\n                df_ct = pd.merge(ct, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n\n            else:\n                df_combined = pd.merge(struct, plan, left_on=\"instance_uid\", right_on=\"reference_rs\")\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n\n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        return df_edges\n\n    def _form_edge_study(self, df, all_study, study_id):\n        '''\n        For a given study id forms edge table\n        '''\n        \n        df_study = df.loc[self.df[\"study\"] == all_study[study_id]]\n        df_list = []\n        \n        # Split into each modality\n        plan = df_study.loc[df_study[\"modality\"] == \"RTPLAN\"]\n        dose = df_study.loc[df_study[\"modality\"] == \"RTDOSE\"]\n        struct = df_study.loc[df_study[\"modality\"] == \"RTSTRUCT\"]\n        ct = df_study.loc[df_study[\"modality\"] == \"CT\"]\n        mr = df_study.loc[df_study[\"modality\"] == \"MR\"]\n        pet = df_study.loc[df_study[\"modality\"] == \"PT\"]\n        seg = df_study.loc[df_study[\"modality\"] == \"SEG\"]\n        edge_types = np.arange(8)\n\n        for edge in edge_types:\n            if edge==0:    # FORMS RTDOSE->RTSTRUCT, can be formed on both series and instance uid\n                df_comb1    = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2    = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                # Cases where both series and instance_uid are the same for struct\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n\n            elif edge==1:  # FORMS RTDOSE->CT \n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==2:  # FORMS RTSTRUCT->CT/MR on ref_ct to series\n                df_ct = pd.merge(ct, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n\n            elif edge==3:  # FORMS RTSTRUCT->PET on ref_ct to series\n                df_combined = pd.merge(pet, struct, left_on=\"series\", right_on=\"reference_ct\")\n\n            elif edge==4:  # FORMS PET->CT on study\n                df_combined = pd.merge(ct, pet, left_on=\"study\", right_on=\"study\")\n            \n            elif edge==5:  # FORMS RTPLAN->RTDOSE on ref_pl \n                df_combined = pd.merge(plan, dose, left_on=\"instance\", right_on=\"reference_pl\")\n\n            elif edge==7:  # FORMS SEG->CT/MR on ref_ct to series\n                df_ct_seg = pd.merge(ct, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr_seg = pd.merge(mr, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct_seg, df_mr_seg])\n\n            else:\n                df_combined = pd.merge(struct, plan, left_on=\"instance\", right_on=\"reference_rs\")\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n                \n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        self.df_master.append(df_edges)\n    \n    def parser(self, query_string: str) -> pd.DataFrame:\n        '''\n        For a given query string(Check the documentation), returns the dataframe consisting of two columns namely modality and folder location of the connected nodes\n        Parameters\n        ----------\n        df\n            Dataframe consisting of the crawled data\n        df_edges\n            Processed Dataframe forming a graph, stored in the form of edge table\n        query_string\n            Query string based on which dataset will be formed\n        \n        Query ideas:\n        There are four basic supported modalities are RTDOSE, RTSTRUCT, CT, PT, MRI\n        The options are, the string can be in any order:\n        1) RTDOSE\n        2) RTSTRUCT\n        3) CT\n        4) PT\n        5) PT,RTSTRUCT\n        6) CT,PT\n        7) CT,RTSTRUCT\n        8) CT,RTDOSE\n        9) RTDOSE,RTSTRUCT,CT\n        10) RTDOSE,CT,PT\n        11) RTSTRUCT,CT,PT\n        12) RTDOSE,RTSTRUCT,CT,PT\n        '''\n        # Basic processing of just one modality\n        supp_mods   = [\"RTDOSE\", \"RTSTRUCT\", \"CT\", \"PT\", 'MR', 'SEG']\n        edge_def    = {\"RTSTRUCT,RTDOSE\" : 0, \"CT,RTDOSE\" : 1, \"CT,RTSTRUCT\" : 2, \"PET,RTSTRUCT\" : 3, \"CT,PT\" : 4, 'MR,RTSTRUCT': 2, \"RTPLAN,RTSTRUCT\": 6, \"RTPLAN,RTDOSE\": 5, \"CT,SEG\": 7, \"MR,SEG\": 7}\n        self.mods   = query_string.split(\",\")\n        self.mods_n = len(self.mods)\n\n        # Deals with single node queries\n        if query_string in supp_mods:\n            final_df = self.df.loc[self.df.modality == query_string, [\"study\", \"patient_ID\", \"series\", \"folder\", \"subseries\"]]\n            final_df.rename(columns = {\"series\": f\"series_{query_string}\", \n                                       \"study\": f\"study_{query_string}\", \n                                       \"folder\": f\"folder_{query_string}\",\n                                       \"subseries\": f\"subseries_{query_string}\", }, inplace=True)\n        \n        elif self.mods_n == 2:\n            # Reverse the query string\n            query_string_rev = (\",\").join(self.mods[::-1])\n            if query_string in edge_def.keys():\n                edge_type = edge_def[query_string]\n                valid = query_string\n            elif query_string_rev in edge_def.keys():\n                edge_type = edge_def[query_string_rev]\n                valid = query_string_rev\n            else:\n                raise ValueError(\"Invalid Query. Select valid pairs.\")\n            \n            # For cases such as the CT-RTSTRUCT and CT-RTDOSE, there exists multiple pathways due to which just searching on the edgetype gives wrong results\n            if edge_type in [0, 1, 2]:\n                edge_list = [0, 1, 2]\n                if edge_type==0:\n                    # Search for subgraphs with edges 0 or (1 and 2)\n                    regex_term = '(((?=.*0)|(?=.*5)(?=.*6))|((?=.*1)(?=.*2)))'\n                    mod = [i for i in self.mods if i in ['CT', 'MR']][0]  # making folder_mod CT/MR agnostic <-- still needs testing\n                    final_df = self.graph_query(regex_term, edge_list, f\"folder_{mod}\")\n                elif edge_type==1:\n                    # Search for subgraphs with edges 1 or (0 and 2)\n                    regex_term = '((?=.*1)|(((?=.*0)|(?=.*5)(?=.*6))(?=.*2)))'\n                    final_df = self.graph_query(regex_term, edge_list, \"RTSTRUCT\")\n                elif edge_type==2:\n                    #Search for subgraphs with edges 2 or (1 and 0)\n                    regex_term = '((?=.*2)|(((?=.*0)|(?=.*5)(?=.*6))(?=.*1)))'\n                    final_df = self.graph_query(regex_term, edge_list, \"RTDOSE\") \n            else:\n                final_df = self.df_edges.loc[self.df_edges.edge_type == edge_type, [\"study\",\"patient_ID_x\", \"study_x\", \"study_y\", \"series_x\",\"folder_x\",\"series_y\",\"folder_y\", \"subseries_x\", \"subseries_y\"]]\n                node_dest = valid.split(\",\")[0]\n                node_origin = valid.split(\",\")[1]\n                final_df.rename(columns={\"study\": \"study\", \n                                         \"patient_ID_x\": \"patient_ID\",\n                                         \"series_x\": f\"series_{node_dest}\", \n                                         \"series_y\": f\"series_{node_origin}\", \n                                         \n                                         \"study_x\": f\"study_{node_dest}\", \n                                         \"study_y\": f\"study_{node_origin}\", \n                                         \"folder_x\": f\"folder_{node_dest}\", \n                                         \"folder_y\": f\"folder_{node_origin}\",\n                                         \n                                         \"subseries_x\": f\"subseries_{node_dest}\", \n                                         \"subseries_y\": f\"subseries_{node_origin}\", }, inplace=True)\n\n        elif self.mods_n > 2:\n            # Processing of combinations of modality\n            bads = [\"RTPLAN\"]\n            # CT/MR,RTSTRUCT,RTDOSE\n            if ((\"CT\" in query_string) or ('MR' in query_string)) & (\"RTSTRUCT\" in query_string) & (\"RTDOSE\" in query_string) & (\"PT\" not in query_string):\n                # Fetch the required data. Checks whether each study has edge 2 and (1 or 0)\n                regex_term = '((?=.*1)|(?=.*0)|(?=.*5)(?=.*6))(?=.*2)'\n                edge_list = [0, 1, 2, 5, 6]\n            # CT/MR,RTSTRUCT,RTDOSE,PT\n            elif ((\"CT\" in query_string) or ('MR' in query_string))  & (\"RTSTRUCT\" in query_string) & (\"RTDOSE\" in query_string) & (\"PT\" in query_string):\n                # Fetch the required data. Checks whether each study has edge 2,3,4 and (1 or 0)\n                regex_term = '((?=.*1)|(?=.*0)|(?=.*5)(?=.*6))(?=.*2)(?=.*3)(?=.*4)' # fix\n                edge_list = [0, 1, 2, 3, 4]\n            #CT/MR,RTSTRUCT,PT\n            elif ((\"CT\" in query_string) or ('MR' in query_string))  & (\"RTSTRUCT\" in query_string) & (\"PT\" in query_string) & (\"RTDOSE\" not in query_string):\n                # Fetch the required data. Checks whether each study has edge 2,3,4\n                regex_term = '(?=.*2)(?=.*3)(?=.*4)'\n                edge_list = [2, 3, 4]            \n            #CT/MR,RTDOSE,PT\n            elif ((\"CT\" in query_string) or ('MR' in query_string))  & (\"RTSTRUCT\" not in query_string) & (\"PT\" in query_string) & (\"RTDOSE\" in query_string):\n                # Fetch the required data. Checks whether each study has edge 4 and (1 or (2 and 0)). Remove RTSTRUCT later\n                regex_term = '(?=.*4)((?=.*1)|((?=.*2)((?=.*0)|(?=.*5)(?=.*6))))'\n                edge_list = [0, 1, 2, 4, 5, 6]\n                bads.append(\"RTSTRUCT\")\n            else:\n                raise ValueError(\"Please enter the correct query\")\n            \n            final_df = self.graph_query(regex_term, edge_list, bads)\n        else:\n            raise ValueError(\"Please enter the correct query\")\n        \n        final_df.reset_index(drop=True, inplace=True)\n        final_df[\"index_chng\"] = final_df.index.astype(str) + \"_\" + final_df[\"patient_ID\"].astype(str)\n        final_df.set_index(\"index_chng\", inplace=True)\n        final_df.rename_axis(None, inplace=True)\n        # change relative paths to absolute paths\n        for col in final_df.columns:\n            if col.startswith(\"folder\"):\n                # print(self.edge_path, os.path.dirname(self.edge_path))\n                final_df[col] = final_df[col].apply(lambda x: pathlib.Path(os.path.split(os.path.dirname(self.edge_path))[0], x).as_posix() if isinstance(x, str) else x)  # input folder joined with the rel path\n        return final_df\n    \n    def graph_query(self, \n                    regex_term: str,\n                    edge_list: List[int],\n                    change_df: List[str],\n                    return_components: bool = False,\n                    remove_less_comp: bool = True):\n        '''\n        Based on the regex forms the final dataframe. You can \n        query the edge table based on the regex to get the \n        subgraph in which the queried edges will be present.\n        \n        The components are process further to get the final \n        dataframe of the required modalities.\n        \n        Parameters\n        ----------\n        regex_term\n            To search the string in edge_type column of self.df_new which is aggregate of all the edges in a single study\n\n        edge_list\n            The list of edges that should be returned in the subgraph\n\n        return_components\n            True to return the dictionary of the componets present with the condition present in the regex\n\n        change_df\n            Use only when you want to remove columns containing that string\n\n        remove_less_comp\n            False when you want to keep components with modalities less than the modalitiy listed in the query\n        '''\n        if self.df_new is None:\n            self._form_agg()  # Form aggregates\n        \n        # Fetch the required data. Checks whether each study has edge 4 and (1 or (2 and 0)). Can remove later\n        relevant_study_id = self.df_new.loc[(self.df_new.edge_type.str.contains(regex_term)), \"study_x\"].unique()\n        \n        # Based on the correct study ids, fetches the relevant edges\n        df_processed = self.df_edges.loc[self.df_edges.study_x.isin(relevant_study_id) & (self.df_edges.edge_type.isin(edge_list))]\n        \n        # The components are deleted if it has less number of nodes than the passed modalities, change this so as to alter that condition\n        final_df = self._get_df(df_processed, relevant_study_id, remove_less_comp)\n\n        # Removing columns\n        for bad in change_df:\n            # Find columns with change_df string present\n            col_ids = [cols for cols in list(final_df.columns)[1:] if bad != cols.split(\"_\")[1]]\n            final_df = final_df[[*list(final_df.columns)[:1], *col_ids]]\n        \n        if return_components:\n            return self.final_dict\n        else:\n            return final_df\n\n    def _form_agg(self):\n        '''\n        Form aggregates for easier parsing, gets the edge types for each study and aggregates as a string. This way one can do regex based on what type of subgraph the user wants\n        '''\n        self.df_edges['edge_type_str'] = self.df_edges['edge_type'].astype(str)\n        self.df_new = self.df_edges.groupby(\"study_x\").agg({'edge_type_str':self.list_edges})\n        self.df_new.reset_index(level=0, inplace=True) \n        self.df_new[\"edge_type\"] = self.df_new[\"edge_type_str\"]\n\n    def _get_df(self, \n                df_edges_processed,\n                rel_studyids,\n                remove_less_comp = True):\n    \n        '''\n        Assumption\n        ----------\n        The components are determined based on the unique CTs. \n        Please ensure the data conforms to this case. Based on \n        our preliminary analysis, there are no cases where CT \n        and PT are present but are disconnected.\n\n        Hence this assumption should hold for most of the cases\n        This function returns dataframe consisting of folder \n        location and modality for subgraphs\n\n        Parameters\n        ----------\n        df_edges_processed\n            Dataframe processed containing only the desired edges from the full graph\n\n        rel_studyids\n            Relevant study ids to process(This operation is a bit costly \n            so better not to perform on full graph for maximum performance)\n\n        remove_less_comp\n            True for removing components with less number of edges than the query\n\n        Changelog\n        ---------\n        * June 14th, 2022: Changing from studyID-based to sample-based for loop\n        * Oct 11th, 2022: Reverted to studyID-based loop + improved readability and make CT,RTSTRUCT,RTDOSE mode pass tests\n        '''\n        # Storing all the components across all the studies\n        self.final_dict = []\n        final_df = []\n        # For checking later if all the required modalities are present in a component or not\n        mods_wanted = set(self.mods)\n        \n        # Determine the number of components\n        for i, study in enumerate(rel_studyids): # per study_id\n            df_temp   = df_edges_processed.loc[df_edges_processed.study_x == study]\n            CT_locs   = df_temp.loc[df_temp.modality_x.isin(['CT', 'MR'])]\n            CT_series = CT_locs.series_x.unique()\n            A = []\n            save_folder_comp = []\n            \n            # Initialization. For each component intialize a dictionary with the CTs and their connections\n            for ct in CT_series:\n                df_connections = CT_locs.loc[CT_locs.series_x == ct]\n                \n                if len(df_connections) > 0:\n                    row = df_connections.iloc[0]\n                else:\n                    row = df_connections\n                    \n                series   = row.series_x\n                modality = row.modality_x\n                folder   = row.folder_x\n                \n                # For each component, this loop stores the CT and its connections\n                temp = {\"study\": study,\n                          ct: {\"modality\": modality,\n                               \"folder\": folder}}\n                \n                # For saving the components in a format easier for the main pipeline\n                folder_save = {\"study\": study,\n                               'patient_ID': row.patient_ID_x,\n                                f'series_{modality}': series,\n                                f'folder_{modality}': folder}\n                \n                # This loop stores connection of the CT\n                for k in range(len(df_connections)):\n                    row_y      = df_connections.iloc[k]\n                    series_y   = row_y.series_y\n                    folder_y   = row_y.folder_y\n                    modality_y = row_y.modality_y\n                    \n                    temp[row.series_y] = {\"modality\": modality_y,\n                                          \"folder\": folder_y,\n                                          \"conn_to\": modality}\n\n                    # Checks if there is already existing connection\n                    key, key_series = self._check_save(folder_save, modality_y, modality) #CT/MR                    \n                    folder_save[key_series] = series_y\n                    folder_save[key] = folder_y\n                \n                A.append(temp)\n                save_folder_comp.append(folder_save)\n                       \n            # For rest of the edges left out, the connections are formed by going through the dictionary. For cases such as RTstruct-RTDose and PET-RTstruct\n            rest_locs = df_temp.loc[~df_temp.modality_x.isin(['CT', 'MR']), [\"series_x\", \"modality_x\",\"folder_x\", \"series_y\", \"modality_y\", \"folder_y\"]]            \n            for j in range(len(rest_locs)):\n                edge = rest_locs.iloc[j]\n                for k in range(len(CT_series)):\n                    A[k][edge['series_y']] = {\"modality\": edge['modality_y'], \n                                              \"folder\": edge['folder_y'], \n                                              \"conn_to\": edge['modality_x']}\n                    modality_origin = edge['modality_x']\n\n                    # RTDOSE is connected via either RTstruct or/and CT, but we usually don't care, so naming it commonly\n                    if edge['modality_y'] == \"RTDOSE\":\n                        modality_origin = \"CT\"\n\n                    key, key_series = self._check_save(save_folder_comp[k], edge['modality_y'], modality_origin)\n                    save_folder_comp[k][key_series] = edge['series_y']\n                    save_folder_comp[k][key] = edge['folder_y']\n                    # flag = False\n\n            remove_index = []\n            if remove_less_comp:\n                for j in range(len(CT_series)):\n                    # Check if the number of nodes in a components isn't less than the query nodes, if yes then remove that component\n                    mods_present = set([items.split(\"_\")[1] for items in save_folder_comp[j].keys() if items.split(\"_\")[0] == \"folder\"])\n                    # Checking if all the read modalities are present in a component\n                    if mods_wanted.issubset(mods_present):\n                        remove_index.append(j)\n                save_folder_comp = [save_folder_comp[idx] for idx in remove_index]        \n                A = [A[idx] for idx in remove_index]    \n\n            self.final_dict.extend(A)\n            final_df.extend(save_folder_comp)\n        \n        final_df = pd.DataFrame(final_df)\n        return final_df\n    \n    @staticmethod\n    def _check_save(save_dict,node,dest):\n        key = f\"folder_{node}_{dest}\"\n        key_series = f\"series_{node}_{dest}\"\n        i = 1\n        while key in save_dict.keys():\n            key = f\"folder_{node}_{dest}_{i}\"\n            key_series = f\"series_{node}_{dest}_{i}\"\n            i +=1\n        return key,key_series\n    \n    @staticmethod\n    def list_edges(series):\n        return reduce(lambda x, y:str(x) + str(y), series)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the DataGraph class and how does it handle different types of DICOM connections?",
        "answer": "The DataGraph class is designed to create a graph structure from a crawled dataset of DICOM files. It handles different types of DICOM connections by assigning edge types to various relationships between modalities. For example, edge_type 0 represents a connection from RTDOSE to RTSTRUCT, edge_type 1 represents RTDOSE to CT, and so on. This graph structure allows for efficient querying and retrieval of interconnected studies containing specified DICOM modalities."
      },
      {
        "question": "How does the 'parser' method in the DataGraph class work, and what types of queries does it support?",
        "answer": "The 'parser' method in the DataGraph class processes user queries to return a dataframe of connected nodes. It supports queries for single modalities (e.g., 'CT') and combinations of modalities (e.g., 'CT,RTSTRUCT,RTDOSE'). The method uses regex patterns to search for specific edge combinations in the graph, allowing for complex queries like finding studies with CT, RTSTRUCT, and RTDOSE that are all interconnected. It can handle various combinations of the supported modalities: RTDOSE, RTSTRUCT, CT, PT, and MR."
      },
      {
        "question": "What is the significance of the '_form_edges' method in the DataGraph class, and how does it contribute to the overall functionality?",
        "answer": "The '_form_edges' method is crucial for creating the edge table that represents the graph structure. It processes the input dataframe to identify and create connections between different DICOM modalities. The method handles various edge types, such as RTDOSE to RTSTRUCT, RTDOSE to CT, RTSTRUCT to CT/MR, etc. By creating this comprehensive edge table, the method enables efficient querying and traversal of the DICOM data graph, which is essential for the class's ability to find interconnected studies based on user queries."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class DataGraph:\n    def __init__(self,\n                 path_crawl: str,\n                 edge_path: str = \"./patient_id_full_edges.csv\",\n                 visualize: bool = False) -> None:\n        self.df = pd.read_csv(path_crawl, index_col=0)\n        self.edge_path = edge_path\n        self.df_new = None\n        if os.path.exists(self.edge_path):\n            print(\"Edge table is already present. Loading the data...\")\n            self.df_edges = pd.read_csv(self.edge_path)\n        else:\n            print(\"Edge table not present. Forming the edge table based on the crawl data...\")\n            self.form_graph()\n        if visualize:\n            self.visualize_graph()",
        "complete": "class DataGraph:\n    def __init__(self,\n                 path_crawl: str,\n                 edge_path: str = \"./patient_id_full_edges.csv\",\n                 visualize: bool = False) -> None:\n        self.df = pd.read_csv(path_crawl, index_col=0)\n        self.edge_path = edge_path\n        self.df_new = None\n        if os.path.exists(self.edge_path):\n            print(\"Edge table is already present. Loading the data...\")\n            self.df_edges = pd.read_csv(self.edge_path)\n        else:\n            print(\"Edge table not present. Forming the edge table based on the crawl data...\")\n            self.form_graph()\n        if visualize:\n            self.visualize_graph()\n\n    def form_graph(self):\n        for col in self.df:\n            self.df[col] = self.df[col].astype(str)\n        \n        df_filter = pd.merge(self.df, self.df[[\"instance_uid\",\"reference_rs\"]].apply(lambda x: x.astype(str), axis=1), \n                             left_on=\"reference_pl\", \n                             right_on=\"instance_uid\", \n                             how=\"left\")\n        \n        df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_x\"] = df_filter.loc[(df_filter.reference_rs_x.isna()) & (~df_filter.reference_rs_y.isna()),\"reference_rs_y\"].values\n        df_filter.drop(columns=[\"reference_rs_y\", \"instance_uid_y\"], inplace=True)\n        df_filter.rename(columns={\"reference_rs_x\":\"reference_rs\", \"instance_uid_x\":\"instance_uid\"}, inplace=True)\n        \n        df_filter = df_filter.loc[~((df_filter[\"modality\"] == \"RTDOSE\") & (df_filter[\"reference_ct\"].isna()) & (df_filter[\"reference_rs\"].isna()))]\n\n        start = time.time()\n        \n        self.df_edges = self._form_edges(self.df)\n        end = time.time()\n        print(f\"\\nTotal time taken: {end - start}\")\n\n        self.df_edges.loc[self.df_edges.study_x.isna(),\"study_x\"] = self.df_edges.loc[self.df_edges.study_x.isna(), \"study\"]\n        self.df_edges.drop(columns=[\"study_y\", \"patient_ID_y\", \"series_description_y\", \"study_description_y\", \"study\"],inplace=True)\n        self.df_edges.sort_values(by=\"patient_ID_x\", ascending=True)\n        print(f\"Saving edge table in {self.edge_path}\")\n        self.df_edges.to_csv(self.edge_path, index=False)"
      },
      {
        "partial": "def _form_edges(self, df):\n        df_list = []\n\n        plan = df[df[\"modality\"] == \"RTPLAN\"]\n        dose = df[df[\"modality\"] == \"RTDOSE\"]\n        struct = df[df[\"modality\"] == \"RTSTRUCT\"]\n        seg = df[df[\"modality\"] == \"SEG\"]\n        ct = df[df[\"modality\"] == \"CT\"]\n        mr = df[df[\"modality\"] == \"MR\"]\n        pet = df[df[\"modality\"] == \"PT\"]\n\n        edge_types = np.arange(7)\n        for edge in edge_types:\n            if edge == 0:\n                df_comb1 = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2 = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n            elif edge == 1:\n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n            # ... (other edge cases)\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n\n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        return df_edges",
        "complete": "def _form_edges(self, df):\n        df_list = []\n\n        plan = df[df[\"modality\"] == \"RTPLAN\"]\n        dose = df[df[\"modality\"] == \"RTDOSE\"]\n        struct = df[df[\"modality\"] == \"RTSTRUCT\"]\n        seg = df[df[\"modality\"] == \"SEG\"]\n        ct = df[df[\"modality\"] == \"CT\"]\n        mr = df[df[\"modality\"] == \"MR\"]\n        pet = df[df[\"modality\"] == \"PT\"]\n\n        edge_types = np.arange(7)\n        for edge in edge_types:\n            if edge == 0:\n                df_comb1 = pd.merge(struct, dose, left_on=\"instance_uid\", right_on=\"reference_rs\")\n                df_comb2 = pd.merge(struct, dose, left_on=\"series\", right_on=\"reference_rs\")\n                df_combined = pd.concat([df_comb1, df_comb2])\n                df_combined = df_combined.drop_duplicates(subset=[\"instance_uid_x\"])\n            elif edge == 1:\n                df_combined = pd.merge(ct, dose, left_on=\"series\", right_on=\"reference_ct\")\n            elif edge == 2:\n                df_ct = pd.merge(ct, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, struct, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n            elif edge == 3:\n                df_combined = pd.merge(pet, struct, left_on=\"series\", right_on=\"reference_ct\")\n            elif edge == 4:\n                df_combined = pd.merge(ct, pet, left_on=\"study\", right_on=\"study\")\n            elif edge == 5:\n                df_combined = pd.merge(plan, dose, left_on=\"instance_uid\", right_on=\"reference_pl\")\n            elif edge == 7:\n                df_ct = pd.merge(ct, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_mr = pd.merge(mr, seg, left_on=\"series\", right_on=\"reference_ct\")\n                df_combined = pd.concat([df_ct, df_mr])\n            else:\n                df_combined = pd.merge(struct, plan, left_on=\"instance_uid\", right_on=\"reference_rs\")\n\n            df_combined[\"edge_type\"] = edge\n            df_list.append(df_combined)\n\n        df_edges = pd.concat(df_list, axis=0, ignore_index=True)\n        return df_edges"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "time",
        "pathlib",
        "numpy",
        "pandas"
      ],
      "from_imports": [
        "typing.List",
        "functools.reduce",
        "pyvis.network.Network"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugPerturbation.R",
    "language": "R",
    "content": "#' @importFrom stats median\n#' @importFrom stats complete.cases\n#' @importFrom stats lm\n#' @importFrom stats anova\n#' @importFrom stats pf\n#'\n\n## function computing gene-drug associations from perturbation data (CMAP)\ngeneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n## input:\n##  x: numeric vector of gene expression values\n##  concentration: numeric vector with drug concentrations/doses\n##  type: vector of factors specifying the cell lines or type types\n##  batch: vector of factors specifying the batch\n##  duration: numeric vector of measurement times (in hours)\n##  model: Should the full linear model be returned? Default set to FALSE\n##\n## output:\n##  vector reporting the effect size (estimateof the coefficient of drug concentration), standard error (se), sample size (n), t statistic, and f statistics and its corresponding p-value\n\n    ## NOTE:: The use of T/F warning from BiocCheck is a false positive on the string 'Pr(>F)'\n\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        tt <- rep(NA, length(nc))\n        names(tt) <- nc\n        return(tt)\n    }\n    ff0 <- sprintf(\"x ~ 1\")\n    ff <- sprintf(\"%s + concentration\", ff0)\n\n\n    if (length(sort(unique(type))) > 1) {\n        ff0 <- sprintf(\"%s + type\", ff0)\n        ff <- sprintf(\"%s + type\", ff)\n    }\n    if (length(sort(unique(batch))) > 1) {\n        ff0 <- sprintf(\"%s + batch\", ff0)\n        ff <- sprintf(\"%s + batch\", ff)\n    }\n\n### add experiment duration if the vector consists of more than one different value\n\n  if (length(sort(unique(duration))) > 2) {\n      ff0 <- sprintf(\"%s + duration\", ff0)\n      ff <- sprintf(\"%s + duration\", ff)\n  }\n\n    dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n        names(dd)[1]<-\"x\"\n        mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n        mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n\n        mmc <- stats::anova(mm0, mm)\n        mm <- summary(mm)\n## extract statistics\n        tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n    }\n    names(tt) <- nc\n## add tissue type/cell line statistics\n    if(length(sort(unique(type))) > 1) {\n        rr <- summary(mm0)\n        ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n        names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n    } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n    tt <- c(tt, ttype)\n## add model\n    if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n    return(tt)\n}\n\n\n## End\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `geneDrugPerturbation` function and what are its main inputs?",
        "answer": "The `geneDrugPerturbation` function computes gene-drug associations from perturbation data (CMAP). Its main inputs are: 'x' (numeric vector of gene expression values), 'concentration' (numeric vector with drug concentrations/doses), 'type' (vector of factors specifying cell lines or types), 'batch' (vector of factors specifying the batch), 'duration' (numeric vector of measurement times in hours), and 'model' (boolean indicating whether to return the full linear model, default is FALSE)."
      },
      {
        "question": "How does the function handle cases where there are insufficient drug concentrations or complete cases?",
        "answer": "The function checks for insufficient data in two ways: 1) If there are less than 2 unique drug concentrations, it issues a warning 'No drug concentrations tested' and returns a vector of NA values. 2) If there are less than 3 complete cases in the data frame, it returns a vector with NA values for most statistics, except for 'n' which reports the number of complete cases."
      },
      {
        "question": "What statistical methods does the function use to compute gene-drug associations, and what are the main output statistics?",
        "answer": "The function uses linear regression models to compute gene-drug associations. It creates two models: one with and one without the drug concentration as a predictor. It then uses ANOVA to compare these models. The main output statistics are: 'estimate' (effect size of drug concentration), 'se' (standard error), 'n' (sample size), 'tstat' (t-statistic), 'fstat' (F-statistic), and 'pvalue' (p-value from ANOVA). If multiple cell types are present, it also includes 'type.fstat' and 'type.pvalue' for the cell type effect."
      }
    ],
    "completion_tasks": [
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        tt <- rep(NA, length(nc))\n        names(tt) <- nc\n        return(tt)\n    }\n    ff0 <- sprintf(\"x ~ 1\")\n    ff <- sprintf(\"%s + concentration\", ff0)\n\n    # Add code here to handle type, batch, and duration\n\n    dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n        # Add code here to create and analyze models\n    }\n    # Add code here to handle type statistics and model return\n    return(tt)\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        tt <- rep(NA, length(nc))\n        names(tt) <- nc\n        return(tt)\n    }\n    ff0 <- sprintf(\"x ~ 1\")\n    ff <- sprintf(\"%s + concentration\", ff0)\n\n    if (length(sort(unique(type))) > 1) {\n        ff0 <- sprintf(\"%s + type\", ff0)\n        ff <- sprintf(\"%s + type\", ff)\n    }\n    if (length(sort(unique(batch))) > 1) {\n        ff0 <- sprintf(\"%s + batch\", ff0)\n        ff <- sprintf(\"%s + batch\", ff)\n    }\n    if (length(sort(unique(duration))) > 2) {\n        ff0 <- sprintf(\"%s + duration\", ff0)\n        ff <- sprintf(\"%s + duration\", ff)\n    }\n\n    dd <- data.frame(\"x\"=x, \"concentration\"=concentration, \"duration\"=duration, \"type\"=type, \"batch\"=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        tt <- c(\"estimate\"=NA, \"se\"=NA, \"n\"=nn, \"tsat\"=NA, \"fstat\"=NA, \"pvalue\"=NA)\n    } else {\n        names(dd)[1] <- \"x\"\n        mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n        mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n        mmc <- stats::anova(mm0, mm)\n        mm <- summary(mm)\n        tt <- c(\"estimate\"=mm$coefficients[\"concentration\", \"Estimate\"], \"se\"=mm$coefficients[\"concentration\", \"Std. Error\"], \"n\"=nn, \"tsat\"=mm$coefficients[\"concentration\", \"t value\"], \"fstat\"=mmc$F[2], \"pvalue\"=mmc$'Pr(>F)'[2])\n    }\n    names(tt) <- nc\n    if(length(sort(unique(type))) > 1) {\n        rr <- summary(mm0)\n        ttype <- c(\"type.fstat\"=rr$fstatistic[\"value\"], \"type.pvalue\"=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n        names(ttype) <- c(\"type.fstat\", \"type.pvalue\")\n    } else { ttype <- c(\"type.fstat\"=NA, \"type.pvalue\"=NA) }\n    tt <- c(tt, ttype)\n    if (model) { tt <- list(\"stats\"=tt, \"model\"=mm)}\n    return(tt)\n}"
      },
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        return(setNames(rep(NA, length(nc)), nc))\n    }\n    ff0 <- \"x ~ 1\"\n    ff <- paste(ff0, \"+ concentration\")\n\n    # Add code here to handle type, batch, and duration\n\n    dd <- data.frame(x=x, concentration=concentration, duration=duration, type=type, batch=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        return(c(estimate=NA, se=NA, n=nn, tsat=NA, fstat=NA, pvalue=NA))\n    }\n    # Add code here to create and analyze models\n    # Add code here to handle type statistics and model return\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration, model=FALSE) {\n    nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n    if (length(sort(unique(concentration))) < 2) {\n        warning(\"No drug concentrations tested\")\n        return(setNames(rep(NA, length(nc)), nc))\n    }\n    ff0 <- \"x ~ 1\"\n    ff <- paste(ff0, \"+ concentration\")\n\n    if (length(sort(unique(type))) > 1) {\n        ff0 <- paste(ff0, \"+ type\")\n        ff <- paste(ff, \"+ type\")\n    }\n    if (length(sort(unique(batch))) > 1) {\n        ff0 <- paste(ff0, \"+ batch\")\n        ff <- paste(ff, \"+ batch\")\n    }\n    if (length(sort(unique(duration))) > 2) {\n        ff0 <- paste(ff0, \"+ duration\")\n        ff <- paste(ff, \"+ duration\")\n    }\n\n    dd <- data.frame(x=x, concentration=concentration, duration=duration, type=type, batch=batch)\n    nn <- sum(complete.cases(dd))\n    if(nn < 3) {\n        return(c(estimate=NA, se=NA, n=nn, tsat=NA, fstat=NA, pvalue=NA))\n    }\n    mm0 <- lm(formula=ff0, data=dd, model=FALSE, x=FALSE, y=FALSE, qr=TRUE)\n    mm <- lm(formula=ff, data=dd, model=model, x=FALSE, y=FALSE, qr=TRUE)\n    mmc <- stats::anova(mm0, mm)\n    mm_summary <- summary(mm)\n    tt <- c(estimate=mm_summary$coefficients[\"concentration\", \"Estimate\"],\n            se=mm_summary$coefficients[\"concentration\", \"Std. Error\"],\n            n=nn,\n            tsat=mm_summary$coefficients[\"concentration\", \"t value\"],\n            fstat=mmc$F[2],\n            pvalue=mmc$'Pr(>F)'[2])\n    if(length(sort(unique(type))) > 1) {\n        rr <- summary(mm0)\n        ttype <- c(type.fstat=rr$fstatistic[\"value\"],\n                   type.pvalue=pf(q=rr$fstatistic[\"value\"], df1=rr$fstatistic[\"numdf\"], df2=rr$fstatistic[\"dendf\"], lower.tail=FALSE))\n    } else {\n        ttype <- c(type.fstat=NA, type.pvalue=NA)\n    }\n    tt <- c(tt, ttype)\n    if (model) tt <- list(stats=tt, model=mm)\n    return(tt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_modalities.py",
    "language": "py",
    "content": "'''\nThis code is for testing functioning of different modalities \n'''\n\n\nimport os\nimport pathlib\n\nimport SimpleITK as sitk\nimport pytest\nimport pydicom\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops import StructureSetToSegmentation\n\n@pytest.fixture\ndef modalities_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    print(os.listdir(curr_path))\n    \n    # qc_path = pathlib.Path(os.path.join(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-082\"))\n    # path = {}\n    # path[\"CT\"] = os.path.join(qc_path, \"08-27-1885-CA ORL FDG TEP-06980/3.000000-Merged-05195\")\n    # path[\"RTSTRUCT\"] = os.path.join(qc_path, \"08-27-1885-06980/Pinnacle POI-67882\")\n    # path[\"RTDOSE\"] = os.path.join(qc_path, \"08-27-1885-06980/89632\")\n    # path[\"PT\"] = os.path.join(qc_path, \"08-27-1885-TEP cancerologique TEP-06980/552650.000000-LOR-RAMLA-72508\")\n    qc_path = pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-052\").as_posix()\n    assert os.path.exists(qc_path), \"Dataset not found\"\n    \n    path = {}\n    path[\"CT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/3.000000-Merged-06362\").as_posix()\n    path[\"RTSTRUCT\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/Pinnacle POI-41418\").as_posix()\n    path[\"RTDOSE\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/11376\").as_posix()\n    path[\"PT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/532790.000000-LOR-RAMLA-44600\").as_posix()\n    return path\n\n@pytest.mark.parametrize(\"modalities\", [\"CT\", \"RTSTRUCT\", \"RTDOSE\", \"PT\"])\ndef test_modalities(modalities, modalities_path):\n    path = modalities_path\n    img = read_dicom_auto(path[\"CT\"]).image\n    if modalities != \"RTSTRUCT\":\n        #Checks for dimensions\n        dcm = pydicom.dcmread(pathlib.Path(path[modalities],os.listdir(path[modalities])[0]).as_posix()).pixel_array\n        instances = len(os.listdir(path[modalities]))\n        dicom = read_dicom_auto(path[modalities])\n        if modalities == 'CT':\n            dicom = dicom.image\n        if instances>1: #For comparing CT and PT modalities\n            assert dcm.shape == (dicom.GetHeight(),dicom.GetWidth())\n            assert instances == dicom.GetDepth()\n        else: #For comparing RTDOSE modalties\n            assert dcm.shape == (dicom.GetDepth(),dicom.GetHeight(),dicom.GetWidth())\n        if modalities == \"PT\":\n            dicom = dicom.resample_pet(img)\n            assert dicom.GetSize()==img.GetSize()\n        if modalities == \"RTDOSE\":\n            dicom = dicom.resample_dose(img)\n            assert dicom.GetSize()==img.GetSize()\n    else:\n        struc = read_dicom_auto(path[modalities])\n        make_binary_mask = StructureSetToSegmentation(roi_names=['GTV.?', 'LARYNX'], continuous=False)\n        mask = make_binary_mask(struc, img, {\"background\": 0}, False)\n        A = sitk.GetArrayFromImage(mask)\n        assert len(A.shape) == 4\n        assert A.shape[0:3] == (img.GetDepth(),img.GetHeight(),img.GetWidth())\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `modalities_path` fixture in this code, and how does it handle different imaging modalities?",
        "answer": "The `modalities_path` fixture is used to set up the file paths for different imaging modalities (CT, RTSTRUCT, RTDOSE, and PT) in a medical imaging dataset. It creates a dictionary with keys for each modality and values containing the corresponding file paths. This fixture ensures that the correct data paths are available for testing different modalities in the subsequent test function."
      },
      {
        "question": "How does the code handle the comparison of dimensions for different modalities, and what is the significance of the `instances` variable?",
        "answer": "The code compares dimensions differently based on the modality:\n1. For CT and PT: It checks if the shape of a single DICOM slice matches the height and width of the loaded image, and if the number of instances (files) matches the depth.\n2. For RTDOSE: It checks if the shape of the DICOM data matches the depth, height, and width of the loaded image.\nThe `instances` variable represents the number of DICOM files in the modality's directory. It's used to determine if the modality has multiple slices (CT, PT) or a single file (RTDOSE), which affects how the dimension comparison is performed."
      },
      {
        "question": "What special processing is done for the RTSTRUCT modality, and what assertions are made about the resulting mask?",
        "answer": "For the RTSTRUCT modality, the code:\n1. Reads the RTSTRUCT DICOM file.\n2. Creates a binary mask using the `StructureSetToSegmentation` class, focusing on ROIs with names matching 'GTV.?' and 'LARYNX'.\n3. Generates a mask aligned with the CT image.\n4. Converts the mask to a numpy array.\n5. Asserts that the resulting array has 4 dimensions.\n6. Checks if the first three dimensions of the mask array match the depth, height, and width of the CT image.\nThis process ensures that the RTSTRUCT data is correctly converted into a binary segmentation mask that aligns with the corresponding CT image."
      }
    ],
    "completion_tasks": [
      {
        "partial": "import os\nimport pathlib\nimport SimpleITK as sitk\nimport pytest\nimport pydicom\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops import StructureSetToSegmentation\n\n@pytest.fixture\ndef modalities_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    qc_path = pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-052\").as_posix()\n    assert os.path.exists(qc_path), \"Dataset not found\"\n    \n    path = {}\n    # Complete the path dictionary\n\n    return path\n",
        "complete": "import os\nimport pathlib\nimport SimpleITK as sitk\nimport pytest\nimport pydicom\n\nfrom imgtools.io import read_dicom_auto\nfrom imgtools.ops import StructureSetToSegmentation\n\n@pytest.fixture\ndef modalities_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    qc_path = pathlib.Path(curr_path, \"data\", \"Head-Neck-PET-CT\", \"HN-CHUS-052\").as_posix()\n    assert os.path.exists(qc_path), \"Dataset not found\"\n    \n    path = {}\n    path[\"CT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/3.000000-Merged-06362\").as_posix()\n    path[\"RTSTRUCT\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/Pinnacle POI-41418\").as_posix()\n    path[\"RTDOSE\"] = pathlib.Path(qc_path, \"08-27-1885-OrophCB.0OrophCBTRTID derived StudyInstanceUID.-94629/11376\").as_posix()\n    path[\"PT\"] = pathlib.Path(qc_path, \"08-27-1885-CA ORL FDG TEP POS TX-94629/532790.000000-LOR-RAMLA-44600\").as_posix()\n    return path\n"
      },
      {
        "partial": "@pytest.mark.parametrize(\"modalities\", [\"CT\", \"RTSTRUCT\", \"RTDOSE\", \"PT\"])\ndef test_modalities(modalities, modalities_path):\n    path = modalities_path\n    img = read_dicom_auto(path[\"CT\"]).image\n    if modalities != \"RTSTRUCT\":\n        dcm = pydicom.dcmread(pathlib.Path(path[modalities],os.listdir(path[modalities])[0]).as_posix()).pixel_array\n        instances = len(os.listdir(path[modalities]))\n        dicom = read_dicom_auto(path[modalities])\n        if modalities == 'CT':\n            dicom = dicom.image\n        # Complete the rest of the function\n",
        "complete": "@pytest.mark.parametrize(\"modalities\", [\"CT\", \"RTSTRUCT\", \"RTDOSE\", \"PT\"])\ndef test_modalities(modalities, modalities_path):\n    path = modalities_path\n    img = read_dicom_auto(path[\"CT\"]).image\n    if modalities != \"RTSTRUCT\":\n        dcm = pydicom.dcmread(pathlib.Path(path[modalities],os.listdir(path[modalities])[0]).as_posix()).pixel_array\n        instances = len(os.listdir(path[modalities]))\n        dicom = read_dicom_auto(path[modalities])\n        if modalities == 'CT':\n            dicom = dicom.image\n        if instances > 1:\n            assert dcm.shape == (dicom.GetHeight(), dicom.GetWidth())\n            assert instances == dicom.GetDepth()\n        else:\n            assert dcm.shape == (dicom.GetDepth(), dicom.GetHeight(), dicom.GetWidth())\n        if modalities == \"PT\":\n            dicom = dicom.resample_pet(img)\n            assert dicom.GetSize() == img.GetSize()\n        if modalities == \"RTDOSE\":\n            dicom = dicom.resample_dose(img)\n            assert dicom.GetSize() == img.GetSize()\n    else:\n        struc = read_dicom_auto(path[modalities])\n        make_binary_mask = StructureSetToSegmentation(roi_names=['GTV.?', 'LARYNX'], continuous=False)\n        mask = make_binary_mask(struc, img, {\"background\": 0}, False)\n        A = sitk.GetArrayFromImage(mask)\n        assert len(A.shape) == 4\n        assert A.shape[0:3] == (img.GetDepth(), img.GetHeight(), img.GetWidth())\n"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "SimpleITK",
        "pytest",
        "pydicom"
      ],
      "from_imports": [
        "imgtools.io.read_dicom_auto",
        "imgtools.ops.StructureSetToSegmentation"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_Hill.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    expect_equal(.Hill(-Inf, c(1, 0.2, 1)), 1)\n    expect_equal(.Hill(Inf, c(1, 0.2, 1)), 0.2)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .Hill function in this code snippet, and how is it being tested?",
        "answer": "The .Hill function appears to be a mathematical function used in pharmacology or biochemistry. It's being tested for its behavior under various input conditions using the testthat framework in R. The tests check the function's output for different combinations of input parameters, including edge cases like 0, infinity, and negative infinity."
      },
      {
        "question": "Explain the significance of the test case: expect_equal(.Hill(0, c(0, 0, 0)), 1/2)",
        "answer": "This test case is checking the behavior of the .Hill function when all its parameters are zero. The expected output of 1/2 suggests that the function has a built-in default or limiting behavior when faced with all-zero inputs. This could represent a mid-point or baseline response in the context of the Hill equation, which is often used to model dose-response relationships."
      },
      {
        "question": "How does the code handle extreme values, and what does this imply about the .Hill function's behavior?",
        "answer": "The code tests the .Hill function with extreme values like Inf (infinity) and -Inf (negative infinity). For example, .Hill(-Inf, c(1, 0.2, 1)) is expected to return 1, while .Hill(Inf, c(1, 0.2, 1)) is expected to return 0.2. This implies that the .Hill function has well-defined asymptotic behavior, approaching different limits as the input goes to positive or negative infinity. This is consistent with the typical behavior of the Hill equation in describing saturation phenomena."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    # Add two more test cases\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    expect_equal(.Hill(-Inf, c(1, 0.2, 1)), 1)\n    expect_equal(.Hill(Inf, c(1, 0.2, 1)), 0.2)\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    # Add five test cases for the .Hill function\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Testing .Hill function\")\n\ntest_that(\"Returns right maths\",{\n    expect_equal(.Hill(0, c(1, 0, -Inf)), 0)\n    expect_equal(.Hill(0, c(0, 0, 0)), 1/2)\n    expect_equal(.Hill(0, c(1, 0, Inf)), 1)\n    expect_equal(.Hill(-Inf, c(1, 0.2, 1)), 1)\n    expect_equal(.Hill(Inf, c(1, 0.2, 1)), 0.2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/__init__.py",
    "language": "py",
    "content": "from .segmentation import *\nfrom .structureset import *\nfrom .pet import *\nfrom .dose import *\nfrom .datagraph import *\nfrom .sparsemask import *\nfrom .scan import *\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of using asterisks (*) in the import statements of this Python code snippet?",
        "answer": "The asterisks (*) in the import statements are used for wildcard imports. This means that all public names (functions, classes, variables) defined in the specified modules are imported directly into the current namespace. For example, 'from .segmentation import *' imports all public names from the 'segmentation' module located in the same package."
      },
      {
        "question": "What potential issue might arise from using wildcard imports as shown in this code snippet?",
        "answer": "Using wildcard imports can lead to namespace pollution, where imported names may unintentionally override existing names in the current module. This can make the code harder to understand and maintain, as it's not immediately clear where specific functions or classes are coming from. It's generally recommended to use explicit imports or import specific names to avoid these issues."
      },
      {
        "question": "What does the dot (.) before the module names in the import statements indicate?",
        "answer": "The dot (.) before the module names in the import statements indicates relative imports. It means that the imported modules are located in the same package as the current module. This is typically used in larger Python projects with a hierarchical structure to import modules from the same package or subpackages."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .segmentation import *\nfrom .structureset import *\nfrom .pet import *\nfrom .dose import *\n# Import remaining modules\n",
        "complete": "from .segmentation import *\nfrom .structureset import *\nfrom .pet import *\nfrom .dose import *\nfrom .datagraph import *\nfrom .sparsemask import *\nfrom .scan import *"
      },
      {
        "partial": "from . import (\n    segmentation,\n    structureset,\n    pet,\n    dose,\n    # Import remaining modules\n)",
        "complete": "from . import (\n    segmentation,\n    structureset,\n    pet,\n    dose,\n    datagraph,\n    sparsemask,\n    scan\n)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "segmentation.*",
        "structureset.*",
        "pet.*",
        "dose.*",
        "datagraph.*",
        "sparsemask.*",
        "scan.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_cellosaurus_helpers.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".create_cellosaurus_queries is acting as expected\", {\n  queries <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n  expect_character(queries)\n  expect_equal(queries, c(\"Accession:ID1\", \"Accession:ID2\", \"Accession:ID3\"))\n\n  queries2 <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n  expect_equal(queries2, c(\"Accession:ID1\", \"Name:ID2\", \"Species:ID3\"))\n})\n\ntest_that(\".cellosaurus_schema is acting as expected\", {\n  schema <- AnnotationGx:::.cellosaurus_schema()\n  expect_list(schema)\n  names_list <- c(\"openapi\", \"info\", \"paths\", \"components\", \"tags\")\n\n  expect_names(names(schema), subset.of = names_list)\n})\n\ntest_that(\".build_cellosaurus_request is acting as expected\", {\n  request <- AnnotationGx:::.build_cellosaurus_request()\n\n  expect_class(request, \"httr2_request\")\n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Chi%2Cca%2Csx%2Cag%2Cdi%2Cderived-from-site%2Cmisspelling&format=tsv&rows=1\"\n  expect_equal(request$url, expected)\n\n  response <- AnnotationGx:::.perform_request(request) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_class(response, \"spec_tbl_df\")\n  expect_equal(nrow(response), 1)\n\n  request2 <- AnnotationGx:::.build_cellosaurus_request(\n    query = \"id:HeLa\",\n    to = c(\n      \"id\", \"ac\", \"sy\", \"acas\", \"sx\", \"ag\", \"di\", \"dio\", \"din\", \"dr\", \"cell-type\",\n      \"derived-from-site\", \"misspelling\", \"dt\", \"dtc\", \"dtu\", \"dtv\", \"genome-ancestry\"\n    ),\n    numResults = 2, apiResource = \"search/cell-line\", output = \"TSV\"\n  )\n  expect_equal(\n    request2$url,\n    \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Csy%2Cacas%2Csx%2Cag%2Cdi%2Cdio%2Cdin%2Cdr%2Ccell-type%2Cderived-from-site%2Cmisspelling%2Cdt%2Cdtc%2Cdtu%2Cdtv%2Cgenome-ancestry&format=tsv&rows=2\"\n  )\n  response <- AnnotationGx:::.perform_request(request2) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_equal(nrow(response), 2)\n})\n\n\ntest_that(\"common_cellosaurus_fields returns the expected fields\", {\n  fields <- AnnotationGx::cellosaurus_fields(common = T, upper = T)\n  expect_character(fields)\n  expect_fields <- c(\n    \"id\", \"ac\", \"acas\", \"sy\", \"dr\", \"di\", \"din\", \"dio\", \"ox\", \"cc\",  \"sx\", \"ag\", \"oi\",\n    \"hi\", \"ch\", \"ca\",  \"dt\", \"dtc\", \"dtu\", \"dtv\", \"from\", \"group\"\n  )\n\n\n  expect_equal(fields, toupper(expect_fields))\n})\n\ntest_that(\".cellosaurus_extResources returns the expected external resources\", {\n  resources <- AnnotationGx:::.cellosaurus_extResources()\n  expect_character(resources)\n\n  expected_resources <- c(\n    \"4DN\", \"Abcam\", \"ABCD\", \"ABM\", \"AddexBio\", \"ArrayExpress\",\n    \"ATCC\", \"BCGO\", \"BCRC\", \"BCRJ\", \"BEI_Resources\",\n    \"BioGRID_ORCS_Cell_line\", \"BTO\", \"BioSample\", \"BioSamples\",\n    \"cancercelllines\", \"CancerTools\", \"CBA\", \"CCLV\", \"CCRID\",\n    \"CCTCC\", \"Cell_Biolabs\", \"Cell_Model_Passport\", \"CGH-DB\",\n    \"ChEMBL-Cells\", \"ChEMBL-Targets\", \"CLDB\", \"CLO\", \"CLS\",\n    \"ColonAtlas\", \"Coriell\", \"Cosmic\", \"Cosmic-CLP\", \"dbGAP\",\n    \"dbMHC\", \"DepMap\", \"DGRC\", \"DiscoverX\", \"DSHB\", \"DSMZ\",\n    \"DSMZCellDive\", \"EBiSC\", \"ECACC\", \"EFO\", \"EGA\", \"ENCODE\",\n    \"ESTDAB\", \"FCDI\", \"FCS-free\", \"FlyBase_Cell_line\", \"GDSC\",\n    \"GeneCopoeia\", \"GEO\", \"HipSci\", \"HIVReagentProgram\", \"Horizon_Discovery\",\n    \"hPSCreg\", \"IARC_TP53\", \"IBRC\", \"ICLC\", \"ICLDB\", \"IGRhCellID\",\n    \"IGSR\", \"IHW\", \"Imanis\", \"Innoprot\", \"IPD-IMGT/HLA\", \"ISCR\",\n    \"IZSLER\", \"JCRB\", \"KCB\", \"KCLB\", \"Kerafast\", \"KYinno\", \"LiGeA\",\n    \"LIMORE\", \"LINCS_HMS\", \"LINCS_LDP\", \"Lonza\", \"MCCL\", \"MeSH\",\n    \"MetaboLights\", \"Millipore\", \"MMRRC\", \"NCBI_Iran\", \"NCI-DTP\", \"NHCDR\",\n    \"NIHhESC\", \"NISES\", \"NRFC\", \"PerkinElmer\", \"PharmacoDB\", \"PRIDE\",\n    \"Progenetix\", \"PubChem_Cell_line\", \"RCB\", \"Rockland\", \"RSCB\", \"SKIP\",\n    \"SKY/M-FISH/CGH\", \"SLKBase\", \"TKG\", \"TNGB\", \"TOKU-E\", \"Ubigene\",\n    \"WiCell\", \"Wikidata\", \"Ximbio\"\n  )\n\n  expect_equal(resources, expected_resources)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.create_cellosaurus_queries` function in the given code snippet?",
        "answer": "The `.create_cellosaurus_queries` function is used to create formatted query strings for the Cellosaurus database. It takes two arguments: a vector of IDs and a vector of query types (e.g., 'Accession', 'Name', 'Species'). It combines these to create queries in the format 'QueryType:ID'. This function is useful for preparing search queries for the Cellosaurus API."
      },
      {
        "question": "How does the `.build_cellosaurus_request` function construct API requests, and what parameters can be customized?",
        "answer": "The `.build_cellosaurus_request` function constructs API requests for the Cellosaurus database. It creates an `httr2_request` object with a URL that includes query parameters. The function allows customization of several parameters: 'query' for the search term, 'to' for specifying fields to return, 'numResults' for the number of results, 'apiResource' for the API endpoint, and 'output' for the response format. These parameters are encoded into the URL of the request."
      },
      {
        "question": "What is the purpose of the `cellosaurus_fields` function, and how does it differ when called with `common = T, upper = T`?",
        "answer": "The `cellosaurus_fields` function returns a character vector of field names used in the Cellosaurus database. When called with `common = T, upper = T`, it returns a subset of commonly used fields with their names in uppercase. This is useful for quickly accessing the most frequently used fields in a standardized format. The function helps users specify which fields they want to retrieve from the Cellosaurus API without needing to remember all available fields."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\".create_cellosaurus_queries is acting as expected\", {\n  queries <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n  expect_character(queries)\n  expect_equal(queries, c(\"Accession:ID1\", \"Accession:ID2\", \"Accession:ID3\"))\n\n  queries2 <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n  # Complete the test for queries2\n})",
        "complete": "test_that(\".create_cellosaurus_queries is acting as expected\", {\n  queries <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n  expect_character(queries)\n  expect_equal(queries, c(\"Accession:ID1\", \"Accession:ID2\", \"Accession:ID3\"))\n\n  queries2 <- AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n  expect_equal(queries2, c(\"Accession:ID1\", \"Name:ID2\", \"Species:ID3\"))\n})"
      },
      {
        "partial": "test_that(\".build_cellosaurus_request is acting as expected\", {\n  request <- AnnotationGx:::.build_cellosaurus_request()\n\n  expect_class(request, \"httr2_request\")\n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Chi%2Cca%2Csx%2Cag%2Cdi%2Cderived-from-site%2Cmisspelling&format=tsv&rows=1\"\n  expect_equal(request$url, expected)\n\n  response <- AnnotationGx:::.perform_request(request) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_class(response, \"spec_tbl_df\")\n  expect_equal(nrow(response), 1)\n\n  # Complete the test for request2\n})",
        "complete": "test_that(\".build_cellosaurus_request is acting as expected\", {\n  request <- AnnotationGx:::.build_cellosaurus_request()\n\n  expect_class(request, \"httr2_request\")\n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Chi%2Cca%2Csx%2Cag%2Cdi%2Cderived-from-site%2Cmisspelling&format=tsv&rows=1\"\n  expect_equal(request$url, expected)\n\n  response <- AnnotationGx:::.perform_request(request) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_class(response, \"spec_tbl_df\")\n  expect_equal(nrow(response), 1)\n\n  request2 <- AnnotationGx:::.build_cellosaurus_request(\n    query = \"id:HeLa\",\n    to = c(\n      \"id\", \"ac\", \"sy\", \"acas\", \"sx\", \"ag\", \"di\", \"dio\", \"din\", \"dr\", \"cell-type\",\n      \"derived-from-site\", \"misspelling\", \"dt\", \"dtc\", \"dtu\", \"dtv\", \"genome-ancestry\"\n    ),\n    numResults = 2, apiResource = \"search/cell-line\", output = \"TSV\"\n  )\n  expect_equal(\n    request2$url,\n    \"https://api.cellosaurus.org/search/cell-line?q=id%3AHeLa&sort=ac%20asc&fields=id%2Cac%2Csy%2Cacas%2Csx%2Cag%2Cdi%2Cdio%2Cdin%2Cdr%2Ccell-type%2Cderived-from-site%2Cmisspelling%2Cdt%2Cdtc%2Cdtu%2Cdtv%2Cgenome-ancestry&format=tsv&rows=2\"\n  )\n  response <- AnnotationGx:::.perform_request(request2) |> AnnotationGx:::.parse_resp_tsv(show_col_types = FALSE, skip = 14)\n  expect_equal(nrow(response), 2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/getRawSensitivityMatrix.R",
    "language": "R",
    "content": "##TODO:: Add function documentation\ngetRawSensitivityMatrix <-\n  function(pSet, cell.id, drug.id, max.conc, quality) {\n    cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n    if(!missing(quality)) {\n      if(is.element(\"quality\", colnames(sensitivityInfo(pSet)))) {\n        cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n      }\n    }\n    if(!missing(max.conc)) {\n        if(is.element(\"max.conc\", colnames(sensitivityInfo(pSet)))) {\n          if(length(max.conc) > 1) {\n            max.conc <- paste(max.conc, collapse=\"///\")\n        }\n        cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n      }\n    }\n    if(length(drug.id) > 1) {\n      drug.id <- paste(drug.id, collapse=\"///\")\n    }\n    cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n\n    exp.id <- which(eval(parse(text=cond)))\n\n    sensitivity.raw.matrix <- list()\n    if(length(exp.id) > 0) {\n      for(i in seq_len(length(exp.id))){\n        if(length(grep(\"///\", drug.id)) > 0) {\n          all.exp.id <- which(sensitivityInfo(pSet)$combination.exp.id == sensitivityInfo(pSet)[exp.id[i], \"combination.exp.id\"])\n          drug.1 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == unlist(strsplit(drug.id, split=\"///\"))[1])\n          drug.2 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == unlist(strsplit(drug.id, split=\"///\"))[2])\n          drug.1.doses <- length(which(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.1], , \"Dose\"])))\n          drug.2.doses <- length(which(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.2], , \"Dose\"])))\n\n          tt <- matrix(NA, ncol=drug.2.doses, nrow=drug.1.doses)\n          colnames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Dose\"]\n          rownames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Dose\"]\n          tt[ ,1] <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Viability\"]\n          tt[1, ] <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Viability\"]\n          tt[2:nrow(tt), 2:ncol(tt)] <- sensitivityRaw(pSet)[exp.id[i], , \"Viability\"]\n          sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- tt\n        }else{\n          sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- sensitivityRaw(pSet)[exp.id[i], , ]\n        }\n      }\n    }\n    return(sensitivity.raw.matrix)\n  }\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'cond' variable in the getRawSensitivityMatrix function, and how is it constructed?",
        "answer": "The 'cond' variable is used to build a conditional string for filtering the sensitivity data. It starts with a base condition checking if the sample ID matches the given cell.id. Additional conditions are appended based on the provided parameters (quality, max.conc, and drug.id) using string concatenation with the '&' operator. This string is later evaluated to select the appropriate experiments."
      },
      {
        "question": "How does the function handle multiple drug IDs, and what is the significance of the '///' separator?",
        "answer": "When multiple drug IDs are provided, the function joins them with '///' as a separator. This is used to handle combination experiments. Later in the function, if a '///' is detected in the drug.id, it splits the string to process each drug separately. The function then creates a matrix representing the combination of doses for both drugs, with viability data for individual drugs on the edges and combination data in the center."
      },
      {
        "question": "What is the structure of the returned 'sensitivity.raw.matrix' list, and how does it differ for single drug vs. combination experiments?",
        "answer": "The 'sensitivity.raw.matrix' is a list where each element corresponds to an experiment. For single drug experiments, each element contains the raw sensitivity data directly from sensitivityRaw(pSet). For combination experiments, each element is a matrix where rows and columns represent doses of the two drugs, cell values represent viability, and the first row and column contain individual drug viability data. The list keys are the row names from sensitivityInfo(pSet) for the selected experiments."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  if(!missing(quality) && \"quality\" %in% colnames(sensitivityInfo(pSet))) {\n    cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n  }\n  if(!missing(max.conc) && \"max.conc\" %in% colnames(sensitivityInfo(pSet))) {\n    max.conc <- if(length(max.conc) > 1) paste(max.conc, collapse=\"///) else max.conc\n    cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n  }\n  drug.id <- if(length(drug.id) > 1) paste(drug.id, collapse=\"///) else drug.id\n  cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      # TODO: Implement logic for single and combination drug experiments\n    }\n  }\n  return(sensitivity.raw.matrix)\n}",
        "complete": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  if(!missing(quality) && \"quality\" %in% colnames(sensitivityInfo(pSet))) {\n    cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n  }\n  if(!missing(max.conc) && \"max.conc\" %in% colnames(sensitivityInfo(pSet))) {\n    max.conc <- if(length(max.conc) > 1) paste(max.conc, collapse=\"///) else max.conc\n    cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n  }\n  drug.id <- if(length(drug.id) > 1) paste(drug.id, collapse=\"///) else drug.id\n  cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      if(grepl(\"///\", drug.id)) {\n        all.exp.id <- which(sensitivityInfo(pSet)$combination.exp.id == sensitivityInfo(pSet)[exp.id[i], \"combination.exp.id\"])\n        drugs <- strsplit(drug.id, \"///\")[[1]]\n        drug.1 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[1])\n        drug.2 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[2])\n        drug.1.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.1], , \"Dose\"]))\n        drug.2.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.2], , \"Dose\"]))\n        \n        tt <- matrix(NA, ncol=drug.2.doses, nrow=drug.1.doses)\n        colnames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Dose\"]\n        rownames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Dose\"]\n        tt[,1] <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Viability\"]\n        tt[1,] <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Viability\"]\n        tt[2:nrow(tt), 2:ncol(tt)] <- sensitivityRaw(pSet)[exp.id[i], , \"Viability\"]\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- tt\n      } else {\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- sensitivityRaw(pSet)[exp.id[i], , ]\n      }\n    }\n  }\n  return(sensitivity.raw.matrix)\n}"
      },
      {
        "partial": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  # TODO: Add conditions for quality, max.conc, and drug.id\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      # TODO: Implement logic for single and combination drug experiments\n    }\n  }\n  return(sensitivity.raw.matrix)\n}",
        "complete": "getRawSensitivityMatrix <- function(pSet, cell.id, drug.id, max.conc, quality) {\n  cond <- \"sensitivityInfo(pSet)$sampleid == cell.id\"\n  if(!missing(quality) && \"quality\" %in% colnames(sensitivityInfo(pSet))) {\n    cond <- paste(cond, \"sensitivityInfo(pSet)$quality == quality\", sep=\" & \")\n  }\n  if(!missing(max.conc) && \"max.conc\" %in% colnames(sensitivityInfo(pSet))) {\n    max.conc <- if(length(max.conc) > 1) paste(max.conc, collapse=\"///) else max.conc\n    cond <- paste(cond, \"sensitivityInfo(pSet)$max.conc == max.conc\", sep=\" & \")\n  }\n  drug.id <- if(length(drug.id) > 1) paste(drug.id, collapse=\"///) else drug.id\n  cond <- paste(cond, \"sensitivityInfo(pSet)$treatmentid == drug.id\", sep=\" & \")\n  exp.id <- which(eval(parse(text=cond)))\n  \n  sensitivity.raw.matrix <- list()\n  if(length(exp.id) > 0) {\n    for(i in seq_len(length(exp.id))){\n      if(grepl(\"///\", drug.id)) {\n        all.exp.id <- which(sensitivityInfo(pSet)$combination.exp.id == sensitivityInfo(pSet)[exp.id[i], \"combination.exp.id\"])\n        drugs <- strsplit(drug.id, \"///\")[[1]]\n        drug.1 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[1])\n        drug.2 <- which(sensitivityInfo(pSet)[all.exp.id, \"treatmentid\"] == drugs[2])\n        drug.1.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.1], , \"Dose\"]))\n        drug.2.doses <- sum(!is.na(sensitivityRaw(pSet)[all.exp.id[drug.2], , \"Dose\"]))\n        \n        tt <- matrix(NA, ncol=drug.2.doses, nrow=drug.1.doses)\n        colnames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Dose\"]\n        rownames(tt) <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Dose\"]\n        tt[,1] <- sensitivityRaw(pSet)[all.exp.id[drug.1], seq_len(drug.1.doses), \"Viability\"]\n        tt[1,] <- sensitivityRaw(pSet)[all.exp.id[drug.2], seq_len(drug.2.doses), \"Viability\"]\n        tt[2:nrow(tt), 2:ncol(tt)] <- sensitivityRaw(pSet)[exp.id[i], , \"Viability\"]\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- tt\n      } else {\n        sensitivity.raw.matrix[[rownames(sensitivityInfo(pSet))[exp.id[i]]]] <- sensitivityRaw(pSet)[exp.id[i], , ]\n      }\n    }\n  }\n  return(sensitivity.raw.matrix)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_image_processing.py",
    "language": "py",
    "content": "from readii.loaders import *\nfrom readii.image_processing import *\nimport pytest\n\n@pytest.fixture\ndef nsclcCTImage():\n    nsclcCTPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n    return loadDicomSITK(nsclcCTPath)\n\n@pytest.fixture\ndef nsclcSEGImage():\n    nsclcSEGPath = \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n    segDictionary = loadSegmentation(nsclcSEGPath, modality = 'SEG')\n    return segDictionary['Heart']\n\n@pytest.fixture\ndef lung4DRTSTRUCTImage():\n    lung4DRTSTRUCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-47.35/1-1.dcm\"\n    lung4DCTPath = \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n    segDictionary = loadSegmentation(lung4DRTSTRUCTPath, modality = 'RTSTRUCT',\n                                     baseImageDirPath = lung4DCTPath, roiNames = 'Tumor_c.*')\n    return segDictionary['Tumor_c40']\n\n\ndef test_flattenImage(nsclcSEGImage):\n    \"\"\"Test removing extra dimension of image that has size 1\"\"\"\n    actual = flattenImage(nsclcSEGImage)\n    assert isinstance(actual, sitk.Image), \\\n        \"Wrong object type, need to convert back to sitk.Image\"\n    assert actual.GetSize() == (512, 512, 304), \\\n        \"Wrong image size\"\n\n\ndef test_alignImages(nsclcCTImage, nsclcSEGImage):\n    \"\"\"Test setting segmentation origin, direction, and spacing to match the CT after flattening\"\"\"\n    flattenedSEG = flattenImage(nsclcSEGImage)\n    actual = alignImages(nsclcCTImage, flattenedSEG)\n    assert actual.GetSize() == (512, 512, 304), \\\n        \"Wrong image size\"\n    assert actual.GetSpacing() == (0.693359375, 0.693359375, 1.0), \\\n        \"Wrong spacing\"\n    assert actual.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0), \\\n        \"Wrong origin\"\n\n@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", 255),\n        (\"lung4DRTSTRUCTImage\", 1)\n    ]\n)\ndef test_getROIVoxelLabel(segImage, expected, request):\n    \"\"\"Test getting the voxel value in the ROI in a segmentation for both SEG and RTSTRUCT images\"\"\"\n    segImage = request.getfixturevalue(segImage)\n    assert getROIVoxelLabel(segImage) == expected\n\n@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", (238, 252, 124)),\n        (\"lung4DRTSTRUCTImage\", (63, 314, 318))\n    ]\n)\ndef test_getROICenterCoords(segImage, expected, request):\n    \"\"\"Test getting the center slice and coordinates for an ROI in both SEG and RTSTRUCT images\"\"\"\n    segImage = request.getfixturevalue(segImage)\n    flatSegImage = flattenImage(segImage)\n    centerSliceIdx, centerColumnPixelIdx, centerRowPixelIdx = getROICenterCoords(flatSegImage)\n    assert centerSliceIdx == expected[0], \\\n        \"Slice number is wrong\"\n    assert centerColumnPixelIdx == expected[1], \\\n        \"Center column pixel value is wrong\"\n    assert centerRowPixelIdx == expected[2], \\\n        \"Center row pixel value is wrong\"",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `flattenImage` function in the given code, and how is it tested?",
        "answer": "The `flattenImage` function is used to remove an extra dimension of an image that has a size of 1. It's tested in the `test_flattenImage` function, which checks if the returned object is an instance of `sitk.Image` and if it has the correct size of (512, 512, 304) after flattening."
      },
      {
        "question": "How does the `alignImages` function work, and what properties of the images does it align?",
        "answer": "The `alignImages` function sets the segmentation image's origin, direction, and spacing to match those of the CT image. It's tested in the `test_alignImages` function, which checks if the aligned image has the correct size (512, 512, 304), spacing (0.693359375, 0.693359375, 1.0), and origin (-182.1533203125, -314.1533203125, -305.0)."
      },
      {
        "question": "What is the purpose of the `@pytest.mark.parametrize` decorator in the `test_getROIVoxelLabel` and `test_getROICenterCoords` functions?",
        "answer": "The `@pytest.mark.parametrize` decorator is used to run the same test function with different input parameters. In this case, it's used to test the `getROIVoxelLabel` and `getROICenterCoords` functions with both SEG and RTSTRUCT image types, ensuring that these functions work correctly for different types of segmentation images."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_flattenImage(nsclcSEGImage):\n    actual = flattenImage(nsclcSEGImage)\n    assert isinstance(actual, sitk.Image)\n    assert actual.GetSize() == (512, 512, 304)",
        "complete": "def test_flattenImage(nsclcSEGImage):\n    actual = flattenImage(nsclcSEGImage)\n    assert isinstance(actual, sitk.Image), \"Wrong object type, need to convert back to sitk.Image\"\n    assert actual.GetSize() == (512, 512, 304), \"Wrong image size\""
      },
      {
        "partial": "@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", 255),\n        (\"lung4DRTSTRUCTImage\", 1)\n    ]\n)\ndef test_getROIVoxelLabel(segImage, expected, request):\n    segImage = request.getfixturevalue(segImage)\n    assert getROIVoxelLabel(segImage) == expected",
        "complete": "@pytest.mark.parametrize(\n    \"segImage, expected\",\n    [\n        (\"nsclcSEGImage\", 255),\n        (\"lung4DRTSTRUCTImage\", 1)\n    ]\n)\ndef test_getROIVoxelLabel(segImage, expected, request):\n    \"\"\"Test getting the voxel value in the ROI in a segmentation for both SEG and RTSTRUCT images\"\"\"\n    segImage = request.getfixturevalue(segImage)\n    assert getROIVoxelLabel(segImage) == expected"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest"
      ],
      "from_imports": [
        "readii.loaders.*",
        "readii.image_processing.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/__init__.py",
    "language": "py",
    "content": "from . import io, ops, utils, pipeline\n\n__all__ = [\"io\", \"ops\", \"utils\", \"pipeline\"]\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the __all__ variable in this Python module?",
        "answer": "The __all__ variable in this module explicitly defines the public interface of the module. It specifies which names will be imported when a user does 'from module import *'. In this case, it includes 'io', 'ops', 'utils', and 'pipeline', indicating that these are the main components that should be accessible when the module is imported."
      },
      {
        "question": "How does the import statement in this code differ from a standard import, and what does it imply about the module structure?",
        "answer": "The import statement 'from . import io, ops, utils, pipeline' uses a relative import (indicated by the dot '.'). This implies that the current module is part of a package, and it's importing the specified modules (io, ops, utils, pipeline) from the same package. This structure suggests a well-organized, modular codebase where related functionality is grouped together in a package."
      },
      {
        "question": "What would be the effect if the __all__ variable was omitted from this module?",
        "answer": "If the __all__ variable was omitted, it would not affect the relative imports at the top of the file. However, it would change the behavior when someone uses 'from module import *'. Without __all__, Python would import all names that don't start with an underscore. By explicitly defining __all__, the module author has more control over the public interface, potentially hiding implementation details and reducing namespace pollution."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from . import io, ops, utils, pipeline\n\n__all__ = [",
        "complete": "from . import io, ops, utils, pipeline\n\n__all__ = [\"io\", \"ops\", \"utils\", \"pipeline\"]"
      },
      {
        "partial": "from . import io, ops, utils, pipeline\n\n__all__ =",
        "complete": "from . import io, ops, utils, pipeline\n\n__all__ = [\"io\", \"ops\", \"utils\", \"pipeline\"]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "None.io"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/image.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\nfrom typing import NamedTuple, Sequence, Union\n\n# ImageGeometry = namedtuple(\"ImageGeometry\", \"origin, direction, spacing\")\n\nclass ImageGeometry(NamedTuple):\n    size:      Sequence[int]\n    origin:    Sequence[float]\n    direction: Sequence[float]\n    spacing:   Sequence[float]\n\n\ndef physical_point_to_index(point, reference_geometry, continuous=False):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type `ImageGeometry`, `imgtools.Image` or `sitk.Image`, got {type(reference_geometry)}\")\n\n    if continuous:\n        return ref_image.TransformPhysicalPointToContinuousIndex(point[::-1])[::-1]\n    else:\n        return ref_image.TransformPhysicalPointToIndex(point[::-1])[::-1]\n\n\ndef index_to_physical_point(index, reference_geometry):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type ImageGeometry, imgtools.Image or sitk.Image, got {type(reference_geometry)}\")\n\n    continuous = any([isinstance(i, float) for i in index])\n\n    if continuous:\n        return ref_image.TransformContinuousIndexToPhysicalPoint(index[::-1])[::-1]\n    else:\n        return ref_image.TransformIndexToPhysicalPoint(index[::-1])[::-1]\n\n\n# Goals for this module:\n# - One Image Class to Rule Them All (no more juggling numpy & sitk)\n# - consistent indexing (z, y, x) (maybe fancy indexing?)\n# - can be created either from array or sitk.Image\n# - implements all basic operators of sitk.Image\n# - easy conversion to array or sitk\n# - method to apply arbitrary sitk filter\n# - nicer repr\n\n# TODO:\n# - better support for masks (e.g. what happens when we pass a one-hot encoded mask?)\n# - (optional) specify indexing order when creating new Image (sitk or numpy)\n\n\nclass Image:\n    def __init__(self,\n                 image:     Union[sitk.Image, np.ndarray] = None,\n                 geometry:  ImageGeometry   = None,\n                 origin:    Sequence[float] = None,\n                 direction: Sequence[float] = None,\n                 spacing:   Sequence[float] = None):\n\n        if isinstance(image, sitk.Image):\n            self._image = image\n        elif isinstance(image, np.ndarray):\n            if geometry is None and any((origin is None, direction is None, spacing is None)):\n                raise ValueError(\n                    \"If image is a Numpy array, either geometry must be specified.\"\n                )\n\n            if geometry is not None:\n                _, origin, direction, spacing = geometry\n\n            self._image = sitk.GetImageFromArray(image)\n            self._image.SetOrigin(origin[::-1])\n            direction = tuple(direction)\n            self._image.SetDirection(direction[:-3] + direction[3:6] + direction[:3])\n            self._image.SetSpacing(spacing[::-1])\n        else:\n            raise TypeError(\n                f\"image must be either numpy.ndarray or SimpleITK.Image, not {type(image)}.\"\n            )\n\n    @property\n    def size(self):\n        return self._image.GetSize()[::-1]\n\n    @property\n    def origin(self):\n        return self._image.GetOrigin()[::-1]\n\n    @property\n    def direction(self):\n        direction = self._image.GetDirection()\n        direction = direction[:-3] + direction[3:6] + direction[:3]\n        return direction\n\n    @property\n    def spacing(self):\n        return self._image.GetSpacing()[::-1]\n\n    @property\n    def geometry(self):\n        return ImageGeometry(size=self.size,\n                             origin=self.origin,\n                             direction=self.direction,\n                             spacing=self.spacing)\n\n    @property\n    def ndim(self):\n        return len(self.size)\n\n    @property\n    def dtype(self):\n        return self._image.GetPixelIDType()\n\n    def astype(self, new_type):\n        return Image(sitk.Cast(self._image, new_type))\n\n    def to_sitk_image(self):\n        return self._image\n\n    def to_numpy(self, return_geometry=False, view=False):\n        if view:\n            array = sitk.GetArrayViewFromImage(self._image)\n        else:\n            array = sitk.GetArrayFromImage(self._image)\n        if return_geometry:\n            return array, self.geometry\n        return array\n\n    def __getitem__(self, idx):\n        if isinstance(idx, (int, slice)):\n            idx = (idx, )\n        if len(idx) < self.ndim:\n            idx += (slice(None), ) * (self.ndim - len(idx))\n\n        idx = idx[::-1]  # SimpleITK uses (x, y, z) ordering internally\n\n        value = self._image[idx]\n\n        try:  # XXX there probably is a nicer way to check if value is a scalar\n            return Image(value)\n        except TypeError:\n            return value\n\n    def __setitem__(self, idx, value):\n        if isinstance(idx, (int, slice)):\n            idx = (idx, )\n        if len(idx) < self.ndim:\n            idx += (slice(None), ) * (self.ndim - len(idx))\n\n        idx = idx[::-1]  # SimpleITK uses (x, y, z) ordering internally\n\n        value = self._image[idx]\n\n        try:  # XXX there probably is a nicer way to check if value is a scalar\n            return Image(value)\n        except TypeError:\n            return value\n\n    def apply_filter(self, sitk_filter):\n        result = sitk_filter.Execute(self._image)\n        if isinstance(result, sitk.Image):\n            return Image(result)\n        else:\n            return result\n\n    def __neg__(self):\n        return Image(-self._image)\n\n    def __abs__(self):\n        return Image(abs(self._image))\n\n    def __invert__(self):\n        return Image(~self._image)\n\n    def __add__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image + other_val)\n\n    def __sub__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image - other_val)\n\n    def __mul__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image * other_val)\n\n    def __div__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image / other_val)\n\n    def __floordiv__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image / other_val)\n\n    def __pow__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        return Image(self._image ** other_val)\n\n    def __iadd__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image += other_val\n\n    def __isub__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image -= other_val\n\n    def __imul__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image *= other_val\n\n    def __idiv__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image /= other_val\n\n    def __ifloordiv__(self, other):\n        other_val = getattr(other, \"_image\", other)\n        self._image //= other_val\n\n    def __iter__(self):\n        pass\n\n    def __repr__(self):\n        return f\"Image(image={self._image}, origin={self.origin}, spacing={self.spacing}, direction={self.direction})\"\n\n    def __str__(self):\n        return f\"origin = {self.origin}\\nspacing = {self.spacing}\\ndirection = {self.direction}\\nvalues = \\n{self.to_numpy(view=True)}\"\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `ImageGeometry` class in this code snippet, and how is it structured?",
        "answer": "The `ImageGeometry` class is a NamedTuple that represents the geometric properties of an image. It has four attributes: size (Sequence[int]), origin (Sequence[float]), direction (Sequence[float]), and spacing (Sequence[float]). This class provides a convenient way to store and access the geometric information of an image, which is essential for various image processing operations."
      },
      {
        "question": "Explain the functionality of the `physical_point_to_index` function and its parameters.",
        "answer": "The `physical_point_to_index` function converts a physical point in 3D space to its corresponding index in the image grid. It takes three parameters: 'point' (the physical coordinates to convert), 'reference_geometry' (the geometry of the reference image, which can be an ImageGeometry, imgtools.Image, or sitk.Image), and 'continuous' (a boolean flag to determine if the output should be a continuous index). The function handles different input types for reference_geometry, creates a SimpleITK image if necessary, and then uses SimpleITK's transformation methods to convert the point to an index."
      },
      {
        "question": "Describe the key features and functionalities of the `Image` class in this code snippet.",
        "answer": "The `Image` class is a wrapper around SimpleITK's Image class, providing a more user-friendly interface. Key features include: 1) Initialization from either a SimpleITK Image or a NumPy array with geometry information. 2) Properties for accessing image attributes (size, origin, direction, spacing, geometry, ndim, dtype). 3) Methods for type conversion (astype), exporting to SimpleITK or NumPy formats. 4) Indexing support with SimpleITK's (x, y, z) order converted to (z, y, x). 5) Application of SimpleITK filters through the apply_filter method. 6) Overloaded arithmetic operators for image operations. 7) A custom string representation for easier debugging and visualization."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def physical_point_to_index(point, reference_geometry, continuous=False):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type `ImageGeometry`, `imgtools.Image` or `sitk.Image`, got {type(reference_geometry)}\")\n\n    # Complete the function here",
        "complete": "def physical_point_to_index(point, reference_geometry, continuous=False):\n    if isinstance(reference_geometry, ImageGeometry):\n        ref_image = sitk.Image(reference_geometry.size[::-1], 0)\n        ref_image.SetOrigin(reference_geometry.origin[::-1])\n        direction = reference_geometry.direction\n        direction_rev = direction[:-3] + direction[3:6] + direction[:3]\n        ref_image.SetDirection(direction_rev)\n        ref_image.SetSpacing(reference_geometry.spacing[::-1])\n    elif isinstance(reference_geometry, Image):\n        ref_image = reference_geometry._image\n    elif isinstance(reference_geometry, sitk.Image):\n        ref_image = reference_geometry\n    else:\n        raise ValueError(f\"Reference geometry must be of type `ImageGeometry`, `imgtools.Image` or `sitk.Image`, got {type(reference_geometry)}\")\n\n    if continuous:\n        return ref_image.TransformPhysicalPointToContinuousIndex(point[::-1])[::-1]\n    else:\n        return ref_image.TransformPhysicalPointToIndex(point[::-1])[::-1]"
      },
      {
        "partial": "class Image:\n    def __init__(self,\n                 image:     Union[sitk.Image, np.ndarray] = None,\n                 geometry:  ImageGeometry   = None,\n                 origin:    Sequence[float] = None,\n                 direction: Sequence[float] = None,\n                 spacing:   Sequence[float] = None):\n\n        if isinstance(image, sitk.Image):\n            self._image = image\n        elif isinstance(image, np.ndarray):\n            if geometry is None and any((origin is None, direction is None, spacing is None)):\n                raise ValueError(\n                    \"If image is a Numpy array, either geometry must be specified.\"\n                )\n\n            if geometry is not None:\n                _, origin, direction, spacing = geometry\n\n            # Complete the initialization for numpy array here\n        else:\n            raise TypeError(\n                f\"image must be either numpy.ndarray or SimpleITK.Image, not {type(image)}.\"\n            )",
        "complete": "class Image:\n    def __init__(self,\n                 image:     Union[sitk.Image, np.ndarray] = None,\n                 geometry:  ImageGeometry   = None,\n                 origin:    Sequence[float] = None,\n                 direction: Sequence[float] = None,\n                 spacing:   Sequence[float] = None):\n\n        if isinstance(image, sitk.Image):\n            self._image = image\n        elif isinstance(image, np.ndarray):\n            if geometry is None and any((origin is None, direction is None, spacing is None)):\n                raise ValueError(\n                    \"If image is a Numpy array, either geometry must be specified.\"\n                )\n\n            if geometry is not None:\n                _, origin, direction, spacing = geometry\n\n            self._image = sitk.GetImageFromArray(image)\n            self._image.SetOrigin(origin[::-1])\n            direction = tuple(direction)\n            self._image.SetDirection(direction[:-3] + direction[3:6] + direction[:3])\n            self._image.SetSpacing(spacing[::-1])\n        else:\n            raise TypeError(\n                f\"image must be either numpy.ndarray or SimpleITK.Image, not {type(image)}.\"\n            )"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy"
      ],
      "from_imports": [
        "typing.NamedTuple"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/metadata.py",
    "language": "py",
    "content": "import os\nimport pandas as pd\nfrom typing import Optional, Literal\nfrom readii.utils import get_logger\n\nlogger = get_logger()\n\ndef createImageMetadataFile(outputDir, parentDirPath, datasetName, segType, imageFileListPath, update = False):\n    imageMetadataPath = os.path.join(outputDir, \"ct_to_seg_match_list_\" + datasetName + \".csv\")\n    if os.path.exists(imageMetadataPath) and not update:\n        logger.info(f\"Image metadata file {imageMetadataPath} already exists & update flag is {update}.\")\n        return imageMetadataPath\n    elif update:\n        logger.info(f\"{update=}, Image metadata file {imageMetadataPath} will be overwritten.\")\n    else:\n        logger.info(f\"Image metadata file {imageMetadataPath} not found.. creating...\")\n    \n    if segType == \"RTSTRUCT\":\n        imageFileEdgesPath = os.path.join(parentDirPath + \"/.imgtools/imgtools_\" + datasetName + \"_edges.csv\")\n        getCTWithSegmentation(imgFileEdgesPath = imageFileEdgesPath,\n                                segType = segType,\n                                outputFilePath = imageMetadataPath)\n    elif segType == \"SEG\":\n        matchCTtoSegmentation(imgFileListPath = imageFileListPath,\n                                segType = segType,\n                                outputFilePath = imageMetadataPath)\n    else:\n        logger.info(f\"Expecting either RTSTRUCT or SEG segmentation type. Found {segType}.\")\n        raise ValueError(\"Incorrect segmentation type or segmentation type is missing from med-imagetools output. Must be RTSTRUCT or SEG.\")\n    return imageMetadataPath\n\ndef saveDataframeCSV(\n    dataframe: pd.DataFrame, \n    outputFilePath: str\n) -> None:\n    \"\"\"Function to save a pandas Dataframe as a csv file with the index removed.\n            Checks if the path in the outputFilePath exists and will create any missing directories.\n\n    Parameters\n    ----------\n    dataframe : pd.DataFrame\n        Pandas dataframe to save out as a csv\n    outputFilePath : str\n        Full file path to save the dataframe out to.\n            \n    Raises\n    ------\n    ValueError\n        If the outputFilePath does not end in .csv, if the dataframe is not a pandas DataFrame, \n        or if an error occurs while saving the dataframe.\n    \"\"\"\n    \n    if not outputFilePath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function saves .csv files, so outputFilePath must end in .csv\"\n        )\n\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError(\"Function expects a pandas DataFrame to save out.\")\n\n    # Make directory if it doesn't exist, but don't fail if it already exists\n    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n\n    try:\n        # Save out DataFrame\n        dataframe.to_csv(outputFilePath, index=False)\n    except Exception as e:\n        error_msg = f\"An error occurred while saving the DataFrame: {str(e)}\"\n        raise ValueError(error_msg) from e\n    else:\n        return\n\n\ndef matchCTtoSegmentation(\n    imgFileListPath: str, \n    segType: str, \n    outputFilePath: Optional[str] = None,\n) -> pd.DataFrame:\n    \"\"\"From full list of image files, extract CT and corresponding segmentation files and create new table.\n    One row of the table contains both the CT and segmentation data for one patient.\n    This function currently assumes there is one segmentation for each patient.\n\n    Parameters\n    ----------\n    imgFileListPath : str\n        Path to csv containing list of image directories/paths in the dataset.\n        Expecting output from med-imagetools autopipeline .imgtools_[dataset]\n    segType : str\n        Type of file segmentation is in. Can be SEG or RTSTRUCT.\n    outputDirPath : str\n        Optional file path to save the dataframe to as a csv.\n\n    Returns\n    -------\n    pd.Dataframe\n        Dataframe containing the CT and corresponding segmentation data for each patient\n    \n    Raises\n    ------\n    ValueError\n        If the segmentation file type is not RTSTRUCT or SEG, or if the imgFileListPath does not end in .csv\n\n    Note: All subseries of CT will be kept in the dataframe in this function\n    \"\"\"\n    # Check that segmentation file type is acceptable\n    if segType not in [\"RTSTRUCT\", \"SEG\"]:\n        raise ValueError(\"Incorrect segmentation file type. Must be RTSTRUCT or SEG.\")\n\n    # Check that imgFileListPath is a csv file to properly be loaded in\n    if not imgFileListPath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function expects to load in a .csv file, so imgFileListPath must end in .csv\"\n        )\n\n    if not os.path.exists(imgFileListPath):\n        logger.error(f\"Image file list {imgFileListPath} not found.\")\n        raise FileNotFoundError(\"Image file list not found.\")\n    \n    # Load in complete list of patient image directories of all modalities (output from med-imagetools crawl)\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n\n    # Extract all CT rows\n    allCTRows: pd.DataFrame = fullDicomList.loc[fullDicomList[\"modality\"] == \"CT\"]\n\n    # Extract all SEG rows\n    allSegRows: pd.DataFrame = fullDicomList.loc[fullDicomList[\"modality\"] == segType]\n\n    # Merge the CT and segmentation dataframes based on the CT ID (referenced in the segmentation rows)\n    # Uses only segmentation keys, so no extra CTs are kept\n    # If multiple CTs have the same ID, they are both included in this table\n    samplesWSeg: pd.DataFrame = allCTRows.merge(\n        allSegRows,\n        how=\"inner\",\n        left_on=[\"series\", \"patient_ID\"],\n        right_on=[\"reference_ct\", \"patient_ID\"],\n        suffixes=(\"_CT\", \"_seg\"),\n    )\n\n    # Sort dataframe by ascending patient ID value\n    samplesWSeg.sort_values(by=\"patient_ID\", inplace=True)\n\n    # Save out the combined list\n    if outputFilePath != None:\n        saveDataframeCSV(samplesWSeg, outputFilePath)\n\n    return samplesWSeg\n\n\ndef getCTWithSegmentation(imgFileEdgesPath: str, \n                          segType: str = \"RTSTRUCT\",\n                          outputFilePath: Optional[str] = None,\n) -> pd.DataFrame:\n    \"\"\"From full list of image files edges from med-imagetools, get the list of CTs with segmentation.\n    These are marked as edge type 2 in the edges file.\n    Note: This function can only handle RTSTRUCT segmentations as this is what med-imagetools catches at this point.\n\n    Parameters\n    ----------\n    imgFileEdgesPath : str\n        Path to csv containing list of image directories/paths in the dataset with the edge types.\n        Expecting output from med-imagetools autopipeline .imgtools_[dataset]_edges\n    segType : str\n        Type of file segmentation is in. Must be RTSTRUCT.\n    outputFilePath : Optional[str]\n        Optional file path to save the dataframe to as a csv.\n\n    Returns\n    -------\n    pd.Dataframe\n        Dataframe containing the CT and corresponding segmentation data for each patient\n    \n    Raises\n    ------\n    ValueError\n        If the segmentation file type is not RTSTRUCT, or if the imgFileEdgesPath does not end in .csv\n    \"\"\"\n\n    # Check that segmentation file type is acceptable\n    if segType != \"RTSTRUCT\":\n        raise ValueError(\"Incorrect segmentation file type. Must be RTSTRUCT. For SEG, use matchCTtoSegmentation.\")\n\n    # Check that imgFileListPath is a csv file to properly be loaded in\n    if not imgFileEdgesPath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function expects to load in a .csv file, so imgFileEdgesPath must end in .csv\"\n        )\n    \n    # Load in complete list of patient image directories of all modalities and edge types(output from med-imagetools crawl)\n    fullDicomEdgeList: pd.DataFrame = pd.read_csv(imgFileEdgesPath)\n\n    # Get just the CTs with segmentations, marked as edge type 2 in the edges file\n    samplesWSeg: pd.DataFrame = fullDicomEdgeList.loc[fullDicomEdgeList[\"edge_type\"] == 2]\n\n    # Replace the _x and _y suffixes in the column names with _CT and _seg to match matchCTtoSegmentation\n    samplesWSeg.columns = samplesWSeg.columns.str.replace(\"_x\", \"_CT\", regex=True)\n    samplesWSeg.columns = samplesWSeg.columns.str.replace(\"_y\", \"_seg\", regex=True)\n\n    # Remove the _CT suffix from the patient_ID column to match matchCTtoSegmentation\n    samplesWSeg.rename(columns={\"patient_ID_CT\": \"patient_ID\"}, inplace=True)\n\n    sortedSamplesWSeg = samplesWSeg.sort_values(by=\"patient_ID\")\n\n    # Save out the combined list\n    if outputFilePath != None:\n        saveDataframeCSV(sortedSamplesWSeg, outputFilePath)\n\n    return sortedSamplesWSeg\n\n\ndef getSegmentationType(\n    imgFileListPath: str\n) -> Literal['RTSTRUCT', 'SEG']:\n    \"\"\"Find the segmentation type from the full list of image files.\n\n    Parameters\n    ----------\n    imgFileListPath : str\n        Path to csv containing list of image directories/paths in the dataset.\n        Expecting output from med-imagetools autopipeline .imgtools_[dataset]\n\n    Returns\n    -------\n    str\n        Segmentation type (RTSTRUCT or SEG)\n        \n    Raises\n    ------\n    ValueError\n        If the imgFileListPath does not end in .csv\n    RuntimeError\n        If no suitable segmentation type is found in the dataset\n    \"\"\"\n    # Check that imgFileListPath is a csv file to properly be loaded in\n    if not imgFileListPath.endswith(\".csv\"):\n        raise ValueError(\n            \"This function expects to load in a .csv file, so imgFileListPath must end in .csv\"\n        )\n\n    # Load in complete list of patient image directories of all modalities (output from med-imagetools crawl)\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n\n    # Get list of unique modalities\n    modalities = list(fullDicomList[\"modality\"].unique())\n    logger.debug(f\"Modalities found: {modalities}\")\n    \n    if \"RTSTRUCT\" in modalities:\n        segType = \"RTSTRUCT\"\n    elif \"SEG\" in modalities:\n        segType = \"SEG\"\n    else:\n        raise RuntimeError(\n            \"No suitable segmentation type found. READII can only use RTSTRUCTs and DICOM-SEG segmentations.\"\n        )\n\n    return segType\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `createImageMetadataFile` function and how does it handle different segmentation types?",
        "answer": "The `createImageMetadataFile` function creates or updates a metadata file for CT images and their corresponding segmentations. It handles two segmentation types: RTSTRUCT and SEG. For RTSTRUCT, it calls `getCTWithSegmentation`, and for SEG, it calls `matchCTtoSegmentation`. The function checks if the file already exists and respects an update flag. It returns the path to the created or existing metadata file."
      },
      {
        "question": "Explain the purpose and functionality of the `saveDataframeCSV` function. What error handling does it include?",
        "answer": "The `saveDataframeCSV` function saves a pandas DataFrame as a CSV file without the index. It includes error handling for: 1) Ensuring the output file path ends with '.csv', 2) Verifying that the input is a pandas DataFrame, 3) Creating any missing directories in the output path, and 4) Catching and re-raising any exceptions that occur during the save operation with a custom error message. It also uses a try-except-else block for proper exception handling."
      },
      {
        "question": "How does the `getSegmentationType` function determine the segmentation type, and what types does it support?",
        "answer": "The `getSegmentationType` function determines the segmentation type by analyzing the 'modality' column in a CSV file containing image metadata. It supports two types of segmentations: RTSTRUCT and SEG. The function first checks for 'RTSTRUCT' in the unique modalities list, and if not found, it checks for 'SEG'. If neither is found, it raises a RuntimeError. The function returns a Literal type of either 'RTSTRUCT' or 'SEG'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def getSegmentationType(imgFileListPath: str) -> Literal['RTSTRUCT', 'SEG']:\n    if not imgFileListPath.endswith('.csv'):\n        raise ValueError('This function expects to load in a .csv file, so imgFileListPath must end in .csv')\n\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n    modalities = list(fullDicomList['modality'].unique())\n    logger.debug(f'Modalities found: {modalities}')\n\n    # Complete the function by checking for RTSTRUCT or SEG in modalities\n    # and returning the appropriate segmentation type\n    # Raise a RuntimeError if no suitable segmentation type is found",
        "complete": "def getSegmentationType(imgFileListPath: str) -> Literal['RTSTRUCT', 'SEG']:\n    if not imgFileListPath.endswith('.csv'):\n        raise ValueError('This function expects to load in a .csv file, so imgFileListPath must end in .csv')\n\n    fullDicomList: pd.DataFrame = pd.read_csv(imgFileListPath, index_col=0)\n    modalities = list(fullDicomList['modality'].unique())\n    logger.debug(f'Modalities found: {modalities}')\n\n    if 'RTSTRUCT' in modalities:\n        return 'RTSTRUCT'\n    elif 'SEG' in modalities:\n        return 'SEG'\n    else:\n        raise RuntimeError('No suitable segmentation type found. READII can only use RTSTRUCTs and DICOM-SEG segmentations.')"
      },
      {
        "partial": "def saveDataframeCSV(dataframe: pd.DataFrame, outputFilePath: str) -> None:\n    if not outputFilePath.endswith('.csv'):\n        raise ValueError('This function saves .csv files, so outputFilePath must end in .csv')\n\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError('Function expects a pandas DataFrame to save out.')\n\n    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n\n    # Complete the function by saving the dataframe to the specified path\n    # Handle any exceptions that may occur during the save operation",
        "complete": "def saveDataframeCSV(dataframe: pd.DataFrame, outputFilePath: str) -> None:\n    if not outputFilePath.endswith('.csv'):\n        raise ValueError('This function saves .csv files, so outputFilePath must end in .csv')\n\n    if not isinstance(dataframe, pd.DataFrame):\n        raise ValueError('Function expects a pandas DataFrame to save out.')\n\n    os.makedirs(os.path.dirname(outputFilePath), exist_ok=True)\n\n    try:\n        dataframe.to_csv(outputFilePath, index=False)\n    except Exception as e:\n        error_msg = f'An error occurred while saving the DataFrame: {str(e)}'\n        raise ValueError(error_msg) from e"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pandas"
      ],
      "from_imports": [
        "typing.Optional",
        "readii.utils.get_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/report.py",
    "language": "py",
    "content": "",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `isValidParentheses` function in the given code snippet?",
        "answer": "The `isValidParentheses` function checks if a given string of parentheses is valid. It ensures that every opening parenthesis has a corresponding closing parenthesis in the correct order."
      },
      {
        "question": "How does the function handle different types of parentheses (e.g., '(', '[', '{') in the input string?",
        "answer": "The function uses a dictionary `pairs` to map closing parentheses to their corresponding opening parentheses. This allows it to handle multiple types of parentheses, including round '()', square '[]', and curly '{}' brackets."
      },
      {
        "question": "What data structure is used in the `isValidParentheses` function to keep track of opening parentheses, and why is it appropriate for this task?",
        "answer": "The function uses a stack data structure (implemented as a list in Python) to keep track of opening parentheses. This is appropriate because a stack follows the Last-In-First-Out (LIFO) principle, which matches the nested nature of parentheses where the most recently opened parenthesis should be closed first."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            # Complete the code here\n    return -1",
        "complete": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1"
      },
      {
        "partial": "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    # Complete the code here",
        "complete": "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_view.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\n\n\ntest_that(\"AnnotationGx:::.get_all_heading_types\", {\n  res <- AnnotationGx:::.get_all_heading_types()\n  checkmate::expect_data_table(res, min.rows = 1, min.cols = 2, any.missing = FALSE)\n  checkmate::expect_names(names(res), must.include = c(\"Heading\", \"Type\"))\n})\n\n\ntest_that(\"AnnotationGx::getPubchemAnnotationHeadings\", {\n  query <- getPubchemAnnotationHeadings(\"compound\", \"ChEMBL ID\")\n  expect_data_table(query, ncols = 2, nrows = 1)\n  expect_equal(names(query), c(\"Heading\", \"Type\"))\n\n  dt <- capture.output(\n    query <- capture.output(getPubchemAnnotationHeadings(\"compound\", \"fake_placeholder\"), type = c(\"message\"))\n  )\n  assert(any(grepl(\"WARNING\", query)))\n  expect_equal(dt, \"Empty data.table (0 rows and 2 cols): Heading,Type\")\n})\n\ntest_that(\"AnnotationGx::getAnotationHeadings Failure\", {\n  expect_error(getPubchemAnnotationHeadings(\"substance\", \"ChEMBL ID\"))\n})\n\n\n\ntest_that(\"AnnotationGx::annotatePubchemCompound\", {\n  CID <- 176870 # Erlotonib\n  expected <- \"CHEMBL553\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"183321-74-6\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  query <- annotatePubchemCompound(CID, \"ChEMBL ID\", query_only=T)\n  expect_class(query[[1]], \"httr2_request\")\n\n  response <- annotatePubchemCompound(CID, \"ChEMBL ID\", raw=T)\n  expect_class(response[[1]], \"httr2_response\")\n\n  expected <- NA_character_\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"L01EB02\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT01214\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n\n  CID <- 3672 # Ibuprofen\n  expected <- \"CHEMBL521\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"15687-27-1\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  expected <- \"NSC 757073; NSC 256857\"\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"M02AA13; C01EB16; R02AX02; G02CC01; M01AE01\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT00199\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\"))\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\", parse_function = fake_parser))\n  \n  fake_parser <- function(x) {\n    return(data.table::data.table(Heading = \"CAS\", Value = \"fake_value\"))\n  }\n  annotatePubchemCompound(CID, heading = \"CAS\", parse_function = fake_parser)\n})\n\ntest_that(\"AnnotationGx:::.build_pubchem_view_query\", {\n  # Test case 1: Test with default parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(id = \"12345\")\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/12345/JSON\"\n  expect_equal(query$url, expected_url)\n\n  # Test case 2: Test with custom parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\", page = 2\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/67890/JSON?page=2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", heading = \"ChEMBL ID\", output = \"XML\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/XML?heading=ChEMBL%20ID\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", output = \"JSON\", source = \"DrugBank\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?source=DrugBank\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"substance\", version = \"1.2\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/176870/JSON?version=1.2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", version = 1\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?version=1\"\n  expect_equal(query$url, expected_url)\n})\n\n\ntest_that(\"AnnotationGx:::.build_pubchem_view_query Failure\", {\n  # Test case 1: Test with invalid annotation\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\",\n    page = 2, version = 1, heading = \"Heading1\", source = \"Source1\", output = \"XML\"\n  ))\n  expect_error(AnnotationGx:::.build_pubchem_view_query(id = \"67890\", record = \"substance\", version = 1.5))\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"substance\", version = 1\n  ))\n\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", output = \"JSON\", source = \"\"\n  ))\n\n  expect_error(AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"compound\", heading = \"fale\"\n  ))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `AnnotationGx:::.get_all_heading_types` function and how is it tested?",
        "answer": "The `AnnotationGx:::.get_all_heading_types` function likely retrieves all available heading types for PubChem annotations. It's tested using `testthat` to ensure it returns a data table with at least one row and two columns, containing 'Heading' and 'Type' columns without any missing values."
      },
      {
        "question": "How does the `annotatePubchemCompound` function handle different types of annotations, and what are some examples of its usage?",
        "answer": "The `annotatePubchemCompound` function retrieves specific annotations for a given PubChem Compound ID (CID). It can handle various annotation types such as 'ChEMBL ID', 'CAS', 'NSC Number', 'ATC Code', and 'Drug Induced Liver Injury'. The function is tested with two compounds (Erlotinib and Ibuprofen) and various annotation types. It also supports options like `query_only` to return the request object and `raw` to return the raw response."
      },
      {
        "question": "What is the purpose of the `AnnotationGx:::.build_pubchem_view_query` function and how does it handle different parameters?",
        "answer": "The `AnnotationGx:::.build_pubchem_view_query` function constructs a URL for querying the PubChem PUG View API. It can handle various parameters such as `id`, `record` type (compound or substance), `page`, `heading`, `output` format, `source`, and API `version`. The function builds the appropriate URL based on these parameters, encoding them correctly in the query string. It also includes error checking for invalid parameter combinations or values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"AnnotationGx:::.build_pubchem_view_query\", {\n  # Test case 1: Test with default parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(id = \"12345\")\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/12345/JSON\"\n  expect_equal(query$url, expected_url)\n\n  # Test case 2: Test with custom parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\", page = 2\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/67890/JSON?page=2\"\n  expect_equal(query$url, expected_url)\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"AnnotationGx:::.build_pubchem_view_query\", {\n  # Test case 1: Test with default parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(id = \"12345\")\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/12345/JSON\"\n  expect_equal(query$url, expected_url)\n\n  # Test case 2: Test with custom parameters\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"67890\", record = \"substance\", page = 2\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/67890/JSON?page=2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", heading = \"ChEMBL ID\", output = \"XML\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/XML?heading=ChEMBL%20ID\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", output = \"JSON\", source = \"DrugBank\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?source=DrugBank\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", record = \"substance\", version = \"1.2\"\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/substance/176870/JSON?version=1.2\"\n  expect_equal(query$url, expected_url)\n\n  query <- AnnotationGx:::.build_pubchem_view_query(\n    id = \"176870\", version = 1\n  )\n  expected_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view/data/compound/176870/JSON?version=1\"\n  expect_equal(query$url, expected_url)\n})"
      },
      {
        "partial": "test_that(\"AnnotationGx::annotatePubchemCompound\", {\n  CID <- 176870 # Erlotonib\n  expected <- \"CHEMBL553\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"183321-74-6\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  query <- annotatePubchemCompound(CID, \"ChEMBL ID\", query_only=T)\n  expect_class(query[[1]], \"httr2_request\")\n\n  response <- annotatePubchemCompound(CID, \"ChEMBL ID\", raw=T)\n  expect_class(response[[1]], \"httr2_response\")\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"AnnotationGx::annotatePubchemCompound\", {\n  CID <- 176870 # Erlotonib\n  expected <- \"CHEMBL553\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"183321-74-6\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  query <- annotatePubchemCompound(CID, \"ChEMBL ID\", query_only=T)\n  expect_class(query[[1]], \"httr2_request\")\n\n  response <- annotatePubchemCompound(CID, \"ChEMBL ID\", raw=T)\n  expect_class(response[[1]], \"httr2_response\")\n\n  expected <- NA_character_\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"L01EB02\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT01214\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n  CID <- 3672 # Ibuprofen\n  expected <- \"CHEMBL521\"\n  expect_equal(annotatePubchemCompound(CID, \"ChEMBL ID\"), expected)\n\n  expected <- \"15687-27-1\"\n  expect_equal(annotatePubchemCompound(CID, \"CAS\"), expected)\n\n  expected <- \"NSC 757073; NSC 256857\"\n  expect_equal(annotatePubchemCompound(CID, \"NSC Number\"), expected)\n\n  expected <- \"M02AA13; C01EB16; R02AX02; G02CC01; M01AE01\"\n  expect_equal(annotatePubchemCompound(CID, \"ATC Code\"), expected)\n\n  expected <- \"LT00199\"\n  expect_equal(annotatePubchemCompound(CID, \"Drug Induced Liver Injury\"), expected)\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\"))\n\n  expect_error(annotatePubchemCompound(CID, heading = \"fake_placeholder\", parse_function = fake_parser))\n  \n  fake_parser <- function(x) {\n    return(data.table::data.table(Heading = \"CAS\", Value = \"fake_value\"))\n  }\n  annotatePubchemCompound(CID, heading = \"CAS\", parse_function = fake_parser)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/nifti_to_dicom.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nfrom rt_utils import RTStructBuilder\nimport logging\nimport numpy as np\nimport os\n\n'''You need to install rt_utils first using \n    pip install rt_utils'''\n\n\ndef segmentations_to_dicom(save_path:str,orig_series_info:dict, segmentation:sitk.Image,segmentations_labels:dict,\n                             color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    \"\"\"\n    This function takes a list of the original dicom data, moves it to the save_path and creates a corresponding rt_struct\n    @param image_set_name: name of the image set used to create the dicom sub directory\n    @param save_path: directory to save dicom data\n    @param orig_series_info: original series info see @get_dicom_series_dict_pyd\n    @param segmentation: the image containing integer segmentation data\n    @param segmentations_labels: a dictionary with segmentation label information mapping ints in @segmentation to label\n    name e.g. {1: 'Bladder', 2: 'Rectum', 3: 'Sigmoid', 4: 'SmallBowel'}\n    @return:\n    \"\"\"\n    _logger=logging.getLogger(__name__)\n    #make output dir\n    if not os.path.isdir(save_path):\n        _logger.warning(f'Making path: {save_path}')\n        os.makedirs(save_path)\n\n\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i,(key,name) in enumerate(segmentations_labels.items()):\n        mask = np.where(im_array!=key,0,im_array)\n        mask = np.array(mask,dtype=bool)\n        mask = np.transpose(mask, (1,2,0))\n\n        index = i%len(color_list)\n\n        rtstruct.add_roi(mask=mask, name=name, color=color_list[index],approximate_contours=False)\n\n\n    rtstruct_name = os.path.join(save_path,rtstruct_basefilename)\n    _logger.info(f'Saving rtstruct data: {rtstruct_name}')\n    rtstruct.save(rtstruct_name)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `segmentations_to_dicom` function, and what are its main input parameters?",
        "answer": "The `segmentations_to_dicom` function is designed to create a DICOM RT-Structure file from segmentation data. Its main input parameters are:\n1. `save_path`: directory to save the DICOM data\n2. `orig_series_info`: original series information\n3. `segmentation`: a SimpleITK Image containing integer segmentation data\n4. `segmentations_labels`: a dictionary mapping integer labels to segmentation names\n5. `color_list`: an optional list of colors for the segmentations\n6. `rtstruct_basefilename`: the filename for the output RT-Structure file"
      },
      {
        "question": "How does the function handle the conversion of the segmentation data to a format suitable for the RT-Structure file?",
        "answer": "The function converts the segmentation data to a format suitable for the RT-Structure file through these steps:\n1. It extracts the numpy array from the SimpleITK Image using `sitk.GetArrayFromImage(segmentation)`.\n2. For each label in the `segmentations_labels` dictionary:\n   a. It creates a binary mask for the current label using numpy operations.\n   b. The mask is transposed to match the expected dimensions (x,y,z).\n   c. The mask is added to the RT-Structure using `rtstruct.add_roi()`, along with the label name and a color from the `color_list`.\n3. Finally, it saves the RT-Structure file using `rtstruct.save()`."
      },
      {
        "question": "What external libraries does this code rely on, and what are their roles in the function?",
        "answer": "The code relies on several external libraries:\n1. SimpleITK (imported as sitk): Used for handling medical imaging data, particularly for reading and writing DICOM files.\n2. rt_utils: Provides the RTStructBuilder class, which is used to create and manipulate RT-Structure files.\n3. numpy (imported as np): Used for array operations on the segmentation data.\n4. os: Used for file and directory operations.\n5. logging: Used for logging information and warnings during the execution of the function.\n\nThese libraries work together to process the segmentation data, create the RT-Structure file, and handle file operations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def segmentations_to_dicom(save_path:str, orig_series_info:dict, segmentation:sitk.Image, segmentations_labels:dict, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    if not os.path.isdir(save_path):\n        os.makedirs(save_path)\n\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i, (key, name) in enumerate(segmentations_labels.items()):\n        # Complete the code to create and add ROI to rtstruct\n\n    rtstruct_name = os.path.join(save_path, rtstruct_basefilename)\n    rtstruct.save(rtstruct_name)",
        "complete": "def segmentations_to_dicom(save_path:str, orig_series_info:dict, segmentation:sitk.Image, segmentations_labels:dict, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    if not os.path.isdir(save_path):\n        os.makedirs(save_path)\n\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i, (key, name) in enumerate(segmentations_labels.items()):\n        mask = np.where(im_array != key, 0, im_array)\n        mask = np.array(mask, dtype=bool)\n        mask = np.transpose(mask, (1, 2, 0))\n        index = i % len(color_list)\n        rtstruct.add_roi(mask=mask, name=name, color=color_list[index], approximate_contours=False)\n\n    rtstruct_name = os.path.join(save_path, rtstruct_basefilename)\n    rtstruct.save(rtstruct_name)"
      },
      {
        "partial": "import SimpleITK as sitk\nfrom rt_utils import RTStructBuilder\nimport numpy as np\nimport os\n\ndef segmentations_to_dicom(save_path, orig_series_info, segmentation, segmentations_labels, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    # Complete the function implementation\n    pass",
        "complete": "import SimpleITK as sitk\nfrom rt_utils import RTStructBuilder\nimport numpy as np\nimport os\n\ndef segmentations_to_dicom(save_path, orig_series_info, segmentation, segmentations_labels, color_list=None, rtstruct_basefilename='rtstruct.dcm'):\n    os.makedirs(save_path, exist_ok=True)\n    im_array = sitk.GetArrayFromImage(segmentation)\n    rtstruct = RTStructBuilder.create_new(dicom_series_path=orig_series_info)\n    for i, (key, name) in enumerate(segmentations_labels.items()):\n        mask = (im_array == key).astype(bool)\n        mask = np.transpose(mask, (1, 2, 0))\n        index = i % len(color_list)\n        rtstruct.add_roi(mask=mask, name=name, color=color_list[index], approximate_contours=False)\n    rtstruct.save(os.path.join(save_path, rtstruct_basefilename))"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "logging",
        "numpy",
        "os"
      ],
      "from_imports": [
        "rt_utils.RTStructBuilder"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_unichem.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"getUnichemSources returns a data.table with the correct columns\", {\n  sources <- getUnichemSources(all_columns = TRUE)\n  \n  expected_columns <- c(\n    \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n    \"BaseURL\", \"URL\", \"Details\", \"Description\", \"ReleaseNumber\", \n    \"ReleaseDate\", \"LastUpdated\", \"UpdateComments\"\n  )\n  \n  expect_data_table(\n    sources,\n    all.missing = FALSE,\n    min.rows = 40, # As of March 2024\n    min.cols = 13, # As of March 2024\n    col.names = 'named',\n    info = \"The data.table should have the correct columns. \n        The min number of rows and columns may change over time and is set on\n        from UniChem as of March 2024.\",\n    )\n})\n\n\ntest_that(\"queryUnichemCompound returns the expected results\", {\n  # Test case 1\n  result1 <- queryUnichemCompound(type = \"sourceID\", compound = \"444795\", sourceID = 22)\n  expect_true(is.list(result1))\n  expect_true(\"External_Mappings\" %in% names(result1))\n  expect_true(\"UniChem_Mappings\" %in% names(result1))\n  \n  # Test case 2\n  expect_error(queryUnichemCompound(type = \"inchikey\", compound = \"InchiKey123\"))\n\n})\n\ntest_that(\"queryUnichemCompound returns the expected results 2\", {\n  # Test case 1\n  result1 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = T)\n\n  expect_true(is.list(result1))\n\n\n  checkmate::expect_names(\n    names(result1), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(result1$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n\n  result2 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = F)\n\n  expect_true(is.list(result2))\n\n  checkmate::expect_names(\n    names(result2$External_Mappings),\n    subset.of = c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n  )\n\n  checkmate::expect_names(\n    names(result2$UniChem_Mappings),\n    subset.of = c(\n      \"UniChem.UCI\", \"UniChem.InchiKey\", 'UniChem.Inchi',\n      'UniChem.formula','UniChem.connections','UniChem.hAtoms'\n    )\n  )\n\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getUnichemSources` function and what are the key expectations in its test?",
        "answer": "The `getUnichemSources` function retrieves information about UniChem data sources. The test expects it to return a data.table with specific columns, including 'Name', 'SourceID', 'CompoundCount', etc. It should have at least 40 rows and 13 columns as of March 2024, though these numbers may change over time. The test uses `expect_data_table` to verify the structure and content of the returned data."
      },
      {
        "question": "How does the `queryUnichemCompound` function behave with different input types, and what are the expected outputs?",
        "answer": "The `queryUnichemCompound` function accepts different input types and returns varying results. When called with `type = 'sourceID'` and valid compound and sourceID, it returns a list containing 'External_Mappings' and 'UniChem_Mappings'. When called with `type = 'inchikey'` and `raw = TRUE`, it returns a list with keys like 'compounds', 'notFound', 'response', and 'totalCompounds'. With `raw = FALSE`, it returns a list with 'External_Mappings' and 'UniChem_Mappings', each containing specific subfields. The function throws an error for invalid inputs, such as an incorrect InChIKey."
      },
      {
        "question": "What testing libraries are used in this code, and how are they applied to ensure the correctness of the UniChem-related functions?",
        "answer": "The code uses three testing libraries: testthat, AnnotationGx, and checkmate. testthat is the primary testing framework, providing the `test_that` function to group related tests. AnnotationGx likely contains the UniChem-related functions being tested. checkmate is used for additional assertions, particularly `expect_names` to verify the structure of returned objects. These libraries are applied to test the behavior of `getUnichemSources` and `queryUnichemCompound` functions, checking for correct return types, expected data structures, and proper error handling."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"getUnichemSources returns a data.table with the correct columns\", {\n  sources <- getUnichemSources(all_columns = TRUE)\n  \n  expected_columns <- c(\n    \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n    \"BaseURL\", \"URL\", \"Details\", \"Description\", \"ReleaseNumber\", \n    \"ReleaseDate\", \"LastUpdated\", \"UpdateComments\"\n  )\n  \n  expect_data_table(\n    sources,\n    all.missing = FALSE,\n    min.rows = 40,\n    min.cols = 13,\n    col.names = 'named',\n    info = \"The data.table should have the correct columns. \n        The min number of rows and columns may change over time and is set on\n        from UniChem as of March 2024.\",\n    )\n})",
        "complete": "test_that(\"getUnichemSources returns a data.table with the correct columns\", {\n  sources <- getUnichemSources(all_columns = TRUE)\n  \n  expected_columns <- c(\n    \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n    \"BaseURL\", \"URL\", \"Details\", \"Description\", \"ReleaseNumber\", \n    \"ReleaseDate\", \"LastUpdated\", \"UpdateComments\"\n  )\n  \n  expect_data_table(\n    sources,\n    all.missing = FALSE,\n    min.rows = 40,\n    min.cols = 13,\n    col.names = 'named',\n    info = \"The data.table should have the correct columns. \n        The min number of rows and columns may change over time and is set on\n        from UniChem as of March 2024.\",\n  )\n  expect_true(all(expected_columns %in% colnames(sources)))\n})"
      },
      {
        "partial": "test_that(\"queryUnichemCompound returns the expected results 2\", {\n  result1 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = T)\n\n  expect_true(is.list(result1))\n\n  checkmate::expect_names(\n    names(result1), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(result1$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n\n  result2 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = F)\n\n  expect_true(is.list(result2))\n\n  # Add assertions for result2 here\n})",
        "complete": "test_that(\"queryUnichemCompound returns the expected results 2\", {\n  result1 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = T)\n\n  expect_true(is.list(result1))\n\n  checkmate::expect_names(\n    names(result1), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(result1$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n\n  result2 <- queryUnichemCompound(type = \"inchikey\", compound = \"BSYNRYMUTXBXSQ-UHFFFAOYSA-N\", raw = F)\n\n  expect_true(is.list(result2))\n\n  checkmate::expect_names(\n    names(result2$External_Mappings),\n    subset.of = c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n  )\n\n  checkmate::expect_names(\n    names(result2$UniChem_Mappings),\n    subset.of = c(\n      \"UniChem.UCI\", \"UniChem.InchiKey\", 'UniChem.Inchi',\n      'UniChem.formula','UniChem.connections','UniChem.hAtoms'\n    )\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-httr2.R",
    "language": "R",
    "content": "#' Builds a URL by concatenating the input arguments and encoding it.\n#'\n#' @param ... The components of the URL.\n#' @return The encoded URL.\n#' @noRd\n#' @keywords internal\n.buildURL <- function(...) {\n  paste0(stats::na.omit(unlist(list(...))), collapse = \"/\") |> utils::URLencode()\n}\n\n#' Builds an HTTP request using the provided URL.\n#'\n#' @param url The URL for the request.\n#' @return The built HTTP request.\n#' @noRd\n#' @keywords internal\n.build_request <- function(url) {\n  httr2::request(url) |>\n    httr2::req_retry(max_tries = 5, backoff = ~ 10) |>\n    httr2::req_error(is_error = \\(resp) FALSE)\n}\n\n#' Performs an HTTP request.\n#'\n#' @param request The HTTP request to perform.\n#' @return The response of the HTTP request.\n#' @noRd\n#' @keywords internal\n.perform_request <- function(request) {\n  httr2::req_perform(request)\n}\n\n#' Performs an HTTP request in parallel.\n#' @param reqs The HTTP requests to perform.\n#' @param on_error The action to take when an error occurs. Can be \"stop\" or \"continue\".\n#' @param progress Whether to show a progress bar.\n#'\n#' @return The responses of the HTTP requests.\n#'\n#' @noRd\n#' @keywords internal\n.perform_request_parallel <- function(reqs, on_error = \"continue\", progress = TRUE, ...) {\n  httr2::req_perform_parallel(reqs, on_error = on_error, progress = progress, ...)\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @param resp The response object from the HTTP request.\n#' @return The parsed JSON response.\n#' @noRd\n#' @keywords internal\n.parse_resp_json <- function(resp, simplifyVector = TRUE) {\n  httr2::resp_body_json(resp, simplifyVector = simplifyVector)\n}\n\n\n#' Parses the TSV response from an HTTP request.\n#' @param resp The response object from the HTTP request.\n#' @return The parsed TSV response.\n#' @noRd\n#' @keywords internal\n.parse_resp_tsv <- function(resp, show_col_types = FALSE, skip = 0) {\n  readr::read_tsv(resp$body, skip = skip, show_col_types = show_col_types)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.buildURL` function and how does it handle its input?",
        "answer": "The `.buildURL` function is designed to construct a URL by concatenating its input arguments and encoding the result. It takes any number of arguments (`...`), combines them into a single URL string using '/' as a separator, and then URL-encodes the result. The function uses `stats::na.omit` and `unlist` to handle potential NA values and nested lists in the input, ensuring a clean URL string."
      },
      {
        "question": "How does the `.build_request` function enhance the reliability of HTTP requests?",
        "answer": "The `.build_request` function enhances the reliability of HTTP requests in two ways: 1) It implements a retry mechanism using `httr2::req_retry`, allowing up to 5 attempts with a backoff strategy that increases the wait time between retries. 2) It modifies the error handling behavior with `httr2::req_error(is_error = \\(resp) FALSE)`, which prevents the request from automatically throwing an error on HTTP status codes typically considered as errors. This allows for more flexible error handling in the calling code."
      },
      {
        "question": "What are the key differences between `.perform_request` and `.perform_request_parallel`, and when would you use each?",
        "answer": "`.perform_request` is used for sending a single HTTP request synchronously, while `.perform_request_parallel` is designed for sending multiple requests concurrently. The parallel version takes an array of requests, has options for error handling (`on_error`) and displaying a progress bar. You would use `.perform_request` for simple, one-off API calls, and `.perform_request_parallel` when you need to make multiple API calls efficiently, such as batch processing or when dealing with paginated API responses."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Builds a URL by concatenating the input arguments and encoding it.\n#'\n#' @param ... The components of the URL.\n#' @return The encoded URL.\n#' @noRd\n#' @keywords internal\n.buildURL <- function(...) {\n  # Complete the function body\n}",
        "complete": "#' Builds a URL by concatenating the input arguments and encoding it.\n#'\n#' @param ... The components of the URL.\n#' @return The encoded URL.\n#' @noRd\n#' @keywords internal\n.buildURL <- function(...) {\n  paste0(stats::na.omit(unlist(list(...))), collapse = \"/\") |> utils::URLencode()\n}"
      },
      {
        "partial": "#' Performs an HTTP request in parallel.\n#' @param reqs The HTTP requests to perform.\n#' @param on_error The action to take when an error occurs. Can be \"stop\" or \"continue\".\n#' @param progress Whether to show a progress bar.\n#'\n#' @return The responses of the HTTP requests.\n#'\n#' @noRd\n#' @keywords internal\n.perform_request_parallel <- function(reqs, on_error = \"continue\", progress = TRUE, ...) {\n  # Complete the function body\n}",
        "complete": "#' Performs an HTTP request in parallel.\n#' @param reqs The HTTP requests to perform.\n#' @param on_error The action to take when an error occurs. Can be \"stop\" or \"continue\".\n#' @param progress Whether to show a progress bar.\n#'\n#' @return The responses of the HTTP requests.\n#'\n#' @noRd\n#' @keywords internal\n.perform_request_parallel <- function(reqs, on_error = \"continue\", progress = TRUE, ...) {\n  httr2::req_perform_parallel(reqs, on_error = on_error, progress = progress, ...)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeSlope.R",
    "language": "R",
    "content": "#' Return Slope (normalized slope of the drug response curve) for an experiment of a pSet by taking\n#' its concentration and viability as input.\n#'\n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeSlope(dose, viability)\n#'\n#' @param concentration `numeric` A concentration range that the AUC should be computed for that range.\n#' Concentration range by default considered as not logarithmic scaled. Converted to numeric by function if necessary.\n#' @param viability `numeric` Viablities corresponding to the concentration range passed as first parameter.\n#' The range of viablity values by definition should be between 0 and 100. But the viabalities greater than\n#' 100 and lower than 0 are also accepted.\n#' @param trunc `logical(1)` A flag that identify if the viabality values should be truncated to be in the\n#' range of (0,100)\n#' @param verbose `logical(1)` If 'TRUE' the function will retrun warnings and other infomrative messages.\n#' @return Returns the normalized linear slope of the drug response curve\n#'\n#' @export\ncomputeSlope <- function(concentration, viability, trunc=TRUE, verbose=TRUE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n  ##convert to nanomolar with the assumption that always concentrations are in micro molar\n  concentration <- concentration\n  concentration <- log10(concentration) + 6\n  if(trunc) {\n    viability <- pmin(viability, 100)\n    viability <- pmax(viability, 0)\n  }\n\n  most.sensitive <- NULL\n  for(dose in concentration)\n  {\n    most.sensitive <- rbind(most.sensitive, cbind(dose,0))\n  }\n\n  slope.prime <- .optimizeRegression(x = most.sensitive[,1], y = most.sensitive[,2])\n  slope <- .optimizeRegression(x = concentration, y = viability)\n  slope <- round(slope/abs(slope.prime),digits=2)\n  return(-slope)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeSlope` function and what are its main input parameters?",
        "answer": "The `computeSlope` function calculates the normalized slope of a drug response curve. Its main input parameters are `concentration` (a numeric vector of drug concentrations) and `viability` (a numeric vector of corresponding cell viabilities). It also has optional parameters `trunc` (to truncate viability values) and `verbose` (for displaying messages)."
      },
      {
        "question": "How does the function handle concentration values of zero, and why is this important?",
        "answer": "The function removes concentration values of zero and their corresponding viability values using the line `if(length(ii) > 0) { concentration <- concentration[-ii]; viability <- viability[-ii] }`. This is important because log10(0) is undefined, and the function later applies log10 to the concentration values. Removing zero values prevents errors in the calculation."
      },
      {
        "question": "What is the purpose of the `most.sensitive` variable in the `computeSlope` function, and how is it used?",
        "answer": "The `most.sensitive` variable creates a reference curve representing the most sensitive possible drug response. It's a matrix where the first column is the log-transformed concentrations, and the second column is all zeros (representing complete cell death). This reference curve is used to normalize the actual slope by comparing it to the slope of this most sensitive case, which is calculated using `.optimizeRegression(x = most.sensitive[,1], y = most.sensitive[,2])`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeSlope <- function(concentration, viability, trunc=TRUE, verbose=TRUE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n  concentration <- log10(concentration) + 6\n  if(trunc) {\n    viability <- pmin(viability, 100)\n    viability <- pmax(viability, 0)\n  }\n\n  # Complete the code to calculate and return the slope\n}",
        "complete": "computeSlope <- function(concentration, viability, trunc=TRUE, verbose=TRUE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n  concentration <- log10(concentration) + 6\n  if(trunc) {\n    viability <- pmin(viability, 100)\n    viability <- pmax(viability, 0)\n  }\n\n  most.sensitive <- matrix(c(concentration, rep(0, length(concentration))), ncol=2)\n  slope.prime <- .optimizeRegression(x = most.sensitive[,1], y = most.sensitive[,2])\n  slope <- .optimizeRegression(x = concentration, y = viability)\n  slope <- round(slope/abs(slope.prime), digits=2)\n  return(-slope)\n}"
      },
      {
        "partial": ".optimizeRegression <- function(x, y) {\n  # Implement the regression optimization function\n}",
        "complete": ".optimizeRegression <- function(x, y) {\n  fit <- lm(y ~ x)\n  return(coef(fit)[2])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/crawl.py",
    "language": "py",
    "content": "from argparse import ArgumentParser\nimport os\nimport pathlib\nimport glob\nimport json\nimport pandas as pd\nfrom pydicom import dcmread\nfrom tqdm import tqdm\nfrom joblib import Parallel, delayed\n\n\ndef crawl_one(folder):\n    folder_path = pathlib.Path(folder)\n    database = {}\n    for path, _, _ in os.walk(folder):\n        # find dicoms\n        dicoms = glob.glob(pathlib.Path(path, \"**\", \"*.dcm\").as_posix(), recursive=True)\n        # print('\\n', folder, dicoms)\n        # instance (slice) information\n        for dcm in dicoms:\n            try:\n                dcm_path  = pathlib.Path(dcm)\n                # parent    = dcm_path.parent#.as_posix()\n                fname     = dcm_path.name\n                rel_path  = dcm_path.relative_to(folder_path.parent.parent)                                        # rel_path of dicom from folder\n                rel_posix = rel_path.parent.as_posix()     # folder name + until parent folder of dicom\n\n                meta      = dcmread(dcm, force=True, stop_before_pixels=True)\n                patient   = str(meta.PatientID)\n                study     = str(meta.StudyInstanceUID)\n                series    = str(meta.SeriesInstanceUID)\n                instance  = str(meta.SOPInstanceUID)\n\n                reference_ct, reference_rs, reference_pl,  = \"\", \"\", \"\"\n                tr, te, tesla, scan_seq, elem = \"\", \"\", \"\", \"\", \"\"\n                try:\n                    orientation = str(meta.ImageOrientationPatient)  # (0020, 0037)\n                except:\n                    orientation = \"\"\n\n                try:\n                    orientation_type = str(meta.AnatomicalOrientationType)  # (0010, 2210)\n                except:\n                    orientation_type = \"\"\n\n                try:  # RTSTRUCT\n                    reference_ct = str(meta.ReferencedFrameOfReferenceSequence[0].RTReferencedStudySequence[0].RTReferencedSeriesSequence[0].SeriesInstanceUID)\n                except: \n                    try: # SEGMENTATION\n                        reference_ct = str(meta.ReferencedSeriesSequence[0].SeriesInstanceUID)\n                    except:\n                        try:  # RTDOSE\n                            reference_rs = str(meta.ReferencedStructureSetSequence[0].ReferencedSOPInstanceUID)\n                        except:\n                            pass\n                        try:\n                            reference_ct = str(meta.ReferencedImageSequence[0].ReferencedSOPInstanceUID)\n                        except:\n                            pass\n                        try:\n                            reference_pl = str(meta.ReferencedRTPlanSequence[0].ReferencedSOPInstanceUID)\n                        except:\n                            pass\n                \n                # MRI Tags\n                try:\n                    tr = float(meta.RepetitionTime)\n                except:\n                    pass\n                try:\n                    te = float(meta.EchoTime)\n                except:\n                    pass\n                try:\n                    scan_seq = str(meta.ScanningSequence)\n                except:\n                    pass\n                try:\n                    tesla = float(meta.MagneticFieldStrength)\n                except:\n                    pass\n                try:\n                    elem = str(meta.ImagedNucleus)\n                except:\n                    pass\n                \n                # Frame of Reference UIDs\n                try:\n                    reference_frame = str(meta.FrameOfReferenceUID)\n                except:\n                    try:\n                        reference_frame = str(meta.ReferencedFrameOfReferenceSequence[0].FrameOfReferenceUID)\n                    except:\n                        reference_frame = \"\"\n        \n                try:\n                    study_description = str(meta.StudyDescription)\n                except:\n                    study_description = \"\"\n\n                try:\n                    series_description = str(meta.SeriesDescription)\n                except:\n                    series_description = \"\"\n\n                try:\n                    subseries = str(meta.AcquisitionNumber)\n                except:\n                    subseries = \"default\"\n\n                if patient not in database:\n                    database[patient] = {}\n                if study not in database[patient]:\n                    database[patient][study] = {'description': study_description}\n                if series not in database[patient][study]:\n                    rel_crawl_path = rel_posix\n                    if meta.Modality == 'RTSTRUCT':\n                        rel_crawl_path = os.path.join(rel_crawl_path, fname)\n                    \n                    database[patient][study][series] = {'description': series_description}\n                if subseries not in database[patient][study][series]:\n                    database[patient][study][series][subseries] = {'instances': {},\n                                                                   'instance_uid': instance,\n                                                                   'modality': meta.Modality,\n                                                                   'reference_ct': reference_ct,\n                                                                   'reference_rs': reference_rs,\n                                                                   'reference_pl': reference_pl,\n                                                                   'reference_frame': reference_frame,\n                                                                   'folder': rel_crawl_path,\n                                                                   'orientation': orientation,\n                                                                   'orientation_type': orientation_type,\n                                                                   'repetition_time':tr,\n                                                                   'echo_time':te,\n                                                                   'scan_sequence': scan_seq,\n                                                                   'mag_field_strength': tesla,\n                                                                   'imaged_nucleus': elem,\n                                                                   'fname': rel_path.as_posix()  # temporary until we switch to json-based loading\n                                                                   }\n                database[patient][study][series][subseries]['instances'][instance] = rel_path.as_posix()\n            except Exception as e:\n                print(folder, e)\n                pass\n    \n    return database\n\n\ndef to_df(database_dict):\n    df = pd.DataFrame()\n    for pat in database_dict:\n        for study in database_dict[pat]:\n            for series in database_dict[pat][study]:\n                if series != 'description':  # skip description key in dict\n                    for subseries in database_dict[pat][study][series]:\n                        if subseries != 'description':  # skip description key in dict\n                            columns = ['patient_ID', 'study', 'study_description', \n                                       'series', 'series_description', 'subseries', 'modality', \n                                       'instances', 'instance_uid', \n                                       'reference_ct', 'reference_rs', 'reference_pl', 'reference_frame', 'folder',\n                                       'orientation', 'orientation_type', 'MR_repetition_time', 'MR_echo_time', \n                                       'MR_scan_sequence', 'MR_magnetic_field_strength', 'MR_imaged_nucleus', 'file_path']\n                            values = [pat, study, database_dict[pat][study]['description'], \n                                      series, database_dict[pat][study][series]['description'], \n                                      subseries, database_dict[pat][study][series][subseries]['modality'], \n                                      len(database_dict[pat][study][series][subseries]['instances']), database_dict[pat][study][series][subseries]['instance_uid'], \n                                      database_dict[pat][study][series][subseries]['reference_ct'], database_dict[pat][study][series][subseries]['reference_rs'], \n                                      database_dict[pat][study][series][subseries]['reference_pl'], database_dict[pat][study][series][subseries]['reference_frame'], database_dict[pat][study][series][subseries]['folder'],\n                                      database_dict[pat][study][series][subseries]['orientation'], database_dict[pat][study][series][subseries]['orientation_type'],\n                                      database_dict[pat][study][series][subseries]['repetition_time'], database_dict[pat][study][series][subseries]['echo_time'],\n                                      database_dict[pat][study][series][subseries]['scan_sequence'], database_dict[pat][study][series][subseries]['mag_field_strength'], database_dict[pat][study][series][subseries]['imaged_nucleus'],\n                                      database_dict[pat][study][series][subseries]['fname']\n                                      ]\n\n                            df_add = pd.DataFrame([values], columns=columns)\n                            df = pd.concat([df, df_add], ignore_index=True)\n    return df\n\n\ndef crawl(top, \n          n_jobs: int = -1):\n    # top is the input directory in the argument parser from autotest.py\n    database_list = []\n    folders = glob.glob(pathlib.Path(top, \"*\").as_posix())\n    \n    database_list = Parallel(n_jobs=n_jobs)(delayed(crawl_one)(pathlib.Path(top, folder).as_posix()) for folder in tqdm(folders))\n\n    # convert list to dictionary\n    database_dict = {}\n    for db in database_list:\n        for key in db:\n            database_dict[key] = db[key]\n    \n    # save one level above imaging folders\n    parent, dataset  = os.path.split(top)\n\n    parent_imgtools = pathlib.Path(parent, \".imgtools\").as_posix()\n\n    if not os.path.exists(parent_imgtools):\n        try:\n            os.makedirs(parent_imgtools)\n        except:\n            pass\n    \n    # save as json\n    with open(pathlib.Path(parent_imgtools, f'imgtools_{dataset}.json').as_posix(), 'w') as f:\n        json.dump(database_dict, f, indent=4)\n    \n    # save as dataframe\n    df = to_df(database_dict)\n    df_path = pathlib.Path(parent_imgtools, f'imgtools_{dataset}.csv').as_posix()\n    df.to_csv(df_path)\n    \n    return database_dict\n\n\nif __name__ == \"__main__\":\n    parser = ArgumentParser(\"Dataset DICOM Crawler\")\n    parser.add_argument(\"directory\",\n                        type=str,\n                        help=\"Top-level directory of the dataset.\")\n    parser.add_argument(\"--n_jobs\",\n                        type=int,\n                        default=16,\n                        help=\"Number of parallel processes for multiprocessing.\")\n\n    args = parser.parse_args()\n    db = crawl(args.directory, n_jobs=args.n_jobs)\n    print(\"# patients:\", len(db))\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `crawl_one` function in this code?",
        "answer": "The `crawl_one` function is responsible for crawling through a single folder, extracting metadata from DICOM files, and organizing the information into a nested dictionary structure. It processes various DICOM modalities, extracts relevant tags, and handles different types of references between DICOM files (e.g., CT, RTSTRUCT, RTDOSE). The function returns a dictionary containing patient, study, series, and instance-level information for all DICOM files found in the given folder."
      },
      {
        "question": "How does the code handle parallel processing of multiple folders?",
        "answer": "The code uses the `joblib` library for parallel processing. In the `crawl` function, it utilizes `Parallel` and `delayed` from joblib to apply the `crawl_one` function to multiple folders concurrently. The number of parallel jobs is controlled by the `n_jobs` parameter, which can be set by the user through the command-line argument. This approach allows for efficient processing of large datasets with multiple patient folders."
      },
      {
        "question": "What are the main outputs generated by this DICOM crawler, and where are they saved?",
        "answer": "The DICOM crawler generates two main outputs:\n1. A JSON file containing the nested dictionary structure of all DICOM metadata.\n2. A CSV file containing a flattened representation of the DICOM metadata as a pandas DataFrame.\n\nBoth outputs are saved in a hidden folder named '.imgtools' located one level above the input directory. The file names are prefixed with 'imgtools_' followed by the name of the dataset (derived from the input directory name). The JSON file is saved using `json.dump()`, while the CSV file is saved using the pandas `to_csv()` method."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def crawl_one(folder):\n    folder_path = pathlib.Path(folder)\n    database = {}\n    for path, _, _ in os.walk(folder):\n        dicoms = glob.glob(pathlib.Path(path, \"**\", \"*.dcm\").as_posix(), recursive=True)\n        for dcm in dicoms:\n            try:\n                dcm_path = pathlib.Path(dcm)\n                fname = dcm_path.name\n                rel_path = dcm_path.relative_to(folder_path.parent.parent)\n                rel_posix = rel_path.parent.as_posix()\n                meta = dcmread(dcm, force=True, stop_before_pixels=True)\n                patient = str(meta.PatientID)\n                study = str(meta.StudyInstanceUID)\n                series = str(meta.SeriesInstanceUID)\n                instance = str(meta.SOPInstanceUID)\n                # TODO: Extract additional metadata and populate the database\n            except Exception as e:\n                print(folder, e)\n                pass\n    return database",
        "complete": "def crawl_one(folder):\n    folder_path = pathlib.Path(folder)\n    database = {}\n    for path, _, _ in os.walk(folder):\n        dicoms = glob.glob(pathlib.Path(path, \"**\", \"*.dcm\").as_posix(), recursive=True)\n        for dcm in dicoms:\n            try:\n                dcm_path = pathlib.Path(dcm)\n                fname = dcm_path.name\n                rel_path = dcm_path.relative_to(folder_path.parent.parent)\n                rel_posix = rel_path.parent.as_posix()\n                meta = dcmread(dcm, force=True, stop_before_pixels=True)\n                patient = str(meta.PatientID)\n                study = str(meta.StudyInstanceUID)\n                series = str(meta.SeriesInstanceUID)\n                instance = str(meta.SOPInstanceUID)\n                subseries = str(meta.get('AcquisitionNumber', 'default'))\n                if patient not in database:\n                    database[patient] = {}\n                if study not in database[patient]:\n                    database[patient][study] = {'description': str(meta.get('StudyDescription', ''))}\n                if series not in database[patient][study]:\n                    database[patient][study][series] = {'description': str(meta.get('SeriesDescription', ''))}\n                if subseries not in database[patient][study][series]:\n                    database[patient][study][series][subseries] = {\n                        'instances': {},\n                        'instance_uid': instance,\n                        'modality': meta.Modality,\n                        'reference_ct': '',\n                        'reference_rs': '',\n                        'reference_pl': '',\n                        'reference_frame': str(meta.get('FrameOfReferenceUID', '')),\n                        'folder': rel_posix,\n                        'orientation': str(meta.get('ImageOrientationPatient', '')),\n                        'orientation_type': str(meta.get('AnatomicalOrientationType', '')),\n                        'repetition_time': float(meta.get('RepetitionTime', 0)),\n                        'echo_time': float(meta.get('EchoTime', 0)),\n                        'scan_sequence': str(meta.get('ScanningSequence', '')),\n                        'mag_field_strength': float(meta.get('MagneticFieldStrength', 0)),\n                        'imaged_nucleus': str(meta.get('ImagedNucleus', '')),\n                        'fname': rel_path.as_posix()\n                    }\n                database[patient][study][series][subseries]['instances'][instance] = rel_path.as_posix()\n            except Exception as e:\n                print(folder, e)\n                pass\n    return database"
      },
      {
        "partial": "def to_df(database_dict):\n    df = pd.DataFrame()\n    for pat in database_dict:\n        for study in database_dict[pat]:\n            for series in database_dict[pat][study]:\n                if series != 'description':\n                    for subseries in database_dict[pat][study][series]:\n                        if subseries != 'description':\n                            # TODO: Define columns and values\n                            # TODO: Create DataFrame and concatenate\n    return df",
        "complete": "def to_df(database_dict):\n    df = pd.DataFrame()\n    for pat in database_dict:\n        for study in database_dict[pat]:\n            for series in database_dict[pat][study]:\n                if series != 'description':\n                    for subseries in database_dict[pat][study][series]:\n                        if subseries != 'description':\n                            columns = ['patient_ID', 'study', 'study_description', 'series', 'series_description', 'subseries', 'modality', 'instances', 'instance_uid', 'reference_ct', 'reference_rs', 'reference_pl', 'reference_frame', 'folder', 'orientation', 'orientation_type', 'MR_repetition_time', 'MR_echo_time', 'MR_scan_sequence', 'MR_magnetic_field_strength', 'MR_imaged_nucleus', 'file_path']\n                            values = [pat, study, database_dict[pat][study]['description'], series, database_dict[pat][study][series]['description'], subseries, database_dict[pat][study][series][subseries]['modality'], len(database_dict[pat][study][series][subseries]['instances']), database_dict[pat][study][series][subseries]['instance_uid'], database_dict[pat][study][series][subseries]['reference_ct'], database_dict[pat][study][series][subseries]['reference_rs'], database_dict[pat][study][series][subseries]['reference_pl'], database_dict[pat][study][series][subseries]['reference_frame'], database_dict[pat][study][series][subseries]['folder'], database_dict[pat][study][series][subseries]['orientation'], database_dict[pat][study][series][subseries]['orientation_type'], database_dict[pat][study][series][subseries]['repetition_time'], database_dict[pat][study][series][subseries]['echo_time'], database_dict[pat][study][series][subseries]['scan_sequence'], database_dict[pat][study][series][subseries]['mag_field_strength'], database_dict[pat][study][series][subseries]['imaged_nucleus'], database_dict[pat][study][series][subseries]['fname']]\n                            df_add = pd.DataFrame([values], columns=columns)\n                            df = pd.concat([df, df_add], ignore_index=True)\n    return df"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "glob",
        "json",
        "pandas"
      ],
      "from_imports": [
        "argparse.ArgumentParser",
        "pydicom.dcmread",
        "tqdm.tqdm",
        "joblib.Parallel"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/arrayutils.py",
    "language": "py",
    "content": "import numpy as np\nimport SimpleITK as sitk\n\n\ndef array_to_image(array,\n                   origin=(0., 0., 0.),\n                   direction=(1., 0., 0., 0., 1., 0., 0., 0., 1.),\n                   spacing=(1., 1., 1.),\n                   reference_image=None):\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        image.CopyInformation(reference_image)\n    else:\n        image.SetOrigin(origin)\n        image.SetDirection(direction)\n        image.SetSpacing(spacing)\n\n    return image\n\n\ndef find_slices_with_labels(array, labels=None):\n    if not labels:\n        return np.where(array.sum(axis=(1, 2)) > 0)[0]\n    else:\n        if isinstance(labels, int):\n            labels = [labels]\n        return np.where(np.isin(array, labels).sum(axis=(1, 2)) > 0)[0]\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `array_to_image` function in the given code snippet?",
        "answer": "The `array_to_image` function converts a numpy array to a SimpleITK image. It sets the image's origin, direction, and spacing either from provided parameters or by copying information from a reference image if provided. This function is useful for creating SimpleITK images with proper metadata from numpy arrays, which is common in medical image processing."
      },
      {
        "question": "How does the `find_slices_with_labels` function handle different input types for the `labels` parameter?",
        "answer": "The `find_slices_with_labels` function is flexible in handling the `labels` parameter:\n1. If `labels` is None, it finds slices where any non-zero value exists.\n2. If `labels` is an integer, it converts it to a list containing that integer.\n3. If `labels` is a list, it uses it directly.\nThe function then uses numpy's `isin` function to check for the presence of specified labels in each slice, returning the indices of slices containing any of the specified labels."
      },
      {
        "question": "What is the significance of the `direction` parameter in the `array_to_image` function, and what does its default value represent?",
        "answer": "The `direction` parameter in the `array_to_image` function represents the direction cosines of the image axes. Its default value `(1., 0., 0., 0., 1., 0., 0., 0., 1.)` corresponds to an identity matrix, which means the image axes are aligned with the patient coordinate system. This parameter is important in medical imaging to correctly orient the image in 3D space, especially when dealing with DICOM images or other modalities where patient orientation is crucial."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def array_to_image(array,\n                   origin=(0., 0., 0.),\n                   direction=(1., 0., 0., 0., 1., 0., 0., 0., 1.),\n                   spacing=(1., 1., 1.),\n                   reference_image=None):\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        # Complete the code here\n    else:\n        # Complete the code here\n\n    return image",
        "complete": "def array_to_image(array,\n                   origin=(0., 0., 0.),\n                   direction=(1., 0., 0., 0., 1., 0., 0., 0., 1.),\n                   spacing=(1., 1., 1.),\n                   reference_image=None):\n    image = sitk.GetImageFromArray(array)\n    if reference_image is not None:\n        image.CopyInformation(reference_image)\n    else:\n        image.SetOrigin(origin)\n        image.SetDirection(direction)\n        image.SetSpacing(spacing)\n\n    return image"
      },
      {
        "partial": "def find_slices_with_labels(array, labels=None):\n    if not labels:\n        # Complete the code here\n    else:\n        if isinstance(labels, int):\n            labels = [labels]\n        # Complete the code here",
        "complete": "def find_slices_with_labels(array, labels=None):\n    if not labels:\n        return np.where(array.sum(axis=(1, 2)) > 0)[0]\n    else:\n        if isinstance(labels, int):\n            labels = [labels]\n        return np.where(np.isin(array, labels).sum(axis=(1, 2)) > 0)[0]"
      }
    ],
    "dependencies": {
      "imports": [
        "numpy",
        "SimpleITK"
      ],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugSensitivityPCorr.R",
    "language": "R",
    "content": "\ncor.boot <- function(data, w){\n  ddd <- data[w,]\n        ## A question here is what to do when our bootstrap sample has 0 variance in one of\n      ## the two variables (usually the molecular feature)\n      ## If we return NA, we effectively condition on a sampling procedure that samples at least\n      ## one expressor. If we return near 0 (the default of coop::pcor), then we effectively say that conditioned\n      ## on not sampling any expressors, there is no association. Neither is correct, but the latter is certainly more\n      ## conservative. We probably want to use a completely different model to discover \"rare\" biomarkers\n      ## We will go with the later conservative option, however we will set it to zero exactly instead of relying on\n      ## the behaviour of coop.\n\n  if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n    return(0)\n  }\n\n  return(coop::pcor(ddd[,1], ddd[,2]))\n}\n\n\n#' Calculate The Gene Drug Sensitivity\n#'\n#' This version of the function uses a partial correlation instead of standardized linear models.\n#'\n#' @param x A \\code{numeric} vector of gene expression values\n#' @param type A \\code{vector} of factors specifying the cell lines or type types\n#' @param batch A \\code{vector} of factors specifying the batch\n#' @param drugpheno A \\code{numeric} vector of drug sensitivity values (e.g.,\n#'   IC50 or AUC)\n#' @param test A \\code{character} string indicating whether resampling or analytic based tests should be used\n#' @param req_alpha \\code{numeric}, number of permutations for p value calculation\n#' @param nBoot \\code{numeric}, number of bootstrap resamplings for confidence interval estimation\n#' @param conf.level \\code{numeric}, between 0 and 1. Size of the confidence interval required\n#' @param max_perm \\code{numeric} the maximum number of permutations that QUICKSTOP can do before giving up and returning NA.\n#'   Can be set globally by setting the option \"PharmacoGx_Max_Perm\", or left at the default of \\code{ceiling(1/req_alpha*100)}.\n#' @param verbose \\code{boolean} Should the function display messages?\n#'\n#' @return A \\code{vector} reporting the effect size (estimateof the coefficient\n#'   of drug concentration), standard error (se), sample size (n), t statistic,\n#'   and F statistics and its corresponding p-value.\n#'\n#' @examples\n#' print(\"TODO::\")\n#'\n#' @importFrom stats sd complete.cases lm glm anova pf formula var pt qnorm cor residuals runif\n#' @importFrom boot boot boot.ci\n#' @importFrom coop pcor\ngeneDrugSensitivityPCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    ## not enough samples with complete information or no variation in gene expression\n    return(rest)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n\n\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  # ff1 <- sprintf(\"%s + x\", ff0)\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  ## control for tissue type\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  ## control for batch\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n  ## control for duration\n  # if(length(sort(unique(duration))) > 1){\n  #   ff0 <- sprintf(\"%s + duration\", ff0)\n  #   ff <- sprintf(\"%s + duration\", ff)\n  # }\n\n  # if(is.factor(drugpheno[,1])){\n\n  #   drugpheno <- drugpheno[,1]\n\n  # } else {\n\n  #   drugpheno <- as.matrix(drugpheno)\n\n  # }\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n\n    stop(\"Currently only continous output allowed for partial correlations\")\n\n\n  } else {\n\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L # taking the residual degrees of freedom minus 2 parameters estimated for pearson cor.\n    } else { ## doing this if statement in the case there are some numerical differences between mean centred values and raw values\n    var1 <- dd[,\"drugpheno.1\"]\n    var2 <- dd[,\"x\"]\n    df <- nn - 2L\n  }\n\n  obs.cor <- coop::pcor(var1, var2, use=\"complete.obs\")\n\n\n    ## NB: just permuting the residuals would leads to Type I error inflation,\n    ## from an underestimation due to ignoring variance in the effects of the covariates.\n    ## See: https://www.tandfonline.com/doi/abs/10.1080/00949650008812035\n    ## Note that the above paper does not provide a single method recommended in all cases\n    ## We apply the permutation of raw data method, as it is most robust to small sample sizes\n  if(test == \"resampling\"){\n      ## While the logic is equivalent regardless of if there are covariates for calculating the point estimate,\n      ## (correlation is a subcase of partial correlation), for computational efficency in permuation testing we\n      ## split here and don't do extranous calls to lm if it is unnecessay.\n\n    if(ncol(dd) > 2){\n\n      if(!getOption(\"PharmacoGx_useC\")|| ncol(dd)!=3){ ## currently implementing c code only for 1 single grouping variable\n\n        ## implementing a much more efficient method for the particular case where we have 3 columns with assumption that\n        ## column 3 is the tissue.\n        if(ncol(dd)==3){\n          sample_function <- function(){\n\n            partial.dp <- sample(dd[,1], nrow(dd))\n            partial.x <- sample(dd[,2], nrow(dd))\n\n            for(gp in unique(dd[,3])){\n              partial.x[dd[,3]==gp] <- partial.x[dd[,3]==gp]-mean(partial.x[dd[,3]==gp])\n              partial.dp[dd[,3]==gp] <- partial.dp[dd[,3]==gp]-mean(partial.dp[dd[,3]==gp])\n            }\n\n            perm.cor <- coop::pcor(partial.dp, partial.x, use=\"complete.obs\")\n            return(abs(obs.cor) < abs(perm.cor))\n          }\n        } else {\n          sample_function <- function(){\n            # browser()\n            dd2 <- dd\n            dd2[,1] <- sample(dd[,1], nrow(dd))\n            dd2[,2] <- sample(dd[,2], nrow(dd))\n\n            partial.dp <- residuals(lm(formula(ffd), dd2))\n            partial.x <- residuals(lm(formula(ffx), dd2))\n\n            perm.cor <- coop::pcor(partial.dp, partial.x, use=\"complete.obs\")\n            return(abs(obs.cor) < abs(perm.cor))\n          }\n        }\n\n        p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n        significant <- p.value$significant\n        p.value <- p.value$p.value\n\n\n      } else {\n\n        x <- dd[,1]\n        y <- dd[,2]\n        GR <- as.integer(factor(dd[,3]))-1L\n        GS <- as.integer(table(factor(dd[,3])))\n        NG <- length(table(factor(dd[,3])))\n        N <- as.numeric(length(x))\n\n        p.value <-PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, max_perm, N, req_alpha, req_alpha/100, 10L, runif(2))\n        significant <- p.value[[1]]\n        p.value <- p.value[[2]]\n      }\n\n\n    pcor.boot <- function(ddd, w){\n      ddd <- ddd[w,]\n          ## Taking care of an edge case where only one factor level is left after resampling\n          ## However, we need to keep the first two numeric columns to properly return a value, otherwise\n          ## if we remove gene expression because there were only non-detected samples, for example,\n          ## we will try to take the correlation against a character vector.\n      ddd <- ddd[,c(TRUE, TRUE, apply(ddd[,-c(1,2),drop=FALSE], 2, function(x) return(length(unique(x))))>=2)]\n\n\n      ## A question here is what to do when our bootstrap sample has 0 variance in one of\n      ## the two variables (usually the molecular feature)\n      ## If we return NA, we effectively condition on a sampling procedure that samples at least\n      ## one expressor. If we return near 0 (the default of coop::pcor), then we effectively say that conditioned\n      ## on not sampling any expressors, there is no association. Neither is correct, but the latter is certainly more\n      ## conservative. We probably want to use a completely different model to discover \"rare\" biomarkers\n      ## We will go with the later conservative option, however we will set it to zero exactly instead of relying on\n      ## the behaviour of coop.\n\n      if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n        return(0)\n      }\n\n      if(ncol(ddd)==3){\n        partial.dp <- ddd[,1]\n        partial.x <- ddd[,2]\n        for(gp in unique(ddd[,3])){\n          partial.x[ddd[,3]==gp] <- partial.x[ddd[,3]==gp]-mean(partial.x[ddd[,3]==gp])\n          partial.dp[ddd[,3]==gp] <- partial.dp[ddd[,3]==gp]-mean(partial.dp[ddd[,3]==gp])\n        }\n\n      } else if(ncol(ddd)==2){\n        partial.dp <- ddd[,1]\n        partial.x <- ddd[,2]\n      } else {\n\n        partial.dp <- residuals(lm(formula(ffd), ddd))\n        partial.x <- residuals(lm(formula(ffx), ddd))\n\n      }\n\n      return(coop::pcor(partial.dp, partial.x, use=\"complete.obs\"))\n    }\n\n    boot.out <- boot(dd, pcor.boot, R=nBoot)\n    cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n      error = function(e) {\n        if(e$message == \"estimated adjustment 'w' is infinite\"){\n          warning(\"estimated adjustment 'w' is infinite for some features\")\n          return(c(NA_real_,NA_real_))\n        } else {\n          stop(e)\n        }\n      })\n  } else {\n    if(!getOption(\"PharmacoGx_useC\")){\n      sample_function <- function(){\n        v1 <- sample(var1)\n        return(abs(obs.cor) < abs(coop::pcor(v1, var2, use=\"complete.obs\")))\n      }\n\n      p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n      significant <- p.value$significant\n      p.value <- p.value$p.value\n    } else {\n\n      x <- as.numeric(var1)\n      y <- as.numeric(var2)\n      GR <- rep(0L, length(x))\n      GS <- as.integer(length(x))\n      NG <- 1L\n      N <- as.numeric(length(x))\n\n      p.value <-PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, max_perm, N, req_alpha, req_alpha/100, 10L, runif(2))\n      significant <- p.value[[1]]\n      p.value <- p.value[[2]]\n    }\n\n\n\n\n    boot.out <- boot(data.frame(var1, var2), cor.boot, R=nBoot)\n    cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n      error = function(e) {\n        if(e$message == \"estimated adjustment 'w' is infinite\" || e$message == \"Error in if (const(t, min(1e-08, mean(t, na.rm = TRUE)/1e+06))) { : \\n  missing value where TRUE/FALSE needed\\n\"){\n          warning(\"estimated adjustment 'w' is infinite for some features\")\n          return(c(NA_real_,NA_real_))\n        } else {\n          stop(e)\n        }\n      })\n  }\n\n\n} else if(test == \"analytic\"){\n      # if(ncol(dd) > 2){\n      #   df <- nn - 2L - controlled.var\n\n      # } else {\n      #   df <- nn - 2L\n      #   # cor.test.res <- cor.test(dd[,\"drugpheno.1\"], dd[,\"x\"], method=\"pearson\", use=\"complete.obs\")\n      # }\n  stat <- sqrt(df) * obs.cor/sqrt(1-obs.cor^2) ## Note, this is implemented in same order of operations as cor.test\n  p.value <- 2*min(pt(stat, df=df), pt(stat, df=df, lower.tail = FALSE))\n      ## Implementing with fisher transform and normal dist for consistency with R's cor.test\n  z <- atanh(obs.cor)\n  sigma <- 1/sqrt(df - 1)\n  cint <- tanh(z + c(-1, 1) * sigma * qnorm((1 + conf.level)/2))\n  significant <- p.value < req_alpha\n}\n\n}\n\nrest <- c(\"estimate\"=obs.cor, \"n\"=nn, df=df, significant = as.numeric(significant), \"pvalue\"=p.value, \"lower\" = cint[1], \"upper\" = cint[2])\n\n\nreturn(rest)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cor.boot` function and how does it handle edge cases?",
        "answer": "The `cor.boot` function is used for bootstrap resampling to estimate partial correlations. It handles edge cases where the bootstrap sample has zero variance in one of the two variables by returning 0 instead of NA. This is a conservative approach for dealing with rare biomarkers or non-expressor samples."
      },
      {
        "question": "How does the `geneDrugSensitivityPCorr` function handle missing or infinite values in the input data?",
        "answer": "The function handles missing or infinite values by first removing infinite values from non-factor columns in the `drugpheno` data frame, replacing them with NA. Then, it uses `complete.cases` to filter out any rows with NA values across all input variables (x, type, batch, and drugpheno) before performing the analysis."
      },
      {
        "question": "What statistical methods does the `geneDrugSensitivityPCorr` function use for hypothesis testing and confidence interval estimation?",
        "answer": "The function offers two methods for hypothesis testing: resampling and analytic. For resampling, it uses permutation tests to calculate p-values and bootstrap resampling to estimate confidence intervals. For the analytic method, it uses the Fisher transformation and t-distribution to calculate p-values and confidence intervals. The choice between methods is controlled by the `test` parameter."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cor.boot <- function(data, w){\n  ddd <- data[w,]\n  if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n    return(0)\n  }\n  return(coop::pcor(ddd[,1], ddd[,2]))\n}",
        "complete": "cor.boot <- function(data, w){\n  ddd <- data[w,]\n  if(length(unique(ddd[,1]))<2 || length(unique(ddd[,2]))<2){\n    return(0)\n  }\n  return(coop::pcor(ddd[,1], ddd[,2]))\n}"
      },
      {
        "partial": "geneDrugSensitivityPCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    return(rest)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n    stop(\"Currently only continous output allowed for partial correlations\")\n  } else {\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L\n    } else {\n      var1 <- dd[,\"drugpheno.1\"]\n      var2 <- dd[,\"x\"]\n      df <- nn - 2L\n    }\n\n    obs.cor <- coop::pcor(var1, var2, use=\"complete.obs\")\n\n    # ... Rest of the function ...\n  }\n}",
        "complete": "geneDrugSensitivityPCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || isTRUE(all.equal(var(x[ccix], na.rm=TRUE), 0))) {\n    return(rest)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n    stop(\"Currently only continous output allowed for partial correlations\")\n  } else {\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L\n    } else {\n      var1 <- dd[,\"drugpheno.1\"]\n      var2 <- dd[,\"x\"]\n      df <- nn - 2L\n    }\n\n    obs.cor <- coop::pcor(var1, var2, use=\"complete.obs\")\n\n    if(test == \"resampling\"){\n      if(!getOption(\"PharmacoGx_useC\") || ncol(dd)!=3){\n        sample_function <- function(){\n          dd2 <- dd\n          dd2[,1] <- sample(dd[,1], nrow(dd))\n          dd2[,2] <- sample(dd[,2], nrow(dd))\n          partial.dp <- residuals(lm(formula(ffd), dd2))\n          partial.x <- residuals(lm(formula(ffx), dd2))\n          perm.cor <- coop::pcor(partial.dp, partial.x, use=\"complete.obs\")\n          return(abs(obs.cor) < abs(perm.cor))\n        }\n        p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n        significant <- p.value$significant\n        p.value <- p.value$p.value\n      } else {\n        x <- dd[,1]\n        y <- dd[,2]\n        GR <- as.integer(factor(dd[,3]))-1L\n        GS <- as.integer(table(factor(dd[,3])))\n        NG <- length(table(factor(dd[,3])))\n        N <- as.numeric(length(x))\n        p.value <- PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, max_perm, N, req_alpha, req_alpha/100, 10L, runif(2))\n        significant <- p.value[[1]]\n        p.value <- p.value[[2]]\n      }\n      boot.out <- boot(dd, cor.boot, R=nBoot)\n      cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n        error = function(e) {\n          if(e$message == \"estimated adjustment 'w' is infinite\"){\n            warning(\"estimated adjustment 'w' is infinite for some features\")\n            return(c(NA_real_,NA_real_))\n          } else {\n            stop(e)\n          }\n        })\n    } else if(test == \"analytic\"){\n      stat <- sqrt(df) * obs.cor/sqrt(1-obs.cor^2)\n      p.value <- 2*min(pt(stat, df=df), pt(stat, df=df, lower.tail = FALSE))\n      z <- atanh(obs.cor)\n      sigma <- 1/sqrt(df - 1)\n      cint <- tanh(z + c(-1, 1) * sigma * qnorm((1 + conf.level)/2))\n      significant <- p.value < req_alpha\n    }\n  }\n\n  rest <- c(\"estimate\"=obs.cor, \"n\"=nn, df=df, significant = as.numeric(significant), \"pvalue\"=p.value, \"lower\" = cint[1], \"upper\" = cint[2])\n  return(rest)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/updateObject-methods.R",
    "language": "R",
    "content": "#' Update the PharmacoSet class after changes in it struture or API\n#'\n#' @param object A `PharmacoSet` object to update the class structure for.\n#'\n#' @return `PharmacoSet` with update class structure.\n#' \n#' @examples\n#' data(GDSCsmall)\n#' updateObject(GDSCsmall)\n#' \n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        colnames(curation(pSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(pSet)$treatment))\n    }\n    validObject(pSet)\n    return(pSet)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `updateObject` method for the `PharmacoSet` class?",
        "answer": "The `updateObject` method is used to update the class structure of a `PharmacoSet` object after changes in its structure or API. It performs the following tasks: 1) Calls the next method in the method dispatch chain, 2) Converts the result back to a `PharmacoSet` object, 3) Updates the naming conventions in the curation data, and 4) Validates the updated object before returning it."
      },
      {
        "question": "How does this method handle the renaming of 'drug' to 'treatment' in the curation data?",
        "answer": "The method renames 'drug' to 'treatment' in the curation data using two steps: 1) It uses `gsub()` to replace 'drug' with 'treatment' in the names of the curation list. 2) If a 'treatment' element exists in the curation data, it renames the 'treatmentid' column to 'treatmentid' (which effectively does nothing but might be a placeholder for future changes)."
      },
      {
        "question": "What S4 object-oriented programming concepts are demonstrated in this code snippet?",
        "answer": "This code snippet demonstrates several S4 object-oriented programming concepts: 1) Method definition using `setMethod()`, 2) Method dispatch with `callNextMethod()`, 3) Object conversion using `as()`, 4) Slot access and modification with the `curation()` function, 5) Object validation with `validObject()`, and 6) Use of method signatures with `signature('PharmacoSet')`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        # Complete the code to update column names\n    }\n    validObject(pSet)\n    return(pSet)\n})",
        "complete": "setMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        colnames(curation(pSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(pSet)$treatment))\n    }\n    validObject(pSet)\n    return(pSet)\n})"
      },
      {
        "partial": "#' Update the PharmacoSet class after changes in it struture or API\n#'\n#' @param object A `PharmacoSet` object to update the class structure for.\n#'\n#' @return `PharmacoSet` with update class structure.\n#' \n#' @examples\n#' data(GDSCsmall)\n#' updateObject(GDSCsmall)\n#' \n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    # Complete the function body\n})",
        "complete": "#' Update the PharmacoSet class after changes in it struture or API\n#'\n#' @param object A `PharmacoSet` object to update the class structure for.\n#'\n#' @return `PharmacoSet` with update class structure.\n#' \n#' @examples\n#' data(GDSCsmall)\n#' updateObject(GDSCsmall)\n#' \n#' @md\n#' @importMethodsFrom CoreGx updateObject\n#' @export\nsetMethod(\"updateObject\", signature(\"PharmacoSet\"), function(object) {\n    cSet <- callNextMethod(object)\n    pSet <- as(cSet, \"PharmacoSet\")\n    names(curation(pSet)) <- gsub(\"drug\", \"treatment\", names(curation(pSet)))\n    if (\"treatment\" %in% names(curation(pSet))) {\n        colnames(curation(pSet)$treatment) <- gsub(\"treatmentid\", \"treatmentid\",\n            colnames(curation(pSet)$treatment))\n    }\n    validObject(pSet)\n    return(pSet)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/autopipeline.py",
    "language": "py",
    "content": "from aifc import Error\nimport os\nimport pathlib\nimport shutil\nimport glob\nimport pickle\nimport numpy as np\nimport warnings\n\nimport yaml\nimport json\nimport SimpleITK as sitk\n\nfrom imgtools.ops import StructureSetToSegmentation, ImageAutoInput, ImageAutoOutput, Resample\nfrom imgtools.pipeline import Pipeline\nfrom imgtools.utils.nnunet import generate_dataset_json, markdown_report_images\nfrom imgtools.utils.args import parser\nfrom joblib import Parallel, delayed\nfrom imgtools.modules import Segmentation\nfrom sklearn.model_selection import train_test_split\n\nimport dill\n###############################################################\n# Example usage:\n# python radcure_simple.py ./data/RADCURE/data ./RADCURE_output\n###############################################################\n\n\nclass AutoPipeline(Pipeline):\n    \"\"\"Example processing pipeline for the RADCURE dataset.\n    This pipeline loads the CT images and structure sets, re-samples the images,\n    and draws the GTV contour using the resampled image.\n    \"\"\"\n\n    def __init__(self,\n                 input_directory=\"\",\n                 output_directory=\"\",\n                 modalities=\"CT\",\n                 spacing=(1., 1., 0.),\n                 n_jobs=-1,\n                 visualize=False,\n                 missing_strategy=\"drop\",\n                 show_progress=False,\n                 warn_on_error=False,\n                 overwrite=False,\n                 nnunet=False,\n                 train_size=1.0,\n                 random_state=42,\n                 read_yaml_label_names=False,\n                 ignore_missing_regex=False,\n                 roi_yaml_path=\"\",\n                 custom_train_test_split=False,\n                 nnunet_inference=False,\n                 dataset_json_path=\"\",\n                 continue_processing=False,\n                 dry_run=False,\n                 verbose=False,\n                 update=False,\n                 roi_select_first=False,\n                 roi_separate=False):\n        \"\"\"Initialize the pipeline.\n\n        Parameters\n        ----------\n        input_directory: str\n            Directory containing the input data\n        output_directory: str\n            Directory where the output data will be stored\n        modalities: str, default=\"CT\"\n            Modalities to load. Can be a comma-separated list of modalities with no spaces\n        spacing: tuple of floats, default=(1., 1., 0.)\n            Spacing of the output image\n        n_jobs: int, default=-1\n            Number of jobs to run in parallel. If -1, use all cores\n        visualize: bool, default=False\n            Whether to visualize the results of the pipeline using pyvis. Outputs to an HTML file\n        missing_strategy: str, default=\"drop\"\n            How to handle missing modalities. Can be \"drop\" or \"fill\"\n        show_progress: bool, default=False\n            Whether to show progress bars\n        warn_on_error: bool, default=False\n            Whether to warn on errors\n        overwrite: bool, default=False\n            Whether to write output files even if existing output files exist\n        nnunet: bool, default=False\n            Whether to format the output for nnunet\n        train_size: float, default=1.0\n            Proportion of the dataset to use for training, as a decimal\n        random_state: int, default=42\n            Random state for train_test_split\n        read_yaml_label_names: bool, default=False\n            Whether to read dictionary representing the label that regexes are mapped to from YAML. For example, \"GTV\": \"GTV.*\" will combine all regexes that match \"GTV.*\" into \"GTV\"\n        ignore_missing_regex: bool, default=False\n            Whether to ignore missing regexes. Will raise an error if none of the regexes in label_names are found for a patient\n        roi_yaml_path: str, default=\"\"\n            The path to the yaml file defining regexes\n        custom_train_test_split: bool, default=False\n            Whether to use a custom train/test split. The remaining patients will be randomly split using train_size and random_state\n        nnunet_inference: bool, default=False\n            Whether to format the output for nnUNet inference\n        dataset_json_path: str, default=\"\"\n            The path to the dataset.json file for nnUNet inference\n        continue_processing: bool, default=False\n            Whether to continue processing a partially processed dataset\n        dry_run: bool, default=False\n            Whether to run the pipeline without writing any output files\n        \"\"\"\n\n        self.continue_processing = continue_processing\n        self.dry_run = dry_run\n        self.v = verbose\n\n        if dry_run:\n            nnunet = False\n            nnunet_inference = False\n\n        if dry_run and continue_processing:\n            raise ValueError(\"Cannot continue processing a dry run. Set --continue_processing to False to do a dry run.\")\n\n        if not dry_run and output_directory == \"\":\n            raise ValueError(\"Must specify an output directory\")\n        \n        # input/output directory configuration\n        if not os.path.isabs(input_directory):\n            input_directory = pathlib.Path(os.getcwd(), input_directory).as_posix()\n        else:\n            input_directory = pathlib.Path(input_directory).as_posix()  # consistent parsing. ensures last child directory doesn't end with slash\n        \n        if not os.path.isabs(output_directory):\n            output_directory = pathlib.Path(os.getcwd(), output_directory).as_posix()\n        else:\n            output_directory = pathlib.Path(output_directory).as_posix()  # consistent parsing. ensures last child directory doesn't end with slash\n\n        # check/make output directory\n        if not os.path.exists(output_directory):\n            os.makedirs(output_directory)\n\n        # check input directory exists\n        if not os.path.exists(input_directory):\n            raise FileNotFoundError(f\"Input directory {input_directory} does not exist\")\n        \n        self.input_directory = pathlib.Path(input_directory).as_posix()\n        self.output_directory = pathlib.Path(output_directory).as_posix()\n        \n        # if wanting to continue processing but no .temp folders\n        if not nnunet and continue_processing and not os.path.exists(pathlib.Path(output_directory, \".temp\").as_posix()):\n            raise FileNotFoundError(f\"Cannot continue processing. .temp directory does not exist in {output_directory}. Run without --continue_processing to start from scratch.\")\n\n        study_name = os.path.split(self.input_directory)[1]\n        if nnunet_inference:\n            roi_yaml_path = \"\"\n            custom_train_test_split = False\n            nnunet = False\n            if modalities != \"CT\" and modalities != \"MR\":\n                raise ValueError(\"nnUNet inference can only be run on image files. Please set modalities to 'CT' or 'MR'\")\n        if nnunet:\n            self.base_output_directory = self.output_directory\n            if not os.path.exists(pathlib.Path(self.output_directory, \"nnUNet_preprocessed\").as_posix()):\n                os.makedirs(pathlib.Path(self.output_directory, \"nnUNet_preprocessed\").as_posix())\n            if not os.path.exists(pathlib.Path(self.output_directory, \"nnUNet_trained_models\").as_posix()):\n                os.makedirs(pathlib.Path(self.output_directory, \"nnUNet_trained_models\").as_posix())\n            self.output_directory = pathlib.Path(self.output_directory, \"nnUNet_raw_data_base\",\n                                                 \"nnUNet_raw_data\").as_posix()\n            if not os.path.exists(self.output_directory):\n                os.makedirs(self.output_directory)\n            all_nnunet_folders = glob.glob(pathlib.Path(self.output_directory, \"*\", \" \").as_posix())\n            # print(all_nnunet_folders)\n            numbers = [int(os.path.split(os.path.split(folder)[0])[1][4:7]) for folder in all_nnunet_folders if os.path.split(os.path.split(folder)[0])[1].startswith(\"Task\")]\n            # print(numbers, continue_processing)\n            if (len(numbers) == 0 and continue_processing) or not continue_processing or not os.path.exists(pathlib.Path(self.output_directory, f\"Task{max(numbers)}_{study_name}\", \".temp\").as_posix()):\n                available_numbers = list(range(500, 1000))\n                for folder in all_nnunet_folders:\n                    folder_name = os.path.split(os.path.split(folder)[0])[1]\n                    if folder_name.startswith(\"Task\") and folder_name[4:7].isnumeric() and int(folder_name[4:7]) in available_numbers:\n                        available_numbers.remove(int(folder_name[4:7]))\n                if len(available_numbers) == 0:\n                    raise Error(\"There are not enough task ID's for the nnUNet output. Please make sure that there is at least one task ID available between 500 and 999, inclusive\")\n                task_folder_name = f\"Task{available_numbers[0]}_{study_name}\"\n                self.output_directory = pathlib.Path(self.output_directory, task_folder_name).as_posix()\n                self.task_id = available_numbers[0]\n            else:\n                self.task_id = max(numbers)\n                task_folder_name = f\"Task{self.task_id}_{study_name}\"\n                self.output_directory = pathlib.Path(self.output_directory, task_folder_name).as_posix()\n            if not os.path.exists(pathlib.Path(self.output_directory, \".temp\").as_posix()):\n                os.makedirs(pathlib.Path(self.output_directory, \".temp\").as_posix())\n        \n        if not dry_run:\n            # Make a directory\n            if not os.path.exists(pathlib.Path(self.output_directory,\".temp\").as_posix()):\n                os.mkdir(pathlib.Path(self.output_directory,\".temp\").as_posix())\n                \n            with open(pathlib.Path(self.output_directory, \".temp\", \"init_parameters.pkl\").as_posix(), \"wb\") as f:\n                parameters = locals()  # save all the parameters in case we need to continue processing\n                dill.dump(parameters, f)\n\n            # continue processing operations\n            self.finished_subjects = [pathlib.Path(e).name[:-4] for e in glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())]  # remove the .pkl\n            \n\n        super().__init__(n_jobs=n_jobs,\n                         missing_strategy=missing_strategy,\n                         show_progress=show_progress,\n                         warn_on_error=warn_on_error)\n        self.overwrite = overwrite\n        self.spacing = spacing\n        self.existing = [None]   # self.existing_patients()\n        self.is_nnunet = nnunet\n        if nnunet or nnunet_inference:\n            self.nnunet_info = {}\n        else:\n            self.nnunet_info = None\n        self.train_size = train_size\n        self.random_state = random_state\n        self.label_names = {}\n        self.ignore_missing_regex = ignore_missing_regex\n        self.custom_train_test_split = custom_train_test_split\n        self.is_nnunet_inference = nnunet_inference\n        self.roi_select_first = roi_select_first\n        self.roi_separate = roi_separate\n\n        if roi_yaml_path != \"\" and not read_yaml_label_names:\n            warnings.warn(\"The YAML will not be read since it has not been specified to read them. To use the file, run the CLI with --read_yaml_label_names\")\n\n        roi_path = pathlib.Path(self.input_directory, \"roi_names.yaml\").as_posix() if roi_yaml_path == \"\" else roi_yaml_path\n        if read_yaml_label_names:\n            if os.path.exists(roi_path):\n                with open(roi_path, \"r\") as f:\n                    try:\n                        self.label_names = yaml.safe_load(f)\n                    except yaml.YAMLError as exc:\n                        print(exc)\n            else:\n                raise FileNotFoundError(f\"No file named roi_names.yaml found at {roi_path}. If you did not intend on creating ROI regexes, run the CLI without --read_yaml_label_names\")\n        \n        if not isinstance(self.label_names, dict):\n            raise ValueError(\"roi_names.yaml must parse as a dictionary\")\n\n        for k, v in self.label_names.items():\n            if not isinstance(v, list) and not isinstance(v, str):\n                raise ValueError(f\"Label values must be either a list of strings or a string. Got {v} for {k}\")\n            elif isinstance(v, list):\n                for a in v:\n                    if not isinstance(a, str):\n                        raise ValueError(f\"Label values must be either a list of strings or a string. Got {a} in list {v} for {k}\")\n            elif not isinstance(k, str):\n                raise ValueError(f\"Label names must be a string. Got {k} for {v}\")\n\n        if self.train_size == 1.0 and nnunet:\n            warnings.warn(\"Train size is 1, all data will be used for training\")\n        \n        if self.train_size == 0.0 and nnunet:\n            warnings.warn(\"Train size is 0, all data will be used for testing\")\n\n        if self.train_size != 1 and not nnunet:\n            warnings.warn(\"Cannot run train/test split without nnunet, ignoring train_size\")\n\n        if self.train_size > 1 or self.train_size < 0 and nnunet:\n            raise ValueError(\"train_size must be between 0 and 1\")\n        \n        if nnunet and (not read_yaml_label_names or self.label_names == {}):\n            raise ValueError(\"YAML label names must be provided for nnunet\")\n        \n        if custom_train_test_split and not nnunet:\n            raise ValueError(\"Cannot use custom train/test split without nnunet\")\n\n        custom_train_test_split_path = pathlib.Path(self.input_directory, \"custom_train_test_split.yaml\").as_posix()\n        if custom_train_test_split and nnunet:\n            if os.path.exists(custom_train_test_split_path):\n                with open(custom_train_test_split_path, \"r\") as f:\n                    try:\n                        self.custom_split = yaml.safe_load(f)\n                        if isinstance(self.custom_split, list):\n                            for e in self.custom_split:\n                                if not isinstance(e, str):\n                                    raise ValueError(\"Custom split must be a list of strings. Place quotes around patient ID's that don't parse as YAML strings\")\n                            self.custom_split = {\"train\": [], \"test\": self.custom_split}\n                        if isinstance(self.custom_split, dict):\n                            if sorted(list(self.custom_split.keys())) != [\"test\", \"train\"] and list(self.custom_split.keys()) != [\"train\"] and list(self.custom_split.keys()) != [\"test\"]:\n                                raise ValueError(\"Custom split must be a dictionary with keys 'train' and 'test'\")\n                            for k, v in self.custom_split.items():\n                                if not isinstance(v, list):\n                                    raise ValueError(f\"Custom split must be a list of strings. Place quotes around patient ID's that don't parse as YAML strings. Got {v} for {k}\")\n                                for e in v:\n                                    if not isinstance(e, str):\n                                        raise ValueError(\"Custom split must be a list of strings. Place quotes around patient ID's that don't parse as YAML strings\")\n                            if list(self.custom_split.keys()) == [\"train\"]:\n                                self.custom_split = {\"train\": self.custom_split[\"train\"], \"test\": []}\n                            elif list(self.custom_split.keys()) == [\"test\"]:\n                                self.custom_split = {\"train\": [], \"test\": self.custom_split[\"test\"]}\n                        for e in self.custom_split[\"train\"]:\n                            if e in self.custom_split[\"test\"]:\n                                raise ValueError(\"Custom split cannot contain the same patient ID in both train and test\")\n                    except yaml.YAMLError as exc:\n                        print(exc)\n            else:\n                raise FileNotFoundError(f\"No file named custom_train_test_split.yaml found at {custom_train_test_split_path}. If you did not intend on creating a custom train-test-split, run the CLI without --custom_train_test_split\")\n\n        if self.is_nnunet:\n            self.nnunet_info[\"modalities\"] = {\"CT\": \"0000\"}  # modality to 4-digit code\n\n        if nnunet_inference:\n            if not os.path.exists(dataset_json_path):\n                raise FileNotFoundError(f\"No file named {dataset_json_path} found. Image modality definitions are required for nnUNet inference\")\n            else:\n                with open(dataset_json_path, \"r\") as f:\n                    self.nnunet_info[\"modalities\"] = {v: k.zfill(4) for k, v in json.load(f)[\"modality\"].items()}\n\n        # Input operations\n        self.input = ImageAutoInput(input_directory, modalities, n_jobs, visualize, update)\n        self.output_df_path = pathlib.Path(self.output_directory, \"dataset.csv\").as_posix()\n\n        # Output component table\n        self.output_df = self.input.df_combined\n\n        # Name of the important columns which needs to be saved\n        self.output_streams = self.input.output_streams\n        \n        # image processing ops\n        self.resample = Resample(spacing=self.spacing)\n        self.make_binary_mask = StructureSetToSegmentation(roi_names=self.label_names, continuous=False)\n\n        # output ops\n        self.output = ImageAutoOutput(self.output_directory, self.output_streams, self.nnunet_info, self.is_nnunet_inference)\n        \n        self.existing_roi_indices = {\"background\": 0}\n        if nnunet or nnunet_inference:\n            self.total_modality_counter = {}\n            self.patients_with_missing_labels = set()\n\n    def glob_checker_nnunet(self, subject_id):\n        folder_names = [\"imagesTr\", \"labelsTr\", \"imagesTs\", \"labelsTs\"]\n        files = []\n        for folder_name in folder_names:\n            if os.path.exists(pathlib.Path(self.input_directory, folder_name).as_posix()):\n                files.extend(glob.glob(pathlib.Path(self.output_directory,folder_name,\"*.nii.gz\").as_posix()))\n        for f in files:\n            if f.startswith(subject_id):\n                return True\n        return False\n\n    def process_one_subject(self, subject_id):\n        \"\"\"Define the processing operations for one subject.\n        This method must be defined for all pipelines. It is used to define\n        the preprocessing steps for a single subject (note: that might mean\n        multiple images, structures, etc.). During pipeline execution, this\n        method will receive one argument, subject_id, which can be used to\n        retrieve inputs and save outputs.\n\n        Parameters\n        ----------\n        subject_id : str\n           The ID of subject to process\n        \"\"\"\n        if self.continue_processing:\n            if subject_id in self.finished_subjects:\n                return\n        # if we want overwriting or if we don't want it and the file doesn't exist, we can process\n        if self.overwrite or (not self.overwrite and not (os.path.exists(pathlib.Path(self.output_directory, subject_id).as_posix()) or self.glob_checker_nnunet(subject_id))):\n            # Check if the subject_id has already been processed\n            if os.path.exists(pathlib.Path(self.output_directory,\".temp\",f'temp_{subject_id}.pkl').as_posix()):\n                print(f\"{subject_id} already processed\")\n                return\n\n            print(\"Processing:\", subject_id)\n\n            read_results = self.input(subject_id)\n            # print(read_results)\n\n            print(subject_id, \" start\")\n            \n            metadata = {}\n            subject_modalities = set()  # all the modalities that this subject has\n            num_rtstructs = 0\n\n            for i, colname in enumerate(self.output_streams):  # sorted(self.output_streams)):  # CT comes before MR before PT before RTDOSE before RTSTRUCT\n                modality = colname.split(\"_\")[0]\n                subject_modalities.add(modality)  # set add\n                \n                # Taking modality pairs if it exists till _{num}\n                output_stream = (\"_\").join([item for item in colname.split(\"_\") if not item.isnumeric()])\n\n                # If there are multiple connections existing, multiple connections means two modalities connected to one modality. They end with _1\n                # mult_conn = colname.split(\"_\")[-1].isnumeric()\n                # num = colname.split(\"_\")[-1]\n\n                if self.v:\n                    print(\"output_stream:\", output_stream)\n\n                if read_results[i] is None:\n                    print(\"The subject id: {} has no {}\".format(subject_id, colname))\n                    pass\n\n                # Process image (CT/MR)\n                elif modality == \"CT\" or modality == 'MR':\n                    image = read_results[i].image\n                    if len(image.GetSize()) == 4:\n                        assert image.GetSize()[-1] == 1, f\"There is more than one volume in this CT file for {subject_id}.\"\n                        extractor = sitk.ExtractImageFilter()\n                        extractor.SetSize([*image.GetSize()[:3], 0])\n                        extractor.SetIndex([0, 0, 0, 0])    \n                        \n                        image = extractor.Execute(image)\n                        if self.v:\n                            print(\"image.GetSize():\", image.GetSize())\n                    try:\n                        image = self.resample(image)\n                    except Exception as e:\n                        print(e)\n                        warnings.warn(\"Could not resample {} for subject {}\".format(colname, subject_id))\n\n                    # update the metadata for this image\n                    if hasattr(read_results[i], \"metadata\") and read_results[i].metadata is not None:\n                        metadata.update(read_results[i].metadata)\n\n                    # modality is MR and the user has selected to have nnunet output\n                    if self.is_nnunet:\n                        if modality == \"MR\":  # MR images can have various modalities like FLAIR, T1, etc.\n                            if metadata[\"AcquisitionContrast\"] not in self.total_modality_counter.keys():\n                                self.total_modality_counter[metadata[\"AcquisitionContrast\"]] = 1\n                            else:\n                                self.total_modality_counter[metadata[\"AcquisitionContrast\"]] += 1\n                            self.nnunet_info['current_modality'] = metadata[\"AcquisitionContrast\"]\n                            if metadata[\"AcquisitionContrast\"] not in self.nnunet_info[\"modalities\"].keys():  # if the modality is new\n                                self.nnunet_info[\"modalities\"][metadata[\"AcquisitionContrast\"]] = str(len(self.nnunet_info[\"modalities\"])).zfill(4)  # fill to 4 digits\n                        else:\n                            self.nnunet_info['current_modality'] = modality  # CT\n                            if modality not in self.total_modality_counter.keys():\n                                self.total_modality_counter[modality] = 1\n                            else:\n                                self.total_modality_counter[modality] += 1\n                        if \"_\".join(subject_id.split(\"_\")[1::]) in self.train:\n                            self.output(subject_id, image, output_stream, nnunet_info=self.nnunet_info)\n                        else:\n                            self.output(subject_id, image, output_stream, nnunet_info=self.nnunet_info, train_or_test=\"Ts\")\n                    elif self.is_nnunet_inference:\n                        self.nnunet_info[\"current_modality\"] = modality if modality == \"CT\" else metadata[\"AcquisitionContrast\"]\n                        if self.nnunet_info[\"current_modality\"] not in self.nnunet_info[\"modalities\"].keys():\n                            raise ValueError(f\"The modality {self.nnunet_info['current_modality']} is not in the list of modalities that are present in dataset.json.\")\n                        self.output(subject_id, image, output_stream, nnunet_info=self.nnunet_info)\n                    else:\n                        self.output(subject_id, image, output_stream)\n\n                    metadata[f\"size_{output_stream}\"] = str(image.GetSize())\n                    print(subject_id, \" SAVED IMAGE\")\n\n                # Process dose\n                elif modality == \"RTDOSE\":\n                    try:   # For cases with no image present\n                        doses = read_results[i].resample_dose(image)\n                    except:\n                        Warning(\"No CT image present. Returning dose image without resampling\")\n                        doses = read_results[i]\n                    \n                    # save output\n                    self.output(subject_id, doses, output_stream)\n                    metadata[f\"size_{output_stream}\"] = str(doses.GetSize())\n                    metadata[f\"metadata_{colname}\"] = [read_results[i].get_metadata()]\n\n                    if hasattr(doses, \"metadata\") and doses.metadata is not None:\n                        metadata.update(doses.metadata)\n\n                    print(subject_id, \" SAVED DOSE\")\n                    \n                # Process PET\n                elif modality == \"PT\":\n                    try:\n                        # For cases with no image present\n                        pet = read_results[i].resample_pet(image)\n                    except:\n                        Warning(\"No CT image present. Returning PT/PET image without resampling.\")\n                        pet = read_results[i]\n\n                    # output\n                    self.output(subject_id, pet, output_stream)\n                    metadata[f\"size_{output_stream}\"] = str(pet.GetSize())\n                    metadata[f\"metadata_{colname}\"] = [read_results[i].get_metadata()]\n\n                    if hasattr(pet, \"metadata\") and pet.metadata is not None:\n                        metadata.update(pet.metadata)\n\n                    print(subject_id, \" SAVED PET\")\n\n                # Process contour\n                elif modality == \"RTSTRUCT\":\n                    num_rtstructs += 1\n                    # For RTSTRUCT, you need image or PT\n                    structure_set = read_results[i]\n                    conn_to = output_stream.split(\"_\")[-1]\n\n                    # make_binary_mask relative to ct/pet\n                    if conn_to in [\"CT\", \"MR\", \"PT\"]:\n                        if conn_to == \"CT\" or conn_to == \"MR\":\n                            img = image\n                        elif conn_to == \"PT\":\n                            img = pet  # noqa: F821\n                        \n                        mask = self.make_binary_mask(structure_set, img, \n                                                     self.existing_roi_indices, \n                                                     self.ignore_missing_regex, \n                                                     roi_select_first=self.roi_select_first, \n                                                     roi_separate=self.roi_separate)\n\n                    else:\n                        raise ValueError(\"You need to pass a reference CT or PT/PET image to map contours to.\")\n                    \n                    if mask is None:  # ignored the missing regex, and exit the loop\n                        if self.is_nnunet:\n                            image_test_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n                            image_train_path = pathlib.Path(self.output_directory, \"imagesTr\").as_posix()\n                            if os.path.exists(image_test_path):\n                                all_files = glob.glob(pathlib.Path(image_test_path, \"*.nii.gz\").as_posix())\n                                # print(all_files)\n                                for file in all_files:\n                                    if subject_id in os.path.split(file)[1]:\n                                        os.remove(file)\n                            if os.path.exists(image_train_path):\n                                all_files = glob.glob(pathlib.Path(image_train_path, \"*.nii.gz\").as_posix())\n                                # print(all_files)\n                                for file in all_files:\n                                    if subject_id in os.path.split(file)[1]:\n                                        os.remove(file)\n                            warnings.warn(f\"Patient {subject_id} is missing a complete image-label pair\")\n                            self.patients_with_missing_labels.add(\"\".join(subject_id.split(\"_\")[1:]))\n                            return\n                        else:\n                            break\n                    \n                    for name in mask.roi_indices.keys():\n                        if name not in self.existing_roi_indices.keys():\n                            self.existing_roi_indices[name] = len(self.existing_roi_indices)\n                    mask.existing_roi_indices = self.existing_roi_indices\n\n                    if self.v:\n                        print(\"mask.GetSize():\", mask.GetSize())\n                    mask_arr = np.transpose(sitk.GetArrayFromImage(mask))\n                    \n                    if self.is_nnunet:\n                        sparse_mask = np.transpose(mask.generate_sparse_mask().mask_array)\n                        sparse_mask = sitk.GetImageFromArray(sparse_mask)  # convert the nparray to sitk image\n                        sparse_mask.CopyInformation(image)\n                        if \"_\".join(subject_id.split(\"_\")[1::]) in self.train:\n                            self.output(subject_id, sparse_mask, output_stream, nnunet_info=self.nnunet_info, label_or_image=\"labels\")  # rtstruct is label for nnunet\n                        else:\n                            self.output(subject_id, sparse_mask, output_stream, nnunet_info=self.nnunet_info, label_or_image=\"labels\", train_or_test=\"Ts\")\n                    else:\n                        # if there is only one ROI, sitk.GetArrayFromImage() will return a 3d array instead of a 4d array with one slice\n                        if len(mask_arr.shape) == 3:\n                            mask_arr = mask_arr.reshape(1, mask_arr.shape[0], mask_arr.shape[1], mask_arr.shape[2])\n                        \n                        if self.v:\n                            print(mask_arr.shape)\n\n                        roi_names_list = list(mask.roi_indices.keys())\n                        for i in range(mask_arr.shape[0]):\n                            new_mask = sitk.GetImageFromArray(np.transpose(mask_arr[i]))\n                            new_mask.CopyInformation(mask)\n                            new_mask = Segmentation(new_mask)\n                            mask_to_process = new_mask\n                            \n                            # output\n                            self.output(subject_id, mask_to_process, output_stream, True, roi_names_list[i])\n                    \n                    if hasattr(structure_set, \"metadata\") and structure_set.metadata is not None:\n                        metadata.update(structure_set.metadata)\n\n                    metadata[f\"metadata_{colname}\"] = [structure_set.roi_names]\n                    for roi, labels in mask.raw_roi_names.items():\n                        metadata[f\"raw_labels_{roi}\"] = labels                    \n\n                    print(subject_id, \"SAVED MASK ON\", conn_to)\n                \n                \n                \n                metadata[f\"output_folder_{colname}\"] = pathlib.Path(subject_id, colname).as_posix()\n            \n            # Saving all the metadata in multiple text files\n            metadata[\"Modalities\"] = str(list(subject_modalities))\n            metadata[\"numRTSTRUCTs\"] = num_rtstructs\n            if self.is_nnunet:\n                metadata[\"Train or Test\"] = \"train\" if \"_\".join(subject_id.split(\"_\")[1::]) in self.train else \"test\"\n            with open(pathlib.Path(self.output_directory,\".temp\",f'{subject_id}.pkl').as_posix(),'wb') as f:  # the continue flag depends on this being the last line in this method\n                pickle.dump(metadata,f)\n            return \n    \n    def save_data(self):\n        \"\"\"\n        Saves metadata about processing. \n        \"\"\"\n        files = glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())\n        for file in files:\n            filename = pathlib.Path(file).name\n            if filename == \"init_parameters.pkl\":\n                continue\n            subject_id = os.path.splitext(filename)[0]\n            with open(file,\"rb\") as f:\n                metadata = pickle.load(f)\n\n            self.output_df.loc[subject_id, metadata.keys()] = metadata.values()  # subject id targets the rows with that subject id and it is reassigning all the metadata values by key\n\n        folder_renames = {}\n        for col in self.output_df.columns:\n            if col.startswith(\"folder\"):\n                self.output_df[col] = self.output_df[col].apply(lambda x: x if not isinstance(x, str) else pathlib.Path(x).as_posix().split(self.input_directory)[1][1:]) # rel path, exclude the slash at the beginning\n                folder_renames[col] = f\"input_{col}\"\n        self.output_df.rename(columns=folder_renames, inplace=True)  # append input_ to the column name\n        self.output_df.to_csv(self.output_df_path)  # dataset.csv\n\n        shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n\n        # Save dataset json\n        if self.is_nnunet:  # dataset.json for nnunet and .sh file to run to process it\n            imagests_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n            images_test_location = imagests_path if os.path.exists(imagests_path) else None\n            # print(self.existing_roi_indices)\n            generate_dataset_json(pathlib.Path(self.output_directory, \"dataset.json\").as_posix(),\n                                  pathlib.Path(self.output_directory, \"imagesTr\").as_posix(),\n                                  images_test_location,\n                                  tuple(self.nnunet_info[\"modalities\"].keys()),\n                                  {v: k for k, v in self.existing_roi_indices.items()},\n                                  os.path.split(self.input_directory)[1])\n            _, child = os.path.split(self.output_directory)\n            shell_path = pathlib.Path(self.output_directory, child.split(\"_\")[1]+\".sh\").as_posix()\n            if os.path.exists(shell_path):\n                os.remove(shell_path)\n            with open(shell_path, \"w\", newline=\"\\n\") as f:\n                output = \"#!/bin/bash\\n\"\n                output += \"set -e\"\n                output += f'export nnUNet_raw_data_base=\"{self.base_output_directory}/nnUNet_raw_data_base\"\\n'\n                output += f'export nnUNet_preprocessed=\"{self.base_output_directory}/nnUNet_preprocessed\"\\n'\n                output += f'export RESULTS_FOLDER=\"{self.base_output_directory}/nnUNet_trained_models\"\\n\\n'\n                output += f'nnUNet_plan_and_preprocess -t {self.task_id} --verify_dataset_integrity\\n\\n'\n                output += 'for (( i=0; i<5; i++ ))\\n'\n                output += 'do\\n'\n                output += f'    nnUNet_train 3d_fullres nnUNetTrainerV2 {os.path.split(self.output_directory)[1]} $i --npz\\n'\n                output += 'done'\n                f.write(output)\n            markdown_report_images(self.output_directory, self.total_modality_counter)  # images saved to the output directory\n        \n        # Save summary info (factor into different file)\n        markdown_path = pathlib.Path(self.output_directory, \"report.md\").as_posix()\n        with open(markdown_path, \"w\", newline=\"\\n\") as f:\n            output = \"# Dataset Report\\n\\n\"\n            if not self.is_nnunet:\n                output += \"## Patients with broken DICOM references\\n\\n\"\n                output += \"<details>\\n\"\n                output += \"\\t<summary>Click to see the list of patients with broken DICOM references</summary>\\n\\n\\t\"\n                formatted_list = \"\\n\\t\".join(self.broken_patients)\n                output += f\"{formatted_list}\\n\"\n                output += \"</details>\\n\\n\"\n\n            if self.is_nnunet:\n                output += \"## Train Test Split\\n\\n\"\n                # pie_path = pathlib.Path(self.output_directory, \"markdown_images\", \"nnunet_train_test_pie.png\").as_posix()\n                pie_path = pathlib.Path(\"markdown_images\", \"nnunet_train_test_pie.png\").as_posix()\n                output += f\"![Pie Chart of Train Test Split]({pie_path})\\n\\n\"\n                output += \"## Image Modality Distribution\\n\\n\"\n                # bar_path = pathlib.Path(self.output_directory, \"markdown_images\", \"nnunet_modality_count.png\").as_posix()\n                bar_path = pathlib.Path(\"markdown_images\", \"nnunet_modality_count.png\").as_posix()\n                output += f\"![Pie Chart of Image Modality Distribution]({bar_path})\\n\\n\"\n            f.write(output)\n\n    def run(self):\n        \"\"\"Execute the pipeline, possibly in parallel.\n        \"\"\"\n        # Joblib prints progress to stdout if verbose > 50\n        verbose = 51 if self.v or self.show_progress else 0\n\n        subject_ids = self._get_loader_subject_ids()\n        patient_ids = []\n        if not self.dry_run:\n            for subject_id in subject_ids:\n                if subject_id.split(\"_\")[1::] not in patient_ids:\n                    patient_ids.append(\"_\".join(subject_id.split(\"_\")[1::]))\n            if self.is_nnunet:\n                custom_train = []\n                custom_test = []\n                if self.custom_train_test_split:\n                    patient_ids = [item for item in patient_ids if item not in self.custom_split[\"train\"] and item not in self.custom_split[\"test\"]]\n                    custom_test = self.custom_split[\"test\"]\n                    custom_train = self.custom_split[\"train\"]\n                if self.train_size == 1:\n                    self.train = patient_ids\n                    self.test = []\n                    self.train.extend(custom_train)\n                    self.test.extend(custom_test)\n                else:\n                    self.train, self.test = train_test_split(sorted(patient_ids), train_size=self.train_size, random_state=self.random_state)\n                    self.train.extend(custom_train)\n                    self.test.extend(custom_test)\n            else:\n                self.train, self.test = [], []\n            # Note that returning any SimpleITK object in process_one_subject is\n            # not supported yet, since they cannot be pickled\n            if os.path.exists(self.output_df_path) and not self.overwrite:\n                print(\"Dataset already processed...\")\n                shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n            else:\n                Parallel(n_jobs=self.n_jobs, verbose=verbose, require='sharedmem')(\n                        delayed(self._process_wrapper)(subject_id) for subject_id in subject_ids)\n                # for subject_id in subject_ids:\n                #     self._process_wrapper(subject_id)\n                self.broken_patients = []\n                if not self.is_nnunet:\n                    all_patient_names = glob.glob(pathlib.Path(self.input_directory, \"*\", \" \").as_posix()[0:-1])\n                    all_patient_names = [os.path.split(os.path.split(x)[0])[1] for x in all_patient_names]\n                    for e in all_patient_names:\n                        if e not in patient_ids:\n                            warnings.warn(f\"Patient {e} does not have proper DICOM references\")\n                            self.broken_patients.append(e)\n                self.save_data()\n\n\ndef main():\n    args = parser()\n    args_dict = vars(args)\n    \n    if args.continue_processing:\n        try:\n            with open(pathlib.Path(args.output_directory, \".temp\", \"init_parameters.pkl\").as_posix(), \"rb\") as f:\n                args_dict = dill.load(f)\n        except:\n            print(\"Could not resume processing. Starting processing from the beginning.\")\n\n    print('initializing AutoPipeline...')\n    pipeline = AutoPipeline(**args_dict)\n\n    if not args.dry_run:\n        print('starting AutoPipeline...')\n        pipeline.run()\n        print('finished AutoPipeline!')\n    else:\n        print('dry run complete, no processing done')\n\n    \"\"\"Print general summary info\"\"\"\n\n    \"\"\"Print nnU-Net specific info here:\n    * dataset.json can be found at /path/to/dataset/json\n    * You can train nnU-Net by cloning /path/to/nnunet/repo and run `nnUNet_plan_and_preprocess -t taskID` to let the nnU-Net package prepare\n    \"\"\"\n    print(f\"Outputted data to {args.output_directory}\")\n    csv_path = pathlib.Path(args.output_directory, \"dataset.csv\").as_posix()\n    print(f\"Dataset info found at {csv_path}\")\n    if args.nnunet:\n        json_path = pathlib.Path(args.output_directory, \"dataset.json\").as_posix()\n        print(f\"dataset.json for nnU-net can be found at {json_path}\")\n        print(\"You can train nnU-net by cloning https://github.com/MIC-DKFZ/nnUNet/ and run `nnUNet_plan_and_preprocess -t taskID` to let the nnU-Net package prepare\")\n\n\nif __name__ == \"__main__\":\n    main()",
    "qa_pairs": [
      {
        "question": "What is the purpose of the AutoPipeline class in this code?",
        "answer": "The AutoPipeline class is designed to process medical imaging data, particularly for the RADCURE dataset. It loads CT images and structure sets, resamples the images, and draws GTV contours using the resampled image. It can also prepare data for nnUNet training and inference."
      },
      {
        "question": "How does the code handle different modalities of medical images?",
        "answer": "The code handles different modalities (CT, MR, PT, RTDOSE, RTSTRUCT) separately in the process_one_subject method. Each modality has its own processing steps, such as resampling for CT/MR, dose calculation for RTDOSE, and mask generation for RTSTRUCT. The code also supports multi-modal inputs and can prepare data for nnUNet, which requires specific formatting for different modalities."
      },
      {
        "question": "What is the purpose of the 'dry_run' parameter in the AutoPipeline class?",
        "answer": "The 'dry_run' parameter allows the pipeline to be run without actually processing or writing any output files. When set to True, it simulates the pipeline execution, which is useful for testing the setup and configuration without modifying any data. This helps in verifying the pipeline's logic and identifying potential issues before running the actual processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class AutoPipeline(Pipeline):\n    def __init__(self,\n                 input_directory=\"\",\n                 output_directory=\"\",\n                 modalities=\"CT\",\n                 spacing=(1., 1., 0.),\n                 n_jobs=-1,\n                 visualize=False,\n                 missing_strategy=\"drop\",\n                 show_progress=False,\n                 warn_on_error=False,\n                 overwrite=False,\n                 nnunet=False,\n                 train_size=1.0,\n                 random_state=42,\n                 read_yaml_label_names=False,\n                 ignore_missing_regex=False,\n                 roi_yaml_path=\"\",\n                 custom_train_test_split=False,\n                 nnunet_inference=False,\n                 dataset_json_path=\"\",\n                 continue_processing=False,\n                 dry_run=False,\n                 verbose=False,\n                 update=False,\n                 roi_select_first=False,\n                 roi_separate=False):\n        # Initialize attributes\n        self.continue_processing = continue_processing\n        self.dry_run = dry_run\n        self.v = verbose\n\n        if dry_run:\n            nnunet = False\n            nnunet_inference = False\n\n        if dry_run and continue_processing:\n            raise ValueError(\"Cannot continue processing a dry run. Set --continue_processing to False to do a dry run.\")\n\n        if not dry_run and output_directory == \"\":\n            raise ValueError(\"Must specify an output directory\")\n        \n        # Configure input/output directories\n        self.input_directory = self.configure_directory(input_directory)\n        self.output_directory = self.configure_directory(output_directory)\n        \n        # Check/create output directory\n        if not os.path.exists(self.output_directory):\n            os.makedirs(self.output_directory)\n\n        # Check input directory exists\n        if not os.path.exists(self.input_directory):\n            raise FileNotFoundError(f\"Input directory {self.input_directory} does not exist\")\n        \n        # Continue processing checks\n        if not nnunet and continue_processing and not os.path.exists(pathlib.Path(self.output_directory, \".temp\").as_posix()):\n            raise FileNotFoundError(f\"Cannot continue processing. .temp directory does not exist in {self.output_directory}. Run without --continue_processing to start from scratch.\")\n\n        # Initialize other attributes and perform additional checks\n        # ...\n\n    def configure_directory(self, directory):\n        if not os.path.isabs(directory):\n            return pathlib.Path(os.getcwd(), directory).as_posix()\n        return pathlib.Path(directory).as_posix()",
        "complete": "class AutoPipeline(Pipeline):\n    def __init__(self,\n                 input_directory=\"\",\n                 output_directory=\"\",\n                 modalities=\"CT\",\n                 spacing=(1., 1., 0.),\n                 n_jobs=-1,\n                 visualize=False,\n                 missing_strategy=\"drop\",\n                 show_progress=False,\n                 warn_on_error=False,\n                 overwrite=False,\n                 nnunet=False,\n                 train_size=1.0,\n                 random_state=42,\n                 read_yaml_label_names=False,\n                 ignore_missing_regex=False,\n                 roi_yaml_path=\"\",\n                 custom_train_test_split=False,\n                 nnunet_inference=False,\n                 dataset_json_path=\"\",\n                 continue_processing=False,\n                 dry_run=False,\n                 verbose=False,\n                 update=False,\n                 roi_select_first=False,\n                 roi_separate=False):\n        # Initialize attributes\n        self.continue_processing = continue_processing\n        self.dry_run = dry_run\n        self.v = verbose\n\n        if dry_run:\n            nnunet = False\n            nnunet_inference = False\n\n        if dry_run and continue_processing:\n            raise ValueError(\"Cannot continue processing a dry run. Set --continue_processing to False to do a dry run.\")\n\n        if not dry_run and output_directory == \"\":\n            raise ValueError(\"Must specify an output directory\")\n        \n        # Configure input/output directories\n        self.input_directory = self.configure_directory(input_directory)\n        self.output_directory = self.configure_directory(output_directory)\n        \n        # Check/create output directory\n        if not os.path.exists(self.output_directory):\n            os.makedirs(self.output_directory)\n\n        # Check input directory exists\n        if not os.path.exists(self.input_directory):\n            raise FileNotFoundError(f\"Input directory {self.input_directory} does not exist\")\n        \n        # Continue processing checks\n        if not nnunet and continue_processing and not os.path.exists(pathlib.Path(self.output_directory, \".temp\").as_posix()):\n            raise FileNotFoundError(f\"Cannot continue processing. .temp directory does not exist in {self.output_directory}. Run without --continue_processing to start from scratch.\")\n\n        # Initialize other attributes\n        self.overwrite = overwrite\n        self.spacing = spacing\n        self.is_nnunet = nnunet\n        self.nnunet_info = {} if nnunet or nnunet_inference else None\n        self.train_size = train_size\n        self.random_state = random_state\n        self.label_names = {}\n        self.ignore_missing_regex = ignore_missing_regex\n        self.custom_train_test_split = custom_train_test_split\n        self.is_nnunet_inference = nnunet_inference\n        self.roi_select_first = roi_select_first\n        self.roi_separate = roi_separate\n\n        # Perform additional checks and initializations\n        self.check_nnunet_settings(nnunet, nnunet_inference, modalities, dataset_json_path)\n        self.load_roi_names(read_yaml_label_names, roi_yaml_path)\n        self.check_train_size()\n        self.check_custom_train_test_split()\n\n        # Initialize pipeline components\n        self.initialize_pipeline_components(input_directory, modalities, n_jobs, visualize, update, output_directory)\n\n        super().__init__(n_jobs=n_jobs,\n                         missing_strategy=missing_strategy,\n                         show_progress=show_progress,\n                         warn_on_error=warn_on_error)\n\n    def configure_directory(self, directory):\n        if not os.path.isabs(directory):\n            return pathlib.Path(os.getcwd(), directory).as_posix()\n        return pathlib.Path(directory).as_posix()\n\n    def check_nnunet_settings(self, nnunet, nnunet_inference, modalities, dataset_json_path):\n        if nnunet_inference:\n            if modalities not in [\"CT\", \"MR\"]:\n                raise ValueError(\"nnUNet inference can only be run on image files. Please set modalities to 'CT' or 'MR'\")\n            self.load_nnunet_inference_info(dataset_json_path)\n\n    def load_roi_names(self, read_yaml_label_names, roi_yaml_path):\n        if read_yaml_label_names:\n            self.label_names = self.load_yaml_file(roi_yaml_path or pathlib.Path(self.input_directory, \"roi_names.yaml\").as_posix())\n            self.validate_label_names()\n\n    def check_train_size(self):\n        if self.train_size == 1.0 and self.is_nnunet:\n            warnings.warn(\"Train size is 1, all data will be used for training\")\n        if self.train_size == 0.0 and self.is_nnunet:\n            warnings.warn(\"Train size is 0, all data will be used for testing\")\n        if self.train_size != 1 and not self.is_nnunet:\n            warnings.warn(\"Cannot run train/test split without nnunet, ignoring train_size\")\n        if self.train_size > 1 or self.train_size < 0 and self.is_nnunet:\n            raise ValueError(\"train_size must be between 0 and 1\")\n\n    def check_custom_train_test_split(self):\n        if self.custom_train_test_split and not self.is_nnunet:\n            raise ValueError(\"Cannot use custom train/test split without nnunet\")\n        if self.custom_train_test_split and self.is_nnunet:\n            self.load_custom_split()\n\n    def initialize_pipeline_components(self, input_directory, modalities, n_jobs, visualize, update, output_directory):\n        self.input = ImageAutoInput(input_directory, modalities, n_jobs, visualize, update)\n        self.output_df_path = pathlib.Path(output_directory, \"dataset.csv\").as_posix()\n        self.output_df = self.input.df_combined\n        self.output_streams = self.input.output_streams\n        self.resample = Resample(spacing=self.spacing)\n        self.make_binary_mask = StructureSetToSegmentation(roi_names=self.label_names, continuous=False)\n        self.output = ImageAutoOutput(output_directory, self.output_streams, self.nnunet_info, self.is_nnunet_inference)\n        self.existing_roi_indices = {\"background\": 0}\n\n    def load_yaml_file(self, file_path):\n        if os.path.exists(file_path):\n            with open(file_path, \"r\") as f:\n                try:\n                    return yaml.safe_load(f)\n                except yaml.YAMLError as exc:\n                    print(exc)\n        else:\n            raise FileNotFoundError(f\"No file named {os.path.basename(file_path)} found at {file_path}\")\n\n    def validate_label_names(self):\n        if not isinstance(self.label_names, dict):\n            raise ValueError(\"roi_names.yaml must parse as a dictionary\")\n        for k, v in self.label_names.items():\n            if not isinstance(v, (list, str)) or (isinstance(v, list) and not all(isinstance(a, str) for a in v)):\n                raise ValueError(f\"Label values must be either a list of strings or a string. Got {v} for {k}\")\n            if not isinstance(k, str):\n                raise ValueError(f\"Label names must be a string. Got {k} for {v}\")\n\n    def load_nnunet_inference_info(self, dataset_json_path):\n        if not os.path.exists(dataset_json_path):\n            raise FileNotFoundError(f\"No file named {dataset_json_path} found. Image modality definitions are required for nnUNet inference\")\n        with open(dataset_json_path, \"r\") as f:\n            self.nnunet_info[\"modalities\"] = {v: k.zfill(4) for k, v in json.load(f)[\"modality\"].items()}\n\n    def load_custom_split(self):\n        custom_split_path = pathlib.Path(self.input_directory, \"custom_train_test_split.yaml\").as_posix()\n        self.custom_split = self.load_yaml_file(custom_split_path)\n        self.validate_custom_split()"
      },
      {
        "partial": "def process_one_subject(self, subject_id):\n    if self.continue_processing and subject_id in self.finished_subjects:\n        return\n    \n    if self.overwrite or not self.subject_exists(subject_id):\n        if self.subject_already_processed(subject_id):\n            print(f\"{subject_id} already processed\")\n            return\n\n        print(\"Processing:\", subject_id)\n        read_results = self.input(subject_id)\n        print(subject_id, \" start\")\n        \n        metadata = {}\n        subject_modalities = set()\n        num_rtstructs = 0\n\n        for i, colname in enumerate(self.output_streams):\n            modality = colname.split(\"_\")[0]\n            subject_modalities.add(modality)\n            \n            output_stream = (\"_\").join([item for item in colname.split(\"_\") if not item.isnumeric()])\n\n            if self.v:\n                print(\"output_stream:\", output_stream)\n\n            if read_results[i] is None:\n                print(f\"The subject id: {subject_id} has no {colname}\")\n                continue\n\n            # Process image (CT/MR)\n            if modality in [\"CT\", \"MR\"]:\n                image ="
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "shutil",
        "glob",
        "pickle",
        "numpy",
        "warnings",
        "yaml",
        "json",
        "SimpleITK",
        "dill"
      ],
      "from_imports": [
        "aifc.Error",
        "imgtools.ops.StructureSetToSegmentation",
        "imgtools.pipeline.Pipeline",
        "imgtools.utils.nnunet.generate_dataset_json",
        "imgtools.utils.args.parser",
        "joblib.Parallel",
        "imgtools.modules.Segmentation",
        "sklearn.model_selection.train_test_split"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/filterNoisyCurves.R",
    "language": "R",
    "content": "#' Viability measurements in dose-reponse curves must remain stable or decrease\n#' monotonically reflecting response to the drug being tested.\n#' filterNoisyCurves flags dose-response curves that strongly violate these\n#' assumptions.\n#'\n#' @examples\n#' data(GDSCsmall)\n#' filterNoisyCurves(GDSCsmall)\n#'\n#' @param pSet [PharmacoSet] a PharmacoSet object\n#' @param epsilon `numeric` a value indicates assumed threshold for the\n#'   distance between to consecutive viability values on the drug-response curve\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv\n#' @param positive.cutoff.percent `numeric` This value indicates that function\n#'   may violate epsilon rule for how many points on drug-response curve\n#' @param mean.viablity `numeric` average expected viability value\n#' @param nthread `numeric` if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#'\n#' @return a list with two elements 'noisy' containing the rownames of the\n#'   noisy curves, and 'ok' containing the rownames of the non-noisy curves\n#'\n#' @export\nfilterNoisyCurves <- function(pSet, epsilon=25 , positive.cutoff.percent=.80,\n        mean.viablity=200, nthread=1) {\n\n    acceptable <- mclapply(rownames(sensitivityInfo(pSet)), function(xp) {\n        #for(xp in rownames(sensitivityInfo(pSet))){\n        drug.responses <- as.data.frame(apply(sensitivityRaw(pSet)[xp , ,], 2, as.numeric), stringsAsFactors=FALSE)\n        drug.responses <- drug.responses[complete.cases(drug.responses), ]\n        doses.no <- nrow(drug.responses)\n\n        drug.responses[, \"delta\"] <- .computeDelta(drug.responses$Viability)\n\n        delta.sum <- sum(drug.responses$delta, na.rm = TRUE)\n\n        max.cum.sum <- .computeCumSumDelta(drug.responses$Viability)\n\n        if (\n            (table(drug.responses$delta < epsilon)[\"TRUE\"] >=\n                (doses.no * positive.cutoff.percent)) &\n            (delta.sum < epsilon) &\n            (max.cum.sum < (2 * epsilon)) &\n            (mean(drug.responses$Viability) < mean.viablity)\n        ) {\n            return (xp)\n        }\n    }, mc.cores=nthread)\n    acceptable <- unlist(acceptable)\n    noisy <- setdiff(rownames(sensitivityInfo(pSet)), acceptable)\n    return(list(\"noisy\"=noisy, \"ok\"=acceptable))\n}\n\n.computeDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if (trunc) {\n        return(c(pmin(100, xx[seq(2,length(xx))]) - pmin(100, xx[seq_along(xx)-1]), 0))\n    } else {\n        return(c(xx[seq(2, length(xx))] - xx[seq_along(xx) - 1]), 0)\n    }\n}\n\n#' @importFrom utils combn\n.computeCumSumDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if(trunc) {\n        xx <- pmin(xx, 100)\n    }\n    tt <- t(combn(seq_along(xx), 2 , simplify = TRUE))\n    tt <- tt[which(((tt[,2] - tt[,1]) >= 2) == TRUE),]\n    cum.sum <- unlist(lapply(seq_len(nrow(tt)), function(x) { xx[tt[x, 2]] - xx[tt[x, 1]]}))\n    return(max(cum.sum))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `filterNoisyCurves` function and what are its main input parameters?",
        "answer": "The `filterNoisyCurves` function is designed to flag dose-response curves that violate assumptions about viability measurements in drug response experiments. It takes a PharmacoSet object as its main input, along with optional parameters like epsilon (threshold for consecutive viability differences), positive.cutoff.percent (allowed violation percentage), mean.viability (average expected viability), and nthread (for parallelization)."
      },
      {
        "question": "Explain the logic behind determining if a curve is 'acceptable' in the `filterNoisyCurves` function.",
        "answer": "A curve is considered 'acceptable' if it meets four criteria: 1) At least 80% (by default) of the delta values are less than epsilon, 2) The sum of all delta values is less than epsilon, 3) The maximum cumulative sum of deltas is less than twice epsilon, and 4) The mean viability is less than the specified mean.viability threshold. These criteria ensure that the curve is mostly monotonically decreasing and within expected viability ranges."
      },
      {
        "question": "What is the purpose of the `.computeCumSumDelta` function and how does it contribute to the noise filtering process?",
        "answer": "The `.computeCumSumDelta` function calculates the maximum cumulative sum of differences between all pairs of viability values that are at least two positions apart in the dose-response curve. This helps identify large increases in viability over the course of the experiment, which would be unexpected in a typical dose-response scenario. By comparing this value to a threshold in the main function, it helps filter out curves with unusual or noisy patterns."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filterNoisyCurves <- function(pSet, epsilon=25, positive.cutoff.percent=.80, mean.viablity=200, nthread=1) {\n    acceptable <- mclapply(rownames(sensitivityInfo(pSet)), function(xp) {\n        drug.responses <- as.data.frame(apply(sensitivityRaw(pSet)[xp , ,], 2, as.numeric), stringsAsFactors=FALSE)\n        drug.responses <- drug.responses[complete.cases(drug.responses), ]\n        doses.no <- nrow(drug.responses)\n\n        drug.responses[, \"delta\"] <- .computeDelta(drug.responses$Viability)\n\n        delta.sum <- sum(drug.responses$delta, na.rm = TRUE)\n\n        max.cum.sum <- .computeCumSumDelta(drug.responses$Viability)\n\n        if (\n            # Complete the condition here\n        ) {\n            return (xp)\n        }\n    }, mc.cores=nthread)\n    acceptable <- unlist(acceptable)\n    noisy <- setdiff(rownames(sensitivityInfo(pSet)), acceptable)\n    return(list(\"noisy\"=noisy, \"ok\"=acceptable))\n}",
        "complete": "filterNoisyCurves <- function(pSet, epsilon=25, positive.cutoff.percent=.80, mean.viablity=200, nthread=1) {\n    acceptable <- mclapply(rownames(sensitivityInfo(pSet)), function(xp) {\n        drug.responses <- as.data.frame(apply(sensitivityRaw(pSet)[xp , ,], 2, as.numeric), stringsAsFactors=FALSE)\n        drug.responses <- drug.responses[complete.cases(drug.responses), ]\n        doses.no <- nrow(drug.responses)\n\n        drug.responses[, \"delta\"] <- .computeDelta(drug.responses$Viability)\n\n        delta.sum <- sum(drug.responses$delta, na.rm = TRUE)\n\n        max.cum.sum <- .computeCumSumDelta(drug.responses$Viability)\n\n        if (\n            (table(drug.responses$delta < epsilon)[\"TRUE\"] >= (doses.no * positive.cutoff.percent)) &&\n            (delta.sum < epsilon) &&\n            (max.cum.sum < (2 * epsilon)) &&\n            (mean(drug.responses$Viability) < mean.viablity)\n        ) {\n            return (xp)\n        }\n    }, mc.cores=nthread)\n    acceptable <- unlist(acceptable)\n    noisy <- setdiff(rownames(sensitivityInfo(pSet)), acceptable)\n    return(list(\"noisy\"=noisy, \"ok\"=acceptable))\n}"
      },
      {
        "partial": ".computeCumSumDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if(trunc) {\n        xx <- pmin(xx, 100)\n    }\n    tt <- t(combn(seq_along(xx), 2 , simplify = TRUE))\n    tt <- tt[which(((tt[,2] - tt[,1]) >= 2) == TRUE),]\n    # Complete the function here\n}",
        "complete": ".computeCumSumDelta <- function(xx, trunc = TRUE) {\n    xx <- as.numeric(xx)\n    if(trunc) {\n        xx <- pmin(xx, 100)\n    }\n    tt <- t(combn(seq_along(xx), 2 , simplify = TRUE))\n    tt <- tt[which(((tt[,2] - tt[,1]) >= 2) == TRUE),]\n    cum.sum <- unlist(lapply(seq_len(nrow(tt)), function(x) { xx[tt[x, 2]] - xx[tt[x, 1]]} ))\n    return(max(cum.sum))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_4_split_utils.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\n\ntest_that(\"strSplit splits a character vector into a matrix based on a delimiter\", {\n  input <- \"Hello,World\"\n  expected <- matrix(c(\"Hello\", \"World\"), ncol = 2, byrow = TRUE)\n  result <- strSplit(input, \",\")\n  expect_equal(result, expected)\n})\n\ntest_that(\".strSplitFinite splits a string into multiple substrings based on a delimiter\", {\n  input <- \"Hello,World,How,Are,You\"\n  result <- .strSplitFinite(input, \",\", 3, fixed = TRUE)\n  expected <- c(\"Hello\", \"World\", \"How,Are,You\")\n  expect_equal(result[[1]], expected)\n\n  result2 <- .strSplitFinite(input, \",\", 4, fixed = TRUE)\n  expected2 <- c(\"Hello\", \"World\", \"How\", \"Are,You\")\n  expect_equal(result2[[1]], expected2)\n\n  result3 <- .strSplitFinite(input, \",\", 3, fixed = FALSE)\n  expected3 <- c(\"Hello\", \"World\", \"How,Are,You\")\n  expect_equal(result3[[1]], expected3)\n})\n\ntest_that(\".strSplitInfinite splits a character vector into substrings based on a delimiter\", {\n  input <- c(\"apple,banana,orange\", \"cat,dog,rabbit\")\n  expected <- list(c(\"apple\", \"banana\", \"orange\"), c(\"cat\", \"dog\", \"rabbit\"))\n  result <- .strSplitInfinite(input, \",\", fixed = TRUE)\n  expect_equal(result, expected)\n})\n\ntest_that(\".splitCol splits a column into a character list\", {\n  input <- data.table(col = c(\"apple;banana\", \"orange;grape\"))\n  result <- .splitCol(input, \"col\", split = \";\")\n  expected <- data.table(col = list(c(\"apple\", \"banana\"), c(\"orange\", \"grape\")))\n  expect_equal(result, expected)\n\n  result2 <- .splitCol(input, \"col\", split = \"; \")\n  expected2 <- data.table(col = c(list(\"apple;banana\"), list(\"orange;grape\")))\n  expect_equal(result2, expected2)\n  \n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `strSplit` function in the given code snippet, and how does it differ from `.strSplitFinite`?",
        "answer": "The `strSplit` function splits a character vector into a matrix based on a delimiter. It returns a matrix with each element of the input vector split into columns. On the other hand, `.strSplitFinite` splits a string into a specified number of substrings based on a delimiter. The main difference is that `strSplit` always splits the entire string, while `.strSplitFinite` allows you to control the number of splits and returns the remaining unsplit portion as the last element."
      },
      {
        "question": "How does the `fixed` parameter in `.strSplitFinite` and `.strSplitInfinite` functions affect their behavior?",
        "answer": "The `fixed` parameter in both `.strSplitFinite` and `.strSplitInfinite` functions determines whether the delimiter should be interpreted as a fixed string or as a regular expression. When `fixed = TRUE`, the delimiter is treated as a literal string. When `fixed = FALSE`, the delimiter is interpreted as a regular expression pattern. This allows for more flexible splitting options, especially when dealing with complex patterns or multiple delimiters."
      },
      {
        "question": "What is the purpose of the `.splitCol` function, and how does it handle different types of delimiters?",
        "answer": "The `.splitCol` function is designed to split a column in a data.table into a character list based on a specified delimiter. It takes a data.table, a column name, and a split parameter as input. The function can handle different types of delimiters, including single characters (like ';') and multi-character strings (like '; '). When the delimiter exactly matches the split parameter, it splits the column values into a list of character vectors. If the delimiter doesn't match exactly, it treats the entire column value as a single element in the resulting list."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"strSplit splits a character vector into a matrix based on a delimiter\", {\n  input <- \"Hello,World\"\n  expected <- matrix(c(\"Hello\", \"World\"), ncol = 2, byrow = TRUE)\n  result <- strSplit(input, \",\")\n  # Add assertion here\n})",
        "complete": "test_that(\"strSplit splits a character vector into a matrix based on a delimiter\", {\n  input <- \"Hello,World\"\n  expected <- matrix(c(\"Hello\", \"World\"), ncol = 2, byrow = TRUE)\n  result <- strSplit(input, \",\")\n  expect_equal(result, expected)\n})"
      },
      {
        "partial": "test_that(\".splitCol splits a column into a character list\", {\n  input <- data.table(col = c(\"apple;banana\", \"orange;grape\"))\n  result <- .splitCol(input, \"col\", split = \";\")\n  # Add assertion for the first test case\n\n  result2 <- .splitCol(input, \"col\", split = \"; \")\n  # Add assertion for the second test case\n})",
        "complete": "test_that(\".splitCol splits a column into a character list\", {\n  input <- data.table(col = c(\"apple;banana\", \"orange;grape\"))\n  result <- .splitCol(input, \"col\", split = \";\")\n  expected <- data.table(col = list(c(\"apple\", \"banana\"), c(\"orange\", \"grape\")))\n  expect_equal(result, expected)\n\n  result2 <- .splitCol(input, \"col\", split = \"; \")\n  expected2 <- data.table(col = c(list(\"apple;banana\"), list(\"orange;grape\")))\n  expect_equal(result2, expected2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/connectivityScore.R",
    "language": "R",
    "content": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  CoreGx::connectivityScore(x, y, method, nperm, nthread, gwc.method, ...)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@inherit` and `@inheritParams` tags in the function documentation?",
        "answer": "The `@inherit` tag is used to inherit the entire documentation from the `CoreGx::connectivityScore` function, while `@inheritParams` is used to inherit the parameter documentation from the same function. This allows the new function to reuse the documentation from the original function without duplicating it."
      },
      {
        "question": "What does the `...` argument in the function signature represent, and why is it important?",
        "answer": "The `...` (ellipsis) argument allows additional arguments to be passed to the function. It's important because it enables the function to accept and forward any extra parameters to the `CoreGx::connectivityScore` function, maintaining flexibility and compatibility with the original function's interface."
      },
      {
        "question": "How does this function relate to the `CoreGx::connectivityScore` function, and what is its main purpose?",
        "answer": "This function is a wrapper for the `CoreGx::connectivityScore` function. Its main purpose is to provide a simplified interface to the original function, potentially within a different package or namespace. It maintains the same functionality and parameters as the original function but allows for easier access or customization within the current package context."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  # Complete the function body\n}",
        "complete": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  CoreGx::connectivityScore(x, y, method, nperm, nthread, gwc.method, ...)\n}"
      },
      {
        "partial": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, ...) \n{\n  # Complete the function signature and body\n}",
        "complete": "#' Function computing connectivity scores between two signatures\n#' \n#' @inherit CoreGx::connectivityScore\n#' @inheritParams CoreGx::connectivityScore\n#' \n#' @export\nconnectivityScore <- \n  function(x, y, method=c(\"gsea\", \"fgsea\", \"gwc\"), nperm=1e4, nthread=1, \n           gwc.method=c(\"spearman\", \"pearson\"), ...) \n{\n  CoreGx::connectivityScore(x, y, method, nperm, nthread, gwc.method, ...)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/cellosaurus_helpers.R",
    "language": "R",
    "content": "#' Create a query list for Cellosaurus database\n#'\n#' This function creates a query list for the Cellosaurus database based on the provided IDs and fields.\n#' The query list is used to retrieve specific information from the database.\n#'\n#' @param ids A vector of IDs to be included in the query list.\n#' @param from A character vector specifying the fields to be included in the query list.\n#'             If the length of 'from' is 1, the same field will be used for all IDs.\n#'             If the length of 'from' is equal to the length of 'ids', each ID will be paired with its corresponding field.\n#'             Otherwise, an error will be thrown.\n#' @param fuzzy A logical value indicating whether to perform a fuzzy search. Default is FALSE.\n#'\n#' @return A character vector representing the query list.\n#'\n#' @examples\n#' AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), \"Accession\")\n#' # Returns: \"Accession:ID1\" \"Accession:ID2\" \"Accession:ID3\"\n#'\n#' AnnotationGx:::.create_cellosaurus_queries(c(\"ID1\", \"ID2\", \"ID3\"), c(\"Accession\", \"Name\", \"Species\"))\n#' # Returns: \"Accession:ID1\" \"Name:ID2\" \"Species:ID3\"\n#'\n#' @keywords internal\n#' @noRd\n.create_cellosaurus_queries <- function(ids, from, fuzzy = FALSE) {\n\n  if(fuzzy){\n    ids <- paste0(cleanCharacterStrings(ids), \"~\")\n  }\n\n  # either from has to be one field, or the same length as ids\n  if (length(from) == 1) {\n    return(paste0(from, \":\", ids))\n  }\n  if (length(from) != length(ids)) {\n    stop(\"Length of 'from' must be 1 or the same length as 'ids'\")\n  }\n  sapply(1:length(ids), function(i) {\n    paste(from[i], ids[i], sep = \":\")\n  })\n}\n\n\n\n#' Build a Cellosaurus API request\n#'\n#' This function builds a Cellosaurus API request based on the provided parameters.\n#'\n#' @param query A character vector specifying the query terms for the Cellosaurus API.\n#' @param to A character vector specifying the fields to include in the API response.\n#' @param numResults An integer specifying the maximum number of results to return.\n#' @param apiResource A character string specifying the API resource to query.\n#' @param output A character string specifying the desired output format of the API response.\n#' @param sort A character string specifying the field to sort the results by.\n#' @param query_only A logical value indicating whether to return only the constructed URL without making the request.\n#' @param ... Additional arguments to be passed to the function.\n#'\n#' @return A character string representing the constructed URL for the Cellosaurus API request.\n#'\n#' @examples\n#' .build_cellosaurus_request(query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n#'                           numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n#'                           query_only = FALSE)\n#' \n#' @keywords internal\n#' @noRd\n.build_cellosaurus_request <- function(\n    query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n    numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n    query_only = FALSE,  ...) {\n  \n  checkmate::assert_character(c(query, output))\n  checkmate::assert_choice(apiResource, c(\"search/cell-line\", \"cell-line\", \"release-info\"))\n  checkmate::assert_choice(output, c(\"TSV\", \"TXT\", \"JSON\", \"XML\"))\n\n  base_url <- \"https://api.cellosaurus.org\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, apiResource)\n\n  opts <- list()\n  \n  if(apiResource == \"search/cell-line\"){\n  \"https://api.cellosaurus.org/search/cell-line?q=idsy%3ADOR%2013&sort=ac%20asc&fields=ac%2Cid%2Csy%2Cmisspelling%2Cdr%2Ccc&format=txt&rows=10000\"\n    opts$q <- paste0(query, collapse = \" \")\n  } else if(apiResource == \"cell-line\"){\n    url$path <- .buildURL(url$path, query)\n  }\n\n  if (!is.null(sort)) {\n    opts$sort <- paste0(sort, \" asc\")\n  }\n\n  opts$fields <- paste0(to, collapse = \",\")\n  opts$format <- tolower(output)\n  opts$rows <- numResults\n\n\n  url$query <- opts\n  url <- url |> httr2::url_build()\n  if (query_only) {\n    return(url)\n  }\n  url |> .build_request()\n}\n\n\n\n#' Get the Cellosaurus schema\n#'\n#' This function retrieves the Cellosaurus schema from the Cellosaurus API.\n#' It internally calls the `.buildURL()`, `.build_request()`, `.perform_request()`,\n#' and `.parse_resp_json()` functions to construct the API URL, send the request,\n#' and parse the response.\n#'\n#' @return A list representing the Cellosaurus schema.\n#'\n#' @keywords internal\n#' @noRd\n.cellosaurus_schema <- function() {\n  url <- .buildURL(\"https://api.cellosaurus.org/openapi.json\")\n  request <- .build_request(url)\n\n  resp <- .perform_request(request)\n  .parse_resp_json(resp)\n}\n\n\n\n\n\n#' Internal function to return the list of external resources available in Cellosaurus\n#' @return A character vector of external resources available in Cellosaurus\n#'\n#' @keywords internal\n#' @noRd\n.cellosaurus_extResources <- function() {\n  c(\n    \"4DN\", \"Abcam\", \"ABCD\", \"ABM\", \"AddexBio\", \"ArrayExpress\",\n    \"ATCC\", \"BCGO\", \"BCRC\", \"BCRJ\", \"BEI_Resources\",\n    \"BioGRID_ORCS_Cell_line\", \"BTO\", \"BioSample\", \"BioSamples\",\n    \"cancercelllines\", \"CancerTools\", \"CBA\", \"CCLV\", \"CCRID\",\n    \"CCTCC\", \"Cell_Biolabs\", \"Cell_Model_Passport\", \"CGH-DB\",\n    \"ChEMBL-Cells\", \"ChEMBL-Targets\", \"CLDB\", \"CLO\", \"CLS\",\n    \"ColonAtlas\", \"Coriell\", \"Cosmic\", \"Cosmic-CLP\", \"dbGAP\",\n    \"dbMHC\", \"DepMap\", \"DGRC\", \"DiscoverX\", \"DSHB\", \"DSMZ\",\n    \"DSMZCellDive\", \"EBiSC\", \"ECACC\", \"EFO\", \"EGA\", \"ENCODE\",\n    \"ESTDAB\", \"FCDI\", \"FCS-free\", \"FlyBase_Cell_line\", \"GDSC\",\n    \"GeneCopoeia\", \"GEO\", \"HipSci\", \"HIVReagentProgram\", \"Horizon_Discovery\",\n    \"hPSCreg\", \"IARC_TP53\", \"IBRC\", \"ICLC\", \"ICLDB\", \"IGRhCellID\",\n    \"IGSR\", \"IHW\", \"Imanis\", \"Innoprot\", \"IPD-IMGT/HLA\", \"ISCR\",\n    \"IZSLER\", \"JCRB\", \"KCB\", \"KCLB\", \"Kerafast\", \"KYinno\", \"LiGeA\",\n    \"LIMORE\", \"LINCS_HMS\", \"LINCS_LDP\", \"Lonza\", \"MCCL\", \"MeSH\",\n    \"MetaboLights\", \"Millipore\", \"MMRRC\", \"NCBI_Iran\", \"NCI-DTP\", \"NHCDR\",\n    \"NIHhESC\", \"NISES\", \"NRFC\", \"PerkinElmer\", \"PharmacoDB\", \"PRIDE\",\n    \"Progenetix\", \"PubChem_Cell_line\", \"RCB\", \"Rockland\", \"RSCB\", \"SKIP\",\n    \"SKY/M-FISH/CGH\", \"SLKBase\", \"TKG\", \"TNGB\", \"TOKU-E\", \"Ubigene\",\n    \"WiCell\", \"Wikidata\", \"Ximbio\"\n  )\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.create_cellosaurus_queries` function and how does it handle different input scenarios?",
        "answer": "The `.create_cellosaurus_queries` function creates a query list for the Cellosaurus database based on provided IDs and fields. It handles three scenarios: 1) If a single field is provided, it pairs it with all IDs. 2) If the number of fields matches the number of IDs, it pairs them one-to-one. 3) If the lengths don't match, it throws an error. The function also supports fuzzy search by appending '~' to cleaned IDs when the `fuzzy` parameter is TRUE."
      },
      {
        "question": "How does the `.build_cellosaurus_request` function construct the API URL, and what are its key parameters?",
        "answer": "The `.build_cellosaurus_request` function constructs a Cellosaurus API URL by combining a base URL with various parameters. Key parameters include: `query` for search terms, `to` for specifying response fields, `numResults` for limiting results, `apiResource` for choosing the API endpoint, `output` for response format, and `sort` for result ordering. It uses `httr2::url_parse` and `httr2::url_build` to construct the URL, and can optionally return just the URL without making the request if `query_only` is TRUE."
      },
      {
        "question": "What is the purpose of the `.cellosaurus_extResources` function and how might it be used in the context of the Cellosaurus API?",
        "answer": "The `.cellosaurus_extResources` function returns a character vector containing a list of external resources available in Cellosaurus. This function could be used to validate or suggest external resource names when querying the Cellosaurus API. For example, it might be used to filter or validate the 'to' parameter in the `.build_cellosaurus_request` function, ensuring that only valid external resources are requested in the API call."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".create_cellosaurus_queries <- function(ids, from, fuzzy = FALSE) {\n  if(fuzzy){\n    ids <- paste0(cleanCharacterStrings(ids), \"~\")\n  }\n\n  if (length(from) == 1) {\n    return(paste0(from, \":\", ids))\n  }\n  if (length(from) != length(ids)) {\n    stop(\"Length of 'from' must be 1 or the same length as 'ids'\")\n  }\n  # Complete the function here\n}",
        "complete": ".create_cellosaurus_queries <- function(ids, from, fuzzy = FALSE) {\n  if(fuzzy){\n    ids <- paste0(cleanCharacterStrings(ids), \"~\")\n  }\n\n  if (length(from) == 1) {\n    return(paste0(from, \":\", ids))\n  }\n  if (length(from) != length(ids)) {\n    stop(\"Length of 'from' must be 1 or the same length as 'ids'\")\n  }\n  sapply(1:length(ids), function(i) {\n    paste(from[i], ids[i], sep = \":\")\n  })\n}"
      },
      {
        "partial": ".build_cellosaurus_request <- function(\n    query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n    numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n    query_only = FALSE,  ...) {\n  \n  checkmate::assert_character(c(query, output))\n  checkmate::assert_choice(apiResource, c(\"search/cell-line\", \"cell-line\", \"release-info\"))\n  checkmate::assert_choice(output, c(\"TSV\", \"TXT\", \"JSON\", \"XML\"))\n\n  base_url <- \"https://api.cellosaurus.org\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, apiResource)\n\n  opts <- list()\n  \n  # Complete the function here\n}",
        "complete": ".build_cellosaurus_request <- function(\n    query = c(\"id:HeLa\"), to = c(\"id\", \"ac\", \"hi\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\"),\n    numResults = 1, apiResource = \"search/cell-line\", output = \"TSV\", sort = \"ac\",\n    query_only = FALSE,  ...) {\n  \n  checkmate::assert_character(c(query, output))\n  checkmate::assert_choice(apiResource, c(\"search/cell-line\", \"cell-line\", \"release-info\"))\n  checkmate::assert_choice(output, c(\"TSV\", \"TXT\", \"JSON\", \"XML\"))\n\n  base_url <- \"https://api.cellosaurus.org\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, apiResource)\n\n  opts <- list()\n  \n  if(apiResource == \"search/cell-line\"){\n    opts$q <- paste0(query, collapse = \" \")\n  } else if(apiResource == \"cell-line\"){\n    url$path <- .buildURL(url$path, query)\n  }\n\n  if (!is.null(sort)) {\n    opts$sort <- paste0(sort, \" asc\")\n  }\n\n  opts$fields <- paste0(to, collapse = \",\")\n  opts$format <- tolower(output)\n  opts$rows <- numResults\n\n  url$query <- opts\n  url <- url |> httr2::url_build()\n  if (query_only) {\n    return(url)\n  }\n  url |> .build_request()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_unichem_helpers.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Valid endpoint returns correct URL\", {\n  endpoint <- \"compounds\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  actual_url <- .build_unichem_query(endpoint)\n  expect_equal(actual_url, expected_url)\n})\n\ntest_that(\"Invalid endpoint throws an error\", {\n  endpoint <- \"invalid_endpoint\"\n  expect_error(.build_unichem_query(endpoint))\n})\n\ntest_that(\"Query only option returns httr2::httr2_url object\", {\n  endpoint <- \"images\"\n  query_only <- TRUE\n  expected_class <- \"httr2_url\"\n  actual_url <- .build_unichem_query(endpoint, query_only)\n  expect_class(actual_url, expected_class)\n})\n\n\ntest_that(\"Valid compound request is built correctly\", {\n  type <- \"uci\"\n  compound <- \"538323\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound\n  )\n  actual_request <- .build_unichem_compound_req(type, compound)\n  expect_equal(actual_request$url, expected_url)\n  expect_equal(actual_request$body$data, expected_body)\n})\n\ntest_that(\"Valid sourceID compound request is built correctly\", {\n  type <- \"sourceID\"\n  compound <- \"2244\"\n  sourceID <- 22\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound,\n    sourceID = sourceID\n  )\n  actual_request <- .build_unichem_compound_req(type, compound, sourceID)\n  expect_equal(actual_request$url, expected_url)\n  expect_equal(actual_request$body$data, expected_body)\n\n\n  response <- actual_request |> \n    .perform_request() |>  \n    .parse_resp_json()  \n\n  checkmate::expect_names(\n    names(response), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(response$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n  \n\n})\n\ntest_that(\"Invalid type throws an error\", {\n  type <- \"invalid_type\"\n  compound <- \"538323\"\n  expect_error(.build_unichem_compound_req(type, compound))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_unichem_query()` function in this code, and how is it being tested?",
        "answer": "The `.build_unichem_query()` function appears to construct URLs for the UniChem API. It's being tested for three scenarios: 1) It correctly builds a URL for a valid endpoint, 2) It throws an error for an invalid endpoint, and 3) It returns an `httr2_url` object when the `query_only` parameter is set to TRUE."
      },
      {
        "question": "How does the `.build_unichem_compound_req()` function handle different types of compound requests, and what are its parameters?",
        "answer": "The `.build_unichem_compound_req()` function builds requests for compound information. It takes parameters `type`, `compound`, and optionally `sourceID`. It constructs a request with a URL and a body containing these parameters. The function handles different types of requests, including 'uci' and 'sourceID', adjusting the body of the request accordingly."
      },
      {
        "question": "What assertions are made about the structure of the API response in the last test case, and why might these be important?",
        "answer": "The last test case checks the structure of the API response using `checkmate::expect_names()`. It verifies that the response contains expected top-level keys ('compounds', 'notFound', 'response', 'totalCompounds') and that the 'compounds' object has expected keys ('inchi', 'sources', 'standardInchiKey', 'uci'). These assertions are important to ensure the API response maintains a consistent structure, which is crucial for reliable data parsing and processing in the application."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Valid endpoint returns correct URL\", {\n  endpoint <- \"compounds\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  actual_url <- .build_unichem_query(endpoint)\n  # Complete the test\n})",
        "complete": "test_that(\"Valid endpoint returns correct URL\", {\n  endpoint <- \"compounds\"\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  actual_url <- .build_unichem_query(endpoint)\n  expect_equal(actual_url, expected_url)\n})"
      },
      {
        "partial": "test_that(\"Valid sourceID compound request is built correctly\", {\n  type <- \"sourceID\"\n  compound <- \"2244\"\n  sourceID <- 22\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound,\n    sourceID = sourceID\n  )\n  actual_request <- .build_unichem_compound_req(type, compound, sourceID)\n  # Complete the test assertions and response checking\n})",
        "complete": "test_that(\"Valid sourceID compound request is built correctly\", {\n  type <- \"sourceID\"\n  compound <- \"2244\"\n  sourceID <- 22\n  expected_url <- \"https://www.ebi.ac.uk/unichem/api/v1/compounds\"\n  expected_body <- list(\n    type = type,\n    compound = compound,\n    sourceID = sourceID\n  )\n  actual_request <- .build_unichem_compound_req(type, compound, sourceID)\n  expect_equal(actual_request$url, expected_url)\n  expect_equal(actual_request$body$data, expected_body)\n\n  response <- actual_request |> \n    .perform_request() |>  \n    .parse_resp_json()  \n\n  checkmate::expect_names(\n    names(response), \n    subset.of=c(\"compounds\", \"notFound\", \"response\", \"totalCompounds\"))\n\n  checkmate::expect_names(\n    names(response$compounds),\n    subset.of=c(\"inchi\", \"sources\", \"standardInchiKey\", \"uci\")\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/args.py",
    "language": "py",
    "content": "from argparse import ArgumentParser\n\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    # arguments\n    parser.add_argument(\"input_directory\", type=str,\n                        help=\"Path to top-level directory of dataset.\")\n\n    parser.add_argument(\"output_directory\", type=str,\n                        help=\"Path to output directory to save processed images.\")\n\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\",\n                        help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n\n    parser.add_argument(\"--visualize\", default=False, action=\"store_true\",\n                        help=\"Whether to visualize the data graph\")\n\n    parser.add_argument(\"--spacing\", nargs=3, type=float, default=(1., 1., 0.),\n                        help=\"The resampled voxel spacing in  (x, y, z) directions.\")\n\n    parser.add_argument(\"--n_jobs\", type=int, default=-1,\n                        help=\"The number of parallel processes to use.\")\n\n    parser.add_argument(\"--show_progress\", action=\"store_true\",\n                        help=\"Whether to print progress to standard output.\")\n\n    parser.add_argument(\"--warn_on_error\", default=False, action=\"store_true\",\n                        help=\"Whether to warn on error.\")\n\n    parser.add_argument(\"--overwrite\", default=False, action=\"store_true\",\n                        help=\"Whether to write output files even if existing output files exist.\")\n    \n    parser.add_argument(\"--nnunet\", default=False, action=\"store_true\",\n                        help=\"Whether to make the output conform to nnunet requirements.\")\n\n    parser.add_argument(\"--train_size\", type=float, default=1.0,\n                        help=\"The proportion of data to be used for training, as a decimal.\")\n\n    parser.add_argument(\"--random_state\", type=int, default=42,\n                        help=\"The random state to be used for the train-test-split.\")\n\n    parser.add_argument(\"--read_yaml_label_names\", default=False, action=\"store_true\",\n                        help=\"Whether to read the label names from roi_names.yaml in the input directory.\")\n\n    parser.add_argument(\"--ignore_missing_regex\", default=False, action=\"store_true\",\n                        help=\"Whether to ignore patients with no ROI regexes that match the given ones. Will throw an error on patients without matches if this is not set.\")\n\n    parser.add_argument(\"--roi_yaml_path\", type=str, default=\"\",\n                        help=\"Path to the YAML file defining ROI regexes\")\n\n    parser.add_argument(\"--custom_train_test_split\", default=False, action=\"store_true\",\n                        help=\"Whether to use a custom train-test-split, stored in custom_train_test_split.yaml in the input directory.\")\n\n    parser.add_argument(\"--nnunet_inference\", default=False, action=\"store_true\",\n                        help=\"Whether to generate data for nnUNet inference.\")\n    \n    parser.add_argument(\"--dataset_json_path\", type=str, default=\"\",\n                        help=\"Path to the dataset.json file defining image modality indices for nnUNet inference.\")\n\n    parser.add_argument(\"--continue_processing\", default=False, action=\"store_true\",\n                        help=\"Whether to continue processing a partially completed dataset.\")\n    \n    parser.add_argument(\"--dry_run\", default=False, action=\"store_true\",\n                        help=\"Make a dry run of the pipeline, only producing the edge table and dataset.csv.\")\n\n    parser.add_argument(\"--verbose\", default=False, action=\"store_true\",\n                        help=\"Verbose output flag.\")\n\n    parser.add_argument(\"--update\", default=False, action=\"store_true\",\n                        help=\"Update crawled index. In other words, process from scratch.\")\n\n    parser.add_argument(\"--roi_select_first\", default=False, action=\"store_true\",\n                        help=\"Only select first matching regex/ROI name.\")\n\n    parser.add_argument(\"--roi_separate\", default=False, action=\"store_true\",\n                        help=\"Process each matching regex/ROI into separate masks. Each matched mask will be saved as ROI_n. (n = index from list of regex/ROIs)\")\n\n    # parser.add_argument(\"--custom_train_test_split_path\", type=str,\n    #                     help=\"Path to the YAML file defining the custom train-test-split.\")\n\n    return parser.parse_known_args()[0]\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'parser()' function in this code snippet?",
        "answer": "The 'parser()' function creates and configures an ArgumentParser object for a command-line interface. It defines various command-line arguments for an image processing pipeline, including input and output directories, processing options, and data handling preferences. The function returns the parsed arguments as an object."
      },
      {
        "question": "How does the code handle optional boolean flags, such as '--visualize' or '--overwrite'?",
        "answer": "Optional boolean flags are handled using the 'action=\"store_true\"' parameter in the add_argument() method. This means that if the flag is present in the command-line arguments, its value will be set to True; otherwise, it defaults to False. For example, '--visualize' and '--overwrite' are both implemented this way, allowing users to enable these options by simply including the flag in their command."
      },
      {
        "question": "What is the purpose of the '--spacing' argument, and how is it configured?",
        "answer": "The '--spacing' argument is used to specify the resampled voxel spacing in the x, y, and z directions for image processing. It is configured using 'nargs=3' to accept exactly three float values, with a default of (1., 1., 0.). This allows users to customize the voxel spacing of the processed images, which can be crucial for maintaining consistency across different imaging modalities or datasets."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    # arguments\n    parser.add_argument(\"input_directory\", type=str,\n                        help=\"Path to top-level directory of dataset.\")\n\n    parser.add_argument(\"output_directory\", type=str,\n                        help=\"Path to output directory to save processed images.\")\n\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\",\n                        help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n\n    parser.add_argument(\"--visualize\", default=False, action=\"store_true\",\n                        help=\"Whether to visualize the data graph\")\n\n    # Add more arguments here\n\n    return parser.parse_known_args()[0]",
        "complete": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    parser.add_argument(\"input_directory\", type=str, help=\"Path to top-level directory of dataset.\")\n    parser.add_argument(\"output_directory\", type=str, help=\"Path to output directory to save processed images.\")\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\", help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n    parser.add_argument(\"--visualize\", default=False, action=\"store_true\", help=\"Whether to visualize the data graph\")\n    parser.add_argument(\"--spacing\", nargs=3, type=float, default=(1., 1., 0.), help=\"The resampled voxel spacing in (x, y, z) directions.\")\n    parser.add_argument(\"--n_jobs\", type=int, default=-1, help=\"The number of parallel processes to use.\")\n    parser.add_argument(\"--show_progress\", action=\"store_true\", help=\"Whether to print progress to standard output.\")\n    parser.add_argument(\"--warn_on_error\", default=False, action=\"store_true\", help=\"Whether to warn on error.\")\n    parser.add_argument(\"--overwrite\", default=False, action=\"store_true\", help=\"Whether to write output files even if existing output files exist.\")\n    parser.add_argument(\"--nnunet\", default=False, action=\"store_true\", help=\"Whether to make the output conform to nnunet requirements.\")\n    parser.add_argument(\"--train_size\", type=float, default=1.0, help=\"The proportion of data to be used for training, as a decimal.\")\n    parser.add_argument(\"--random_state\", type=int, default=42, help=\"The random state to be used for the train-test-split.\")\n    parser.add_argument(\"--read_yaml_label_names\", default=False, action=\"store_true\", help=\"Whether to read the label names from roi_names.yaml in the input directory.\")\n    parser.add_argument(\"--ignore_missing_regex\", default=False, action=\"store_true\", help=\"Whether to ignore patients with no ROI regexes that match the given ones. Will throw an error on patients without matches if this is not set.\")\n    parser.add_argument(\"--roi_yaml_path\", type=str, default=\"\", help=\"Path to the YAML file defining ROI regexes\")\n    parser.add_argument(\"--custom_train_test_split\", default=False, action=\"store_true\", help=\"Whether to use a custom train-test-split, stored in custom_train_test_split.yaml in the input directory.\")\n    parser.add_argument(\"--nnunet_inference\", default=False, action=\"store_true\", help=\"Whether to generate data for nnUNet inference.\")\n    parser.add_argument(\"--dataset_json_path\", type=str, default=\"\", help=\"Path to the dataset.json file defining image modality indices for nnUNet inference.\")\n    parser.add_argument(\"--continue_processing\", default=False, action=\"store_true\", help=\"Whether to continue processing a partially completed dataset.\")\n    parser.add_argument(\"--dry_run\", default=False, action=\"store_true\", help=\"Make a dry run of the pipeline, only producing the edge table and dataset.csv.\")\n    parser.add_argument(\"--verbose\", default=False, action=\"store_true\", help=\"Verbose output flag.\")\n    parser.add_argument(\"--update\", default=False, action=\"store_true\", help=\"Update crawled index. In other words, process from scratch.\")\n    parser.add_argument(\"--roi_select_first\", default=False, action=\"store_true\", help=\"Only select first matching regex/ROI name.\")\n    parser.add_argument(\"--roi_separate\", default=False, action=\"store_true\", help=\"Process each matching regex/ROI into separate masks. Each matched mask will be saved as ROI_n. (n = index from list of regex/ROIs)\")\n\n    return parser.parse_known_args()[0]"
      },
      {
        "partial": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    # Add arguments here\n\n    return parser.parse_known_args()[0]",
        "complete": "from argparse import ArgumentParser\n\ndef parser():\n    parser = ArgumentParser(\"imgtools Automatic Processing Pipeline.\")\n\n    parser.add_argument(\"input_directory\", type=str, help=\"Path to top-level directory of dataset.\")\n    parser.add_argument(\"output_directory\", type=str, help=\"Path to output directory to save processed images.\")\n    parser.add_argument(\"--modalities\", type=str, default=\"CT\", help=\"List of desired modalities. Type as string for ex: RTSTRUCT,CT,RTDOSE\")\n    parser.add_argument(\"--visualize\", action=\"store_true\", help=\"Whether to visualize the data graph\")\n    parser.add_argument(\"--spacing\", nargs=3, type=float, default=(1., 1., 0.), help=\"The resampled voxel spacing in (x, y, z) directions.\")\n    parser.add_argument(\"--n_jobs\", type=int, default=-1, help=\"The number of parallel processes to use.\")\n    parser.add_argument(\"--show_progress\", action=\"store_true\", help=\"Whether to print progress to standard output.\")\n    parser.add_argument(\"--warn_on_error\", action=\"store_true\", help=\"Whether to warn on error.\")\n    parser.add_argument(\"--overwrite\", action=\"store_true\", help=\"Whether to write output files even if existing output files exist.\")\n    parser.add_argument(\"--nnunet\", action=\"store_true\", help=\"Whether to make the output conform to nnunet requirements.\")\n    parser.add_argument(\"--train_size\", type=float, default=1.0, help=\"The proportion of data to be used for training, as a decimal.\")\n    parser.add_argument(\"--random_state\", type=int, default=42, help=\"The random state to be used for the train-test-split.\")\n    parser.add_argument(\"--read_yaml_label_names\", action=\"store_true\", help=\"Whether to read the label names from roi_names.yaml in the input directory.\")\n    parser.add_argument(\"--ignore_missing_regex\", action=\"store_true\", help=\"Whether to ignore patients with no ROI regexes that match the given ones. Will throw an error on patients without matches if this is not set.\")\n    parser.add_argument(\"--roi_yaml_path\", type=str, default=\"\", help=\"Path to the YAML file defining ROI regexes\")\n    parser.add_argument(\"--custom_train_test_split\", action=\"store_true\", help=\"Whether to use a custom train-test-split, stored in custom_train_test_split.yaml in the input directory.\")\n    parser.add_argument(\"--nnunet_inference\", action=\"store_true\", help=\"Whether to generate data for nnUNet inference.\")\n    parser.add_argument(\"--dataset_json_path\", type=str, default=\"\", help=\"Path to the dataset.json file defining image modality indices for nnUNet inference.\")\n    parser.add_argument(\"--continue_processing\", action=\"store_true\", help=\"Whether to continue processing a partially completed dataset.\")\n    parser.add_argument(\"--dry_run\", action=\"store_true\", help=\"Make a dry run of the pipeline, only producing the edge table and dataset.csv.\")\n    parser.add_argument(\"--verbose\", action=\"store_true\", help=\"Verbose output flag.\")\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"Update crawled index. In other words, process from scratch.\")\n    parser.add_argument(\"--roi_select_first\", action=\"store_true\", help=\"Only select first matching regex/ROI name.\")\n    parser.add_argument(\"--roi_separate\", action=\"store_true\", help=\"Process each matching regex/ROI into separate masks. Each matched mask will be saved as ROI_n. (n = index from list of regex/ROIs)\")\n\n    return parser.parse_known_args()[0]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "argparse.ArgumentParser"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_rest_2.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n# Basic Tests\noptions(\"log_level\" = \"DEBUG\")\ntest_that(\"AnnotationGx:::.build_pubchem_rest_query\", {\n  res <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\")\n  expect_class(res, \"httr2_request\")\n\n  res2 <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\", namespace = \"name\", operation = \"cids\", output = \"JSON\")\n  expect_class(res2, \"httr2_request\")\n\n  expect_equal(res, res2)\n\n  res3 <- AnnotationGx:::.build_pubchem_rest_query(3672, namespace = \"cid\", operation = \"property/InChIKey\", output = \"JSON\")\n  expect_class(res3, \"httr2_request\")\n\n  res4 <- AnnotationGx:::.build_pubchem_rest_query(3672,\n    namespace = \"cid\",\n    operation = \"property/InChIKey\", output = \"JSON\", query_only = T\n  )\n  expect_class(res4, \"character\")\n})\n\noptions(\"log_level\" = \"WARN\")\n\ntest_that(\"AnnotationGx:::.build_pubchem_rest_query Failure\", {\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(NA))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query())\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"subStance\", namespace = \"cid\", operation = \"record\", output = \"JSON\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, operation = \"fake\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(1, domain = \"substance\", namespace = \"cid\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"compound\", namespace = \"cid\", operation = \"Title\", output = \"JSON\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(c(\"TRETINOIN\", \"erlotinib\", \"TRAMETINIB\"),\n    domain = \"compound\", namespace = \"name\",\n    operation = \"cids\", output = \"JSON\"\n  ))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, raw = \"TRUE\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, query_only = \"TRUE\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"substance\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"assay\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"cell\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"gene\", namespace = \"not choice\"))\n\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"protein\", namespace = \"not choice\"))\n\n  lapply(c(\"TSV\", \"PDF\", \"XLSX\"), function(x) expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, output = x)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_pubchem_rest_query` function in the AnnotationGx package, and how is it being tested in this code snippet?",
        "answer": "The `.build_pubchem_rest_query` function in the AnnotationGx package is likely used to construct REST API queries for the PubChem database. The code snippet tests this function's behavior under various input conditions. It checks if the function correctly builds httr2_request objects for valid inputs, handles different parameters (like namespace, operation, and output format), and properly throws errors for invalid inputs or unsupported options."
      },
      {
        "question": "How does the code handle error cases for the `.build_pubchem_rest_query` function, and what types of errors are being tested?",
        "answer": "The code uses the `expect_error()` function from the testthat package to check for various error conditions. It tests for errors when: passing NA or no arguments, using invalid domain/namespace combinations, specifying non-existent operations, passing multiple inputs for single-input parameters, using unsupported output formats, and providing invalid data types for boolean parameters. This comprehensive error testing ensures the function behaves correctly under various edge cases and invalid inputs."
      },
      {
        "question": "What is the significance of the `options(\"log_level\" = ...)` statements in the code, and how might they affect the test execution?",
        "answer": "The `options(\"log_level\" = ...)` statements are used to set the logging level for the tests. At the beginning, it's set to \"DEBUG\", which likely enables more verbose logging for the first set of tests. Later, it's changed to \"WARN\", probably to reduce the verbosity for the error-case tests. This approach allows for more detailed logging during the positive test cases while keeping the output cleaner for the expected error cases, helping to focus on the relevant information during different parts of the test suite."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"AnnotationGx:::.build_pubchem_rest_query\", {\n  res <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\")\n  expect_class(res, \"httr2_request\")\n\n  res2 <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\", namespace = \"name\", operation = \"cids\", output = \"JSON\")\n  expect_class(res2, \"httr2_request\")\n\n  expect_equal(res, res2)\n\n  # Complete the test for res3 and res4\n})",
        "complete": "test_that(\"AnnotationGx:::.build_pubchem_rest_query\", {\n  res <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\")\n  expect_class(res, \"httr2_request\")\n\n  res2 <- AnnotationGx:::.build_pubchem_rest_query(\"erlotinib\", namespace = \"name\", operation = \"cids\", output = \"JSON\")\n  expect_class(res2, \"httr2_request\")\n\n  expect_equal(res, res2)\n\n  res3 <- AnnotationGx:::.build_pubchem_rest_query(3672, namespace = \"cid\", operation = \"property/InChIKey\", output = \"JSON\")\n  expect_class(res3, \"httr2_request\")\n\n  res4 <- AnnotationGx:::.build_pubchem_rest_query(3672, namespace = \"cid\", operation = \"property/InChIKey\", output = \"JSON\", query_only = TRUE)\n  expect_class(res4, \"character\")\n})"
      },
      {
        "partial": "test_that(\"AnnotationGx:::.build_pubchem_rest_query Failure\", {\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(NA))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query())\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"subStance\", namespace = \"cid\", operation = \"record\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, operation = \"fake\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(1, domain = \"substance\", namespace = \"cid\"))\n  # Complete the remaining error tests\n})",
        "complete": "test_that(\"AnnotationGx:::.build_pubchem_rest_query Failure\", {\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(NA))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query())\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"subStance\", namespace = \"cid\", operation = \"record\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, operation = \"fake\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(1, domain = \"substance\", namespace = \"cid\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, domain = \"compound\", namespace = \"cid\", operation = \"Title\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(c(\"TRETINOIN\", \"erlotinib\", \"TRAMETINIB\"), domain = \"compound\", namespace = \"name\", operation = \"cids\", output = \"JSON\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, raw = \"TRUE\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, query_only = \"TRUE\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"substance\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"assay\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"cell\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"gene\", namespace = \"not choice\"))\n  expect_error(AnnotationGx:::.build_pubchem_rest_query(\"test\", domain = \"protein\", namespace = \"not choice\"))\n  lapply(c(\"TSV\", \"PDF\", \"XLSX\"), function(x) expect_error(AnnotationGx:::.build_pubchem_rest_query(2244, output = x)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/CancerTargetDiscovery.R",
    "language": "R",
    "content": "# CancerTargetDiscoveryDevelopment Functions\n\n#' Map Compound to CTD\n#'\n#' This function maps a drug compound to the Cancer Target Discovery (CTD) database.\n#' It retrieves information about the compound from the CTD database and returns the results as a data table.\n#'\n#' @param compounds A character vector of drug compounds to map to the CTD database.\n#' @param base_url The base URL of the CTD API. Default is \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\".\n#' @param endpoint The API endpoint for the compound mapping. Default is \"compound\".\n#' @param nParallel The number of parallel processes to use. Default is one less than the number of available cores.\n#' @param raw Logical indicating whether to return the raw response from the API. Default is FALSE.\n#' @param query_only Logical indicating whether to only return the API request URL without making the actual request. Default is FALSE.\n#'\n#' @return A data table containing the mapped information for the drug compound.\n#' If the API request fails, a data table with the drug compound name will be returned.\n#' If \\code{raw} is set to TRUE, the raw response from the API will be returned.\n#'\n#' @examples\n#' mapCompound2CTD(\"Bax channel blocker\", nParallel = 1)\n#'\n#' @export\nmapCompound2CTD <- function(\n    compounds,\n    base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", \n    endpoint = \"compound\",\n    nParallel = parallel::detectCores() - 1,\n    raw = FALSE,\n    query_only = FALSE\n) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                dt <- data.table::data.table(\n                    displayName = compound)\n                return(dt)\n            }\n\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n\n            original_dt <- .asDT(resp$xrefs)[, c(\"databaseId\", \"databaseName\")]\n            original_dt[, \"displayName\" := resp$displayName]\n\n            dt <- data.table::dcast(\n                    original_dt, \n                    formula = displayName ~ databaseName, \n                    value.var = \"databaseId\"\n                )\n            return(dt)\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) {\n        names(results) <- compounds\n        return(results)\n    }\n\n    return(data.table::rbindlist(results, fill = TRUE))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CTD` function and what are its main input parameters?",
        "answer": "The `mapCompound2CTD` function maps drug compounds to the Cancer Target Discovery (CTD) database. It retrieves information about the compounds from the CTD database and returns the results as a data table. The main input parameters are:\n1. `compounds`: A character vector of drug compounds to map.\n2. `base_url`: The base URL of the CTD API (default provided).\n3. `endpoint`: The API endpoint for compound mapping (default is 'compound').\n4. `nParallel`: Number of parallel processes to use.\n5. `raw`: Boolean to return raw API response.\n6. `query_only`: Boolean to return only the API request URL without making the actual request."
      },
      {
        "question": "How does the function handle parallel processing of multiple compound requests, and what happens if a request fails?",
        "answer": "The function uses parallel processing through the `parallel::mclapply` function to handle multiple compound requests simultaneously. The number of parallel processes is determined by the `nParallel` parameter, which defaults to one less than the number of available cores.\n\nIf a request fails or returns a non-200 status code, the function handles it gracefully:\n1. It creates a data table with only the compound name (displayName).\n2. This ensures that even if some requests fail, the function still returns a result for all input compounds.\n3. The successful requests are processed normally, extracting the relevant information from the API response."
      },
      {
        "question": "Explain the data transformation process in the function when `raw = FALSE`. How is the final output structured?",
        "answer": "When `raw = FALSE` (default), the function performs the following data transformation:\n1. It extracts the 'xrefs' data from the API response and converts it to a data table with 'databaseId' and 'databaseName' columns.\n2. It adds a 'displayName' column with the compound name.\n3. The data is then reshaped using `data.table::dcast` to create a wide format table where:\n   - Each row represents a compound\n   - Columns are created for each unique 'databaseName'\n   - Cell values contain the corresponding 'databaseId'\n4. Finally, all compound results are combined using `data.table::rbindlist` with `fill = TRUE` to handle missing columns.\n\nThe final output is a single data table where each row represents a compound, and columns represent different database identifiers for that compound."
      }
    ],
    "completion_tasks": [
      {
        "partial": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                dt <- data.table::data.table(\n                    displayName = compound)\n                return(dt)\n            }\n\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n\n            # Complete the code here\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) {\n        names(results) <- compounds\n        return(results)\n    }\n\n    return(data.table::rbindlist(results, fill = TRUE))\n}",
        "complete": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                dt <- data.table::data.table(\n                    displayName = compound)\n                return(dt)\n            }\n\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n\n            original_dt <- .asDT(resp$xrefs)[, c(\"databaseId\", \"databaseName\")]\n            original_dt[, \"displayName\" := resp$displayName]\n\n            dt <- data.table::dcast(\n                    original_dt, \n                    formula = displayName ~ databaseName, \n                    value.var = \"databaseId\"\n                )\n            return(dt)\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) {\n        names(results) <- compounds\n        return(results)\n    }\n\n    return(data.table::rbindlist(results, fill = TRUE))\n}"
      },
      {
        "partial": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    # Complete the code here\n}",
        "complete": "mapCompound2CTD <- function(compounds, base_url = \"https://ctd2-dashboard.nci.nih.gov/dashboard/get\", endpoint = \"compound\", nParallel = parallel::detectCores() - 1, raw = FALSE, query_only = FALSE) {\n    funContext <- .funContext(\"mapCompound2CTD\")\n    \n    # Check input types\n    checkmate::assert_character(compounds)\n    checkmate::assert_character(base_url)\n    checkmate::assert_character(endpoint)\n    checkmate::assert_logical(raw)\n    checkmate::assert_logical(query_only)\n\n    .info(funContext, \n        sprintf(\"Creating requests for %s compounds\", length(compounds)))\n\n    requests <- parallel::mclapply(compounds, function(compound){\n        compound <- gsub(\" \", \"-\", compound)\n        .buildURL(base_url, endpoint, compound) |>\n            .build_request()\n    })\n    if(query_only) return(requests)\n\n    .info(funContext, \"Performing requests w/\", nParallel, \"parallel processes..\")\n    resps <- .perform_request_parallel(requests)\n    names(resps) <- compounds\n\n    results <- parallel::mclapply(\n        names(resps) , \n        function(compound){\n            resp <- resps[[compound]]\n            if(all(class(resp) != \"httr2_response\") || resp$status_code != 200){\n                return(data.table::data.table(displayName = compound))\n            }\n            resp <- .parse_resp_json(resp)\n            if(raw) return(resp)\n            original_dt <- .asDT(resp$xrefs)[, c(\"databaseId\", \"databaseName\")]\n            original_dt[, \"displayName\" := resp$displayName]\n            data.table::dcast(original_dt, formula = displayName ~ databaseName, value.var = \"databaseId\")\n        }, \n        mc.cores = nParallel    \n    )\n    if(raw) return(setNames(results, compounds))\n    data.table::rbindlist(results, fill = TRUE)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-logging.R",
    "language": "R",
    "content": "#' Default Log formatter\n#'\n#' @title Default Log formatter\n#' @description log_fmt function to format log messages\n#' @param level `character` The log level\n#' @param ... `character` The messages to log\n#' @keywords internal\n#' @noRd\n.log_fmt <- function(level, ...) {\n  time_str <- format(Sys.time(), \"[%H:%M:%S]\")\n  level_str <- paste0(\"[\", level, \"]\")\n  msg <- paste(..., sep = \" \")\n  paste0(time_str, level_str, msg, sep = \" \", collapse = \"\\n\")\n}\n\n\n#' Info message function\n#' \n#' This function is used to print messages when the verbose option is enabled.\n#' \n#' @param ... `character` The messages to print\n#' \n#' @keywords internal\n#' @noRd\n#' @export\n#' @examples\n#' \\dontrun{\n#' .info(\"This is an info message\")\n#' }\n.info <- function(...) {\n  msg <- .log_fmt(\"INFO\", ...)\n  optionIsTRUE <- options(\"log_level\") == \"INFO\" || (options(\"log_level\") %in% c(\"WARN\", \"DEBUG\", \"ERROR\"))\n  if(optionIsTRUE) {\n    message(crayon::green(msg))\n  }\n}\n\n#' Custom message function for verbose output\n#'\n#' This function is used to print messages when the verbose option is enabled.\n#' It checks if the package-specific verbose option is set or if the global verbose option is set.\n#' If either of these options is TRUE, the message is printed in blue and bold format.\n#'\n#' @param ... `character` The messages to print\n#'\n#' @examples\n#' \\dontrun{\n#' options(\"myPackage.verbose\" = TRUE)\n#' }\n#'\n#' @keywords internal\n#' @noRd\n.debug <- function(...) {\n  msg <- .log_fmt(\"DEBUG\", ...)\n  optionIsTRUE <- options(\"log_level\") == \"DEBUG\"\n  if (optionIsTRUE) {\n    message(crayon::blue(msg))\n  }\n}\n\n#' @keywords internal\n#' @noRd\n.warn <- function(...) {\n  msg <- .log_fmt(\"WARNING\", ...)\n  optionIsTRUE <- options(\"log_level\") != \"ERROR\"\n  message(crayon::yellow(msg))\n}\n\n#' @keywords internal\n#' @noRd\n.err <- function(...) {\n  msg <- .log_fmt(\"ERROR\", ...)\n  optionIsTRUE <- options(\"log_level\") != NULL\n  stop(\"\\r\", crayon::red(msg), call. = FALSE)\n}\n\n#' Generate a function context string\n#'\n#' This function takes the name of a function and returns a string that\n#' represents the function context.\n#' The string is formatted as [packageName functionName].\n#'\n#' @keywords internal\n#' @noRd\n.funContext <- function(funName) paste0(\"[\", utils::packageName(), \"::\", funName, \"]\")\n\n# # test\n# .debug(\"This is a debug message\")\n# .info(\"This is an info message\")\n# .warn(\"This is a warning message\")\n# .error(\"This is an error message\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.log_fmt` function in the given code snippet?",
        "answer": "The `.log_fmt` function is an internal utility function used to format log messages. It takes a log level and additional message arguments, and returns a formatted string that includes the current time, log level, and the message. The function combines these elements into a single string, separating them with spaces and collapsing multiple lines with newline characters."
      },
      {
        "question": "How does the `.info` function determine whether to display a message or not?",
        "answer": "The `.info` function checks the 'log_level' option to determine whether to display the message. It will display the message if the 'log_level' option is set to 'INFO', 'WARN', 'DEBUG', or 'ERROR'. The function uses the `optionIsTRUE` variable to store this condition. If `optionIsTRUE` is TRUE, the function displays the message in green color using the `crayon::green` function."
      },
      {
        "question": "What is the difference between how the `.warn` and `.err` functions handle message display?",
        "answer": "The `.warn` function displays a warning message in yellow color using `crayon::yellow` whenever the 'log_level' option is not set to 'ERROR'. On the other hand, the `.err` function stops the execution with an error message in red color using `crayon::red` as long as the 'log_level' option is not NULL. The `.err` function uses `stop()` to halt execution, while `.warn` uses `message()` to display the warning without stopping the program."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Default Log formatter\n#'\n#' @title Default Log formatter\n#' @description log_fmt function to format log messages\n#' @param level `character` The log level\n#' @param ... `character` The messages to log\n#' @keywords internal\n#' @noRd\n.log_fmt <- function(level, ...) {\n  time_str <- format(Sys.time(), \"[%H:%M:%S]\")\n  level_str <- paste0(\"[\", level, \"]\")\n  msg <- paste(..., sep = \" \")\n  # Complete the function to format the log message\n}",
        "complete": "#' Default Log formatter\n#'\n#' @title Default Log formatter\n#' @description log_fmt function to format log messages\n#' @param level `character` The log level\n#' @param ... `character` The messages to log\n#' @keywords internal\n#' @noRd\n.log_fmt <- function(level, ...) {\n  time_str <- format(Sys.time(), \"[%H:%M:%S]\")\n  level_str <- paste0(\"[\", level, \"]\")\n  msg <- paste(..., sep = \" \")\n  paste0(time_str, level_str, msg, sep = \" \", collapse = \"\\n\")\n}"
      },
      {
        "partial": "#' Custom message function for verbose output\n#'\n#' This function is used to print messages when the verbose option is enabled.\n#' It checks if the package-specific verbose option is set or if the global verbose option is set.\n#' If either of these options is TRUE, the message is printed in blue and bold format.\n#'\n#' @param ... `character` The messages to print\n#'\n#' @examples\n#' \\dontrun{\n#' options(\"myPackage.verbose\" = TRUE)\n#' }\n#'\n#' @keywords internal\n#' @noRd\n.debug <- function(...) {\n  msg <- .log_fmt(\"DEBUG\", ...)\n  # Complete the function to check the log level and print the message if appropriate\n}",
        "complete": "#' Custom message function for verbose output\n#'\n#' This function is used to print messages when the verbose option is enabled.\n#' It checks if the package-specific verbose option is set or if the global verbose option is set.\n#' If either of these options is TRUE, the message is printed in blue and bold format.\n#'\n#' @param ... `character` The messages to print\n#'\n#' @examples\n#' \\dontrun{\n#' options(\"myPackage.verbose\" = TRUE)\n#' }\n#'\n#' @keywords internal\n#' @noRd\n.debug <- function(...) {\n  msg <- .log_fmt(\"DEBUG\", ...)\n  if (options(\"log_level\") == \"DEBUG\") {\n    message(crayon::blue(msg))\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-[.R",
    "language": "R",
    "content": "# ==== PharmacoSet Class\n#'`[`\n#'\n#' @examples\n#' data(CCLEsmall)\n#' CCLEsmall[\"WM1799\", \"Sorafenib\"]\n#'\n#' @param x object\n#' @param i Cell lines to keep in object\n#' @param j Drugs to keep in object\n#' @param ... further arguments\n#' @param drop A boolean flag of whether to drop single dimensions or not\n#'\n#'@return Returns the subsetted object\n#'\n#' @export\nsetMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i)&&is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],  molecular.data.cells=sampleNames(x)[i]))\n  }\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `[` method defined for the PharmacoSet class?",
        "answer": "The `[` method is defined to allow subsetting of a PharmacoSet object. It enables users to extract specific cell lines and drugs from the dataset, either by their names (as character vectors) or by their indices (as numeric vectors)."
      },
      {
        "question": "How does the method handle different types of input for the `i` and `j` parameters?",
        "answer": "The method checks if both `i` and `j` are character vectors or if both are numeric vectors. For character vectors, it directly uses them as cell and drug names. For numeric vectors, it treats them as indices and retrieves the corresponding cell and drug names from the object before subsetting."
      },
      {
        "question": "What is the significance of the `drop` parameter in this method, and what is its default value?",
        "answer": "The `drop` parameter is a boolean flag that determines whether to drop single dimensions in the resulting subset. Its default value is set to `FALSE`, meaning that by default, the method will preserve the dimensionality of the subset even if it contains only one cell line or drug."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i) && is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    # Complete the code here\n  }\n})",
        "complete": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i) && is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],  molecular.data.cells=sampleNames(x)[i]))\n  }\n})"
      },
      {
        "partial": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  # Complete the code here\n})",
        "complete": "setMethod(`[`, 'PharmacoSet', function(x, i, j, ..., drop = FALSE){\n  if(is.character(i) && is.character(j)){\n    return(subsetTo(x, cells=i, drugs=j,  molecular.data.cells=i))\n  }\n  else if(is.numeric(i) && is.numeric(j) && all(as.integer(i)==i) && all(as.integer(j)==j)){\n    return(subsetTo(x, cells=sampleNames(x)[i], drugs=treatmentNames(x)[j],  molecular.data.cells=sampleNames(x)[i]))\n  }\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-summarizeMolecularProfiles.R",
    "language": "R",
    "content": "#' Takes molecular data from a PharmacoSet, and summarises them\n#' into one entry per drug\n#'\n#' Given a PharmacoSet with molecular data, this function will summarize\n#' the data into one profile per cell line, using the chosen summary.stat. Note\n#' that this does not really make sense with perturbation type data, and will\n#' combine experiments and controls when doing the summary if run on a\n#' perturbation dataset.\n#'\n#' @examples\n#' data(GDSCsmall)\n#' GDSCsmall <- summarizeMolecularProfiles(GDSCsmall, mDataType = \"rna\", cell.lines=sampleNames(GDSCsmall), summary.stat = 'median', fill.missing = TRUE, verbose=TRUE)\n#' GDSCsmall\n#'\n#' @param object \\code{PharmacoSet} The PharmacoSet to summarize\n#' @param mDataType \\code{character} which one of the molecular data types\n#' to use in the analysis, out of all the molecular data types available for the pset\n#' for example: rna, rnaseq, snp\n#' @param cell.lines \\code{character} The cell lines to be summarized.\n#'   If any cell.line has no data, missing values will be created\n#' @param features \\code{caracter} A vector of the feature names to include in the summary\n#' @param summary.stat \\code{character} which summary method to use if there are repeated\n#'   cell.lines? Choices are \"mean\", \"median\", \"first\", or \"last\"\n#'   In case molecular data type is mutation or fusion \"and\" and \"or\" choices are available\n#' @param fill.missing \\code{boolean} should the missing cell lines not in the\n#'   molecular data object be filled in with missing values?\n#' @param summarize A flag which when set to FALSE (defaults to TRUE) disables summarizing and\n#'   returns the data unchanged as a ExpressionSet\n#' @param verbose \\code{boolean} should messages be printed\n#' @param binarize.threshold \\code{numeric} A value on which the molecular data is binarized.\n#'   If NA, no binarization is done.\n#' @param binarize.direction \\code{character} One of \"less\" or \"greater\", the direction of binarization on\n#'   binarize.threshold, if it is not NA.\n#' @param removeTreated \\code{logical} If treated/perturbation experiments are present, should they\n#'   be removed? Defaults to yes.\n#'\n#' @return \\code{matrix} An updated PharmacoSet with the molecular data summarized\n#'   per cell line.\n#'\n#' @importMethodsFrom CoreGx summarizeMolecularProfiles\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom SummarizedExperiment SummarizedExperiment rowData rowData<- colData colData<- assays assays<- assayNames assayNames<-\n#' @importFrom Biobase AnnotatedDataFrame\n#' @keywords internal\n#' @export\nsetMethod('summarizeMolecularProfiles', signature(object='PharmacoSet'),\n    function(object, mDataType, cell.lines, features,\n        summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n        fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n        binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n        removeTreated=TRUE)\n{\n\n    mDataTypes <- mDataNames(object)\n    if (!(mDataType %in% mDataTypes)) {\n      stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n    }\n\n    if(summarize==FALSE){\n      return(molecularProfilesSlot(object)[[mDataType]])\n    }\n\n    if (missing(features)) {\n      features <- rownames(featureInfo(object, mDataType))\n    } else {\n      fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n      if (verbose && !all(fix)) {\n        warning (sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n      }\n      features <- features[fix]\n    }\n\n    summary.stat <- match.arg(summary.stat)\n    binarize.direction <- match.arg(binarize.direction)\n\n    if((!S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) {\n      stop (\"Invalid summary.stat, choose among: mean, median, first, last\" )\n    }\n    if((S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"and\", \"or\"))) {\n      stop (\"Invalid summary.stat, choose among: and, or\" )\n    }\n\n    if (missing(cell.lines)) {\n      cell.lines <- sampleNames(object)\n    }\n\n    if(datasetType(object) %in% c(\"perturbation\", \"both\") && removeTreated){\n      if(!\"xptype\" %in% colnames(phenoInfo(object, mDataType))) {\n        warning(\"The passed in molecular data had no column: xptype.\n                 \\rEither the mDataType does not include perturbations, or the PSet is malformed.\n                 \\rAssuming the former and continuing.\")\n      } else {\n        keepCols <- phenoInfo(object, mDataType)$xptype %in% c(\"control\", \"untreated\")\n        molecularProfilesSlot(object)[[mDataType]] <- molecularProfilesSlot(object)[[mDataType]][,keepCols]\n      }\n    }\n\n\n    ##TODO:: have less confusing variable names\n    dd <- molecularProfiles(object, mDataType)\n    pp <- phenoInfo(object, mDataType)\n\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation == \"mutation\") {\n      tt <- dd\n      tt[which(!is.na(dd) & dd ==\"wt\")] <- FALSE\n      tt[which(!is.na(dd) & dd !=\"wt\")] <- TRUE\n      tt <- apply(tt, 2, as.logical)\n      dimnames(tt) <- dimnames(dd)\n      dd <- tt\n    }\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation == \"fusion\") {\n      tt <- dd\n      tt[which(!is.na(dd) & dd ==\"0\")] <- FALSE\n      tt[which(!is.na(dd) & dd !=\"0\")] <- TRUE\n      tt <- apply(tt, 2, as.logical)\n      dimnames(tt) <- dimnames(dd)\n      dd <- tt\n    }\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"cnv\", \"rna\", \"rnaseq\", \"isoform\")\n       && !is.na(binarize.threshold)) {\n      tt <- dd\n      switch(binarize.direction, \"less\" = {\n            tt[which(!is.na(dd) & dd < binarize.threshold)] <- TRUE\n            tt[which(!is.na(dd) & dd >= binarize.threshold)] <- FALSE\n      }, \"greater\" = {\n            tt[which(!is.na(dd) & dd > binarize.threshold)] <- TRUE\n            tt[which(!is.na(dd) & dd <= binarize.threshold)] <- FALSE\n      })\n      tt <- apply(tt, 2, as.logical)\n      dimnames(tt) <- dimnames(dd)\n      dd <- tt\n    }\n    if (any(colnames(dd) != rownames(pp))) {\n      warning (\"Samples in phenodata and expression matrices must be ordered the same way\")\n      dd <- dd[ , rownames(pp), drop=FALSE]\n    }\n    if (!fill.missing) {\n      cell.lines <- intersect(cell.lines, unique(pp[!is.na(pp[ , \"sampleid\"]), \"sampleid\"]))\n    }\n    if (length(cell.lines) == 0) {\n      stop (\"No cell lines in common\")\n    }\n\n    ## select profiles with no replicates\n    duplix <- unique(pp[!is.na(pp[ , \"sampleid\"]) & duplicated(pp[ , \"sampleid\"]), \"sampleid\"])\n    ucell <- setdiff(cell.lines, duplix)\n\n    ## keep the non ambiguous cases\n    dd2 <- dd[ , match(ucell, pp[ , \"sampleid\"]), drop=FALSE]\n    pp2 <- pp[match(ucell, pp[ , \"sampleid\"]), , drop=FALSE]\n    if (length(duplix) > 0) {\n      if (verbose) {\n        message(sprintf(\"Summarizing %s molecular data for:\\t%s\", mDataType, annotation(object)$name))\n        total <- length(duplix)\n        # create progress bar\n        pb <- utils::txtProgressBar(min=0, max=total, style=3)\n        i <- 1\n      }\n      ## replace factors by characters to allow for merging duplicated experiments\n      pp2 <- apply(pp2, 2, function (x) {\n        if (is.factor(x)) {\n          return (as.character(x))\n        } else {\n          return (x)\n        }\n      })\n      ## there are some replicates to collapse\n      for (x in duplix) {\n        myx <- which(!is.na(pp[ , \"sampleid\"]) & is.element(pp[ , \"sampleid\"], x))\n        switch(summary.stat,\n          \"mean\" = {\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, mean)\n          },\n          \"median\"={\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, median)\n          },\n          \"first\"={\n            ddt <- dd[ , myx[1], drop=FALSE]\n          },\n          \"last\" = {\n            ddt <- dd[ , myx[length(myx)], drop=FALSE]\n          },\n          \"and\" = {\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, function(x) do.call(`&`, as.list(x)))\n          },\n          \"or\" = {\n            ddt <- apply(dd[ , myx, drop=FALSE], 1, function(x) do.call(`|`, as.list(x)))\n          }\n        )\n        ppt <- apply(pp[myx, , drop=FALSE], 2, function (x) {\n          x <- paste(unique(as.character(x[!is.na(x)])), collapse=\"///\")\n          return (x)\n        })\n        ppt[!is.na(ppt) & ppt == \"\"] <- NA\n        dd2 <- cbind(dd2, ddt)\n        pp2 <- rbind(pp2, ppt)\n        if (verbose){\n          utils::setTxtProgressBar(pb, i)\n          i <- i + 1\n        }\n      }\n      if (verbose) {\n        close(pb)\n      }\n    }\n    colnames(dd2) <- rownames(pp2) <- c(ucell, duplix)\n\n    ## reorder cell lines\n    dd2 <- dd2[ , cell.lines, drop=FALSE]\n    pp2 <- pp2[cell.lines, , drop=FALSE]\n    pp2[ , \"sampleid\"] <- cell.lines\n    res <- molecularProfilesSlot(object)[[mDataType]]\n    if(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\", \"fusion\")) {\n      tt <- dd2\n      tt[which(!is.na(dd2) & dd2)] <- \"1\"\n      tt[which(!is.na(dd2) & !dd2)] <- \"0\"\n      dd2 <- tt\n    }\n    res <- SummarizedExperiment::SummarizedExperiment(dd2)\n    pp2 <- S4Vectors::DataFrame(pp2, row.names=rownames(pp2))\n    pp2$tissueid <- sampleInfo(object)[pp2$sampleid, \"tissueid\"]\n    SummarizedExperiment::colData(res) <- pp2\n    SummarizedExperiment::rowData(res) <- featureInfo(object, mDataType)\n    ##TODO:: Generalize this to multiple assay SummarizedExperiments!\n    # if(!is.null(SummarizedExperiment::assay(res, 1))) {\n    #   SummarizedExperiment::assay(res, 2) <- matrix(rep(NA,\n    #                                                     length(assay(res, 1))\n    #                                                     ),\n    #                                                     nrow=nrow(assay(res, 1)),\n    #                                                     ncol=ncol(assay(res, 1)),\n    #                                                 dimnames=dimnames(assay(res, 1))\n    #                                                 )\n    # }\n    assayNames(res) <- assayNames(molecularProfilesSlot(object)[[mDataType]])[[1]]\n    res <- res[features,]\n    S4Vectors::metadata(res) <- S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])\n    return(res)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeMolecularProfiles` function in this code snippet?",
        "answer": "The `summarizeMolecularProfiles` function is designed to summarize molecular data from a PharmacoSet object into one entry per cell line. It processes various types of molecular data (e.g., RNA, mutations, fusions) and can apply different summary statistics (mean, median, first, last, and, or) to handle replicate experiments. The function also allows for data binarization and can handle perturbation experiments."
      },
      {
        "question": "How does the function handle different types of molecular data, such as mutations or fusions?",
        "answer": "For mutation and fusion data, the function converts the data to a logical format. Mutations are transformed so that 'wt' (wild type) becomes FALSE, and any other value becomes TRUE. For fusions, '0' becomes FALSE, and any other value becomes TRUE. The function then uses 'and' or 'or' operations to summarize replicate experiments for these data types, instead of mean or median used for other data types."
      },
      {
        "question": "What is the purpose of the `binarize.threshold` and `binarize.direction` parameters in this function?",
        "answer": "The `binarize.threshold` and `binarize.direction` parameters are used to binarize continuous molecular data types such as CNV, RNA, RNAseq, or isoform data. If a `binarize.threshold` is provided (not NA), the function will convert the data to TRUE/FALSE based on this threshold. The `binarize.direction` parameter (either 'less' or 'greater') determines whether values less than or greater than the threshold are set to TRUE. This allows for flexible transformation of continuous data into binary format for specific analysis needs."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('summarizeMolecularProfiles', signature(object='PharmacoSet'),\n    function(object, mDataType, cell.lines, features,\n        summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n        fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n        binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n        removeTreated=TRUE)\n{\n    mDataTypes <- mDataNames(object)\n    if (!(mDataType %in% mDataTypes)) {\n      stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n    }\n\n    if(summarize==FALSE){\n      return(molecularProfilesSlot(object)[[mDataType]])\n    }\n\n    # Complete the rest of the function\n}",
        "complete": "setMethod('summarizeMolecularProfiles', signature(object='PharmacoSet'),\n    function(object, mDataType, cell.lines, features,\n        summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n        fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n        binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n        removeTreated=TRUE)\n{\n    mDataTypes <- mDataNames(object)\n    if (!(mDataType %in% mDataTypes)) {\n      stop (sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n    }\n\n    if(summarize==FALSE){\n      return(molecularProfilesSlot(object)[[mDataType]])\n    }\n\n    if (missing(features)) {\n      features <- rownames(featureInfo(object, mDataType))\n    } else {\n      fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n      if (verbose && !all(fix)) {\n        warning (sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n      }\n      features <- features[fix]\n    }\n\n    summary.stat <- match.arg(summary.stat)\n    binarize.direction <- match.arg(binarize.direction)\n\n    if((!S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\"))) {\n      stop (\"Invalid summary.stat, choose among: mean, median, first, last\" )\n    }\n    if((S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation %in% c(\"mutation\",\"fusion\")) & (!summary.stat %in% c(\"and\", \"or\"))) {\n      stop (\"Invalid summary.stat, choose among: and, or\" )\n    }\n\n    if (missing(cell.lines)) {\n      cell.lines <- sampleNames(object)\n    }\n\n    # ... (rest of the function implementation)\n}"
      },
      {
        "partial": "# Function to summarize molecular profiles\nsummarizeMolecularProfiles <- function(object, mDataType, cell.lines, features,\n                                       summary.stat, fill.missing, summarize, verbose,\n                                       binarize.threshold, binarize.direction, removeTreated) {\n  # Implement the function body\n}",
        "complete": "# Function to summarize molecular profiles\nsummarizeMolecularProfiles <- function(object, mDataType, cell.lines, features,\n                                       summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"and\", \"or\"),\n                                       fill.missing = TRUE, summarize = TRUE, verbose = TRUE,\n                                       binarize.threshold = NA, binarize.direction = c(\"less\", \"greater\"),\n                                       removeTreated = TRUE) {\n  mDataTypes <- mDataNames(object)\n  if (!(mDataType %in% mDataTypes)) {\n    stop(sprintf(\"Invalid mDataType, choose among: %s\", paste(names(molecularProfilesSlot(object)), collapse=\", \")))\n  }\n\n  if (!summarize) return(molecularProfilesSlot(object)[[mDataType]])\n\n  features <- if (missing(features)) rownames(featureInfo(object, mDataType)) else {\n    fix <- features %in% rownames(featureInfo(object, mDataType))\n    if (verbose && !all(fix)) warning(sprintf(\"Only %i/%i features can be found\", sum(fix), length(features)))\n    features[fix]\n  }\n\n  summary.stat <- match.arg(summary.stat)\n  binarize.direction <- match.arg(binarize.direction)\n\n  annotation <- S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation\n  if (!annotation %in% c(\"mutation\", \"fusion\") && !summary.stat %in% c(\"mean\", \"median\", \"first\", \"last\")) {\n    stop(\"Invalid summary.stat, choose among: mean, median, first, last\")\n  }\n  if (annotation %in% c(\"mutation\", \"fusion\") && !summary.stat %in% c(\"and\", \"or\")) {\n    stop(\"Invalid summary.stat, choose among: and, or\")\n  }\n\n  cell.lines <- if (missing(cell.lines)) sampleNames(object) else cell.lines\n\n  # ... (rest of the function implementation)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/unichem_helpers.R",
    "language": "R",
    "content": "#' Build a UniChem query URL\n#'\n#' This function builds a UniChem query URL based on the specified endpoint.\n#'\n#' @param endpoint The UniChem endpoint to query (valid options: \"compounds\", \"connectivity\", \"images\", \"sources\")\n#' @param query_only Logical indicating whether to return only the query URL without building it (default: FALSE)\n#'\n#' @return `httr2::httr2_url` object if `query_only` is TRUE, otherwise the built URL.\n#'\n#' @examples\n#' .build_unichem_query(\"sources\")\n#' .build_unichem_query(\"connectivity\", query_only = TRUE)\n#' \n#' @noRd\n#' @keywords internal\n.build_unichem_query <- function(\n    endpoint, query_only = FALSE\n) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_query\")\n\n    valid_endpoints <- c(\"compounds\", \"connectivity\", \"images\", \"sources\")\n    checkmate::assert_subset(endpoint, valid_endpoints)\n\n    unichem_api <- \"https://www.ebi.ac.uk/unichem/api/v1\"\n    url <- httr2::url_parse(unichem_api)\n    url$path <- .buildURL(url$path, endpoint)\n\n    output <- httr2::url_build(url)\n\n    .debug(funContext, \"URL: \", output )\n\n    if (query_only) return(url)\n    httr2::url_build(url) \n}\n\n\n#' Build a UniChem compound request\n#'\n#' This function builds a UniChem compound request based on the provided parameters.\n#'\n#' @param type The type of compound identifier to search for. Valid types are \"uci\", \"inchi\", \"inchikey\", and \"sourceID\".\n#' @param compound The compound identifier to search for.\n#' @param sourceID The source ID to search for if the type is \"sourceID\". Defaults to NULL.\n#' @param ... Additional arguments.\n#'\n#' @return A `httr2_request`  request object for the UniChem compound query.\n#'\n#' @examples\n#' .build_unichem_compound_req(type = \"uci\", compound = \"538323\")\n#' .build_unichem_compound_req(type = \"sourceID\", sourceID = 22, compound = \"2244\")\n#' \n#' @noRd\n#' @keywords internal\n.build_unichem_compound_req <- function(\n    type, compound, sourceID = NULL, ...\n){\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_compound_req\")\n\n    valid_types <- c(\"uci\", \"inchi\", \"inchikey\", \"sourceID\")\n    checkmate::assert_subset(type, valid_types)\n\n    base_url <- .build_unichem_query(\"compounds\")\n\n    .debug(funContext, \"Base URL: \", base_url)\n\n    body <- list(\n        type = type,\n        compound = compound\n    )\n\n    body$sourceID <- if (type == \"sourceID\") {\n        checkmate::assert_integerish(\n            x = sourceID,\n            lower = 1,\n            upper = max(getUnichemSources()$SourceID),\n            len = 1\n            )\n        sourceID\n    } else NULL\n\n\n    request <- base_url |> \n        .build_request() |>\n        httr2::req_body_json(body) \n\n    .debug(funContext, \"Request: \", request)\n    return(request)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_unichem_query` function and what are its parameters?",
        "answer": "The `.build_unichem_query` function builds a UniChem query URL based on a specified endpoint. It takes two parameters: `endpoint` (a string specifying the UniChem endpoint to query, with valid options being 'compounds', 'connectivity', 'images', or 'sources') and `query_only` (a logical value indicating whether to return only the query URL without building it, defaulting to FALSE)."
      },
      {
        "question": "In the `.build_unichem_compound_req` function, how is the `sourceID` parameter handled?",
        "answer": "The `sourceID` parameter is handled conditionally in the `.build_unichem_compound_req` function. If the `type` parameter is 'sourceID', the function checks that `sourceID` is an integer within a valid range (between 1 and the maximum SourceID from `getUnichemSources()`). If `type` is not 'sourceID', the `sourceID` is set to NULL in the request body."
      },
      {
        "question": "How does the `.build_unichem_compound_req` function construct and return the HTTP request?",
        "answer": "The `.build_unichem_compound_req` function constructs the HTTP request by first calling `.build_unichem_query('compounds')` to get the base URL. It then creates a request body with the `type` and `compound` parameters, and `sourceID` if applicable. Finally, it uses `httr2::req_body_json()` to add the JSON-encoded body to the request, and returns the resulting `httr2_request` object."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".build_unichem_query <- function(endpoint, query_only = FALSE) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_query\")\n    valid_endpoints <- c(\"compounds\", \"connectivity\", \"images\", \"sources\")\n    checkmate::assert_subset(endpoint, valid_endpoints)\n    unichem_api <- \"https://www.ebi.ac.uk/unichem/api/v1\"\n    url <- httr2::url_parse(unichem_api)\n    url$path <- .buildURL(url$path, endpoint)\n    output <- httr2::url_build(url)\n    .debug(funContext, \"URL: \", output)\n    if (query_only) return(url)\n    # Complete the function here\n}",
        "complete": ".build_unichem_query <- function(endpoint, query_only = FALSE) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_query\")\n    valid_endpoints <- c(\"compounds\", \"connectivity\", \"images\", \"sources\")\n    checkmate::assert_subset(endpoint, valid_endpoints)\n    unichem_api <- \"https://www.ebi.ac.uk/unichem/api/v1\"\n    url <- httr2::url_parse(unichem_api)\n    url$path <- .buildURL(url$path, endpoint)\n    output <- httr2::url_build(url)\n    .debug(funContext, \"URL: \", output)\n    if (query_only) return(url)\n    httr2::url_build(url)\n}"
      },
      {
        "partial": ".build_unichem_compound_req <- function(type, compound, sourceID = NULL, ...) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_compound_req\")\n    valid_types <- c(\"uci\", \"inchi\", \"inchikey\", \"sourceID\")\n    checkmate::assert_subset(type, valid_types)\n    base_url <- .build_unichem_query(\"compounds\")\n    .debug(funContext, \"Base URL: \", base_url)\n    body <- list(type = type, compound = compound)\n    # Complete the function here\n}",
        "complete": ".build_unichem_compound_req <- function(type, compound, sourceID = NULL, ...) {\n    funContext <- .funContext(\"AnnotationGx:::.build_unichem_compound_req\")\n    valid_types <- c(\"uci\", \"inchi\", \"inchikey\", \"sourceID\")\n    checkmate::assert_subset(type, valid_types)\n    base_url <- .build_unichem_query(\"compounds\")\n    .debug(funContext, \"Base URL: \", base_url)\n    body <- list(type = type, compound = compound)\n    body$sourceID <- if (type == \"sourceID\") {\n        checkmate::assert_integerish(\n            x = sourceID,\n            lower = 1,\n            upper = max(getUnichemSources()$SourceID),\n            len = 1\n        )\n        sourceID\n    } else NULL\n    request <- base_url |>\n        .build_request() |>\n        httr2::req_body_json(body)\n    .debug(funContext, \"Request: \", request)\n    return(request)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat.R",
    "language": "R",
    "content": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\nlibrary(AnnotationGx)\n\ntest_check(\"AnnotationGx\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of this code snippet in the context of R package development?",
        "answer": "This code snippet is typically found in the 'tests/testthat.R' file of an R package. Its purpose is to set up the testing environment for the package 'AnnotationGx' using the testthat framework. It loads necessary libraries (testthat, checkmate, and AnnotationGx) and then calls test_check() to run all the tests for the package."
      },
      {
        "question": "Why does the comment suggest not modifying this file, and where should additional test configuration be done?",
        "answer": "The comment advises against modifying this file because it's part of the standard setup for testthat. Additional test configuration should be done in separate test files within the 'tests/testthat/' directory. This separation helps maintain a clean and organized testing structure, keeping the main test runner file untouched while allowing developers to add specific tests in individual files."
      },
      {
        "question": "What is the significance of including the 'checkmate' library in this test setup?",
        "answer": "The inclusion of the 'checkmate' library in this test setup suggests that it's being used for additional assertion and validation in the package's tests. Checkmate provides a comprehensive set of assertion functions that can be used to validate function inputs and outputs, enhancing the robustness of the tests. Its presence indicates that the package developers are likely using these advanced assertion capabilities in their test suite for 'AnnotationGx'."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_check(\"",
        "complete": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\nlibrary(AnnotationGx)\n\ntest_check(\"AnnotationGx\")"
      },
      {
        "partial": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\n",
        "complete": "# This file is part of the standard setup for testthat.\n# It is recommended that you do not modify it.\n#\n# Where should you do additional test configuration?\n# Learn more about the roles of various files in:\n# * https://r-pkgs.org/testing-design.html#sec-tests-files-overview\n# * https://testthat.r-lib.org/articles/special-files.html\n\nlibrary(testthat)\nlibrary(checkmate)\nlibrary(AnnotationGx)\n\ntest_check(\"AnnotationGx\")"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/src/rCPP_bridge.cpp",
    "language": "cpp",
    "content": "#include <Rcpp.h>\n\n\n//' QUICKSTOP significance testing for partial correlation\n//'\n//' This function will test whether the observed partial correlation is significant\n//' at a level of req_alpha, doing up to MaxIter permutations. Currently, it\n//' supports only grouping by discrete categories when calculating a partial correlation.\n//' Currenlty, only does two sided tests.\n//'\n//' @param pin_x one of the two vectors to correlate.\n//' @param pin_y the other vector to calculate\n//' @param pobsCor the observed (partial) correlation between these varaiables\n//' @param pGroupFactor an integer vector labeling group membership, to correct\n//' for in the partial correlation. NEEDS TO BE ZERO BASED!\n//' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n//' the number of members of each group (basically table(pGroupFactor)) as integer vector\n//' @param pnumGroup how many groups are there (len(pGroupSize))\n//' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n//' @param pn length of x and y, as a REAL NUMBER\n//' @param preq_alpha the required alpha for significance\n//' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n//' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n//' per quickstop paper (used to determine the log-odds)\n//' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n//' random number generator. Note that currently, these values get modified per call, so pass in a copy\n//' if you wish to keep a seed for running same simulation twice\n//'\n//' @return a double vector of length 4, entry 1 is either 0, 1 (for TRUE/FALSE) or NA_REAL_ for significance determination\n//' NA_REAL_ is returned when the MaxIter were reached before a decision is made. Usually, this occurs when the real p value is close to, or\n//' falls within the tolerance region of (req_alpha, req_alpha+tolerance_par). Entry 2 is the current p value estimate. entry 3 is the total\n//' number of iterations performed. Entry 4 is the number of time a permuted value was larger in absolute value than the observed cor.\n//'\n//' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP\n//'\n//'\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed);\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `partialCorQUICKSTOP` function in this Rcpp code?",
        "answer": "The `partialCorQUICKSTOP` function performs significance testing for partial correlation using the QUICKSTOP method. It tests whether the observed partial correlation is significant at a given alpha level, performing up to a maximum number of permutations. The function supports grouping by discrete categories when calculating partial correlation and currently only performs two-sided tests."
      },
      {
        "question": "What are the key input parameters for the `partialCorQUICKSTOP` function and their purposes?",
        "answer": "The key input parameters include: `pin_x` and `pin_y` (vectors to correlate), `pobsCor` (observed partial correlation), `pGroupFactor` (group membership labels), `pGroupSize` (count of members in each group), `pMaxIter` (maximum number of iterations), `preq_alpha` (required alpha for significance), `ptolerance_par` (tolerance region for quickstop), and `pseed` (seed for random number generator). These parameters allow the function to perform the partial correlation analysis with specific constraints and settings."
      },
      {
        "question": "What does the function return, and how should the output be interpreted?",
        "answer": "The function returns a double vector of length 4. The first entry is 0, 1, or NA_REAL_ for the significance determination (FALSE, TRUE, or undetermined). NA_REAL_ is returned when MaxIter is reached before a decision. The second entry is the current p-value estimate. The third entry is the total number of iterations performed. The fourth entry is the number of times a permuted value was larger in absolute value than the observed correlation. This output provides both the significance result and additional information about the testing process."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#include <Rcpp.h>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    // Implementation here\n}",
        "complete": "#include <Rcpp.h>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    Rcpp::NumericVector x(pin_x);\n    Rcpp::NumericVector y(pin_y);\n    double obsCor = Rcpp::as<double>(pobsCor);\n    Rcpp::IntegerVector groupFactor(pGroupFactor);\n    Rcpp::IntegerVector groupSize(pGroupSize);\n    int numGroup = Rcpp::as<int>(pnumGroup);\n    double maxIter = Rcpp::as<double>(pMaxIter);\n    double n = Rcpp::as<double>(pn);\n    double req_alpha = Rcpp::as<double>(preq_alpha);\n    double tolerance_par = Rcpp::as<double>(ptolerance_par);\n    double log_decision_boundary = Rcpp::as<double>(plog_decision_boundary);\n    Rcpp::NumericVector seed(pseed);\n\n    // Implement QUICKSTOP algorithm here\n    // ...\n\n    Rcpp::NumericVector result(4);\n    // Set result values\n    // ...\n\n    return result;\n}"
      },
      {
        "partial": "#include <Rcpp.h>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    // Convert input parameters\n    // ...\n\n    // Implement QUICKSTOP algorithm\n    double pValue = 0.0;\n    int iterations = 0;\n    int exceedCount = 0;\n\n    // Main loop\n    while (iterations < maxIter) {\n        // Permutation and correlation calculation\n        // ...\n\n        // Update pValue and exceedCount\n        // ...\n\n        // Check for early stopping condition\n        // ...\n\n        iterations++;\n    }\n\n    // Prepare and return result\n    // ...\n}",
        "complete": "#include <Rcpp.h>\n#include <random>\n#include <algorithm>\n\n// [[Rcpp::export]]\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x,\n               SEXP pin_y,\n               SEXP pobsCor,\n               SEXP pGroupFactor,\n               SEXP pGroupSize,\n               SEXP pnumGroup,\n               SEXP pMaxIter,\n               SEXP pn,\n               SEXP preq_alpha,\n               SEXP ptolerance_par,\n               SEXP plog_decision_boundary,\n               SEXP pseed) {\n    Rcpp::NumericVector x(pin_x), y(pin_y);\n    double obsCor = Rcpp::as<double>(pobsCor);\n    Rcpp::IntegerVector groupFactor(pGroupFactor), groupSize(pGroupSize);\n    int numGroup = Rcpp::as<int>(pnumGroup);\n    double maxIter = Rcpp::as<double>(pMaxIter);\n    double n = Rcpp::as<double>(pn);\n    double req_alpha = Rcpp::as<double>(preq_alpha);\n    double tolerance_par = Rcpp::as<double>(ptolerance_par);\n    double log_decision_boundary = Rcpp::as<double>(plog_decision_boundary);\n    Rcpp::NumericVector seed(pseed);\n\n    std::mt19937 gen(seed[0]);\n    double pValue = 0.0;\n    int iterations = 0;\n    int exceedCount = 0;\n\n    while (iterations < maxIter) {\n        std::vector<double> permuted_x = Rcpp::as<std::vector<double>>(x);\n        for (int i = 0; i < numGroup; ++i) {\n            std::shuffle(permuted_x.begin() + groupFactor[i], \n                         permuted_x.begin() + groupFactor[i] + groupSize[i], gen);\n        }\n\n        double permCor = calculatePartialCorrelation(permuted_x, y, groupFactor);\n        if (std::abs(permCor) >= std::abs(obsCor)) exceedCount++;\n\n        pValue = (exceedCount + 1.0) / (iterations + 1.0);\n        iterations++;\n\n        if (pValue <= req_alpha - tolerance_par || pValue > req_alpha + tolerance_par) {\n            if (std::log(pValue / (1 - pValue)) <= -log_decision_boundary) break;\n        }\n    }\n\n    Rcpp::NumericVector result = Rcpp::NumericVector::create(\n        (pValue <= req_alpha) ? 1.0 : (iterations == (int)maxIter ? NA_REAL : 0.0),\n        pValue, iterations, exceedCount\n    );\n    return result;\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/mergePSets.R",
    "language": "R",
    "content": "#' @include PharmacoSet-class.R\nNULL\n\nmergePSets <- function(mDataPSet, sensDataPSet, commonCellsOnly=FALSE, ...){\n\n\tif(commonCellsOnly){\n\t\tcommon <- intersectPSet(list(mDataPSet, sensDataPSet), intersectOn=c(\"celllines\"))\n\t\tmDataPSet <- common[[1]]\n\t\tsensDataPSet <- common[[2]]\n\t}\n\n  ### union of cell lines\n\tucell <- union(sampleNames(sensDataPSet), sampleNames(mDataPSet))\n\n  ### molecular profiles\n\tmergePSet <- sensDataPSet\n\tmolecularProfilesSlot(mergePSet) <- molecularProfilesSlot(mDataPSet)\n\n  ### number of sensitivity experiments\n  #acell <- setdiff(rownames(sampleInfo(mDataPSet)), rownames(sampleInfo(sensDataPSet)))\n\n\t### cell line annotations\n\tnew.cell.info <- c(union(colnames(sampleInfo(sensDataPSet)), colnames(sampleInfo(mDataPSet))), \"dataset\")\n\tcell.info.df <- data.frame(matrix(NA, nrow=length(ucell), ncol=length(new.cell.info), dimnames=list(ucell, new.cell.info)), check.names=FALSE)\n\tcell.info.df[rownames(sampleInfo(mDataPSet)), colnames(sampleInfo(mDataPSet))] <- sampleInfo(mDataPSet)\n\tcell.info.df[rownames(sampleInfo(sensDataPSet)), colnames(sampleInfo(sensDataPSet))] <- sampleInfo(sensDataPSet)\n\tcell.info.df[setdiff(rownames(sampleInfo(sensDataPSet)), rownames(sampleInfo(mDataPSet))), \"dataset\"] <- name(sensDataPSet)\n\tcell.info.df[setdiff(rownames(sampleInfo(mDataPSet)), rownames(sampleInfo(sensDataPSet))), \"dataset\"] <- name(mDataPSet)\n\tcell.info.df[intersect(rownames(sampleInfo(mDataPSet)), rownames(sampleInfo(sensDataPSet))), \"dataset\"] <- paste0(name(sensDataPSet), \"///\", name(mDataPSet))\n\tsampleInfo(mergePSet) <- cell.info.df\n\n\t### curation of cell line names\n\tnew.cell.curation <- c(union(colnames(curation(sensDataPSet)$sample), colnames(curation(mDataPSet)$sample)), \"dataset\")\n\tcell.curation.df <- data.frame(matrix(NA, nrow=length(ucell), ncol=length(new.cell.curation), dimnames=list(ucell, new.cell.curation)), check.names=FALSE)\n\tcell.curation.df[rownames(curation(mDataPSet)$sample), colnames(curation(mDataPSet)$sample)] <- curation(mDataPSet)$sample\n\tcell.curation.df[rownames(curation(sensDataPSet)$sample), colnames(curation(sensDataPSet)$sample)] <- curation(sensDataPSet)$sample\n\tcell.curation.df[setdiff(rownames(curation(sensDataPSet)$sample), rownames(curation(mDataPSet)$sample)), \"dataset\"] <- name(sensDataPSet)\n\tcell.curation.df[setdiff(rownames(curation(mDataPSet)$sample), rownames(curation(sensDataPSet)$sample)), \"dataset\"] <- name(mDataPSet)\n\tcell.curation.df[intersect(rownames(curation(mDataPSet)$sample), rownames(curation(sensDataPSet)$sample)), \"dataset\"] <- paste0(name(sensDataPSet), \"///\", name(mDataPSet))\n\tcuration(mergePSet)$sample <- cell.curation.df\n\n\t### curation of tissue names\n\tnew.tissue.curation <- c(union(colnames(curation(sensDataPSet)$tissue), colnames(curation(mDataPSet)$tissue)), \"dataset\")\n\ttissue.curation.df <- data.frame(matrix(NA, nrow=length(ucell), ncol=length(new.tissue.curation), dimnames=list(ucell, new.tissue.curation)), check.names=FALSE)\n\ttissue.curation.df[rownames(curation(mDataPSet)$tissue), colnames(curation(mDataPSet)$tissue)] <- curation(mDataPSet)$tissue\n\ttissue.curation.df[rownames(curation(sensDataPSet)$tissue), colnames(curation(sensDataPSet)$tissue)] <- curation(sensDataPSet)$tissue\n\ttissue.curation.df[setdiff(rownames(curation(sensDataPSet)$tissue), rownames(curation(mDataPSet)$tissue)), \"dataset\"] <- name(sensDataPSet)\n\ttissue.curation.df[setdiff(rownames(curation(mDataPSet)$tissue), rownames(curation(sensDataPSet)$tissue)), \"dataset\"] <- name(mDataPSet)\n\ttissue.curation.df[intersect(rownames(curation(mDataPSet)$tissue), rownames(curation(sensDataPSet)$tissue)), \"dataset\"] <- paste0(name(sensDataPSet), \"///\", name(mDataPSet))\n\tcuration(mergePSet)$tissue <- tissue.curation.df\n\n\tsensNumber(mergePSet) <- PharmacoGx:::.summarizeSensitivityNumbers(mergePSet)\n\n\tannotation(mergePSet)$name <- paste(name(mDataPSet), name(sensDataPSet), sep=\".\")\n\n\tannotation(mergePSet)$dateCreated <- date()\n\n\tcheckPSetStructure(mergePSet)\n\n    return(mergePSet)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mergePSets` function and what are its main input parameters?",
        "answer": "The `mergePSets` function is designed to merge two PharmacoSet objects: one containing molecular data (mDataPSet) and another containing sensitivity data (sensDataPSet). The main input parameters are:\n1. mDataPSet: A PharmacoSet object with molecular data\n2. sensDataPSet: A PharmacoSet object with sensitivity data\n3. commonCellsOnly: A boolean parameter to determine if only common cell lines should be kept (default is FALSE)\n\nThe function combines the molecular profiles from mDataPSet with the sensitivity data from sensDataPSet, merging cell line annotations, curations, and other relevant information."
      },
      {
        "question": "How does the function handle the merging of cell line information when there are differences between the two input PharmacoSet objects?",
        "answer": "The function handles merging cell line information as follows:\n1. It creates a union of cell lines from both input PharmacoSets.\n2. For sample information, it combines columns from both inputs into a new data frame.\n3. It fills in values from both input PharmacoSets, giving priority to sensDataPSet when there's an overlap.\n4. A new 'dataset' column is added to track the origin of each cell line:\n   - Unique to sensDataPSet: labeled with sensDataPSet's name\n   - Unique to mDataPSet: labeled with mDataPSet's name\n   - Common to both: labeled with both names separated by '///'\n5. This process is repeated for both sample information and curation data (sample and tissue).\n\nThis approach ensures that all cell line information is preserved and the source of each piece of data is traceable."
      },
      {
        "question": "What steps does the function take to ensure the integrity and completeness of the merged PharmacoSet object?",
        "answer": "The function takes several steps to ensure the integrity and completeness of the merged PharmacoSet object:\n1. It uses the `union` function to ensure all unique cell lines from both input sets are included.\n2. It merges molecular profiles, sample information, and curation data comprehensively.\n3. It handles potential conflicts in cell line data by preserving information from both input sets and adding a 'dataset' column to track the origin.\n4. It updates the sensitivity numbers using the `.summarizeSensitivityNumbers` function.\n5. It creates a new name for the merged set by combining names of input sets.\n6. It adds a 'dateCreated' annotation to record when the merge occurred.\n7. Finally, it calls `checkPSetStructure` to verify that the resulting merged PharmacoSet object has a valid structure.\n\nThese steps ensure that the merged object contains all necessary data, maintains traceability, and adheres to the expected PharmacoSet structure."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_view_helpers.R",
    "language": "R",
    "content": "#' Get all available annotation headings\n#'\n#' https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON will return a list of all available headings\n#'\n#' @keywords internal\n#' @noRd\n.get_all_heading_types_base <- function() {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON\"\n  req <- .build_pubchem_request(url)\n  response <- httr2::req_perform(req) |> .parse_resp_json()\n  .asDT(response[[1]][[1]])\n}\n\n#' @keywords internal\n.get_all_heading_types <- memoise::memoise(.get_all_heading_types_base)\n\n#' Build a PubChem REST query URL\n#'\n#' pubchem<DOT>ncbi<DOT>nlm<DOT>nih<DOT>gov/rest/pug_view/<ANNOTATION>/<RECORD>/<ID>/<OUTPUT><?OPTIONS>\n#'\n#' Optional Parameters:\n#'   - annotation: data, index, annotations, categories, neighbors, literature, structure, image, qr, linkout\n#'  - record: compound, substance, assay, cell, gene, protein\n#' - filter (Parameter, Value, Description):\n#'      - Page, n <integer>, The page number to retrieve. Retrieves the nth page of the output data when the data is paginated.\n#'          This option is useful when downloading the annotations under a specific heading for all compounds\n#'          By default, retrieve all pages of the output data.\n#'      - Version, n <int> for substance | n.m <int.int> for assay, data for a particular version of a substance or assay record\n#'      - Heading, (Sub)heading name, The name of the annotation heading or subheading to retrieve\n#'      - Source, source name, The name of the annotation source to retrieve from a specified source\n#'\n#' @param id The identifier for the query.\n#' @param annotation The type of annotation to retrieve. Options include \"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", or \"linkout\".\n#' @param record The type of record to retrieve. Options include \"compound\", \"substance\", \"assay\", \"cell\", \"gene\", or \"protein\".\n#' @param page The page number to retrieve. Retrieves the nth page of the output data when the data is paginated. By default, retrieve all pages of the output data.\n#' @param version The version of the record to retrieve. For substance, this is an integer. For assay, this is a string in the format \"n.m\".\n#' @param heading The name of the annotation heading or subheading to retrieve.\n#' @param source The name of the annotation source to retrieve from a specified source.\n#' @param output The desired output format. Options are \"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\".\n#'\n#' @return The query URL\n#'\n#' @keywords internal\n#' @noRd\n.build_pubchem_view_query <- function(\n    id, annotation = \"data\", record = \"compound\", \n    page = NULL, version = NULL, heading = NULL, source = NULL,\n    output = \"JSON\", ...\n) {\n  funContext <- .funContext(\".build_pubchem_view_query\")\n\n  # Check the inputs\n  checkmate::assert_choice(\n    annotation,\n    c(\"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", \"linkout\")\n  )\n  checkmate::assert_choice(record, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n\n  # Configure the options for the query\n  opts_ <- list()\n  if (!is.null(heading)) {\n    if (record == \"substance\") {\n      .debug(\n        funContext,\n        \" fyi: https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON\n                 has no substance headings\"\n      )\n    } else {\n      checkmate::assert(heading %in% .get_all_heading_types()$Heading)\n    }\n    opts_ <- c(opts_, list(heading = heading))\n  }\n  if (!is.null(version)) {\n    if (record == \"substance\") {\n      checkmate::assert_string(version, min.chars = 1)\n    } else {\n      checkmate::assert_numeric(version, lower = 1)\n    }\n    opts_ <- c(opts_, list(version = version))\n  }\n\n  if (!is.null(source)) {\n    checkmate::assert_string(source, min.chars = 1)\n    opts_ <- c(opts_, list(source = source))\n  }\n\n  if (!is.null(page)) {\n    checkmate::assert_numeric(page, lower = 1)\n    opts_ <- c(opts_, list(page = page))\n  }\n\n  base_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, annotation, record, id, output)\n  url$query <- opts_\n\n  url |>\n    httr2::url_build() |>\n    httr2::request()\n  \n  # url |>\n    # httr2::url_build() |>\n    # .build_request()\n}\n\n#' Generic function to parse one of the annotation helpers\n#'\n#' @noRd\n#' @keywords internal\n.clean_parsed_annotation <- function(result) {\n  # If returned value is a list, concatenate the elements into a single string with \";\"\n  if (length(result) > 1) {\n    return(paste(result, collapse = \"; \"))\n  }\n  return(result)\n}\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseCHEMBLresponse <- function(result) {\n  gsub(\"Compound::\", \"\", result[[\"Record\"]][[\"Reference\"]][[\"SourceID\"]]) |>\n    .clean_parsed_annotation()\n}\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseCASresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"CAS Common Chemistry\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseNSCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"DTP/NCI\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseATCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"WHO Anatomical Therapeutic Chemical (ATC) Classification\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n\n#' Parses the JSON response from an HTTP request.\n#'\n#' @noRd\n#' @keywords internal\n.parseDILIresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"Drug Induced Liver Injury Rank (DILIrank) Dataset\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.get_all_heading_types_base` function and how is it used in conjunction with `.get_all_heading_types`?",
        "answer": "The `.get_all_heading_types_base` function retrieves all available annotation headings from the PubChem API. It sends a request to the specified URL, parses the JSON response, and returns the result as a data table. The `.get_all_heading_types` function is a memoized version of `.get_all_heading_types_base`, which caches the results for improved performance on subsequent calls."
      },
      {
        "question": "In the `.build_pubchem_view_query` function, what are the key parameters and how are they validated?",
        "answer": "The key parameters for `.build_pubchem_view_query` include `id`, `annotation`, `record`, `page`, `version`, `heading`, `source`, and `output`. The function validates these parameters using `checkmate::assert_choice` for `annotation` and `record`, ensuring they match predefined options. It also checks if the `heading` is valid by comparing it against the list of available headings retrieved from `.get_all_heading_types()`. The `version` parameter is validated differently for 'substance' and other record types."
      },
      {
        "question": "How does the `.clean_parsed_annotation` function handle different types of input, and where is it used in the code snippet?",
        "answer": "The `.clean_parsed_annotation` function is designed to handle both single values and lists. If the input `result` has a length greater than 1, it concatenates the elements into a single string using semicolons as separators. Otherwise, it returns the single value as-is. This function is used in various parsing functions like `.parseCHEMBLresponse`, `.parseCASresponse`, `.parseNSCresponse`, `.parseATCresponse`, and `.parseDILIresponse` to standardize the output format of parsed annotations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Partial code for .build_pubchem_view_query function\n.build_pubchem_view_query <- function(\n    id, annotation = \"data\", record = \"compound\", \n    page = NULL, version = NULL, heading = NULL, source = NULL,\n    output = \"JSON\", ...\n) {\n  funContext <- .funContext(\".build_pubchem_view_query\")\n\n  # Check the inputs\n  checkmate::assert_choice(\n    annotation,\n    c(\"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", \"linkout\")\n  )\n  checkmate::assert_choice(record, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n\n  # Configure the options for the query\n  opts_ <- list()\n  # Add code here to handle options\n\n  base_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view\"\n  url <- httr2::url_parse(base_url)\n  # Complete the URL construction\n\n  # Return the request\n}",
        "complete": "# Complete code for .build_pubchem_view_query function\n.build_pubchem_view_query <- function(\n    id, annotation = \"data\", record = \"compound\", \n    page = NULL, version = NULL, heading = NULL, source = NULL,\n    output = \"JSON\", ...\n) {\n  funContext <- .funContext(\".build_pubchem_view_query\")\n\n  # Check the inputs\n  checkmate::assert_choice(\n    annotation,\n    c(\"data\", \"index\", \"annotations\", \"categories\", \"neighbors\", \"literature\", \"structure\", \"image\", \"qr\", \"linkout\")\n  )\n  checkmate::assert_choice(record, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n\n  # Configure the options for the query\n  opts_ <- list()\n  if (!is.null(heading)) {\n    if (record == \"substance\") {\n      .debug(funContext, \" fyi: https://pubchem.ncbi.nlm.nih.gov/rest/pug/annotations/headings/JSON has no substance headings\")\n    } else {\n      checkmate::assert(heading %in% .get_all_heading_types()$Heading)\n    }\n    opts_ <- c(opts_, list(heading = heading))\n  }\n  if (!is.null(version)) {\n    if (record == \"substance\") {\n      checkmate::assert_string(version, min.chars = 1)\n    } else {\n      checkmate::assert_numeric(version, lower = 1)\n    }\n    opts_ <- c(opts_, list(version = version))\n  }\n  if (!is.null(source)) {\n    checkmate::assert_string(source, min.chars = 1)\n    opts_ <- c(opts_, list(source = source))\n  }\n  if (!is.null(page)) {\n    checkmate::assert_numeric(page, lower = 1)\n    opts_ <- c(opts_, list(page = page))\n  }\n\n  base_url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug_view\"\n  url <- httr2::url_parse(base_url)\n  url$path <- .buildURL(url$path, annotation, record, id, output)\n  url$query <- opts_\n\n  url |>\n    httr2::url_build() |>\n    httr2::request()\n}"
      },
      {
        "partial": "# Partial code for parsing functions\n.clean_parsed_annotation <- function(result) {\n  # Implement the function\n}\n\n.parseCHEMBLresponse <- function(result) {\n  # Implement the function\n}\n\n.parseCASresponse <- function(result) {\n  # Implement the function\n}\n\n.parseNSCresponse <- function(result) {\n  # Implement the function\n}\n\n.parseATCresponse <- function(result) {\n  # Implement the function\n}\n\n.parseDILIresponse <- function(result) {\n  # Implement the function\n}",
        "complete": "# Complete code for parsing functions\n.clean_parsed_annotation <- function(result) {\n  if (length(result) > 1) {\n    return(paste(result, collapse = \"; \"))\n  }\n  return(result)\n}\n\n.parseCHEMBLresponse <- function(result) {\n  gsub(\"Compound::\", \"\", result[[\"Record\"]][[\"Reference\"]][[\"SourceID\"]]) |>\n    .clean_parsed_annotation()\n}\n\n.parseCASresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"CAS Common Chemistry\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n.parseNSCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"DTP/NCI\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n.parseATCresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"WHO Anatomical Therapeutic Chemical (ATC) Classification\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}\n\n.parseDILIresponse <- function(result) {\n  df <- result[[\"Record\"]][[\"Reference\"]]\n  df[df$SourceName == \"Drug Induced Liver Injury Rank (DILIrank) Dataset\", \"SourceID\"] |>\n    .clean_parsed_annotation()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/ops/ops.py",
    "language": "py",
    "content": "from typing import List, TypeVar, Sequence, Union, Tuple, Optional, Any\n\nimport numpy as np\nimport SimpleITK as sitk\n\nfrom .functional import *\nfrom ..io import *\nfrom ..utils import image_to_array, array_to_image, crawl, physical_points_to_idxs\nfrom ..modules import map_over_labels\nfrom ..modules import DataGraph\n\n\nLoaderFunction = TypeVar('LoaderFunction')\nImageFilter = TypeVar('ImageFilter')\nFunction = TypeVar('Function')\nStructureSet = TypeVar('StructureSet')\n\n\n# Base class\nclass BaseOp:\n    def __call__(self, *args, **kwargs):\n        raise NotImplementedError\n\n    def __repr__(self):\n        attrs = [(k, v) for k, v in self.__dict__.items()\n                 if not k.startswith(\"_\")]\n        attrs = [(k, f\"'{v}'\") if isinstance(v, str) else (k, v) for k, v in attrs]\n        args = \", \".join(f\"{k}={v}\" for k, v in attrs)\n        return f\"{self.__class__.__name__}({args})\"\n\n\n# Input/output\nclass BaseInput(BaseOp):\n    def __init__(self, loader):\n        if not isinstance(loader, BaseLoader):\n            raise ValueError(\n                f\"loader must be a subclass of io.BaseLoader, got {type(loader)}\"\n            )\n        self._loader = loader\n\n    def __call__(self, key):\n        inputs = self._loader.get(key)\n        return inputs\n\n\nclass BaseOutput(BaseOp):\n    def __init__(self, writer):\n        if not isinstance(writer, BaseWriter):\n            raise ValueError(\n                f\"writer must be a subclass of io.BaseWriter, got {type(writer)}\"\n            )\n        self._writer = writer\n\n    def __call__(self, key, *args, **kwargs):\n        self._writer.put(key, *args, **kwargs)\n\n\nclass BetaAutoInput(BaseInput):\n    \"\"\"ImageAutoInput class is a wrapper class around ImgCSVloader which looks for the specified directory and crawls through it as the first step. Using the crawled output data, a graph on modalties present in the dataset is formed\n    which stores the relation between all the modalities.\n    Based on the user provided modalities, this class loads the information of the user provided modalities\n\n    Parameters\n    ----------\n    dir_path: str\n        Path to dataset top-level directory. The crawler/indexer will start at this directory.\n\n    modalities: str\n        List of modalities to process. Only samples with ALL modalities will be processed. Make sure there are no space between list elements as it is parsed as a string.\n\n    visualize: bool\n        Whether to return visualization of the data graph\n\n    update: bool\n        Whether to update crawled index\n    \"\"\"\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        # To be changed later\n        df_crawl_path   = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        tree_crawl_path = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.json\").as_posix()\n\n        if not os.path.exists(df_crawl_path) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs = n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        import json\n        with open(tree_crawl_path, 'r') as f:\n            tree_db = json.load(f)  # currently unused, TO BE implemented in the future\n            assert tree_db is not None, \"There was no crawler output\" # dodging linter\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent, \".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=df_crawl_path, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [(\"_\").join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.study_names  = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"study\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        self.subseries_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"subseries\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]\n\n        loader = ImageCSVLoader(self.df_combined,\n                                col_names=self.column_names,\n                                study_names=self.study_names,\n                                series_names=self.series_names,\n                                subseries_names=self.subseries_names,\n                                id_column=None,\n                                expand_paths=False,\n                                readers=self.readers)\n        \n        super().__init__(loader)\n\n\nclass ImageAutoInput(BaseInput):\n    \"\"\"ImageAutoInput class is a wrapper class around ImgCSVloader which looks for the specified directory and crawls through it as the first step. Using the crawled output data, a graph on modalties present in the dataset is formed\n    which stores the relation between all the modalities.\n    Based on the user provided modalities, this class loads the information of the user provided modalities\n\n    Parameters\n    ----------\n    dir_path: str\n        Path to dataset top-level directory. The crawler/indexer will start at this directory.\n\n    modalities: str\n        List of modalities to process. Only samples with ALL modalities will be processed. Make sure there are no space between list elements as it is parsed as a string.\n\n    visualize: bool\n        Whether to return visualization of the data graph\n\n    update: bool\n        Whether to update crawled index\n    \"\"\"\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        # To be changed later\n        path_crawl = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        if not os.path.exists(path_crawl) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs=n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent,\".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=path_crawl, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [(\"_\").join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]\n\n        loader = ImageCSVLoader(self.df_combined,\n                                colnames=self.column_names,\n                                seriesnames=self.series_names,\n                                id_column=None,\n                                expand_paths=False,\n                                readers=self.readers)\n        \n        super().__init__(loader)\n\n\nclass ImageCSVInput(BaseInput):\n    \"\"\"ImageCSVInput class looks for a CSV file in a specified directory and loads images based on the information in the file.\n\n    Parameters\n    ----------\n    csv_path: str\n        Directory where the CSV file is stored.\n\n    colnames: List[str]\n        List of column names in the CSV file to be used for image loading.\n\n    id_column: str, optional\n        A column name to be used as the subject ID.\n\n    reader: LoaderFunction\n        The functions used to read images.\n        The functions are implemented in the imgtools.io module.\n        The options are:\n        - read_image\n        - read_dicom_series\n        - read_dicom_rtstruct\n        - read_segmentation\n    \"\"\"\n    def __init__(self,\n                 csv_path_or_dataframe: str,\n                 colnames: List[str] = None,\n                 id_column: Optional[str] = None,\n                 expand_paths: bool = True,\n                 readers: List[LoaderFunction] = None):  # no mutable defaults: https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n        if colnames is None:\n            colnames = []\n        if readers is None:\n            readers = [read_image]\n        self.csv_path_or_dataframe = csv_path_or_dataframe\n        self.colnames = colnames\n        self.id_column = id_column\n        self.expand_paths = expand_paths\n        self.readers = readers\n        loader = ImageCSVLoader(self.csv_path_or_dataframe,\n                                colnames=self.colnames,\n                                id_column=self.id_column,\n                                expand_paths=self.expand_paths,\n                                readers=self.readers)\n        super().__init__(loader)\n\n\nclass ImageFileInput(BaseInput):\n    \"\"\"ImageFileInput class looks for images in a specified directory and reads them by using a reader function.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the image files are stored.\n\n    get_subject_id_from: str\n        How to extract the subject ID. The options are:\n        - 'filename' indicates the image reader to use filename as the subject ID. (default)\n        - 'subject_directory' indicates the image reader to use the name of the subject directory.\n\n    subdir_path: str, optional\n        Path to a subdirectory where the image is stored.\n\n    exclude_paths: List[str], optional\n        Directory paths to be excluded when searching for the image files to be loaded.\n\n    reader: LoaderFunction\n        The function used to read individual images.\n        The functions are implemented in the imgtools.io module.\n        The options are:\n        - read_image\n        - read_dicom_series\n        - read_dicom_rtstruct\n        - read_segmentation\n        - read_dicom_auto\n        - read_dicom_rtdose\n        - read_dicom_pet\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 get_subject_id_from: str = \"filename\",\n                 subdir_path: Optional[str] = None,\n                 exclude_paths: Optional[List[str]] = None,  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n                 reader: LoaderFunction = None):\n        if exclude_paths is None:\n            exclude_paths = []\n        if reader is None:\n            reader = read_image\n        self.root_directory = root_directory\n        self.get_subject_id_from = get_subject_id_from\n        self.subdir_path = subdir_path\n        self.exclude_paths = exclude_paths\n        self.reader = reader\n        loader = ImageFileLoader(self.root_directory,\n                                 self.get_subject_id_from,\n                                 self.subdir_path,\n                                 self.exclude_paths,\n                                 self.reader)\n        super().__init__(loader)\n\n\nclass ImageFileOutput(BaseOutput):\n    \"\"\"ImageFileOutput class outputs processed images as one of the image file formats.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed image files will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.nrrd as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    compress: bool, optional\n        Specify whether to enable compression for NRRD format.\n        Set to be true as default.\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.nrrd\",\n                 create_dirs: Optional[bool] =True,\n                 compress: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n        \n        if \".seg\" in filename_format:  # from .seg.nrrd bc it is now .nii.gz\n            writer_class = SegNrrdWriter\n        else:\n            writer_class = ImageFileWriter\n            \n        writer = writer_class(self.root_directory,\n                              self.filename_format,\n                              self.create_dirs,\n                              self.compress)\n        \n        super().__init__(writer)\n\n\n# Resampling ops\nclass ImageSubjectFileOutput(BaseOutput):\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] = \"{subject_id}.nii.gz\",\n                 create_dirs: Optional[bool] = True,\n                 compress: Optional[bool] = True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.compress = compress\n\n        writer = BaseSubjectWriter(self.root_directory,\n                                   self.filename_format,\n                                   self.create_dirs,\n                                   self.compress)\n        \n        super().__init__(writer)\n\n\nclass ImageAutoOutput:\n    \"\"\"\n    Wrapper class around ImageFileOutput. This class supports multiple modalities writers and calls ImageFileOutput for writing the files\n    \n    Parameters\n    ----------\n    root_directory: str\n        The directory where all the processed files will be stored in the form of nrrd\n\n    output_streams: List[str]\n        The modalties that should be stored. This is typically equal to the column names of the table returned after graph querying. Examples is provided in the \n        dictionary file_name\n    \"\"\"\n    def __init__(self,\n                 root_directory: str,\n                 output_streams: List[str],\n                 nnunet_info: Dict = None,\n                 inference: bool = False,):\n                 \n        self.output = {}\n        for colname in output_streams:\n            # Not considering colnames ending with alphanumeric\n            colname_process = (\"_\").join([item for item in colname.split(\"_\") if not item.isnumeric()])\n            colname_process = colname  # temproary force #\n            if not nnunet_info and not inference:\n                self.output[colname_process] = ImageSubjectFileOutput(pathlib.Path(root_directory,\"{subject_id}\",colname_process.split(\".\")[0]).as_posix(),\n                                                                      filename_format=\"{}.nii.gz\".format(colname_process))\n            elif inference:\n                self.output[colname_process] = ImageSubjectFileOutput(root_directory,\n                                                                      filename_format=\"{subject_id}_{modality_index}.nii.gz\")\n            else:\n                self.output[colname_process] = ImageSubjectFileOutput(pathlib.Path(root_directory,\"{label_or_image}{train_or_test}\").as_posix(),\n                                                                      filename_format=\"{subject_id}_{modality_index}.nii.gz\")\n    \n    def __call__(self, \n                 subject_id: str,\n                 img: sitk.Image,\n                 output_stream,\n                 is_mask: bool = False,\n                 mask_label: Optional[str] = \"\",\n                 label_or_image: str=\"images\",\n                 train_or_test: str=\"Tr\",\n                 nnunet_info: Dict=None):\n                 \n        self.output[output_stream](subject_id, img, is_mask=is_mask, mask_label=mask_label, label_or_image=label_or_image, train_or_test=train_or_test, nnunet_info=nnunet_info)\n\n\nclass NumpyOutput(BaseOutput):\n    \"\"\"NumpyOutput class processed images as NumPy files.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed NumPy files will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.npy as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.npy\",\n                 create_dirs: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        writer = NumpyWriter(self.root_directory, self.filename_format, self.create_dirs)\n        super().__init__(writer)\n\n\nclass HDF5Output(BaseOutput):\n    \"\"\"HDF5Output class outputs the processed image data in HDF5 format.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed .h5 file will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.h5 as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    save_geometry: bool, optional\n        Specify whether to save geometry data.\n        Set to be True as default.\n\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.h5\",\n                 create_dirs: Optional[bool] =True,\n                 save_geometry: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        self.save_geometry = save_geometry\n        writer = HDF5Writer(self.root_directory,\n                            self.filename_format,\n                            self.create_dirs,\n                            self.save_geometry)\n        super().__init__(writer)\n\n\nclass MetadataOutput(BaseOutput):\n    \"\"\"MetadataOutput class outputs the metadata of processed image files in .json format.\n\n    Parameters\n    ----------\n    root_directory: str\n        Root directory where the processed .json file will be stored.\n\n    filename_format: str, optional\n        The filename template.\n        Set to be {subject_id}.json as default.\n        {subject_id} will be replaced by each subject's ID at runtime.\n\n    create_dirs: bool, optional\n        Specify whether to create an output directory if it does not exit.\n        Set to be True as default.\n\n    \"\"\"\n\n    def __init__(self,\n                 root_directory: str,\n                 filename_format: Optional[str] =\"{subject_id}.json\",\n                 create_dirs: Optional[bool] =True):\n        self.root_directory = root_directory\n        self.filename_format = filename_format\n        self.create_dirs = create_dirs\n        writer = MetadataWriter(self.root_directory, self.filename_format, self.create_dirs)\n        super().__init__(writer)\n\n\n# Resampling ops\nclass Resample(BaseOp):\n    \"\"\"Resample operation class:\n    A callable class that resamples image to a given spacing, optionally applying a transformation.\n\n    To instantiate:\n        obj = Resample(spacing, interpolation, anti_alias, anti_alias_sigma, transform, output_size)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    spacing\n        The new image spacing. If float, assumes the same spacing in all\n        directions. Alternatively, a sequence of floats can be passed to\n        specify spacing along each dimension. Passing 0 at any position will\n        keep the original spacing along that dimension (useful for in-plane\n        resampling).\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `spacing < image.GetSpacing()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n    transform, optional\n        Transform to apply to input coordinates when resampling. If None,\n        defaults to identity transformation.\n\n    output_size, optional\n        Size of the output image. If None, it is computed to preserve the\n        whole extent of the input image.\n    \"\"\"\n\n    def __init__(self,\n                 spacing: Union[float, Sequence[float], np.ndarray],\n                 interpolation: str =\"linear\",\n                 anti_alias: bool =True,\n                 anti_alias_sigma: Optional[float] =None,\n                 transform: Optional[sitk.Transform] =None,\n                 output_size: Optional[Sequence[float]] =None):\n        self.spacing = spacing\n        self.interpolation = interpolation\n        self.anti_alias = anti_alias\n        self.anti_alias_sigma = anti_alias_sigma\n        self.transform = transform\n        self.output_size = output_size\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Resample callable object:\n        Resamples image to a given spacing, optionally applying a transformation..\n\n        Parameters\n        ----------\n        image\n            The image to resample.\n\n        Returns\n        -------\n        sitk.Image\n            The resampled image.\n        \"\"\"\n\n        return resample(image,\n                        spacing=self.spacing,\n                        interpolation=self.interpolation,\n                        anti_alias=self.anti_alias,\n                        anti_alias_sigma=self.anti_alias_sigma,\n                        transform=self.transform,\n                        output_size=self.output_size)\n\n\nclass Resize(BaseOp):\n    \"\"\"Resize operation class:\n    A callable class that resizes image to a given size by resampling coordinates.\n\n    To instantiate:\n        obj = Resize(size, interpolation, anti_alias, anti_alias_sigma)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    size\n        The new image size. If float, assumes the same size in all directions.\n        Alternatively, a sequence of floats can be passed to specify size along\n        each dimension. Passing 0 at any position will keep the original\n        size along that dimension.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `size < image.GetSize()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n    \"\"\"\n\n    def __init__(self,\n                 size: Union[int, Sequence[int], np.ndarray],\n                 interpolation: str =\"linear\",\n                 anti_alias: bool =True,\n                 anti_alias_sigma: Optional[float] =None):\n        self.size = size\n        self.interpolation = interpolation\n        self.anti_alias = anti_alias\n        self.anti_alias_sigma = anti_alias_sigma\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Resize callable object: Resizes image to a given size by resampling coordinates.\n\n        Parameters\n        ----------\n        image\n            The image to resize.\n\n        Returns\n        -------\n        sitk.Image\n            The resized image.\n        \"\"\"\n\n        return resize(image,\n                      size=self.size,\n                      interpolation=self.interpolation,\n                      anti_alias_sigma=self.anti_alias_sigma)\n\n\nclass Zoom(BaseOp):\n    \"\"\"Zoom operation class: A callable class that rescales image, preserving its spatial extent.\n\n    To instantiate:\n        obj = Zoom(scale_factor, interpolation, anti_alias, anti_alias_sigma)\n\n    To call:\n        result = obj(image)\n\n    The rescaled image will have the same spatial extent (size) but will be\n    rescaled by `scale_factor` in each dimension. Alternatively, a separate\n    scale factor for each dimension can be specified by passing a sequence\n    of floats.\n\n    Parameters\n    ----------\n    scale_factor\n        If float, each dimension will be scaled by that factor. If tuple, each\n        dimension will be scaled by the corresponding element.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `size < image.GetSize()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n    \"\"\"\n\n    def __init__(self,\n                 scale_factor: Union[float, Sequence[float]],\n                 interpolation: str =\"linear\",\n                 anti_alias: bool =True,\n                 anti_alias_sigma: Optional[float] =None):\n        self.scale_factor = scale_factor\n        self.interpolation = interpolation\n        self.anti_alias = anti_alias\n        self.anti_alias_sigma = anti_alias_sigma\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Zoom callable object: Rescales image, preserving its spatial extent.\n\n        The rescaled image will have the same spatial extent (size) but will be\n        rescaled by `scale_factor` in each dimension. Alternatively, a separate\n        scale factor for each dimension can be specified by passing a sequence\n        of floats.\n\n        Parameters\n        ----------\n        image\n            The image to rescale.\n\n        Returns\n        -------\n        sitk.Image\n            The rescaled image.\n        \"\"\"\n\n        return zoom(image,\n                    self.scale_factor,\n                    interpolation=self.interpolation,\n                    anti_alias=self.anti_alias,\n                    anti_alias_sigma=self.anti_alias_sigma)\n\n\nclass Rotate(BaseOp):\n    \"\"\"Rotate operation class: A callable class that rotates an image around a given centre.\n\n    To instantiate:\n        obj = Rotate(rotation_centre, angles, interpolation)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    rotation_centre\n        The centre of rotation in image coordinates.\n\n    angles\n        The angles of rotation around x, y and z axes.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n    \"\"\"\n\n    def __init__(self,\n                 rotation_centre: Sequence[float],\n                 angles: Union[float, Sequence[float]],\n                 interpolation: str =\"linear\"):\n        self.rotation_centre = rotation_centre\n        self.angles = angles\n        self.interpolation = interpolation\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"Rotate callable object: Rotates an image around a given centre.\n\n        Parameters\n        ----------\n        image\n            The image to rotate.\n\n        Returns\n        -------\n        sitk.Image\n            The rotated image.\n        \"\"\"\n\n        return rotate(image,\n                      rotation_centre=self.rotation_centre,\n                      angles=self.angles,\n                      interpolation=self.interpolation)\n\n\nclass InPlaneRotate(BaseOp):\n    \"\"\"InPlaneRotate operation class: A callable class that rotates an image on a plane.\n\n    To instantiate:\n        obj = InPlaneRotate(angle, interpolation)\n\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    angle\n        The angle of rotation.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n    \"\"\"\n    def __init__(self, angle: float, interpolation: str =\"linear\"):\n        self.angle = angle\n        self.interpolation = interpolation\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"InPlaneRotate callable object: Rotates an image on a plane.\n\n        Parameters\n        ----------\n        image\n            The image to rotate.\n\n        Returns\n        -------\n        sitk.Image\n            The rotated image.\n        \"\"\"\n\n        image_size = np.array(image.GetSize())\n        image_centre = image_size // 2\n        angles = (0., 0., self.angle)\n        return rotate(image,\n                      rotation_centre=image_centre.tolist(),\n                      angles=angles,\n                      interpolation=self.interpolation)\n\n\n# Cropping & mask ops\nclass Crop(BaseOp):\n    \"\"\"Crop operation class: A callable class that crops an image to\n    the desired size around a given centre.\n\n    To instantiate:\n        obj = Crop(crop_centre, size)\n\n    To call:\n        result = obj(image)\n\n    Note that the cropped image might be smaller than size in a particular\n    direction if the cropping window exceeds image boundaries.\n\n    Parameters\n    ----------\n\n    crop_centre\n        The centre of the cropping window in image coordinates.\n\n    size\n        The size of the cropping window along each dimension in pixels. If\n        float, assumes the same size in all directions. Alternatively, a\n        sequence of floats can be passed to specify size along x, y and z\n        dimensions. Passing 0 at any position will keep the original size along\n        that dimension.\n    \"\"\"\n\n    def __init__(self, crop_centre: Sequence[float], size: Union[int, Sequence[int], np.ndarray]):\n        self.crop_centre = crop_centre\n        self.size = size\n\n    def __call__(self, image) -> sitk.Image:\n        \"\"\"Crop callable object: Crops an image to the desired size around a given centre.\n\n        Note that the cropped image might be smaller than size in a particular\n        direction if the cropping window exceeds image boundaries.\n\n        Parameters\n        ----------\n        image\n            The image to crop.\n\n        Returns\n        -------\n        sitk.Image\n            The cropped image.\n        \"\"\"\n\n        return crop(image, crop_centre=self.crop_centre, size=self.size)\n\n\nclass CentreCrop(BaseOp):\n    \"\"\"CentreCrop operation class: A callable class that crops an image to the desired size\n    around the centre of an image.\n\n    To instantiate:\n        obj = CentreCrop(size)\n\n    To call:\n        result = obj(image)\n\n    Note that the cropped image might be smaller than size in a particular\n    direction if the cropping window exceeds image boundaries.\n\n    Parameters\n    ----------\n    size\n        The size of the cropping window along each dimension in pixels. If\n        float, assumes the same size in all directions. Alternatively, a\n        sequence of floats can be passed to specify size along x, y and z\n        dimensions. Passing 0 at any position will keep the original size along\n        that dimension.\n    \"\"\"\n\n    def __init__(self, size: Union[int, Sequence[int]]):\n        self.size = size\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"CentreCrop callable object: Crops an image to the desired size\n        around the centre of an image.\n\n        Note that the cropped image might be smaller than size in a particular\n        direction if the cropping window exceeds image boundaries.\n\n        Parameters\n        ----------\n        image\n            The image to crop.\n\n        Returns\n        -------\n        sitk.Image\n            The cropped image.\n        \"\"\"\n        image_size = np.array(image.GetSize())\n        image_centre = image_size // 2\n        return crop(image, crop_centre=image_centre, size=self.size)\n\n\nclass BoundingBox(BaseOp):\n    \"\"\"BoundingBox opetation class: A callable class that find the axis-aligned\n    bounding box of a region descriibed by a segmentation mask.\n\n    To instantiate:\n        obj = BoundingBox()\n\n    To call:\n        result = obj(mask, label)\n    \"\"\"\n\n    def __call__(self, mask: sitk.Image, label: int = 1) -> Tuple[Tuple, Tuple]:\n        \"\"\"BoundingBox callable object: Find the axis-aligned\n        bounding box of a region descriibed by a segmentation mask.\n\n        Parameters\n        ----------\n        mask\n            Segmentation mask describing the region of interest. Can be an image of\n            type unsigned int representing a label map or `segmentation.Segmentation`.\n\n        label, optional\n            Label to use when computing bounding box if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        tuple of tuples\n            The bounding box location and size. The first tuple gives the\n            coordinates of the corner closest to the origin and the second\n            gives the size in pixels along each dimension.\n        \"\"\"\n\n        return bounding_box(mask, label=label)\n\n\nclass Centroid(BaseOp):\n    \"\"\"Centroid operation class: A callable class that finds the centroid of\n    a labelled region specified by a segmentation mask.\n\n    To instantiate:\n        obj = Centroid(world_coordinates)\n\n    To call:\n        result = obj(mask, label)\n\n    Parameters\n    ----------\n    world_coordinates, optional\n        If True, return centroid in world coordinates, otherwise in image\n        (voxel) coordinates (default).\n    \"\"\"\n\n    def __init__(self, world_coordinates: bool = False):\n                 self.world_coordinates = world_coordinates\n\n    def __call__(self,\n                 mask: sitk.Image,\n                 label: Optional[int] = 1) -> tuple:\n        \"\"\"Centroid callable object: Finds the centroid of\n        a labelled region specified by a segmentation mask.\n\n        Parameters\n        ----------\n        mask\n            Segmentation mask describing the region of interest. Can be an image of\n            type unsigned int representing a label map or `segmentation.Segmentation`.\n\n        label, optional\n            Label to use when computing the centroid if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        tuple\n            The centroid coordinates.\n        \"\"\"\n\n        return centroid(mask,\n                        label=label,\n                        world_coordinates=self.world_coordinates)\n\n\nclass CropToMaskBoundingBox(BaseOp):\n    \"\"\"CropToMaskBoundingBox opetation class:\n    A callable class that crops the image using the bounding box of a region of interest specified\n    by a segmentation mask.\n\n    To instantiate:\n        obj = CropToMaskBoundingBox(margin)\n\n    To call:\n        result = obj(image, mask, label)\n\n    Parameters\n    ----------\n    margin\n        A margin that will be added to each dimension when cropping. If int,\n        add the same margin to each dimension. A sequence of ints can also be\n        passed to specify the margin separately along each dimension.\n    \"\"\"\n\n    def __init__(self, margin: Union[int, Sequence[int], np.ndarray]):\n                 self.margin = margin\n\n    def __call__(self,\n                 image: sitk.Image,\n                 mask: Union[int, Sequence[int], np.ndarray] = None,\n                 label: Optional[int] = 1) -> Tuple[sitk.Image]:\n        \"\"\"CropToMaskBoundingBox callable object:\n        Crops the image using the bounding box of a region of interest specified\n        by a segmentation mask.\n\n        Parameters\n        ----------\n        image\n            The image to crop.\n\n        mask\n            Segmentation mask describing the region of interest. Can be an image of\n            type unsigned int representing a label map or `segmentation.Segmentation`.\n\n        label, optional\n            Label to use when computing the centroid if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        tuple of sitk.Image\n            The cropped image and mask.\n        \"\"\"\n\n        return crop_to_mask_bounding_box(image,\n                                         mask,\n                                         margin=self.margin,\n                                         label=label)\n\n\n# Intensity ops\nclass ClipIntensity(BaseOp):\n    \"\"\"ClipIntensity operation class:\n    A callable class that clips image grey level intensities to specified range.\n\n    To instantiate:\n        obj = ClipIntensity(lower, upper)\n\n    To call:\n        result = obj(image)\n\n    The grey level intensities in the resulting image will fall in the range\n    [lower, upper].\n\n    Parameters\n    ----------\n    lower\n        The lower bound on grey level intensity. Voxels with lower intensity\n        will be set to this value.\n\n    upper\n        The upper bound on grey level intensity. Voxels with higer intensity\n        will be set to this value.\n    \"\"\"\n\n    def __init__(self, lower: float, upper: float):\n        self.lower = lower\n        self.upper = upper\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"ClipIntensity callable object:\n        Clips image grey level intensities to specified range.\n\n        Parameters\n        ----------\n        image\n            The intensity image to clip.\n\n        Returns\n        -------\n        sitk.Image\n            The clipped intensity image.\n        \"\"\"\n        return clip_intensity(image, self.lower, self.upper)\n\n\nclass WindowIntensity(BaseOp):\n    \"\"\"WindowIntensity operation class:\n    A callable class that restricts image grey level intensities to a given window and level.\n\n    To instantiate:\n        obj = WindowIntensity(window, level)\n\n    To call:\n        result = obj(image)\n\n    The grey level intensities in the resulting image will fall in the range\n    [level - window / 2, level + window / 2].\n\n    Parameters\n    ----------\n    window\n        The width of the intensity window.\n\n    level\n        The mid-point of the intensity window.\n    \"\"\"\n\n    def __init__(self, window: float, level: float):\n        self.window = window\n        self.level = level\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"WindowIntensity callable object:\n        Restricts image grey level intensities to a given window and level.\n\n        Parameters\n        ----------\n        image\n            The intensity image to window.\n\n        Returns\n        -------\n        sitk.Image\n            The windowed intensity image.\n        \"\"\"\n\n        return window_intensity(image, self.window, self.level)\n\n\nclass ImageStatistics(BaseOp):\n    \"\"\"ImageStatistics operation class:\n    A callable class that computes the intensity statistics of an image.\n\n    To instantiate:\n        obj = ImageStatistics()\n\n    To call:\n        result = obj(image, mask, label)\n\n    Returns the minimum, maximum, sum, mean, variance and standard deviation\n    of image intensities.\n    This function also supports computing the statistics in a specific\n    region of interest if `mask` and `label` are passed.\n    \"\"\"\n\n    def __call__(self,\n                 image: sitk.Image,\n                 mask: Optional[sitk.Image] = None,\n                 label: Optional[int] =1) -> float:\n        \"\"\"ImageStatistics callable object:\n        Computes the intensity statistics of an image.\n\n        Returns the minimum, maximum, sum, mean, variance and standard deviation\n        of image intensities.\n        This function also supports computing the statistics in a specific\n        region of interest if `mask` and `label` are passed.\n\n        Parameters\n        ----------\n        image\n            The image used to compute the statistics.\n\n        mask, optional\n            Segmentation mask specifying a region of interest used in computation.\n            Can be an image of type unsigned int representing a label map or\n            `segmentation.Segmentation`. Only voxels falling within the ROI will\n            be considered. If None, use the whole image.\n\n        label, optional\n            Label to use when computing the statistics if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        collections.namedtuple\n            The computed intensity statistics in the image or region.\n        \"\"\"\n\n        return image_statistics(image, mask, label=label)\n\n\nclass StandardScale(BaseOp):\n    \"\"\"StandardScale operation class:\n    A callable class that rescales image intensities by subtracting\n    the mean and dividing by standard deviation.\n\n    To instantiate:\n        obj = StandardScale(rescale_mean, rescale_std)\n    To call\n        result = obj(image, mask, label)\n\n    If `rescale_mean` and `rescale_std` are None, image mean and standard\n    deviation will be used, i.e. the resulting image intensities will have\n    0 mean and unit variance. Alternatively, a specific mean and standard\n    deviation can be passed to e.g. standardize a whole dataset of images.\n    If a segmentation mask is passed, only the voxels falling within the mask\n    will be considered when computing the statistics. However, the whole image\n    will still be normalized using the computed values.\n\n    Parameters\n    ----------\n\n    rescale_mean, optional\n        The mean intensity used in rescaling. If None, image mean will be used.\n\n    rescale_std, optional\n        The standard deviation used in rescaling. If None, image standard\n        deviation will be used.\n    \"\"\"\n\n    def __init__(self, rescale_mean: Optional[float] = 0., rescale_std: Optional[float] = 1.):\n        self.rescale_mean = rescale_mean\n        self.rescale_std = rescale_std\n\n    def __call__(self, image: sitk.Image, mask: Optional[sitk.Image] = None, label: Optional[int] =1) -> sitk.Image:\n        \"\"\"StandardScale callable object:\n        A callable class that rescales image intensities by subtracting\n        the mean and dividing by standard deviation.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be rescaled.\n\n        mask, optional\n            Segmentation mask specifying a region of interest used in computation.\n            Can be an image of type unsigned int representing a label map or\n            `segmentation.Segmentation`. Only voxels falling within the ROI will\n            be considered. If None, use the whole image.\n\n        label, optional\n            Label to use when computing the statistics if segmentation mask contains\n            more than 1 labelled region.\n\n        Returns\n        -------\n        sitk.Image\n            The rescaled image.\n        \"\"\"\n\n        return standard_scale(image, mask, self.rescale_mean, self.rescale_std,\n                              label)\n\n\nclass MinMaxScale(BaseOp):\n    \"\"\"MinMaxScale operation class:\n    A callable class that rescales image intensities to a given minimum and maximum.\n\n    Applies a linear transformation to image intensities such that the minimum\n    and maximum intensity values in the resulting image are equal to minimum\n    (default 0) and maximum (default 1) respectively.\n\n    To instantiante:\n        obj = MinMaxScale(minimum, maximum)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n\n    minimum, optional\n        The minimum intensity in the rescaled image.\n\n    maximum, optional\n        The maximum intensity in the rescaled image.\n    \"\"\"\n\n    def __init__(self, minimum: float, maximum: float):\n        self.minimum = minimum\n        self.maximum = maximum\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"MinMaxScale callable object:\n        Rescales image intensities to a given minimum and maximum.\n\n        Applies a linear transformation to image intensities such that the minimum\n        and maximum intensity values in the resulting image are equal to minimum\n        (default 0) and maximum (default 1) respectively.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be rescaled.\n\n        Returns\n        -------\n        sitk.Image\n            The rescaled image.\n        \"\"\"\n        return min_max_scale(image, self.minimum, self.maximum)\n\n\n# Lambda ops\n\nclass SimpleITKFilter(BaseOp):\n    \"\"\"SimpleITKFilter operation class:\n    A callable class that accepts an sitk.ImageFilter object to add a filter to an image.\n\n    To instantiate:\n        obj = SimpleITKFilter(sitk_filter, *execute_args)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    sitk_filter\n        An ImageFilter object in sitk library.\n\n    execute_args, optional\n        Any arguments to be passed to the Execute() function of the selected ImageFilter object.\n    \"\"\"\n\n    def __init__(self, sitk_filter: ImageFilter, *execute_args: Optional[Any]):\n        self.sitk_filter = sitk_filter\n        self.execute_args = execute_args\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"SimpleITKFilter callable object:\n        A callable class that uses an sitk.ImageFilter object to add a filter to an image.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be processed.\n\n        Returns\n        -------\n        sitk.Image\n            The processed image with a given filter.\n        \"\"\"\n        return self.sitk_filter.Execute(image, *self.execute_args)\n\n\nclass ImageFunction(BaseOp):\n    \"\"\"ImageFunction operation class:\n    A callable class that takens in a function to be used to process an image,\n    and executes it.\n\n    To instantiate:\n        obj = ImageFunction(function, copy_geometry, **kwargs)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    function\n        A function to be used for image processing.\n        This function needs to have the following signature:\n        - function(image: sitk.Image, **args)\n        - The first argument needs to be an sitkImage, followed by optional arguments.\n\n    copy_geometry, optional\n        An optional argument to specify whether information about the image should be copied to the\n        resulting image. Set to be true as a default.\n\n    kwargs, optional\n        Any number of arguements used in the given function.\n    \"\"\"\n\n    def __init__(self, function: Function, copy_geometry: bool = True, **kwargs: Optional[Any]):\n        self.function = function\n        self.copy_geometry = copy_geometry\n        self.kwargs = kwargs\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"ImageFunction callable object:\n        Process an image based on a given function.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be processed.\n\n        Returns\n        -------\n        sitk.Image\n            The image processed with the given function.\n        \"\"\"\n\n        result = self.function(image, **self.kwargs)\n        if self.copy_geometry:\n            result.CopyInformation(image)\n        return result\n\n\nclass ArrayFunction(BaseOp):\n    \"\"\"ArrayFunction operation class:\n    A callable class that takes in a function to be used to process an image from numpy array,\n    and executes it.\n\n    To instantiate:\n        obj = ArrayFunction(function, copy_geometry, **kwargs)\n    To call:\n        result = obj(image)\n\n    Parameters\n    ----------\n    function\n        A function to be used for image processing.\n        This function needs to have the following signature:\n        - function(image: sitk.Image, **args)\n        - The first argument needs to be an sitkImage, followed by optional arguments.\n\n    copy_geometry, optional\n        An optional argument to specify whether information about the image should be copied to the\n        resulting image. Set to be true as a default.\n\n    kwargs, optional\n        Any number of arguements used in the given function.\n    \"\"\"\n\n    def __init__(self, function: Function, copy_geometry: bool =True, **kwargs: Optional[Any]):\n        self.function = function\n        self.copy_geometry = copy_geometry\n        self.kwargs = kwargs\n\n    def __call__(self, image: sitk.Image) -> sitk.Image:\n        \"\"\"ArrayFunction callable object:\n        Processes an image from numpy array.\n\n        Parameters\n        ----------\n        image\n            sitk.Image object to be processed.\n\n        Returns\n        -------\n        sitk.Image\n            The image processed with a given function.\n        \"\"\"\n\n        array, origin, direction, spacing = image_to_array(image)\n        result = self.function(array, **self.kwargs)\n        if self.copy_geometry:\n            result = array_to_image(result, origin, direction, spacing)\n        else:\n            result = array_to_image(result)\n        return result\n\n\n# Segmentation ops\n\nclass StructureSetToSegmentation(BaseOp):\n    \"\"\"StructureSetToSegmentation operation class:\n    A callable class that accepts ROI names, a StrutureSet object, and a reference image, and\n    returns Segmentation mask.\n\n    To instantiate:\n        obj = StructureSet(roi_names)\n    To call:\n        mask = obj(structure_set, reference_image)\n\n    Parameters\n    ----------\n    roi_names\n        List of Region of Interests\n    \"\"\"\n\n    def __init__(self, \n                 roi_names: Dict[str, str], \n                 continuous: bool = True):\n        \"\"\"Initialize the op.\n\n        Parameters\n        ----------\n        roi_names\n            List of ROI names to export. Both full names and\n            case-insensitive regular expressions are allowed.\n            All labels within one sublist will be assigned\n            the same label.\n        continuous\n            flag passed to 'physical_points_to_idxs' in 'StructureSet.to_segmentation'. \n            Helps to resolve errors caused by ContinuousIndex > Index. \n\n        Notes\n        -----\n        If `self.roi_names` contains lists of strings, each matching\n        name within a sublist will be assigned the same label. This means\n        that `roi_names=['pat']` and `roi_names=[['pat']]` can lead\n        to different label assignments, depending on how many ROI names\n        match the pattern. E.g. if `self.roi_names = ['fooa', 'foob']`,\n        passing `roi_names=['foo(a|b)']` will result in a segmentation with \n        two labels, but passing `roi_names=[['foo(a|b)']]` will result in\n        one label for both `'fooa'` and `'foob'`.\n\n        If `roi_names` is kept empty ([]), the pipeline will process all ROIs/contours \n        found according to their original names.\n\n        In general, the exact ordering of the returned labels cannot be\n        guaranteed (unless all patterns in `roi_names` can only match\n        a single name or are lists of strings).\n        \"\"\"\n        self.roi_names = roi_names\n        self.continuous = continuous\n\n    def __call__(self, \n                 structure_set: StructureSet, \n                 reference_image: sitk.Image, \n                 existing_roi_indices: Dict[str, int], \n                 ignore_missing_regex: bool,\n                 roi_select_first: bool = False,\n                 roi_separate: bool = False) -> Segmentation:\n        \"\"\"Convert the structure set to a Segmentation object.\n\n        Parameters\n        ----------\n        structure_set\n            The structure set to convert.\n        reference_image\n            Image used as reference geometry.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation object.\n        \"\"\"\n        return structure_set.to_segmentation(reference_image,\n                                             roi_names=self.roi_names,\n                                             continuous=self.continuous,\n                                             existing_roi_indices=existing_roi_indices,\n                                             ignore_missing_regex=ignore_missing_regex,\n                                             roi_select_first=roi_select_first,\n                                             roi_separate=roi_separate)\n\n\nclass FilterSegmentation():\n    \"\"\"FilterSegmentation operation class:\n    A callable class that accepts ROI names, a Segmentation mask with all labels\n    and returns only the desired Segmentation masks based on accepted ROI names.\n\n    To instantiate:\n        obj = StructureSet(roi_names)\n    To call:\n        mask = obj(structure_set, reference_image)\n\n    Parameters\n    ----------\n    roi_names\n        List of Region of Interests\n    \"\"\"\n\n    def __init__(self, \n                 roi_patterns: Dict[str, str],\n                 continuous: bool = False):\n        \"\"\"Initialize the op.\n\n        Parameters\n        ----------\n        roi_names\n            List of ROI names to export. Both full names and\n            case-insensitive regular expressions are allowed.\n            All labels within one sublist will be assigned\n            the same label.\n        \n        \"\"\"\n        self.roi_patterns = roi_patterns\n        self.continuous = continuous\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        \"\"\"\n        Parameters\n        ----\n        roi_select_first\n            Select the first matching ROI/regex for each OAR, no duplicate matches. \n\n        roi_separate\n            Process each matching ROI/regex as individual masks, instead of consolidating into one mask\n            Each mask will be named ROI_n, where n is the nth regex/name/string.\n        \"\"\"\n        labels = {}\n        cur_label = 0\n        if names == self.roi_patterns:\n            for i, name in enumerate(self.roi_patterns):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):  # checks if all ROIs have already been processed.\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:  # if multiple regex/names to match\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched: \n                            break  # break if roi_select_first and we're matched\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels\n\n    def get_mask(self, reference_image, seg, mask, label, idx, continuous):\n        size = seg.GetSize()\n        seg_arr = sitk.GetArrayFromImage(seg)\n        if len(size) == 5:\n            size = size[:-1]\n        elif len(size) == 3:\n            size = size.append(1)\n\n        idx_seg = self.roi_names[label] - 1         # SegmentSequence numbering starts at 1 instead of 0\n        if size[:-1] == reference_image.GetSize():  # Assumes `size` is length of 4: (x, y, z, channels)\n            mask[:,:,:,idx] += seg[:,:,:,idx_seg]\n        else:                                       # if 2D segmentations on 3D images\n            frame        = seg.frame_groups[idx_seg]\n            ref_uid      = frame.DerivationImageSequence[0].SourceImageSequence[0].ReferencedSOPInstanceUID  # unused but references InstanceUID of slice\n            assert ref_uid is not None, \"There was no ref_uid\" # dodging linter\n\n            frame_coords = np.array(frame.PlanePositionSequence[0].ImagePositionPatient)\n            img_coords   = physical_points_to_idxs(reference_image, np.expand_dims(frame_coords, (0, 1)))[0][0]\n            z            = img_coords[0]\n\n            mask[z,:,:,idx] += seg_arr[0,idx_seg,:,:]\n\n    def __call__(self, \n                 reference_image: sitk.Image,\n                 seg: Segmentation, \n                 existing_roi_indices: Dict[str, int], \n                 ignore_missing_regex: bool = False,\n                 roi_select_first: bool = False,\n                 roi_separate: bool = False) -> Segmentation:\n        \"\"\"Convert the structure set to a Segmentation object.\n\n        Parameters\n        ----------\n        structure_set\n            The structure set to convert.\n        reference_image\n            Image used as reference geometry.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation object.\n        \"\"\"\n        from itertools import groupby\n\n        # variable name isn't ideal, but follows StructureSet.to_segmentation convention\n        self.roi_names    = seg.raw_roi_names \n        labels = {}\n        \n        # `roi_names` in .to_segmentation() method = self.roi_patterns\n        if self.roi_patterns is None or self.roi_patterns == {}:\n            self.roi_patterns = self.roi_names\n            labels = self._assign_labels(self.roi_patterns, roi_select_first, roi_separate) #only the ones that match the regex\n        elif isinstance(self.roi_patterns, dict):\n            for name, pattern in self.roi_patterns.items():\n                if isinstance(pattern, str):\n                    matching_names = list(self._assign_labels([pattern], roi_select_first).keys())\n                    if matching_names:\n                        labels[name] = matching_names  # {\"GTV\": [\"GTV1\", \"GTV2\"]} is the result of _assign_labels()\n                elif isinstance(pattern, list):        # for inputs that have multiple patterns for the input, e.g. {\"GTV\": [\"GTV.*\", \"HTVI.*\"]}\n                    labels[name] = []\n                    for pattern_one in pattern:\n                        matching_names = list(self._assign_labels([pattern_one], roi_select_first).keys())\n                        if matching_names:\n                            labels[name].extend(matching_names)  # {\"GTV\": [\"GTV1\", \"GTV2\"]}\n        elif isinstance(self.roi_patterns, list):      # won't this always trigger after the previous?\n            labels = self._assign_labels(self.roi_patterns, roi_select_first)\n        else:\n            raise ValueError(f\"{self.roi_patterns} not expected datatype\")\n        print(\"labels:\", labels)\n        \n        # removing empty labels from dictionary to prevent processing empty masks\n        all_empty = True\n        for v in labels.values():\n            if v != []:\n                all_empty = False\n        if all_empty:\n            if not ignore_missing_regex:\n                raise ValueError(f\"No ROIs matching {self.roi_patterns} found in {self.roi_names}.\")\n            else:\n                return None\n\n        labels = {k:v for (k,v) in labels.items() if v != [] }\n        size = reference_image.GetSize()[::-1] + (len(labels),)\n        mask = np.zeros(size, dtype=np.uint8)\n\n        seg_roi_indices = {}\n        if self.roi_patterns != {} and isinstance(self.roi_patterns, dict):\n            for i, (name, label_list) in enumerate(labels.items()):\n                for label in label_list:\n                    self.get_mask(reference_image, seg, mask, label, i, self.continuous)\n                seg_roi_indices[name] = i\n        else:\n            for name, label in labels.items():\n                self.get_mask(reference_image, seg, mask, name, label, self.continuous)\n            seg_roi_indices = {\"_\".join(k): v for v, k in groupby(labels, key=lambda x: labels[x])}\n\n        mask[mask > 1] = 1\n        mask = sitk.GetImageFromArray(mask, isVector=True)\n        mask.CopyInformation(reference_image)\n        return Segmentation(mask, \n                            roi_indices=seg_roi_indices, \n                            existing_roi_indices=existing_roi_indices, \n                            raw_roi_names=labels)\n\n\nclass MapOverLabels(BaseOp):\n    \"\"\"MapOverLabels operation class:\n\n    To instantiate:\n        obj = MapOverLabels(op, include_background, return_segmentation)\n    To call:\n        mask = obj(segmentation, **kwargs)\n\n    Parameters\n    ----------\n    op\n        A processing function to be used for the operation.\n\n    \"\"\"\n\n    def __init__(self, op, include_background: bool = False, return_segmentation: bool =True):\n        self.op = op\n        self.include_background = include_background\n        self.return_seg = return_segmentation\n\n    def __call__(self, segmentation: Segmentation, **kwargs: Optional[Any]) -> Segmentation:\n        \"\"\"MapOverLabels callable object:\n\n        Parameters\n        ----------\n        include_background\n            Specify whether to include background. Set to be false as a default.\n\n        return_segmentation\n            Specify whether to return segmentation. Set to be true as a default.\n\n        **kwargs\n            Arguments used for the processing function.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation mask.\n        \"\"\"\n\n        return map_over_labels(segmentation,\n                               self.op,\n                               include_background=self.include_background,\n                               return_segmentation=self.return_seg,\n                               **kwargs)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the BaseOp class in this code?",
        "answer": "The BaseOp class serves as a base class for all operation classes in the code. It defines a common interface with a __call__ method that must be implemented by subclasses, and a __repr__ method for string representation of the object's attributes."
      },
      {
        "question": "How does the Resample class handle anti-aliasing when downsampling an image?",
        "answer": "The Resample class uses a Gaussian kernel for anti-aliasing when downsampling an image. This is controlled by the 'anti_alias' parameter, which is set to True by default. The 'anti_alias_sigma' parameter allows specifying the standard deviation of the Gaussian kernel used for anti-aliasing."
      },
      {
        "question": "What is the purpose of the ImageAutoInput class and how does it differ from ImageCSVInput?",
        "answer": "The ImageAutoInput class is a wrapper around ImgCSVloader that automatically crawls through a specified directory, forms a graph of modalities present in the dataset, and loads information for user-provided modalities. It differs from ImageCSVInput in that it doesn't require a pre-existing CSV file, but instead generates the necessary information from the directory structure and file contents."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class ImageAutoInput(BaseInput):\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        path_crawl = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        if not os.path.exists(path_crawl) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs=n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent,\".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=path_crawl, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [\"_\".join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]",
        "complete": "class ImageAutoInput(BaseInput):\n    def __init__(self,\n                 dir_path: str,\n                 modalities: str,\n                 n_jobs: int = -1,\n                 visualize: bool = False,\n                 update: bool = False):\n        self.dir_path = dir_path\n        self.modalities = modalities\n        self.parent, self.dataset_name = os.path.split(self.dir_path)\n\n        # CRAWLER\n        # -------\n        # Checks if dataset has already been indexed\n        path_crawl = pathlib.Path(self.parent, \".imgtools\", f\"imgtools_{self.dataset_name}.csv\").as_posix()\n        if not os.path.exists(path_crawl) or update:\n            print(\"Indexing the dataset...\")\n            db = crawl(self.dir_path, n_jobs=n_jobs)\n            print(f\"Number of patients in the dataset: {len(db)}\")\n        else:\n            print(\"The dataset has already been indexed.\")\n\n        # GRAPH\n        # -----\n        # Form the graph\n        edge_path = pathlib.Path(self.parent,\".imgtools\",f\"imgtools_{self.dataset_name}_edges.csv\").as_posix()\n        graph = DataGraph(path_crawl=path_crawl, edge_path=edge_path, visualize=visualize)\n        print(f\"Forming the graph based on the given modalities: {self.modalities}\")\n        self.df_combined = graph.parser(self.modalities)\n        self.output_streams = [\"_\".join(cols.split(\"_\")[1:]) for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.column_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"folder\"]\n        self.series_names = [cols for cols in self.df_combined.columns if cols.split(\"_\")[0] == \"series\"]\n        print(f\"There are {len(self.df_combined)} cases containing all {modalities} modalities.\")\n\n        self.readers = [read_dicom_auto for _ in range(len(self.output_streams))]\n\n        loader = ImageCSVLoader(self.df_combined,\n                                colnames=self.column_names,\n                                seriesnames=self.series_names,\n                                id_column=None,\n                                expand_paths=False,\n                                readers=self.readers)\n        \n        super().__init__(loader)"
      },
      {
        "partial": "class FilterSegmentation():\n    def __init__(self, \n                 roi_patterns: Dict[str, str],\n                 continuous: bool = False):\n        self.roi_patterns = roi_patterns\n        self.continuous = continuous\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        labels = {}\n        cur_label = 0\n        if names == self.roi_patterns:\n            for i, name in enumerate(self.roi_patterns):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched: \n                            break\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels",
        "complete": "class FilterSegmentation():\n    def __init__(self, \n                 roi_patterns: Dict[str, str],\n                 continuous: bool = False):\n        self.roi_patterns = roi_patterns\n        self.continuous = continuous\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        labels = {}\n        cur_label = 0\n        if names == self.roi_patterns:\n            for i, name in enumerate(self.roi_patterns):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched: \n                            break\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels\n\n    def get_mask(self, reference_image, seg, mask, label, idx, continuous):\n        size = seg.GetSize()\n        seg_arr = sitk.GetArrayFromImage(seg)\n        if len(size) == 5:\n            size = size[:-1]\n        elif len(size) == 3:\n            size = size.append(1)\n\n        idx_seg = self.roi_names[label] - 1\n        if size[:-1] == reference_image.GetSize():\n            mask[:,:,:,idx] += seg[:,:,:,idx_seg]\n        else:\n            frame = seg.frame_groups[idx_seg]\n            ref_uid = frame.DerivationImageSequence[0].SourceImageSequence[0].ReferencedSOPInstanceUID\n            assert ref_uid is not None\n\n            frame_coords = np.array(frame.PlanePositionSequence[0].ImagePositionPatient)\n            img_coords = physical_points_to_idxs(reference_image, np.expand_dims(frame_coords, (0, 1)))[0][0]\n            z = img_coords[0]\n\n            mask[z,:,:,idx] += seg_arr[0,idx_seg,:,:]\n\n    def __call__(self, \n                 reference_image: sitk.Image,\n                 seg: Segmentation, \n                 existing_roi_indices: Dict[str, int], \n                 ignore_missing_regex: bool = False,\n                 roi_select_first: bool = False,\n                 roi_separate: bool = False) -> Segmentation:\n        from itertools import groupby\n\n        self.roi_names = seg.raw_roi_names \n        labels = {}\n        \n        if self.roi_patterns is None or self.roi_patterns == {}:\n            self.roi_patterns = self.roi_names\n            labels = self._assign_labels(self.roi_patterns, roi_select_first, roi_separate)\n        elif isinstance(self.roi_patterns, dict):\n            for name, pattern in self.roi_patterns.items():\n                if isinstance(pattern, str):\n                    matching_names = list(self._assign_labels([pattern], roi_select_first).keys())\n                    if matching_names:\n                        labels[name] = matching_names\n                elif isinstance(pattern, list):\n                    labels[name] = []\n                    for pattern_one in pattern:\n                        matching_names = list(self._assign_labels([pattern_one], roi_select_first).keys())\n                        if matching_names:\n                            labels[name].extend(matching_names)\n        elif isinstance(self.roi_patterns, list):\n            labels = self._assign_labels(self.roi_patterns, roi_select_first)\n        else:\n            raise ValueError(f\"{self.roi_patterns} not expected datatype\")\n        print(\"labels:\", labels)\n        \n        all_empty = True\n        for v in labels.values():\n            if v != []:\n                all_empty = False\n        if all_empty:\n            if not ignore_missing_regex:\n                raise ValueError(f\"No ROIs matching {self.roi_patterns} found in {self.roi_names}.\")\n            else:\n                return None\n\n        labels = {k:v for (k,v) in labels.items() if v != [] }\n        size = reference_image.GetSize()[::-1] + (len(labels),)\n        mask = np.zeros(size, dtype=np.uint8)\n\n        seg_roi_indices = {}\n        if self.roi_patterns != {} and isinstance(self.roi_patterns, dict):\n            for i, (name, label_list) in enumerate(labels.items()):\n                for label in label_list:\n                    self.get_mask(reference_image, seg, mask, label, i, self.continuous)\n                seg_roi_indices[name] = i\n        else:\n            for name, label in labels.items():\n                self.get_mask(reference_image, seg, mask, name, label, self.continuous)\n            seg_roi_indices = {\"_\".join(k): v for v, k in groupby(labels, key=lambda x: labels[x])}\n\n        mask[mask > 1] = 1\n        mask = sitk.GetImageFromArray(mask, isVector=True)\n        mask.CopyInformation(reference_image)\n        return Segmentation(mask, \n                            roi_indices=seg_roi_indices, \n                            existing_roi_indices=existing_roi_indices, \n                            raw_roi_names=labels)"
      }
    ],
    "dependencies": {
      "imports": [
        "numpy",
        "SimpleITK",
        "json"
      ],
      "from_imports": [
        "typing.List",
        "functional.*",
        "io.*",
        "utils.image_to_array",
        "modules.map_over_labels",
        "modules.DataGraph",
        "itertools.groupby"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/drugPerturbationSig.R",
    "language": "R",
    "content": "#' Creates a signature representing gene expression (or other molecular profile)\n#' change induced by administrating a drug, for use in drug effect analysis.\n#'\n#' Given a Pharmacoset of the perturbation experiment type, and a list of drugs,\n#' the function will compute a signature for the effect of drug concentration on\n#' the molecular profile of a cell. The algorithm uses a regression model which\n#' corrects for experimental batch effects, cell specific differences, and\n#' duration of experiment to isolate the effect of the concentration of the drug\n#' applied. The function returns the estimated coefficient for concentration,\n#' the t-stat, the p-value and the false discovery rate associated with that\n#' coefficient, in a 3 dimensional array, with genes in the first direction,\n#' drugs in the second, and the selected return values in the third.\n#'\n#' @examples\n#' data(CMAPsmall)\n#' drug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\n#' print(drug.perturbation)\n#'\n#' @param pSet [PharmacoSet] a PharmacoSet of the perturbation experiment type\n#' @param mDataType `character` which one of the molecular data types to use\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv\n#' @param drugs `character` a vector of drug names for which to compute the\n#'   signatures. Should match the names used in the PharmacoSet.\n#' @param cells `character` a vector of cell names to use in computing the\n#'   signatures. Should match the names used in the PharmacoSet.\n#' @param features `character` a vector of features for which to compute the\n#'   signatures. Should match the names used in correspondant molecular data in PharmacoSet.\n#' @param nthread `numeric` if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#' @param returnValues `character` Which of estimate, t-stat, p-value and fdr\n#'   should the function return for each gene drug pair?\n#' @param verbose `logical(1)` Should diagnostive messages be printed? (default false)\n#'\n#' @return `list` a 3D array with genes in the first dimension, drugs in the\n#'   second, and return values in the third.\n#'\n#' @export\ndrugPerturbationSig <- function(pSet, mDataType, drugs, cells, features,\n  nthread=1, returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE)\n{\n\tavailcore <- parallel::detectCores()\n\tif ( nthread > availcore) {\n\t  nthread <- availcore\n\t}\n  options(\"mc.cores\"=nthread)\n  if(!missing(cells)){\n    if(!all(cells%in%sampleNames(pSet))){\n      stop(\"The cell names should match to the names used in sampleNames(pSet)\")\n    }\n    pSet <- subsetTo(pSet, cells=cells)\n  }\n  if (mDataType %in% names(pSet@molecularProfiles)) {\n    #eset <- pSet@molecularProfiles[[mDataType]]\n\t\tif(S4Vectors::metadata(pSet@molecularProfiles[[mDataType]])$annotation != \"rna\"){\n\t\t\tstop(sprintf(\"Only rna data type perturbations are currently implemented\"))\n\t\t}\n  } else {\n    stop (sprintf(\"This pSet does not have any molecular data of type %s, choose among: %s\", mDataType), paste(names(pSet@molecularProfiles), collapse=\", \"))\n  }\n\n\n  if (missing(drugs)) {\n    drugn <- treatmentNames(pSet)\n  } else {\n    drugn <- drugs\n  }\n  dix <- is.element(drugn, PharmacoGx::phenoInfo(pSet, mDataType)[ , \"treatmentid\"])\n  if (verbose && !all(dix)) {\n    warning (sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) {\n    stop(\"None of the drugs were found in the dataset\")\n  }\n  drugn <- drugn[dix]\n\n  if (missing(features)) {\n    features <- rownames(featureInfo(pSet, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(pSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning (sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  # splitix <- parallel::splitIndices(nx=length(drugn), ncl=nthread)\n  # splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n  mcres <- lapply(drugn, function(x, exprs, sampleinfo) {\n    res <- NULL\n    i = x\n    ## using a linear model (x ~ concentration + cell + batch + duration)\n    res <- rankGeneDrugPerturbation(data=exprs, drug=i, drug.id=as.character(sampleinfo[ , \"treatmentid\"]), drug.concentration=as.numeric(sampleinfo[ , \"concentration\"]), type=as.character(sampleinfo[ , \"sampleid\"]), xp=as.character(sampleinfo[ , \"xptype\"]), batch=as.character(sampleinfo[ , \"batchid\"]), duration=as.character(sampleinfo[ , \"duration\"]) ,single.type=FALSE, nthread=nthread, verbose=FALSE)$all[ , returnValues, drop=FALSE]\n    res <- list(res)\n    names(res) <- i\n    return(res)\n  }, exprs=t(molecularProfiles(pSet, mDataType)[features, , drop=FALSE]), sampleinfo=PharmacoGx::phenoInfo(pSet, mDataType))\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE=logical(1))]\n  drug.perturbation <- array(NA, dim=c(nrow(featureInfo(pSet, mDataType)[features,, drop=FALSE]), length(res), ncol(res[[1]])), dimnames=list(rownames(featureInfo(pSet, mDataType)[features,,drop=FALSE]), names(res), colnames(res[[1]])))\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n              xx <- array(NA, dim=length(k), dimnames=list(k))\n              xx[rownames(x)] <- x[ , j, drop=FALSE]\n              return (xx)\n              }, j=j, k=rownames(featureInfo(pSet, mDataType)[features,, drop=FALSE]),\n            FUN.VALUE=numeric(dim(drug.perturbation)[1]))\n    drug.perturbation[rownames(featureInfo(pSet, mDataType)[features,, drop=FALSE]), names(res), j] <- ttt\n  }\n\n  drug.perturbation <- PharmacoSig(drug.perturbation, PSetName = name(pSet), Call = as.character(match.call()), SigType='Perturbation')\n\n  return(drug.perturbation)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `drugPerturbationSig` function?",
        "answer": "The main purpose of the `drugPerturbationSig` function is to create a signature representing gene expression (or other molecular profile) changes induced by administering a drug. It computes a signature for the effect of drug concentration on the molecular profile of a cell, using a regression model that corrects for experimental batch effects, cell-specific differences, and duration of experiment to isolate the effect of the drug concentration."
      },
      {
        "question": "How does the function handle multiple cores for parallel processing?",
        "answer": "The function uses the `nthread` parameter to determine the number of cores for parallel processing. It first detects the available cores using `parallel::detectCores()`. If the specified `nthread` is greater than the available cores, it sets `nthread` to the number of available cores. Then it sets the 'mc.cores' option to the determined number of threads for parallel processing."
      },
      {
        "question": "What is the structure of the returned object from the `drugPerturbationSig` function?",
        "answer": "The function returns a `PharmacoSig` object, which is a 3-dimensional array. The first dimension represents genes, the second dimension represents drugs, and the third dimension contains the selected return values (such as estimate, t-stat, p-value, and fdr). The array is wrapped in a `PharmacoSig` object that includes additional metadata like the PharmacoSet name and the function call."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugPerturbationSig <- function(pSet, mDataType, drugs, cells, features,\n  nthread=1, returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE)\n{\n  # Check and set number of threads\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) {\n    nthread <- availcore\n  }\n  options(\"mc.cores\"=nthread)\n\n  # Subset pSet if cells are specified\n  if(!missing(cells)){\n    if(!all(cells %in% sampleNames(pSet))){\n      stop(\"The cell names should match to the names used in sampleNames(pSet)\")\n    }\n    pSet <- subsetTo(pSet, cells=cells)\n  }\n\n  # Check mDataType\n  if (mDataType %in% names(pSet@molecularProfiles)) {\n    if(S4Vectors::metadata(pSet@molecularProfiles[[mDataType]])$annotation != \"rna\"){\n      stop(\"Only rna data type perturbations are currently implemented\")\n    }\n  } else {\n    stop(sprintf(\"This pSet does not have any molecular data of type %s, choose among: %s\", mDataType, paste(names(pSet@molecularProfiles), collapse=\", \")))\n  }\n\n  # Set drug names\n  if (missing(drugs)) {\n    drugn <- treatmentNames(pSet)\n  } else {\n    drugn <- drugs\n  }\n  dix <- is.element(drugn, PharmacoGx::phenoInfo(pSet, mDataType)[ , \"treatmentid\"])\n  if (verbose && !all(dix)) {\n    warning(sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) {\n    stop(\"None of the drugs were found in the dataset\")\n  }\n  drugn <- drugn[dix]\n\n  # Set features\n  if (missing(features)) {\n    features <- rownames(featureInfo(pSet, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(pSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning(sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  # Main computation\n  mcres <- lapply(drugn, function(x, exprs, sampleinfo) {\n    # TODO: Implement the main computation logic here\n  }, exprs=t(molecularProfiles(pSet, mDataType)[features, , drop=FALSE]), sampleinfo=PharmacoGx::phenoInfo(pSet, mDataType))\n\n  # TODO: Process results and create drug.perturbation object\n\n  return(drug.perturbation)\n}",
        "complete": "drugPerturbationSig <- function(pSet, mDataType, drugs, cells, features,\n  nthread=1, returnValues=c(\"estimate\",\"tstat\", \"pvalue\", \"fdr\"),\n  verbose=FALSE)\n{\n  availcore <- parallel::detectCores()\n  if (nthread > availcore) {\n    nthread <- availcore\n  }\n  options(\"mc.cores\"=nthread)\n\n  if(!missing(cells)){\n    if(!all(cells %in% sampleNames(pSet))){\n      stop(\"The cell names should match to the names used in sampleNames(pSet)\")\n    }\n    pSet <- subsetTo(pSet, cells=cells)\n  }\n\n  if (mDataType %in% names(pSet@molecularProfiles)) {\n    if(S4Vectors::metadata(pSet@molecularProfiles[[mDataType]])$annotation != \"rna\"){\n      stop(\"Only rna data type perturbations are currently implemented\")\n    }\n  } else {\n    stop(sprintf(\"This pSet does not have any molecular data of type %s, choose among: %s\", mDataType, paste(names(pSet@molecularProfiles), collapse=\", \")))\n  }\n\n  drugn <- if(missing(drugs)) treatmentNames(pSet) else drugs\n  dix <- is.element(drugn, PharmacoGx::phenoInfo(pSet, mDataType)[ , \"treatmentid\"])\n  if (verbose && !all(dix)) {\n    warning(sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) stop(\"None of the drugs were found in the dataset\")\n  drugn <- drugn[dix]\n\n  features <- if(missing(features)) rownames(featureInfo(pSet, mDataType)) else {\n    fix <- is.element(features, rownames(featureInfo(pSet, mDataType)))\n    if (verbose && !all(fix)) {\n      warning(sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features[fix]\n  }\n\n  mcres <- lapply(drugn, function(x, exprs, sampleinfo) {\n    res <- rankGeneDrugPerturbation(data=exprs, drug=x, drug.id=as.character(sampleinfo[ , \"treatmentid\"]),\n                                    drug.concentration=as.numeric(sampleinfo[ , \"concentration\"]),\n                                    type=as.character(sampleinfo[ , \"sampleid\"]),\n                                    xp=as.character(sampleinfo[ , \"xptype\"]),\n                                    batch=as.character(sampleinfo[ , \"batchid\"]),\n                                    duration=as.character(sampleinfo[ , \"duration\"]),\n                                    single.type=FALSE, nthread=nthread, verbose=FALSE)$all[ , returnValues, drop=FALSE]\n    list(res) %>% setNames(x)\n  }, exprs=t(molecularProfiles(pSet, mDataType)[features, , drop=FALSE]), sampleinfo=PharmacoGx::phenoInfo(pSet, mDataType))\n\n  res <- do.call(c, mcres) %>% .[!vapply(., is.null, logical(1))]\n  drug.perturbation <- array(NA, dim=c(nrow(featureInfo(pSet, mDataType)[features,, drop=FALSE]), length(res), ncol(res[[1]])),\n                             dimnames=list(rownames(featureInfo(pSet, mDataType)[features,,drop=FALSE]), names(res), colnames(res[[1]])))\n\n  for (j in seq_len(ncol(res[[1]]))) {\n    drug.perturbation[,,j] <- vapply(res, function(x) {\n      xx <- array(NA, dim=nrow(drug.perturbation), dimnames=list(rownames(drug.perturbation)))\n      xx[rownames(x)] <- x[ , j, drop=FALSE]\n      xx\n    }, FUN.VALUE=numeric(nrow(drug.perturbation)))\n  }\n\n  PharmacoSig(drug.perturbation, PSetName = name(pSet), Call = as.character(match.call()), SigType='Perturbation')\n}"
      },
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  # TODO: Implement the function body\n  # This function should perform the following tasks:\n  # 1. Prepare the data for analysis\n  # 2. Fit a linear model for each gene\n  # 3. Calculate statistics (estimate, t-stat, p-value, FDR)\n  # 4. Return the results\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  require(limma)\n  require(stats)\n\n  # Prepare data\n  drug.ix <- drug.id == drug\n  X <- data[, drug.ix]\n  conc <- drug.concentration[drug.ix]\n  type <- type[drug.ix]\n  xp <- xp[drug.ix]\n  batch <- batch[drug.ix]\n  duration <- duration[drug.ix]\n\n  # Create design matrix\n  design <- model.matrix(~ conc + type + xp + batch + duration)\n\n  # Fit linear model for each gene\n  fit <- lmFit(X, design)\n  fit <- eBayes(fit)\n\n  # Extract results\n  results <- topTable(fit, coef=\"conc\", number=Inf, sort.by=\"none\")\n\n  # Prepare output\n  out <- list(\n    estimate = results$logFC,\n    tstat = results$t,\n    pvalue = results$P.Value,\n    fdr = results$adj.P.Val\n  )\n\n  list(all = as.data.frame(out))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/adaptiveMatthewCor.R",
    "language": "R",
    "content": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    CoreGx::amcc(x, y, step.prct, min.cat, nperm, nthread)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@inherit` tag in the function documentation?",
        "answer": "The `@inherit` tag is used to inherit documentation from another function. In this case, it inherits the documentation from the `amcc` function in the `CoreGx` package. This allows the developer to reuse existing documentation without duplicating it, ensuring consistency and reducing maintenance overhead."
      },
      {
        "question": "Why might a developer choose to create a wrapper function like this instead of directly using the `CoreGx::amcc` function?",
        "answer": "A developer might create a wrapper function like this for several reasons: 1) To provide a simplified interface within their own package, 2) To set default values for certain parameters, 3) To add additional functionality or pre/post-processing, or 4) To make the function available in the global namespace of their package without requiring users to specify the `CoreGx::` prefix each time. In this case, it appears to be primarily for simplifying the interface and potentially setting package-specific defaults."
      },
      {
        "question": "What does the `#' @export` tag do in this code snippet?",
        "answer": "The `#' @export` tag is used in R package development to indicate that this function should be made available to users of the package. When the package is built, functions marked with `@export` will be included in the package's namespace and can be called by users after they load the package. Without this tag, the function would be internal to the package and not directly accessible to users."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    # Complete the function body\n}",
        "complete": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    CoreGx::amcc(x, y, step.prct, min.cat, nperm, nthread)\n}"
      },
      {
        "partial": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, ...) {\n    # Complete the function signature and body\n}",
        "complete": "#' Adaptive Matthews Correlation Coefficient\n#'\n#' @inherit CoreGx::amcc\n#'\n#' @examples\n#' amcc(0.6^(1:5), 0.5^(1:5))\n#'\n#' @export\namcc <- function(x, y, step.prct=0, min.cat=3, nperm=1000, nthread=1) {\n    CoreGx::amcc(x, y, step.prct, min.cat, nperm, nthread)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeIC50.R",
    "language": "R",
    "content": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose=TRUE,\n                    trunc=TRUE))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeIC50` function and how does it relate to the `computeICn` function?",
        "answer": "The `computeIC50` function is a specific implementation of the more general `computeICn` function. It calculates the IC50 (Inhibitory Concentration 50%) of a drug dose-response curve. The IC50 is the concentration of a drug that reduces the response by 50%. This function essentially calls `computeICn` with `n` set to 50 (or 0.5 if `viability_as_pct` is FALSE), which represents the 50% inhibition point."
      },
      {
        "question": "How does the `viability_as_pct` parameter affect the behavior of the `computeIC50` function?",
        "answer": "The `viability_as_pct` parameter determines how the viability data is interpreted and how the IC50 is calculated. If `viability_as_pct` is TRUE (default), the function assumes the viability data is in percentages, and sets `n` to 50 when calling `computeICn`. If FALSE, it assumes the viability data is in decimal form (0 to 1), and sets `n` to 0.5. This ensures that the IC50 is correctly calculated regardless of how the viability data is represented."
      },
      {
        "question": "What is the significance of the `@describeIn` and `@export` tags in the function documentation?",
        "answer": "The `@describeIn` tag indicates that this function is part of a family of related functions, specifically the `computeICn` family. It suggests that there's a main documentation page for `computeICn`, and this function (`computeIC50`) will be described in that documentation. The `@export` tag means that this function should be made publicly available when the package is built and installed. It allows users of the package to access and use the `computeIC50` function directly."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ,\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration = concentration,\n                    viability = viability,\n                    Hill_fit = Hill_fit,\n                    n = ifelse(viability_as_pct, 50, .5),\n                    conc_as_log = conc_as_log,\n                    viability_as_pct = viability_as_pct,\n                    verbose = verbose,\n                    trunc = trunc))\n}"
      },
      {
        "partial": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn())\n}",
        "complete": "#' @describeIn computeICn Returns the IC50 of a Drug Dose response curve\n#'\n#' @return `numeric(1)` The ICn of the Hill curve over the specified dose\n#' range.\n#'\n#' @export\ncomputeIC50 <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  return(computeICn(concentration, viability, Hill_fit,\n                    ifelse(viability_as_pct, 50, .5),\n                    conc_as_log, viability_as_pct, verbose, trunc))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_PharmacoSetClass.R",
    "language": "R",
    "content": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking PharmacoSet Class Methods.\")\n\n\ntest_that(\"cellInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sampleInfo(GDSCsmall), \"cellInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"drugInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(treatmentInfo(GDSCsmall), \"drugInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"phenoInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(phenoInfo(GDSCsmall, \"rna\"), \"phenoInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"molecularProfiles result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(molecularProfiles(GDSCsmall, \"rna\"), \"molecularProfiles.GDSCsmall.rds\")\n})\n\ntest_that(\"featureInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(featureInfo(GDSCsmall, \"rna\"), \"featureInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"sensitivityInfo result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sensitivityInfo(GDSCsmall), \"sensitivityInfo.GDSCsmall.rds\")\n})\n\ntest_that(\"sensitivityProfiles result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sensitivityProfiles(GDSCsmall), \"sensitivityProfiles.GDSCsmall.rds\")\n})\n\n\ntest_that(\"sensitivityMeasures result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sensitivityMeasures(GDSCsmall), \"sensitivityMeasures.GDSCsmall.rds\")\n})\n\n\ntest_that(\"drugNames result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(treatmentNames(GDSCsmall), \"drugNames.GDSCsmall.rds\")\n})\n\ntest_that(\"cellNames result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(sampleNames(GDSCsmall), \"cellNames.GDSCsmall.rds\")\n})\n\n\ntest_that(\"fNames result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal_to_reference(fNames(GDSCsmall, \"rna\"), \"fNames.GDSCsmall.rds\")\n})\n\n\ntest_that(\"name result did not change since last time\", {\n\tdata(GDSCsmall)\n\texpect_equal(name(GDSCsmall), \"GDSC\")\n})\n\ntest_that(\"updateSampleId works without duplicates\", {\n\tdata(GDSCsmall)\n\tnewNames <- c(\"Test\",\"Test2\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n\n\tsampleNames(GDSCsmall) <- newNames\n\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n\texpect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(newNames))\n\texpect_equal(sort(rownames(sensNumber(GDSCsmall))), sort(newNames))\n\n})\n\n\ntest_that(\"updateSampleId works with duplicates\", {\n\tdata(GDSCsmall)\n\tnewNames <- c(\"Test\",\"Test\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n\n\texpect_warning(sampleNames(GDSCsmall) <- newNames, \"Duplicated ids passed to updateSampleId. Merging old ids into the same identifier\")\n\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n\texpect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(unique(newNames)))\n\texpect_equal(sort(rownames(sensNumber(GDSCsmall))), sort(unique(newNames)))\n\n})\n\n\n\ntest_that(\"updateTreatmentId works without duplicates\", {\n\tdata(GDSCsmall)\n\tnewNames <- c(\"Test\",\"Test2\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n\ttreatmentNames(GDSCsmall) <- newNames\n\n\texpect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n\texpect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(newNames))\n\texpect_equal(sort(colnames(sensNumber(GDSCsmall))), sort(newNames))\n\n})\n\n\ntest_that(\"updateTreatmentId works with duplicates\", {\n    data(GDSCsmall)\n    newNames <- c(\"Test\",\"Test\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n    expect_warning(treatmentNames(GDSCsmall) <- newNames,\n\t\t\"Duplicated ids passed to updateTreatmentId. Merging old ids into the same identifier\")\n\n    expect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n    expect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(unique(newNames)))\n    expect_equal(sort(colnames(sensNumber(GDSCsmall))), sort(unique(newNames)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'expect_equal_to_reference' function used throughout this code snippet?",
        "answer": "The 'expect_equal_to_reference' function is used to compare the output of various PharmacoSet class methods with previously saved reference data. This ensures that the results of these methods have not changed since the last time the tests were run, maintaining consistency in the package's functionality."
      },
      {
        "question": "How does the code handle the case of duplicate sample IDs when updating sample names?",
        "answer": "When updating sample names with duplicates, the code issues a warning message: 'Duplicated ids passed to updateSampleId. Merging old ids into the same identifier'. It then proceeds to update the sample names, merging the data for duplicate IDs. This is tested in the 'updateSampleId works with duplicates' test case, where it checks if the updated sample names are correctly reflected in various components of the PharmacoSet object."
      },
      {
        "question": "What is the significance of the 'GDSCsmall' dataset used in these tests?",
        "answer": "The 'GDSCsmall' dataset is a sample PharmacoSet object used throughout the test cases. It is loaded at the beginning of each test using the 'data(GDSCsmall)' command. This dataset provides a consistent and known set of data to test various methods and functions of the PharmacoGx package, ensuring that the package's functionality works correctly with real-world data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"updateSampleId works without duplicates\", {\n  data(GDSCsmall)\n  newNames <- c(\"Test\",\"Test2\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n  sampleNames(GDSCsmall) <- newNames\n\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(newNames))\n  # Complete the test by adding an expectation for sensNumber\n})",
        "complete": "test_that(\"updateSampleId works without duplicates\", {\n  data(GDSCsmall)\n  newNames <- c(\"Test\",\"Test2\",sampleNames(GDSCsmall)[3:length(sampleNames(GDSCsmall))])\n\n  sampleNames(GDSCsmall) <- newNames\n\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_true(all(unique(sensitivityInfo(GDSCsmall)$sampleid) %in% newNames))\n  expect_equal(sort(unique(rownames(sampleInfo(GDSCsmall)))), sort(newNames))\n  expect_equal(sort(rownames(sensNumber(GDSCsmall))), sort(newNames))\n})"
      },
      {
        "partial": "test_that(\"updateTreatmentId works with duplicates\", {\n    data(GDSCsmall)\n    newNames <- c(\"Test\",\"Test\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n    expect_warning(treatmentNames(GDSCsmall) <- newNames,\n        \"Duplicated ids passed to updateTreatmentId. Merging old ids into the same identifier\")\n\n    expect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n    expect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(unique(newNames)))\n    # Complete the test by adding an expectation for sensNumber\n})",
        "complete": "test_that(\"updateTreatmentId works with duplicates\", {\n    data(GDSCsmall)\n    newNames <- c(\"Test\",\"Test\",treatmentNames(GDSCsmall)[3:length(treatmentNames(GDSCsmall))])\n\n    expect_warning(treatmentNames(GDSCsmall) <- newNames,\n        \"Duplicated ids passed to updateTreatmentId. Merging old ids into the same identifier\")\n\n    expect_true(all(unique(sensitivityInfo(GDSCsmall)$treatmentid) %in% newNames))\n    expect_equal(sort(unique(rownames(treatmentInfo(GDSCsmall)))), sort(unique(newNames)))\n    expect_equal(sort(colnames(sensNumber(GDSCsmall))), sort(unique(newNames)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/GR.R",
    "language": "R",
    "content": ".GR <- function(x, pars, tau) {\n  #GR takes in a vector of log concentrations, a vector of DRC parameters from the .Hill()\n  #function, and a coefficient tau equal to the number of doubling times occuring between\n  #the start of the experiment and the taking of the viability measurements. It then returns\n  #the GR-value associated with those conditions.\n  return((.Hill(x, pars)) ^ (1 / tau))\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the .GR function and what are its input parameters?",
        "answer": "The .GR function calculates the GR-value (Growth Rate value) based on given conditions. It takes three input parameters: 'x' (a vector of log concentrations), 'pars' (a vector of DRC parameters from the .Hill() function), and 'tau' (a coefficient representing the number of doubling times occurring between the start of the experiment and the viability measurements)."
      },
      {
        "question": "How does the .GR function calculate the GR-value?",
        "answer": "The .GR function calculates the GR-value by first calling the .Hill() function with the input parameters 'x' and 'pars'. It then raises the result to the power of (1 / tau). The formula used is: GR-value = (.Hill(x, pars)) ^ (1 / tau)."
      },
      {
        "question": "What is the significance of the 'tau' parameter in the .GR function?",
        "answer": "The 'tau' parameter in the .GR function represents the number of doubling times occurring between the start of the experiment and the taking of the viability measurements. It is used as an exponent in the calculation (1 / tau) to adjust the result of the .Hill() function, likely to account for the growth rate over the specified time period."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".GR <- function(x, pars, tau) {\n  # Function description\n  return((___(x, pars)) ^ (1 / tau))\n}",
        "complete": ".GR <- function(x, pars, tau) {\n  # GR calculates growth rate value based on Hill function\n  return((.Hill(x, pars)) ^ (1 / tau))\n}"
      },
      {
        "partial": ".GR <- function(x, pars, tau) {\n  # Calculate GR value\n  hill_result <- ___\n  return(hill_result ^ (1 / tau))\n}",
        "complete": ".GR <- function(x, pars, tau) {\n  # Calculate GR value\n  hill_result <- .Hill(x, pars)\n  return(hill_result ^ (1 / tau))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/setup.py",
    "language": "py",
    "content": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `with open('README.md', 'r') as fh:` block in this setup script?",
        "answer": "This block reads the contents of the README.md file and assigns it to the `long_description` variable. The long description is typically used to provide detailed information about the package, which is then displayed on package repositories like PyPI. It allows package authors to include comprehensive documentation directly from their README file."
      },
      {
        "question": "How does this setup script handle package dependencies, and what are the optional dependency groups defined?",
        "answer": "The script reads package dependencies from a 'requirements.txt' file using `with open('requirements.txt', 'r') as fh:`. These are set as the main install requirements. Additionally, it defines optional dependency groups using the `extras_require` parameter. Two groups are defined: 'debug' (which includes 'pyvis') and 'torch' (which includes 'torch' and 'torchio'). Users can install these optional dependencies by specifying the group name when installing the package."
      },
      {
        "question": "What command-line tools does this package provide, and how are they defined in the setup script?",
        "answer": "The package provides two command-line tools: 'autopipeline' and 'betapipeline'. These are defined in the `entry_points` parameter of the setup function, under the 'console_scripts' key. 'autopipeline' maps to the `main` function in the `imgtools.autopipeline` module, while 'betapipeline' maps to the `main` function in the `imgtools.autopipeline_refactored` module. This allows users to run these tools directly from the command line after installing the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        # Add classifiers here\n    ],\n    python_requires='>=3.7',\n)",
        "complete": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)"
      },
      {
        "partial": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        # Add extras_require here\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)",
        "complete": "from setuptools import setup, find_packages\n__version__ = \"1.5.4\"\n\nwith open(\"README.md\", \"r\") as fh:\n    long_description = fh.read()\n\nwith open(\"requirements.txt\", \"r\") as fh:\n    reqs = fh.read()\n    \nsetup(\n    name=\"med-imagetools\",\n    version=__version__,\n    author=\"Sejin Kim, Michal Kazmierski, Kevin Qu, Vishwesh Ramanathan, Benjamin Haibe-Kains\",\n    author_email=\"benjamin.haibe.kains@utoronto.ca\",\n    description=\"Transparent and reproducible image processing pipelines in Python.\",\n    long_description=long_description,\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/bhklab/med-imagetools\",\n    install_requires=reqs,\n    packages=find_packages(),\n    extras_require={\n        'debug': ['pyvis'],\n        'torch': ['torch', 'torchio']\n    },\n    entry_points={'console_scripts': ['autopipeline = imgtools.autopipeline:main', 'betapipeline = imgtools.autopipeline_refactored:main']},\n    classifiers=[\n        \"Programming Language :: Python :: 3\",\n        \"Operating System :: OS Independent\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Development Status :: 2 - Pre-Alpha\"\n    ],\n    python_requires='>=3.7',\n)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "setuptools.setup"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-general.R",
    "language": "R",
    "content": "#' simple wrapper for the data.table::as.data.table() function\n#' @param x object to convert to a data.table\n#' @param ... additional arguments to pass to data.table::as.data.table()\n#' @return a data.table\n#' @keywords internal\n#' @noRd\n.asDT <- function(x, ...) data.table::as.data.table(x, ...)\n\n#' Parses the query response into a data table\n#'\n#' This function takes a query response and converts it into a data table using the `as.data.table` function from the `data.table` package.\n#'\n#' @param resp The query response to be parsed\n#' @return A data table containing the parsed query response\n#'\n#' @noRd\n#' @keywords internal\n.parseQueryToDT <- function(resp) {\n  data.table::as.data.table(resp[[1]][[1]])\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.asDT` function in the given code snippet?",
        "answer": "The `.asDT` function is a simple wrapper for the `data.table::as.data.table()` function. It takes an object `x` and any additional arguments, then converts `x` to a data.table using the `as.data.table()` function from the data.table package. This wrapper function provides a convenient way to convert objects to data.tables within the package."
      },
      {
        "question": "Explain the purpose and functionality of the `.parseQueryToDT` function.",
        "answer": "The `.parseQueryToDT` function is designed to parse a query response into a data table. It takes a `resp` parameter, which is expected to be a query response object. The function extracts the first element of the first element of `resp` using double square bracket notation (`resp[[1]][[1]]`), and then converts this extracted data into a data.table using the `data.table::as.data.table()` function. This function is likely used to standardize the format of query responses for further processing or analysis within the package."
      },
      {
        "question": "What do the `@noRd` and `@keywords internal` tags indicate in the function documentation?",
        "answer": "The `@noRd` and `@keywords internal` tags in the function documentation are Roxygen2 tags used for R package development. `@noRd` stands for 'no Rd file' and indicates that no separate documentation file should be generated for this function. `@keywords internal` marks the function as internal, meaning it's not intended for direct use by package users. These tags suggest that both `.asDT` and `.parseQueryToDT` are helper functions meant for internal use within the package rather than part of the public API."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".asDT <- function(x, ...) {\n  # Complete the function body\n}",
        "complete": ".asDT <- function(x, ...) data.table::as.data.table(x, ...)"
      },
      {
        "partial": ".parseQueryToDT <- function(resp) {\n  # Complete the function body\n}",
        "complete": ".parseQueryToDT <- function(resp) {\n  data.table::as.data.table(resp[[1]][[1]])\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_metadata.py",
    "language": "py",
    "content": "import pytest\nimport os\n\nfrom readii.metadata import (\n    matchCTtoSegmentation,\n    getSegmentationType,\n    saveDataframeCSV,\n    getCTWithSegmentation\n)\n\n@pytest.fixture\ndef nsclcSummaryFilePath():\n    return \"tests/.imgtools/imgtools_NSCLC_Radiogenomics.csv\"\n\n@pytest.fixture\ndef lung4DSummaryFilePath():\n    return \"tests/.imgtools/imgtools_4D-Lung.csv\"\n\n@pytest.fixture\ndef lung4DEdgesSummaryFilePath():\n    return \"tests/.imgtools/imgtools_4D-Lung_edges.csv\"\n\n\ndef test_matchCTtoSEG(nsclcSummaryFilePath):\n    \"\"\"Test generating matched summary file for CT and DICOM SEG\"\"\"\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\")\n    assert len(actual) == 1, \\\n        \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.4334.1501.312037286778380630549945195741', \\\n        \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \\\n        \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'SEG', \\\n        \"Incorrect segmentation type has been found\"\n\n\ndef test_matchCTtoRTSTRUCT(lung4DSummaryFilePath):\n    \"\"\"Test generating matched summary file for CT and RTSTRUCT\"\"\"\n    actual = matchCTtoSegmentation(lung4DSummaryFilePath, \n                                   segType = \"RTSTRUCT\")\n    assert len(actual) == 1, \\\n        \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.6834.5010.339023390306606021995936229543', \\\n        \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \\\n        \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'RTSTRUCT', \\\n        \"Incorrect segmentation type has been found\"\n\n\ndef test_matchCTtoSegmentation_output(nsclcSummaryFilePath):\n    \"\"\"Test saving output of summary file\"\"\"\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\",\n                                   outputFilePath = \"tests/output/ct_to_seg_match_list_NSCLC_Radiogenomics.csv\")\n    assert os.path.exists(\"tests/output/ct_to_seg_match_list_NSCLC_Radiogenomics.csv\") == True, \\\n        \"Output does not exist, double check output file is named correctly.\"\n\n\ndef test_getCTWithRTTRUCT(lung4DEdgesSummaryFilePath):\n    \"\"\"Test getting CTs with RTSTRUCT segmentation from edges file\"\"\"\n    actual = getCTWithSegmentation(lung4DEdgesSummaryFilePath, \n                                   segType = \"RTSTRUCT\")\n    assert len(actual) == 1, \\\n        \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.6834.5010.339023390306606021995936229543', \\\n        \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \\\n        \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'RTSTRUCT', \\\n        \"Incorrect segmentation type has been found\"\n\n\ndef test_getCTtoSegmentation_output(lung4DEdgesSummaryFilePath):\n    \"\"\"Test saving output of summary file\"\"\"\n    actual = getCTWithSegmentation(lung4DEdgesSummaryFilePath, \n                                   segType = \"RTSTRUCT\",\n                                   outputFilePath = \"tests/output/ct_to_seg_match_list_4D-Lung.csv\")\n    assert os.path.exists(\"tests/output/ct_to_seg_match_list_4D-Lung.csv\") == True, \\\n        \"Output does not exist, double check output file is named correctly.\"\n\n\n@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_matchCTtoSegmentation_error(nsclcSummaryFilePath, wrongSeg):\n    \"\"\"Check ValueError is raised when incorrect segType is passed\"\"\"\n    with pytest.raises(ValueError):\n        matchCTtoSegmentation(nsclcSummaryFilePath,\n                              segType = wrongSeg)\n\n\ndef test_saveDataframeCSV_outputFilePath_error(nsclcSummaryFilePath):\n    \"\"\"Check ValueError is raised when incorrect outputFilePath is passed\"\"\"\n    testDataframe = matchCTtoSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\")\n    badFilePath = \"notacsv.xlsx\"\n    with pytest.raises(ValueError):\n        saveDataframeCSV(testDataframe, badFilePath)\n\n@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_getCTWithRTSTRUCT_error(lung4DEdgesSummaryFilePath, wrongSeg):\n    \"\"\"Check ValueError is raised when incorrect segType is passed\"\"\"\n    with pytest.raises(ValueError):\n        getCTWithSegmentation(lung4DEdgesSummaryFilePath, \n                                   segType = wrongSeg)\n\ndef test_getCTWithSEG_error(nsclcSummaryFilePath):\n    \"\"\"Check ValueError is raised when incorrect segType is passed\"\"\"\n    with pytest.raises(ValueError):\n        getCTWithSegmentation(nsclcSummaryFilePath, \n                                   segType = \"SEG\")\n\n\n@pytest.mark.parametrize(\n    \"notADataFrame\",\n    [\n        ['list', 'of', 'features'],\n        \"Just a string\",\n        {\"feat1\": 34, \"feat2\": 10000, \"feat3\": 3.141592}\n    ]\n)\ndef test_saveDataframeCSV_dataframe_error(notADataFrame):\n    \"\"\"Check ValueError is raised when something other than pd.DataFrame is passed\"\"\"\n    goodFilePath = \"tests/output/badDataframeExample.csv\"\n    with pytest.raises(ValueError):\n        saveDataframeCSV(notADataFrame, goodFilePath)\n\n\ndef test_getSegmentationType_SEG(nsclcSummaryFilePath):\n    \"\"\"Test getting segmentation type from summary file with SEG and CT\"\"\"\n    actual = getSegmentationType(nsclcSummaryFilePath)\n    assert actual == \"SEG\", \\\n        \"Wrong segmentation type found\"\n\n\ndef test_getSegmentationType_RTSTRUCT(lung4DSummaryFilePath):\n    \"\"\"Test getting segmentation type from summary file with RTSTRUCT and CT\"\"\"\n    actual = getSegmentationType(lung4DSummaryFilePath)\n    assert actual == \"RTSTRUCT\", \\\n        \"Wrong segmentation type found\"\n\n\n@pytest.mark.parametrize(\n    \"notACSV\",\n    [\n        \"tests/.imgtools/imgtools_NSCLC_Radiogenomics.json\",\n        \"Just a string\",\n        \"tests/.imgtools/imgtools_4D-Lung.json\"\n    ]\n)\ndef test_getSegmentation_dataframe_error(notACSV):\n    \"\"\"Check ValueError is raised when something other than a csv file is passed\"\"\"\n    with pytest.raises(ValueError):\n        getSegmentationType(notACSV)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `matchCTtoSegmentation` function and how does it handle different segmentation types?",
        "answer": "The `matchCTtoSegmentation` function is designed to match CT images with their corresponding segmentation data. It can handle two types of segmentations: 'SEG' (DICOM Segmentation) and 'RTSTRUCT' (Radiotherapy Structure Set). The function takes a summary file path and a segmentation type as input, and returns a DataFrame containing matched CT and segmentation information. It ensures that the segmentation's reference CT ID matches the CT series ID and verifies the correct segmentation type is found."
      },
      {
        "question": "How does the code handle error cases in the `matchCTtoSegmentation` and `getCTWithSegmentation` functions?",
        "answer": "The code uses pytest's parametrize decorator to test multiple error cases. For both `matchCTtoSegmentation` and `getCTWithSegmentation` functions, it checks if a ValueError is raised when an incorrect segmentation type is passed. The test cases include invalid segmentation types like 'CT', 'Nonsense', and an empty string. This ensures that the functions properly validate their input parameters and raise appropriate exceptions when given invalid data."
      },
      {
        "question": "What is the purpose of the `getSegmentationType` function and how is it tested?",
        "answer": "The `getSegmentationType` function is used to determine the type of segmentation (either 'SEG' or 'RTSTRUCT') present in a given summary file. It is tested using two separate test functions: `test_getSegmentationType_SEG` and `test_getSegmentationType_RTSTRUCT`. These tests check if the function correctly identifies the segmentation type for different input files. Additionally, there's an error case test `test_getSegmentation_dataframe_error` that ensures the function raises a ValueError when given an invalid file format (non-CSV)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_matchCTtoSEG(nsclcSummaryFilePath):\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, segType=\"SEG\")\n    assert len(actual) == 1\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.4334.1501.312037286778380630549945195741'\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0]\n    # Complete the assertion for modality_seg",
        "complete": "def test_matchCTtoSEG(nsclcSummaryFilePath):\n    actual = matchCTtoSegmentation(nsclcSummaryFilePath, segType=\"SEG\")\n    assert len(actual) == 1, \"Incorrect merge, should result in only 1 row\"\n    assert actual['reference_ct_seg'][0] == '1.3.6.1.4.1.14519.5.2.1.4334.1501.312037286778380630549945195741', \"The segmentation's reference CT ID is wrong/missing\"\n    assert actual['reference_ct_seg'][0] == actual['series_CT'][0], \"Segmentation reference ID does not match CT series ID\"\n    assert actual['modality_seg'][0] == 'SEG', \"Incorrect segmentation type has been found\""
      },
      {
        "partial": "@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_matchCTtoSegmentation_error(nsclcSummaryFilePath, wrongSeg):\n    # Complete the function body",
        "complete": "@pytest.mark.parametrize(\n    \"wrongSeg\",\n    [\n        'CT',\n        'Nonsense',\n        \"\"\n    ]\n)\ndef test_matchCTtoSegmentation_error(nsclcSummaryFilePath, wrongSeg):\n    with pytest.raises(ValueError):\n        matchCTtoSegmentation(nsclcSummaryFilePath, segType=wrongSeg)"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest",
        "os"
      ],
      "from_imports": [
        "readii.metadata.matchCTtoSegmentation"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/RcppExports.R",
    "language": "R",
    "content": "# Generated by using Rcpp::compileAttributes() -> do not edit by hand\n# Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393\n\n#' QUICKSTOP significance testing for partial correlation\n#'\n#' This function will test whether the observed partial correlation is significant\n#' at a level of req_alpha, doing up to MaxIter permutations. Currently, it\n#' supports only grouping by discrete categories when calculating a partial correlation.\n#' Currenlty, only does two sided tests.\n#'\n#' @param pin_x one of the two vectors to correlate.\n#' @param pin_y the other vector to calculate\n#' @param pobsCor the observed (partial) correlation between these varaiables\n#' @param pGroupFactor an integer vector labeling group membership, to correct\n#' for in the partial correlation. NEEDS TO BE ZERO BASED!\n#' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n#' the number of members of each group (basically table(pGroupFactor)) as integer vector\n#' @param pnumGroup how many groups are there (len(pGroupSize))\n#' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n#' @param pn length of x and y, as a REAL NUMBER\n#' @param preq_alpha the required alpha for significance\n#' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n#' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n#' per quickstop paper (used to determine the log-odds)\n#' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n#' random number generator. Note that currently, these values get modified per call, so pass in a copy\n#' if you wish to keep a seed for running same simulation twice\n#'\n#' @return a double vector of length 4, entry 1 is either 0, 1 (for TRUE/FALSE) or NA_REAL_ for significance determination\n#' NA_REAL_ is returned when the MaxIter were reached before a decision is made. Usually, this occurs when the real p value is close to, or\n#' falls within the tolerance region of (req_alpha, req_alpha+tolerance_par). Entry 2 is the current p value estimate. entry 3 is the total\n#' number of iterations performed. Entry 4 is the number of time a permuted value was larger in absolute value than the observed cor.\n#'\n#' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP\n#'\n#'\npartialCorQUICKSTOP <- function(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed) {\n    .Call('_PharmacoGx_partialCorQUICKSTOP', PACKAGE = 'PharmacoGx', pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `partialCorQUICKSTOP` function and what statistical test does it perform?",
        "answer": "The `partialCorQUICKSTOP` function performs significance testing for partial correlation. It tests whether the observed partial correlation is significant at a specified alpha level using permutation testing. The function implements the QUICKSTOP algorithm, which allows for early stopping of the permutation test when significance can be determined with high confidence. It specifically supports grouping by discrete categories when calculating partial correlations and currently only performs two-sided tests."
      },
      {
        "question": "What are the key parameters of the `partialCorQUICKSTOP` function and their purposes?",
        "answer": "The key parameters of the `partialCorQUICKSTOP` function include:\n1. `pin_x` and `pin_y`: The two vectors to correlate.\n2. `pobsCor`: The observed (partial) correlation between the variables.\n3. `pGroupFactor`: An integer vector labeling group membership (zero-based).\n4. `pGroupSize`: An integer vector counting the number of members in each group.\n5. `pMaxIter`: Maximum number of iterations to perform.\n6. `preq_alpha`: The required alpha for significance.\n7. `ptolerance_par`: The tolerance region for quickstop.\n8. `plog_decision_boundary`: Log of 1/probability of incorrectly calling significance.\n9. `pseed`: A numeric vector of length 2 for seeding the random number generator."
      },
      {
        "question": "What does the function return, and how should the output be interpreted?",
        "answer": "The function returns a double vector of length 4:\n1. Entry 1: 0 (FALSE), 1 (TRUE), or NA_REAL_ for significance determination. NA_REAL_ indicates that MaxIter was reached before a decision was made.\n2. Entry 2: The current p-value estimate.\n3. Entry 3: The total number of iterations performed.\n4. Entry 4: The number of times a permuted value was larger in absolute value than the observed correlation.\n\nThe significance determination (entry 1) is based on the QUICKSTOP algorithm, which may terminate early if significance can be confidently determined before reaching MaxIter iterations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "partialCorQUICKSTOP <- function(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed) {\n    # Complete the function body\n}",
        "complete": "partialCorQUICKSTOP <- function(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed) {\n    .Call('_PharmacoGx_partialCorQUICKSTOP', PACKAGE = 'PharmacoGx', pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed)\n}"
      },
      {
        "partial": "#' QUICKSTOP significance testing for partial correlation\n#'\n#' @param pin_x one of the two vectors to correlate.\n#' @param pin_y the other vector to calculate\n#' @param pobsCor the observed (partial) correlation between these varaiables\n#' @param pGroupFactor an integer vector labeling group membership, to correct\n#' for in the partial correlation. NEEDS TO BE ZERO BASED!\n#' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n#' the number of members of each group (basically table(pGroupFactor)) as integer vector\n#' @param pnumGroup how many groups are there (len(pGroupSize))\n#' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n#' @param pn length of x and y, as a REAL NUMBER\n#' @param preq_alpha the required alpha for significance\n#' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n#' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n#' per quickstop paper (used to determine the log-odds)\n#' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n#' random number generator. Note that currently, these values get modified per call, so pass in a copy\n#' if you wish to keep a seed for running same simulation twice\n#'\n#' @return # Complete the return description\n#'\n#' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP",
        "complete": "#' QUICKSTOP significance testing for partial correlation\n#'\n#' @param pin_x one of the two vectors to correlate.\n#' @param pin_y the other vector to calculate\n#' @param pobsCor the observed (partial) correlation between these varaiables\n#' @param pGroupFactor an integer vector labeling group membership, to correct\n#' for in the partial correlation. NEEDS TO BE ZERO BASED!\n#' @param pGroupSize an integer vector of size length(unique(pGroupFactor)), counting\n#' the number of members of each group (basically table(pGroupFactor)) as integer vector\n#' @param pnumGroup how many groups are there (len(pGroupSize))\n#' @param pMaxIter maximum number of iterations to do, as a REAL NUMBER\n#' @param pn length of x and y, as a REAL NUMBER\n#' @param preq_alpha the required alpha for significance\n#' @param ptolerance_par the tolerance region for quickstop. Suggested to be 1/100th of req_alpha'\n#' @param plog_decision_boundary log (base e) of 1/probability of incorrectly calling significance, as\n#' per quickstop paper (used to determine the log-odds)\n#' @param pseed A numeric vector of length 2, used to seed the internal xoroshiro128+ 1.0\n#' random number generator. Note that currently, these values get modified per call, so pass in a copy\n#' if you wish to keep a seed for running same simulation twice\n#'\n#' @return a double vector of length 4, entry 1 is either 0, 1 (for TRUE/FALSE) or NA_REAL_ for significance determination\n#' NA_REAL_ is returned when the MaxIter were reached before a decision is made. Usually, this occurs when the real p value is close to, or\n#' falls within the tolerance region of (req_alpha, req_alpha+tolerance_par). Entry 2 is the current p value estimate. entry 3 is the total\n#' number of iterations performed. Entry 4 is the number of time a permuted value was larger in absolute value than the observed cor.\n#'\n#' @useDynLib PharmacoGx _PharmacoGx_partialCorQUICKSTOP"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/utils/logging_config.py",
    "language": "py",
    "content": "import logging\nimport logging.config\nimport os\nfrom typing import Optional\n\nBASE_LOGGING: dict = {\n    'version': 1,\n    'disable_existing_loggers': False,\n    'formatters': {\n        # 'json': {\n        #     'class': 'pythonjsonlogger.jsonlogger.JsonFormatter',\n        #     'format': '%(asctime)s %(name)s %(levelname)s %(module)s %(message)s %(pathname)s %(lineno)s %(funcName)s %(threadName)s %(thread)s %(process)s %(processName)s',  # noqa: E501\n        #     'datefmt': '%Y-%m-%d %H:%M:%S',\n        # },\n        'stdout': {\n            'format': '%(asctime)s %(levelname)s: %(message)s (%(module)s:%(funcName)s:%(lineno)d)',\n            'datefmt': '%Y-%m-%d %H:%M:%S',\n        },\n    },\n    'handlers': {\n        'console': {\n            'class': 'logging.StreamHandler',\n            'formatter': 'stdout',\n        },\n    },\n    'loggers': {\n        'devel': {\n            'handlers': ['console'],\n            'level': os.getenv('READII_VERBOSITY', 'INFO'),\n            'propagate': True,\n        },\n    },\n}\n\n\ndef setup_logger(\n    logger_name: str = 'root',\n    config: Optional[dict] = None,\n    extra_handlers: Optional[list] = None,\n  ) -> logging.Logger:\n    \"\"\"Set up a logger with optional custom configuration and log file.\"\"\"\n    valid_loggers = ['devel', 'prod', 'root']\n    assert (\n        logger_name in valid_loggers\n    ), f'Invalid logger name. Available options are {valid_loggers}'\n    \n    # Merge the base config with any custom config\n    logging_config = BASE_LOGGING.copy()\n    if config:\n        logging_config.update(config)\n    logging.config.dictConfig(logging_config)\n\n    logger = logging.getLogger(logger_name)\n\n    # Add any extra handlers provided\n    if extra_handlers:\n        for handler in extra_handlers:\n            logger.addHandler(handler)\n\n    return logger\n\ndef get_logger(config: Optional[dict] = None) -> logging.Logger:\n    \"\"\"Retrieve logger based on the environment, with an optional configuration.\"\"\"\n    env = os.getenv('READII_ENV', 'development')\n\n    logger_name = 'devel' if env in ['devel', 'development'] else 'prod'\n    return setup_logger(logger_name=logger_name, config=config)\n\n# Example usage\nif __name__ == \"__main__\":\n    logger = get_logger()\n    logger.info(\"This is an informational message.\")\n    logger.debug(\"This is a debug message.\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `BASE_LOGGING` dictionary in the given code snippet?",
        "answer": "The `BASE_LOGGING` dictionary defines the basic configuration for the logging system. It specifies the logging format, handlers, and default logger settings. This dictionary serves as a template for setting up loggers with consistent formatting and behavior across the application."
      },
      {
        "question": "Explain the function of the `setup_logger` function and its parameters.",
        "answer": "The `setup_logger` function is responsible for creating and configuring a logger. It takes three parameters: `logger_name` (default 'root'), `config` (optional custom configuration), and `extra_handlers` (optional additional log handlers). The function merges the base configuration with any custom config, sets up the logger using `logging.config.dictConfig`, and adds any extra handlers provided. It also validates the logger name against a list of valid options."
      },
      {
        "question": "How does the `get_logger` function determine which logger to use, and what is its purpose?",
        "answer": "The `get_logger` function determines the logger to use based on the `READII_ENV` environment variable. If the environment is 'devel' or 'development', it uses the 'devel' logger; otherwise, it uses the 'prod' logger. This function serves as a convenient way to retrieve the appropriate logger for the current environment, allowing for different logging configurations in development and production settings."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [
        "logging",
        "logging.config",
        "os"
      ],
      "from_imports": [
        "typing.Optional"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeAUC.R",
    "language": "R",
    "content": "#' Computes the AUC for a Drug Dose Viability Curve\n#' \n#' Returns the AUC (Area Under the drug response Curve) given concentration and viability as input, normalized by the concentration\n#' range of the experiment. The area returned is the response (1-Viablility) area, i.e. area under the curve when the response curve \n#' is plotted on a log10 concentration scale, with high AUC implying high sensitivity to the drug. The function can calculate both \n#' the area under a fitted Hill Curve to the data, and a trapz numeric integral of the actual data provided. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known. \n#' \n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8) \n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeAUC(dose, viability)\n#' \n#' \n#' @param concentration `numeric` is a vector of drug concentrations.\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells. \n#' @param Hill_fit `list` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope \n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal, \n#' and EC50 as a concentration. \n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and returns AUC as a decimal. Otherwise, viability is interpreted as percent, and AUC is returned 0-100.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param area.type Should the area be computed using the actual data (\"Actual\"), or a fitted curve (\"Fitted\")\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return Numeric AUC value\n#' \n#' @export\n#' @import caTools\ncomputeAUC <- function (concentration,\n   viability,\n   Hill_fit,\n   conc_as_log = FALSE,\n   viability_as_pct = TRUE,\n   trunc = TRUE,\n   area.type = c(\"Fitted\", \"Actual\"),\n   verbose = TRUE\n   #, ...\n   ) {\n\n  if (missing(concentration)) {\n\n    stop(\"The concentration values to integrate over must always be provided.\")\n\n  }\nif (missing(area.type)) {\n    area.type <- \"Fitted\"\n} else {\n    area.type <- match.arg(area.type)\n}\nif (area.type == \"Fitted\" && missing(Hill_fit)) {\n\n    Hill_fit <- logLogisticRegression(concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n} else if (area.type == \"Fitted\" && !missing(Hill_fit)) {\n\n  cleanData <- sanitizeInput(conc = concentration,\n    viability = viability,\n    Hill_fit = Hill_fit,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n} else if (area.type == \"Actual\" && !missing(viability)){\n  cleanData <- sanitizeInput(conc = concentration,\n     viability = viability,\n     conc_as_log = conc_as_log,\n     viability_as_pct = viability_as_pct,\n     trunc = trunc,\n     verbose = verbose)\n  concentration <- cleanData[[\"log_conc\"]]\n  viability <- cleanData[[\"viability\"]]\n} else if (area.type == \"Actual\" && missing(viability)) {\n\n  stop(\"To calculate the actual area using a trapezoid integral, the raw viability values are needed!\")\n}\n\nif (length(concentration) < 2) {\n  return(NA)\n}\n\na <- min(concentration)\nb <- max(concentration)\nif (area.type == \"Actual\") {\n  trapezoid.integral <- caTools::trapz(concentration, viability)\n  AUC <- 1 - trapezoid.integral / (b - a)\n}\nelse {\n    if (pars[2] == 1) {\n        AUC <- 0\n    } else if (pars[1] == 0){\n        AUC <- (1 - pars[2]) / 2\n    } else {\n        AUC <- as.numeric(\n        (1 - pars[2]) / (pars[1] * (b - a)) *\n        log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) /\n            (1 + (10 ^ (a - pars[3])) ^ pars[1])))\n    }\n}\n\nif(viability_as_pct){\n\n  AUC <- AUC*100\n\n}\n\nreturn(AUC)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAUC` function and what are its main input parameters?",
        "answer": "The `computeAUC` function computes the Area Under the Curve (AUC) for a Drug Dose Viability Curve. It calculates the response area (1-Viability) when plotted on a log10 concentration scale, with a high AUC implying high drug sensitivity. The main input parameters are:\n1. `concentration`: a vector of drug concentrations\n2. `viability`: a vector of viability values corresponding to the concentrations\n3. `Hill_fit`: optional parameters of a Hill Slope\n4. `conc_as_log`: boolean indicating if concentration is given in log scale\n5. `viability_as_pct`: boolean indicating if viability is given as a percentage\n6. `area.type`: specifies whether to use actual data or a fitted curve for calculation"
      },
      {
        "question": "How does the function handle different types of input data and what data preprocessing steps are performed?",
        "answer": "The function handles different types of input data through several preprocessing steps:\n1. It checks if concentration data is provided, which is mandatory.\n2. It uses the `sanitizeInput` function to clean and standardize the input data.\n3. If `conc_as_log` is FALSE, it converts concentration to log scale.\n4. If `viability_as_pct` is TRUE, it converts viability from percentage to decimal.\n5. If `trunc` is TRUE, it truncates viability data to be between 0 and 1.\n6. It handles both raw data ('Actual' area.type) and fitted curve data ('Fitted' area.type).\n7. For 'Fitted' area.type, it either uses provided Hill_fit parameters or calculates them using `logLogisticRegression` function."
      },
      {
        "question": "How is the AUC calculated differently for 'Actual' and 'Fitted' area types, and what is the significance of the calculation method?",
        "answer": "The AUC calculation differs for 'Actual' and 'Fitted' area types:\n\n1. For 'Actual' area type:\n   - Uses the `trapz` function from the caTools package to perform a trapezoidal integration of the raw data.\n   - Calculates AUC as: 1 - (trapezoid integral / concentration range)\n\n2. For 'Fitted' area type:\n   - Uses the Hill Slope parameters (slope, E_inf, EC50) to calculate the AUC analytically.\n   - Handles special cases where the slope is 0 or E_inf is 1.\n   - For normal cases, uses a logarithmic formula to calculate the area.\n\nThe significance of these methods:\n- 'Actual' provides a direct measure of the observed data but may be sensitive to noise.\n- 'Fitted' smooths out noise and can interpolate between data points, but assumes the data follows a Hill Slope model.\n\nThe choice between methods depends on the data quality and the specific requirements of the analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAUC <- function (concentration,\n   viability,\n   Hill_fit,\n   conc_as_log = FALSE,\n   viability_as_pct = TRUE,\n   trunc = TRUE,\n   area.type = c(\"Fitted\", \"Actual\"),\n   verbose = TRUE\n   ) {\n\n  if (missing(concentration)) {\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(area.type)) {\n    area.type <- \"Fitted\"\n  } else {\n    area.type <- match.arg(area.type)\n  }\n  \n  # TODO: Implement the main logic for AUC calculation\n  \n  # Return the calculated AUC\n  return(AUC)\n}",
        "complete": "computeAUC <- function (concentration,\n   viability,\n   Hill_fit,\n   conc_as_log = FALSE,\n   viability_as_pct = TRUE,\n   trunc = TRUE,\n   area.type = c(\"Fitted\", \"Actual\"),\n   verbose = TRUE\n   ) {\n\n  if (missing(concentration)) {\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(area.type)) {\n    area.type <- \"Fitted\"\n  } else {\n    area.type <- match.arg(area.type)\n  }\n  \n  cleanData <- sanitizeInput(conc = concentration,\n    viability = viability,\n    Hill_fit = Hill_fit,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  \n  concentration <- cleanData[[\"log_conc\"]]\n  if (area.type == \"Actual\") {\n    viability <- cleanData[[\"viability\"]]\n  } else {\n    pars <- cleanData[[\"Hill_fit\"]]\n  }\n  \n  if (length(concentration) < 2) return(NA)\n  \n  a <- min(concentration)\n  b <- max(concentration)\n  \n  if (area.type == \"Actual\") {\n    AUC <- 1 - caTools::trapz(concentration, viability) / (b - a)\n  } else {\n    if (pars[2] == 1) {\n      AUC <- 0\n    } else if (pars[1] == 0) {\n      AUC <- (1 - pars[2]) / 2\n    } else {\n      AUC <- (1 - pars[2]) / (pars[1] * (b - a)) *\n        log10((1 + (10 ^ (b - pars[3])) ^ pars[1]) /\n          (1 + (10 ^ (a - pars[3])) ^ pars[1]))\n    }\n  }\n  \n  if (viability_as_pct) AUC <- AUC * 100\n  \n  return(AUC)\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n  # TODO: Implement input sanitization logic\n  \n  # Return a list with sanitized inputs\n  return(list(log_conc = log_conc,\n               viability = viability,\n               Hill_fit = Hill_fit))\n}",
        "complete": "sanitizeInput <- function(conc,\n                        viability,\n                        Hill_fit,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose = TRUE) {\n  if (!conc_as_log) {\n    log_conc <- log10(conc)\n  } else {\n    log_conc <- conc\n  }\n  \n  if (!missing(viability)) {\n    if (viability_as_pct) {\n      viability <- viability / 100\n    }\n    if (trunc) {\n      viability <- pmin(pmax(viability, 0), 1)\n    }\n  }\n  \n  if (!missing(Hill_fit)) {\n    if (viability_as_pct) {\n      Hill_fit[2] <- Hill_fit[2] / 100\n    }\n    if (!conc_as_log) {\n      Hill_fit[3] <- log10(Hill_fit[3])\n    }\n  }\n  \n  return(list(log_conc = log_conc,\n               viability = viability,\n               Hill_fit = Hill_fit))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_match-methods.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"unlistNested returns the correct result for a nested list\", {\n  nested_list <- list(list(1, 2), list(3, 4), list(5, 6))\n  expected_result <- c(1, 2, 3, 4, 5, 6)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with NA values\", {\n  nested_list <- list(list(1, NA), list(3, 4), list(NA, 6))\n  expected_result <- c(1, 3, 4, 6)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with duplicate values\", {\n  nested_list <- list(list(1, 2), list(2, 3), list(3, 4))\n  expected_result <- c(1, 2, 3, 4)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns an empty vector for an empty nested list\", {\n  nested_list <- list()\n  expected_result <- NULL\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with character elements\", {\n  nested_list <- list(list(\"a\", \"b\"), list(\"c\", \"d\"), list(\"e\", \"f\"))\n  expected_result <- c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with mixed data types\", {\n  nested_list <- list(list(1, \"a\"), list(TRUE, 2.5), list(\"b\", FALSE))\n  expected_result <- c(1, \"a\", TRUE, 2.5, \"b\", FALSE)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a deeply nested list\", {\n  nested_list <- list(list(list(1, 2), list(3, 4)), list(list(5, 6), list(7, 8)))\n  expected_result <- c(1, 2, 3, 4, 5, 6, 7, 8)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with NULL values\", {\n  nested_list <- list(list(NULL, 1), list(2, NULL), list(NULL, NULL))\n  expected_result <- c(1, 2)\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with empty sublists\", {\n  nested_list <- list(list(), list(), list())\n  expected_result <- NULL\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\ntest_that(\"unlistNested returns the correct result for a nested list with a single element\", {\n  nested_list <- list(list(42))\n  expected_result <- 42\n  expect_equal(unlistNested(nested_list), expected_result)\n})\n\n####################################################################################################\n# matchNested,list\n####################################################################################################\n\ntest_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(1, 2), list(3, 4), list(5, 6))\n  x <- 3\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\ntest_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(\"1\", \"2\"), list(\"3\", \"4\"), list(\"5\", \"6\"))\n  x <- \"3\"\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\n\ntest_that(\"matchNested,list returns the correct index for a nested list with NA values\", {\n  table <- list(list(1, NA), list(3, 4), list(NA, 6))\n  x <- 4\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\ntest_that(\"matchNested,list returns the correct index for a nested list with duplicate values\", {\n  table <- list(list(1, 2), list(2, 3), list(3, 4))\n  x <- 2\n  expected_result <- 1\n  expected_result_dups <- c(1, 2)\n\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result_dups)\n\n  x <- 4\n  expected_result <- 3\n  expected_result_dups <- c(3)\n\n  expect_equal(matchNested(x, table), expected_result)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result_dups)\n})\n\n####################################################################################################\n# matchNested,data.table\n####################################################################################################\n# Test case 1: Matching a character value in a data.table\ntest_that(\"matchNested,data.table returns the correct index for a character value\", {\n  table <- data.table(col1 = c(\"apple\", \"banana\", \"orange\"), col2 = c(1, 2, 3))\n  x <- \"banana\"\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})\n\n# Test case 2: Matching a character value in a data.table with NA values\ntest_that(\"matchNested,data.table returns the correct index for a character value with NA values\", {\n  table <- data.table(col1 = c(\"apple\", NA, \"orange\"), col2 = c(1, 2, 3))\n  x <- \"orange\"\n  expected_result <- 3\n  expect_equal(matchNested(x, table), expected_result)\n})\n\n# Test case 3: Matching a character value in a data.table with duplicate values\ntest_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(col1 = c(\"apple\", \"banana\", \"banana\"), col2 = c(1, 2, 3))\n  x <- \"banana\"\n  expected_result <- c(2, 3)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result)\n  \n  expected_result <- 2\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n  expect_equal(matchNested(x, table), expected_result)\n\n  idx <- matchNested(x, table, keep_duplicates = FALSE)\n\n  data.table::setkeyv(table, \"col1\")\n  matched <- table[idx]  \n\n  # make sure that x is in one of the columns\n  expect_true(any(matched$col1 == x | matched$col2 == x))\n})\n\ntest_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(\n    col1 = list(\n      list(\"apple\", \"banana\"), \n      list(\"mango\", \"orange\"), \n      list(\"banana\", \"orange\")), \n    col2 = c(1, \"banana\", 3)\n  )\n  x <- \"banana\"\n  expected_result <- c(1, 2, 3)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result)\n\n  expected_result <- 1\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n\n  x <- \"orange\"\n  expected_result <- c(2, 3)\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), expected_result)\n\n  expected_result <- 2\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), expected_result)\n\n})\n\n# Test case 4: Matching a character value in an empty data.table\ntest_that(\"matchNested,data.table returns NULL for an empty data.table\", {\n  table <- data.table()\n  x <- \"apple\"\n  expected_result <- NULL\n  expect_error(matchNested(x, table))\n})\n\ntest_that(\"matchNested returns the correct matches for character and data.frame inputs\", {\n  # Test case 1: Matching single character with data.frame\n  x1 <- \"apple\"\n  table1 <- data.frame(fruit = c(\"apple\", \"banana\", \"orange\"), color = c(\"red\", \"yellow\", \"orange\"))\n  result1 <- matchNested(x1, table1)\n  expect_equal(result1, 1)\n\n  # Test case 2: Matching multiple characters with data.frame\n  x2 <- c(\"apple\", \"banana\")\n  table2 <- data.frame(fruit = c(\"apple\", \"banana\", \"orange\"), color = c(\"red\", \"yellow\", \"orange\"))\n  expect_warning(result2 <-matchNested(x2, table2))\n\n  expect_equal(result2,  1)\n\n\n  x3 <- c(\"apple\", \"orange\")\n  expect_warning(result3 <- matchNested(x3, table2))\n  expect_equal(result3,  1)\n\n  x4 <- c(\"red\", \"yellow\")\n  expect_warning(result4 <- matchNested(x4, table2))\n  expect_equal(result4,  2)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `unlistNested` function based on the test cases provided?",
        "answer": "The `unlistNested` function is designed to flatten a nested list structure into a single vector. It removes NA and NULL values, eliminates duplicates, and can handle various data types including numeric, character, and logical values. The function works with lists of different depths and returns NULL for empty lists."
      },
      {
        "question": "How does the `matchNested` function behave differently when applied to a list versus a data.table?",
        "answer": "When applied to a list, `matchNested` searches for a value `x` within the nested structure and returns the index of the first matching sublist. For a data.table, it searches across all columns and can return multiple indices if `keep_duplicates = TRUE`. With a data.table, it can also handle more complex nested structures within columns."
      },
      {
        "question": "What is the significance of the `keep_duplicates` parameter in the `matchNested` function?",
        "answer": "The `keep_duplicates` parameter determines whether `matchNested` returns all matching indices or just the first one. When set to `TRUE`, it returns a vector of all indices where the value is found. When `FALSE` (default), it returns only the first matching index. This is particularly useful when dealing with data structures that may contain duplicate values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(1, 2), list(3, 4), list(5, 6))\n  x <- 3\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n})",
        "complete": "test_that(\"matchNested,list returns the correct index for a nested list\", {\n  table <- list(list(1, 2), list(3, 4), list(5, 6))\n  x <- 3\n  expected_result <- 2\n  expect_equal(matchNested(x, table), expected_result)\n\n  x <- 6\n  expected_result <- 3\n  expect_equal(matchNested(x, table), expected_result)\n\n  x <- 7\n  expect_equal(matchNested(x, table), NULL)\n})"
      },
      {
        "partial": "test_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(\n    col1 = list(\n      list(\"apple\", \"banana\"), \n      list(\"mango\", \"orange\"), \n      list(\"banana\", \"orange\")), \n    col2 = c(1, \"banana\", 3)\n  )\n  x <- \"banana\"\n  # Add test cases here\n})",
        "complete": "test_that(\"matchNested,data.table returns the correct index for a character value with duplicate values\", {\n  table <- data.table(\n    col1 = list(\n      list(\"apple\", \"banana\"), \n      list(\"mango\", \"orange\"), \n      list(\"banana\", \"orange\")), \n    col2 = c(1, \"banana\", 3)\n  )\n  x <- \"banana\"\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), c(1, 2, 3))\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), 1)\n\n  x <- \"orange\"\n  expect_equal(matchNested(x, table, keep_duplicates = TRUE), c(2, 3))\n  expect_equal(matchNested(x, table, keep_duplicates = FALSE), 2)\n\n  x <- \"grape\"\n  expect_equal(matchNested(x, table), NULL)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/loaders.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport json\nimport glob\nimport re\nfrom typing import Optional\nfrom collections import namedtuple\n# import copy\n\nimport pandas as pd\nimport SimpleITK as sitk\nfrom pydicom import dcmread\n\n# from joblib import Parallel, delayed\n# from tqdm.auto import tqdm\n\nfrom ..modules import StructureSet, Dose, PET, Scan, Segmentation\nfrom ..utils.crawl import *\nfrom ..utils.dicomutils import *\n\n\ndef read_image(path):\n    return sitk.ReadImage(path)\n\n\ndef read_dicom_series(path: str,\n                      series_id: Optional[str] = None,\n                      recursive: bool = False, \n                      file_names: list = None):\n    \"\"\"Read DICOM series as SimpleITK Image.\n\n    Parameters\n    ----------\n    path\n       Path to directory containing the DICOM series.\n\n    recursive, optional\n       Whether to recursively parse the input directory when searching for\n       DICOM series,\n\n    series_id, optional\n       Specifies the DICOM series to load if multiple series are present in\n       the directory. If None and multiple series are present, loads the first\n       series found.\n\n    file_names, optional\n        If there are multiple acquisitions/\"subseries\" for an individual series,\n        use the provided list of file_names to set the ImageSeriesReader.\n\n    Returns\n    -------\n    The loaded image.\n\n    \"\"\"\n    reader = sitk.ImageSeriesReader()\n    if file_names is None:\n        file_names = reader.GetGDCMSeriesFileNames(path,\n                                                   seriesID=series_id if series_id else \"\",\n                                                   recursive=recursive)\n        # extract the names of the dicom files that are in the path variable, which is a directory\n    \n    reader.SetFileNames(file_names)\n    \n    # Configure the reader to load all of the DICOM tags (public+private):\n    # By default tags are not loaded (saves time).\n    # By default if tags are loaded, the private tags are not loaded.\n    # We explicitly configure the reader to load tags, including the\n    # private ones.\n    reader.MetaDataDictionaryArrayUpdateOn()\n    reader.LoadPrivateTagsOn()\n\n    return reader.Execute()\n\n    \ndef read_dicom_scan(path, series_id=None, recursive: bool=False, file_names=None) -> Scan:\n    image = read_dicom_series(path, series_id=series_id, recursive=recursive, file_names=file_names)\n    return Scan(image, {})\n\n\ndef read_dicom_rtstruct(path):\n    return StructureSet.from_dicom_rtstruct(path)\n\n\ndef read_dicom_rtdose(path):\n    return Dose.from_dicom_rtdose(path)\n\n\ndef read_dicom_pet(path, series=None):\n    return PET.from_dicom_pet(path, series, \"SUV\")\n\n\ndef read_dicom_seg(path, meta, series=None):\n    seg_img = read_dicom_series(path, series)\n    return Segmentation.from_dicom_seg(seg_img, meta)\n\n\ndef read_dicom_auto(path, series=None, file_names=None):\n    if path is None:\n        return None\n    if path.endswith(\".dcm\"):\n        dcms = [path]\n    else:\n        dcms = glob.glob(pathlib.Path(path, \"*.dcm\").as_posix())\n        \n    for dcm in dcms:\n        meta = dcmread(dcm)\n        if meta.SeriesInstanceUID != series and series is not None:\n            continue\n        \n        modality = meta.Modality\n        if modality in ['CT', 'MR']:\n            obj = read_dicom_scan(path, series, file_names=file_names)\n        elif modality == 'PT':\n            obj = read_dicom_pet(path, series)\n        elif modality == 'RTSTRUCT':\n            obj = read_dicom_rtstruct(dcm)\n        elif modality == 'RTDOSE':\n            obj = read_dicom_rtdose(dcm)\n        elif modality == 'SEG':\n            obj = read_dicom_seg(path, meta, series)\n        else:\n            if len(dcms) == 1:\n                print(modality, 'at', dcms[0], 'is NOT implemented yet.')\n                raise NotImplementedError\n            else:\n                print(\"There were no dicoms in this path.\")\n                return None\n        \n        obj.metadata.update(get_modality_metadata(meta, modality))\n        return obj\n\n\nclass BaseLoader:\n    def __getitem__(self, subject_id):\n        raise NotImplementedError\n\n    def __len__(self):\n        return len(self.keys())\n\n    def keys(self):\n        raise NotImplementedError\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n\n    def values(self):\n        return (self[k] for k in self.keys())\n\n    def get(self, subject_id, default=None):\n        try:\n            return self[subject_id]\n        except KeyError:\n            return default\n\n\nclass ImageTreeLoader(BaseLoader):\n    def __init__(self,\n                 json_path,\n                 csv_path_or_dataframe,\n                 col_names=[],\n                 study_names=[],\n                 series_names=[],\n                 subseries_names=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n\n        if readers is None:\n            readers = [read_image]  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n\n        self.expand_paths = expand_paths\n        self.readers = readers\n        self.colnames = col_names\n        self.studynames = study_names\n        self.seriesnames = series_names\n        self.subseriesnames = subseries_names\n\n        if isinstance(csv_path_or_dataframe, str):\n            if id_column is not None and id_column not in self.colnames:\n                self.colnames.append(id_column)\n            self.paths = pd.read_csv(csv_path_or_dataframe,\n                                     index_col=id_column)\n        elif isinstance(csv_path_or_dataframe, pd.DataFrame):\n            self.paths = csv_path_or_dataframe\n            if id_column:\n                self.paths = self.paths.set_index(id_column)\n            if len(self.colnames) == 0:\n                self.colnames = self.paths.columns\n        else:\n            raise ValueError(f\"Expected a path to csv file or pd.DataFrame, not {type(csv_path_or_dataframe)}.\")\n        \n        if isinstance(json_path, str):\n            with open(json_path, 'r') as f:\n                self.tree = json.load(f)\n        else:\n            raise ValueError(f\"Expected a path to a json file, not {type(json_path)}.\")\n\n        if not isinstance(readers, list):\n            readers = [readers] * len(self.colnames)\n\n        self.output_tuple = namedtuple(\"Output\", self.colnames)\n\n    def __getitem__(self, subject_id):\n        row = self.paths.loc[subject_id]\n        paths = {col: row[col] for col in self.colnames}\n        study = {col: row[col] for col in self.studynames}\n        series = {col: row[col] for col in self.seriesnames}\n        subseries = {col: row[col] for col in self.subseriesnames}\n        paths = {k: v if pd.notna(v) else None for k, v in paths.items()}\n        \n        if self.expand_paths:\n            # paths = {col: glob.glob(path)[0] for col, path in paths.items()}\n            paths = {col: glob.glob(path)[0] if pd.notna(path) else None for col, path in paths.items()}\n        \n        for i, (col, path) in enumerate(paths.items()):\n            files = self.tree[subject_id][study[\"study_\"+(\"_\").join(col.split(\"_\")[1:])]][series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])]][subseries[\"subseries_\"+(\"_\").join(col.split(\"_\")[1:])]]\n            self.readers[i](path, series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])])\n        outputs = {col: self.readers[i](path, series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])], file_names=files) for i, (col, path) in enumerate(paths.items())}\n        return self.output_tuple(**outputs)\n\n    def keys(self):\n        return list(self.paths.index)\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n    \n\nclass ImageCSVLoader(BaseLoader):\n    def __init__(self,\n                 csv_path_or_dataframe,\n                 colnames=[],\n                 seriesnames=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n\n        if readers is None:\n            readers = [read_image]  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n\n        self.expand_paths = expand_paths\n        self.readers = readers\n\n        self.colnames = colnames\n        self.seriesnames = seriesnames\n        if isinstance(csv_path_or_dataframe, str):\n            if id_column is not None and id_column not in colnames:\n                colnames.append(id_column)\n            self.paths = pd.read_csv(csv_path_or_dataframe,\n                                     index_col=id_column)\n        elif isinstance(csv_path_or_dataframe, pd.DataFrame):\n            self.paths = csv_path_or_dataframe\n            if id_column:\n                self.paths = self.paths.set_index(id_column)\n            if len(self.colnames) == 0:\n                self.colnames = self.paths.columns\n        else:\n            raise ValueError(f\"Expected a path to csv file or pd.DataFrame, not {type(csv_path_or_dataframe)}.\")\n\n        if not isinstance(readers, list):\n            readers = [readers] * len(self.colnames)\n\n        self.output_tuple = namedtuple(\"Output\", self.colnames)\n\n    def __getitem__(self, subject_id):\n        row = self.paths.loc[subject_id]\n        paths = {col: row[col] for col in self.colnames}\n        series = {col: row[col] for col in self.seriesnames}\n        paths = {k: v if pd.notna(v) else None for k, v in paths.items()}\n        if self.expand_paths:\n            # paths = {col: glob.glob(path)[0] for col, path in paths.items()}\n            paths = {col: glob.glob(path)[0] if pd.notna(path) else None for col, path in paths.items()}\n        \n        outputs = {col: self.readers[i](path,series[\"series_\"+(\"_\").join(col.split(\"_\")[1:])]) for i, (col, path) in enumerate(paths.items())}\n        return self.output_tuple(**outputs)\n\n    def keys(self):\n        return list(self.paths.index)\n\n    def items(self):\n        return ((k, self[k]) for k in self.keys())\n\n\nclass ImageFileLoader(BaseLoader):\n    def __init__(self,\n                 root_directory,\n                 get_subject_id_from=\"filename\",\n                 subdir_path=None,\n                 exclude_paths=None,\n                 reader=None):\n\n        if exclude_paths is None:\n            exclude_paths = []\n        if reader is None:\n            reader = read_image  # no mutable defaults https://florimond.dev/en/posts/2018/08/python-mutable-defaults-are-the-source-of-all-evil/\n\n        self.root_directory = root_directory\n        self.get_subject_id_from = get_subject_id_from\n        self.subdir_path = subdir_path\n        self.exclude_paths = []\n        for path in exclude_paths:\n            if not path.startswith(self.root_directory):\n                full_paths = glob.glob(pathlib.Path(root_directory, path).as_posix())\n                self.exclude_paths.extend(full_paths)\n            else:\n                full_path = path\n                self.exclude_paths.append(full_path)\n        self.reader = reader\n\n        self.paths = self._generate_paths()\n\n    def _generate_paths(self):\n        paths = {}\n        for f in os.scandir(self.root_directory):\n            if f.path in self.exclude_paths:\n                continue\n            subject_dir_path = f.path\n            if self.subdir_path:\n                full_path = pathlib.Path(subject_dir_path, self.subdir_path).as_posix()\n            else:\n                full_path = subject_dir_path\n            try:\n                full_path = glob.glob(full_path)[0]\n            except IndexError:\n                continue\n            if os.path.isdir(full_path):\n                full_path = pathlib.Path(full_path, \"\").as_posix()\n            subject_dir_name = os.path.basename(os.path.normpath(subject_dir_path))\n            subject_id = self._extract_subject_id_from_path(full_path, subject_dir_name)\n            paths[subject_id] = full_path\n        return paths\n\n    def _extract_subject_id_from_path(self, full_path, subject_dir_name):\n        filename, _ = os.path.splitext(os.path.basename(full_path))\n        if isinstance(self.get_subject_id_from, str):\n            if self.get_subject_id_from == \"filename\":\n                subject_id = filename\n            elif self.get_subject_id_from == \"subject_directory\":\n                subject_id = subject_dir_name\n            else:\n                subject_id = re.search(self.get_subject_id_from, full_path)[0]\n        else:\n            return self.get_subject_id_from(full_path, filename, subject_dir_name)\n        return subject_id\n\n    def __getitem__(self, subject_id):\n        path = self.paths[subject_id]\n        return self.reader(path)\n\n    def keys(self):\n        return self.paths.keys()\n\n\n# class CombinedLoader(BaseLoader):\n#     def __init__(self, **kwargs):\n#         self.loaders = kwargs\n#         self.output_tuple = namedtuple(\"Output\", list(self.loaders.keys()))\n\n#     def __getitem__(self, subject_id):\n#         outputs = {name: loader[subject_id] for name, loader in self.loaders.items()}\n#         return self.output_tuple(**outputs)\n\n#     def keys(self):\n#         return set(chain.from_iterable(loader.keys() for loader in self.loaders))\n\n#     def items(self):\n#         return ((k, self[k]) for k in self.keys())\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read_dicom_series` function in the given code snippet?",
        "answer": "The `read_dicom_series` function is designed to read a DICOM series as a SimpleITK Image. It takes a path to a directory containing DICOM files, optionally a specific series ID, and other parameters. The function uses SimpleITK's ImageSeriesReader to load the DICOM series, including all metadata and private tags. It returns the loaded image as a SimpleITK Image object."
      },
      {
        "question": "How does the `ImageTreeLoader` class handle different types of input for the `csv_path_or_dataframe` parameter?",
        "answer": "The `ImageTreeLoader` class can handle two types of input for the `csv_path_or_dataframe` parameter: 1) A string representing the path to a CSV file, in which case it reads the CSV using pandas' `read_csv` function. 2) A pandas DataFrame object, which it uses directly. If an id_column is specified, it sets that column as the index of the DataFrame. If neither a string nor a DataFrame is provided, it raises a ValueError."
      },
      {
        "question": "What is the purpose of the `_extract_subject_id_from_path` method in the `ImageFileLoader` class?",
        "answer": "The `_extract_subject_id_from_path` method in the `ImageFileLoader` class is responsible for extracting a subject ID from a given file path. It can extract the ID in three ways based on the `get_subject_id_from` parameter: 1) From the filename, 2) From the subject directory name, or 3) Using a regular expression pattern. If `get_subject_id_from` is a function, it calls that function with the full path, filename, and subject directory name to extract the ID. This method allows flexible identification of subjects based on the file structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class ImageCSVLoader(BaseLoader):\n    def __init__(self,\n                 csv_path_or_dataframe,\n                 colnames=[],\n                 seriesnames=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n        # TODO: Implement the constructor\n        pass\n\n    def __getitem__(self, subject_id):\n        # TODO: Implement the item retrieval\n        pass\n\n    def keys(self):\n        # TODO: Implement the keys method\n        pass",
        "complete": "class ImageCSVLoader(BaseLoader):\n    def __init__(self,\n                 csv_path_or_dataframe,\n                 colnames=[],\n                 seriesnames=[],\n                 id_column=None,\n                 expand_paths=False,\n                 readers=None):\n        self.expand_paths = expand_paths\n        self.readers = readers or [read_image]\n        self.colnames = colnames\n        self.seriesnames = seriesnames\n\n        if isinstance(csv_path_or_dataframe, str):\n            if id_column is not None and id_column not in colnames:\n                colnames.append(id_column)\n            self.paths = pd.read_csv(csv_path_or_dataframe, index_col=id_column)\n        elif isinstance(csv_path_or_dataframe, pd.DataFrame):\n            self.paths = csv_path_or_dataframe\n            if id_column:\n                self.paths = self.paths.set_index(id_column)\n            if len(self.colnames) == 0:\n                self.colnames = self.paths.columns\n        else:\n            raise ValueError(f\"Expected a path to csv file or pd.DataFrame, not {type(csv_path_or_dataframe)}.\")\n\n        if not isinstance(self.readers, list):\n            self.readers = [self.readers] * len(self.colnames)\n\n        self.output_tuple = namedtuple(\"Output\", self.colnames)\n\n    def __getitem__(self, subject_id):\n        row = self.paths.loc[subject_id]\n        paths = {col: row[col] for col in self.colnames}\n        series = {col: row[col] for col in self.seriesnames}\n        paths = {k: v if pd.notna(v) else None for k, v in paths.items()}\n        \n        if self.expand_paths:\n            paths = {col: glob.glob(path)[0] if pd.notna(path) else None for col, path in paths.items()}\n        \n        outputs = {col: self.readers[i](path, series.get(f\"series_{'_'.join(col.split('_')[1:])}\")) for i, (col, path) in enumerate(paths.items())}\n        return self.output_tuple(**outputs)\n\n    def keys(self):\n        return list(self.paths.index)"
      },
      {
        "partial": "def read_dicom_auto(path, series=None, file_names=None):\n    # TODO: Implement the function to automatically read DICOM files\n    pass",
        "complete": "def read_dicom_auto(path, series=None, file_names=None):\n    if path is None:\n        return None\n    if path.endswith(\".dcm\"):\n        dcms = [path]\n    else:\n        dcms = glob.glob(pathlib.Path(path, \"*.dcm\").as_posix())\n        \n    for dcm in dcms:\n        meta = dcmread(dcm)\n        if meta.SeriesInstanceUID != series and series is not None:\n            continue\n        \n        modality = meta.Modality\n        if modality in ['CT', 'MR']:\n            obj = read_dicom_scan(path, series, file_names=file_names)\n        elif modality == 'PT':\n            obj = read_dicom_pet(path, series)\n        elif modality == 'RTSTRUCT':\n            obj = read_dicom_rtstruct(dcm)\n        elif modality == 'RTDOSE':\n            obj = read_dicom_rtdose(dcm)\n        elif modality == 'SEG':\n            obj = read_dicom_seg(path, meta, series)\n        else:\n            if len(dcms) == 1:\n                print(modality, 'at', dcms[0], 'is NOT implemented yet.')\n                raise NotImplementedError\n            else:\n                print(\"There were no dicoms in this path.\")\n                return None\n        \n        obj.metadata.update(get_modality_metadata(meta, modality))\n        return obj"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "json",
        "glob",
        "re",
        "pandas",
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Optional",
        "collections.namedtuple",
        "pydicom.dcmread",
        "modules.StructureSet",
        "utils.crawl.*",
        "utils.dicomutils.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_helpers.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"getPubchemStatus works\", {\n  output <- capture.output(getPubchemStatus(printMessage = TRUE), type = \"message\")\n  expect_match(output[1], \"Throttling status:\")\n  result <- getPubchemStatus(printMessage = FALSE)\n  expect_equal(result, NULL)\n\n  result <- getPubchemStatus(returnMessage = TRUE, printMessage = FALSE)\n\n  expect_class(result, \"list\")\n  expect_equal(names(result), c(\"request_count\", \"request_time\", \"service\"))\n})\n\n\ntest_that(\"checkThrottlingStatus Works\", {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\"\n\n\n\n  response <- AnnotationGx:::.buildURL(url) |>\n    AnnotationGx:::.build_pubchem_request() |>\n    httr2::req_perform()\n  message <- \"Request Count status: Yellow (60%), Request Time status: Yellow (60%), Service status: Yellow (60%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = TRUE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Yellow\", percent = 60),\n    request_time = list(status = \"Yellow\", percent = 60),\n    service = list(status = \"Yellow\", percent = 60)\n  ))\n\n\n  message <- \"Request Count status: Red (80%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Red\", percent = 80),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n\n  message <- \"Request Count status: Black (100%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Black\", percent = 100),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemStatus` function and how is it tested in the given code snippet?",
        "answer": "The `getPubchemStatus` function is used to retrieve the throttling status of PubChem API requests. It is tested in two ways: 1) It checks if the function prints a message containing 'Throttling status:' when `printMessage = TRUE`. 2) It verifies that the function returns a list with three specific keys ('request_count', 'request_time', 'service') when `returnMessage = TRUE` and `printMessage = FALSE`."
      },
      {
        "question": "How does the `.checkThrottlingStatus2` function parse the throttling status message, and what data structure does it return?",
        "answer": "The `.checkThrottlingStatus2` function parses a throttling status message and returns a nested list. The returned list contains three main keys: 'request_count', 'request_time', and 'service'. Each of these keys contains a sub-list with 'status' (a string like 'Yellow' or 'Red') and 'percent' (an integer representing the usage percentage). The function can handle different status levels, including 'Yellow', 'Red', and 'Black'."
      },
      {
        "question": "What testing framework and assertions are used in this code snippet, and how are they applied?",
        "answer": "This code snippet uses the `testthat` framework for unit testing in R. The main assertions used are `expect_match`, `expect_equal`, and `expect_class`. `expect_match` is used to check if the output contains a specific string. `expect_equal` is used to compare the actual output with expected values, such as comparing parsed throttling status information. `expect_class` is used to verify that the returned object is of the expected class (in this case, a list)."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"getPubchemStatus works\", {\n  output <- capture.output(getPubchemStatus(printMessage = TRUE), type = \"message\")\n  expect_match(output[1], \"Throttling status:\")\n  result <- getPubchemStatus(printMessage = FALSE)\n  expect_equal(result, NULL)\n\n  result <- getPubchemStatus(returnMessage = TRUE, printMessage = FALSE)\n\n  expect_class(result, \"list\")\n  # Complete the test by adding an expectation for the names of the result\n})",
        "complete": "test_that(\"getPubchemStatus works\", {\n  output <- capture.output(getPubchemStatus(printMessage = TRUE), type = \"message\")\n  expect_match(output[1], \"Throttling status:\")\n  result <- getPubchemStatus(printMessage = FALSE)\n  expect_equal(result, NULL)\n\n  result <- getPubchemStatus(returnMessage = TRUE, printMessage = FALSE)\n\n  expect_class(result, \"list\")\n  expect_equal(names(result), c(\"request_count\", \"request_time\", \"service\"))\n})"
      },
      {
        "partial": "test_that(\"checkThrottlingStatus Works\", {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\"\n\n  response <- AnnotationGx:::.buildURL(url) |>\n    AnnotationGx:::.build_pubchem_request() |>\n    httr2::req_perform()\n  message <- \"Request Count status: Yellow (60%), Request Time status: Yellow (60%), Service status: Yellow (60%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = TRUE)\n  # Complete the test by adding an expectation for the parsed_info\n\n  # Add two more test cases for different status messages\n})",
        "complete": "test_that(\"checkThrottlingStatus Works\", {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\"\n\n  response <- AnnotationGx:::.buildURL(url) |>\n    AnnotationGx:::.build_pubchem_request() |>\n    httr2::req_perform()\n  message <- \"Request Count status: Yellow (60%), Request Time status: Yellow (60%), Service status: Yellow (60%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = TRUE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Yellow\", percent = 60),\n    request_time = list(status = \"Yellow\", percent = 60),\n    service = list(status = \"Yellow\", percent = 60)\n  ))\n\n  message <- \"Request Count status: Red (80%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Red\", percent = 80),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n\n  message <- \"Request Count status: Black (100%), Request Time status: Red (80%), Service status: Red (80%)\"\n  parsed_info <- AnnotationGx:::.checkThrottlingStatus2(message, printMessage = FALSE)\n  expect_equal(parsed_info, list(\n    request_count = list(status = \"Black\", percent = 100),\n    request_time = list(status = \"Red\", percent = 80),\n    service = list(status = \"Red\", percent = 80)\n  ))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_helpers.R",
    "language": "R",
    "content": "#' Parses PubChem REST responses\n#'\n#' This function takes a list of PubChem REST responses and parses them into a\n#' standardized format. It checks the input for validity and handles error\n#' responses appropriately.\n#'\n#' @param responses A list of PubChem REST responses.\n#' @return A list of parsed PubChem responses, with each response parsed into a\n#'         data table format.\n#'\n#' @noRd\n#' @keywords internal\n.parse_pubchem_rest_responses <- function(responses) {\n  checkmate::assert_list(\n    x = responses,\n    any.missing = FALSE,\n    names = \"named\",\n    min.len = 1\n  )\n\n  responses_parsed <- lapply(names(responses), function(i) {\n    resp <- responses[[i]]\n    body <- .parse_resp_json(resp)\n    if (httr2::resp_is_error(resp)) {\n      return(.parseQueryToDT(NA_integer_))\n    }\n\n    return(.parseQueryToDT(body))\n  })\n  names(responses_parsed) <- names(responses)\n  return(responses_parsed)\n}\n\n\n\n#' Build a query for the PubChem REST API\n#'\n#' This function builds a query for the PubChem REST API based on the provided parameters.\n#'\n#' @param id The identifier(s) for the query. If namespace is 'name', id must be a single value.\n#' @param domain The domain of the query. Options are 'compound', 'substance', 'assay', 'cell', 'gene', 'protein'.\n#' @param namespace The namespace of the query. Options depend on the chosen domain.\n#' @param operation The operation to perform. Options depend on the chosen domain and namespace.\n#' @param output The desired output format. Options are 'JSON', 'XML', 'SDF', 'TXT', 'CSV'.\n#' @param url The base URL for the PubChem REST API.\n#' @param raw Logical indicating whether to return the raw response or parse it.\n#' @param query_only Logical indicating whether to return the query URL only.\n#' @param ... Additional arguments to be passed to the query.\n#'\n#' @return The query URL or the parsed response, depending on the arguments.\n#'\n#' @importFrom checkmate assert assert_choice assert_logical assert_atomic test_choice assert_integerish test_atomic\n#'\n#' @noRd\n#' @keywords internal\n.build_pubchem_rest_query <- function(id, domain = \"compound\", namespace = \"name\", operation = \"cids\",\n                                      output = \"JSON\", url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\",\n                                      raw = FALSE, query_only = FALSE, ...) {\n  # -------------------------------------- Argument checking --------------------------------------\n  assert_choice(domain, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n  switch(domain,\n    \"compound\" = {\n      assert_choice(namespace, c(\"cid\", \"name\", \"smiles\", \"inchi\", \"sdf\", \"inchikey\", \"formula\"))\n      assert(test_choice(\n        operation, c(\"record\", \"synonyms\", \"sids\", \"cids\", \"aids\", \"assaysummary\")\n      ) ||\n        grepl(\"property\", operation))\n    },\n    \"substance\" = assert_choice(namespace, c(\"sid\", \"sourceid\", \"sourceall\", \"name\")),\n    \"assay\" = assert_choice(namespace, c(\"aid\", \"listkey\", \"type\", \"sourceall\", \"target\", \"activity\")),\n    \"cell\" = assert_choice(namespace, c(\"cellacc\", \"synonym\")),\n    \"gene\" = assert_choice(namespace, c(\"geneid\", \"genesymbol\", \"synonym\")),\n    \"protein\" = assert_choice(namespace, c(\"accession\", \"gi\", \"synonym\"))\n  )\n  assert_choice(output, c(\"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\"))\n  assert_logical(raw, query_only)\n  if (!test_atomic(id, any.missing = FALSE)) .err(\"id must be an atomic vector with no missing/NA values\")\n\n  if (namespace == \"cid\") assert_integerish(id)\n\n  # -------------------------------------- Function context --------------------------------------\n  funContext <- .funContext(\"query_pubchem_rest\")\n  if (length(id) > 1 && namespace == \"name\") .err(funContext, \" id must be a single value when namespace is 'name'\")\n\n  url <- .buildURL(url, domain, namespace, id, operation, output)\n  .debug(funContext, \" Query URL: \", url)\n  if (query_only) {\n    return(url)\n  }\n\n  # -------------------------------------- Querying PubChem REST API --------------------------------------\n  .build_pubchem_request(url)\n}\n\n\n#' Builds a PubChem HTTP request using the provided URL.\n#'\n#' @param url The URL for the request.\n#' @return The built PubChem HTTP request.\n#' @noRd\n.build_pubchem_request <- function(url) {\n  .build_request(url) |>\n    httr2::req_throttle(rate = 1000 / 60)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.parse_pubchem_rest_responses` function and what are its key components?",
        "answer": "The `.parse_pubchem_rest_responses` function is designed to parse a list of PubChem REST responses into a standardized format. Its key components include: 1) Input validation using `checkmate::assert_list`, 2) Iterating through the responses using `lapply`, 3) Parsing each response body with `.parse_resp_json`, 4) Handling error responses by returning `NA_integer_`, and 5) Parsing successful responses using `.parseQueryToDT`."
      },
      {
        "question": "In the `.build_pubchem_rest_query` function, how does the code handle different domains and their corresponding namespaces?",
        "answer": "The function uses a `switch` statement to handle different domains and their corresponding namespaces. For each domain (compound, substance, assay, cell, gene, protein), it asserts that the provided namespace is valid using `assert_choice`. For example, if the domain is 'compound', it checks that the namespace is one of 'cid', 'name', 'smiles', 'inchi', 'sdf', 'inchikey', or 'formula'. This ensures that only valid combinations of domains and namespaces are used in the query construction."
      },
      {
        "question": "What is the purpose of the `.build_pubchem_request` function and how does it contribute to responsible API usage?",
        "answer": "The `.build_pubchem_request` function builds an HTTP request for the PubChem API using the provided URL. It contributes to responsible API usage by implementing rate limiting through the `httr2::req_throttle` function. Specifically, it sets a throttle rate of 1000 milliseconds per 60 requests (approximately 1 request per second), which helps prevent overwhelming the PubChem server and ensures compliance with their usage guidelines."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".parse_pubchem_rest_responses <- function(responses) {\n  checkmate::assert_list(\n    x = responses,\n    any.missing = FALSE,\n    names = \"named\",\n    min.len = 1\n  )\n\n  responses_parsed <- lapply(names(responses), function(i) {\n    resp <- responses[[i]]\n    body <- .parse_resp_json(resp)\n    if (httr2::resp_is_error(resp)) {\n      return(.parseQueryToDT(NA_integer_))\n    }\n\n    # Complete the function here\n  })\n  # Complete the function here\n}",
        "complete": ".parse_pubchem_rest_responses <- function(responses) {\n  checkmate::assert_list(\n    x = responses,\n    any.missing = FALSE,\n    names = \"named\",\n    min.len = 1\n  )\n\n  responses_parsed <- lapply(names(responses), function(i) {\n    resp <- responses[[i]]\n    body <- .parse_resp_json(resp)\n    if (httr2::resp_is_error(resp)) {\n      return(.parseQueryToDT(NA_integer_))\n    }\n\n    return(.parseQueryToDT(body))\n  })\n  names(responses_parsed) <- names(responses)\n  return(responses_parsed)\n}"
      },
      {
        "partial": ".build_pubchem_rest_query <- function(id, domain = \"compound\", namespace = \"name\", operation = \"cids\",\n                                      output = \"JSON\", url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\",\n                                      raw = FALSE, query_only = FALSE, ...) {\n  # Argument checking\n  assert_choice(domain, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n  switch(domain,\n    \"compound\" = {\n      assert_choice(namespace, c(\"cid\", \"name\", \"smiles\", \"inchi\", \"sdf\", \"inchikey\", \"formula\"))\n      assert(test_choice(\n        operation, c(\"record\", \"synonyms\", \"sids\", \"cids\", \"aids\", \"assaysummary\")\n      ) ||\n        grepl(\"property\", operation))\n    },\n    \"substance\" = assert_choice(namespace, c(\"sid\", \"sourceid\", \"sourceall\", \"name\")),\n    \"assay\" = assert_choice(namespace, c(\"aid\", \"listkey\", \"type\", \"sourceall\", \"target\", \"activity\")),\n    \"cell\" = assert_choice(namespace, c(\"cellacc\", \"synonym\")),\n    \"gene\" = assert_choice(namespace, c(\"geneid\", \"genesymbol\", \"synonym\")),\n    \"protein\" = assert_choice(namespace, c(\"accession\", \"gi\", \"synonym\"))\n  )\n  assert_choice(output, c(\"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\"))\n  assert_logical(raw, query_only)\n  if (!test_atomic(id, any.missing = FALSE)) .err(\"id must be an atomic vector with no missing/NA values\")\n\n  if (namespace == \"cid\") assert_integerish(id)\n\n  # Function context\n  funContext <- .funContext(\"query_pubchem_rest\")\n  if (length(id) > 1 && namespace == \"name\") .err(funContext, \" id must be a single value when namespace is 'name'\")\n\n  # Complete the function here\n}",
        "complete": ".build_pubchem_rest_query <- function(id, domain = \"compound\", namespace = \"name\", operation = \"cids\",\n                                      output = \"JSON\", url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug\",\n                                      raw = FALSE, query_only = FALSE, ...) {\n  # Argument checking\n  assert_choice(domain, c(\"compound\", \"substance\", \"assay\", \"cell\", \"gene\", \"protein\"))\n  switch(domain,\n    \"compound\" = {\n      assert_choice(namespace, c(\"cid\", \"name\", \"smiles\", \"inchi\", \"sdf\", \"inchikey\", \"formula\"))\n      assert(test_choice(\n        operation, c(\"record\", \"synonyms\", \"sids\", \"cids\", \"aids\", \"assaysummary\")\n      ) ||\n        grepl(\"property\", operation))\n    },\n    \"substance\" = assert_choice(namespace, c(\"sid\", \"sourceid\", \"sourceall\", \"name\")),\n    \"assay\" = assert_choice(namespace, c(\"aid\", \"listkey\", \"type\", \"sourceall\", \"target\", \"activity\")),\n    \"cell\" = assert_choice(namespace, c(\"cellacc\", \"synonym\")),\n    \"gene\" = assert_choice(namespace, c(\"geneid\", \"genesymbol\", \"synonym\")),\n    \"protein\" = assert_choice(namespace, c(\"accession\", \"gi\", \"synonym\"))\n  )\n  assert_choice(output, c(\"JSON\", \"XML\", \"SDF\", \"TXT\", \"CSV\"))\n  assert_logical(raw, query_only)\n  if (!test_atomic(id, any.missing = FALSE)) .err(\"id must be an atomic vector with no missing/NA values\")\n\n  if (namespace == \"cid\") assert_integerish(id)\n\n  # Function context\n  funContext <- .funContext(\"query_pubchem_rest\")\n  if (length(id) > 1 && namespace == \"name\") .err(funContext, \" id must be a single value when namespace is 'name'\")\n\n  url <- .buildURL(url, domain, namespace, id, operation, output)\n  .debug(funContext, \" Query URL: \", url)\n  if (query_only) return(url)\n\n  .build_pubchem_request(url)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/utils/__init__.py",
    "language": "py",
    "content": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]",
    "qa_pairs": [
      {
        "question": "What is the purpose of the __all__ list in this Python module?",
        "answer": "The __all__ list in this module explicitly defines which names should be imported when using 'from module import *'. In this case, it specifies that only 'setup_logger' and 'get_logger' should be available for import, providing better control over the module's public interface."
      },
      {
        "question": "How does this code snippet demonstrate the use of relative imports in Python?",
        "answer": "The code uses a relative import with the line 'from .logging_config import setup_logger, get_logger'. The dot (.) before 'logging_config' indicates that the 'logging_config' module is in the same package as the current module, demonstrating how to import from sibling modules within the same package."
      },
      {
        "question": "What is the relationship between the imported functions and the __all__ list in this code?",
        "answer": "The functions 'setup_logger' and 'get_logger' are imported from the 'logging_config' module and then explicitly listed in the __all__ variable. This pattern is often used to re-export specific functions from a submodule, making them appear as if they were defined in the current module while maintaining a clean internal structure."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    # Complete the __all__ list\n]",
        "complete": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]"
      },
      {
        "partial": "# Import the necessary functions from logging_config\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]",
        "complete": "from .logging_config import setup_logger, get_logger\n\n__all__ = [\n    'setup_logger',\n    'get_logger',\n]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "logging_config.setup_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CCLE/CCLE_treatmentdata.R",
    "language": "R",
    "content": "# https://data.broadinstitute.org/ccle_legacy_data/pharmacological_profiling/CCLE_NP24.2009_profiling_2012.02.20.csv\nfilePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code, and how is it being used?",
        "answer": "The `system.file()` function is used to locate and construct the file path for a specific CSV file within the 'AnnotationGx' package. It's being used to find the file 'CCLE_NP24.2009_profiling_2012.02.20.csv' in the 'extdata/CCLE' directory of the package. This allows the code to access package-specific data files in a portable manner, regardless of where the package is installed on the user's system."
      },
      {
        "question": "How is the `data.table::fread()` function being used in this code snippet, and what are its parameters?",
        "answer": "The `data.table::fread()` function is being used to read the CSV file specified by `filePath` into R. It's called with two parameters: 'input', which is set to the `filePath` variable containing the path to the CSV file, and 'encoding', which is set to 'Latin-1' to specify the character encoding of the file. The `fread()` function is part of the data.table package and is known for its fast reading of large data files."
      },
      {
        "question": "What does the code do with the `rawdata` after reading it, and what is the purpose of the `CCLE_treatmentMetadata` variable?",
        "answer": "After reading the raw data, the code creates a new data table called `CCLE_treatmentMetadata`. This new table contains only one column, renamed as 'CCLE.treatmentid', which is extracted from the 'Compound (code or generic name)' column of the original `rawdata`. The purpose of `CCLE_treatmentMetadata` is likely to create a simplified dataset containing only the treatment IDs from the original data, which can be used for further analysis or as a reference table."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = # Complete this line\n\n# Complete the code to save the data",
        "complete": "filePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the CSV file and create CCLE_treatmentMetadata\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)",
        "complete": "filePath <- system.file(\"extdata/CCLE\", \"CCLE_NP24.2009_profiling_2012.02.20.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(\n    input = filePath, \n    encoding = \"Latin-1\")\n\nCCLE_treatmentMetadata <- \n    rawdata[, .(CCLE.treatmentid = `Compound (code or generic name)`)]\n\nusethis::use_data(CCLE_treatmentMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_standardize_names.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\n\ntest_that(\"standardize_names converts names to lowercase, removes trailing information, removes non-alphanumeric characters, replaces empty names with 'invalid', and converts names to uppercase\", {\n  # Test case 1: Standardize names without any special characters\n  names1 <- c(\"John Doe\", \"Jane Smith\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with trailing information\n  names2 <- c(\"John Doe, Manager\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n  # Test case 3: Standardize names with square brackets and parentheses\n  names3 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected3 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result3 <- standardize_names(names3)\n  expect_equal(result3, expected3)\n\n  # Test case 5: Standardize names with empty names\n  names5 <- c(\"John Doe\", \"\", \"Alice\")\n  expected5 <- c(\"JOHNDOE\", NA, \"ALICE\")\n  result5 <- standardize_names(names5)\n  expect_equal(result5, expected5)\n\n\n  # Test case 7: Standardize names with leading and trailing spaces\n  names7 <- c(\"  John Doe  \", \" Jane Smith \", \" Alice \")\n  expected7 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result7 <- standardize_names(names7)\n  expect_equal(result7, expected7)\n\n  # Test case 8: Standardize names with numbers\n  names8 <- c(\"John Doe 1\", \"Jane Smith 2\", \"Alice 3\")\n  expected8 <- c(\"JOHNDOE1\", \"JANESMITH2\", \"ALICE3\")\n  result8 <- standardize_names(names8)\n  expect_equal(result8, expected8)\n\n  # Test case 11: Standardize names with non-alphanumeric characters and numbers\n  names11 <- c(\"John Doe\", 1, \"Alice\")\n  expected11 <- c(\"JOHNDOE\", \"1\", \"ALICE\")\n  result11 <- standardize_names(names11)\n  expect_equal(result11, expected11)\n})\n\ntest_that(\"standardize_names Error\", {\n  names <- c(\"John Doe\", NA, \"Alice\")\n  expect_error(standardize_names(names))\n\n  names <- c(1, 1, 1)\n  expect_error(standardize_names(names))\n}) \n\n\n# Test case 6: Standardize names with special characters\ntest_that(\"cleanCharacterStrings removes special characters, formatting, and unwanted substrings\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n\n  # Test case 5: Clean string with hyphen\n  input5 <- \"Bio-informatics\"\n  expected5 <- \"BIOINFORMATICS\"\n  result5 <- cleanCharacterStrings(input5)\n  expect_equal(result5, expected5)\n})\n\ntest_that(\"standardize_names handles different scenarios\", {\n  # Test case 1: Standardize names with trailing information after a comma\n  names1 <- c(\"John Doe, Manager\", \"Jane Smith, PhD\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with information within square brackets or parentheses\n  names2 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n\n  # Test case 4: Standardize names with empty names\n  names4 <- c(\"John Doe\", \"\", \"Alice\")\n  expected4 <- c(\"JOHNDOE\", NA, \"ALICE\")\n  result4 <- standardize_names(names4)\n  expect_equal(result4, expected4)\n\n  # Test case 5: Standardize names with leading and trailing spaces\n  names5 <- c(\"  John Doe  \", \" Jane Smith \", \" Alice \")\n  expected5 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result5 <- standardize_names(names5)\n  expect_equal(result5, expected5)\n\n  # Test case 6: Standardize names with numbers\n  names6 <- c(\"John Doe 1\", \"Jane Smith 2\", \"Alice 3\")\n  expected6 <- c(\"JOHNDOE1\", \"JANESMITH2\", \"ALICE3\")\n  result6 <- standardize_names(names6)\n  expect_equal(result6, expected6)\n\n  # Test case 9: Standardize names with non-alphanumeric characters and numbers\n  names9 <- c(\"John Doe\", 1, \"Alice\")\n  expected9 <- c(\"JOHNDOE\", \"1\", \"ALICE\")\n  result9 <- standardize_names(names9)\n  expect_equal(result9, expected9)\n})\n\ntest_that(\"cleanCharacterStrings handles different scenarios\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n\n  # Test case 5: Clean string with hyphen\n  input5 <- \"Bio-informatics\"\n  expected5 <- \"BIOINFORMATICS\"\n  result5 <- cleanCharacterStrings(input5)\n  expect_equal(result5, expected5)\n\n  # Test case 6: Clean numeric input\n  input6 <- 12345\n  expected6 <- \"12345\"\n  result6 <- cleanCharacterStrings(input6)\n  expect_equal(result6, expected6)\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `standardize_names` function based on the test cases provided?",
        "answer": "The `standardize_names` function is designed to standardize names by converting them to uppercase, removing trailing information (such as titles or qualifications), removing non-alphanumeric characters, replacing empty names with NA, and trimming leading and trailing spaces. It handles various scenarios including names with special characters, numbers, and different formatting."
      },
      {
        "question": "How does the `cleanCharacterStrings` function differ from `standardize_names`, and what specific task does it perform?",
        "answer": "The `cleanCharacterStrings` function is more focused on cleaning individual strings rather than standardizing a vector of names. It removes special characters, formatting, and unwanted substrings from input strings. Unlike `standardize_names`, it doesn't handle empty strings or convert them to NA. It also appears to handle numeric inputs by converting them to strings."
      },
      {
        "question": "What are the key differences in how `standardize_names` handles empty strings versus non-string inputs?",
        "answer": "For empty strings, `standardize_names` converts them to NA values. For non-string inputs like numbers, it converts them to string representations. However, the function throws an error when encountering NA values in the input vector, as shown in the error test case."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"standardize_names handles different scenarios\", {\n  # Test case 1: Standardize names with trailing information after a comma\n  names1 <- c(\"John Doe, Manager\", \"Jane Smith, PhD\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with information within square brackets or parentheses\n  names2 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"standardize_names handles different scenarios\", {\n  # Test case 1: Standardize names with trailing information after a comma\n  names1 <- c(\"John Doe, Manager\", \"Jane Smith, PhD\", \"Alice\")\n  expected1 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result1 <- standardize_names(names1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Standardize names with information within square brackets or parentheses\n  names2 <- c(\"John Doe [Manager]\", \"Jane Smith (Manager)\", \"Alice, PhD\")\n  expected2 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result2 <- standardize_names(names2)\n  expect_equal(result2, expected2)\n\n  # Test case 3: Standardize names with empty names\n  names3 <- c(\"John Doe\", \"\", \"Alice\")\n  expected3 <- c(\"JOHNDOE\", NA, \"ALICE\")\n  result3 <- standardize_names(names3)\n  expect_equal(result3, expected3)\n\n  # Test case 4: Standardize names with leading and trailing spaces\n  names4 <- c(\"  John Doe  \", \" Jane Smith \", \" Alice \")\n  expected4 <- c(\"JOHNDOE\", \"JANESMITH\", \"ALICE\")\n  result4 <- standardize_names(names4)\n  expect_equal(result4, expected4)\n\n  # Test case 5: Standardize names with numbers\n  names5 <- c(\"John Doe 1\", \"Jane Smith 2\", \"Alice 3\")\n  expected5 <- c(\"JOHNDOE1\", \"JANESMITH2\", \"ALICE3\")\n  result5 <- standardize_names(names5)\n  expect_equal(result5, expected5)\n\n  # Test case 6: Standardize names with non-alphanumeric characters and numbers\n  names6 <- c(\"John Doe\", 1, \"Alice\")\n  expected6 <- c(\"JOHNDOE\", \"1\", \"ALICE\")\n  result6 <- standardize_names(names6)\n  expect_equal(result6, expected6)\n})"
      },
      {
        "partial": "test_that(\"cleanCharacterStrings handles different scenarios\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n  # Add more test cases here\n})",
        "complete": "test_that(\"cleanCharacterStrings handles different scenarios\", {\n  # Test case 1: Clean string without any special characters\n  input1 <- \"John Doe\"\n  expected1 <- \"JOHNDOE\"\n  result1 <- cleanCharacterStrings(input1)\n  expect_equal(result1, expected1)\n\n  # Test case 2: Clean string with special characters and formatting\n  input2 <- \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  expected2 <- \"CISPLATIN\"\n  result2 <- cleanCharacterStrings(input2)\n  expect_equal(result2, expected2)\n\n  # Test case 3: Clean string with hyphen\n  input3 <- \"Bio-informatics\"\n  expected3 <- \"BIOINFORMATICS\"\n  result3 <- cleanCharacterStrings(input3)\n  expect_equal(result3, expected3)\n\n  # Test case 4: Clean numeric input\n  input4 <- 12345\n  expected4 <- \"12345\"\n  result4 <- cleanCharacterStrings(input4)\n  expect_equal(result4, expected4)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_logLogisticRegression.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Testing LogLogisticRegression.\")\n\n##TO-DO::Supress print to console from this test file\n\ntest_that(\"Errors are checked.\",{\n\t\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60))) #should complain\n    expect_warning(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), viability_as_pct = FALSE)) #should complain\n    expect_error(logLogisticRegression(c(-1, 2, 3), c(70, 60, 50), conc_as_log = FALSE)) #should complain\n\n    expect_error(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), median_n = 0)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), median_n = 3/2)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1))) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1, -1))) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), precision = 0)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), scale = 0)) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), lower_bounds = c(0, 0, 0), upper_bounds = c(1, 1, -1))) #should complain\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), family = \"The Addams Family\")) #should complain\n})\n\ntest_that(\"Values returned as expected (previous runs of function).\",{\n\t\n\texpect_equivalent(\n\t\tlogLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0))\n\t\t\t, conc_as_log=TRUE, viability_as_pct = FALSE), list(1,0,0))\n\n\texpect_equivalent(\n\t\tlogLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0))\n\t\t\t, conc_as_log=TRUE, viability_as_pct = FALSE, family=\"Cauchy\"), list(1,0,0))\n\n\texpect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40)), \n        structure(list(HS = 1.28651256396627, E_inf = 36.2653101620223, EC50 = 1.1810533048852), Rsquare = 0.994776857838702), \n        tolerance=1e-3) #should run with no objections\n\n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40), family=\"Cauchy\"), \n        structure(list(HS = 1.29137210106265, E_inf = 36.32166034246, EC50 = 1.17645746710051), Rsquare = 0.994810069836551), \n        tolerance=1e-3) #should run with no objections\n\n\texpect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE), \n        structure(structure(list(HS = 1.33880390747459, E_inf = 36.8315342784204, \n        EC50 = 1.17467467087487), Rsquare = 0.993325611444731), Rsquare = 0.992907719144335), \n        tolerance=1e-3) #should run with no objections\n\n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE, family=\"Cauchy\"), \n        structure(list(HS = 1.33972330068866, E_inf = 36.8279821440339, EC50 = 1.17127888613006), Rsquare = 0.993391659104375),\n        tolerance=1e-3) #should run with no objections\n\n    ## These next few tests make sure trunc is doing something sensible\n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=FALSE), \n        structure(list(HS = 1.83941027802297, E_inf = 46.2252841409534, EC50 = 0.929240163785174), Rsquare = 0.950154102951421),\n        tolerance=1e-3) #should run with no objections\n    \n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=TRUE), \n        structure(list(HS = 2.06741101065827, E_inf = 48.0764684303728, EC50 = 0.900808050726654), Rsquare = 0.986745954925997),\n        tolerance=1e-3) #should run with no objections\n\n    expect_equivalent(logLogisticRegression(c(0.1, 1, 2, 3), c(100, 70, 60, 50), trunc=TRUE), \n        logLogisticRegression(c(0.1, 1, 2, 3), c(500, 70, 60, 50), trunc=TRUE))\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `logLogisticRegression` function in this code, and what are its main parameters?",
        "answer": "The `logLogisticRegression` function appears to be a custom implementation for fitting a log-logistic regression model. Its main parameters include:\n1. A vector of concentrations\n2. A vector of corresponding viability or response values\n3. `conc_as_log`: Boolean indicating if concentrations are already log-transformed\n4. `viability_as_pct`: Boolean indicating if viability is expressed as a percentage\n5. `family`: The distribution family (e.g., 'Cauchy' or default)\n6. `trunc`: Boolean for truncation of values\n7. Other parameters like `median_n`, `density`, `precision`, `scale`, and `lower_bounds`/`upper_bounds` for fine-tuning the regression."
      },
      {
        "question": "How does this test suite handle different types of input validation for the `logLogisticRegression` function?",
        "answer": "The test suite handles input validation for `logLogisticRegression` in several ways:\n1. It checks for mismatched vector lengths between concentrations and viability values.\n2. It verifies that warnings are raised when `viability_as_pct` is set incorrectly.\n3. It ensures errors are thrown for invalid concentration values when `conc_as_log` is false.\n4. It tests for proper error handling with invalid `median_n`, `density`, `precision`, and `scale` values.\n5. It checks that appropriate errors are raised for invalid `lower_bounds` and `upper_bounds`.\n6. It verifies that an error is thrown when an invalid `family` parameter is provided.\nThese tests use `expect_error()` and `expect_warning()` functions to assert that the appropriate exceptions are raised for invalid inputs."
      },
      {
        "question": "What are the key aspects of the `logLogisticRegression` function's output that are being tested in this code?",
        "answer": "The test suite checks several key aspects of the `logLogisticRegression` function's output:\n1. Correctness: It verifies that the function returns expected values for known inputs, using `expect_equivalent()` and `expect_equal()` with predefined tolerance levels.\n2. Consistency: It ensures that the function produces similar results for different input scales when using the `trunc` parameter.\n3. Parameter estimation: The tests check if the function correctly estimates the Hill Slope (HS), E_inf (minimum effect), and EC50 (half maximal effective concentration) parameters.\n4. R-squared values: The tests verify the goodness of fit by checking the returned R-squared values.\n5. Different families: It compares results between the default family and the 'Cauchy' family to ensure both options work correctly.\n6. Truncation effects: The tests examine how the `trunc` parameter affects the output, especially for edge cases with values outside the expected range."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Errors are checked.\", {\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60)))\n    expect_warning(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), viability_as_pct = FALSE))\n    expect_error(logLogisticRegression(c(-1, 2, 3), c(70, 60, 50), conc_as_log = FALSE))\n    # Add more expect_error statements here\n})",
        "complete": "test_that(\"Errors are checked.\", {\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60)))\n    expect_warning(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), viability_as_pct = FALSE))\n    expect_error(logLogisticRegression(c(-1, 2, 3), c(70, 60, 50), conc_as_log = FALSE))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(70, 60, 50), median_n = 0))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), median_n = 3/2))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1)))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), density = c(1, 1, -1)))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), precision = 0))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), scale = 0))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), lower_bounds = c(0, 0, 0), upper_bounds = c(1, 1, -1)))\n    expect_error(logLogisticRegression(c(1, 2, 3), c(50, 60, 70), family = \"The Addams Family\"))\n})"
      },
      {
        "partial": "test_that(\"Values returned as expected (previous runs of function).\", {\n    expect_equivalent(\n        logLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0)),\n            conc_as_log=TRUE, viability_as_pct = FALSE), list(1,0,0))\n    \n    # Add more expect_equivalent and expect_equal statements here\n})",
        "complete": "test_that(\"Values returned as expected (previous runs of function).\", {\n    expect_equivalent(\n        logLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0)),\n            conc_as_log=TRUE, viability_as_pct = FALSE), list(1,0,0))\n    \n    expect_equivalent(\n        logLogisticRegression(seq(-10,10,0.1), .Hill(seq(-10,10,0.1), c(1,0,0)),\n            conc_as_log=TRUE, viability_as_pct = FALSE, family=\"Cauchy\"), list(1,0,0))\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40)), \n        structure(list(HS = 1.28651256396627, E_inf = 36.2653101620223, EC50 = 1.1810533048852), Rsquare = 0.994776857838702), \n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(99,70, 60, 50,40), family=\"Cauchy\"), \n        structure(list(HS = 1.29137210106265, E_inf = 36.32166034246, EC50 = 1.17645746710051), Rsquare = 0.994810069836551), \n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE), \n        structure(structure(list(HS = 1.33880390747459, E_inf = 36.8315342784204, \n        EC50 = 1.17467467087487), Rsquare = 0.993325611444731), Rsquare = 0.992907719144335), \n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1,1, 2, 3,10), c(100,70, 60, 50,40), trunc=FALSE, family=\"Cauchy\"), \n        structure(list(HS = 1.33972330068866, E_inf = 36.8279821440339, EC50 = 1.17127888613006), Rsquare = 0.993391659104375),\n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=FALSE), \n        structure(list(HS = 1.83941027802297, E_inf = 46.2252841409534, EC50 = 0.929240163785174), Rsquare = 0.950154102951421),\n        tolerance=1e-3)\n    \n    expect_equal(logLogisticRegression(c(0.1, 1, 2, 3), c(110, 70, 60, 50), family=\"Cauchy\", trunc=TRUE), \n        structure(list(HS = 2.06741101065827, E_inf = 48.0764684303728, EC50 = 0.900808050726654), Rsquare = 0.986745954925997),\n        tolerance=1e-3)\n    \n    expect_equivalent(logLogisticRegression(c(0.1, 1, 2, 3), c(100, 70, 60, 50), trunc=TRUE), \n        logLogisticRegression(c(0.1, 1, 2, 3), c(500, 70, 60, 50), trunc=TRUE))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/matthewCor.R",
    "language": "R",
    "content": "## Matthews correlatipon coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    CoreGx::mcc(x, y, nperm, nthread)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mcc` function in this code snippet, and what package does it rely on?",
        "answer": "The `mcc` function is designed to compute a Matthews Correlation Coefficient. It relies on the `CoreGx` package, specifically calling the `CoreGx::mcc` function internally. This function appears to be a wrapper around the `CoreGx::mcc` function, providing the same functionality while potentially making it more accessible or consistent with the current package's naming conventions."
      },
      {
        "question": "What does the `#' @inherit CoreGx::mcc` comment do in the context of R package documentation?",
        "answer": "The `#' @inherit CoreGx::mcc` comment is a roxygen2 documentation tag. It indicates that this function should inherit the documentation from the `mcc` function in the `CoreGx` package. This is useful when creating wrapper functions, as it allows you to reuse existing documentation without duplicating it, ensuring consistency and reducing maintenance overhead."
      },
      {
        "question": "What are the parameters of the `mcc` function, and how does it handle them?",
        "answer": "The `mcc` function takes four parameters: `x`, `y`, `nperm` (with a default value of 1000), and `nthread` (with a default value of 1). These parameters are passed directly to the `CoreGx::mcc` function without any modification. This suggests that the wrapper function maintains the same interface as the original function, allowing users to specify the same parameters they would use with `CoreGx::mcc`."
      }
    ],
    "completion_tasks": [
      {
        "partial": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    # Complete the function body\n}",
        "complete": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    CoreGx::mcc(x, y, nperm, nthread)\n}"
      },
      {
        "partial": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, ...) {\n    # Complete the function signature and body\n}",
        "complete": "## Matthews correlation coefficient\n#' Compute a Mathews Correlation Coefficient\n#'\n#' @inherit CoreGx::mcc\n#'\n#' @export\nmcc <- function(x, y, nperm=1000, nthread=1) {\n    CoreGx::mcc(x, y, nperm, nthread)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/downloadSignatures.R",
    "language": "R",
    "content": "#' Download Drug Perturbation Signatures\n#' \n#' This function allows you to download an array of drug perturbation\n#' signatures, as would be computed by the `drugPerturbationSig` function,\n#' for the available perturbation `PharmacoSets`. This function allows the\n#' user to skip these very lengthy calculation steps for the datasets available,\n#' and start their analysis from the already computed signatures\n#' \n#' @examples\n#'\n#' \\dontrun{\n#'     if (interactive()) downloadPertSig(\"CMAP_2016\")\n#' }\n#' \n#' @param name A `character(1)` string, the name of the PharmacoSet for which\n#'   to download signatures. The name should match the names returned in the\n#'   `PSet Name` column of `availablePSets(canonical=FALSE)`.\n#' @param saveDir A `character(1)` string with the folder path where the\n#'   PharmacoSet should be saved. Defaults to `\"./PSets/Sigs/\"`. Will\n#'   create directory if it does not exist.\n#' @param fileName `character(1)` What to name the downloaded file. Defaults\n#' to '`name`_signature.RData' when excluded.\n#' @param verbose `logical(1)` Should `downloader` show detailed messages?\n#' @param ... `pairlist` Force subsequent arguments to be named.\n#' @param myfn `character(1)` A deprecated version of `fileName`. Still works\n#' for now, but will be deprecated in future releases.\n#'\n#' @return An array type object contaning the signatures\n#'\n#' @export\n#' @importFrom CoreGx .warning .funContext\n#' @import downloader\ndownloadPertSig <- function(name, saveDir=file.path(\".\", \"PSets\", \"Sigs\"), \n    fileName, verbose=TRUE, ..., myfn) \n{\n    funContext <- .funContext('::downloadPertSig')\n    if (missing(fileName) && !missing(myfn)) {\n        .warning(funContext, 'The `myfn` parameter is being deprecated in \n            favour of `fileName`. It still works for now, but will be retired\n            in a future release.')\n        fileName <- myfn\n    }\n\n    # change the download timeout since the files are big\n    opts <- options()\n    options(timeout=600)\n    on.exit(options(opts))\n\n    # get the annotations for available data\n    pSetTable <- availablePSets(canonical=FALSE)\n\n    # pick a signature from the list\n    whichx <- match(name, pSetTable[, 3])\n    if (is.na(whichx)){\n        stop('Unknown Dataset. Please use the `Dataset Name` column in the\n            data.frame returned by the availablePSet function to select a\n            PharmacoSet')\n    }\n    if (!pSetTable[whichx, \"type\"] %in% c(\"perturbation\", \"both\")){\n        stop('Signatures are available only for perturbation type datasets')\n    }\n\n    if(!file.exists(saveDir)) {\n        dir.create(saveDir, recursive=TRUE)\n    }\n\n    if (missing(fileName)) {\n        fileName <- paste(pSetTable[whichx, ]$`Dataset Name`, \n            \"_signatures.RData\", sep=\"\")\n    }\n\n    downloader::download(\n        paste(\"https://www.pmgenomics.ca/bhklab/sites/default/files/downloads\", \n            fileName, sep='/'), \n        destfile=file.path(saveDir, fileName), quiet=!verbose, mode='wb')\n\n    sig <- load(file.path(saveDir, fileName))\n\n    return(get(sig))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `downloadPertSig` function and what type of data does it return?",
        "answer": "The `downloadPertSig` function is designed to download pre-computed drug perturbation signatures for available PharmacoSets. It returns an array-type object containing the signatures, allowing users to skip lengthy calculation steps and start their analysis with already computed signatures."
      },
      {
        "question": "How does the function handle the deprecated `myfn` parameter, and what is the recommended alternative?",
        "answer": "The function checks if `fileName` is missing and `myfn` is provided. If so, it issues a warning that `myfn` is being deprecated in favor of `fileName`. It then assigns the value of `myfn` to `fileName`. The recommended alternative is to use `fileName` instead of `myfn` for specifying the name of the downloaded file."
      },
      {
        "question": "What checks does the function perform before downloading the signatures, and what errors might it throw?",
        "answer": "The function performs several checks: 1) It verifies if the provided `name` exists in the available PharmacoSets. 2) It checks if the dataset is of type 'perturbation' or 'both'. If either check fails, it throws an error. The function may stop with an 'Unknown Dataset' error if the name is not found, or a 'Signatures are available only for perturbation type datasets' error if the dataset type is incorrect."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/inst/extdata/test_cellosaurus_detailed.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\nids <- c(\"HT\")\nto = cellosaurus_fields(common=T)\nfrom <- \"idsy\"\nfuzzy <- FALSE\nnumResults <- 1000\nsort = \"ac\"\nparsed = FALSE\nkeep_duplicates = FALSE\nquery <- AnnotationGx:::.create_cellosaurus_queries(ids, from, fuzzy)\nnames(query) <- ids\nrequests <- AnnotationGx:::.build_cellosaurus_request(\n    query = query,\n    to = to,\n    numResults = numResults,\n    sort = sort,\n    output = \"TXT\",\n    fuzzy = fuzzy\n)\nresponses <- AnnotationGx:::.perform_request_parallel(list(requests))\nnames(responses) <- as.character(ids)\n\nlines <- httr2::resp_body_string(responses[[ids[1]]]) |>\n    strsplit(\"\\n\") |>\n    unlist()\n\n# Test case 1: Test with a valid cell line name\nlines <- readRDS(system.file(\"extdata\", \"cellosaurus_HT_raw_lines.RDS\", package = \"AnnotationGx\"))\n\nparsed_lines <- \n    Map(\n    f = function(lines, i, j) {\n        lines[i:(j - 1L)]\n    },\n    i = grep(pattern = \"^ID\\\\s+\", x = lines, value = FALSE),\n    j = grep(pattern = \"^//$\", x = lines, value = FALSE),\n    MoreArgs = list(\"lines\" = lines),\n    USE.NAMES = FALSE\n)\n\n\n\nrequiredKeys = c(\"AC\", \"CA\", \"DT\", \"ID\")\nnestedKeys = c(\"DI\", \"DR\", \"HI\", \"OI\", \"OX\", \"WW\")\noptionalKeys = c(\"AG\", \"SX\", \"SY\", \"ACAS\", \"DIN\", \"DIO\", \"CH\", \"DTC\", \"DTU\", \"DTV\", \"FROM\", \"GROUP\")\nspecialKeys = c(\"CC\")\n\nx <- strSplit(parsed_lines[[1]], split = \"   \")\nx <- split(x[, 2L], f = x[, 1L])\n\n\ntest_that(\".formatComments works as expected\", {\n    # cc_column <- AnnotationGx:::.formatComments(x)\n\n    dt <- data.table::data.table(rbind(x))\n\n    da(); .formatComments(dt)\n\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cellosaurus_fields` function in this code snippet, and how is it used?",
        "answer": "The `cellosaurus_fields` function is used to retrieve a list of common fields from the Cellosaurus database. In this code, it's called with the argument `common=T`, which likely means it returns only the commonly used fields. The result is stored in the `to` variable, which is later used as a parameter in the `.build_cellosaurus_request` function to specify which fields should be included in the request."
      },
      {
        "question": "Explain the purpose and functionality of the `.perform_request_parallel` function in this code.",
        "answer": "The `.perform_request_parallel` function is used to execute multiple HTTP requests in parallel. It takes a list of requests (in this case, `list(requests)`) as an argument. The function likely sends these requests concurrently to improve efficiency when querying the Cellosaurus database. The responses are then stored in the `responses` variable, with the names of the responses set to the corresponding cell line IDs."
      },
      {
        "question": "What is the purpose of the `Map` function used near the end of the code snippet, and what does it do with the `lines` variable?",
        "answer": "The `Map` function is used here to parse the raw response lines into separate entries. It applies a custom function to each set of lines between 'ID' and '//' markers in the response. This function extracts each individual cell line entry from the raw response. The result, `parsed_lines`, is a list where each element contains the lines for a single cell line entry, effectively separating the raw response into structured data for further processing."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeAmax.R",
    "language": "R",
    "content": "#' Fits dose-response curves to data given by the user\n#' and returns the Amax of the fitted curve.\n#' Amax: 100 - viability at maximum concentarion (in fitted curve)\n#'\n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeAmax(dose, viability)\n#'\n#' @param concentration `numeric` is a vector of drug concentrations.\n#'\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of the log_conc, expressed as percentages\n#' of viability in the absence of any drug.\n#'\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical` should warnings be printed\n#' @return The numerical Amax\n#' @export\ncomputeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n\n  #CHECK THAT FUNCTION INPUTS ARE APPROPRIATE\n  if (!all(is.finite(concentration))) {\n    print(concentration)\n    stop(\"Concentration vector contains elements which are not real numbers.\")\n  }\n\n  if (!all(is.finite(viability))) {\n    print(viability)\n    stop(\"Viability vector contains elements which are not real numbers.\")\n  }\n\n  if (is.logical(trunc) == FALSE) {\n    print(trunc)\n    stop(\"'trunc' is not a logical.\")\n  }\n\n  if (length(concentration) != length(viability)) {\n    print(concentration)\n    print(viability)\n    stop(\"Concentration vector is not of same length as viability vector.\")\n  }\n\n  if (min(concentration) < 0) {\n    stop(\"Concentration vector contains negative data.\")\n  }\n\n  if (min(viability) < 0 & verbose) {\n    warning(\"Warning: Negative viability data.\")\n  }\n\n  if (max(viability) > 100 & verbose) {\n    warning(\"Warning: Viability data exceeds negative control.\")\n  }\n\n  #CONVERT DOSE-RESPONSE DATA TO APPROPRIATE INTERNAL REPRESENTATION\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc == TRUE) {\n    viability[which(viability < 0)] <- 0\n    viability[which(viability > 1)] <- 1\n  }\n\n  #FIT CURVE AND CALCULATE IC50\n  pars <- unlist(logLogisticRegression(log_conc,\n                                       viability,\n                                       conc_as_log = TRUE,\n                                       viability_as_pct = FALSE,\n                                       trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeAmax` function and what does it return?",
        "answer": "The `computeAmax` function fits dose-response curves to given data and returns the Amax of the fitted curve. Amax is defined as 100 minus the viability at the maximum concentration in the fitted curve. The function returns a numerical value representing Amax."
      },
      {
        "question": "How does the function handle missing or zero concentration values in the input data?",
        "answer": "The function removes NA values from both concentration and viability vectors using `!is.na()`. It also removes entries where the concentration is zero. This is done to ensure that only valid, non-zero concentration data is used for curve fitting."
      },
      {
        "question": "What data preprocessing steps does the function perform before fitting the curve?",
        "answer": "The function performs several preprocessing steps: 1) It converts concentration to log10 scale. 2) It normalizes viability data by dividing by 100. 3) If `trunc=TRUE`, it truncates viability values to be between 0 and 1. 4) It checks for negative concentrations, negative viabilities, and viabilities exceeding 100%, issuing warnings or errors as appropriate."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n\n  # Input validation checks\n  # ...\n\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc == TRUE) {\n    viability[which(viability < 0)] <- 0\n    viability[which(viability > 1)] <- 1\n  }\n\n  # Fit curve and calculate Amax\n  # ...\n\n  return(x)\n}",
        "complete": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  ii <- which(concentration == 0)\n  if(length(ii) > 0) {\n    concentration <- concentration[-ii]\n    viability <- viability[-ii]\n  }\n\n  if (!all(is.finite(concentration)) || !all(is.finite(viability))) {\n    stop(\"Concentration or viability vector contains non-finite elements.\")\n  }\n  if (!is.logical(trunc)) stop(\"'trunc' must be logical.\")\n  if (length(concentration) != length(viability)) stop(\"Concentration and viability vectors must have the same length.\")\n  if (min(concentration) < 0) stop(\"Concentration vector contains negative data.\")\n  if (verbose) {\n    if (min(viability) < 0) warning(\"Negative viability data.\")\n    if (max(viability) > 100) warning(\"Viability data exceeds negative control.\")\n  }\n\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc) {\n    viability <- pmax(pmin(viability, 1), 0)\n  }\n\n  pars <- unlist(logLogisticRegression(log_conc, viability, conc_as_log = TRUE, viability_as_pct = FALSE, trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n}"
      },
      {
        "partial": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  # Data preprocessing\n  # ...\n\n  # Input validation\n  # ...\n\n  # Data transformation\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc) {\n    # Truncate viability\n    # ...\n  }\n\n  # Curve fitting and Amax calculation\n  pars <- unlist(logLogisticRegression(log_conc, viability, conc_as_log = TRUE, viability_as_pct = FALSE, trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n}",
        "complete": "computeAmax <- function(concentration, viability, trunc = TRUE, verbose=FALSE) {\n  concentration <- as.numeric(concentration[!is.na(concentration)])\n  viability <- as.numeric(viability[!is.na(viability)])\n  concentration <- concentration[concentration != 0]\n  viability <- viability[concentration != 0]\n\n  if (!all(is.finite(c(concentration, viability)))) stop(\"Non-finite elements in concentration or viability.\")\n  if (!is.logical(trunc)) stop(\"'trunc' must be logical.\")\n  if (length(concentration) != length(viability)) stop(\"Mismatched vector lengths.\")\n  if (min(concentration) < 0) stop(\"Negative concentration data.\")\n  if (verbose) {\n    if (min(viability) < 0) warning(\"Negative viability data.\")\n    if (max(viability) > 100) warning(\"Viability exceeds 100%.\")\n  }\n\n  log_conc <- log10(concentration)\n  viability <- viability / 100\n\n  if (trunc) {\n    viability <- pmax(pmin(viability, 1), 0)\n  }\n\n  pars <- unlist(logLogisticRegression(log_conc, viability, conc_as_log = TRUE, viability_as_pct = FALSE, trunc = trunc))\n  x <- 100 - .Hill(max(log_conc), pars) * 100\n  names(x) <- \"Amax\"\n  return(x)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/unichem.R",
    "language": "R",
    "content": "\n# Unichem API documentation: https://www.ebi.ac.uk/unichem/info/webservices\n\n#' Get the list of sources in UniChem.\n#' \n#' @param all_columns `boolean` Whether to return all columns. Defaults to FALSE.\n#' \n\n#' \n#' Returns a `data.table` with the following columns:\n#' - `CompoundCount` (integer): Total of compounds provided by that source\n#' - `BaseURL` (string): Source Base URL for compounds\n#' - `Description` (string): Source database description\n#' - `LastUpdated` (string): Date in which the source database was last updated\n#' - `Name` (string): Short name of the source database\n#' - `NameLabel` (string): Machine readable label name of the source database\n#' - `NameLong` (string): Full name of the source database\n#' - `SourceID` (integer): Unique ID for the source database\n#' - `Details` (string): Notes about the source\n#' - `ReleaseDate` (string): Date in which the source database was released\n#' - `ReleaseNumber` (integer): Release number of the source database data stored in UniChEM\n#' - `URL` (string): Main URL for the source\n#' - `UpdateComments` (string): Notes about the update process of that source to UniChEM\n#' \n#' \n#' @return A data.table with the list of sources in UniChem.\n#' \n#' @export\ngetUnichemSources <- function(all_columns = FALSE) {\n    funContext <- .funContext(\"AnnotationGx::getUnichemSources\")\n\n    response <- .build_unichem_query(\"sources\") |>\n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() \n    \n    if(response$response != \"Success\"){\n        .err(funContext, \"Unichem API request failed.\")\n    }\n\n    .debug(funContext, sprintf(\"Unichem sourceCount: %s\", response$totalSources))\n\n    sources_dt <- .asDT(response$sources)\n\n    old_names <- c(\n        \"UCICount\", \"baseIdUrl\", \"description\", \"lastUpdated\", \"name\", \n        \"nameLabel\", \"nameLong\", \"sourceID\", \"srcDetails\", \"srcReleaseDate\", \n        \"srcReleaseNumber\", \"srcUrl\", \"updateComments\")\n\n    new_names <- c(\n        \"CompoundCount\", \"BaseURL\", \"Description\", \"LastUpdated\", \"Name\", \n        \"NameLabel\", \"NameLong\", \"SourceID\", \"Details\", \"ReleaseDate\",\n        \"ReleaseNumber\", \"URL\", \"UpdateComments\")\n    \n    data.table::setnames(sources_dt, old_names, new_names)\n\n    new_order <- c(\n        \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n        \"BaseURL\", \"URL\", \"Details\",\n        \"Description\", \"ReleaseNumber\", \"ReleaseDate\", \"LastUpdated\", \n        \"UpdateComments\"\n    )\n\n\n    sources_dt <- sources_dt[, new_order, with = FALSE]\n\n    if(all_columns) return(sources_dt)\n\n    sources_dt[, c(\"Name\", \"SourceID\")]\n\n}\n\n#' Query UniChem for a compound.\n#' \n#' This function queries the UniChem API for a compound based on the provided parameters.\n#' \n#' @param compound `character` or `integer` The compound identifier to search for.\n#' @param type `character` The type of compound identifier to search for. Valid types are \"uci\", \"inchi\", \"inchikey\", and \"sourceID\".\n#' @param sourceID `integer` The source ID to search for if the type is \"sourceID\". Defaults to NULL.\n#' @param request_only `boolean` Whether to return the request only. Defaults to FALSE.\n#' @param raw `boolean` Whether to return the raw response. Defaults to FALSE.\n#' @param ... Additional arguments.\n#' \n#' @return A list with the external mappings and the UniChem mappings.\n#' \n#' @examples\n#' queryUnichemCompound(type = \"sourceID\", compound = \"444795\", sourceID = 22)\n#' \n#' @export\nqueryUnichemCompound <- function(\n    compound, type, sourceID = NA_integer_, request_only = FALSE, raw = FALSE, ...\n){\n    checkmate::assert_string(type)\n    checkmate::assert_atomic(compound)\n    checkmate::assert_integerish(sourceID)\n    checkmate::assertLogical(request_only)\n    checkmate::assertLogical(raw)\n\n    request <- .build_unichem_compound_req(type, compound, sourceID,...)\n    if(request_only) return(request)\n\n    response <- request |> \n        .perform_request() |>  \n        .parse_resp_json() \n    \n    if(raw) return(response)\n\n    if(response$response != \"Success\"){\n        msg <- paste(\n            \"Unichem API request failed for compound\", compound, \"\n            with type\", type, \n            \" . Error:\", response$error\n        )\n        .err(.funContext(\"AnnotationGx::queryUnichemCompound\"), msg)\n    }\n\n    # Mapping names to be consistent with other API calls\n    mapped_sources_dt <- .asDT(response$compounds$sources)\n    old_names <- c(\"compoundId\", \"shortName\", \"longName\", \"id\", \"url\")\n\n    new_names <- c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n    data.table::setnames(mapped_sources_dt, old = old_names, new = new_names)\n\n    External_Mappings <- mapped_sources_dt[, new_names, with = FALSE]\n    \n    UniChem_Mappings <- list(\n        UniChem.UCI = response$compounds$uci,\n        UniChem.InchiKey = response$compounds$standardInchiKey,\n        UniChem.Inchi = response$compounds$inchi$inchi,\n        UniChem.formula = response$compounds$inchi$formula,\n        UniChem.connections = response$compounds$inchi$connections,\n        UniChem.hAtoms = response$compounds$inchi$hAtoms\n    )\n\n    list(\n        External_Mappings = External_Mappings,\n        UniChem_Mappings = UniChem_Mappings\n    )\n\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getUnichemSources` function and what are its key parameters?",
        "answer": "The `getUnichemSources` function retrieves a list of sources from the UniChem database. It has one key parameter: `all_columns` (boolean), which determines whether to return all columns (when TRUE) or just the 'Name' and 'SourceID' columns (when FALSE, which is the default)."
      },
      {
        "question": "In the `queryUnichemCompound` function, what are the valid types for the `type` parameter, and how does the function handle different types of compound identifiers?",
        "answer": "The valid types for the `type` parameter in `queryUnichemCompound` are 'uci', 'inchi', 'inchikey', and 'sourceID'. The function builds a request based on the specified type and compound identifier. If the type is 'sourceID', an additional `sourceID` parameter is required. The function then performs the request and parses the JSON response, returning a list with External_Mappings and UniChem_Mappings."
      },
      {
        "question": "How does the `getUnichemSources` function handle column renaming and reordering, and why might this be important?",
        "answer": "The `getUnichemSources` function renames columns using `data.table::setnames()` to provide more descriptive and consistent names (e.g., 'UCICount' to 'CompoundCount'). It then reorders the columns using a predefined `new_order` vector. This renaming and reordering is important for improving readability, maintaining consistency across the API, and ensuring that the most relevant information (like 'Name' and 'SourceID') appears first in the output."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getUnichemSources <- function(all_columns = FALSE) {\n    funContext <- .funContext(\"AnnotationGx::getUnichemSources\")\n\n    response <- .build_unichem_query(\"sources\") |>\n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() \n    \n    if(response$response != \"Success\"){\n        .err(funContext, \"Unichem API request failed.\")\n    }\n\n    .debug(funContext, sprintf(\"Unichem sourceCount: %s\", response$totalSources))\n\n    sources_dt <- .asDT(response$sources)\n\n    old_names <- c(\n        \"UCICount\", \"baseIdUrl\", \"description\", \"lastUpdated\", \"name\", \n        \"nameLabel\", \"nameLong\", \"sourceID\", \"srcDetails\", \"srcReleaseDate\", \n        \"srcReleaseNumber\", \"srcUrl\", \"updateComments\")\n\n    new_names <- c(\n        \"CompoundCount\", \"BaseURL\", \"Description\", \"LastUpdated\", \"Name\", \n        \"NameLabel\", \"NameLong\", \"SourceID\", \"Details\", \"ReleaseDate\",\n        \"ReleaseNumber\", \"URL\", \"UpdateComments\")\n    \n    data.table::setnames(sources_dt, old_names, new_names)\n\n    new_order <- c(\n        \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n        \"BaseURL\", \"URL\", \"Details\",\n        \"Description\", \"ReleaseNumber\", \"ReleaseDate\", \"LastUpdated\", \n        \"UpdateComments\"\n    )\n\n\n    sources_dt <- sources_dt[, new_order, with = FALSE]\n\n    if(all_columns) return(sources_dt)\n\n    # Complete the function by returning the appropriate columns\n}",
        "complete": "getUnichemSources <- function(all_columns = FALSE) {\n    funContext <- .funContext(\"AnnotationGx::getUnichemSources\")\n\n    response <- .build_unichem_query(\"sources\") |>\n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() \n    \n    if(response$response != \"Success\"){\n        .err(funContext, \"Unichem API request failed.\")\n    }\n\n    .debug(funContext, sprintf(\"Unichem sourceCount: %s\", response$totalSources))\n\n    sources_dt <- .asDT(response$sources)\n\n    old_names <- c(\n        \"UCICount\", \"baseIdUrl\", \"description\", \"lastUpdated\", \"name\", \n        \"nameLabel\", \"nameLong\", \"sourceID\", \"srcDetails\", \"srcReleaseDate\", \n        \"srcReleaseNumber\", \"srcUrl\", \"updateComments\")\n\n    new_names <- c(\n        \"CompoundCount\", \"BaseURL\", \"Description\", \"LastUpdated\", \"Name\", \n        \"NameLabel\", \"NameLong\", \"SourceID\", \"Details\", \"ReleaseDate\",\n        \"ReleaseNumber\", \"URL\", \"UpdateComments\")\n    \n    data.table::setnames(sources_dt, old_names, new_names)\n\n    new_order <- c(\n        \"Name\", \"NameLabel\", \"NameLong\", \"SourceID\", \"CompoundCount\", \n        \"BaseURL\", \"URL\", \"Details\",\n        \"Description\", \"ReleaseNumber\", \"ReleaseDate\", \"LastUpdated\", \n        \"UpdateComments\"\n    )\n\n\n    sources_dt <- sources_dt[, new_order, with = FALSE]\n\n    if(all_columns) return(sources_dt)\n\n    sources_dt[, c(\"Name\", \"SourceID\")]\n}"
      },
      {
        "partial": "queryUnichemCompound <- function(\n    compound, type, sourceID = NA_integer_, request_only = FALSE, raw = FALSE, ...\n){\n    checkmate::assert_string(type)\n    checkmate::assert_atomic(compound)\n    checkmate::assert_integerish(sourceID)\n    checkmate::assertLogical(request_only)\n    checkmate::assertLogical(raw)\n\n    request <- .build_unichem_compound_req(type, compound, sourceID,...)\n    if(request_only) return(request)\n\n    response <- request |> \n        .perform_request() |>  \n        .parse_resp_json() \n    \n    if(raw) return(response)\n\n    if(response$response != \"Success\"){\n        msg <- paste(\n            \"Unichem API request failed for compound\", compound, \"\n            with type\", type, \n            \" . Error:\", response$error\n        )\n        .err(.funContext(\"AnnotationGx::queryUnichemCompound\"), msg)\n    }\n\n    # Mapping names to be consistent with other API calls\n    mapped_sources_dt <- .asDT(response$compounds$sources)\n    old_names <- c(\"compoundId\", \"shortName\", \"longName\", \"id\", \"url\")\n\n    new_names <- c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n    data.table::setnames(mapped_sources_dt, old = old_names, new = new_names)\n\n    External_Mappings <- mapped_sources_dt[, new_names, with = FALSE]\n    \n    # Complete the function by creating the UniChem_Mappings list and returning the result\n}",
        "complete": "queryUnichemCompound <- function(\n    compound, type, sourceID = NA_integer_, request_only = FALSE, raw = FALSE, ...\n){\n    checkmate::assert_string(type)\n    checkmate::assert_atomic(compound)\n    checkmate::assert_integerish(sourceID)\n    checkmate::assertLogical(request_only)\n    checkmate::assertLogical(raw)\n\n    request <- .build_unichem_compound_req(type, compound, sourceID,...)\n    if(request_only) return(request)\n\n    response <- request |> \n        .perform_request() |>  \n        .parse_resp_json() \n    \n    if(raw) return(response)\n\n    if(response$response != \"Success\"){\n        msg <- paste(\n            \"Unichem API request failed for compound\", compound, \"\n            with type\", type, \n            \" . Error:\", response$error\n        )\n        .err(.funContext(\"AnnotationGx::queryUnichemCompound\"), msg)\n    }\n\n    # Mapping names to be consistent with other API calls\n    mapped_sources_dt <- .asDT(response$compounds$sources)\n    old_names <- c(\"compoundId\", \"shortName\", \"longName\", \"id\", \"url\")\n\n    new_names <- c(\"compoundID\", \"Name\", \"NameLong\", \"sourceID\", \"sourceURL\")\n    data.table::setnames(mapped_sources_dt, old = old_names, new = new_names)\n\n    External_Mappings <- mapped_sources_dt[, new_names, with = FALSE]\n    \n    UniChem_Mappings <- list(\n        UniChem.UCI = response$compounds$uci,\n        UniChem.InchiKey = response$compounds$standardInchiKey,\n        UniChem.Inchi = response$compounds$inchi$inchi,\n        UniChem.formula = response$compounds$inchi$formula,\n        UniChem.connections = response$compounds$inchi$connections,\n        UniChem.hAtoms = response$compounds$inchi$hAtoms\n    )\n\n    list(\n        External_Mappings = External_Mappings,\n        UniChem_Mappings = UniChem_Mappings\n    )\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeABC.R",
    "language": "R",
    "content": "#' Fits dose-response curves to data given by the user\n#' and returns the ABC of the fitted curves.\n#'\n#' @examples\n#' dose1 <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability1 <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' dose2 <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8)\n#' viability2 <- c(100.94,112.5,86,104.16,75,68,48,29)\n#' computeABC(dose1, dose2, viability1, viability2)\n#'\n#' @param conc1 `numeric` is a vector of drug concentrations.\n#' @param conc2 `numeric` is a vector of drug concentrations.\n#' @param viability1 `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc1, expressed as percentages\n#' of viability in the absence of any drug.\n#' @param viability2 `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc2, expressed as percentages\n#' of viability in the absence of any drug.\n#' @param Hill_fit1 `list` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param Hill_fit2 `lis` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope\n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal,\n#' and EC50 as a concentration.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and returns ABC as a decimal. Otherwise, viability is interpreted as percent, and AUC is returned 0-100.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#'\n#' @author Mark Freeman\n#'\n#' @return The numeric area of the absolute difference between the two hill slopes\n#'\n#' @importFrom CoreGx .getSupportVec\n#' @export\ncomputeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n\nif (missing(conc1) | missing(conc2)){\n\n    stop(\"Both Concentration vectors the drugs were tested on must always be provided.\")\n\n}\nif (missing(Hill_fit1) | missing(Hill_fit2)) {\n\n    Hill_fit1 <- logLogisticRegression(conc1,\n      viability1,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=conc1,\n      Hill_fit=Hill_fit1,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars1 <- cleanData[[\"Hill_fit\"]]\n    log_conc1 <- cleanData[[\"log_conc\"]]\n    Hill_fit2 <- logLogisticRegression(conc2,\n      viability2,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=conc2,\n      Hill_fit=Hill_fit2,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars2 <- cleanData[[\"Hill_fit\"]]\n    log_conc2 <- cleanData[[\"log_conc\"]]\n\n} else {\n\n  cleanData <- sanitizeInput(conc = conc1,\n    viability = viability1,\n    Hill_fit = Hill_fit1,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  pars1 <- cleanData[[\"Hill_fit\"]]\n  log_conc1 <- cleanData[[\"log_conc\"]]\n  cleanData <- sanitizeInput(conc = conc2,\n    viability = viability2,\n    Hill_fit = Hill_fit2,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  pars2 <- cleanData[[\"Hill_fit\"]]\n  log_conc2 <- cleanData[[\"log_conc\"]]\n}\n\n  #FIT CURVE AND CALCULATE IC50\n  if (max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)) {\n    return(NA)\n  } else {\n    extrema <- sort(c(min(log_conc1), max(log_conc1), min(log_conc2), max(log_conc2)))\n    support <- .getSupportVec(c(extrema[2], extrema[3]))\n    ABC <- as.numeric(caTools::trapz(support, abs(.Hill(support, pars1) - .Hill(support, pars2))) / (extrema[3] - extrema[2]))\n    if(viability_as_pct){\n      ABC <- ABC*100\n    }\n    return(ABC)\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeABC` function and what does it return?",
        "answer": "The `computeABC` function fits dose-response curves to given data and returns the Area Between Curves (ABC) of the fitted curves. It calculates the numeric area of the absolute difference between two Hill slopes, which represents the difference in drug response between two conditions or treatments."
      },
      {
        "question": "How does the function handle missing `Hill_fit` parameters?",
        "answer": "If `Hill_fit1` or `Hill_fit2` are missing, the function automatically calculates them using the `logLogisticRegression` function. It then sanitizes the input data using the `sanitizeInput` function before proceeding with the ABC calculation. This allows the function to work with either pre-calculated Hill fits or raw concentration and viability data."
      },
      {
        "question": "What condition causes the function to return NA, and why is this check important?",
        "answer": "The function returns NA if the maximum concentration of one dataset is less than the minimum concentration of the other dataset (i.e., `max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)`). This check is important because it ensures that there is an overlapping concentration range between the two datasets, which is necessary for a meaningful ABC calculation. Without this overlap, the comparison would not be valid."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n\n  if (missing(conc1) | missing(conc2)){\n    stop(\"Both Concentration vectors the drugs were tested on must always be provided.\")\n  }\n  if (missing(Hill_fit1) | missing(Hill_fit2)) {\n    # Code to handle missing Hill fits\n  } else {\n    # Code to handle provided Hill fits\n  }\n\n  # Fit curve and calculate IC50\n  if (max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)) {\n    return(NA)\n  } else {\n    # Code to calculate ABC\n  }\n}",
        "complete": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n\n  if (missing(conc1) | missing(conc2)){\n    stop(\"Both Concentration vectors the drugs were tested on must always be provided.\")\n  }\n  if (missing(Hill_fit1) | missing(Hill_fit2)) {\n    Hill_fit1 <- logLogisticRegression(conc1, viability1, conc_as_log, viability_as_pct, trunc, verbose)\n    cleanData <- sanitizeInput(conc1, Hill_fit=Hill_fit1, conc_as_log, viability_as_pct, trunc, verbose)\n    pars1 <- cleanData[\"Hill_fit\"]\n    log_conc1 <- cleanData[\"log_conc\"]\n    Hill_fit2 <- logLogisticRegression(conc2, viability2, conc_as_log, viability_as_pct, trunc, verbose)\n    cleanData <- sanitizeInput(conc2, Hill_fit=Hill_fit2, conc_as_log, viability_as_pct, trunc, verbose)\n    pars2 <- cleanData[\"Hill_fit\"]\n    log_conc2 <- cleanData[\"log_conc\"]\n  } else {\n    cleanData <- sanitizeInput(conc1, viability1, Hill_fit1, conc_as_log, viability_as_pct, trunc, verbose)\n    pars1 <- cleanData[\"Hill_fit\"]\n    log_conc1 <- cleanData[\"log_conc\"]\n    cleanData <- sanitizeInput(conc2, viability2, Hill_fit2, conc_as_log, viability_as_pct, trunc, verbose)\n    pars2 <- cleanData[\"Hill_fit\"]\n    log_conc2 <- cleanData[\"log_conc\"]\n  }\n\n  if (max(log_conc1) < min(log_conc2) | max(log_conc2) < min(log_conc1)) {\n    return(NA)\n  } else {\n    extrema <- sort(c(min(log_conc1), max(log_conc1), min(log_conc2), max(log_conc2)))\n    support <- .getSupportVec(c(extrema[2], extrema[3]))\n    ABC <- as.numeric(caTools::trapz(support, abs(.Hill(support, pars1) - .Hill(support, pars2))) / (extrema[3] - extrema[2]))\n    if(viability_as_pct) ABC <- ABC * 100\n    return(ABC)\n  }\n}"
      },
      {
        "partial": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n  # Check if concentration vectors are provided\n  \n  # Handle missing or provided Hill fits\n  \n  # Calculate ABC\n  \n  # Return result\n}",
        "complete": "computeABC <- function(conc1, conc2, viability1, viability2,\n                        Hill_fit1,\n                        Hill_fit2,\n                        conc_as_log = FALSE,\n                        viability_as_pct = TRUE,\n                        trunc = TRUE,\n                        verbose=TRUE) {\n  if (missing(conc1) | missing(conc2)) stop(\"Both Concentration vectors must be provided.\")\n  \n  process_data <- function(conc, viability, Hill_fit) {\n    if (missing(Hill_fit)) {\n      Hill_fit <- logLogisticRegression(conc, viability, conc_as_log, viability_as_pct, trunc, verbose)\n    }\n    cleanData <- sanitizeInput(conc, viability, Hill_fit, conc_as_log, viability_as_pct, trunc, verbose)\n    list(pars = cleanData[\"Hill_fit\"], log_conc = cleanData[\"log_conc\"])\n  }\n  \n  data1 <- process_data(conc1, viability1, Hill_fit1)\n  data2 <- process_data(conc2, viability2, Hill_fit2)\n  \n  if (max(data1$log_conc) < min(data2$log_conc) | max(data2$log_conc) < min(data1$log_conc)) return(NA)\n  \n  extrema <- sort(c(range(data1$log_conc), range(data2$log_conc)))\n  support <- .getSupportVec(extrema[2:3])\n  ABC <- as.numeric(caTools::trapz(support, abs(.Hill(support, data1$pars) - .Hill(support, data2$pars))) / diff(extrema[2:3]))\n  \n  if(viability_as_pct) ABC <- ABC * 100\n  ABC\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-split.R",
    "language": "R",
    "content": "\n# The following functions are taken from the AcidBase package by acidgenomics using their\n# license. Adding the package as a dependency is the better approach but fails on the \n# CI/CD pipeline as the package is not available on CRAN.\n# TODO:: Add the package as a dependency and remove the following functions.\n# TODO:: reach out to the author to discuss the license and the possibility of \n#        adding the package as a dependency.\n\n#' Split a character vector into a matrix based on a delimiter\n#'\n#' This function splits a character vector into a matrix based on a specified delimiter.\n#' It can handle both finite and infinite splits.\n#'\n#' @param x A character vector to be split\n#' @param split A character string specifying the delimiter\n#' @param fixed A logical value indicating whether the delimiter should be treated as a fixed string\n#' @param n An integer specifying the maximum number of splits to be performed\n#'\n#' @return A matrix where each row represents a split element\n#'\n#' @examples\n#' strSplit(\"Hello,World\", \",\")\n#' # Output:\n#' #      [,1]    [,2]   \n#' # [1,] \"Hello\" \"World\"\n#'\n#' @export\nstrSplit <- function(x, split, fixed = TRUE, n = Inf) {\n\n  if (is.finite(n)) {\n    x <- .strSplitFinite(x = x, split = split, n = n, fixed = fixed)\n  } else {\n    x <- .strSplitInfinite(x = x, split = split, fixed = fixed)\n  }\n  n2 <- lengths(x)\n  assert(\n    length(unique(n2)) == 1L,\n    msg = sprintf(\n      \"Split mismatch detected: %s.\",\n      toString(which(n2 != n2[[1L]]))\n    )\n  )\n  n2 <- n2[[1L]]\n  x <- unlist(x = x, recursive = FALSE, use.names = FALSE)\n  x <- matrix(data = x, ncol = n2, byrow = TRUE)\n  x\n}\n\n\n#' Split a string into multiple substrings based on a delimiter\n#'\n#' This function splits a given string into multiple substrings based on a specified delimiter.\n#' The number of resulting substrings can be controlled using the 'n' parameter.\n#'\n#' @param x The input string to be split.\n#' @param split The delimiter used to split the string.\n#' @param n The maximum number of substrings to be generated.\n#' @param fixed A logical value indicating whether the 'split' parameter should be treated as a fixed string or a regular expression.\n#'\n#' @return A character vector containing the resulting substrings.\n#'\n#' @examples\n#' str <- \"Hello,World,How,Are,You\"\n#' .strSplitFinite(str, \",\", 3, fixed = TRUE)\n#'\n#' @noRd\n#' @keywords internal\n.strSplitFinite <- function(x, split, n, fixed) {\n\n    checkmate::assertString(split)\n    checkmate::assertFlag(fixed)\n    checkmate::assert_integerish(n, lower = 2L, upper = Inf)\n    checkmate::assert_character(x)\n\n    m <- gregexpr(pattern = split, text = x, fixed = fixed)\n    ln <- lengths(m)\n    assert(\n        all((ln + 1L) >= n),\n        msg = sprintf(\n            \"Not enough to split: %s.\",\n            toString(which((ln + 1L) < n))\n        )\n    )\n    Map(\n        x = x,\n        m = m,\n        n = n,\n        f = function(x, m, n) {\n            ml <- attr(m, \"match.length\")\n            nl <- seq_len(n)\n            m <- m[nl]\n            ml <- ml[nl]\n            out <- substr(x = x, start = 1L, stop = m[[1L]] - 1L)\n            i <- 1L\n            while (i < (length(m) - 1L)) {\n                out <- append(\n                    x = out,\n                    values = substr(\n                        x = x,\n                        start = m[[i]] + ml[[i]],\n                        stop = m[[i + 1L]] - 1L\n                    )\n                )\n                i <- i + 1L\n            }\n            out <- append(\n                x = out,\n                values = substr(\n                    x = x,\n                    start = m[[n - 1L]] + ml[[n - 1L]],\n                    stop = nchar(x)\n                )\n            )\n            out\n        },\n        USE.NAMES = FALSE\n    )\n}\n\n\n#' Split a character vector into substrings based on a delimiter\n#'\n#' This function splits a character vector into substrings based on a specified delimiter.\n#' It uses the `strsplit` function from the base R package.\n#'\n#' @param x A character vector to be split.\n#' @param split A character string specifying the delimiter to use for splitting.\n#' @param fixed A logical value indicating whether the delimiter should be treated as a fixed string.\n#'              If `TRUE`, the delimiter is treated as a fixed string; if `FALSE`, it is treated as a regular expression.\n#'\n#' @return A list of character vectors, where each element of the list corresponds to the substrings obtained from splitting the input vector.\n#'\n#' @examples\n#' x <- c(\"apple,banana,orange\", \"cat,dog,rabbit\")\n#' .strSplitInfinite(x, \",\", fixed = TRUE)\n#'\n#' @noRd\n#' @keywords internal\n.strSplitInfinite <- function(x, split, fixed) {\n    checkmate::assertCharacter(x)\n    checkmate::assertString(split)\n    checkmate::assertFlag(fixed)\n    strsplit(x = x, split = split, fixed = fixed)\n}\n\n\n\n#' Split a column into a character list\n#'\n#' @note Updated 2023-09-22.\n#' @noRd\n.splitCol <- function(object, colName, split = \"; \") {\n  checkmate::assert_class(object, \"data.table\")\n  object[[colName]] <- strsplit(object[[colName]], split = split, fixed = TRUE)\n  object\n}\n\n#' Split a nested column by key\n#'\n#' Don't format key names into camel case -- too CPU intensive.\n#'\n#' @note Updated 2023-09-22.\n#' @noRd\n.splitNestedCol <- function(object, colName, split) {\n    # assert(\n    #     is(object, \"DFrame\"),\n    #     is(object[[colName]], \"CharacterList\"),\n    #     isString(split)\n    # )\n    lst <- lapply(\n        X = object[[colName]],\n        split = split,\n        FUN = function(x, split) {\n            if (identical(x, character())) {\n                return(list())\n            }\n            x <- strSplit(x = x, split = split, n = 2L)\n            ## Formatting into camel case takes too long.\n            ## > x[, 1L] <- camelCase(x[, 1L])\n            x <- split(x = x[, 2L], f = x[, 1L])\n            x\n        }\n    ) |> unlist(recursive = F)\n    object[[colName]] <- lst\n    object\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `strSplit` function and how does it handle different types of splits?",
        "answer": "The `strSplit` function is designed to split a character vector into a matrix based on a specified delimiter. It handles two types of splits: finite and infinite. For finite splits (when `n` is a finite number), it uses the `.strSplitFinite` helper function. For infinite splits (when `n` is `Inf`), it uses the `.strSplitInfinite` helper function. The function ensures that all splits result in the same number of elements and returns a matrix where each row represents a split element."
      },
      {
        "question": "How does the `.strSplitFinite` function differ from the `.strSplitInfinite` function in terms of implementation and use case?",
        "answer": "The `.strSplitFinite` function is used when a specific number of splits is desired. It uses `gregexpr` to find split positions and then manually extracts substrings using `substr`. This allows for precise control over the number of resulting substrings. The `.strSplitInfinite` function, on the other hand, is simpler and uses the base R `strsplit` function to split the input without a limit on the number of resulting substrings. `.strSplitFinite` is more complex but offers more control, while `.strSplitInfinite` is simpler but less flexible in terms of controlling the number of splits."
      },
      {
        "question": "What is the purpose of the `.splitNestedCol` function and how does it process nested data?",
        "answer": "The `.splitNestedCol` function is designed to split a nested column in a data structure (likely a data.frame or data.table) by a specified key. It processes each element of the nested column by splitting it into key-value pairs using the provided split character. The function then creates a list where each key becomes a list element containing its corresponding value. This is useful for unpacking nested data structures within a column, transforming them into a more easily accessible format. The function notably avoids formatting key names into camel case to save processing time."
      }
    ],
    "completion_tasks": [
      {
        "partial": "strSplit <- function(x, split, fixed = TRUE, n = Inf) {\n  if (is.finite(n)) {\n    x <- .strSplitFinite(x = x, split = split, n = n, fixed = fixed)\n  } else {\n    x <- .strSplitInfinite(x = x, split = split, fixed = fixed)\n  }\n  n2 <- lengths(x)\n  assert(\n    length(unique(n2)) == 1L,\n    msg = sprintf(\n      \"Split mismatch detected: %s.\",\n      toString(which(n2 != n2[[1L]]))\n    )\n  )\n  n2 <- n2[[1L]]\n  x <- unlist(x = x, recursive = FALSE, use.names = FALSE)\n  # Complete the function by adding the final step\n}",
        "complete": "strSplit <- function(x, split, fixed = TRUE, n = Inf) {\n  if (is.finite(n)) {\n    x <- .strSplitFinite(x = x, split = split, n = n, fixed = fixed)\n  } else {\n    x <- .strSplitInfinite(x = x, split = split, fixed = fixed)\n  }\n  n2 <- lengths(x)\n  assert(\n    length(unique(n2)) == 1L,\n    msg = sprintf(\n      \"Split mismatch detected: %s.\",\n      toString(which(n2 != n2[[1L]]))\n    )\n  )\n  n2 <- n2[[1L]]\n  x <- unlist(x = x, recursive = FALSE, use.names = FALSE)\n  matrix(data = x, ncol = n2, byrow = TRUE)\n}"
      },
      {
        "partial": ".strSplitFinite <- function(x, split, n, fixed) {\n  checkmate::assertString(split)\n  checkmate::assertFlag(fixed)\n  checkmate::assert_integerish(n, lower = 2L, upper = Inf)\n  checkmate::assert_character(x)\n\n  m <- gregexpr(pattern = split, text = x, fixed = fixed)\n  ln <- lengths(m)\n  assert(\n    all((ln + 1L) >= n),\n    msg = sprintf(\n      \"Not enough to split: %s.\",\n      toString(which((ln + 1L) < n))\n    )\n  )\n  # Complete the function by adding the Map operation\n}",
        "complete": ".strSplitFinite <- function(x, split, n, fixed) {\n  checkmate::assertString(split)\n  checkmate::assertFlag(fixed)\n  checkmate::assert_integerish(n, lower = 2L, upper = Inf)\n  checkmate::assert_character(x)\n\n  m <- gregexpr(pattern = split, text = x, fixed = fixed)\n  ln <- lengths(m)\n  assert(\n    all((ln + 1L) >= n),\n    msg = sprintf(\n      \"Not enough to split: %s.\",\n      toString(which((ln + 1L) < n))\n    )\n  )\n  Map(\n    x = x,\n    m = m,\n    n = n,\n    f = function(x, m, n) {\n      ml <- attr(m, \"match.length\")\n      nl <- seq_len(n)\n      m <- m[nl]\n      ml <- ml[nl]\n      out <- substr(x = x, start = 1L, stop = m[[1L]] - 1L)\n      i <- 1L\n      while (i < (length(m) - 1L)) {\n        out <- append(\n          x = out,\n          values = substr(\n            x = x,\n            start = m[[i]] + ml[[i]],\n            stop = m[[i + 1L]] - 1L\n          )\n        )\n        i <- i + 1L\n      }\n      out <- append(\n        x = out,\n        values = substr(\n          x = x,\n          start = m[[n - 1L]] + ml[[n - 1L]],\n          stop = nchar(x)\n        )\n      )\n      out\n    },\n    USE.NAMES = FALSE\n  )\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CTRP/CTRP_sampleMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]\n\n\nusethis::use_data(CTRP_sampleMetadata, overwrite = TRUE)\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'system.file()' function in the first line of the code snippet?",
        "answer": "The 'system.file()' function is used to locate and construct the file path for a specific file within an R package. In this case, it's locating the 'CTRPv2_meta_per_cell_line.txt' file in the 'extdata/CTRP' directory of the 'AnnotationGx' package. This function is particularly useful for accessing files that are distributed with a package."
      },
      {
        "question": "How does the code extract specific columns from the 'rawdata' data table?",
        "answer": "The code uses the data.table syntax to extract specific columns from 'rawdata'. The line 'CTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]' selects only the 'master_ccl_id' and 'ccl_name' columns from the 'rawdata' data table and assigns them to a new variable 'CTRP_sampleMetadata'."
      },
      {
        "question": "What is the purpose of the 'usethis::use_data()' function in the last line of the code?",
        "answer": "The 'usethis::use_data()' function is used to save R objects (in this case, 'CTRP_sampleMetadata') to an .rda file in the 'data/' directory of a package. The 'overwrite = TRUE' argument allows the function to overwrite an existing file with the same name. This is typically used when creating or updating datasets that will be included in an R package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]\n\n# Complete the code to save CTRP_sampleMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]\n\nusethis::use_data(CTRP_sampleMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the file and extract specific columns\n\nfilePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\n# Read the file and extract columns",
        "complete": "filePath <- system.file(\"extdata/CTRP\", \"CTRPv2_meta_per_cell_line.txt\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names = T)\n\nCTRP_sampleMetadata <- rawdata[, c(\"master_ccl_id\", \"ccl_name\")]"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/io/common.py",
    "language": "py",
    "content": "import os\nimport pathlib\nfrom typing import Dict\n\nfrom pydicom.misc import is_dicom\n\n\ndef find_dicom_paths(root_path: str, yield_directories: bool = False) -> str:\n    \"\"\"Find DICOM file paths in the specified root directory file tree.\n\n    Parameters\n    ----------\n    root_path\n        Path to the root directory specifying the file hierarchy.\n\n    yield_directories, optional\n        Whether to yield paths to directories containing DICOM files\n        or separately to each file (default).\n\n\n    Yields\n    ------\n    The paths to DICOM files or DICOM-containing directories (if\n    `yield_directories` is True).\n\n    \"\"\"\n    # TODO add some filtering options\n    for root, _, files in os.walk(root_path):\n        if yield_directories:\n            if any((is_dicom(pathlib.Path(root, f).as_posix()) for f in files)):\n                yield root\n        else:\n            for f in files:\n                fpath = pathlib.Path(root, f).as_posix()\n                if is_dicom(fpath):\n                    yield fpath\n\n\ndef file_name_convention() -> Dict:\n    \"\"\"\n    This function returns the file name taxonomy which is used by ImageAutoOutput and Dataset class\n    \"\"\"\n    file_name_convention = {\"CT\": \"image\",\n                            \"MR\": \"image\",\n                            \"RTDOSE_CT\": \"dose\", \n                            \"RTSTRUCT_CT\": \"mask_ct\", \n                            \"RTSTRUCT_MR\": \"mask_mr\", \n                            \"RTSTRUCT_PT\": \"mask_pt\", \n                            \"PT_CT\": \"pet\", \n                            \"PT\": \"pet\", \n                            \"RTDOSE\": \"dose\", \n                            \"RTSTRUCT\": \"mask\"}\n                            \n    return file_name_convention\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `find_dicom_paths` function and how does it handle the `yield_directories` parameter?",
        "answer": "The `find_dicom_paths` function is designed to find DICOM file paths in a specified root directory file tree. It uses the `os.walk` function to traverse the directory structure. The `yield_directories` parameter determines the function's behavior: if set to True, it yields paths to directories containing DICOM files; if False (default), it yields paths to individual DICOM files. The function uses the `is_dicom` function from the pydicom library to check if a file is a DICOM file."
      },
      {
        "question": "How does the `file_name_convention` function contribute to the overall functionality of the code, and what type of data does it return?",
        "answer": "The `file_name_convention` function returns a dictionary that defines a naming convention for different types of medical imaging files. It maps modality-specific keys (e.g., 'CT', 'MR', 'RTDOSE_CT') to standardized file type names (e.g., 'image', 'dose', 'mask_ct'). This function is likely used by other parts of the system (such as ImageAutoOutput and Dataset classes) to maintain consistent file naming across different imaging modalities. The function returns a Dict type, which is imported from the typing module."
      },
      {
        "question": "What potential improvement is suggested in the `find_dicom_paths` function, and how might this enhance its functionality?",
        "answer": "The `find_dicom_paths` function includes a TODO comment suggesting the addition of filtering options. This improvement could enhance the function's flexibility by allowing users to specify criteria for including or excluding certain DICOM files or directories. Possible filtering options might include file size, date modified, or specific DICOM tags. Implementing these filters would make the function more versatile and efficient, especially when dealing with large datasets or when only specific types of DICOM files are needed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def find_dicom_paths(root_path: str, yield_directories: bool = False) -> str:\n    for root, _, files in os.walk(root_path):\n        if yield_directories:\n            if any((is_dicom(pathlib.Path(root, f).as_posix()) for f in files)):\n                yield root\n        else:\n            # Complete the code to yield DICOM file paths",
        "complete": "def find_dicom_paths(root_path: str, yield_directories: bool = False) -> str:\n    for root, _, files in os.walk(root_path):\n        if yield_directories:\n            if any((is_dicom(pathlib.Path(root, f).as_posix()) for f in files)):\n                yield root\n        else:\n            for f in files:\n                fpath = pathlib.Path(root, f).as_posix()\n                if is_dicom(fpath):\n                    yield fpath"
      },
      {
        "partial": "def file_name_convention() -> Dict:\n    file_name_convention = {\n        \"CT\": \"image\",\n        \"MR\": \"image\",\n        # Complete the dictionary with the remaining key-value pairs\n    }\n    return file_name_convention",
        "complete": "def file_name_convention() -> Dict:\n    file_name_convention = {\n        \"CT\": \"image\",\n        \"MR\": \"image\",\n        \"RTDOSE_CT\": \"dose\",\n        \"RTSTRUCT_CT\": \"mask_ct\",\n        \"RTSTRUCT_MR\": \"mask_mr\",\n        \"RTSTRUCT_PT\": \"mask_pt\",\n        \"PT_CT\": \"pet\",\n        \"PT\": \"pet\",\n        \"RTDOSE\": \"dose\",\n        \"RTSTRUCT\": \"mask\"\n    }\n    return file_name_convention"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib"
      ],
      "from_imports": [
        "typing.Dict",
        "pydicom.misc.is_dicom"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_oncotree.R",
    "language": "R",
    "content": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `expect_data_table()` function in these test cases, and what parameters are being used to validate the results?",
        "answer": "The `expect_data_table()` function is used to validate that the result of each AnnotationGx function call is a data table with specific characteristics. It checks the number of columns (ncols), minimum number of rows (min.rows), ensures there are no missing values (all.missing = FALSE), and in some cases, verifies that column names are present (col.names = 'named'). This ensures that the returned data structures meet the expected format and content requirements."
      },
      {
        "question": "How do the test cases for `getOncotreeVersions()`, `getOncotreeMainTypes()`, and `getOncotreeTumorTypes()` differ in terms of their expected output structure?",
        "answer": "The test cases differ in their expected output structure as follows:\n1. `getOncotreeVersions()`: Expects 4 columns and at least 25 rows.\n2. `getOncotreeMainTypes()`: Expects 1 column, at least 100 rows, and named columns.\n3. `getOncotreeTumorTypes()`: Expects 12 columns, at least 800 rows, and named columns.\nThese differences reflect the varying complexity and amount of data returned by each function in the AnnotationGx package."
      },
      {
        "question": "What libraries are being used in this test suite, and what is their purpose in the context of these tests?",
        "answer": "The test suite uses three libraries:\n1. `testthat`: A popular R package for unit testing, providing functions like `test_that()` for organizing tests.\n2. `AnnotationGx`: The package being tested, which contains functions for retrieving oncology-related data.\n3. `checkmate`: Although not explicitly used in the shown code, it's likely used for additional assertion and validation functions within the tests.\nThese libraries work together to create a robust testing environment for the AnnotationGx package's data retrieval functions."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  # Complete the expect_data_table function call\n})",
        "complete": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})"
      },
      {
        "partial": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  # Complete the expect_data_table function call\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})",
        "complete": "library(testthat)\nlibrary(AnnotationGx)\nlibrary(checkmate)\n\ntest_that(\"Returns data table for versions\", {\n  result <- AnnotationGx::getOncotreeVersions()\n  expect_data_table(\n    result,\n    ncols = 4,\n    min.rows = 25,\n    all.missing = FALSE,\n  )\n})\n\ntest_that(\"Returns data table for main types\", {\n  result <- AnnotationGx::getOncotreeMainTypes()\n  expect_data_table(\n    result,\n    ncols = 1,\n    min.rows = 100,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})\n\ntest_that(\"Returns data table for tumor types\", {\n  result <- AnnotationGx::getOncotreeTumorTypes()\n  expect_data_table(\n    result,\n    ncols = 12,\n    min.rows = 800,\n    all.missing = FALSE,\n    col.names = 'named'\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_cellosaurus.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\n\n\ntest_that(\"mapCell2Accession works as expected\", {\n  # Test case 1: Test with a valid cell line name\n  cell_line1 <- \"Hela\"\n  expected1 <- \"CVCL_0030\"\n  result1 <- mapCell2Accession(cell_line1)\n  expect_data_table(result1)\n  expect_equal(result1$accession, expected1)\n\n})\n\ntest_that(\"mapCell2Accession prioritizePatient works as expected\", {\n  cell_line <- \"BT474\"\n\n  result1 <- mapCell2Accession(\n    cell_line)\n\n  expect_data_table(result1, nrows = 1, ncols = 3)\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n  expect_equal(result1$accession, \"CVCL_0179\")\n  expect_equal(result1$cellLineName, \"BT-474\")\n\n  expect_error(mapCell2Accession(\"BT474\", numResults = -1, from = \"idsy\"))\n})\n\ntest_that(\"mapCell2Accession fuzzy search works as expected\", {\n  cell_line <- \"BT474\"\n\n  result1 <- mapCell2Accession(\n    cell_line, fuzzy = TRUE)\n\n  expect_data_table(result1, nrows = 1, ncols = 3)\n  expect_equal(result1$accession, \"CVCL_0179\")\n  expect_equal(result1$cellLineName, \"BT-474\")\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n\n  expect_error(mapCell2Accession(\"BT474\", numResults = -1, from = \"idsy\"))\n})\n\ntest_that(\"mapCell2Accession with multiple cell lines works as expected\", {\n  cell_lines <- c(\"BT474\", \"Hela\")\n\n  result1 <- mapCell2Accession(\n    cell_lines)\n\n  expect_data_table(result1, nrows = 2, ncols = 3)\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n  expect_equal(result1$accession, c(\"CVCL_0179\", \"CVCL_0030\"))\n  expect_equal(result1$cellLineName, c(\"BT-474\", \"HeLa\"))\n\n  expect_error(mapCell2Accession(\"BT474\", numResults = -1, from = \"idsy\"))\n})\n\ntest_that(\"mapCell DOR 13 works\", {\n  name <- \"DOR 13\"\n\n  result1 <- mapCell2Accession(name)\n  result2 <- mapCell2Accession(name, fuzzy = T) \n  result3 <- mapCell2Accession(c(name, \"HT\"))\n  \n  expect_data_table(result1, nrows = 1, ncols = 1) # fails\n  expect_data_table(result2, nrows = 1, ncols = 3) # works\n  expect_data_table(result3, nrows = 2, ncols = 3) # works\n\n\n  expect_equal(result2$accession, \"CVCL_6774\")\n  expect_equal(result2$cellLineName, \"DOV13\")\n\n  expect_equal(result3$accession, c(NA_character_, \"CVCL_1290\"))\n  expect_equal(result3$cellLineName, c(NA_character_, \"HT\"))\n  expect_equal(result3$query, c(\"DOR 13\", \"HT\"))\n})\n\n\ntest_that(\"query only paramater works\",{\n  result1 <- mapCell2Accession(\"DOR 13\", query_only = TRUE)\n  \n  expected <- \"https://api.cellosaurus.org/search/cell-line?q=idsy%3ADOR%2013&sort=ac%20asc&fields=ac%2Cid%2Csy%2Cmisspelling%2Cdr%2Ccc%2Cca%2Cdi%2Cag%2Csx%2Chi&format=txt&rows=10000\"\n  expect_equal(result1[[1]], expected)\n  expect_equal(names(result1), \"DOR 13\")\n})\n\ntest_that(\"raw param works\",{\n  \n  result1 <- mapCell2Accession(\"HT\", raw = TRUE)\n  expect_class(result1[[1]], \"httr2_response\")\n  expect_equal(names(result1), \"HT\")\n\n  resp <- result1$HT\n  lines <- httr2::resp_body_string(resp)  |>\n            strsplit(\"\\n\") |> \n            unlist()\n\n  checkmate::expect_character(lines)\n  expect_true(length(lines) > 2000 & 10000 > length(lines) )\n\n\n  parsed_lines <- \n    Map(\n      f = function(lines, i, j) {\n          lines[i:(j - 1L)]\n      },\n      i = grep(pattern = \"^ID\\\\s+\", x = lines, value = FALSE),\n      j = grep(pattern = \"^//$\", x = lines, value = FALSE),\n      MoreArgs = list(\"lines\" = lines),\n      USE.NAMES = FALSE\n    )\n  x <- parsed_lines[[1]]\n  result <- AnnotationGx:::.processEntry(x)\n\n  expect_data_table(result, min.rows = 1, min.cols = 9)\n  expect_true(\n    all(\n      c(\"cellLineName\", \"accession\", \"comments\", \"synonyms\") %in% colnames(result)\n      )\n    )\n\n})\n\n\ntest_that(\"parsed works\", {\n  ( result1 <- mapCell2Accession(\"22RV1\", parsed = FALSE))$diseases\n   expect_data_table(result1, min.rows = 1, min.cols = 3)\n  expect_true(\n    all(\n      c(\"cellLineName\", \"accession\", \"query\") %in% colnames(result1)\n      )\n    )\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCell2Accession` function based on the test cases provided?",
        "answer": "The `mapCell2Accession` function appears to map cell line names to their corresponding accession numbers in the Cellosaurus database. It can handle single or multiple cell line inputs, perform fuzzy searches, and return various levels of detail about the cell lines."
      },
      {
        "question": "How does the `fuzzy` parameter affect the behavior of `mapCell2Accession`?",
        "answer": "When the `fuzzy` parameter is set to TRUE, the function performs a fuzzy search, which allows for approximate matching of cell line names. This is demonstrated in the test case where 'DOR 13' is successfully matched to 'DOV13' when fuzzy search is enabled, but fails to return a result when fuzzy search is not used."
      },
      {
        "question": "What are the different output formats available for the `mapCell2Accession` function, and how are they controlled?",
        "answer": "The `mapCell2Accession` function offers several output formats controlled by different parameters: 1) Default output is a data table with cellLineName, accession, and query columns. 2) Setting `query_only = TRUE` returns the API query URL. 3) Setting `raw = TRUE` returns the raw HTTP response. 4) Setting `parsed = FALSE` returns a simplified data table with fewer columns. These options allow for flexibility in how the data is returned and processed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"mapCell2Accession works as expected\", {\n  cell_line1 <- \"Hela\"\n  expected1 <- \"CVCL_0030\"\n  result1 <- mapCell2Accession(cell_line1)\n  # Add expectations here\n})",
        "complete": "test_that(\"mapCell2Accession works as expected\", {\n  cell_line1 <- \"Hela\"\n  expected1 <- \"CVCL_0030\"\n  result1 <- mapCell2Accession(cell_line1)\n  expect_data_table(result1)\n  expect_equal(result1$accession, expected1)\n})"
      },
      {
        "partial": "test_that(\"mapCell2Accession with multiple cell lines works as expected\", {\n  cell_lines <- c(\"BT474\", \"Hela\")\n  result1 <- mapCell2Accession(cell_lines)\n  # Add expectations here\n})",
        "complete": "test_that(\"mapCell2Accession with multiple cell lines works as expected\", {\n  cell_lines <- c(\"BT474\", \"Hela\")\n  result1 <- mapCell2Accession(cell_lines)\n  expect_data_table(result1, nrows = 2, ncols = 3)\n  expect_named(result1, c(\"cellLineName\", \"accession\", \"query\"))\n  expect_equal(result1$accession, c(\"CVCL_0179\", \"CVCL_0030\"))\n  expect_equal(result1$cellLineName, c(\"BT-474\", \"HeLa\"))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/sparsemask.py",
    "language": "py",
    "content": "from typing import Dict\nimport numpy as np\n\n\nclass SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        self.roi_name_dict = roi_name_dict\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the SparseMask class in this code snippet?",
        "answer": "The SparseMask class is designed to represent a sparse mask, likely for image processing or region of interest (ROI) operations. It stores a mask array and a dictionary mapping ROI names to integer values, allowing for efficient storage and manipulation of sparse mask data."
      },
      {
        "question": "What are the types of the parameters in the __init__ method of the SparseMask class?",
        "answer": "The __init__ method takes two parameters: mask_array of type np.ndarray (NumPy array), and roi_name_dict of type Dict[str, int] (a dictionary with string keys and integer values)."
      },
      {
        "question": "Why might the typing module be imported in this code snippet?",
        "answer": "The typing module is imported to use type hints, specifically for the Dict type used in the roi_name_dict parameter. This improves code readability and allows for better static type checking, which can help catch potential errors early in development."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        # Complete the initialization",
        "complete": "class SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        self.roi_name_dict = roi_name_dict"
      },
      {
        "partial": "from typing import Dict\nimport numpy as np\n\nclass SparseMask:\n    # Complete the class definition",
        "complete": "from typing import Dict\nimport numpy as np\n\nclass SparseMask:\n    def __init__(self, mask_array:np.ndarray, roi_name_dict: Dict[str, int]):\n        self.mask_array = mask_array\n        self.roi_name_dict = roi_name_dict"
      }
    ],
    "dependencies": {
      "imports": [
        "numpy"
      ],
      "from_imports": [
        "typing.Dict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeGR.R",
    "language": "R",
    "content": "#' @keywords internal\n#' @noRd\ngrRegression <- function(conc,\n                  viability,\n                  duration,\n                  Hill_fit,\n                  dbl_time,\n                  conc_as_log = FALSE,\n                  viability_as_pct = TRUE,\n                  verbose = FALSE,\n                  density = c(2, 10, 2),\n                  step = .5 / density,\n                  precision = 0.05,\n                  lower_bounds = c(0, 0, -6),\n                  upper_bounds = c(4, 1, 6),\n                  scale = 0.07,\n                  family = c(\"normal\", \"Cauchy\"),\n                  scale_01 = FALSE,\n                  trunc=TRUE) { #If true, fits parameters to a transformed GR curve\n  #with image [0, 1]. If false, fits parameters to Sorger's original [-1, 1]-image curve.\n\n  #GRFit takes in dose-response data and a bunch of formatting parameters, then returns\n  #the GHS, GEC_50, and G_inf values associated with them in accordance with the Sorger\n  #paper. However, the definitions are corrected in accordance with my adjustment of the\n  #relevant Sorger equations. While this may change the form of the equations, it does\n  #not affect their intuitive meanings. G_inf is still the GR in the presence of arbitrarily\n  #large drug concentration, GEC_50 is the dose that produces a half-minimal GR-value,\n  #and GHS is the magnitude of the slope of the tangent to the log dose-response curve\n  #when the drug concentration is GEC_50.\n\n  #DO SANITY CHECKS ON INPUT\n  # if(missing(concentration)){\n\n  #   stop(\"The concentration values the drug was tested on must always be provided.\")\n  family <- match.arg(family)\n\n\nif (missing(Hill_fit)) {\n\n    Hill_fit <- logLogisticRegression(conc,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose,\n      density = density,\n      step = step,\n      precision = precision,\n      lower_bounds = lower_bounds,\n      upper_bounds = upper_bounds,\n      scale = scale,\n      family = family\n      )\n    cleanData <- sanitizeInput(conc=conc,\n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    Hill_fit <- cleanData[[\"Hill_fit\"]]\n    log_conc <- cleanData[[\"log_conc\"]]\n} else if (!missing(Hill_fit)){\n\n  cleanData <- sanitizeInput(conc = conc,\n    viability = viability,\n    Hill_fit = Hill_fit,\n    conc_as_log = conc_as_log,\n    viability_as_pct = viability_as_pct,\n    trunc = trunc,\n    verbose = verbose)\n  Hill_fit <- cleanData[[\"Hill_fit\"]]\n  # log_conc <- cleanData[[\"log_conc\"]]\n}\n\n  if (missing(viability) && missing(Hill_fit)) {\n    stop(\"Please enter viability data and/or Hill equation parameters.\")\n  }\n\n  if(missing(duration)){\n    stop(\"Cannot calculate GR without duration of experiment\")\n  }\n  if(missing(dbl_time)){\n    stop(\"Cannot calculate GR without cell doubling time\")\n  }\n\n  tau <- duration / dbl_time\n\n  #CALCULATE GR STATISTICS\n\n  Ginf <- (Hill_fit[2]) ^ (1 / tau)\n  GEC50 <- 10 ^ Hill_fit[3] * ((2 ^ tau - (1 + Ginf) ^ tau) / ((1 + Ginf) ^ tau - (2 * Ginf) ^ tau)) ^ (1 / Hill_fit[1])\n  GHS <- (1 - Hill_fit[2]) / tau *\n    (.Hill(log10(GEC50), Hill_fit)) ^ (1 / tau - 1) *\n    1 / (1 + (GEC50 / 10 ^ Hill_fit[3]) ^ Hill_fit[1]) ^ 2 * Hill_fit[1] / 10 ^ Hill_fit[3] *\n    (GEC50 / 10 ^ Hill_fit[3]) ^ (Hill_fit[1] - 1) * GEC50 * log(10)\n\n  #CONVERT OUTPUT TO CONFORM TO FORMATTING PARAMETERS\n\n  if (scale_01 == FALSE) {\n    Ginf <- 2 * Ginf - 1\n    GHS <- 2 * GHS\n  }\n\n  if (viability_as_pct == TRUE) {\n    Ginf <- 100 * Ginf\n    GHS <- 100 * GHS\n  }\n\n  if (conc_as_log == TRUE) {\n    GEC50 <- log10(GEC50)\n  }\n\n  return(list(GHS = as.numeric(GHS), Ginf = as.numeric(Ginf), GEC50 = as.numeric(GEC50)))\n\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `grRegression` function and what are its main input parameters?",
        "answer": "The `grRegression` function is used to calculate GR (Growth Rate) statistics from dose-response data. Its main input parameters include:\n- `conc`: drug concentration values\n- `viability`: cell viability data\n- `duration`: experiment duration\n- `Hill_fit`: parameters for the Hill equation (optional)\n- `dbl_time`: cell doubling time\nThe function also accepts various optional parameters for data formatting and fitting options."
      },
      {
        "question": "How does the function handle missing input data, and what error messages are displayed?",
        "answer": "The function performs several checks for missing input data:\n1. If both `viability` and `Hill_fit` are missing, it stops with the message: \"Please enter viability data and/or Hill equation parameters.\"\n2. If `duration` is missing, it stops with: \"Cannot calculate GR without duration of experiment\"\n3. If `dbl_time` is missing, it stops with: \"Cannot calculate GR without cell doubling time\"\nThese checks ensure that all necessary data for GR calculation is provided."
      },
      {
        "question": "What are the main output values of the `grRegression` function, and how are they calculated?",
        "answer": "The main output values of the `grRegression` function are:\n1. GHS (Growth rate inhibition Hill Slope)\n2. Ginf (G infinity, the GR value at infinite drug concentration)\n3. GEC50 (the drug concentration producing half-maximal GR)\n\nThese values are calculated using the Hill equation parameters and the input data. The function applies various transformations based on the input parameters (e.g., `scale_01`, `viability_as_pct`, `conc_as_log`) to adjust the output format. The calculations involve complex mathematical formulas derived from the Sorger paper on drug response metrics, with some adjustments made by the function author."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/vignettes/DetectingDrugSynergyAndAntagonism.R",
    "language": "R",
    "content": "## ----eval=TRUE, include=FALSE-------------------------------------------------\n# convenience variables\ncgx <- BiocStyle::Biocpkg(\"CoreGx\")\npgx <- BiocStyle::Biocpkg(\"PharmacoGx\")\ndt <- BiocStyle::CRANpkg(\"data.table\")\n\n# knitr options\nknitr::opts_chunk$set(warning=FALSE)\n\n## ----load_dependencies_eval, eval=TRUE, echo=FALSE----------------------------\nsuppressPackageStartupMessages({\n    library(PharmacoGx)\n    library(CoreGx)\n    library(data.table)\n    library(ggplot2)\n})\n\n## ----load_dependencies_echo, eval=FALSE, echo=TRUE----------------------------\n#  library(PharmacoGx)\n#  library(CoreGx)\n#  library(data.table)\n#  library(ggplot2)\n\n## -----------------------------------------------------------------------------\ninput_file <- system.file(\"extdata/mathews_griner.csv.tar.gz\",\n    package=\"PharmacoGx\")\nmathews_griner <- fread(input_file)\n\n## ----experiment_design_hypothesis---------------------------------------------\ngroups <- list(\n    rowDataMap=c(\n        treatment1id=\"RowName\", treatment2id=\"ColName\",\n        treatment1dose=\"RowConcs\", treatment2dose=\"ColConcs\"\n    ),\n    colDataMap=c(\"sampleid\")\n)\ngroups[[\"assayMap\"]] <- c(groups$rowDataMap, groups$colDataMap)\n(groups)\n\n## ----handling_technical_replicates--------------------------------------------\n# The := operator modifies a data.table by reference (i.e., without making a copy)\nmathews_griner[, tech_rep := seq_len(.N), by=c(groups[[\"assayMap\"]])]\nif (max(mathews_griner[[\"tech_rep\"]]) > 1) {\n    groups[[\"colDataMap\"]] <- c(groups[[\"colDataMap\"]], \"tech_rep\")\n    groups[[\"assayMap\"]] <- c(groups[[\"assayMap\"]], \"tech_rep\")\n} else {\n    # delete the additional column if not needed\n    message(\"No technical replicates in this dataset!\")\n    mathews_griner[[\"tech_reps\"]] <- NULL\n}\n\n## ----build_tredatamapper------------------------------------------------------\n(treMapper <- TREDataMapper(rawdata=mathews_griner))\n\n## ----evaluate_tre_mapping_guess-----------------------------------------------\n(guess <- guessMapping(treMapper, groups, subset=TRUE))\n\n## ----update_tredatamapper_with_guess------------------------------------------\nmetadataMap(treMapper) <- list(experiment_metadata=guess$metadata$mapped_columns)\nrowDataMap(treMapper) <- guess$rowDataMap\ncolDataMap(treMapper) <- guess$colDataMap\nassayMap(treMapper) <- list(raw=guess$assayMap)\ntreMapper\n\n## ----metaconstruct_the_tre----------------------------------------------------\n(tre <- metaConstruct(treMapper))\n\n## ----normalize_to_dose_0_0_control--------------------------------------------\nraw <- tre[[\"raw\"]]\nraw[,\n    viability := viability / .SD[treatment1dose == 0 & treatment2dose == 0, viability],\n    by=c(\"treatment1id\", \"treatment2id\", \"sampleid\", \"tech_rep\")\n]\nraw[, viability := pmax(0, viability)]  # truncate min viability at 0\ntre[[\"raw\"]] <- raw\n\n## ----sanity_check_viability---------------------------------------------------\ntre[[\"raw\"]][, range(viability)]\n\n## ----find_bad_viability_treatment, warning=FALSE------------------------------\n(bad_treatments <- tre[[\"raw\"]][viability > 2, unique(treatment1id)])\n\n## ----remove_bad_viability_treatment, warning=FALSE----------------------------\n(tre <- subset(tre, !(treatment1id %in% bad_treatments)))\n\n## ----sanity_check_viability2--------------------------------------------------\ntre[[\"raw\"]][, range(viability)]\n\n## ----creating_monotherapy_assay-----------------------------------------------\ntre_qc <- tre |>\n    endoaggregate(\n        subset=treatment2dose == 0,  # filter to only monotherapy rows\n        assay=\"raw\",\n        target=\"mono_viability\",  # create a new assay named mono_viability\n        mean_viability=pmin(1, mean(viability)),\n        by=c(\"treatment1id\", \"treatment1dose\", \"sampleid\")\n    )\n\n## ----monotherapy_curve_fits, messages=FALSE-----------------------------------\ntre_fit <- tre_qc |>\n    endoaggregate(\n        {  # the entire code block is evaluated for each group in our group by\n            # 1. fit a log logistic curve over the dose range\n            fit <- PharmacoGx::logLogisticRegression(treatment1dose, mean_viability,\n                viability_as_pct=FALSE)\n            # 2. compute curve summary metrics\n            ic50 <- computeIC50(treatment1dose, Hill_fit=fit)\n            aac <- computeAUC(treatment1dose, Hill_fit=fit)\n            # 3. assemble the results into a list, each item will become a\n            #   column in the target assay.\n            list(\n                HS=fit[[\"HS\"]],\n                E_inf = fit[[\"E_inf\"]],\n                EC50 = fit[[\"EC50\"]],\n                Rsq=as.numeric(unlist(attributes(fit))),\n                aac_recomputed=aac,\n                ic50_recomputed=ic50\n            )\n        },\n        assay=\"mono_viability\",\n        target=\"mono_profiles\",\n        enlist=FALSE,  # this option enables the use of a code block for aggregation\n        by=c(\"treatment1id\", \"sampleid\"),\n        nthread=2  # parallelize over multiple cores to speed up the computation\n    )\n\n## ----create_combo_viability, message=FALSE------------------------------------\ntre_combo <- tre_fit |>\n    endoaggregate(\n        assay=\"raw\",\n        target=\"combo_viability\",\n        mean(viability),\n        by=c(\"treatment1id\", \"treatment2id\", \"treatment1dose\", \"treatment2dose\",\n            \"sampleid\")\n    )\n\n## ----add_monotherapy_fits_to_combo_viability----------------------------------\ntre_combo <- tre_combo |>\n    mergeAssays(\n        x=\"combo_viability\",\n        y=\"mono_profiles\",\n        by=c(\"treatment1id\", \"sampleid\")\n    ) |>\n    mergeAssays(\n        x=\"combo_viability\",\n        y=\"mono_profiles\",\n        by.x=c(\"treatment2id\", \"sampleid\"),\n        by.y=c(\"treatment1id\", \"sampleid\"),\n        suffixes=c(\"_1\", \"_2\")  # add sufixes to duplicate column names\n    )\n\n## -----------------------------------------------------------------------------\ntre_combo <- tre_combo |>\n    endoaggregate(\n        viability_1=.SD[treatment2dose == 0, mean_viability],\n        assay=\"combo_viability\",\n        by=c(\"treatment1id\", \"treatment1dose\", \"sampleid\")\n    ) |>\n    endoaggregate(\n        viability_2=.SD[treatment1dose == 0, mean_viability],\n        assay=\"combo_viability\",\n        by=c(\"treatment1id\", \"treatment2dose\", \"sampleid\")\n    )\n\n## ----compute_synergy_null_hypotheses, message=FALSE---------------------------\ntre_synergy <- tre_combo |>\n    endoaggregate(\n        assay=\"combo_viability\",\n        HSA_ref=computeHSA(viability_1, viability_2),\n        Bliss_ref=computeBliss(viability_1, viability_2),\n        Loewe_ref=computeLoewe(\n            treatment1dose, HS_1=HS_1, EC50_1=EC50_1, E_inf_1=E_inf_1,\n            treatment2dose, HS_2=HS_2, EC50_2=EC50_2, E_inf_2=E_inf_2\n        ),\n        ZIP_ref=computeZIP(\n            treatment1dose, HS_1=HS_1, EC50_1=EC50_1, E_inf_1=E_inf_1,\n            treatment2dose, HS_2=HS_2, EC50_2=EC50_2, E_inf_2=E_inf_2\n        ),\n        by=assayKeys(tre_combo, \"combo_viability\"),\n        nthread=2\n    )\n\n## ----synergy_score_vs_reference-----------------------------------------------\ntre_synergy <- tre_synergy |>\n    endoaggregate(\n        assay=\"combo_viability\",\n        HSA_score=HSA_ref - mean_viability,\n        Bliss_score=Bliss_ref - mean_viability,\n        Loewe_score=Loewe_ref - mean_viability,\n        ZIP_score=ZIP_ref - mean_viability,\n        by=assayKeys(tre_synergy, \"combo_viability\")\n    )\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `guessMapping` function in this code, and how is its output used?",
        "answer": "The `guessMapping` function is used to automatically infer the mapping between the raw data columns and the TRE (Treatment Response Experiment) data structure. Its output, stored in the `guess` variable, is used to update the `TREDataMapper` object (`treMapper`) with the inferred mappings for metadata, row data, column data, and assay data. This mapping is crucial for correctly structuring the raw data into a standardized TRE format."
      },
      {
        "question": "How does the code handle technical replicates in the dataset, and what happens if there are no technical replicates?",
        "answer": "The code checks for technical replicates by adding a 'tech_rep' column to the `mathews_griner` data table, assigning a sequence number to each unique combination of assay map variables. If the maximum value of 'tech_rep' is greater than 1, indicating the presence of technical replicates, 'tech_rep' is added to the colDataMap and assayMap. If there are no technical replicates, the code removes the 'tech_rep' column and displays a message saying 'No technical replicates in this dataset!'."
      },
      {
        "question": "What synergy scores are calculated in this code, and how are they computed?",
        "answer": "The code calculates four synergy scores: HSA (Highest Single Agent), Bliss, Loewe, and ZIP (Zero Interaction Potency). These scores are computed by comparing the observed combination effect (mean_viability) to reference values calculated using different synergy models. The synergy scores are calculated as the difference between the reference value and the observed mean viability. For example, HSA_score = HSA_ref - mean_viability. The reference values are computed using specific functions like computeHSA, computeBliss, computeLoewe, and computeZIP, which implement the respective synergy models."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/drugDoseResponseCurve.R",
    "language": "R",
    "content": "#' Plot drug response curve of a given drug and a given cell for a list of pSets (objects of the PharmacoSet class).\n#'\n#' Given a list of PharmacoSets, the function will plot the drug_response curve,\n#' for a given drug/cell pair. The y axis of the plot is the viability percentage\n#' and x axis is the log transformed concentrations. If more than one pSet is\n#' provided, a light gray area would show the common concentration range between pSets.\n#' User can ask for type of sensitivity measurment to be shown in the plot legend.\n#' The user can also provide a list of their own concentrations and viability values,\n#' as in the examples below, and it will be treated as experiments equivalent to values coming\n#' from a pset. The names of the concentration list determine the legend labels.\n#'\n#' @examples\n##TODO:: How do you pass PSets to this?\n#' if (interactive()) {\n#' # Manually enter the plot parameters\n#' drugDoseResponseCurve(concentrations=list(\"Experiment 1\"=c(.008, .04, .2, 1)),\n#'  viabilities=list(c(100,50,30,1)), plot.type=\"Both\")\n#'\n#' # Generate a plot from one or more PSets\n#' data(GDSCsmall)\n#' drugDoseResponseCurve(drug=\"Doxorubicin\", cellline=\"22RV1\", pSets=GDSCsmall)\n#' }\n#'\n#' @param drug `character(1)` A drug name for which the drug response curve should be\n#' plotted. If the plot is desirable for more than one pharmaco set, A unique drug id\n#' should be provided.\n#' @param cellline `character(1)` A cell line name for which the drug response curve should be\n#' plotted. If the plot is desirable for more than one pharmaco set, A unique cell id\n#' should be provided.\n#' @param pSets `list` a list of PharmacoSet objects, for which the function\n#' should plot the curves.\n#' @param concentrations,viabilities `list` A list of concentrations and viabilities to plot, the function assumes that\n#' `concentrations[[i]]` is plotted against `viabilities[[i]]`. The names of the concentration list are used to create the legend labels\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(ICn) should be returned instead of ICn. Applies only to the concentrations parameter.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf passed in as decimal. Applies only to the viabilities parameter.\n#' @param legends.label `numeric` A vector of sensitivity measurment types which could\n#' be any combination of  ic50_published, auc_published, auc_recomputed and auc_recomputed_star.\n#' A legend will be displayed on the top right of the plot which each line of the legend is\n#' the values of requested sensitivity measerments for one of the requested pSets.\n#' If this parameter is missed no legend would be provided for the plot.\n#' @param ylim `numeric` A vector of two numerical values to be used as ylim of the plot.\n#' If this parameter would be missed c(0,100) would be used as the ylim of the plot.\n#' @param xlim `numeric` A vector of two numerical values to be used as xlim of the plot.\n#' If this parameter would be missed the minimum and maximum comncentrations between all\n#' the pSets would be used as plot xlim.\n#' @param mycol `numeric` A vector with the same lenght of the pSets parameter which\n#' will determine the color of the curve for the pharmaco sets. If this parameter is\n#' missed default colors from Rcolorbrewer package will be used as curves color.\n#' @param plot.type `character` Plot type which can be the actual one (\"Actual\") or\n#' the one fitted by logl logistic regression (\"Fitted\") or both of them (\"Both\").\n#' If this parameter is missed by default actual curve is plotted.\n#' @param summarize.replicates `character` If this parameter is set to true replicates\n#' are summarized and replicates are plotted individually otherwise\n#' @param title `character` The title of the graph. If no title is provided, then it defaults to\n#' 'Drug':'Cell Line'.\n#' @param lwd `numeric` The line width to plot with\n#' @param cex `numeric` The cex parameter passed to plot\n#' @param cex.main `numeric` The cex.main parameter passed to plot, controls the size of the titles\n#' @param legend.loc And argument passable to xy.coords for the position to place the legend.\n#' @param trunc `logical(1)` Should the viability values be truncated to lie in \\[0-100\\] before doing the fitting\n#' @param verbose `logical(1)` Should warning messages about the data passed in be printed?\n#' @param sample_col `character(1)` The name of the column in the profiles assay that contains the sample IDs.\n#' @param treatment_col `character(1)` The name of the column in the profiles assay that contains the treatment IDs.\n#'\n#' @return Plots to the active graphics device and returns an invisible NULL.\n#'\n#' @import RColorBrewer\n#'\n#' @importFrom graphics plot rect points lines legend\n#' @importFrom grDevices rgb\n# # ' @importFrom magicaxis magaxis\n#' @importFrom CoreGx .getSupportVec\n#'\n#' @export\ndrugDoseResponseCurve <-\nfunction(drug,\n         cellline,\n         pSets=list(),\n         concentrations=list(),\n         viabilities=list(),\n         conc_as_log = FALSE,\n         viability_as_pct = TRUE,\n         trunc=TRUE,\n         legends.label = c(\"ic50_published\", \"gi50_published\",\"auc_published\",\"auc_recomputed\",\"ic50_recomputed\"),\n         ylim=c(0,100),\n         xlim, mycol,\n         title,\n         plot.type=c(\"Fitted\",\"Actual\", \"Both\"),\n         summarize.replicates=TRUE,\n         lwd = 0.5,\n         cex = 0.7,\n         cex.main = 0.9,\n         legend.loc = \"topright\",\n         verbose=TRUE,\n         sample_col = \"sampleid\",\n         treatment_col = \"treatmentid\") {\n  if(!missing(pSets)){\n    if (!is(pSets, \"list\")) {\n      if (is(pSets, \"PharmacoSet\")) {\n        temp <- name(pSets)\n        pSets <- list(pSets)\n        names(pSets) <- temp\n      } else {\n        stop(\"Type of pSets parameter should be either a pSet or a list of pSets.\")\n      }\n    }\n  }\n  if(!missing(pSets) && (missing(drug) || missing(cellline))){\n    stop(\"If you pass in a pSet then drug and cellline must be set\") }\n  # } else {\n  #   if(missing(drug)){\n  #   drug <- \"Drug\"}\n  #   if(missing(cellline))\n  #   cellline <- \"Cell Line\"\n  # }\n  if(!missing(concentrations)){\n    if(missing(viabilities)){\n\n      stop(\"Please pass in the viabilities to Plot with the concentrations.\")\n\n    }\n    if (!is(concentrations, \"list\")) {\n      if (mode(concentrations) == \"numeric\") {\n        if(mode(viabilities)!=\"numeric\"){\n          stop(\"Passed in 1 vector of concentrations but the viabilities are not numeric!\")\n        }\n        cleanData <- sanitizeInput(concentrations,\n          viabilities,\n          conc_as_log = conc_as_log,\n          viability_as_pct = viability_as_pct,\n          trunc = trunc,\n          verbose = verbose)\n        concentrations <- 10^cleanData[[\"log_conc\"]]\n        concentrations <- list(concentrations)\n        viabilities <- 100*cleanData[[\"viability\"]]\n        viabilities <- list(viabilities)\n        names(concentrations) <- \"Exp1\"\n        names(viabilities) <- \"Exp1\"\n      } else {\n        stop(\"Mode of concentrations parameter should be either numeric or a list of numeric vectors\")\n      }\n    } else{\n      if(length(viabilities)!= length(concentrations)){\n        stop(\"The number of concentration and viability vectors passed in differs\")\n      }\n      if(is.null(names(concentrations))){\n        names(concentrations) <- paste(\"Exp\", seq_len(length(concentrations)))\n      }\n      for(i in seq_len(length(concentrations))){\n\n        if (mode(concentrations[[i]]) == \"numeric\") {\n          if(mode(viabilities[[i]])!=\"numeric\"){\n            stop(sprintf(\"concentrations[[%d]] are numeric but the viabilities[[%d]] are not numeric!\",i,i))\n          }\n          cleanData <- sanitizeInput(concentrations[[i]],\n            viabilities[[i]],\n            conc_as_log = conc_as_log,\n            viability_as_pct = viability_as_pct,\n            trunc = trunc,\n            verbose = verbose)\n          concentrations[[i]] <- 10^cleanData[[\"log_conc\"]]\n          viabilities[[i]] <- 100*cleanData[[\"viability\"]]\n        } else {\n          stop(sprintf(\"Mode of concentrations[[%d]] parameter should be numeric\",i))\n        }\n\n      }\n\n    }\n  }\n\n  if (missing(plot.type)) {\n    plot.type <- \"Actual\"\n  }\n\n  if(is(treatmentResponse(pSets[[1]]), \"LongTable\")){\n    pSets[[1]] <- subsetByTreatment(pSets[[1]], treatments=drug)\n  }\n  pSets[[1]] <- subsetBySample(pSets[[1]], samples=cellline)\n\n  doses <- list(); responses <- list(); legend.values <- list(); j <- 0; pSetNames <- list()\n  if(!missing(pSets)){\n    for(i in seq_len(length(pSets))) {\n      exp_i <- which(sensitivityInfo(pSets[[i]])[ ,sample_col] == cellline & sensitivityInfo(pSets[[i]])[ ,treatment_col] == drug)\n      if(length(exp_i) > 0) {\n        if (summarize.replicates) {\n          pSetNames[[i]] <- name(pSets[[i]])\n          drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"])),\n              \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"]))), stringsAsFactors=FALSE)\n          drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # tryCatch(\n          #   drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"])),\n          #     \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"]))), stringsAsFactors=FALSE)\n          #   drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # , error = function(e) {\n          #   if (length(exp_i) == 1) {\n          #   drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"])),\n          #     \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"]))), stringsAsFactors=FALSE)\n          #   drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # }else{\n          #   drug.responses <- as.data.frame(cbind(\"Dose\"=apply(sensitivityRaw(pSets[[i]])[exp_i, , \"Dose\"], 1, function(x){median(as.numeric(x), na.rm=TRUE)}),\n          #     \"Viability\"=apply(sensitivityRaw(pSets[[i]])[exp_i, , \"Viability\"], 2, function(x){median(as.numeric(x), na.rm=TRUE)})), stringsAsFactors=FALSE)\n          #   drug.responses <- drug.responses[complete.cases(drug.responses), ]\n          # }\n          # })\n\n          \n          doses[[i]] <- drug.responses$Dose\n          responses[[i]] <- drug.responses$Viability\n          names(doses[[i]]) <- names(responses[[i]]) <- seq_len(length(doses[[i]]))\n          if (!missing(legends.label)) {\n            if (length(legends.label) > 1) {\n              legend.values[[i]] <- paste(unlist(lapply(legends.label, function(x){\n                sprintf(\"%s = %s\", x, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp_i,x]), digits=2))\n              })), collapse = \", \")\n            } else {\n              legend.values[[i]] <- sprintf(\"%s = %s\", legends.label, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp_i, legends.label]), digits=2))\n            }\n          } else {\n            legend.values[i] <- \"\"\n          }\n        }else {\n          for (exp in exp_i) {\n            j <- j + 1\n            pSetNames[[j]] <- name(pSets[[i]])\n\n            drug.responses <- as.data.frame(cbind(\"Dose\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp, , \"Dose\"])),\n              \"Viability\"=as.numeric(as.vector(sensitivityRaw(pSets[[i]])[exp, , \"Viability\"]))), stringsAsFactors=FALSE)\n            drug.responses <- drug.responses[complete.cases(drug.responses), ]\n            doses[[j]] <- drug.responses$Dose\n            responses[[j]] <- drug.responses$Viability\n            names(doses[[j]]) <- names(responses[[j]]) <- seq_len(length(doses[[j]]))\n            if (!missing(legends.label)) {\n              if (length(legends.label) > 1) {\n                legend.values[[j]] <- paste(unlist(lapply(legends.label, function(x){\n                  sprintf(\"%s = %s\", x, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp, x]), digits=2))\n                })), collapse = \", \")\n              } else {\n                legend.values[[j]] <- sprintf(\" Exp %s %s = %s\", rownames(sensitivityInfo(pSets[[i]]))[exp], legends.label, round(as.numeric(sensitivityProfiles(pSets[[i]])[exp, legends.label]), digits=2))\n              }\n            } else {\n              tt <- unlist(strsplit(rownames(sensitivityInfo(pSets[[i]]))[exp], split=\"_\"))\n              if (tt[1] == treatment_col) {\n                legend.values[[j]] <- tt[2]\n              }else{\n                legend.values[[j]] <- rownames(sensitivityInfo(pSets[[i]]))[exp]\n              }\n            }\n          }\n        }\n      } else {\n        warning(\"The cell line and drug combo were not tested together. Aborting function.\")\n        return()\n      }\n    }\n  }\n\n  if(!missing(concentrations)){\n    doses2 <- list(); responses2 <- list(); legend.values2 <- list(); j <- 0; pSetNames2 <- list();\n    for (i in seq_len(length(concentrations))){\n      doses2[[i]] <- concentrations[[i]]\n      responses2[[i]] <- viabilities[[i]]\n      if(length(legends.label)>0){\n        if(any(grepl(\"AUC\", x=toupper(legends.label)))){\n          legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"AUC\", round(computeAUC(concentrations[[i]],viabilities[[i]], conc_as_log=FALSE, viability_as_pct=TRUE)/100, digits=2)), sep=\", \")\n        }\n        if(any(grepl(\"IC50\", x=toupper(legends.label)))){\n          legend.values2[[i]] <- paste(legend.values2[i][[1]],sprintf(\"%s = %s\", \"IC50\", round(computeIC50(concentrations[[i]],viabilities[[i]], conc_as_log=FALSE, viability_as_pct=TRUE), digits=2)), sep=\", \")\n        }\n\n      } else{ legend.values2[[i]] <- \"\"}\n\n      pSetNames2[[i]] <- names(concentrations)[[i]]\n    }\n    doses <- c(doses, doses2)\n    responses <- c(responses, responses2)\n    legend.values <- c(legend.values, legend.values2)\n    pSetNames <- c(pSetNames, pSetNames2)\n  }\n\n  if (missing(mycol)) {\n    # require(RColorBrewer) || stop(\"Library RColorBrewer is not available!\")\n    mycol <- RColorBrewer::brewer.pal(n=7, name=\"Set1\")\n  }\n\n  dose.range <- c(10^100 , 0)\n  viability.range <- c(0 , 10)\n  for(i in seq_len(length(doses))) {\n    dose.range <- c(min(dose.range[1], min(doses[[i]], na.rm=TRUE), na.rm=TRUE), max(dose.range[2], max(doses[[i]], na.rm=TRUE), na.rm=TRUE))\n    viability.range <- c(0, max(viability.range[2], max(responses[[i]], na.rm=TRUE), na.rm=TRUE))\n  }\n  x1 <- 10 ^ 10; x2 <- 0\n\n  if(length(doses) > 1) {\n    common.ranges <- .getCommonConcentrationRange(doses)\n\n    for(i in seq_len(length(doses))) {\n      x1 <- min(x1, min(common.ranges[[i]]))\n      x2 <- max(x2, max(common.ranges[[i]]))\n    }\n  }\n  if (!missing(xlim)) {\n    dose.range <- xlim\n  }\n  if (!missing(ylim)) {\n    viability.range <- ylim\n  }\n  if(missing(title)){\n    if(!missing(drug)&&!missing(cellline)){\n      title <- sprintf(\"%s:%s\", drug, cellline)\n    } else {\n      title <- \"Drug Dose Response Curve\"\n    }\n\n  }\n  plot(NA, xlab=\"Concentration (uM)\", ylab=\"% Viability\", axes =FALSE, main=title, log=\"x\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n  magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,3), minorn=c(5,2))\n  legends <- NULL\n  legends.col <- NULL\n  if (length(doses) > 1) {\n    rect(xleft=x1, xright=x2, ybottom=viability.range[1] , ytop=viability.range[2] , col=rgb(240, 240, 240, maxColorValue = 255), border=FALSE)\n  }\n\n  for (i in seq_len(length(doses))) {\n    points(doses[[i]],responses[[i]],pch=20,col = mycol[i], cex=cex)\n\n    switch(plot.type , \"Actual\"={\n      lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i])\n    }, \"Fitted\"={\n      log_logistic_params <- logLogisticRegression(conc=doses[[i]], viability=responses[[i]])\n      log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n      lines(10 ^ log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100 ,lty=1, lwd=lwd, col=mycol[i])\n    },\"Both\"={\n      lines(doses[[i]],responses[[i]],lty=1,lwd=lwd,col = mycol[i])\n      log_logistic_params <- logLogisticRegression(conc = doses[[i]], viability = responses[[i]])\n      log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n      lines(10 ^ log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100 ,lty=1, lwd=lwd, col=mycol[i])\n    })\n    legends<- c(legends, sprintf(\"%s%s\", pSetNames[[i]], legend.values[[i]]))\n    legends.col <-  c(legends.col, mycol[i])\n  }\n\n  legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=c(15,15))\n  return(invisible(NULL))\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `drugDoseResponseCurve` function?",
        "answer": "The primary purpose of the `drugDoseResponseCurve` function is to plot drug response curves for a given drug and cell line combination across one or more PharmacoSet objects. It visualizes the relationship between drug concentration and cell viability, allowing for comparison of drug responses across different datasets or experiments."
      },
      {
        "question": "How does the function handle multiple PharmacoSet objects when plotting the drug response curve?",
        "answer": "When multiple PharmacoSet objects are provided, the function plots curves for each set using different colors. It also adds a light gray area to show the common concentration range between the PharmacoSets. The legend includes sensitivity measurements (e.g., IC50, AUC) for each PharmacoSet if requested."
      },
      {
        "question": "What options does the function provide for customizing the plot output?",
        "answer": "The function offers several customization options, including: specifying plot type (Actual, Fitted, or Both), setting y-axis and x-axis limits, customizing colors, adjusting line width and point size, setting the plot title, controlling legend location and content, and choosing whether to summarize replicates or plot them individually."
      }
    ],
    "completion_tasks": [
      {
        "partial": "drugDoseResponseCurve <- function(drug, cellline, pSets=list(), concentrations=list(), viabilities=list(), conc_as_log=FALSE, viability_as_pct=TRUE, trunc=TRUE, legends.label=c(\"ic50_published\", \"gi50_published\",\"auc_published\",\"auc_recomputed\",\"ic50_recomputed\"), ylim=c(0,100), xlim, mycol, title, plot.type=c(\"Fitted\",\"Actual\", \"Both\"), summarize.replicates=TRUE, lwd=0.5, cex=0.7, cex.main=0.9, legend.loc=\"topright\", verbose=TRUE, sample_col=\"sampleid\", treatment_col=\"treatmentid\") {\n  # Input validation and data preparation\n  # ...\n\n  # Plot setup\n  if(missing(title)) {\n    title <- if(!missing(drug) && !missing(cellline)) sprintf(\"%s:%s\", drug, cellline) else \"Drug Dose Response Curve\"\n  }\n  plot(NA, xlab=\"Concentration (uM)\", ylab=\"% Viability\", axes=FALSE, main=title, log=\"x\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n  magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,3), minorn=c(5,2))\n\n  # Plot data points and curves\n  for (i in seq_len(length(doses))) {\n    points(doses[[i]], responses[[i]], pch=20, col=mycol[i], cex=cex)\n    # Plot lines based on plot.type\n    # ...\n  }\n\n  # Add legend\n  legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=c(15,15))\n  return(invisible(NULL))\n}",
        "complete": "drugDoseResponseCurve <- function(drug, cellline, pSets=list(), concentrations=list(), viabilities=list(), conc_as_log=FALSE, viability_as_pct=TRUE, trunc=TRUE, legends.label=c(\"ic50_published\", \"gi50_published\",\"auc_published\",\"auc_recomputed\",\"ic50_recomputed\"), ylim=c(0,100), xlim, mycol, title, plot.type=c(\"Fitted\",\"Actual\", \"Both\"), summarize.replicates=TRUE, lwd=0.5, cex=0.7, cex.main=0.9, legend.loc=\"topright\", verbose=TRUE, sample_col=\"sampleid\", treatment_col=\"treatmentid\") {\n  if(!missing(pSets)) {\n    if (!is(pSets, \"list\")) {\n      if (is(pSets, \"PharmacoSet\")) {\n        temp <- name(pSets)\n        pSets <- list(pSets)\n        names(pSets) <- temp\n      } else {\n        stop(\"Type of pSets parameter should be either a pSet or a list of pSets.\")\n      }\n    }\n  }\n  if(!missing(pSets) && (missing(drug) || missing(cellline))) {\n    stop(\"If you pass in a pSet then drug and cellline must be set\")\n  }\n\n  # Data preparation and validation\n  doses <- list(); responses <- list(); legend.values <- list(); pSetNames <- list()\n  if(!missing(pSets)) {\n    for(i in seq_len(length(pSets))) {\n      # Extract and process data from pSets\n      # ...\n    }\n  }\n\n  if(!missing(concentrations)) {\n    # Process manually entered concentrations and viabilities\n    # ...\n  }\n\n  # Calculate dose and viability ranges\n  dose.range <- range(unlist(doses), na.rm=TRUE)\n  viability.range <- c(0, max(unlist(responses), na.rm=TRUE))\n\n  if (!missing(xlim)) dose.range <- xlim\n  if (!missing(ylim)) viability.range <- ylim\n\n  # Set up plot\n  if(missing(title)) {\n    title <- if(!missing(drug) && !missing(cellline)) sprintf(\"%s:%s\", drug, cellline) else \"Drug Dose Response Curve\"\n  }\n  plot(NA, xlab=\"Concentration (uM)\", ylab=\"% Viability\", axes=FALSE, main=title, log=\"x\", ylim=viability.range, xlim=dose.range, cex=cex, cex.main=cex.main)\n  magicaxis::magaxis(side=seq_len(2), frame.plot=TRUE, tcl=-.3, majorn=c(5,3), minorn=c(5,2))\n\n  # Plot data points and curves\n  legends <- character(length(doses))\n  legends.col <- mycol[seq_along(doses)]\n  for (i in seq_along(doses)) {\n    points(doses[[i]], responses[[i]], pch=20, col=mycol[i], cex=cex)\n    switch(plot.type,\n      \"Actual\" = lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i]),\n      \"Fitted\" = {\n        log_logistic_params <- logLogisticRegression(conc=doses[[i]], viability=responses[[i]])\n        log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n        lines(10^log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100, lty=1, lwd=lwd, col=mycol[i])\n      },\n      \"Both\" = {\n        lines(doses[[i]], responses[[i]], lty=1, lwd=lwd, col=mycol[i])\n        log_logistic_params <- logLogisticRegression(conc=doses[[i]], viability=responses[[i]])\n        log10_x_vals <- .getSupportVec(log10(doses[[i]]))\n        lines(10^log10_x_vals, .Hill(log10_x_vals, pars=c(log_logistic_params$HS, log_logistic_params$E_inf/100, log10(log_logistic_params$EC50))) * 100, lty=1, lwd=lwd, col=mycol[i])\n      }\n    )\n    legends[i] <- sprintf(\"%s%s\", pSetNames[[i]], legend.values[[i]])\n  }\n\n  # Add legend\n  legend(legend.loc, legend=legends, col=legends.col, bty=\"n\", cex=cex, pch=15)\n  return(invisible(NULL))\n}"
      },
      {
        "partial": "logLogisticRegression <- function(conc, viability, trunc=TRUE, viability_as_pct=TRUE, conc_as_log=FALSE, verbose=TRUE) {\n  # Input validation and data preparation\n  # ...\n\n  # Perform log-logistic regression\n  fit <- try(stats::nls(viability ~ SSlogis(log10_conc, Asym, xmid, scal)), silent=TRUE)\n\n  # Extract and return parameters\n  # ...\n}",
        "complete": "logLogisticRegression <- function(conc, viability, trunc=TRUE, viability_as_pct=TRUE, conc_as_log=FALSE, verbose=TRUE) {\n  if (length(conc) != length(viability)) {\n    stop(\"Length of concentration and viability vectors must be the same.\")\n  }\n\n  # Convert concentration to log10 if necessary\n  log10_conc <- if (conc_as_log) conc else log10(conc)\n\n  # Normalize viability if necessary\n  if (viability_as_pct) {\n    viability <- viability / 100\n  }\n\n  # Truncate viability values if requested\n  if (trunc) {\n    viability[viability > 1] <- 1\n    viability[viability < 0] <- 0\n  }\n\n  # Remove NA and infinite values\n  valid_indices <- which(!is.na(log10_conc) & !is.na(viability) & is.finite(log10_conc) & is.finite(viability))\n  log10_conc <- log10_conc[valid_indices]\n  viability <- viability[valid_indices]\n\n  # Perform log-logistic regression\n  fit <- try(stats::nls(viability ~ SSlogis(log10_conc, Asym, xmid, scal)), silent=TRUE)\n\n  if (class(fit) == \"try-error\") {\n    if (verbose) warning(\"NLS fitting failed. Returning NA for all parameters.\")\n    return(list(HS=NA, E_inf=NA, EC50=NA))\n  }\n\n  # Extract parameters\n  params <- stats::coef(fit)\n  HS <- -1 / params[\"scal\"]\n  E_inf <- params[\"Asym\"] * 100\n  EC50 <- 10^params[\"xmid\"]\n\n  return(list(HS=HS, E_inf=E_inf, EC50=EC50))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/dose.py",
    "language": "py",
    "content": "import warnings\nfrom typing import Dict, Optional, TypeVar\n\nimport numpy as np\nfrom matplotlib import pyplot as plt\n\nimport SimpleITK as sitk\nfrom pydicom import dcmread\n\nT = TypeVar('T')\n\n\ndef read_image(path):\n    reader = sitk.ImageSeriesReader()\n    dicom_names = reader.GetGDCMSeriesFileNames(path)\n    reader.SetFileNames(dicom_names)\n    reader.MetaDataDictionaryArrayUpdateOn()\n    reader.LoadPrivateTagsOn()\n\n    return reader.Execute()\n\n\nclass Dose(sitk.Image):\n    def __init__(self, img_dose, df, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_dose)\n        self.img_dose = img_dose\n        self.df = df\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n        \n    @classmethod\n    def from_dicom_rtdose(cls, path):\n        '''\n        Reads the data and returns the data frame and the image dosage in SITK format\n        '''\n        # change log (2022-10-12)\n        if \".dcm\" in path:\n            dose = sitk.ReadImage(path)\n        else:\n            dose = read_image(path) \n        \n        # if 4D, make 3D\n        if dose.GetDimension() == 4:\n            dose = dose[:,:,:,0]\n        \n        # Get the metadata\n        df = dcmread(path)\n\n        # Convert to SUV\n        factor = float(df.DoseGridScaling)\n        img_dose = sitk.Cast(dose, sitk.sitkFloat32)\n        img_dose = img_dose * factor\n\n        metadata = {}\n\n        return cls(img_dose, df, metadata)\n\n    def resample_dose(self,\n                      ct_scan: sitk.Image) -> sitk.Image:\n        '''\n        Resamples the RTDOSE information so that it can be overlayed with CT scan. The beginning and end slices of the \n        resampled RTDOSE scan might be empty due to the interpolation\n        '''\n        resampled_dose = sitk.Resample(self.img_dose, ct_scan)  # , interpolator=sitk.sitkNearestNeighbor)\n        return resampled_dose\n\n    def show_overlay(self,\n                     ct_scan: sitk.Image,\n                     slice_number: int):\n        '''\n        For a given slice number, the function resamples RTDOSE scan and overlays on top of the CT scan and returns the figure of the\n        overlay\n        '''\n        resampled_dose = self.resample_dose(ct_scan)\n        fig = plt.figure(\"Overlayed RTdose image\", figsize=[15, 10])\n        dose_arr = sitk.GetArrayFromImage(resampled_dose)\n        plt.subplot(1,3,1)\n        plt.imshow(dose_arr[slice_number,:,:])\n        plt.subplot(1,3,2)\n        ct_arr = sitk.GetArrayFromImage(ct_scan)\n        plt.imshow(ct_arr[slice_number,:,:])\n        plt.subplot(1,3,3)\n        plt.imshow(ct_arr[slice_number,:,:], cmap=plt.cm.gray)\n        plt.imshow(dose_arr[slice_number,:,:], cmap=plt.cm.hot, alpha=.4)\n        return fig\n        \n    def get_metadata(self):\n        '''\n        Forms Dose-Value Histogram (DVH) from DICOM metadata\n        {\n            dvh_type\n            dose_type\n            dose_units\n            vol_units\n            ROI_ID: {\n                vol: different volume values for different dosage bins\n                dose_bins: different dose bins\n                max_dose: max dose value\n                mean_dose : mean dose value\n                min_dose: min dose value\n                total_vol: total volume of the ROI\n            }\n        }\n        '''\n        try:\n            n_ROI =  len(self.df.DVHSequence)\n            self.dvh = {}\n            # These properties are uniform across all the ROIs\n            self.dvh[\"dvh_type\"] = self.df.DVHSequence[0].DVHType   \n            self.dvh[\"dose_units\"] = self.df.DVHSequence[0].DoseUnits\n            self.dvh[\"dose_type\"] = self.df.DVHSequence[0].DoseType\n            self.dvh[\"vol_units\"] = self.df.DVHSequence[0].DVHVolumeUnits\n            # ROI specific properties\n            for i in range(n_ROI):\n                raw_data = np.array(self.df.DVHSequence[i].DVHData)\n                n = len(raw_data)\n\n                # ROI ID\n                ROI_reference = self.df.DVHSequence[i].DVHReferencedROISequence[0].ReferencedROINumber\n\n                # Make dictionary for each ROI ID\n                self.dvh[ROI_reference] = {}\n\n                # DVH specifc properties\n                doses_bin = np.cumsum(raw_data[0:n:2])\n                vol = raw_data[1:n:2]\n                self.dvh[ROI_reference][\"dose_bins\"] = doses_bin.tolist()\n                self.dvh[ROI_reference][\"vol\"] = vol.tolist()\n                \n                # ROI specific properties\n                tot_vol = np.sum(vol)\n                non_zero_index = np.where(vol != 0)[0]\n                min_dose = doses_bin[non_zero_index[0]]\n                max_dose = doses_bin[non_zero_index[-1]]\n                mean_dose = np.sum(doses_bin * (vol / np.sum(vol)))\n                self.dvh[ROI_reference][\"max_dose\"] = max_dose\n                self.dvh[ROI_reference][\"mean_dose\"] = mean_dose\n                self.dvh[ROI_reference][\"min_dose\"] = min_dose\n                self.dvh[ROI_reference][\"total_vol\"] = tot_vol\n        except:\n            # TO-DO: more nuanced error catch instead of returning None\n            warnings.warn(\"No DVH information present in the DICOM. Returning empty dictionary\")\n            self.dvh = {}\n            \n        return self.dvh\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read_image` function in the given code snippet?",
        "answer": "The `read_image` function is designed to read a series of DICOM images from a specified path. It uses SimpleITK's ImageSeriesReader to load the DICOM series, enabling metadata dictionary updates and loading of private tags. The function returns the executed reader, which contains the loaded image series."
      },
      {
        "question": "Explain the `from_dicom_rtdose` class method in the `Dose` class. What does it do and how does it handle different input scenarios?",
        "answer": "The `from_dicom_rtdose` class method is a factory method that creates a `Dose` object from a DICOM RT dose file. It handles two scenarios: 1) If the path ends with '.dcm', it reads a single DICOM file. 2) Otherwise, it assumes the path is a directory and reads a series of DICOM files. The method also converts 4D images to 3D if necessary, applies dose grid scaling, and returns a new `Dose` instance with the processed image, DICOM dataset, and metadata."
      },
      {
        "question": "What is the purpose of the `get_metadata` method in the `Dose` class, and what kind of information does it extract?",
        "answer": "The `get_metadata` method extracts and organizes Dose-Volume Histogram (DVH) information from the DICOM metadata. It creates a dictionary structure containing general DVH properties (like dvh_type, dose_units, etc.) and ROI-specific information. For each ROI, it calculates and stores dose bins, volumes, max dose, mean dose, min dose, and total volume. If no DVH information is present, it returns an empty dictionary and issues a warning."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class Dose(sitk.Image):\n    def __init__(self, img_dose, df, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_dose)\n        self.img_dose = img_dose\n        self.df = df\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n    \n    @classmethod\n    def from_dicom_rtdose(cls, path):\n        # TODO: Implement method to read DICOM RT dose file\n        pass\n\n    def resample_dose(self, ct_scan: sitk.Image) -> sitk.Image:\n        # TODO: Implement dose resampling\n        pass",
        "complete": "class Dose(sitk.Image):\n    def __init__(self, img_dose, df, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_dose)\n        self.img_dose = img_dose\n        self.df = df\n        self.metadata = metadata or {}\n    \n    @classmethod\n    def from_dicom_rtdose(cls, path):\n        dose = sitk.ReadImage(path) if \".dcm\" in path else read_image(path)\n        dose = dose[:,:,:,0] if dose.GetDimension() == 4 else dose\n        df = dcmread(path)\n        factor = float(df.DoseGridScaling)\n        img_dose = sitk.Cast(dose, sitk.sitkFloat32) * factor\n        return cls(img_dose, df, {})\n\n    def resample_dose(self, ct_scan: sitk.Image) -> sitk.Image:\n        return sitk.Resample(self.img_dose, ct_scan)"
      },
      {
        "partial": "def get_metadata(self):\n    try:\n        n_ROI = len(self.df.DVHSequence)\n        self.dvh = {\n            \"dvh_type\": self.df.DVHSequence[0].DVHType,\n            \"dose_units\": self.df.DVHSequence[0].DoseUnits,\n            \"dose_type\": self.df.DVHSequence[0].DoseType,\n            \"vol_units\": self.df.DVHSequence[0].DVHVolumeUnits\n        }\n        # TODO: Implement ROI-specific properties\n    except:\n        warnings.warn(\"No DVH information present in the DICOM. Returning empty dictionary\")\n        self.dvh = {}\n    return self.dvh",
        "complete": "def get_metadata(self):\n    try:\n        n_ROI = len(self.df.DVHSequence)\n        self.dvh = {\n            \"dvh_type\": self.df.DVHSequence[0].DVHType,\n            \"dose_units\": self.df.DVHSequence[0].DoseUnits,\n            \"dose_type\": self.df.DVHSequence[0].DoseType,\n            \"vol_units\": self.df.DVHSequence[0].DVHVolumeUnits\n        }\n        for i in range(n_ROI):\n            raw_data = np.array(self.df.DVHSequence[i].DVHData)\n            ROI_reference = self.df.DVHSequence[i].DVHReferencedROISequence[0].ReferencedROINumber\n            doses_bin = np.cumsum(raw_data[0::2])\n            vol = raw_data[1::2]\n            non_zero_index = np.nonzero(vol)[0]\n            self.dvh[ROI_reference] = {\n                \"dose_bins\": doses_bin.tolist(),\n                \"vol\": vol.tolist(),\n                \"max_dose\": doses_bin[non_zero_index[-1]],\n                \"mean_dose\": np.sum(doses_bin * (vol / np.sum(vol))),\n                \"min_dose\": doses_bin[non_zero_index[0]],\n                \"total_vol\": np.sum(vol)\n            }\n    except:\n        warnings.warn(\"No DVH information present in the DICOM. Returning empty dictionary\")\n        self.dvh = {}\n    return self.dvh"
      }
    ],
    "dependencies": {
      "imports": [
        "warnings",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Dict",
        "matplotlib.pyplot",
        "pydicom.dcmread"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/scan.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nfrom typing import Dict, TypeVar\n\nT = TypeVar('T')\n\n\nclass Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        self.metadata = metadata\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `TypeVar('T')` in this code, and how is it used in the `Scan` class?",
        "answer": "The `TypeVar('T')` creates a type variable 'T' which is used to define a generic type. In the `Scan` class, it's used in the type hint for the `metadata` parameter, allowing the dictionary values to be of any consistent type. This provides flexibility while maintaining type safety, as the actual type will be determined when the class is instantiated."
      },
      {
        "question": "How does the `Scan` class combine SimpleITK functionality with custom metadata storage?",
        "answer": "The `Scan` class combines SimpleITK functionality with custom metadata storage by having two attributes: `image` of type `sitk.Image`, which likely contains medical imaging data that can be processed using SimpleITK functions, and `metadata` which is a dictionary that can store any additional information about the scan. This design allows for both image processing capabilities and flexible metadata management within a single object."
      },
      {
        "question": "What are the potential benefits and drawbacks of using a generic type for the `metadata` dictionary values in the `Scan` class?",
        "answer": "Benefits of using a generic type for `metadata` values include: 1) Flexibility to store different types of data for different scans, 2) Type safety when accessing the metadata, as the type is consistent within a single instance. Drawbacks might include: 1) Lack of specific type information at compile-time, which could make it harder to catch type-related errors early, 2) Potential for runtime type errors if not used carefully, 3) Less clear documentation of expected metadata types without additional comments or type hints in method signatures that use the metadata."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        # Complete the initialization",
        "complete": "class Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        self.metadata = metadata"
      },
      {
        "partial": "import SimpleITK as sitk\nfrom typing import Dict, TypeVar\n\nT = TypeVar('T')\n\n# Complete the Scan class definition",
        "complete": "import SimpleITK as sitk\nfrom typing import Dict, TypeVar\n\nT = TypeVar('T')\n\nclass Scan:\n    def __init__(self, image: sitk.Image, metadata: Dict[str, T]):\n        self.image = image\n        self.metadata = metadata"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Dict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/sanityCheck.R",
    "language": "R",
    "content": "sanitizeInput <- function(conc,\n\tviability,\n\tHill_fit,\n\tconc_as_log = FALSE,\n\tviability_as_pct = TRUE,\n\ttrunc = TRUE,\n\tverbose = TRUE # Set to 2 to see debug printouts\n\t) {\n\n\n\n\tif (is.logical(conc_as_log) == FALSE) {\n\t\tprint(conc_as_log)\n\t\tstop(\"'conc_as_log' is not a logical.\")\n\t}\n\n\tif (is.logical(viability_as_pct) == FALSE) {\n\t\tprint(viability_as_pct)\n\t\tstop(\"'viability_as_pct' is not a logical.\")\n\t}\n\n\tif (is.logical(trunc) == FALSE) {\n\t\tprint(trunc)\n\t\tstop(\"'trunc' is not a logical.\")\n\t}\n\tif(!is.finite(verbose)){\n\t\tstop(\"'verbose' should be a logical (or numerical) argument.\")\n\t}\n\tif(!missing(viability)&&!missing(conc)&&missing(Hill_fit))\n\t{\n\t  if (length(conc) != length(viability)) {\n\t    if(verbose==2){\n\t      print(conc)\n\t      print(viability)\n\t    }\n\t    stop(\"Log concentration vector is not of same length as viability vector.\")\n\t  }\n\t\tif( any(is.na(conc)&(!is.na(viability)))){\n\t\t\twarning(\"Missing concentrations with non-missing viability values encountered. Removing viability values correspoding to those concentrations\")\n\n\t\t\tmyx <- !is.na(conc)\n\t\t\tconc <- as.numeric(conc[myx])\n\t\t\tviability <- as.numeric(viability[myx])\n\n\t\t}\n\t\tif(any((!is.na(conc))&is.na(viability))){\n\n\t\t\twarning(\"Missing viability with non-missing concentrations values encountered. Removing concentrations values correspoding to those viabilities\")\n\t\t\tmyx <- !is.na(viability)\n\t\t\tconc <- as.numeric(conc[myx])\n\t\t\tviability <- as.numeric(viability[myx])\n\n\t\t}\n\t\tconc <- as.numeric(conc[!is.na(conc)])\n\t\tviability <- as.numeric(viability[!is.na(viability)])\n\n  #CHECK THAT FUNCTION INPUTS ARE APPROPRIATE\n\t\tif (prod(is.finite(conc)) != 1) {\n\t\t\tprint(conc)\n\t\t\tstop(\"Concentration vector contains elements which are not real numbers.\")\n\t\t}\n\n\t\tif (prod(is.finite(viability)) != 1) {\n\t\t\tprint(viability)\n\t\t\tstop(\"Viability vector contains elements which are not real numbers.\")\n\t\t}\n\n\n\t\tif (min(viability) < 0) {\n\t\t\tif (verbose) {\n\n\t\t\t\twarning(\"Warning: Negative viability data.\")\n\t\t\t}\n\t\t}\n\n\t\tif (max(viability) > (1 + 99 * viability_as_pct)) {\n\t\t\tif (verbose) {\n\t\t\t\twarning(\"Warning: Viability data exceeds negative control.\")\n\t\t\t}\n\t\t}\n\n\n\t\tif (conc_as_log == FALSE && min(conc) < 0) {\n\t\t\tif (verbose == 2) {\n\t\t\t\tprint(conc)\n\t\t\t\tprint(conc_as_log)\n\t\t\t}\n\t\t\tstop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n\t\t}\n\n\t\tif (viability_as_pct == TRUE && max(viability) < 5) {\n\t\t\twarning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\t\t\tif (verbose == 2) {\n\n\t\t\t\tprint(viability)\n\t\t\t\tprint(viability_as_pct)\n\t\t\t}\n\t\t}\n\n\t\tif (viability_as_pct == FALSE && max(viability) > 5) {\n\t\t\twarning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\t\t\tif (verbose == 2) {\n\t\t\t\tprint(viability)\n\t\t\t\tprint(viability_as_pct)\n\t\t\t}\n\t\t}\n\n\t\tif(is.unsorted(conc)){\n\t\t\twarning(\"Concentration Values were unsorted. Sorting concentration and ordering viability in same order\")\n\t\t\tmyx <- order(conc)\n\t\t\tconc <- conc[myx]\n\t\t\tviability <- viability[myx]\n\t\t}\n\n  #CONVERT DOSE-RESPONSE DATA TO APPROPRIATE INTERNAL REPRESENTATION\n\t\tif (conc_as_log == FALSE ) {\n\t\t  ii <- which(conc == 0)\n\t\t  if(length(ii) > 0) {\n\t\t    conc <- conc[-ii]\n\t\t    viability <- viability[-ii]\n\t\t  }\n\n\t\t\tlog_conc <- log10(conc)\n\t\t} else {\n\t\t\tlog_conc <- conc\n\t\t}\n\n\t\tif (viability_as_pct == TRUE) {\n\t\t\tviability <- viability / 100\n\t\t}\n\t\tif (trunc) {\n\t\t\tviability = pmin(as.numeric(viability), 1)\n\t\t\tviability = pmax(as.numeric(viability), 0)\n\t\t}\n\n\t\treturn(list(\"log_conc\"=log_conc, \"viability\"=viability))\n\t}\n\tif(!missing(Hill_fit) && missing(viability)){\n\t\tif(is.list(Hill_fit)){\n\n\t\t\tHill_fit <- unlist(Hill_fit)\n\t\t}\n\t\tif (conc_as_log == FALSE && Hill_fit[[3]] < 0) {\n\t\t\tprint(\"EC50 passed in as:\")\n\t\t\tprint(Hill_fit[[3]])\n\t\t\tstop(\"'conc_as_log' flag may be set incorrectly, as the EC50 is negative when positive value is expected.\")\n\t\t}\n\n\n\t\tif (viability_as_pct == FALSE && Hill_fit[[2]] > 1) {\n\t\t\tprint(\"Einf passed in as:\")\n\t\t\tprint(Hill_fit[[2]])\n\n\t\t\twarning(\"Warning: 'viability_as_pct' flag may be set incorrectly.\")\n\n\t\t}\n\t\tif (conc_as_log == FALSE){\n\t\t\tHill_fit[[3]] <- log10(Hill_fit[[3]])\n\t\t}\n\t\tif (viability_as_pct == TRUE){\n\t\t\tHill_fit[[2]] <- Hill_fit[[2]]/100\n\t\t}\n\t\tif(missing(conc)){\n\t\t\treturn(list(\"Hill_fit\"=Hill_fit))\n\t\t} else {\n\t\t\tconc <- as.numeric(conc[!is.na(conc)])\n\n\t\t\tif (prod(is.finite(conc)) != 1) {\n\t\t\t\tprint(conc)\n\t\t\t\tstop(\"Concentration vector contains elements which are not real numbers.\")\n\t\t\t}\n\t\t\tif (conc_as_log == FALSE && min(conc) < 0) {\n\t\t\t\tprint(conc)\n\t\t\t\tprint(conc_as_log)\n\t\t\t\tstop(\"Negative concentrations encountered. Concentration data may be inappropriate, or 'conc_as_log' flag may be set incorrectly.\")\n\t\t\t}\n\n\t\t\tif (conc_as_log == FALSE ) {\n\t\t\t\tii <- which(conc == 0)\n\t\t\t\tif(length(ii) > 0) {\n\t\t\t\t\tconc <- conc[-ii]\n\t\t\t\t}\n\t\t\t\tlog_conc <- log10(conc)\n\t\t\t} else {\n\t\t\t\tlog_conc <- conc\n\t\t\t}\n\t\t\tif(is.unsorted(conc)){\n\t\t\t\tmyx <- order(conc)\n\t\t\t\tconc <- conc[myx]\n\t\t\t}\n\t\t\treturn(list(\"Hill_fit\"=Hill_fit, \"log_conc\" = log_conc))\n\t\t}\n\n\n\t}\n\tif(!missing(Hill_fit)&&!missing(viability)){\n\n\t\tstop(\"Please pass in only one of 'Hill_fit' and 'viability', it is unclear which to use in the computation.\")\n\t}\n\tif(missing(Hill_fit)&&missing(viability)){\n\n\t\tstop(\"Both 'Hill_fit' and 'viability' missing, please pass in some data!\")\n\t}\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'sanitizeInput' function and what are its main input parameters?",
        "answer": "The 'sanitizeInput' function is designed to validate and preprocess input data for dose-response analysis. Its main input parameters are:\n1. conc: concentration values\n2. viability: corresponding viability values\n3. Hill_fit: parameters for Hill equation fitting\n4. conc_as_log: boolean indicating if concentrations are in log scale\n5. viability_as_pct: boolean indicating if viability is in percentage\n6. trunc: boolean to truncate viability values between 0 and 1\n7. verbose: controls the level of warning messages"
      },
      {
        "question": "How does the function handle missing or NA values in the concentration and viability data?",
        "answer": "The function handles missing or NA values as follows:\n1. If there are missing concentrations with non-missing viability values, it removes the corresponding viability values and issues a warning.\n2. If there are missing viability values with non-missing concentrations, it removes the corresponding concentration values and issues a warning.\n3. After these checks, it removes any remaining NA values from both concentration and viability vectors using the following lines:\n   conc <- as.numeric(conc[!is.na(conc)])\n   viability <- as.numeric(viability[!is.na(viability)])\nThis ensures that only complete pairs of concentration and viability data are used in further analysis."
      },
      {
        "question": "How does the function handle the case when 'Hill_fit' is provided instead of 'viability' data, and what transformations are applied?",
        "answer": "When 'Hill_fit' is provided instead of 'viability' data, the function:\n1. Checks if Hill_fit is a list and unlist it if necessary.\n2. Validates the EC50 (Hill_fit[[3]]) and Einf (Hill_fit[[2]]) values based on the 'conc_as_log' and 'viability_as_pct' flags.\n3. If 'conc_as_log' is FALSE, it converts the EC50 to log scale: Hill_fit[[3]] <- log10(Hill_fit[[3]]).\n4. If 'viability_as_pct' is TRUE, it converts Einf to a proportion: Hill_fit[[2]] <- Hill_fit[[2]]/100.\n5. If 'conc' is also provided, it processes the concentration data similarly to the case with viability data (converting to log scale if necessary and sorting).\n6. Returns a list containing the processed Hill_fit parameters and, if applicable, the processed log_conc values."
      }
    ],
    "completion_tasks": [
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"Input flags must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  # Add code to handle missing arguments and perform data validation\n  \n  # Add code to process and sanitize input data\n  \n  # Return sanitized data\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  if (!is.logical(conc_as_log) || !is.logical(viability_as_pct) || !is.logical(trunc)) {\n    stop(\"Input flags must be logical.\")\n  }\n  if (!is.finite(verbose)) {\n    stop(\"'verbose' should be a logical (or numerical) argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    if (length(conc) != length(viability)) {\n      stop(\"Log concentration vector is not of same length as viability vector.\")\n    }\n    \n    # Remove NA values\n    valid_data <- complete.cases(conc, viability)\n    conc <- as.numeric(conc[valid_data])\n    viability <- as.numeric(viability[valid_data])\n    \n    # Validate data\n    if (!all(is.finite(conc)) || !all(is.finite(viability))) {\n      stop(\"Input vectors contain non-finite elements.\")\n    }\n    \n    # Convert and sanitize data\n    log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    # Sort data\n    sorted_indices <- order(log_conc)\n    log_conc <- log_conc[sorted_indices]\n    viability <- viability[sorted_indices]\n    \n    return(list(log_conc = log_conc, viability = viability))\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill_fit data\n    Hill_fit <- unlist(Hill_fit)\n    if (conc_as_log == FALSE) Hill_fit[3] <- log10(Hill_fit[3])\n    if (viability_as_pct) Hill_fit[2] <- Hill_fit[2] / 100\n    \n    if (!missing(conc)) {\n      log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n      return(list(Hill_fit = Hill_fit, log_conc = sort(log_conc)))\n    }\n    \n    return(list(Hill_fit = Hill_fit))\n  } else {\n    stop(\"Invalid input combination. Provide either 'viability' and 'conc', or 'Hill_fit'.\")\n  }\n}"
      },
      {
        "partial": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  # Validate input flags\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Process concentration and viability data\n    \n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill_fit data\n    \n  } else {\n    stop(\"Invalid input combination.\")\n  }\n}",
        "complete": "sanitizeInput <- function(conc, viability, Hill_fit, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE) {\n  # Validate input flags\n  if (!all(sapply(list(conc_as_log, viability_as_pct, trunc), is.logical)) || !is.finite(verbose)) {\n    stop(\"Invalid input flags or verbose argument.\")\n  }\n  \n  if (!missing(viability) && !missing(conc) && missing(Hill_fit)) {\n    # Process concentration and viability data\n    if (length(conc) != length(viability)) stop(\"Mismatched vector lengths.\")\n    \n    valid_data <- complete.cases(conc, viability)\n    conc <- as.numeric(conc[valid_data])\n    viability <- as.numeric(viability[valid_data])\n    \n    if (!all(is.finite(c(conc, viability)))) stop(\"Non-finite values in input.\")\n    \n    log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n    viability <- if (viability_as_pct) viability / 100 else viability\n    if (trunc) viability <- pmin(pmax(viability, 0), 1)\n    \n    sorted_indices <- order(log_conc)\n    return(list(log_conc = log_conc[sorted_indices], viability = viability[sorted_indices]))\n  } else if (!missing(Hill_fit) && missing(viability)) {\n    # Process Hill_fit data\n    Hill_fit <- unlist(Hill_fit)\n    if (!conc_as_log) Hill_fit[3] <- log10(Hill_fit[3])\n    if (viability_as_pct) Hill_fit[2] <- Hill_fit[2] / 100\n    \n    if (!missing(conc)) {\n      log_conc <- if (conc_as_log) conc else log10(conc[conc > 0])\n      return(list(Hill_fit = Hill_fit, log_conc = sort(log_conc)))\n    }\n    return(list(Hill_fit = Hill_fit))\n  } else {\n    stop(\"Invalid input combination.\")\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/PharmacoSet-class.R",
    "language": "R",
    "content": "#' @importClassesFrom MultiAssayExperiment MultiAssayExperiment\n#' @export\nNULL\n\nsetClassUnion('list_OR_MAE', c('list', 'MultiAssayExperiment'))\n\n# #' @importClassesFrom CoreGx LongTable TreatmentResponseExperiment\n# setClassUnion('list_OR_LongTable', c('list', 'LongTable'))\n\n.local_class=\"PharmacoSet\"\n\n#' A Class to Contain PharmacoGenomic datasets together with their curations\n#'\n#' The PharmacoSet (pSet) class was developed to contain and organise large\n#' PharmacoGenomic datasets, and aid in their metanalysis. It was designed\n#' primarily to allow bioinformaticians and biologists to work with data at the\n#' level of genes, drugs and cell lines, providing a more naturally intuitive\n#' interface and simplifying analyses between several datasets. As such, it was\n#' designed to be flexible enough to hold datasets of two different natures\n#' while providing a common interface. The class can accomidate datasets\n#' containing both drug dose response data, as well as datasets contaning\n#' genetic profiles of cell lines pre and post treatement with compounds, known\n#' respecitively as sensitivity and perturbation datasets.\n#'\n#' @param object A \\code{PharmacoSet} object\n#' @param mDataType A \\code{character} with the type of molecular data to\n#'   return/update\n#' @param value A replacement value\n#'\n#' @slot annotation A \\code{list} of annotation data about the PharmacoSet,\n#'    including the \\code{$name} and the session information for how the object\n#'    was creating, detailing the exact versions of R and all the packages used\n#' @slot molecularProfiles A \\code{list} containing \\code{SummarizedExperiment}\n#'   type object for holding data for RNA, DNA, SNP and CNV\n#'   measurements, with associated \\code{fData} and \\code{pData}\n#'   containing the row and column metadata\n#' @slot sample A \\code{data.frame} containing the annotations for all the cell\n#'   lines profiled in the data set, across all data types\n#' @slot treatment A \\code{data.frame} containg the annotations for all the drugs\n#'   profiled in the data set, across all data types\n#' @slot treatmentResponse A \\code{list} containing all the data for the\n#'   sensitivity experiments, including \\code{$info}, a \\code{data.frame}\n#'   containing the experimental info,\\code{$raw} a 3D \\code{array} containing\n#'   raw data, \\code{$profiles}, a \\code{data.frame} containing sensitivity\n#'   profiles statistics, and \\code{$n}, a \\code{data.frame} detailing the\n#'   number of experiments for each cell-drug pair\n#' @slot perturbation A \\code{list} containting \\code{$n}, a \\code{data.frame}\n#'   summarizing the available perturbation data,\n#' @slot curation A \\code{list} containing mappings for \\code{$treatment},\n#'   \\code{cell}, \\code{tissue} names  used in the data set to universal\n#'   identifiers used between different PharmacoSet objects\n#' @slot datasetType A \\code{character} string of 'sensitivity',\n#'   'perturbation', or both detailing what type of data can be found in the\n#'   PharmacoSet, for proper processing of the data\n#'\n#' @importClassesFrom CoreGx CoreSet\n#' @importClassesFrom CoreGx LongTable\n#' @importClassesFrom CoreGx TreatmentResponseExperiment\n#'\n#' @return An object of the PharmacoSet class\n.PharmacoSet <- setClass('PharmacoSet',\n    contains='CoreSet')\n\n\n# The default constructor above does a poor job of explaining the required\n# structure of a PharmacoSet. The constructor function defined below guides the\n# user into providing the required components of the curation and senstivity\n# lists and hides the annotation slot which the user does not need to manually\n# fill. This also follows the design of the Expression Set class.\n\n#####\n# CONSTRUCTOR -----\n#####\n\n#' PharmacoSet constructor\n#'\n#' A constructor that simplifies the process of creating PharmacoSets, as well\n#' as creates empty objects for data not provided to the constructor. Only\n#' objects returned by this constructor are expected to work with the PharmacoSet\n#' methods. For a much more detailed instruction on creating PharmacoSets, please\n#' see the \"CreatingPharmacoSet\" vignette.\n#'\n#' @examples\n#' ## For help creating a PharmacoSet object, please see the following vignette:\n#' browseVignettes(\"PharmacoGx\")\n#'\n#' @inheritParams CoreGx::CoreSet\n#'\n#' @return An object of class `PharmacoSet`\n#\n#' @import methods\n#' @importFrom utils sessionInfo\n#' @importFrom stats na.omit\n#' @importFrom SummarizedExperiment rowData colData assay assays assayNames Assays\n#' @importFrom S4Vectors DataFrame SimpleList metadata\n#' @importFrom CoreGx CoreSet\n#'\n#' @export\nPharmacoSet <-  function(name, molecularProfiles=list(), sample=data.frame(),\n        treatment=data.frame(), sensitivityInfo=data.frame(),\n        sensitivityRaw=array(dim=c(0,0,0)), sensitivityProfiles=matrix(),\n        sensitivityN=matrix(nrow=0, ncol=0), perturbationN=array(NA, dim=c(0,0,0)),\n        curationTreatment=data.frame(), curationSample = data.frame(),\n        curationTissue = data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n        verify = TRUE, ...) {\n\n    #.Deprecated(\"PharmacoSet2\", )\n\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        molecularProfiles = molecularProfiles,\n        sample=sample,\n        treatment=treatment,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify,\n        ...\n    )\n\n    pSet  <- .PharmacoSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) checkPsetStructure(pSet)\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n        pSet@treatmentResponse$n <- .summarizeSensitivityNumbers(pSet)\n    }\n    if (!length(perturbationN) &&\n            datasetType %in% c(\"perturbation\", \"both\")) {\n        pSet@perturbation$n <- .summarizePerturbationNumbers(pSet)\n    }\n    return(pSet)\n}\n\n#' @eval CoreGx:::.docs_CoreSet2_constructor(class_=.local_class,\n#' sx_=\"Samples in a `PharmacoSet` represent cancer cell-lines.\",\n#' tx_=\"Treatments in a `PharmacoSet` represent pharmaceutical compounds.\",\n#' cx_=\"This class requires an additional curation item, tissue, which maps\n#' from published to standardized tissue idenifiers.\",\n#' data_=.local_data)\n#' @importFrom CoreGx CoreSet2 LongTable TreatmentResponseExperiment\n#' @export\nPharmacoSet2 <- function(name=\"emptySet\", treatment=data.frame(),\n        sample=data.frame(), molecularProfiles=MultiAssayExperiment(),\n        treatmentResponse=TreatmentResponseExperiment(),\n        perturbation=list(),\n        curation=list(sample=data.frame(), treatment=data.frame(),\n        tissue=data.frame()), datasetType=\"sensitivity\"\n) {\n    # -- Leverage existing checks in CoreSet constructor\n    cSet <- CoreSet2(name=name, treatment=treatment,\n        sample=sample, treatmentResponse=treatmentResponse,\n        molecularProfiles=molecularProfiles, curation=curation,\n        perturbation=perturbation, datasetType=datasetType)\n\n    ## -- data integrity\n    # treatment\n    ## TODO\n\n    .PharmacoSet(\n        annotation=cSet@annotation,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        molecularProfiles=cSet@molecularProfiles,\n        treatmentResponse=cSet@treatmentResponse,\n        datasetType=cSet@datasetType,\n        curation=cSet@curation,\n        perturbation=cSet@perturbation\n    )\n}\n\n# Constructor Helper Functions ----------------------------------------------\n\n#' @keywords internal\n#' @importFrom CoreGx idCols . .errorMsg .collapse\n.summarizeSensitivityNumbers <- function(object) {\n    ## TODO:: Checks don't like assigning to global evnironment. Can we return this?\n    assign('object_sumSenNum', object) # Removed envir=.GlobalEnv\n    if (datasetType(object) != 'sensitivity' && datasetType(object) != 'both') {\n        stop ('Data type must be either sensitivity or both')\n    }\n    ## consider all drugs\n    drugn <- treatmentNames(object)\n    ## consider all cell lines\n    celln <- sampleNames(object)\n    sensitivity.info <- matrix(0, nrow=length(celln), ncol=length(drugn),\n        dimnames=list(celln, drugn))\n    drugids <- sensitivityInfo(object)[ , \"treatmentid\"]\n    sampleids <- sensitivityInfo(object)[ , \"sampleid\"]\n    sampleids <- sampleids[grep('///', drugids, invert=TRUE)]\n    drugids <- drugids[grep('///', drugids, invert=TRUE)]\n    tt <- table(sampleids, drugids)\n    sensitivity.info[rownames(tt), colnames(tt)] <- tt\n\n    return(sensitivity.info)\n}\n\n#' @importFrom CoreGx .summarizeMolecularNumbers\n.summarizeMolecularNumbers <- function(object) {\n    CoreGx::.summarizeMolecularNumbers(object)\n}\n\n#' @importFrom CoreGx treatmentNames sampleNames\n.summarizePerturbationNumbers <- function(object) {\n\n    if (datasetType(object) != 'perturbation' && datasetType(object) != 'both') {\n        stop('Data type must be either perturbation or both')\n    }\n\n    ## consider all drugs\n    drugn <- treatmentNames(object)\n\n    ## consider all cell lines\n    celln <- sampleNames(object)\n\n    mprof <- molecularProfilesSlot(object)\n    perturbation.info <- array(0, dim=c(length(celln), length(drugn),\n        length(mprof)),\n        dimnames=list(celln, drugn, names(mprof))\n    )\n    for (i in seq_len(length(mprof))) {\n        if (nrow(colData(mprof[[i]])) > 0 &&\n                all(c(\"sampleid\", \"treatmentid\") %in%\n                    colnames(mprof[[i]]))) {\n            tt <- table(\n                colData(mprof[[i]])[, \"sampleid\"],\n                colData(mprof[[i]])[, \"treatmentid\"]\n            )\n        perturbation.info[rownames(tt), colnames(tt), names(mprof)[i]] <- tt\n        }\n    }\n\n    return(perturbation.info)\n}\n\n### -------------------------------------------------------------------------\n### Class Validity ----------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' A function to verify the structure of a PharmacoSet\n#'\n#' This function checks the structure of a PharamcoSet, ensuring that the\n#' correct annotations are in place and all the required slots are filled so\n#' that matching of cells and drugs can be properly done across different types\n#' of data and with other studies.\n#'\n#' @examples\n#' data(CCLEsmall)\n#' checkPsetStructure(CCLEsmall)\n#'\n#' @param object A \\code{PharmacoSet} to be verified\n#' @param plotDist Should the function also plot the distribution of molecular data?\n#' @param result.dir The path to the directory for saving the plots as a string\n#'\n#' @return Prints out messages whenever describing the errors found in the\n#'   structure of the object object passed in.\n#'\n#' @importFrom graphics hist\n#' @importFrom grDevices dev.off pdf\n#'\n#' @export\ncheckPsetStructure <-\n  function(object, plotDist=FALSE, result.dir='.') {\n\n    # Make directory to store results if it doesn't exist\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n\n    #####\n    # Checking molecularProfiles\n    #####\n    # Can this be parallelized or does it mess with the order of printing warnings?\n    mprof <- molecularProfilesSlot(object)\n    for( i in seq_along(mprof)) {\n      profile <- mprof[[i]]\n      nn <- names(mprof)[i]\n\n      # Testing plot rendering for rna and rnaseq\n      if((S4Vectors::metadata(profile)$annotation == 'rna' || S4Vectors::metadata(profile)$annotation == 'rnaseq') && plotDist)\n      {\n        pdf(file=file.path(result.dir, sprintf('%s.pdf', nn)))\n        hist(assays(profile)[[1]], breaks = 100)\n        dev.off()\n      }\n\n      ## Test if sample and feature annotations dimensions match the assay\n      warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                     sprintf('%s: number of features in fData is different from\n                             SummarizedExperiment slots', nn),\n                     sprintf('%s: rowData dimension is OK', nn)\n                     )\n              )\n      warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                     sprintf('%s: number of cell lines in pData is different\n                             from expression slots', nn),\n                     sprintf('%s: colData dimension is OK', nn)\n                     )\n              )\n\n\n      # Checking sample metadata for required columns\n      warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), '',\n                     sprintf('%s: sampleid does not exist in colData (samples)\n                             columns', nn)))\n      warning(ifelse('batchid' %in% colnames(colData(profile)), '',\n                     sprintf('%s: batchid does not exist in colData (samples)\n                             columns', nn)))\n\n      # Checking mDataType of the SummarizedExperiment for required columns\n      if(S4Vectors::metadata(profile)$annotation == 'rna' |\n         S4Vectors::metadata(profile)$annotation == 'rnaseq')\n      {\n        warning(ifelse('BEST' %in% colnames(rowData(profile)), 'BEST is OK',\n                       sprintf('%s: BEST does not exist in rowData (features)\n                               columns', nn)))\n        warning(ifelse('Symbol' %in% colnames(rowData(profile)), 'Symbol is OK',\n                       sprintf('%s: Symbol does not exist in rowData (features)\n                               columns', nn)))\n      }\n\n      # Check that all sampleids from the object are included in molecularProfiles\n      if(\"sampleid\" %in% colnames(colData(profile))) {\n        if (!all(colData(profile)[,\"sampleid\"] %in% sampleNames(object))) {\n          warning(sprintf('%s: not all the cell lines in this profile are in\n                          cell lines slot', nn))\n        }\n      }else {\n        warning(sprintf('%s: sampleid does not exist in colData (samples)', nn))\n      }\n    }\n\n}\n\n\n### -------------------------------------------------------------------------\n### Method Definitions ------------------------------------------------------\n### -------------------------------------------------------------------------\n\n#' Show a PharamcoSet\n#'\n#' @param object \\code{PharmacoSet}\n#'\n#' @examples\n#' data(CCLEsmall)\n#' CCLEsmall\n#'\n#' @return Prints the PharmacoSet object to the output stream, and returns\n#'   invisible NULL.\n#'\n#'  @importFrom CoreGx show\n#'  @importFrom methods callNextMethod\n#'\n#' @export\nsetMethod('show', signature=signature(object='PharmacoSet'), function(object) {\n    callNextMethod(object)\n})\n\n#' Get the dimensions of a PharmacoSet\n#'\n#' @param x PharmacoSet\n#' @return A named vector with the number of Cells and Drugs in the PharmacoSet\n#' @export\nsetMethod('dim', signature=signature(x='PharmacoSet'), function(x){\n    return(c(Cells=length(sampleNames(x)), Drugs=length(treatmentNames(x))))\n})\n\n\n### TODO:: Add updating of sensitivity Number tables\n#' @importFrom CoreGx updateSampleId\n#' @aliases updateCellId\nupdateSampleId <- updateCellId <- function(object, new.ids = vector('character')){\n    CoreGx::updateSampleId(object, new.ids)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `PharmacoSet` class and what types of data can it accommodate?",
        "answer": "The `PharmacoSet` class is designed to contain and organize large pharmacogenomic datasets. It can accommodate two types of datasets: 1) drug dose response data (sensitivity datasets), and 2) genetic profiles of cell lines pre and post treatment with compounds (perturbation datasets). The class provides a common interface for working with data at the level of genes, drugs, and cell lines, simplifying analyses between several datasets."
      },
      {
        "question": "What are the key slots in the `PharmacoSet` class and what do they contain?",
        "answer": "The key slots in the `PharmacoSet` class include: 1) `annotation`: a list of metadata about the PharmacoSet, 2) `molecularProfiles`: a list containing `SummarizedExperiment` objects for RNA, DNA, SNP, and CNV measurements, 3) `sample`: a data frame with annotations for all cell lines, 4) `treatment`: a data frame with annotations for all drugs, 5) `treatmentResponse`: a list containing sensitivity experiment data, 6) `perturbation`: a list containing perturbation data, 7) `curation`: a list of mappings for treatment, cell, and tissue names, and 8) `datasetType`: a character string specifying the type of data in the PharmacoSet."
      },
      {
        "question": "How does the `PharmacoSet` constructor function simplify the process of creating PharmacoSet objects?",
        "answer": "The `PharmacoSet` constructor function simplifies the creation of PharmacoSet objects by: 1) Guiding the user to provide the required components of the curation and sensitivity lists, 2) Hiding the annotation slot which the user doesn't need to manually fill, 3) Creating empty objects for data not provided to the constructor, 4) Performing structure verification if the `verify` parameter is set to `TRUE`, and 5) Automatically summarizing sensitivity and perturbation numbers if not provided. This approach ensures that only objects returned by this constructor are expected to work with PharmacoSet methods."
      }
    ],
    "completion_tasks": [
      {
        "partial": "PharmacoSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n        treatment=data.frame(), sensitivityInfo=data.frame(),\n        sensitivityRaw=array(dim=c(0,0,0)), sensitivityProfiles=matrix(),\n        sensitivityN=matrix(nrow=0, ncol=0), perturbationN=array(NA, dim=c(0,0,0)),\n        curationTreatment=data.frame(), curationSample = data.frame(),\n        curationTissue = data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n        verify = TRUE, ...) {\n\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        molecularProfiles = molecularProfiles,\n        sample=sample,\n        treatment=treatment,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify,\n        ...\n    )\n\n    pSet  <- .PharmacoSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) checkPsetStructure(pSet)\n    # Complete the function here\n}",
        "complete": "PharmacoSet <- function(name, molecularProfiles=list(), sample=data.frame(),\n        treatment=data.frame(), sensitivityInfo=data.frame(),\n        sensitivityRaw=array(dim=c(0,0,0)), sensitivityProfiles=matrix(),\n        sensitivityN=matrix(nrow=0, ncol=0), perturbationN=array(NA, dim=c(0,0,0)),\n        curationTreatment=data.frame(), curationSample = data.frame(),\n        curationTissue = data.frame(), datasetType=c(\"sensitivity\", \"perturbation\", \"both\"),\n        verify = TRUE, ...) {\n\n    cSet <- CoreGx::CoreSet(\n        name=name,\n        molecularProfiles = molecularProfiles,\n        sample=sample,\n        treatment=treatment,\n        sensitivityInfo=sensitivityInfo,\n        sensitivityRaw=sensitivityRaw,\n        sensitivityProfiles=sensitivityProfiles,\n        sensitivityN=sensitivityN,\n        perturbationN=perturbationN,\n        curationTreatment=curationTreatment,\n        curationSample=curationSample,\n        curationTissue=curationTissue,\n        datasetType=datasetType,\n        verify=verify,\n        ...\n    )\n\n    pSet  <- .PharmacoSet(\n        annotation=cSet@annotation,\n        molecularProfiles=cSet@molecularProfiles,\n        sample=cSet@sample,\n        treatment=cSet@treatment,\n        datasetType=cSet@datasetType,\n        treatmentResponse=cSet@treatmentResponse,\n        perturbation=cSet@perturbation,\n        curation=cSet@curation\n    )\n    if (verify) checkPsetStructure(pSet)\n    if (length(sensitivityN) == 0 && datasetType %in% c(\"sensitivity\", \"both\")) {\n        pSet@treatmentResponse$n <- .summarizeSensitivityNumbers(pSet)\n    }\n    if (!length(perturbationN) &&\n            datasetType %in% c(\"perturbation\", \"both\")) {\n        pSet@perturbation$n <- .summarizePerturbationNumbers(pSet)\n    }\n    return(pSet)\n}"
      },
      {
        "partial": "checkPsetStructure <- function(object, plotDist=FALSE, result.dir='.') {\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n    mprof <- molecularProfilesSlot(object)\n    for( i in seq_along(mprof)) {\n        profile <- mprof[[i]]\n        nn <- names(mprof)[i]\n        if((S4Vectors::metadata(profile)$annotation == 'rna' || S4Vectors::metadata(profile)$annotation == 'rnaseq') && plotDist) {\n            pdf(file=file.path(result.dir, sprintf('%s.pdf', nn)))\n            hist(assays(profile)[[1]], breaks = 100)\n            dev.off()\n        }\n        warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                        sprintf('%s: number of features in fData is different from SummarizedExperiment slots', nn),\n                        sprintf('%s: rowData dimension is OK', nn)))\n        warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                        sprintf('%s: number of cell lines in pData is different from expression slots', nn),\n                        sprintf('%s: colData dimension is OK', nn)))\n        warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), '',\n                        sprintf('%s: sampleid does not exist in colData (samples) columns', nn)))\n        warning(ifelse('batchid' %in% colnames(colData(profile)), '',\n                        sprintf('%s: batchid does not exist in colData (samples) columns', nn)))\n        # Complete the function here\n    }\n}",
        "complete": "checkPsetStructure <- function(object, plotDist=FALSE, result.dir='.') {\n    if(!file.exists(result.dir) & plotDist) { dir.create(result.dir, showWarnings=FALSE, recursive=TRUE) }\n    mprof <- molecularProfilesSlot(object)\n    for( i in seq_along(mprof)) {\n        profile <- mprof[[i]]\n        nn <- names(mprof)[i]\n        if((S4Vectors::metadata(profile)$annotation == 'rna' || S4Vectors::metadata(profile)$annotation == 'rnaseq') && plotDist) {\n            pdf(file=file.path(result.dir, sprintf('%s.pdf', nn)))\n            hist(assays(profile)[[1]], breaks = 100)\n            dev.off()\n        }\n        warning(ifelse(nrow(rowData(profile)) != nrow(assays(profile)[[1]]),\n                        sprintf('%s: number of features in fData is different from SummarizedExperiment slots', nn),\n                        sprintf('%s: rowData dimension is OK', nn)))\n        warning(ifelse(nrow(colData(profile)) != ncol(assays(profile)[[1]]),\n                        sprintf('%s: number of cell lines in pData is different from expression slots', nn),\n                        sprintf('%s: colData dimension is OK', nn)))\n        warning(ifelse(\"sampleid\" %in% colnames(colData(profile)), '',\n                        sprintf('%s: sampleid does not exist in colData (samples) columns', nn)))\n        warning(ifelse('batchid' %in% colnames(colData(profile)), '',\n                        sprintf('%s: batchid does not exist in colData (samples) columns', nn)))\n        if(S4Vectors::metadata(profile)$annotation == 'rna' | S4Vectors::metadata(profile)$annotation == 'rnaseq') {\n            warning(ifelse('BEST' %in% colnames(rowData(profile)), 'BEST is OK',\n                            sprintf('%s: BEST does not exist in rowData (features) columns', nn)))\n            warning(ifelse('Symbol' %in% colnames(rowData(profile)), 'Symbol is OK',\n                            sprintf('%s: Symbol does not exist in rowData (features) columns', nn)))\n        }\n        if(\"sampleid\" %in% colnames(colData(profile))) {\n            if (!all(colData(profile)[,\"sampleid\"] %in% sampleNames(object))) {\n                warning(sprintf('%s: not all the cell lines in this profile are in cell lines slot', nn))\n            }\n        } else {\n            warning(sprintf('%s: sampleid does not exist in colData (samples)', nn))\n        }\n    }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_sanitizeInput.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking the sanitization of input to curve fitting and sensitivity summary funcitons\")\n\ntest_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(100,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70),\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70)/100,\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = FALSE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40)/100,\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE, trunc = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(110,90,80,70,60,50,40)/100))\n})\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "test_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  # Complete the test cases here\n})",
        "complete": "test_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(100,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70),\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70)/100,\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = FALSE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40)/100,\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE, trunc = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(110,90,80,70,60,50,40)/100))\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking the sanitization of input to curve fitting and sensitivity summary funcitons\")\n\ntest_that(\"Function sanitizeInput handles no input correctly.\", {\n  # Add test case here\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  # Add test cases here\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  # Add test cases here\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking the sanitization of input to curve fitting and sensitivity summary funcitons\")\n\ntest_that(\"Function sanitizeInput handles no input correctly.\", {\n  expect_error(sanitizeInput(), \"Both 'Hill_fit' and 'viability'\")\n})\n\ntest_that(\"Function sanitizeInput yells at user sufficiently.\", {\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1),viability=c(100,90,80,70,60,50,40),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_error(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70),conc_as_log = FALSE,viability_as_pct = TRUE, verbose=TRUE))\n  expect_warning(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40),conc_as_log = TRUE,viability_as_pct = FALSE, verbose=TRUE), \"'viability_as_pct' flag may be set incorrectly\")\n})\n\ntest_that(\"Function sanitizeInput returns correct values.\", {\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(100,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70),\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = TRUE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(0,1,2,3),\n                                          viability=c(100,90,80,70)/100,\n                                          conc_as_log = FALSE,\n                                          viability_as_pct = FALSE,\n                                          verbose=TRUE), list(log_conc=log10(c(1,2,3)),viability=c(90,80,70)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40)/100,\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(100,90,80,70,60,50,40)/100))\n  expect_equal(PharmacoGx:::sanitizeInput(conc=c(-3,-2,-1,0,1,2,3),\n                                          viability=c(110,90,80,70,60,50,40),\n                                          conc_as_log = TRUE,\n                                          viability_as_pct = TRUE, trunc = FALSE,\n                                          verbose=FALSE), list(log_conc=c(-3,-2,-1,0,1,2,3),viability=c(110,90,80,70,60,50,40)/100))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/tests/test_loaders.py",
    "language": "py",
    "content": "from readii.loaders import *\nimport pytest\n\n@pytest.fixture\ndef nsclcCTPath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/3.000000-THORAX_1.0_B45f-95741\"\n\n@pytest.fixture\ndef nsclcSEGPath():\n    return \"tests/NSCLC_Radiogenomics/R01-001/09-06-1990-NA-CT_CHEST_ABD_PELVIS_WITH_CON-98785/1000.000000-3D_Slicer_segmentation_result-67652/1-1.dcm\"\n\n@pytest.fixture\ndef lung4DCTPath():\n    return \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-29543\"\n\n@pytest.fixture\ndef lung4DRTSTRUCTPath():\n    return \"tests/4D-Lung/113_HM10395/11-26-1999-NA-p4-13296/1.000000-P4P113S303I10349 Gated 40.0B-47.35/1-1.dcm\"\n\n\ndef test_loadDicomSITK(nsclcCTPath):\n    \"\"\"Test loading DICOM from directory.\"\"\"\n    actual = loadDicomSITK(nsclcCTPath)\n    assert isinstance(actual, sitk.Image), \\\n        \"Wrong object type\"\n    assert actual.GetSize() == (512, 512, 304), \\\n        \"Wrong image size\"\n    assert actual.GetSpacing() == (0.693359375, 0.693359375, 1.0), \\\n        \"Wrong spacing\"\n    assert actual.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0), \\\n        \"Wrong origin\"\n\n\ndef test_loadSegmentationSEG(nsclcSEGPath):\n    \"\"\"Test loading a DICOM SEG file\"\"\"\n    actual = loadSegmentation(segImagePath = nsclcSEGPath,\n                              modality = 'SEG')\n\n    assert isinstance(actual, dict), \\\n        \"Wrong object type, should be dictionary\"\n    assert list(actual.keys()) == ['Heart'], \\\n        \"Segmentation label is wrong, should be Heart\"\n\n    actualImage = actual['Heart']\n\n    assert isinstance(actualImage, sitk.Image), \\\n        \"Wrong object type\"\n    assert actualImage.GetSize() == (512, 512, 304, 1), \\\n        \"Wrong image size\"\n    assert actualImage.GetSpacing() == (0.693359375, 0.693359375, 1.0, 1.0), \\\n        \"Wrong spacing\"\n    assert actualImage.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0, 0.0), \\\n        \"Wrong origin\"\n\n\ndef test_loadSegmentationRTSTRUCT(lung4DRTSTRUCTPath, lung4DCTPath):\n    \"\"\"Test loading a RTSTRUCT file\"\"\"\n    actual = loadSegmentation(segImagePath = lung4DRTSTRUCTPath,\n                              modality = 'RTSTRUCT',\n                              baseImageDirPath = lung4DCTPath,\n                              roiNames = 'Tumor_c.*')\n\n    assert isinstance(actual, dict), \\\n        \"Wrong object type, should be dictionary\"\n    assert list(actual.keys()) == ['Tumor_c40'], \\\n        \"Segmentation label is wrong, should be Heart\"\n    \n    actualImage = actual['Tumor_c40']\n\n    assert isinstance(actualImage, sitk.Image), \\\n        \"Wrong object type\"\n    assert actualImage.GetSize() == (512, 512, 99), \\\n        \"Wrong image size\"\n    assert actualImage.GetSpacing() == (0.9766, 0.9766, 3.0), \\\n        \"Wrong spacing\"\n    assert actualImage.GetOrigin() == (-250.0, -163.019, -1132.0), \\\n        \"Wrong origin\"\n\n\ndef test_loadSegmentation_error(nsclcSEGPath):\n    \"\"\"Check ValueError raised when wrong segmentation type is passed\"\"\"\n    with pytest.raises(ValueError):\n        loadSegmentation(segImagePath = nsclcSEGPath,\n                         modality = 'CT')",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@pytest.fixture` decorators in this code, and how are they used in the test functions?",
        "answer": "The `@pytest.fixture` decorators are used to define reusable test fixtures. These fixtures provide paths to different types of medical imaging files (CT scans and segmentation files) that are used in the test functions. The fixtures are injected into the test functions as arguments, allowing easy access to the file paths without hardcoding them in each test. This promotes code reusability and makes it easier to manage test data across multiple test functions."
      },
      {
        "question": "In the `test_loadDicomSITK` function, what assertions are made to verify the correctness of the loaded DICOM image?",
        "answer": "The `test_loadDicomSITK` function makes four assertions to verify the correctness of the loaded DICOM image:\n1. It checks if the returned object is an instance of `sitk.Image`.\n2. It verifies that the image size is (512, 512, 304).\n3. It checks if the image spacing is (0.693359375, 0.693359375, 1.0).\n4. It ensures that the image origin is (-182.1533203125, -314.1533203125, -305.0).\nThese assertions validate the type, dimensions, spatial properties, and positioning of the loaded image."
      },
      {
        "question": "How does the `loadSegmentation` function handle different types of segmentation files, and what error checking is implemented?",
        "answer": "The `loadSegmentation` function handles different types of segmentation files through the `modality` parameter. It supports 'SEG' and 'RTSTRUCT' modalities, as seen in the `test_loadSegmentationSEG` and `test_loadSegmentationRTSTRUCT` functions. The function returns a dictionary containing the segmentation labels as keys and corresponding `sitk.Image` objects as values. Error checking is implemented in the `test_loadSegmentation_error` function, which verifies that a `ValueError` is raised when an unsupported modality (e.g., 'CT') is passed to the `loadSegmentation` function. This ensures that the function properly handles invalid input and provides appropriate error messages."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def test_loadDicomSITK(nsclcCTPath):\n    actual = loadDicomSITK(nsclcCTPath)\n    assert isinstance(actual, sitk.Image)\n    # Add assertions for size, spacing, and origin",
        "complete": "def test_loadDicomSITK(nsclcCTPath):\n    actual = loadDicomSITK(nsclcCTPath)\n    assert isinstance(actual, sitk.Image), \"Wrong object type\"\n    assert actual.GetSize() == (512, 512, 304), \"Wrong image size\"\n    assert actual.GetSpacing() == (0.693359375, 0.693359375, 1.0), \"Wrong spacing\"\n    assert actual.GetOrigin() == (-182.1533203125, -314.1533203125, -305.0), \"Wrong origin\""
      },
      {
        "partial": "def test_loadSegmentation_error(nsclcSEGPath):\n    # Implement test to check if ValueError is raised\n    # when wrong segmentation type is passed",
        "complete": "def test_loadSegmentation_error(nsclcSEGPath):\n    with pytest.raises(ValueError):\n        loadSegmentation(segImagePath=nsclcSEGPath, modality='CT')"
      }
    ],
    "dependencies": {
      "imports": [
        "pytest"
      ],
      "from_imports": [
        "readii.loaders.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-standardize_names.R",
    "language": "R",
    "content": "#' Standardize Names\n#'\n#' This function takes a character vector and standardizes the names by converting them to lowercase,\n#' removing any trailing information after a comma, removing any information within square brackets or parentheses,\n#' removing any non-alphanumeric characters, replacing empty names with \"invalid\", and converting the names to uppercase.\n#'\n#' @param object A character vector containing the names to be standardized.\n#' @return A character vector with the standardized names.\n#' @examples\n#' standardize_names(c(\"John Doe\", \"Jane Smith (Manager)\", \"Alice, PhD\"))\n#' # Output: [1] \"JOHNDOE\" \"JANESMITH\" \"ALICE\"\n#' @export\nstandardize_names <- function(object) {\n  checkmate::assert_character(object, all.missing = F)\n  object <- tolower(object)\n  object <- gsub(\n    pattern = \",\\\\s.+$\",\n    replacement = \"\",\n    x = object\n  )\n  object <- sub(\n    pattern = \"\\\\s[\\\\[\\\\(].+$\",\n    replacement = \"\",\n    x = object\n  )\n  object <- gsub(\n    pattern = \"[^[:alnum:]]+\",\n    replacement = \"\",\n    x = object\n  )\n  if (any(object == \"\")) {\n    object[object == \"\"] <- NA\n  }\n  object <- toupper(object)\n  object\n}\n\n\n#' Clean character strings by removing special characters and formatting.\n#'\n#' This function takes a character string as input and performs several cleaning operations\n#' to remove special characters, formatting, and unwanted substrings. The cleaned string\n#' is then returned as the output.\n#'\n#' @param name A character string to be cleaned.\n#' @param space_action A character vector specifying the actions to be taken for space characters.\n#'                     One of c(\"\", \"-\", \" \").\n#' @return The cleaned character string.\n#'\n#' @examples\n#' cleanCharacterStrings(\"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\")\n#'\n#' @export\ncleanCharacterStrings <- function(name, space_action = \"\") {\n\n  # make sure name is a string\n  name <- as.character(name)\n\n  # replace space characters based on space_action\n  if (space_action == \"-\") {\n    name <- gsub(\" \", \"-\", name)\n  } else if (space_action == \" \") {\n    name <- gsub(\" \", \" \", name)\n  }else{\n    name <- gsub(\" \", \"\", name)\n  }\n\n  # remove the ~ character\n  name <- gsub(\"~\", \"\", name)\n\n  # if there is a colon like in \"Cisplatin: 1 mg/mL (1.5 mM); 5 mM in DMSO\"\n  # remove everything after the colon\n  name <- gsub(\":.*\", \"\", name)\n\n  # remove ,  ;  -  +  *  $  %  #  ^  _  as well as any spaces\n  name <- gsub(\"[\\\\,\\\\;\\\\+\\\\*\\\\$\\\\%\\\\#\\\\^\\\\_]\", \"\", name, perl = TRUE)\n\n  # remove hyphen \n  if (!space_action == \"-\")  name <- gsub(\"-\", \"\", name)\n\n  # remove substring of round brackets and contents\n  name <- gsub(\"\\\\s*\\\\(.*\\\\)\", \"\", name)\n\n  # remove substring of square brackets and contents\n  name <- gsub(\"\\\\s*\\\\[.*\\\\]\", \"\", name)\n\n  # remove substring of curly brackets and contents\n  name <- gsub(\"\\\\s*\\\\{.*\\\\}\", \"\", name)\n\n\n\n  # convert entire string to uppercase\n  name <- toupper(name)\n\n  # dealing with unicode characters \n  name <- gsub(\"Unicode\", \"\", iconv(name, \"LATIN1\", \"ASCII\", \"Unicode\"), perl=TRUE)\n\n  name\n}\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `standardize_names` function, and how does it handle empty names?",
        "answer": "The `standardize_names` function standardizes a character vector of names by converting them to lowercase, removing trailing information after commas, removing information within brackets or parentheses, removing non-alphanumeric characters, and converting the names to uppercase. Empty names are replaced with NA values."
      },
      {
        "question": "How does the `cleanCharacterStrings` function handle spaces in the input string, and what are the possible options for the `space_action` parameter?",
        "answer": "The `cleanCharacterStrings` function handles spaces based on the `space_action` parameter. The possible options are: '' (remove spaces), '-' (replace spaces with hyphens), and ' ' (keep spaces as is). If no `space_action` is specified, spaces are removed by default."
      },
      {
        "question": "What are the main differences between the `standardize_names` and `cleanCharacterStrings` functions in terms of their approach to cleaning strings?",
        "answer": "The main differences are: 1) `standardize_names` focuses on name standardization, while `cleanCharacterStrings` is more general-purpose. 2) `standardize_names` always converts to uppercase, while `cleanCharacterStrings` allows for different space handling options. 3) `cleanCharacterStrings` removes more types of brackets and special characters, and handles Unicode characters, which `standardize_names` does not."
      }
    ],
    "completion_tasks": [
      {
        "partial": "cleanCharacterStrings <- function(name, space_action = \"\") {\n  name <- as.character(name)\n  if (space_action == \"-\") {\n    name <- gsub(\" \", \"-\", name)\n  } else if (space_action == \" \") {\n    name <- gsub(\" \", \" \", name)\n  } else {\n    name <- gsub(\" \", \"\", name)\n  }\n  name <- gsub(\"~\", \"\", name)\n  name <- gsub(\":.*\", \"\", name)\n  # Complete the function by adding code to remove special characters,\n  # brackets and their contents, and convert to uppercase\n}",
        "complete": "cleanCharacterStrings <- function(name, space_action = \"\") {\n  name <- as.character(name)\n  if (space_action == \"-\") {\n    name <- gsub(\" \", \"-\", name)\n  } else if (space_action == \" \") {\n    name <- gsub(\" \", \" \", name)\n  } else {\n    name <- gsub(\" \", \"\", name)\n  }\n  name <- gsub(\"~\", \"\", name)\n  name <- gsub(\":.*\", \"\", name)\n  name <- gsub(\"[\\\\,\\\\;\\\\+\\\\*\\\\$\\\\%\\\\#\\\\^\\\\_]\", \"\", name, perl = TRUE)\n  if (!space_action == \"-\") name <- gsub(\"-\", \"\", name)\n  name <- gsub(\"\\\\s*\\\\(.*\\\\)\", \"\", name)\n  name <- gsub(\"\\\\s*\\\\[.*\\\\]\", \"\", name)\n  name <- gsub(\"\\\\s*\\\\{.*\\\\}\", \"\", name)\n  name <- toupper(name)\n  name <- gsub(\"Unicode\", \"\", iconv(name, \"LATIN1\", \"ASCII\", \"Unicode\"), perl=TRUE)\n  name\n}"
      },
      {
        "partial": "standardize_names <- function(object) {\n  checkmate::assert_character(object, all.missing = F)\n  object <- tolower(object)\n  object <- gsub(\",\\\\s.+$\", \"\", object)\n  # Complete the function by adding code to remove information within brackets,\n  # remove non-alphanumeric characters, replace empty names with NA, and convert to uppercase\n}",
        "complete": "standardize_names <- function(object) {\n  checkmate::assert_character(object, all.missing = F)\n  object <- tolower(object)\n  object <- gsub(\",\\\\s.+$\", \"\", object)\n  object <- sub(\"\\\\s[\\\\[\\\\(].+$\", \"\", object)\n  object <- gsub(\"[^[:alnum:]]+\", \"\", object)\n  object[object == \"\"] <- NA\n  toupper(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_view.R",
    "language": "R",
    "content": "#' Get annotation headings (name only) based on type and heading criteria.\n#'\n#' @param type The type of annotation headings to retrieve.\n#' Options include \"Compound\", \"Gene\", \"Taxonomy\", \"Element\", \"Assay\", \"Protein\", \"Cell\", \"Pathway\", or \"all\" (default).\n#' @param heading The specific heading to filter the results by. Defaults to NULL, which retrieves all headings.\n#'\n#' @return A `data.table` containing the annotation headings and types.\n#'\n#' @examples\n#' getPubchemAnnotationHeadings()\n#' getPubchemAnnotationHeadings(type = \"Compound\")\n#' getPubchemAnnotationHeadings(heading = \"ChEMBL*\")\n#' getPubchemAnnotationHeadings(type = \"Compound\", heading = \"ChEMBL*\")\n#'\n#' @export\ngetPubchemAnnotationHeadings <- function(\n    type = \"all\", heading = NULL) {\n  funContext <- .funContext(\"getPubchemAnnotationHeadings\")\n\n  .debug(funContext, \" type: \", type, \" heading: \", heading)\n  # TODO:: messy...\n  checkmate::assert(\n    checkmate::test_choice(\n      tolower(type), tolower(c(\n        \"Compound\", \"Gene\", \"Taxonomy\", \"Element\",\n        \"Assay\", \"Protein\", \"Cell\", \"Pathway\"\n      ))\n    ) || type == \"all\"\n  )\n\n  ann_dt <- .get_all_heading_types()\n  .debug(funContext, \" ann_dt: \", utils::capture.output(utils::str(ann_dt)))\n  if (type != \"all\") {\n    ann_dt <- ann_dt[grepl(type, ann_dt$Type, ignore.case = T), ]\n  }\n  if (!is.null(heading)) {\n    ann_dt <- ann_dt[grepl(heading, ann_dt$Heading, ignore.case = F), ]\n  }\n\n  if (nrow(ann_dt) == 0) {\n    .warn(\n      funContext, \" No headings found for type: `\", type, \"` and heading: `\", heading,\n      \"`.\\nTry getPubchemAnnotationHeadings(type = 'all') for available headings and types\"\n    )\n  }\n  ann_dt\n}\n\n#' Annotate PubChem Compound\n#'\n#' This function retrieves information about a PubChem compound based on the provided compound ID (CID).\n#'\n#' @param cids The compound ID (CID) of the PubChem compound.\n#' @param heading The type of information to retrieve. Default is \"ChEMBL ID\".\n#' @param source The data source to use. Default is NULL.\n#' @param parse_function A custom parsing function to process the response. Default is the identity function.\n#' @param query_only Logical indicating whether to return the query URL only. Default is FALSE.\n#' @param raw Logical indicating whether to return the raw response. Default is FALSE.\n#' @param nParallel The number of parallel processes to use. Default is 1.\n#'\n#' @return The annotated information about the PubChem compound.\n#'\n#' @examples\n#' annotatePubchemCompound(cid = 2244)\n#' annotatePubchemCompound(cid = c(2244, 67890), heading = \"CAS\")\n#'\n#' @export\nannotatePubchemCompound <- function(\n    cids, heading = \"ChEMBL ID\", source = NULL, parse_function = identity,\n    query_only = FALSE, raw = FALSE, nParallel = 1\n  ) {\n  funContext <- .funContext(\"annotatePubchemCompound\")\n\n  .info(funContext, sprintf(\"Building requests for %s CIDs\", length(cids)))\n  requests <- lapply(cids, function(cid) {\n    .build_pubchem_view_query(\n      id = cid, record = \"compound\", heading = heading,\n      output = \"JSON\", source = source\n      )\n   }\n  )\n\n  .debug(funContext, paste0(\"query: \", sapply(requests, `[[`, i = \"url\")))\n  if (query_only) return(requests)\n\n  tryCatch({\n    resp_raw <- httr2::req_perform_sequential(\n      reqs = requests, \n      on_error = \"continue\",\n      progress = \"Performing API requests...\"\n  )}, error = function(e) {\n    .err(funContext, \"An error occurred while performing requests:\\n\", e)\n  })\n\n  if (raw) return(resp_raw)\n\n  responses <- lapply(seq_along(resp_raw), function(i){\n    resp <- resp_raw[[i]]\n    if(is.null(resp)) return(NA_character_)\n    tryCatch(\n      {\n        .parse_resp_json(resp)\n      },\n      error = function(e) {\n        warnmsg <- sprintf(\n          \"\\nThe response could not be parsed:\\n\\t%s\\tReturning NA instead for CID: %s for the heading: %s\",\n          e, cids[i], heading\n        )\n        .warn(\n          funContext, warnmsg\n        )\n        resp\n      }\n    )\n  })\n\n  # apply the parse function to each response depending on heading\n  parsed_responses <- parallel::mclapply(responses, function(response) {\n    switch(heading,\n      \"ChEMBL ID\" = .parseCHEMBLresponse(response),\n      \"CAS\" = .parseCASresponse(response),\n      \"NSC Number\" = .parseNSCresponse(response),\n      \"ATC Code\" = .parseATCresponse(response),\n      \"Drug Induced Liver Injury\" = .parseDILIresponse(response),\n      tryCatch(\n        {\n          parse_function(response)\n        },\n        error = function(e) {\n          .warn(\n            funContext, \"The parseFUN function failed: \", e,\n            \". Returning unparsed results instead. Please test the parseFUN\n                  on the returned data.\"\n          )\n          response\n        }\n      )\n    )\n  },\n  mc.cores = nParallel \n)\n  \n\n  sapply(parsed_responses, .replace_null)\n\n}\n\n# helper function to replace NULL with NA\n.replace_null <- function(x) {\n  ifelse(is.null(x), NA_character_, x)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemAnnotationHeadings` function and what are its main parameters?",
        "answer": "The `getPubchemAnnotationHeadings` function retrieves annotation headings based on type and heading criteria from PubChem. Its main parameters are `type` (default 'all'), which specifies the type of annotation headings to retrieve (e.g., 'Compound', 'Gene', 'Taxonomy'), and `heading` (default NULL), which allows filtering results by a specific heading."
      },
      {
        "question": "How does the `annotatePubchemCompound` function handle multiple compound IDs (CIDs) and what is the purpose of the `nParallel` parameter?",
        "answer": "The `annotatePubchemCompound` function can handle multiple CIDs by creating a list of requests using `lapply`. It then uses `httr2::req_perform_sequential` to execute these requests. The `nParallel` parameter is used in the `mclapply` function when parsing responses, allowing for parallel processing of the results to improve performance when dealing with multiple compounds."
      },
      {
        "question": "What is the purpose of the `parse_function` parameter in the `annotatePubchemCompound` function, and how does the function handle different types of headings?",
        "answer": "The `parse_function` parameter allows users to provide a custom parsing function to process the API response. The function handles different types of headings using a `switch` statement, which applies specific parsing functions (e.g., `.parseCHEMBLresponse`, `.parseCASresponse`) based on the heading. If the heading doesn't match any predefined cases, it applies the custom `parse_function` provided by the user, with error handling to return unparsed results if the custom function fails."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getPubchemAnnotationHeadings <- function(type = \"all\", heading = NULL) {\n  funContext <- .funContext(\"getPubchemAnnotationHeadings\")\n\n  .debug(funContext, \" type: \", type, \" heading: \", heading)\n  checkmate::assert(\n    checkmate::test_choice(\n      tolower(type), tolower(c(\n        \"Compound\", \"Gene\", \"Taxonomy\", \"Element\",\n        \"Assay\", \"Protein\", \"Cell\", \"Pathway\"\n      ))\n    ) || type == \"all\"\n  )\n\n  ann_dt <- .get_all_heading_types()\n  .debug(funContext, \" ann_dt: \", utils::capture.output(utils::str(ann_dt)))\n  if (type != \"all\") {\n    ann_dt <- ann_dt[grepl(type, ann_dt$Type, ignore.case = T), ]\n  }\n  if (!is.null(heading)) {\n    ann_dt <- ann_dt[grepl(heading, ann_dt$Heading, ignore.case = F), ]\n  }\n\n  # Complete the function by adding the warning message and returning ann_dt\n}",
        "complete": "getPubchemAnnotationHeadings <- function(type = \"all\", heading = NULL) {\n  funContext <- .funContext(\"getPubchemAnnotationHeadings\")\n\n  .debug(funContext, \" type: \", type, \" heading: \", heading)\n  checkmate::assert(\n    checkmate::test_choice(\n      tolower(type), tolower(c(\n        \"Compound\", \"Gene\", \"Taxonomy\", \"Element\",\n        \"Assay\", \"Protein\", \"Cell\", \"Pathway\"\n      ))\n    ) || type == \"all\"\n  )\n\n  ann_dt <- .get_all_heading_types()\n  .debug(funContext, \" ann_dt: \", utils::capture.output(utils::str(ann_dt)))\n  if (type != \"all\") {\n    ann_dt <- ann_dt[grepl(type, ann_dt$Type, ignore.case = T), ]\n  }\n  if (!is.null(heading)) {\n    ann_dt <- ann_dt[grepl(heading, ann_dt$Heading, ignore.case = F), ]\n  }\n\n  if (nrow(ann_dt) == 0) {\n    .warn(\n      funContext, \" No headings found for type: `\", type, \"` and heading: `\", heading,\n      \"`.\nTry getPubchemAnnotationHeadings(type = 'all') for available headings and types\"\n    )\n  }\n  ann_dt\n}"
      },
      {
        "partial": "annotatePubchemCompound <- function(cids, heading = \"ChEMBL ID\", source = NULL, parse_function = identity,\n    query_only = FALSE, raw = FALSE, nParallel = 1) {\n  funContext <- .funContext(\"annotatePubchemCompound\")\n\n  .info(funContext, sprintf(\"Building requests for %s CIDs\", length(cids)))\n  requests <- lapply(cids, function(cid) {\n    .build_pubchem_view_query(\n      id = cid, record = \"compound\", heading = heading,\n      output = \"JSON\", source = source\n    )\n  })\n\n  .debug(funContext, paste0(\"query: \", sapply(requests, `[[`, i = \"url\")))\n  if (query_only) return(requests)\n\n  # Complete the function by adding the API request, response parsing, and result processing\n}",
        "complete": "annotatePubchemCompound <- function(cids, heading = \"ChEMBL ID\", source = NULL, parse_function = identity,\n    query_only = FALSE, raw = FALSE, nParallel = 1) {\n  funContext <- .funContext(\"annotatePubchemCompound\")\n\n  .info(funContext, sprintf(\"Building requests for %s CIDs\", length(cids)))\n  requests <- lapply(cids, function(cid) {\n    .build_pubchem_view_query(\n      id = cid, record = \"compound\", heading = heading,\n      output = \"JSON\", source = source\n    )\n  })\n\n  .debug(funContext, paste0(\"query: \", sapply(requests, `[[`, i = \"url\")))\n  if (query_only) return(requests)\n\n  tryCatch({\n    resp_raw <- httr2::req_perform_sequential(\n      reqs = requests, \n      on_error = \"continue\",\n      progress = \"Performing API requests...\"\n    )\n  }, error = function(e) {\n    .err(funContext, \"An error occurred while performing requests:\\n\", e)\n  })\n\n  if (raw) return(resp_raw)\n\n  responses <- lapply(seq_along(resp_raw), function(i){\n    resp <- resp_raw[[i]]\n    if(is.null(resp)) return(NA_character_)\n    tryCatch(\n      {\n        .parse_resp_json(resp)\n      },\n      error = function(e) {\n        warnmsg <- sprintf(\n          \"\\nThe response could not be parsed:\\n\\t%s\\tReturning NA instead for CID: %s for the heading: %s\",\n          e, cids[i], heading\n        )\n        .warn(funContext, warnmsg)\n        resp\n      }\n    )\n  })\n\n  parsed_responses <- parallel::mclapply(responses, function(response) {\n    switch(heading,\n      \"ChEMBL ID\" = .parseCHEMBLresponse(response),\n      \"CAS\" = .parseCASresponse(response),\n      \"NSC Number\" = .parseNSCresponse(response),\n      \"ATC Code\" = .parseATCresponse(response),\n      \"Drug Induced Liver Injury\" = .parseDILIresponse(response),\n      tryCatch(\n        {\n          parse_function(response)\n        },\n        error = function(e) {\n          .warn(\n            funContext, \"The parseFUN function failed: \", e,\n            \". Returning unparsed results instead. Please test the parseFUN on the returned data.\"\n          )\n          response\n        }\n      )\n    )\n  }, mc.cores = nParallel)\n\n  sapply(parsed_responses, .replace_null)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/PharmacoSet-accessors.R",
    "language": "R",
    "content": "## Navigating this file:\n## - Slot section names start with ----\n## - Method section names start with ==\n##\n## As a result, you can use Ctrl + f to find the slot or method you are looking\n## for quickly, assuming you know its name.\n##\n## For example Ctrl + f '== molecularProfiles' would take you the molecularProfiles\n## method, while Ctrl +f '---- molecularProfiles' would take you to the slot\n## section.\n\n#' @include PharmacoSet-class.R\nNULL\n\n## Variables for dynamic inheritted roxygen2 docs\n\n.local_class <- 'PharmacoSet'\n.local_data <- 'CCLEsmall'\n\n#### CoreGx inherited methods\n####\n#### Note: The raw documentation lives in CoreGx, see the functions called\n#### in @eval tags for the content of the metaprogrammed roxygen2 docs.\n####\n#### See .parseToRoxygen method in utils-messages.R file of CoreGx to\n#### create similar metaprogrammed docs.\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown = TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n#' @title .parseToRoxygen\n#'\n#' @description\n#' Helper for metaprogramming roxygen2 documentation\n#'\n#' @details\n#' Takes a string block of roxygen2 tags sepearated by new-line\n#'   characteres and parses it to the appropriate format for the @eval tag,\n#'   subtituting any string in { } for the argument of the same name in `...`.\n#'\n#' @keywords internal\n#' @importFrom CoreGx .parseToRoxygen\n#' @export\n#' @noRd\n.parseToRoxygen <- function(string, ...) {\n    CoreGx::.parseToRoxygen(string, ...)\n}\n\n\n# =======================================\n# Accessor Method Documentation Object\n# ---------------------------------------\n\n\n#' @name PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_accessors(class_=.local_class)\n#' @eval .parseToRoxygen(\n#'      \"@examples data({data_})\n#'      \", data_=.local_data)\n#' @importFrom methods callNextMethod\nNULL\n\n\n# ======================================\n# Accessor Methods\n# --------------------------------------\n\n\n## ==============\n## ---- drug slot\n## --------------\n\n\n##\n## == drugInfo\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo\n#' @aliases drugInfo\n#' @export\ndrugInfo <- function(...) treatmentInfo(...)\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentInfo<-\n#' @aliases drugInfo<-\n#' @export\n`drugInfo<-` <- function(..., value) `treatmentInfo<-`(..., value=value)\n\n\n\n##\n## == drugNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames\n#' @aliases drugNames\n#' @export\ndrugNames <- function(...) treatmentNames(...)\n\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentNames<-\n#' @aliases drugNames<-\n#' @export\n`drugNames<-` <- function(..., value) `treatmentNames<-`(..., value=value)\n\n\n\n\n## ====================\n## ---- annotation slot\n## --------------------\n\n\n##\n## == annotation\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation\n#' @export\nsetMethod('annotation', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_annotation(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx annotation<-\n#' @export\nsetReplaceMethod(\"annotation\", signature(\"PharmacoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == dateCreated\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated\n#' @export\nsetMethod('dateCreated', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_dateCreated(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx dateCreated<-\n#' @export\nsetReplaceMethod('dateCreated', signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## === name\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name\nsetMethod('name', signature(\"PharmacoSet\"), function(object){\n    callNextMethod(object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_name(class_=.local_class, data_=.local_data)\n#' @importMethodsFrom CoreGx name<-\nsetReplaceMethod('name', signature(\"PharmacoSet\"), function(object, value){\n    object <- callNextMethod(object, value=value)\n    return(invisible(object))\n})\n\n## ==============\n## ---- sample slot\n## --------------\n\n\n##\n## == sampleInfo\n\n.local_sample <- \"cell\"\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleInfo(class_=.local_class, sample_=.local_sample)\n#' @importFrom CoreGx sampleInfo\n#' @export\nsetMethod(\"sampleInfo\", \"PharmacoSet\", function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleInfo(class_=.local_class,\n#' data_=.local_data, sample_=\"cell\")\n#' @importFrom CoreGx sampleInfo<-\n#' @export\nsetReplaceMethod(\"sampleInfo\", signature(object=\"PharmacoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\n\n##\n## == sampleNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames\nsetMethod(\"sampleNames\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sampleNames(class_=.local_class,\n#' data_=.local_data, sample_=.local_sample)\n#' @importMethodsFrom CoreGx sampleNames<-\nsetReplaceMethod(\"sampleNames\", signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\n\n\n## ------------------\n## ---- curation slot\n\n\n##\n## == curation\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_curation(class_=.local_class,\n#' data_=.local_data, details_=\"Contains three `data.frame`s, 'cell' with\n#' cell-line ids and 'tissue' with tissue ids and 'drug' with drug ids.\")\n#' @importMethodsFrom CoreGx curation\nsetMethod('curation', signature(object=\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_curation(class_=.local_class,\n#' data_=.local_data, details_=\"For a `PharmacoSet` object the slot should\n#' contain tissue, cell-line and drug id `data.frame`s.\")\n#' @importMethodsFrom CoreGx curation<-\nsetReplaceMethod(\"curation\", signature(object=\"PharmacoSet\", value=\"list\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ----------------------\n## ---- datasetType slot\n\n\n#\n# == datasetType\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType\nsetMethod(\"datasetType\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_datasetType(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx datasetType<-\nsetReplaceMethod(\"datasetType\", signature(object=\"PharmacoSet\",\n    value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ---------------------------\n## ---- molecularProfiles slot\n\n\n##\n## == molecularProfiles\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles\nsetMethod(molecularProfiles, \"PharmacoSet\", function(object, mDataType, assay)\n{\n    callNextMethod()\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfiles<-\nsetReplaceMethod(\"molecularProfiles\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\", assay=\"character\", value=\"matrix\"),\n    function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\nsetReplaceMethod(\"molecularProfiles\",\n    signature(object=\"PharmacoSet\", mDataType =\"character\", assay=\"missing\",\n        value=\"matrix\"), function(object, mDataType, assay, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, assay=assay, value=value)\n})\n\n\n##\n## == featureInfo\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_featureInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx featureInfo\nsetMethod(featureInfo, \"PharmacoSet\", function(object, mDataType) {\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_featureInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx featureInfo<-\nsetReplaceMethod(\"featureInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\",value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"featureInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\",value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n\n##\n## == phenoInfo\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo\nsetMethod('phenoInfo', signature(object='PharmacoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_phenoInfo(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx phenoInfo<-\nsetReplaceMethod(\"phenoInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\", value=\"data.frame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\nsetReplaceMethod(\"phenoInfo\", signature(object=\"PharmacoSet\",\n    mDataType =\"character\", value=\"DataFrame\"),\n    function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == fNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames\nsetMethod('fNames', signature(object='PharmacoSet', mDataType='character'),\n    function(object, mDataType)\n{\n    callNextMethod(object=object, mDataType=mDataType)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_fNames(class_=.local_class,\n#' data_=.local_data, mDataType_='rna')\n#' @importMethodsFrom CoreGx fNames<-\nsetReplaceMethod('fNames', signature(object='PharmacoSet', mDataType='character',\n    value='character'), function(object, mDataType, value)\n{\n    callNextMethod(object=object, mDataType=mDataType, value=value)\n})\n\n\n##\n## == mDataNames\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames\nsetMethod(\"mDataNames\", \"PharmacoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_mDataNames(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx mDataNames<-\nsetReplaceMethod(\"mDataNames\", \"PharmacoSet\", function(object, value){\n    callNextMethod(object=object, value=value)\n})\n\n\n\n##\n## == molecularProfilesSlot\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot\nsetMethod(\"molecularProfilesSlot\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_molecularProfilesSlot(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx molecularProfilesSlot<-\nsetReplaceMethod(\"molecularProfilesSlot\", signature(\"PharmacoSet\", \"list_OR_MAE\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n# ---------------------\n## ---- sensitivity slot\n\n\n##\n## == sensitivityInfo\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo\nsetMethod('sensitivityInfo', signature(\"PharmacoSet\"),\n    function(object, dimension, ...)\n{\n    callNextMethod(object=object, dimension=dimension, ...)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityInfo(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityInfo<-\nsetReplaceMethod(\"sensitivityInfo\", signature(object=\"PharmacoSet\",\n    value=\"data.frame\"), function(object, dimension, ..., value)\n{\n    callNextMethod(object=object, dimension=dimension, ..., value=value)\n})\n\n\n##\n## == sensitvityMeasures\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityMeasures(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityMeasures\nsetMethod('sensitivityMeasures', signature(object=\"PharmacoSet\"),\n    function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitityMeasures(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('sensitivityMeasures',\n    signature(object='PharmacoSet', value='character'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensitivityProfiles\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles\nsetMethod('sensitivityProfiles', signature(object=\"PharmacoSet\"), function(object)\n{\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityProfiles(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityProfiles<-\nsetReplaceMethod(\"sensitivityProfiles\",\n    signature(object=\"PharmacoSet\", value=\"data.frame\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n#\n# == sensitivityRaw\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw\nsetMethod(\"sensitivityRaw\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensitivityRaw(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensitivityRaw<-\nsetReplaceMethod('sensitivityRaw', signature(\"PharmacoSet\", \"array\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n#\n# == treatmentResponse\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_treatmentResponse(class_=.local_class,\n#'   data_=.local_data)\n#' @importMethodsFrom CoreGx treatmentResponse\nsetMethod(\"treatmentResponse\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\n\n\n#' @rdname PharmacoSet-accessors\n#' @importMethodsFrom CoreGx treatmentResponse<-\n#' @eval CoreGx:::.docs_CoreSet_set_treatmentResponse(class_=.local_class,\n#' data_=.local_data)\nsetReplaceMethod('treatmentResponse', signature(object='PharmacoSet',\n    value='list_OR_LongTable'), function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n##\n## == sensNumber\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber\nsetMethod('sensNumber', \"PharmacoSet\", function(object){\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_sensNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx sensNumber<-\nsetReplaceMethod('sensNumber', signature(object=\"PharmacoSet\", value=\"matrix\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})\n\n\n## ======================\n## ---- perturbation slot\n\n\n##\n## == pertNumber\n\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_get_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber\nsetMethod('pertNumber', signature(object='PharmacoSet'), function(object) {\n    callNextMethod(object=object)\n})\n\n#' @rdname PharmacoSet-accessors\n#' @eval CoreGx:::.docs_CoreSet_set_pertNumber(class_=.local_class,\n#' data_=.local_data)\n#' @importMethodsFrom CoreGx pertNumber<-\nsetReplaceMethod('pertNumber', signature(object='PharmacoSet', value=\"array\"),\n    function(object, value)\n{\n    callNextMethod(object=object, value=value)\n})",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.parseToRoxygen` function in this code?",
        "answer": "The `.parseToRoxygen` function is a helper for metaprogramming roxygen2 documentation. It takes a string block of roxygen2 tags separated by new-line characters and parses it to the appropriate format for the @eval tag, substituting any string in { } for the argument of the same name in `...`."
      },
      {
        "question": "How does the `drugInfo` method relate to the `treatmentInfo` method in this code?",
        "answer": "The `drugInfo` method is an alias for the `treatmentInfo` method. It uses the same implementation as `treatmentInfo` but provides a more specific name for the PharmacoSet context. This is evident from the line `drugInfo <- function(...) treatmentInfo(...)` and similarly for the setter method."
      },
      {
        "question": "What is the purpose of the `callNextMethod` function used in many of the accessor methods?",
        "answer": "The `callNextMethod` function is used to call the next method in the method dispatch chain. In this context, it's used to delegate the implementation to the parent class (likely CoreSet) while allowing for potential additional behavior specific to the PharmacoSet class. This promotes code reuse and maintains the inheritance hierarchy."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('annotation', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"PharmacoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})",
        "complete": "setMethod('annotation', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod(\"annotation\", signature(\"PharmacoSet\", \"list\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})\n\nsetMethod('dateCreated', signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object=object)\n})\n\nsetReplaceMethod('dateCreated', signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})"
      },
      {
        "partial": "setMethod(\"sampleInfo\", \"PharmacoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"PharmacoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})",
        "complete": "setMethod(\"sampleInfo\", \"PharmacoSet\", function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleInfo\", signature(object=\"PharmacoSet\",\n        value=\"data.frame\"), function(object, value) {\n    callNextMethod(object, value=value)\n})\n\nsetMethod(\"sampleNames\", signature(\"PharmacoSet\"), function(object) {\n    callNextMethod(object)\n})\n\nsetReplaceMethod(\"sampleNames\", signature(object=\"PharmacoSet\", value=\"character\"),\n        function(object, value) {\n    callNextMethod(object=object, value=value)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/pipeline.py",
    "language": "py",
    "content": "import warnings\nfrom itertools import chain\n\nfrom joblib import Parallel, delayed\n\nfrom .ops import BaseOp, BaseInput, BaseOutput\n\n\nclass Pipeline:\n    \"\"\"Base class for image processing pipelines.\n\n    A pipeline can be created by subclassing, instantiating the required\n    data loaders and operations from `ops` in the constructor and implementing\n    the `process_one_case` method, which defines the processing steps for one\n    case (i.e. one subject_id from data loaders).\n    \"\"\"\n    def __init__(self, n_jobs=1, missing_strategy=\"drop\", show_progress=True, warn_on_error=False):\n        \"\"\"Initialize the base class.\n\n        Parameters\n        ----------\n        n_jobs : int, optional\n            The number of worker processes to use for parallel computation\n            (default 0).\n        \"\"\"\n        self.n_jobs = n_jobs\n        self.missing_strategy = missing_strategy.lower()\n        self.show_progress = show_progress\n        self.warn_on_error = warn_on_error\n        if self.missing_strategy not in [\"drop\", \"pass\"]:\n            raise ValueError(f\"missing_strategy must be either of 'drop' or 'pass', got {missing_strategy}\")\n\n    def _get_loader_subject_ids(self):\n        loaders = (v._loader for v in self.__dict__.values() if isinstance(v, BaseInput))\n        all_subject_ids = [loader.keys() for loader in loaders]\n        unique_subject_ids = set(chain.from_iterable(all_subject_ids))\n\n        if not all_subject_ids:\n            raise AttributeError(\"Pipeline must define at least one input op (subclass of ops.BaseInput)\")\n\n        result = []\n        for subject_id in unique_subject_ids:\n            if not all((subject_id in subject_ids for subject_ids in all_subject_ids)):\n                # TODO give more details about which input data is missing\n                message = f\"Subject {subject_id} is missing some of the input data \"\n                if self.missing_strategy == \"drop\":\n                    message += f\"and will be dropped according to current missing strategy ('{self.missing_strategy}').\"\n                elif self.missing_strategy == \"pass\":\n                    message += f\"but will be passed according to current missing strategy ('{self.missing_strategy}').\"\n                    result.append(subject_id)\n                warnings.warn(message, category=RuntimeWarning)\n                continue\n            result.append(subject_id)\n\n        return result\n\n    @property\n    def ops(self):\n        # TODO (Michal) return ops in actual order of execution\n        return [v for v in self.__dict__.values() if isinstance(v, BaseOp)]\n\n    def __repr__(self):\n        attrs = [(k, v) for k, v in self.__dict__.items() if not isinstance(v, BaseOp) and not k.startswith(\"_\")]\n        args = \", \".join(f\"{k}={v}\" for k, v in attrs)\n        return f\"{self.__class__.__module__}.{self.__class__.__name__}({args})\"\n\n    def __str__(self):\n        repr_ = self.__repr__()\n        ops = self.ops()\n        if not ops:\n            ops = \"<none>\"\n        else:\n            ops = \",\\n\".join(ops)\n        return repr_ + \"\\n\" + \"ops: (\\n\" + ops + \"\\n)\"\n\n    def process_one_subject(self, subject_id):\n        \"\"\"Define the processing steps for one case.\n\n        Parameters\n        ----------\n        subject_id : str\n            The ID of the subject to be processed.\n        \"\"\"\n        raise NotImplementedError\n\n    def _process_wrapper(self, subject_id):\n        try:\n            print(subject_id)\n            self.process_one_subject(subject_id)\n        except Exception as e:\n            message = f\"{type(e).__name__} while processing subject {subject_id}: \" + str(e)\n            if self.warn_on_error:\n                warnings.warn(message, category=RuntimeWarning)\n            else:\n                raise RuntimeError(message) from e\n\n    def run(self):\n        \"\"\"Execute the pipeline, possibly in parallel.\n        \"\"\"\n        # Joblib prints progress to stdout if verbose > 50\n        verbose = 51 if self.show_progress else 0\n\n        subject_ids = self._get_loader_subject_ids()\n        # Note that returning any SimpleITK object in process_one_subject is\n        # not supported yet, since they cannot be pickled\n        Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n            delayed(self._process_wrapper)(subject_id) for subject_id in subject_ids)\n\n\nclass SequentialPipeline(Pipeline):\n    def __init__(self, ops_list):\n        self.source = ops_list.pop(0)\n        self.sink = ops_list.pop() if isinstance(ops_list[-1], BaseOutput) else None\n        self.ops_list = ops_list\n\n    def process_one_subject(self, subject_id):\n        image = self.source(subject_id)\n        for op in self.ops_list:\n            image = op(image)\n        if self.sink is not None:\n            self.sink(image)\n        return image\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `_get_loader_subject_ids` method in the `Pipeline` class?",
        "answer": "The `_get_loader_subject_ids` method is responsible for gathering and filtering subject IDs from all input loaders. It ensures that only subject IDs present in all input data sources are included, based on the specified missing strategy ('drop' or 'pass'). It also raises warnings for missing data and handles cases where subjects are missing from some input sources."
      },
      {
        "question": "How does the `SequentialPipeline` class differ from the base `Pipeline` class in terms of operation execution?",
        "answer": "The `SequentialPipeline` class implements a specific processing flow where operations are executed sequentially. It takes a list of operations, with the first operation being the source (input) and the last potentially being the sink (output). The `process_one_subject` method applies each operation in the list to the image data, passing the result of each operation to the next one in the sequence."
      },
      {
        "question": "What is the purpose of the `_process_wrapper` method in the `Pipeline` class, and how does it handle errors?",
        "answer": "The `_process_wrapper` method is a helper function used to execute `process_one_subject` for each subject ID. It provides error handling functionality, catching any exceptions that occur during processing. If `warn_on_error` is set to True, it will issue a warning for any errors encountered. Otherwise, it will raise a `RuntimeError` with details about the exception. This method allows for more robust error handling and reporting when processing multiple subjects."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class SequentialPipeline(Pipeline):\n    def __init__(self, ops_list):\n        self.source = ops_list.pop(0)\n        self.sink = ops_list.pop() if isinstance(ops_list[-1], BaseOutput) else None\n        self.ops_list = ops_list\n\n    def process_one_subject(self, subject_id):\n        # TODO: Implement the processing logic\n        pass",
        "complete": "class SequentialPipeline(Pipeline):\n    def __init__(self, ops_list):\n        self.source = ops_list.pop(0)\n        self.sink = ops_list.pop() if isinstance(ops_list[-1], BaseOutput) else None\n        self.ops_list = ops_list\n\n    def process_one_subject(self, subject_id):\n        image = self.source(subject_id)\n        for op in self.ops_list:\n            image = op(image)\n        if self.sink is not None:\n            self.sink(image)\n        return image"
      },
      {
        "partial": "class Pipeline:\n    def __init__(self, n_jobs=1, missing_strategy=\"drop\", show_progress=True, warn_on_error=False):\n        self.n_jobs = n_jobs\n        self.missing_strategy = missing_strategy.lower()\n        self.show_progress = show_progress\n        self.warn_on_error = warn_on_error\n        # TODO: Implement validation for missing_strategy\n\n    def run(self):\n        # TODO: Implement the run method\n        pass",
        "complete": "class Pipeline:\n    def __init__(self, n_jobs=1, missing_strategy=\"drop\", show_progress=True, warn_on_error=False):\n        self.n_jobs = n_jobs\n        self.missing_strategy = missing_strategy.lower()\n        self.show_progress = show_progress\n        self.warn_on_error = warn_on_error\n        if self.missing_strategy not in [\"drop\", \"pass\"]:\n            raise ValueError(f\"missing_strategy must be either of 'drop' or 'pass', got {missing_strategy}\")\n\n    def run(self):\n        verbose = 51 if self.show_progress else 0\n        subject_ids = self._get_loader_subject_ids()\n        Parallel(n_jobs=self.n_jobs, verbose=verbose)(\n            delayed(self._process_wrapper)(subject_id) for subject_id in subject_ids)"
      }
    ],
    "dependencies": {
      "imports": [
        "warnings"
      ],
      "from_imports": [
        "itertools.chain",
        "joblib.Parallel",
        "ops.BaseOp"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-summarizeSensitivityProfiles.R",
    "language": "R",
    "content": "#' Takes the sensitivity data from a PharmacoSet, and summarises them into a\n#' drug vs cell line table\n#'\n#' This function creates a table with cell lines as rows and drugs as columns,\n#' summarising the drug senstitivity data of a PharmacoSet into drug-cell line\n#' pairs\n#'\n#' @examples\n#' data(GDSCsmall)\n#' GDSCauc <- summarizeSensitivityProfiles(GDSCsmall,\n#'     sensitivity.measure='auc_published')\n#'\n#' @param object [PharmacoSet] The PharmacoSet from which to extract the data\n#' @param sensitivity.measure [character] The sensitivity measure to use. Use the sensitivityMeasures function to find out what measures are available for each object.\n#' @param cell.lines [character] The cell lines to be summarized. If any cell lines have no data, they will be filled with missing values.\n#' @param profiles_assay [character] The name of the assay in the PharmacoSet object that contains the sensitivity profiles.\n#' @param treatment_col [character] The name of the column in the profiles assay that contains the treatment IDs.\n#' @param sample_col [character] The name of the column in the profiles assay that contains the sample IDs.\n#' @param drugs [character] The drugs to be summarized. If any drugs have no data, they will be filled with missing values.\n#' @param summary.stat [character] The summary method to use if there are repeated cell line-drug experiments. Choices are \"mean\", \"median\", \"first\", \"last\", \"max\", or \"min\".\n#' @param fill.missing Should the missing cell lines not in the molecular data object be filled in with missing values?\n#' @param verbose Should the function print progress messages?\n#'\n#' @return [matrix] A matrix with cell lines going down the rows, drugs across the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @importMethodsFrom CoreGx summarizeSensitivityProfiles\n#' @export\nsetMethod(\"summarizeSensitivityProfiles\", signature(object=\"PharmacoSet\"),\n    function(\n      object, \n      sensitivity.measure=\"auc_recomputed\", \n      cell.lines, \n      profiles_assay = \"profiles\",\n      treatment_col = \"treatmentid\", \n      sample_col = \"sampleid\",\n      drugs, \n      summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n      fill.missing=TRUE, \n      verbose=TRUE\n  ) {\n  if (is(treatmentResponse(object), 'LongTable'))\n    .summarizeSensProfiles(object, sensitivity.measure, profiles_assay = profiles_assay,\n      treatment_col, sample_col, cell.lines, drugs, summary.stat, fill.missing)\n  else\n    .summarizeSensitivityProfilesPharmacoSet(object,\n      sensitivity.measure, cell.lines, drugs, summary.stat,\n      fill.missing, verbose)\n})\n\n#' Summarize the sensitivity profiles when the sensitivity slot is a LongTable\n#'\n#' @return [matrix] A matrix with cell lines going down the rows, drugs across\n#'   the columns, with the selected sensitivity statistic for each pair.\n#'\n#' @import data.table\n#' @keywords internal\n.summarizeSensProfiles <- function(object,\n        sensitivity.measure='auc_recomputed', profiles_assay = \"profiles\", \n        treatment_col = \"treatmentid\", sample_col = \"sampleid\", cell.lines, drugs, summary.stat,\n        fill.missing=TRUE) {\n\n    # handle missing\n    if (missing(cell.lines)) cell.lines <- sampleNames(object)\n    if (missing(drugs)) drugs <- treatmentNames(object)\n    if (missing(summary.stat) || length(summary.stat)>1) summary.stat <- 'mean'\n\n    checkmate::assert_class(treatmentResponse(object), 'LongTable')\n    checkmate::assert_string(sensitivity.measure)\n    checkmate::assert_string(profiles_assay)\n    # get LongTable object\n    longTable <- treatmentResponse(object)\n\n    checkmate::assert((profiles_assay %in% names(longTable)),\n      msg = paste0(\"[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] \",\n        \"The assay '\", profiles_assay, \"' is not in the LongTable object.\"))\n\n    # extract the sensitivty profiles\n    sensProfiles <- assay(longTable, profiles_assay, withDimnames=TRUE, key=FALSE)\n    profileOpts <- setdiff(colnames(sensProfiles), idCols(longTable))\n\n    # compute max concentration and add it to the profiles\n    if (sensitivity.measure == 'max.conc') {\n        dose <- copy(assay(longTable, 'dose', withDimnames=TRUE, key=FALSE))\n        dose[, max.conc := max(.SD, na.rm=TRUE),\n            .SDcols=grep('dose\\\\d+id', colnames(dose))]\n        dose <- dose[, .SD, .SDcols=!grepl('dose\\\\d+id', colnames(dose))]\n        sensProfiles <- dose[sensProfiles, on=idCols(longTable)]\n    }\n\n    # deal with drug combo methods\n    if (sensitivity.measure == 'Synergy_score')\n        drugs <- grep('///', drugs, value=TRUE)\n\n    # ensure selected measure is an option\n    if (!(sensitivity.measure %in% profileOpts))\n        stop(.errorMsg('[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] ',\n            'there is no measure ', sensitivity.measure, ' in this PharmacoSet.',\n            ' Please select one of: ', .collapse(profileOpts)))\n\n    # match summary function\n    ## TODO:: extend this function to support passing in a custom summary function\n    summary.function <- function(x) {\n        if (all(is.na(x))) {\n            return(NA_real_)\n        }\n        switch(summary.stat,\n            \"mean\" = { mean(as.numeric(x), na.rm=TRUE) },\n            \"median\" = { median(as.numeric(x), na.rm=TRUE) },\n            \"first\" = { as.numeric(x)[[1]] },\n            \"last\" = { as.numeric(x)[[length(x)]] },\n            \"max\"= { max(as.numeric(x), na.rm=TRUE) },\n            \"min\" = { min(as.numeric(x), na.rm=TRUE)}\n            )\n    }\n    sensProfiles <- data.table::as.data.table(sensProfiles)\n\n    # do the summary\n    profSummary <- sensProfiles[, summary.function(get(sensitivity.measure)),\n        by=c(treatment_col, sample_col)]\n\n    print(profSummary)\n    \n    # NA pad the missing cells and drugs\n    if (fill.missing) {\n        allCombos <- data.table(expand.grid(drugs, cell.lines))\n        colnames(allCombos) <- c(treatment_col, sample_col)\n        profSummary <- profSummary[allCombos, on=c(treatment_col, sample_col)]\n        print(profSummary)\n    }\n\n    # reshape and convert to matrix\n    setorderv(profSummary, c(sample_col, treatment_col))\n    profSummary <- dcast(profSummary, get(treatment_col) ~ get(sample_col), value.var='V1')\n    summaryMatrix <- as.matrix(profSummary, rownames='treatment_col')\n    return(summaryMatrix)\n\n}\n\n\n\n#' @importFrom utils setTxtProgressBar txtProgressBar\n#' @importFrom stats median\n#' @importFrom reshape2 acast\n#' @keywords internal\n.summarizeSensitivityProfilesPharmacoSet <- function(object,\n                                         sensitivity.measure=\"aac_recomputed\",\n                                         cell.lines,\n                                         drugs,\n                                         summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n                                         fill.missing=TRUE, verbose=TRUE) {\n\n\tsummary.stat <- match.arg(summary.stat)\n  #sensitivity.measure <- match.arg(sensitivity.measure)\n  if (!(sensitivity.measure %in% c(colnames(sensitivityProfiles(object)), \"max.conc\"))) {\n    stop (sprintf(\"Invalid sensitivity measure for %s, choose among: %s\", annotation(object)$name, paste(colnames(sensitivityProfiles(object)), collapse=\", \")))\n  }\n  if (missing(cell.lines)) {\n    cell.lines <- sampleNames(object)\n  }\n  if (missing(drugs)) {\n    if (sensitivity.measure != \"Synergy_score\")\n    {\n      drugs <- treatmentNames(object)\n    }else{\n      drugs <- sensitivityInfo(object)[grep(\"///\", sensitivityInfo(object)$treatmentid), \"treatmentid\"]\n    }\n  }\n\n  pp <- sensitivityInfo(object)\n  ppRows <- which(pp$sampleid %in% cell.lines & pp$treatmentid %in% drugs) ### NEEDED to deal with duplicated rownames!!!!!!!\n  if(sensitivity.measure != \"max.conc\") {\n    dd <- sensitivityProfiles(object)\n  } else {\n\n    if(!\"max.conc\" %in% colnames(sensitivityInfo(object))) {\n\n      object <- updateMaxConc(object)\n\n    }\n    dd <- sensitivityInfo(object)\n\n  }\n\n  result <- matrix(NA_real_, nrow=length(drugs), ncol=length(cell.lines))\n  rownames(result) <- drugs\n  colnames(result) <- cell.lines\n\n  if(is.factor(dd[, sensitivity.measure]) | is.character(dd[, sensitivity.measure])){\n    warning(\"Sensitivity measure is stored as a factor or character in the pSet. This is incorrect.\\n\n             Please correct this and/or file an issue. Fixing in the call of this function.\")\n    dd[, sensitivity.measure] <- as.numeric(as.character(dd[, sensitivity.measure]))\n  }\n\n  pp_dd <- cbind(pp[,c(\"sampleid\", \"treatmentid\")], \"sensitivity.measure\"=dd[, sensitivity.measure])\n\n\n  summary.function <- function(x) {\n    if(all(is.na(x))){\n      return(NA_real_)\n    }\n    switch(summary.stat,\n        \"mean\" = { mean(as.numeric(x), na.rm=TRUE) },\n        \"median\" = { median(as.numeric(x), na.rm=TRUE) },\n        \"first\" = { as.numeric(x)[[1]] },\n        \"last\" = { as.numeric(x)[[length(x)]] },\n        \"max\"= { max(as.numeric(x), na.rm=TRUE) },\n        \"min\" = { min(as.numeric(x), na.rm=TRUE)}\n        )\n  }\n\n  pp_dd <- pp_dd[pp_dd[,\"sampleid\"] %in% cell.lines & pp_dd[,\"treatmentid\"]%in%drugs,]\n\n  tt <- reshape2::acast(pp_dd, treatmentid ~ sampleid, fun.aggregate=summary.function, value.var=\"sensitivity.measure\")\n\n  result[rownames(tt), colnames(tt)] <- tt\n\n\tif (!fill.missing) {\n\n    myRows <- apply(result, 1, function(x) !all(is.na(x)))\n    myCols <- apply(result, 2, function(x) !all(is.na(x)))\n    result <- result[myRows, myCols]\n\t}\n  return(result)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeSensitivityProfiles` function in this code?",
        "answer": "The `summarizeSensitivityProfiles` function takes sensitivity data from a PharmacoSet object and summarizes it into a drug vs cell line table. It creates a matrix with cell lines as rows and drugs as columns, containing the selected sensitivity statistic for each drug-cell line pair."
      },
      {
        "question": "How does the function handle missing data or repeated experiments?",
        "answer": "The function handles missing data by filling in missing values for cell lines or drugs not present in the data if the `fill.missing` parameter is set to TRUE. For repeated cell line-drug experiments, it uses a summary statistic specified by the `summary.stat` parameter, which can be 'mean', 'median', 'first', 'last', 'max', or 'min'."
      },
      {
        "question": "What is the difference between `.summarizeSensProfiles` and `.summarizeSensitivityProfilesPharmacoSet` functions?",
        "answer": "The `.summarizeSensProfiles` function is used when the sensitivity slot is a LongTable object, while `.summarizeSensitivityProfilesPharmacoSet` is used for other cases. The former uses data.table operations for efficient processing of large datasets, while the latter uses more traditional R data manipulation techniques."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\"summarizeSensitivityProfiles\", signature(object=\"PharmacoSet\"),\n    function(\n      object, \n      sensitivity.measure=\"auc_recomputed\", \n      cell.lines, \n      profiles_assay = \"profiles\",\n      treatment_col = \"treatmentid\", \n      sample_col = \"sampleid\",\n      drugs, \n      summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n      fill.missing=TRUE, \n      verbose=TRUE\n  ) {\n  if (is(treatmentResponse(object), 'LongTable'))\n    # Complete the function call\n  else\n    # Complete the function call\n})",
        "complete": "setMethod(\"summarizeSensitivityProfiles\", signature(object=\"PharmacoSet\"),\n    function(\n      object, \n      sensitivity.measure=\"auc_recomputed\", \n      cell.lines, \n      profiles_assay = \"profiles\",\n      treatment_col = \"treatmentid\", \n      sample_col = \"sampleid\",\n      drugs, \n      summary.stat=c(\"mean\", \"median\", \"first\", \"last\", \"max\", \"min\"),\n      fill.missing=TRUE, \n      verbose=TRUE\n  ) {\n  if (is(treatmentResponse(object), 'LongTable'))\n    .summarizeSensProfiles(object, sensitivity.measure, profiles_assay = profiles_assay,\n      treatment_col, sample_col, cell.lines, drugs, summary.stat, fill.missing)\n  else\n    .summarizeSensitivityProfilesPharmacoSet(object,\n      sensitivity.measure, cell.lines, drugs, summary.stat,\n      fill.missing, verbose)\n})"
      },
      {
        "partial": ".summarizeSensProfiles <- function(object,\n        sensitivity.measure='auc_recomputed', profiles_assay = \"profiles\", \n        treatment_col = \"treatmentid\", sample_col = \"sampleid\", cell.lines, drugs, summary.stat,\n        fill.missing=TRUE) {\n\n    if (missing(cell.lines)) cell.lines <- sampleNames(object)\n    if (missing(drugs)) drugs <- treatmentNames(object)\n    if (missing(summary.stat) || length(summary.stat)>1) summary.stat <- 'mean'\n\n    checkmate::assert_class(treatmentResponse(object), 'LongTable')\n    checkmate::assert_string(sensitivity.measure)\n    checkmate::assert_string(profiles_assay)\n    longTable <- treatmentResponse(object)\n\n    checkmate::assert((profiles_assay %in% names(longTable)),\n      msg = paste0(\"[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] \",\n        \"The assay '\", profiles_assay, \"' is not in the LongTable object.\"))\n\n    sensProfiles <- assay(longTable, profiles_assay, withDimnames=TRUE, key=FALSE)\n    profileOpts <- setdiff(colnames(sensProfiles), idCols(longTable))\n\n    # Complete the function\n}",
        "complete": ".summarizeSensProfiles <- function(object,\n        sensitivity.measure='auc_recomputed', profiles_assay = \"profiles\", \n        treatment_col = \"treatmentid\", sample_col = \"sampleid\", cell.lines, drugs, summary.stat,\n        fill.missing=TRUE) {\n\n    if (missing(cell.lines)) cell.lines <- sampleNames(object)\n    if (missing(drugs)) drugs <- treatmentNames(object)\n    if (missing(summary.stat) || length(summary.stat)>1) summary.stat <- 'mean'\n\n    checkmate::assert_class(treatmentResponse(object), 'LongTable')\n    checkmate::assert_string(sensitivity.measure)\n    checkmate::assert_string(profiles_assay)\n    longTable <- treatmentResponse(object)\n\n    checkmate::assert((profiles_assay %in% names(longTable)),\n      msg = paste0(\"[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] \",\n        \"The assay '\", profiles_assay, \"' is not in the LongTable object.\"))\n\n    sensProfiles <- assay(longTable, profiles_assay, withDimnames=TRUE, key=FALSE)\n    profileOpts <- setdiff(colnames(sensProfiles), idCols(longTable))\n\n    if (!(sensitivity.measure %in% profileOpts))\n        stop(.errorMsg('[PharmacoGx::summarizeSensivitiyProfiles,LongTable-method] ',\n            'there is no measure ', sensitivity.measure, ' in this PharmacoSet.',\n            ' Please select one of: ', .collapse(profileOpts)))\n\n    summary.function <- function(x) {\n        if (all(is.na(x))) return(NA_real_)\n        switch(summary.stat,\n            \"mean\" = mean(as.numeric(x), na.rm=TRUE),\n            \"median\" = median(as.numeric(x), na.rm=TRUE),\n            \"first\" = as.numeric(x)[[1]],\n            \"last\" = as.numeric(x)[[length(x)]],\n            \"max\" = max(as.numeric(x), na.rm=TRUE),\n            \"min\" = min(as.numeric(x), na.rm=TRUE))\n    }\n\n    sensProfiles <- data.table::as.data.table(sensProfiles)\n    profSummary <- sensProfiles[, summary.function(get(sensitivity.measure)),\n        by=c(treatment_col, sample_col)]\n\n    if (fill.missing) {\n        allCombos <- data.table(expand.grid(drugs, cell.lines))\n        colnames(allCombos) <- c(treatment_col, sample_col)\n        profSummary <- profSummary[allCombos, on=c(treatment_col, sample_col)]\n    }\n\n    setorderv(profSummary, c(sample_col, treatment_col))\n    profSummary <- dcast(profSummary, get(treatment_col) ~ get(sample_col), value.var='V1')\n    summaryMatrix <- as.matrix(profSummary, rownames='treatment_col')\n    return(summaryMatrix)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/chembl.R",
    "language": "R",
    "content": "#' A general function for creating Queries to the ChEMBL API\n#'\n#' @description A general function for creating Queries to the ChEMBL API\n#' www DOT ebi DOT ac DOT uk/chembl/api/data/ <resource>?<field>__<filter_type>=<value>\n#' |       Resource Name       |                                                                                                   Description                                                                                                   |\n#' |:-------------------------:|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:|\n#' | activity                  | Activity values recorded in an Assay                                                                                                                                                                            |\n#' | assay                     | Assay details as reported in source Document/Dataset\n#' | atc_class                 | WHO ATC Classification for drugs                                                                                                                                                                                |                                                                                                                                                           |\n#' | binding_site              | WHO ATC Classification for drugs                                                                                                                                                                                |\n#' | biotherapeutic            | Biotherapeutic molecules, which includes HELM notation and sequence data                                                                                                                                        |\n#' | cell_line                 | Cell line information                                                                                                                                                                                           |\n#' | chembl_id_lookup          | Look up ChEMBL Id entity type                                                                                                                                                                                   |\n#' | compound_record           | Occurence of a given compound in a spcecific document                                                                                                                                                           |\n#' | compound_structural_alert | Indicates certain anomaly in compound structure                                                                                                                                                                 |\n#' | document                  | Document/Dataset from which Assays have been extracted                                                                                                                                                          |\n#' | document_similarity       | Provides documents similar to a given one                                                                                                                                                                       |\n#' | document_term             | Provides keywords extracted from a document using the TextRank algorithm                                                                                                                                        |\n#' | drug                      | Approved drugs information, including (but not limited to) applicants, patent numbers and research codes. This endpoint aggregates data on the parent, please use the parent chembl id found in other endpoints |\n#' | drug_indication           | Joins drugs with diseases providing references to relevant sources                                                                                                                                              |\n#' | drug_warning              | Safety information for drugs withdrawn from one or more regions of the world and drugs that carry a warning for severe or life threatening adverse effects                                                      |\n#' | go_slim                   | GO slim ontology                                                                                                                                                                                                |\n#' | image                     | Graphical (svg) representation of Molecule                                                                                                                                                                      |\n#' | mechanism                 | Mechanism of action information for approved drugs                                                                                                                                                              |\n#' | metabolism                | Metabolic pathways with references                                                                                                                                                                              |\n#' | molecule                  | Molecule information, including properties, structural representations and synonyms                                                                                                                             |\n#' | molecule_form             | Relationships between molecule parents and salts                                                                                                                                                                |\n#' | organism                  | Simple organism classification                                                                                                                                                                                  |\n#' | protein_classification    | Protein family classification of TargetComponents                                                                                                                                                               |\n#' | similarity                | Molecule similarity search                                                                                                                                                                                      |\n#' | source                    | Document/Dataset source                                                                                                                                                                                         |\n#' | status                    | API status with ChEMBL DB version number and API software version number                                                                                                                                        |\n#' | substructure              | Molecule substructure search                                                                                                                                                                                    |\n#' | target                    | Targets (protein and non-protein) defined in Assay                                                                                                                                                              |\n#' | target_component          | Target sequence information (A Target may have 1 or more sequences)                                                                                                                                             |\n#' | target_relation           | Describes relations between targets                                                                                                                                                                             |\n#' | tissue                    | Tissue classification                                                                                                                                                                                           |\n#' | xref_source               | Cross references to other resources for compounds                                                                                                                                                               |\n#'\n#'\n#'\n#' |        Filter Type       |                                                  Description                                                 |\n#' |:------------------------:|:------------------------------------------------------------------------------------------------------------:|\n#' | exact (iexact)           | Exact match with query (case insensitive equivalent)                                                         |\n#' | contains (icontains)     | Wild card search with query (case insensitive equivalent)                                                    |\n#' | startswith (istartswith) | Starts with query (case insensitive equivalent)                                                              |\n#' | endswith (iendswith)     | Ends with query (case insensitive equivalent)                                                                |\n#' | regex (iregex)           | Regular expression query (case insensitive equivalent)                                                       |\n#' | gt (gte)                 | Greater than (or equal)                                                                                      |\n#' | lt (lte)                 | Less than (or equal)                                                                                         |\n#' | range                    | Within a range of values                                                                                     |\n#' | in                       | Appears within list of query values                                                                          |\n#' | isnull                   | Field is null                                                                                                |\n#' | search                   | Special type of filter allowing a full text search based on elastic search queries                           |\n#' | only                     | Select specific properties from the original endpoint and returns only the desired properties on each record |                                                                                                         |\n#'\n#' @param resource `character(1)` Resource to query\n#' @param field `character(1)` Field to query\n#' @param filter_type `character(1)` Filter type\n#' @param value `character(1)` Value to query\n#' @param format `character(1)` Format of the response\n#'\n#' @noRd\n#' @keywords internal\n.build_chembl_request <- function(\n    resource,\n    field = NULL, filter_type = NULL, value = NULL, format = \"json\") {\n  # possible formats for now are XML, JSON and YAML\n  checkmate::assert_choice(resource, c(.chembl_resources(), paste0(.chembl_resources(), \"/schema\")))\n  checkmate::assert_choice(field, getChemblResourceFields(resource), null.ok = TRUE)\n  checkmate::assert_choice(filter_type, .chembl_filter_types(), null.ok = TRUE)\n  checkmate::assert_character(value, null.ok = TRUE)\n  checkmate::assert_choice(format, c(\"json\", \"xml\", \"yaml\"))\n\n  # Construct the URL\n  base_url <- .buildURL(\"https://www.ebi.ac.uk/chembl/api/data\", resource)\n  url <- httr2::url_parse(base_url)\n\n  # Add the query parameters\n  query <- list()\n  fld <- paste0(field, \"__\", filter_type)\n  query[[fld]] <- value\n  query[[\"format\"]] <- format\n  url$query <- query\n\n  final_url <- httr2::url_build(url)\n  final_url |> .build_request()\n}\n\n\n#' Query the ChEMBL API\n#'\n#' This function queries the ChEMBL API using the specified parameters and returns the response in JSON format.\n#'\n#' @param resource The resource to query in the ChEMBL API.\n#' @param field The field to filter on in the ChEMBL API.\n#' @param filter_type The type of filter to apply in the ChEMBL API.\n#' @param value The value to filter on in the ChEMBL API.\n#' @param format The format of the response (default is \"json\").\n#'\n#' @return The response from the ChEMBL API in JSON format.\n#'\n#' @examples\n#' queryChemblAPI(\"mechanism\", \"molecule_chembl_id\", \"in\", \"CHEMBL1413\")\n#'\n#' @export\nqueryChemblAPI <- function(resource, field, filter_type, value, format = \"json\") {\n  .build_chembl_request(resource, field, filter_type, value, format) |> \n    .perform_request() |> \n    .parse_resp_json()\n}\n\n\n#' Get ChEMBL Mechanism\n#'\n#' This function retrieves information about the mechanism of action for a given ChEMBL ID.\n#'\n#' @param chembl.ID The ChEMBL ID of the molecule.\n#' @param resources The ChEMBL resource to query (default: \"mechanism\").\n#' @param field The field to filter on (default: \"molecule_chembl_id\").\n#' @param filter_type The filter type to use (default: \"in\").\n#' @param returnURL Logical indicating whether to return the constructed URL (default: FALSE).\n#' @param raw Logical indicating whether to return the raw response JSON (default: FALSE).\n#'\n#' @return A data.table containing the retrieved mechanism information.\n#'\n#' @examples\n#' getChemblMechanism(\"CHEMBL1413\")\n#' getChemblMechanism(\"CHEMBL1413\",\n#'   resources = \"mechanism\", field = \"molecule_chembl_id\",\n#'   filter_type = \"in\", returnURL = FALSE, raw = FALSE\n#' )\n#'\n#' @export\ngetChemblMechanism <- function(\n    chembl.ID, resources = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\",\n    returnURL = FALSE, raw = FALSE) {\n\n  funContext <- .funContext(\"getChemblMechanism\")\n  # constructChemblQuery(resource = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\", value = \"CHEMBL1413\")\n  # urls <- constructChemblQuery(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n  # urls <- URLencode(urls)\n\n  .info(funContext, \"Retrieving ChEMBL Mechanism information for \", length(chembl.ID), \" ChEMBL IDs.\")\n  response_dts <- lapply(chembl.ID, function(chembl.ID) {\n    request <- .build_chembl_request(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n\n    if (returnURL) {\n      return(request$url)\n    }\n    response <- .perform_request(request)\n\n    response_json <- .parse_resp_json(response)\n    if (raw) {\n      return(response_json)\n    }\n    .asDT(response_json[[\"mechanisms\"]])\n  })\n\n  if (returnURL || raw) {\n    return(response_dts)\n  }\n  all_cols <- .chembl_mechanism_cols()\n  # If any cols are missing, fill with NA\n  response_dts <- lapply(response_dts, function(x) {\n    missing_cols <- setdiff(all_cols, names(x))\n    if (length(missing_cols) > 0) {\n      x[, (missing_cols) := NA]\n    }\n    x\n  })\n  \n  data.table::rbindlist(response_dts, fill = TRUE)\n}\n\n\n#' Get the fields of a Chembl resource\n#'\n#' This function retrieves the fields of a Chembl resource.\n#'\n#' @param resource The Chembl resource.\n#' @return A character vector containing the names of the fields.\n#'\n#' @examples\n#' getChemblResourceFields(\"molecule\")\n#'\n#' @export\ngetChemblResourceFields <- function(resource) {\n  checkmate::assert_choice(resource, .chembl_resources())\n  .chembl_resource_schema(resource)[[\"fields\"]] |> names()\n}\n\n#' getChemblResources function\n#'\n#' This function retrieves the Chembl resources.\n#'\n#' @return A list of Chembl resources.\n#'\n#' @examples\n#' getChemblResources()\n#'\n#' @export\ngetChemblResources <- function(){\n  .chembl_resources()\n}\n\n#' Get the Chembl filter types\n#'\n#' This function retrieves the Chembl filter types.\n#'\n#' @return A list of Chembl filter types.\n#'\n#' @examples\n#' getChemblFilterTypes()\n#'\n#' @export\ngetChemblFilterTypes <- function(){\n  .chembl_filter_types()\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.build_chembl_request` function and what are its main parameters?",
        "answer": "The `.build_chembl_request` function is designed to create queries for the ChEMBL API. Its main parameters are: 'resource' (the API endpoint to query), 'field' (the field to filter on), 'filter_type' (the type of filter to apply), 'value' (the value to filter by), and 'format' (the response format, defaulting to JSON)."
      },
      {
        "question": "How does the `getChemblMechanism` function handle multiple ChEMBL IDs and what does it return?",
        "answer": "The `getChemblMechanism` function can handle multiple ChEMBL IDs by using `lapply` to process each ID. It returns a data.table containing mechanism information for all provided ChEMBL IDs. If any columns are missing for a particular ID, they are filled with NA values to ensure consistent output structure."
      },
      {
        "question": "What is the purpose of the `getChemblResourceFields` function and how does it utilize the `.chembl_resource_schema` function?",
        "answer": "The `getChemblResourceFields` function retrieves the field names for a given ChEMBL resource. It uses the `.chembl_resource_schema` function to get the schema for the specified resource, then extracts and returns the names of the fields from this schema."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getChemblMechanism <- function(chembl.ID, resources = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\", returnURL = FALSE, raw = FALSE) {\n  funContext <- .funContext(\"getChemblMechanism\")\n  .info(funContext, \"Retrieving ChEMBL Mechanism information for \", length(chembl.ID), \" ChEMBL IDs.\")\n  response_dts <- lapply(chembl.ID, function(chembl.ID) {\n    request <- .build_chembl_request(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n\n    if (returnURL) {\n      return(request$url)\n    }\n    response <- .perform_request(request)\n\n    response_json <- .parse_resp_json(response)\n    if (raw) {\n      return(response_json)\n    }\n    .asDT(response_json[[\"mechanisms\"]])\n  })\n\n  if (returnURL || raw) {\n    return(response_dts)\n  }\n  all_cols <- .chembl_mechanism_cols()\n  # Complete the function here\n}",
        "complete": "getChemblMechanism <- function(chembl.ID, resources = \"mechanism\", field = \"molecule_chembl_id\", filter_type = \"in\", returnURL = FALSE, raw = FALSE) {\n  funContext <- .funContext(\"getChemblMechanism\")\n  .info(funContext, \"Retrieving ChEMBL Mechanism information for \", length(chembl.ID), \" ChEMBL IDs.\")\n  response_dts <- lapply(chembl.ID, function(chembl.ID) {\n    request <- .build_chembl_request(resource = resources, field = field, filter_type = filter_type, value = chembl.ID)\n\n    if (returnURL) {\n      return(request$url)\n    }\n    response <- .perform_request(request)\n\n    response_json <- .parse_resp_json(response)\n    if (raw) {\n      return(response_json)\n    }\n    .asDT(response_json[[\"mechanisms\"]])\n  })\n\n  if (returnURL || raw) {\n    return(response_dts)\n  }\n  all_cols <- .chembl_mechanism_cols()\n  response_dts <- lapply(response_dts, function(x) {\n    missing_cols <- setdiff(all_cols, names(x))\n    if (length(missing_cols) > 0) {\n      x[, (missing_cols) := NA]\n    }\n    x\n  })\n  \n  data.table::rbindlist(response_dts, fill = TRUE)\n}"
      },
      {
        "partial": "queryChemblAPI <- function(resource, field, filter_type, value, format = \"json\") {\n  # Complete the function here\n}",
        "complete": "queryChemblAPI <- function(resource, field, filter_type, value, format = \"json\") {\n  .build_chembl_request(resource, field, filter_type, value, format) |> \n    .perform_request() |> \n    .parse_resp_json()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/zzz.R",
    "language": "R",
    "content": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE) # setting default option for using c code across package\n\n}\n# Package Start-up Functions\n\n.onAttach <- function(libname, pkgname) {\n\n    if (interactive() && is.null(options('bhklab.startup_'))) {\n        oldOpts <- options()\n        options(warn=-1)\n        on.exit(options(oldOpts))\n\n        packageStartupMessage(\n        \"\nPharmacoGx package brought to you by:\n\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2557      \\u2588\\u2588\\u2588\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551 \\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2588\\u2588\\u2557 \\u2588\\u2588\\u2551     \\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2551\\u2588\\u2588\\u2554\\u2550\\u2550\\u2588\\u2588\\u2557\n\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2557\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2557\\u2588\\u2588\\u2551  \\u2588\\u2588\\u2551\\u2588\\u2588\\u2588\\u2588\\u2588\\u2588\\u2554\\u255d\n\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d \\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\\u255a\\u2550\\u255d  \\u255a\\u2550\\u255d\\u255a\\u2550\\u2550\\u2550\\u2550\\u2550\\u255d\n\nFor more of our work visit bhklab.ca!\n\nLike PharmacoGx? Check out our companion web-app at PharmacoDB.ca.\n        \"\n        )\n        # Prevent repeated messages when loading multiple lab packages\n        options(bhklab.startup_=FALSE)\n    }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.onLoad` function in this R package?",
        "answer": "The `.onLoad` function is used to set default options when the package is loaded. In this case, it sets the 'PharmacoGx_useC' option to TRUE, which likely enables the use of C code across the package for improved performance."
      },
      {
        "question": "How does the `.onAttach` function control the display of the startup message?",
        "answer": "The `.onAttach` function checks if the session is interactive and if the 'bhklab.startup_' option is null. If both conditions are met, it displays the startup message. It also sets 'bhklab.startup_' to FALSE after displaying the message, preventing repeated messages when loading multiple lab packages."
      },
      {
        "question": "What technique is used in the `.onAttach` function to temporarily suppress warnings?",
        "answer": "The function uses `options(warn=-1)` to temporarily suppress warnings. It stores the old options in `oldOpts` and uses `on.exit(options(oldOpts))` to ensure that the original warning settings are restored when the function exits, regardless of how it exits."
      }
    ],
    "completion_tasks": [
      {
        "partial": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE)\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    oldOpts <- options()\n    options(warn=-1)\n    on.exit(options(oldOpts))\n\n    packageStartupMessage(\n      # Message content here\n    )\n\n    # Complete the code\n  }\n}",
        "complete": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE)\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    oldOpts <- options()\n    options(warn=-1)\n    on.exit(options(oldOpts))\n\n    packageStartupMessage(\n      # Message content here\n    )\n\n    options(bhklab.startup_=FALSE)\n  }\n}"
      },
      {
        "partial": ".onLoad <- function(libname, pkgname) {\n  # Set default option\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    # Set options and display message\n  }\n}",
        "complete": ".onLoad <- function(libname, pkgname) {\n  options(PharmacoGx_useC = TRUE)\n}\n\n.onAttach <- function(libname, pkgname) {\n  if (interactive() && is.null(options('bhklab.startup_'))) {\n    oldOpts <- options()\n    options(warn=-1)\n    on.exit(options(oldOpts))\n    packageStartupMessage(\"PharmacoGx package loaded. Visit bhklab.ca for more.\")\n    options(bhklab.startup_=FALSE)\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/gCSI/gCSI_treatmentMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))\n\nusethis::use_data(gCSI_treatmentMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in the first line of the code snippet?",
        "answer": "The `system.file()` function is used to locate and construct the file path for a specific file within an R package. In this case, it's locating the file 'gCSI_GRmetrics_v1.3.tsv' in the 'extdata/gCSI' directory of the 'AnnotationGx' package. This function is particularly useful for accessing files that are distributed with a package."
      },
      {
        "question": "How does the code ensure unique entries in the `gCSI_treatmentMetadata` data frame?",
        "answer": "The code uses the `unique()` function on a subset of the `rawdata` data frame. Specifically, it selects the columns 'DrugName' and 'Norm_DrugName' using `rawdata[,c('DrugName', 'Norm_DrugName')]`, and then applies `unique()` to this selection. This operation removes any duplicate rows, ensuring that each combination of DrugName and Norm_DrugName appears only once in the resulting `gCSI_treatmentMetadata` data frame."
      },
      {
        "question": "What is the purpose of the `usethis::use_data()` function call at the end of the snippet?",
        "answer": "The `usethis::use_data()` function is used to save R objects (in this case, `gCSI_treatmentMetadata`) to an .rda file in the 'data/' directory of the current package. The `overwrite = TRUE` argument allows the function to overwrite an existing file with the same name if it exists. This is typically used in package development to include data with the package, making it easily accessible to users of the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))\n\n# Complete the code to save the gCSI_treatmentMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))\n\nusethis::use_data(gCSI_treatmentMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the file, extract unique drug metadata, and rename columns\n\nfilePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\n# Add code here",
        "complete": "filePath <- system.file(\"extdata/gCSI\", \"gCSI_GRmetrics_v1.3.tsv\", package = \"AnnotationGx\")\n\nrawdata <- data.table::fread(filePath, check.names=T)\n\ngCSI_treatmentMetadata <- unique(rawdata[,c(\"DrugName\", \"Norm_DrugName\")])\n\ndata.table::setnames(gCSI_treatmentMetadata, c(\"DrugName\", \"Norm_DrugName\"), c(\"gCSI.treatmentid\", \"gCSI.NormDrugName\"))"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_intersectPSet.R",
    "language": "R",
    "content": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\n# ###TO-DO:: This test takes forever to run; consider making the intersections smaller\n# test_that(\"Intersection result did not change since last time\", {\n# \tdata(GDSCsmall)\n# \tdata(CCLEsmall)\n# \tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"))\n# \texpect_equal_to_reference(common, \"intersectedSmallData.rds\", tolerance=1e-3)\n# \texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n# \texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n# \texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n# })\n#\n# test_that(\"Strict Intersection result did not change since last time\", {\n# \tdata(GDSCsmall)\n# \tdata(CCLEsmall)\n# \tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"), strictIntersect=TRUE)\n# \texpect_equal_to_reference(common, \"intersectedSmallDataStrict.rds\", tolerance=1e-3)\n# \texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n# \texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n# \texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n# \texpect_equal(rownames(sensitivityProfiles(common$GDSC)), rownames(common$sensitivityProfiles(CCLE)))\n# })\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'intersectPSet' function in this code snippet?",
        "answer": "The 'intersectPSet' function is used to find the intersection between two PharmacoSet objects (GDSCsmall and CCLEsmall) based on specified criteria such as drugs, cell lines, and concentrations. It allows for comparing and combining data from different pharmacogenomic datasets."
      },
      {
        "question": "Why are the test cases commented out in this code snippet?",
        "answer": "The test cases are commented out because, as mentioned in the TODO comment, they take a long time to run. The comment suggests considering making the intersections smaller to improve performance. This is likely a temporary measure during development to avoid long execution times when running tests frequently."
      },
      {
        "question": "What is the difference between the two commented-out test cases in terms of the 'intersectPSet' function call?",
        "answer": "The main difference is in the 'strictIntersect' parameter. The first test case uses the default value (FALSE), while the second test case explicitly sets 'strictIntersect=TRUE'. This likely affects how the intersection is performed, with the strict version potentially requiring exact matches across all specified criteria."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"))\n\texpect_equal_to_reference(common, \"intersectedSmallData.rds\", tolerance=1e-3)\n\t# Add more expectations here\n})",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"))\n\texpect_equal_to_reference(common, \"intersectedSmallData.rds\", tolerance=1e-3)\n\texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n\texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n\texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n})"
      },
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Strict Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"), strictIntersect=TRUE)\n\texpect_equal_to_reference(common, \"intersectedSmallDataStrict.rds\", tolerance=1e-3)\n\t# Add more expectations here\n})",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking intersectPSet.\")\n\ntest_that(\"Strict Intersection result did not change since last time\", {\n\tdata(GDSCsmall)\n\tdata(CCLEsmall)\n\tcommon <- intersectPSet(list('GDSC'=GDSCsmall, 'CCLE'=CCLEsmall), intersectOn = c(\"drugs\", \"cell.lines\",\"concentrations\"), strictIntersect=TRUE)\n\texpect_equal_to_reference(common, \"intersectedSmallDataStrict.rds\", tolerance=1e-3)\n\texpect_equal(sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)),sum(!is.na(common$sensitivityProfiles(CCLE)$auc_recomputed_star)))\n\texpect_equal(treatmentNames(common$CCLE), treatmentNames(common$GDSC))\n\texpect_equal(sampleNames(common$CCLE), sampleNames(common$GDSC))\n\texpect_equal(rownames(sensitivityProfiles(common$GDSC)), rownames(common$sensitivityProfiles(CCLE)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeDSS.R",
    "language": "R",
    "content": "##TODO:: Add function documentation\ncomputeDSS <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       t_param = 10,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE,\n                       trunc = TRUE,\n                       verbose = TRUE,\n                       dss_type = 3,\n                       censor = FALSE\n                       #, ...\n) {\n  \n  if(missing(concentration)){\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(Hill_fit)) {\n    \n    Hill_fit <- logLogisticRegression(concentration,\n                                      viability,\n                                      conc_as_log = conc_as_log,\n                                      viability_as_pct = viability_as_pct,\n                                      trunc = trunc,\n                                      verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, \n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n    \n  } else {\n    \n    cleanData <- sanitizeInput(conc = concentration, \n                               viability = viability,\n                               Hill_fit = Hill_fit,\n                               conc_as_log = conc_as_log,\n                               viability_as_pct = viability_as_pct,\n                               trunc = trunc,\n                               verbose = verbose) #is this coercing the concentration to log?\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n    \n  }\n  \n  if(pars[[3]] > max(concentration)) {\n    return(0)\n  }\n  \n  if(!viability_as_pct){\n    t_param = t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 = max(concentration)\n  x1 = computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)){return(0)}\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor) {\n    if (pars[[2]] > 50) {\n      return(NA)\n    } else if (all(concentration < pars[[3]])) {\n      return(0)\n    }\n  }\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) {\n    return(DSS)\n  }\n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) {\n    return(DSS)\n  }\n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) {\n    return(DSS)\n  } else {\n    stop(\"Invalid DSS type entered.\")\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeDSS` function and what are its main input parameters?",
        "answer": "The `computeDSS` function is designed to compute the Drug Sensitivity Score (DSS) based on concentration-response data. Its main input parameters are:\n1. `concentration`: The concentration values to integrate over (required)\n2. `viability`: The corresponding viability values (optional if Hill_fit is provided)\n3. `Hill_fit`: A pre-computed Hill equation fit (optional)\n4. `t_param`: A threshold parameter (default: 10)\n5. `conc_as_log`: Boolean indicating if concentration is in log scale (default: FALSE)\n6. `viability_as_pct`: Boolean indicating if viability is in percentage (default: TRUE)\n7. `dss_type`: An integer (1, 2, or 3) specifying the type of DSS calculation (default: 3)"
      },
      {
        "question": "How does the function handle missing or incomplete input data?",
        "answer": "The function handles missing or incomplete input data in several ways:\n1. If `concentration` is missing, it stops execution with an error message.\n2. If `Hill_fit` is missing, it computes the Hill fit using the `logLogisticRegression` function with the provided concentration and viability data.\n3. It uses the `sanitizeInput` function to clean and validate the input data, ensuring consistency in data format and scale.\n4. If the computed IC50 (pars[[3]]) is greater than the maximum concentration, it returns 0.\n5. If `censor` is TRUE, it returns NA if the maximum effect (pars[[2]]) is greater than 50%, and returns 0 if all concentrations are less than the IC50."
      },
      {
        "question": "Explain the different types of DSS calculations (dss_type 1, 2, and 3) implemented in this function.",
        "answer": "The function implements three types of DSS calculations based on the `dss_type` parameter:\n1. DSS Type 1 (dss_type = 1):\n   - Basic DSS calculation: (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n2. DSS Type 2 (dss_type = 2):\n   - DSS Type 1 result divided by log(100 - pars[[2]]), where pars[[2]] is the minimum viability\n3. DSS Type 3 (dss_type = 3):\n   - DSS Type 2 result multiplied by (x2 - x1) / (max(concentration) - min(concentration))\n\nEach type progressively adjusts the DSS calculation to account for different aspects of the dose-response curve. Type 3 is the default and most comprehensive calculation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)){\n    stop(\"The concentration values to integrate over must always be provided.\")\n  }\n  if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  } else {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  }\n  \n  if(pars[[3]] > max(concentration)) {\n    return(0)\n  }\n  \n  if(!viability_as_pct){\n    t_param = t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 = max(concentration)\n  x1 = computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)){return(0)}\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor) {\n    if (pars[[2]] > 50) {\n      return(NA)\n    } else if (all(concentration < pars[[3]])) {\n      return(0)\n    }\n  }\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) {\n    return(DSS)\n  }\n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) {\n    return(DSS)\n  }\n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) {\n    return(DSS)\n  } else {\n    stop(\"Invalid DSS type entered.\")\n  }\n}",
        "complete": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)) stop(\"The concentration values to integrate over must always be provided.\")\n  \n  if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  } else {\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  }\n  \n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n  \n  if(pars[[3]] > max(concentration)) return(0)\n  \n  if(!viability_as_pct) {\n    t_param <- t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 <- max(concentration)\n  x1 <- computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)) return(0)\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor && (pars[[2]] > 50 || all(concentration < pars[[3]]))) return(if(pars[[2]] > 50) NA else 0)\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) return(DSS)\n  \n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) return(DSS)\n  \n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) return(DSS)\n  \n  stop(\"Invalid DSS type entered.\")\n}"
      },
      {
        "partial": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)) stop(\"The concentration values to integrate over must always be provided.\")\n  \n  cleanData <- if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  } else {\n    sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  }\n  \n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n  \n  if(pars[[3]] > max(concentration)) return(0)\n  \n  if(!viability_as_pct) {\n    t_param <- t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 <- max(concentration)\n  x1 <- computeICn(concentration = concentration, Hill_fit = unlist(pars), n = t_param, conc_as_log = TRUE, viability_as_pct = TRUE)\n  if(!is.finite(x1)) return(0)\n  \n  x1 <- max(x1, min(concentration))\n  \n  if (censor && (pars[[2]] > 50 || all(concentration < pars[[3]]))) return(if(pars[[2]] > 50) NA else 0)\n  \n  AUC <- computeAUC(concentration = c(x1, x2), Hill_fit = unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) return(DSS)\n  \n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) return(DSS)\n  \n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) return(DSS)\n  \n  stop(\"Invalid DSS type entered.\")\n}",
        "complete": "computeDSS <- function(concentration, viability, Hill_fit, t_param = 10, conc_as_log = FALSE, viability_as_pct = TRUE, trunc = TRUE, verbose = TRUE, dss_type = 3, censor = FALSE) {\n  if(missing(concentration)) stop(\"The concentration values to integrate over must always be provided.\")\n  \n  cleanData <- if (missing(Hill_fit)) {\n    Hill_fit <- logLogisticRegression(concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    sanitizeInput(conc = concentration, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  } else {\n    sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n  }\n  \n  pars <- cleanData[[\"Hill_fit\"]]\n  concentration <- cleanData[[\"log_conc\"]]\n  \n  if(pars[[3]] > max(concentration) || (censor && (pars[[2]] > 50 || all(concentration < pars[[3]]))))\n    return(if(censor && pars[[2]] > 50) NA else 0)\n  \n  if(!viability_as_pct) {\n    t_param <- t_param * 100\n    pars[[2]] <- pars[[2]] * 100\n  }\n  \n  x2 <- max(concentration)\n  x1 <- max(computeICn(concentration, unlist(pars), t_param, conc_as_log = TRUE, viability_as_pct = TRUE), min(concentration))\n  if(!is.finite(x1)) return(0)\n  \n  AUC <- computeAUC(c(x1, x2), unlist(pars), conc_as_log = TRUE, viability_as_pct = TRUE, verbose = verbose, trunc = trunc)\n  \n  DSS <- (AUC * (x2 - x1) - t_param * (x2 - x1)) / ((100 - t_param) * (max(concentration) - min(concentration)))\n  if (dss_type == 1) return(DSS)\n  \n  DSS <- DSS / log(100 - pars[[2]])\n  if (dss_type == 2) return(DSS)\n  \n  DSS <- DSS * (x2 - x1) / (max(concentration) - min(concentration))\n  if (dss_type == 3) return(DSS)\n  \n  stop(\"Invalid DSS type entered.\")\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeICn.R",
    "language": "R",
    "content": "#' Computes the ICn for any n in 0-100 for a Drug Dose Viability Curve\n#' \n#' Returns the ICn for any given nth percentile when given concentration and viability as input, normalized by the concentration\n#' range of the experiment. A Hill Slope is first fit to the data, and the ICn is inferred from the fitted curve. Alternatively, the parameters\n#' of a Hill Slope returned by logLogisticRegression can be passed in if they already known. \n#' \n#' @examples\n#' dose <- c(0.0025,0.008,0.025,0.08,0.25,0.8,2.53,8) \n#' viability <- c(108.67,111,102.16,100.27,90,87,74,57)\n#' computeIC50(dose, viability)\n#' computeICn(dose, viability, n=10)\n#' \n#' @param concentration `numeric` is a vector of drug concentrations.\n#' @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#' drug concentrations whose logarithms are in the corresponding entries of conc, where viability 0\n#' indicates that all cells died, and viability 1 indicates that the drug had no effect on the cells. \n#' @param Hill_fit `list` or `vector` In the order: c(\"Hill Slope\", \"E_inf\", \"EC50\"), the parameters of a Hill Slope \n#' as returned by logLogisticRegression. If conc_as_log is set then the function assumes logEC50 is passed in, and if\n#' viability_as_pct flag is set, it assumes E_inf is passed in as a percent. Otherwise, E_inf is assumed to be a decimal, \n#' and EC50 as a concentration. \n#' @param n `numeric` The percentile concentration to compute. If viability_as_pct set, assumed to be percentage, otherwise\n#' assumed to be a decimal value.\n#' @param conc_as_log `logical`, if true, assumes that log10-concentration data has been given rather than concentration data,\n#' and that log10(ICn) should be returned instead of ICn.\n#' @param viability_as_pct `logical`, if false, assumes that viability is given as a decimal rather\n#' than a percentage, and that E_inf passed in as decimal.\n#' @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#' curve-fitting is performed.\n#' @param verbose `logical`, if true, causes warnings thrown by the function to be printed.\n#' @return a numeric value for the concentration of the nth precentile viability reduction \n#' @export\ncomputeICn <- function(concentration,\n                       viability,\n                       Hill_fit,\n                       n,\n                       conc_as_log = FALSE,\n                       viability_as_pct = TRUE, \n                       verbose = TRUE,\n                       trunc = TRUE) {\n\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n\n    Hill_fit <- logLogisticRegression(conc = concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n    concentration <- cleanData[[\"log_conc\"]]\n  } else if (!missing(Hill_fit)){\n\n    cleanData <- sanitizeInput(conc = concentration, \n      viability = viability,\n      Hill_fit = Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[[\"Hill_fit\"]]\n  } else {\n\n    stop(\"Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.\")\n\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n\n  \n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) {\n    return(NA_real_)\n  } else if (n == pars[2]) {\n\n    return(Inf)\n\n  } else if (n == 1) {\n\n    return(ifelse(conc_as_log, -Inf, 0))\n\n  } else {\n\n    return(ifelse(conc_as_log,\n      log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n      10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n\n  }\n\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `computeICn` function and what are its main input parameters?",
        "answer": "The `computeICn` function computes the ICn (Inhibitory Concentration) for any nth percentile of a Drug Dose Viability Curve. It takes the following main input parameters: `concentration` (a vector of drug concentrations), `viability` (a vector of corresponding viability values), `Hill_fit` (optional, parameters of a Hill Slope), `n` (the percentile concentration to compute), and several boolean flags for input format and processing options."
      },
      {
        "question": "How does the function handle different input formats for concentration and viability data?",
        "answer": "The function uses boolean flags to handle different input formats: `conc_as_log` determines if the concentration data is given as log10 values, `viability_as_pct` specifies if viability is given as a percentage or decimal, and `trunc` indicates whether to truncate viability data between 0 and 1. These flags allow the function to adapt to various input formats and ensure correct processing of the data."
      },
      {
        "question": "What is the significance of the `Hill_fit` parameter, and how does the function behave if it's missing?",
        "answer": "The `Hill_fit` parameter represents the parameters of a Hill Slope curve (Hill Slope, E_inf, EC50). If `Hill_fit` is missing and both `concentration` and `viability` are provided, the function calls `logLogisticRegression` to compute the Hill Slope parameters. If `Hill_fit` is provided, it uses these parameters directly. This allows flexibility in using pre-computed Hill Slope parameters or calculating them from raw data."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)){\n    cleanData <- sanitizeInput(conc = concentration, \n      viability = viability,\n      Hill_fit = Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  # Complete the function by adding the logic to calculate and return the ICn value\n  \n}",
        "complete": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration,\n      viability,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, \n      Hill_fit=Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)){\n    cleanData <- sanitizeInput(conc = concentration, \n      viability = viability,\n      Hill_fit = Hill_fit,\n      conc_as_log = conc_as_log,\n      viability_as_pct = viability_as_pct,\n      trunc = trunc,\n      verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) {\n    return(NA_real_)\n  } else if (n == pars[2]) {\n    return(Inf)\n  } else if (n == 1) {\n    return(ifelse(conc_as_log, -Inf, 0))\n  } else {\n    return(ifelse(conc_as_log,\n      log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n      10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n  }\n}"
      },
      {
        "partial": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  # Add input validation and data preparation\n  \n  # Calculate n value\n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  # Implement the ICn calculation logic\n  \n}",
        "complete": "computeICn <- function(concentration, viability, Hill_fit, n, conc_as_log = FALSE, viability_as_pct = TRUE, verbose = TRUE, trunc = TRUE) {\n  if (missing(Hill_fit) & !missing(concentration) & !missing(viability)) {\n    Hill_fit <- logLogisticRegression(conc = concentration, viability, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    cleanData <- sanitizeInput(conc=concentration, Hill_fit=Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n    concentration <- cleanData[['log_conc']]\n  } else if (!missing(Hill_fit)){\n    cleanData <- sanitizeInput(conc = concentration, viability = viability, Hill_fit = Hill_fit, conc_as_log = conc_as_log, viability_as_pct = viability_as_pct, trunc = trunc, verbose = verbose)\n    pars <- cleanData[['Hill_fit']]\n  } else {\n    stop('Insufficient information to calculate ICn. Please enter concentration and viability or Hill parameters.')\n  }\n  \n  if(viability_as_pct){\n    n <- n/100\n  }\n  n <- 1 - n\n  \n  if (n < pars[2] || n > 1) return(NA_real_)\n  if (n == pars[2]) return(Inf)\n  if (n == 1) return(ifelse(conc_as_log, -Inf, 0))\n  \n  return(ifelse(conc_as_log,\n    log10(10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])),\n    10 ^ pars[3] * ((n - 1) / (pars[2] - n)) ^ (1 / pars[1])))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-intersect.R",
    "language": "R",
    "content": "# ==== PharmacoSet Class\n\n## TODO:: Can we implement intersect method for PSets?\n\n# ==== LongTable Class\n\n## TODO:: Implement intersection of LongTable objects",
    "qa_pairs": [
      {
        "question": "What are the main classes mentioned in this code snippet, and what is the purpose of the TODO comments associated with them?",
        "answer": "The main classes mentioned in this code snippet are PharmacoSet and LongTable. The TODO comments suggest implementing an 'intersect' method for PSets (likely an abbreviation for PharmacoSet) and an 'intersection' method for LongTable objects. These comments indicate that the developers are planning to add functionality to find common elements or data between instances of these classes."
      },
      {
        "question": "What is the difference between 'intersect' and 'intersection' as mentioned in the TODO comments?",
        "answer": "While 'intersect' and 'intersection' are semantically similar, their usage in the TODO comments suggests a subtle difference in implementation or naming convention between the two classes. For the PharmacoSet class, the comment uses 'intersect' as a method name, while for the LongTable class, it uses 'intersection'. This could indicate either a slight difference in functionality or simply a variation in naming convention between the two classes. In both cases, the intended purpose is likely to find common elements or data between instances of the respective classes."
      },
      {
        "question": "Based on the class names and TODO comments, what kind of application or domain might this code be part of?",
        "answer": "The class names and TODO comments suggest that this code is likely part of a bioinformatics or pharmaceutical research application. The 'PharmacoSet' class name implies a collection or set of pharmacological data, while 'LongTable' suggests a data structure for handling large datasets, which are common in scientific research. The planned intersection functionality could be useful for comparing different drug sets or experimental results, which is a common task in pharmaceutical research and data analysis."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class PharmacoSet:\n    def __init__(self):\n        self.data = []\n    \n    def add(self, item):\n        self.data.append(item)\n    \n    def intersect(self, other):\n        # TODO: Implement intersection of PharmacoSet objects\n        pass",
        "complete": "class PharmacoSet:\n    def __init__(self):\n        self.data = []\n    \n    def add(self, item):\n        self.data.append(item)\n    \n    def intersect(self, other):\n        return PharmacoSet([item for item in self.data if item in other.data])"
      },
      {
        "partial": "class LongTable:\n    def __init__(self):\n        self.data = {}\n    \n    def add(self, key, value):\n        self.data[key] = value\n    \n    def intersection(self, other):\n        # TODO: Implement intersection of LongTable objects\n        pass",
        "complete": "class LongTable:\n    def __init__(self):\n        self.data = {}\n    \n    def add(self, key, value):\n        self.data[key] = value\n    \n    def intersection(self, other):\n        return LongTable({k: v for k, v in self.data.items() if k in other.data and v == other.data[k]})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/pet.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport warnings\nimport datetime\nfrom typing import Optional, Dict, TypeVar\nimport numpy as np\nfrom matplotlib import pyplot as plt\nimport SimpleITK as sitk\nfrom pydicom import dcmread\n\nT = TypeVar('T')\n\n\ndef read_image(path:str,series_id: Optional[str]=None):\n    reader = sitk.ImageSeriesReader()\n    dicom_names = reader.GetGDCMSeriesFileNames(path,seriesID=series_id if series_id else \"\")\n    reader.SetFileNames(dicom_names)\n    reader.MetaDataDictionaryArrayUpdateOn()\n    reader.LoadPrivateTagsOn()\n\n    return reader.Execute()\n\n\nclass PET(sitk.Image):\n    def __init__(self, img_pet, df, factor, calc, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_pet)\n        self.img_pet = img_pet\n        self.df = df\n        self.factor = factor\n        self.calc = calc\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n    \n    @classmethod\n    def from_dicom_pet(cls, path,series_id=None,type=\"SUV\"):\n        '''\n        Reads the PET scan and returns the data frame and the image dosage in SITK format\n        There are two types of existing formats which has to be mentioned in the type\n        type:\n            SUV: gets the image with each pixel value having SUV value\n            ACT: gets the image with each pixel value having activity concentration\n        SUV = Activity concenteration/(Injected dose quantity/Body weight)\n\n        Please refer to the pseudocode: https://qibawiki.rsna.org/index.php/Standardized_Uptake_Value_(SUV) \n        If there is no data on SUV/ACT then backup calculation is done based on the formula in the documentation, although, it may\n        have some error.\n        '''\n        pet      = read_image(path,series_id)\n        path_one = pathlib.Path(path,os.listdir(path)[0]).as_posix()\n        df       = dcmread(path_one)\n        calc     = False\n        try:\n            if type==\"SUV\":\n                factor = df.to_json_dict()['70531000'][\"Value\"][0]\n            else:\n                factor = df.to_json_dict()['70531009']['Value'][0]\n        except:\n            warnings.warn(\"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\")\n            factor = cls.calc_factor(df,type)\n            calc = True\n        img_pet = sitk.Cast(pet, sitk.sitkFloat32)\n\n        # SimpleITK reads some pixel values as negative but with correct value\n        img_pet = sitk.Abs(img_pet * factor)\n\n        metadata = {}\n        return cls(img_pet, df, factor, calc, metadata)\n        # return cls(img_pet, df, factor, calc)\n        \n    def get_metadata(self):\n        '''\n        Forms the important metadata for reference in the dictionary format\n        {\n            scan_time (in seconds): AcquisitionTime \n            injection_time (in seconds): RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime\n            weight (in kg): PatientWeight\n            half_life (in seconds): RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife\n            injected_dose: RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose\n            Values_Assumed: True when some values are not available and are assumed for the calculation of SUV factor\n            factor: factor used for rescaling to bring it to SUV or ACT\n        }\n        '''\n        self.metadata = {}\n        try:\n            self.metadata[\"weight\"] = float(self.df.PatientWeight)\n        except:\n            pass\n        try:\n            self.metadata[\"scan_time\"] = datetime.datetime.strptime(self.df.AcquisitionTime, '%H%M%S.%f')\n            self.metadata[\"injection_time\"] = datetime.datetime.strptime(self.df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n            self.metadata[\"half_life\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n            self.metadata[\"injected_dose\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n        except:\n            pass\n        self.metadata[\"factor\"] = self.factor\n        self.metadata[\"Values_Assumed\"] = self.calc\n        return self.metadata\n\n    def resample_pet(self,\n                     ct_scan: sitk.Image) -> sitk.Image:\n        '''\n        Resamples the PET scan so that it can be overlayed with CT scan. The beginning and end slices of the \n        resampled PET scan might be empty due to the interpolation\n        '''\n        resampled_pt = sitk.Resample(self.img_pet, ct_scan)  # , interpolator=sitk.sitkNearestNeighbor) # commented interporator due to error\n        return resampled_pt\n\n    def show_overlay(self,\n                     ct_scan: sitk.Image,\n                     slice_number: int) -> plt.figure:\n        '''\n        For a given slice number, the function resamples PET scan and overlays on top of the CT scan and returns the figure of the\n        overlay\n        '''\n        resampled_pt = self.resample_pet(ct_scan)\n        fig = plt.figure(\"Overlayed image\", figsize=[15, 10])\n        pt_arr = sitk.GetArrayFromImage(resampled_pt)\n        plt.subplot(1,3,1)\n        plt.imshow(pt_arr[slice_number,:,:])\n        plt.subplot(1,3,2)\n        ct_arr = sitk.GetArrayFromImage(ct_scan)\n        plt.imshow(ct_arr[slice_number,:,:])\n        plt.subplot(1,3,3)\n        plt.imshow(ct_arr[slice_number,:,:], cmap=plt.cm.gray)\n        plt.imshow(pt_arr[slice_number,:,:], cmap=plt.cm.hot, alpha=.4)\n        return fig\n\n    @staticmethod\n    def calc_factor(df, type: str):\n        '''\n        Following the calculation formula stated in https://gist.github.com/pangyuteng/c6a075ba9aa00bb750468c30f13fc603\n        '''\n        # Fetching some required Meta Data\n        try:\n            weight = float(df.PatientWeight) * 1000\n        except:\n            warnings.warn(\"Patient Weight Not Present. Taking 75Kg\")\n            weight = 75000\n        try:\n            scan_time = datetime.datetime.strptime(df.AcquisitionTime, '%H%M%S.%f')\n            injection_time = datetime.datetime.strptime(df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n            half_life = float(df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n            injected_dose = float(df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n\n            # Calculate activity concenteration factor\n            a = np.exp(-np.log(2) * ((scan_time - injection_time).seconds / half_life))\n\n            # Calculate SUV factor\n            injected_dose_decay = a * injected_dose\n        except:\n            warnings.warn(\"Not enough data available, taking average values\")\n            a = np.exp(-np.log(2) * (1.75 * 3600) / 6588)  # 90 min waiting time, 15 min preparation\n            injected_dose_decay = 420000000 * a  # 420 MBq\n\n        suv = weight/injected_dose_decay\n        if type == \"SUV\":\n            return suv\n        else:\n            return 1/a\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `read_image` function in the given code snippet, and how does it handle DICOM series?",
        "answer": "The `read_image` function is designed to read a DICOM image series from a specified path. It uses SimpleITK's ImageSeriesReader to load DICOM files. The function performs the following steps:\n1. Creates an ImageSeriesReader object.\n2. Gets the file names of the DICOM series using GetGDCMSeriesFileNames.\n3. Sets the file names for the reader.\n4. Enables metadata dictionary array updates and loading of private tags.\n5. Executes the reader to load the image series.\n\nThe function can optionally filter by a specific series ID if provided. This allows for reading specific DICOM series from a directory containing multiple series."
      },
      {
        "question": "Explain the `PET` class's `from_dicom_pet` method. What are its parameters, and how does it handle different PET image types?",
        "answer": "The `from_dicom_pet` class method is a factory method for creating a `PET` object from DICOM files. Its parameters are:\n- `path`: The directory containing the DICOM files.\n- `series_id` (optional): To specify a particular series.\n- `type`: Either 'SUV' or 'ACT' to determine the image format.\n\nThe method handles two types of PET images:\n1. SUV (Standardized Uptake Value): Pixel values represent SUV.\n2. ACT (Activity Concentration): Pixel values represent activity concentration.\n\nIt reads the image using `read_image`, extracts metadata from the first DICOM file, and attempts to get the scaling factor from the DICOM tags. If the scaling factor is not available, it calculates it using the `calc_factor` method. The method then scales the image values, converts them to absolute values, and returns a new `PET` object with the processed image and metadata."
      },
      {
        "question": "How does the `resample_pet` method work, and why is it necessary when overlaying PET images on CT scans?",
        "answer": "The `resample_pet` method in the `PET` class is used to resample the PET image to match the dimensions and spacing of a CT scan. Here's how it works:\n\n1. It takes a CT scan (as a SimpleITK Image) as input.\n2. It uses SimpleITK's `Resample` function to resample the PET image (`self.img_pet`) to match the CT scan's properties.\n3. The resampled PET image is returned.\n\nThis method is necessary when overlaying PET images on CT scans because:\n1. PET and CT scans often have different resolutions and slice thicknesses.\n2. To accurately overlay the images, they need to have the same dimensions and spatial alignment.\n3. Resampling ensures that each voxel in the PET image corresponds to the same anatomical location as in the CT image.\n\nBy resampling the PET image to match the CT scan, it allows for proper fusion and visualization of the two imaging modalities, which is crucial for accurate interpretation in medical imaging."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class PET(sitk.Image):\n    def __init__(self, img_pet, df, factor, calc, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_pet)\n        self.img_pet = img_pet\n        self.df = df\n        self.factor = factor\n        self.calc = calc\n        self.metadata = metadata or {}\n    \n    @classmethod\n    def from_dicom_pet(cls, path, series_id=None, type=\"SUV\"):\n        pet = read_image(path, series_id)\n        path_one = pathlib.Path(path, os.listdir(path)[0]).as_posix()\n        df = dcmread(path_one)\n        calc = False\n        try:\n            factor = df.to_json_dict()['70531000' if type == \"SUV\" else '70531009']['Value'][0]\n        except:\n            warnings.warn(\"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\")\n            factor = cls.calc_factor(df, type)\n            calc = True\n        img_pet = sitk.Cast(pet, sitk.sitkFloat32)\n        img_pet = sitk.Abs(img_pet * factor)\n        # Complete the method",
        "complete": "class PET(sitk.Image):\n    def __init__(self, img_pet, df, factor, calc, metadata: Optional[Dict[str, T]] = None):\n        super().__init__(img_pet)\n        self.img_pet = img_pet\n        self.df = df\n        self.factor = factor\n        self.calc = calc\n        self.metadata = metadata or {}\n    \n    @classmethod\n    def from_dicom_pet(cls, path, series_id=None, type=\"SUV\"):\n        pet = read_image(path, series_id)\n        path_one = pathlib.Path(path, os.listdir(path)[0]).as_posix()\n        df = dcmread(path_one)\n        calc = False\n        try:\n            factor = df.to_json_dict()['70531000' if type == \"SUV\" else '70531009']['Value'][0]\n        except:\n            warnings.warn(\"Scale factor not available in DICOMs. Calculating based on metadata, may contain errors\")\n            factor = cls.calc_factor(df, type)\n            calc = True\n        img_pet = sitk.Cast(pet, sitk.sitkFloat32)\n        img_pet = sitk.Abs(img_pet * factor)\n        return cls(img_pet, df, factor, calc)"
      },
      {
        "partial": "def get_metadata(self):\n    self.metadata = {}\n    try:\n        self.metadata[\"weight\"] = float(self.df.PatientWeight)\n    except:\n        pass\n    try:\n        self.metadata[\"scan_time\"] = datetime.datetime.strptime(self.df.AcquisitionTime, '%H%M%S.%f')\n        self.metadata[\"injection_time\"] = datetime.datetime.strptime(self.df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n        self.metadata[\"half_life\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n        self.metadata[\"injected_dose\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n    except:\n        pass\n    # Complete the method",
        "complete": "def get_metadata(self):\n    self.metadata = {}\n    try:\n        self.metadata[\"weight\"] = float(self.df.PatientWeight)\n    except:\n        pass\n    try:\n        self.metadata[\"scan_time\"] = datetime.datetime.strptime(self.df.AcquisitionTime, '%H%M%S.%f')\n        self.metadata[\"injection_time\"] = datetime.datetime.strptime(self.df.RadiopharmaceuticalInformationSequence[0].RadiopharmaceuticalStartTime, '%H%M%S.%f')\n        self.metadata[\"half_life\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideHalfLife)\n        self.metadata[\"injected_dose\"] = float(self.df.RadiopharmaceuticalInformationSequence[0].RadionuclideTotalDose)\n    except:\n        pass\n    self.metadata[\"factor\"] = self.factor\n    self.metadata[\"Values_Assumed\"] = self.calc\n    return self.metadata"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "warnings",
        "datetime",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "typing.Optional",
        "matplotlib.pyplot",
        "pydicom.dcmread"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_pipeline.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport shutil\nimport warnings\nfrom multiprocessing import cpu_count\n\nimport numpy as np\nimport SimpleITK as sitk\nimport pytest\n\nfrom imgtools.io import ImageFileLoader, ImageFileWriter\nfrom imgtools.ops import BaseInput as Input\nfrom imgtools.ops import BaseOutput as Output\nfrom imgtools.pipeline import Pipeline\n\n\n@pytest.fixture\ndef sample_input_output(tmp_path):\n    input_paths = []\n    output_paths = []\n    for i in range(2):\n        input_path = tmp_path / f\"input_{i}\"\n        output_path = tmp_path / f\"output_{i}\"\n        input_path.mkdir(exist_ok=True)\n        output_path.mkdir(exist_ok=True)\n        input_paths.append(input_path)\n        output_paths.append(output_path)\n\n        # generate some test data\n        test_inputs = [sitk.GetImageFromArray(np.random.random((10, 10, 2))) for _ in range(4)]\n        for j, img in enumerate(test_inputs):\n            path = input_path / f\"test{j}.nrrd\"\n            sitk.WriteImage(img, str(path))\n\n    yield input_paths, output_paths\n    # clean up\n    shutil.rmtree(tmp_path)\n\n\nclass SimplePipelineTest(Pipeline):\n    def __init__(self, input_path, output_path, n_jobs):\n        super().__init__(n_jobs=n_jobs, show_progress=False)\n        self.input_path = input_path\n        self.output_path = output_path\n        self.image_input = Input(\n            ImageFileLoader(self.input_path))\n        self.image_output = Output(\n            ImageFileWriter(self.output_path))\n\n    def process_one_subject(self, subject_id):\n        image = self.image_input(subject_id)\n        self.image_output(subject_id, image)\n\n@pytest.mark.parametrize(\"n_jobs\", [1, 2])\ndef test_output(n_jobs, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    pipeline = SimplePipelineTest(input_paths[0], output_paths[0], n_jobs)\n    pipeline.run()\n\n    input_dir, output_dir = input_paths[0], output_paths[0]\n\n    for input_file, output_file in zip(sorted(os.listdir(input_dir)), sorted(os.listdir(output_dir))):\n        assert os.path.exists(pathlib.Path(output_dir, output_file))\n        test_output = sitk.GetArrayFromImage(sitk.ReadImage(pathlib.Path(output_dir, output_file).as_posix()))\n        true_output = sitk.GetArrayFromImage(sitk.ReadImage(pathlib.Path(input_dir, input_file).as_posix()))\n        assert np.allclose(test_output, true_output)\n        print('passed assert')\n\n\nclass MultiInputPipelineTest(Pipeline):\n    def __init__(self, input_path_0, input_path_1, output_path_0, output_path_1, n_jobs, missing_strategy):\n        super().__init__(n_jobs=n_jobs, missing_strategy=missing_strategy, show_progress=False)\n        self.input_path_0 = input_path_0\n        self.input_path_1 = input_path_1\n        self.output_path_0 = output_path_0\n        self.output_path_1 = output_path_1\n        self.image_input_0 = Input(\n            ImageFileLoader(self.input_path_0))\n        self.image_input_1 = Input(\n            ImageFileLoader(self.input_path_1))\n        self.image_output_0 = Output(\n            ImageFileWriter(self.output_path_0))\n        self.image_output_1 = Output(\n            ImageFileWriter(self.output_path_1))\n\n    def process_one_subject(self, subject_id):\n        image_0 = self.image_input_0(subject_id)\n        image_1 = self.image_input_1(subject_id)\n        if image_0 is not None:\n            self.image_output_0(subject_id, image_0)\n        if image_1 is not None:\n            self.image_output_1(subject_id, image_1)\n\n@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"missing_strategy\", [\"pass\", \"drop\"])\ndef test_missing_handling(n_jobs, missing_strategy, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    # simulate partial missing data\n    os.remove(pathlib.Path(input_paths[0], \"test0.nrrd\").as_posix())\n    print(input_paths, output_paths)\n\n    pipeline = MultiInputPipelineTest(input_paths[0],\n                                      input_paths[1],\n                                      output_paths[0],\n                                      output_paths[1],\n                                      n_jobs,\n                                      missing_strategy)\n    with warnings.catch_warnings(record=True) as w:\n        pipeline.run()\n        assert len(w) == 1\n        assert missing_strategy in str(w[-1].message)\n\n    print(os.listdir(input_paths[1]), os.listdir(output_paths[1]))\n    if missing_strategy == \"drop\":\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nrrd\").as_posix()),\n            not os.path.exists(pathlib.Path(output_paths[1], \"test0.nrrd\").as_posix())\n        ])\n    else:\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nii.gz\").as_posix()),\n            os.path.exists(pathlib.Path(output_paths[1], \"test0.nii.gz\").as_posix())\n        ])\n    \n    \n    \n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `sample_input_output` fixture in the given code?",
        "answer": "The `sample_input_output` fixture is used to create temporary input and output directories for testing. It generates random test data (SimpleITK images) in the input directories, yields the paths to these directories for use in tests, and then cleans up the temporary directories after the tests are completed."
      },
      {
        "question": "How does the `SimplePipelineTest` class handle parallel processing, and what parameter controls this behavior?",
        "answer": "The `SimplePipelineTest` class inherits from the `Pipeline` class, which likely implements parallel processing. The `n_jobs` parameter in the constructor controls the number of parallel jobs. When `n_jobs` is set to a value greater than 1, the pipeline will attempt to process multiple subjects concurrently. The actual parallelization is handled by the parent `Pipeline` class."
      },
      {
        "question": "In the `test_missing_handling` function, what are the two different strategies for handling missing data, and how do they affect the output?",
        "answer": "The two strategies for handling missing data are 'pass' and 'drop', controlled by the `missing_strategy` parameter. When set to 'pass', the pipeline will process all subjects, skipping the missing input but still processing other available inputs for that subject. When set to 'drop', the pipeline will completely skip processing any subject with missing input data. This is verified in the test by checking the existence of output files for the deliberately removed input file."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class SimplePipelineTest(Pipeline):\n    def __init__(self, input_path, output_path, n_jobs):\n        super().__init__(n_jobs=n_jobs, show_progress=False)\n        self.input_path = input_path\n        self.output_path = output_path\n        self.image_input = Input(\n            ImageFileLoader(self.input_path))\n        self.image_output = Output(\n            ImageFileWriter(self.output_path))\n\n    def process_one_subject(self, subject_id):\n        # TODO: Implement the processing logic\n        pass",
        "complete": "class SimplePipelineTest(Pipeline):\n    def __init__(self, input_path, output_path, n_jobs):\n        super().__init__(n_jobs=n_jobs, show_progress=False)\n        self.input_path = input_path\n        self.output_path = output_path\n        self.image_input = Input(\n            ImageFileLoader(self.input_path))\n        self.image_output = Output(\n            ImageFileWriter(self.output_path))\n\n    def process_one_subject(self, subject_id):\n        image = self.image_input(subject_id)\n        self.image_output(subject_id, image)"
      },
      {
        "partial": "@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"missing_strategy\", [\"pass\", \"drop\"])\ndef test_missing_handling(n_jobs, missing_strategy, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    # simulate partial missing data\n    os.remove(pathlib.Path(input_paths[0], \"test0.nrrd\").as_posix())\n\n    pipeline = MultiInputPipelineTest(input_paths[0],\n                                      input_paths[1],\n                                      output_paths[0],\n                                      output_paths[1],\n                                      n_jobs,\n                                      missing_strategy)\n    # TODO: Implement the test logic\n    pass",
        "complete": "@pytest.mark.parametrize(\"n_jobs\", [1, 2])\n@pytest.mark.parametrize(\"missing_strategy\", [\"pass\", \"drop\"])\ndef test_missing_handling(n_jobs, missing_strategy, sample_input_output):\n    if cpu_count() < 2 and n_jobs == 2:\n        n_jobs = 0\n    input_paths, output_paths = sample_input_output\n    # simulate partial missing data\n    os.remove(pathlib.Path(input_paths[0], \"test0.nrrd\").as_posix())\n\n    pipeline = MultiInputPipelineTest(input_paths[0],\n                                      input_paths[1],\n                                      output_paths[0],\n                                      output_paths[1],\n                                      n_jobs,\n                                      missing_strategy)\n    with warnings.catch_warnings(record=True) as w:\n        pipeline.run()\n        assert len(w) == 1\n        assert missing_strategy in str(w[-1].message)\n\n    if missing_strategy == \"drop\":\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nrrd\").as_posix()),\n            not os.path.exists(pathlib.Path(output_paths[1], \"test0.nrrd\").as_posix())\n        ])\n    else:\n        assert all([\n            not os.path.exists(pathlib.Path(output_paths[0], \"test0.nii.gz\").as_posix()),\n            os.path.exists(pathlib.Path(output_paths[1], \"test0.nii.gz\").as_posix())\n        ])"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "shutil",
        "warnings",
        "numpy",
        "SimpleITK",
        "pytest"
      ],
      "from_imports": [
        "multiprocessing.cpu_count",
        "imgtools.io.ImageFileLoader",
        "imgtools.ops.BaseInput",
        "imgtools.ops.BaseOutput",
        "imgtools.pipeline.Pipeline"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_rest_3.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\"mapcompound\",{\n  result <- mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})\n\ntest_that(\"mapproperties\",{\n  props <- mapCID2Properties(ids = c(123, 456), properties = c(\"MolecularWeight\", \"CanonicalSMILES\"))\n\n  expect_data_table(\n    x = props,\n    types = c(\"integer\", \"character\", \"character\"),\n    ncols = 3,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})\n\ntest_that(\"getPubchemCompound\", {\n  result <- getPubchemCompound(2244)\n  expect_class(result, \"data.table\")\n\n  res2 <- getPubchemCompound(c(3672), query_only = T)\n  expect_class(res2, \"list\")\n\n  res3 <- getPubchemCompound(c(3672), raw = T)\n  expect_class(res3, \"list\")\n  expect_class(res3[[1]], \"httr2_response\")\n\n  res4 <- getPubchemCompound(\"erlotinib\", \"name\", \"cids\")\n  expect_class(res4, \"data.table\")\n})\n\n\ntest_that(\"getPubchemCompound Failure\", {\n  expect_error(getPubchemCompound(2244, properties = NULL))\n  expect_error(getPubchemCompound(2244, properties = c(1234, 1542)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CID` function in the given code snippet, and what does the test case verify about its output?",
        "answer": "The `mapCompound2CID` function appears to map compound names to their corresponding CID (Compound ID) in PubChem. The test case verifies that the function returns a data table with 2 columns (likely compound name and CID) and 2 rows, where the first column is of character type and the second is of integer type. This suggests the function takes compound names as input and returns their corresponding CIDs."
      },
      {
        "question": "How does the `mapCID2Properties` function differ from `mapCompound2CID`, and what properties are being requested in the test case?",
        "answer": "The `mapCID2Properties` function maps CIDs to specific properties of the compounds. Unlike `mapCompound2CID` which goes from names to CIDs, this function starts with CIDs and retrieves properties. In the test case, it's requesting 'MolecularWeight' and 'CanonicalSMILES' for the CIDs 123 and 456. The test verifies that the output is a data table with 3 columns (likely CID, MolecularWeight, and CanonicalSMILES) and 2 rows, with the first column being integer type and the other two being character type."
      },
      {
        "question": "What are the different ways the `getPubchemCompound` function can be called based on the test cases, and how do the return types differ?",
        "answer": "The `getPubchemCompound` function can be called in several ways:\n1. With a single CID (e.g., 2244), which returns a data.table.\n2. With a CID and `query_only = TRUE`, which returns a list.\n3. With a CID and `raw = TRUE`, which returns a list containing an httr2_response object.\n4. With a compound name, 'name' as the second argument, and 'cids' as the third argument, which returns a data.table.\n\nThe return types differ based on the arguments passed: it can return a data.table, a list, or a list containing an httr2_response object, depending on how it's called."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"mapcompound\", {\n  result <- mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})",
        "complete": "test_that(\"mapcompound\", {\n  result <- mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})"
      },
      {
        "partial": "test_that(\"getPubchemCompound\", {\n  result <- getPubchemCompound(2244)\n  expect_class(result, \"data.table\")\n\n  res2 <- getPubchemCompound(c(3672), query_only = T)\n  expect_class(res2, \"list\")\n\n  res3 <- getPubchemCompound(c(3672), raw = T)\n  expect_class(res3, \"list\")\n  expect_class(res3[[1]], \"httr2_response\")\n\n  res4 <- getPubchemCompound(\"erlotinib\", \"name\", \"cids\")\n  expect_class(res4, \"data.table\")\n})",
        "complete": "test_that(\"getPubchemCompound\", {\n  result <- getPubchemCompound(2244)\n  expect_class(result, \"data.table\")\n\n  res2 <- getPubchemCompound(c(3672), query_only = T)\n  expect_class(res2, \"list\")\n\n  res3 <- getPubchemCompound(c(3672), raw = T)\n  expect_class(res3, \"list\")\n  expect_class(res3[[1]], \"httr2_response\")\n\n  res4 <- getPubchemCompound(\"erlotinib\", \"name\", \"cids\")\n  expect_class(res4, \"data.table\")\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/oncotree.R",
    "language": "R",
    "content": "\n#' Get data from Oncotree API\n#'\n#' This function retrieves data from the Oncotree API based on the specified target.\n#'\n#' @param target A character vector specifying the target data to retrieve. \n#'              Valid options are \"versions\", \"mainTypes\", and \"tumorTypes\".\n#'\n#' @return A data table containing the retrieved data.\n#'\n#' @noRd \n#' @keywords internal\n.getRequestOncotree <- function(\n    target = c(\"versions\", \"mainTypes\", \"tumorTypes\")\n) {\n    \n    url <- \"http://oncotree.mskcc.org\"\n    targetClean <- match.arg(target)\n    .buildURL(url, \"api\", targetClean) |> \n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() |> \n        .asDT()\n}\n#' Get available Oncotree versions\n#'\n#' This function retrieves the available versions of Oncotree.\n#'\n#' @return A `data.table` containing available Oncotree versions.\n#'\n#' @export\ngetOncotreeVersions <- function() {\n    .getRequestOncotree(target=\"versions\")\n}\n\n#' Get the main types from the Oncotree database.\n#'\n#' This function retrieves the main types from the Oncotree database.\n#' \n#' @return A `data.table` containing the main types from the Oncotree database.\n#' \n#' @export\ngetOncotreeMainTypes <- function() {\n    res <- .getRequestOncotree(target=\"mainTypes\") \n    data.table::setnames(res, \"mainType\")\n    return(res)\n}\n\n\n#' Get the tumor types from the Oncotree database.\n#' \n#' This function retrieves the tumor types from the Oncotree database.\n#' \n#' @return A `data.table` containing the tumor types from the Oncotree database.\n#' \n#' @export\ngetOncotreeTumorTypes <- function() {\n    .getRequestOncotree(target=\"tumorTypes\")\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.getRequestOncotree()` function and how does it handle different target parameters?",
        "answer": "The `.getRequestOncotree()` function is an internal function designed to retrieve data from the Oncotree API. It accepts a 'target' parameter which can be 'versions', 'mainTypes', or 'tumorTypes'. The function uses `match.arg()` to validate and clean the input, constructs the appropriate URL, sends a request to the API, parses the JSON response, and returns the result as a data table. This function serves as the core mechanism for the three exported functions: `getOncotreeVersions()`, `getOncotreeMainTypes()`, and `getOncotreeTumorTypes()`."
      },
      {
        "question": "How does the `getOncotreeMainTypes()` function differ from the other two exported functions in terms of its implementation?",
        "answer": "The `getOncotreeMainTypes()` function differs from `getOncotreeVersions()` and `getOncotreeTumorTypes()` in that it performs additional data manipulation after retrieving the data. Specifically, it uses `data.table::setnames(res, \"mainType\")` to rename the column in the returned data table. This suggests that the API response for main types might have a different column name that needs to be standardized. The other two functions simply return the data as received from the `.getRequestOncotree()` function without any further processing."
      },
      {
        "question": "What is the significance of the `#' @noRd` and `#' @keywords internal` tags in the documentation for the `.getRequestOncotree()` function?",
        "answer": "The `#' @noRd` tag stands for 'no Rd', which means this function should not have its own help page in the package documentation. The `#' @keywords internal` tag indicates that this function is intended for internal use within the package and should not be exposed to end users. These tags are used to hide implementation details from users while still allowing package developers to document the function for maintenance purposes. This is in contrast to the other three functions (`getOncotreeVersions()`, `getOncotreeMainTypes()`, and `getOncotreeTumorTypes()`), which are marked with `#' @export` and are intended to be used by end users of the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Get data from Oncotree API\n#'\n#' This function retrieves data from the Oncotree API based on the specified target.\n#'\n#' @param target A character vector specifying the target data to retrieve. \n#'              Valid options are \"versions\", \"mainTypes\", and \"tumorTypes\".\n#'\n#' @return A data table containing the retrieved data.\n#'\n#' @noRd \n#' @keywords internal\n.getRequestOncotree <- function(\n    target = c(\"versions\", \"mainTypes\", \"tumorTypes\")\n) {\n    \n    url <- \"http://oncotree.mskcc.org\"\n    targetClean <- match.arg(target)\n    # Complete the function body\n}",
        "complete": "#' Get data from Oncotree API\n#'\n#' This function retrieves data from the Oncotree API based on the specified target.\n#'\n#' @param target A character vector specifying the target data to retrieve. \n#'              Valid options are \"versions\", \"mainTypes\", and \"tumorTypes\".\n#'\n#' @return A data table containing the retrieved data.\n#'\n#' @noRd \n#' @keywords internal\n.getRequestOncotree <- function(\n    target = c(\"versions\", \"mainTypes\", \"tumorTypes\")\n) {\n    \n    url <- \"http://oncotree.mskcc.org\"\n    targetClean <- match.arg(target)\n    .buildURL(url, \"api\", targetClean) |> \n        .build_request() |>\n        .perform_request() |>\n        .parse_resp_json() |> \n        .asDT()\n}"
      },
      {
        "partial": "#' Get the main types from the Oncotree database.\n#'\n#' This function retrieves the main types from the Oncotree database.\n#' \n#' @return A `data.table` containing the main types from the Oncotree database.\n#' \n#' @export\ngetOncotreeMainTypes <- function() {\n    # Complete the function body\n}",
        "complete": "#' Get the main types from the Oncotree database.\n#'\n#' This function retrieves the main types from the Oncotree database.\n#' \n#' @return A `data.table` containing the main types from the Oncotree database.\n#' \n#' @export\ngetOncotreeMainTypes <- function() {\n    res <- .getRequestOncotree(target=\"mainTypes\") \n    data.table::setnames(res, \"mainType\")\n    return(res)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/segmentation.py",
    "language": "py",
    "content": "from functools import wraps\nimport warnings\n\nimport numpy as np\nimport SimpleITK as sitk\n\nfrom .sparsemask import SparseMask\n\nfrom ..utils import array_to_image, image_to_array\nfrom typing import Optional, Tuple, Set\n\n\ndef accepts_segmentations(f):\n    @wraps(f)\n    def wrapper(img, *args, **kwargs):\n        result = f(img, *args, **kwargs)\n        if isinstance(img, Segmentation):\n            result = sitk.Cast(result, sitk.sitkVectorUInt8)\n            return Segmentation(result, roi_indices=img.roi_indices, raw_roi_names=img.raw_roi_names)\n        else:\n            return result\n    return wrapper\n\n\ndef map_over_labels(segmentation, f, include_background=False, return_segmentation=True, **kwargs):\n    if include_background:\n        labels = range(segmentation.num_labels + 1)\n    else:\n        labels = range(1, segmentation.num_labels + 1)\n    res = [f(segmentation.get_label(label=label), **kwargs) for label in labels]\n    if return_segmentation and isinstance(res[0], sitk.Image):\n        res = [sitk.Cast(r, sitk.sitkUInt8) for r in res]\n        res = Segmentation(sitk.Compose(*res), roi_indices=segmentation.roi_indices, raw_roi_names=segmentation.raw_roi_names)\n    return res\n\n\nclass Segmentation(sitk.Image):\n    def __init__(self, \n                 segmentation, \n                 metadata: Optional[dict] = {}, \n                 roi_indices=None, \n                 existing_roi_indices=None, \n                 raw_roi_names={},\n                 frame_groups=None):\n        \"\"\"Initializes the Segmentation class\n        \n        Parameters\n        ----------\n        roi_indices\n            Dictionary of {\"ROI\": label number}\n        \n        existing_roi_indices\n            Dictionary of {\"ROI\": label number} of the existing ROIs\n\n        raw_roi_names\n            Dictionary of {\"ROI\": original countor names}\n\n        frame_groups\n            PerFrameFunctionalGroupsSequence (5200, 9230) DICOM metadata\n        \"\"\"\n        super().__init__(segmentation)\n        self.num_labels = self.GetNumberOfComponentsPerPixel()\n        self.raw_roi_names = raw_roi_names\n        self.metadata = metadata\n        self.frame_groups = frame_groups\n\n        if not roi_indices:\n            self.roi_indices = {f\"label_{i}\": i for i in range(1, self.num_labels+1)}\n        else:\n            self.roi_indices = roi_indices\n            if 0 in self.roi_indices.values():\n                self.roi_indices = {k : v+1 for k, v in self.roi_indices.items()}\n        \n        if len(self.roi_indices) != self.num_labels:\n            for i in range(1, self.num_labels+1):\n                if i not in self.roi_indices.values():\n                    self.roi_indices[f\"label_{i}\"] = i\n\n        self.existing_roi_indices = existing_roi_indices\n\n    def get_label(self, label=None, name=None, relabel=False):\n        if label is None and name is None:\n            raise ValueError(\"Must pass either label or name.\")\n\n        if label is None:\n            label = self.roi_indices[name]\n\n        if label == 0:\n            # background is stored implicitly and needs to be computed\n            label_arr = sitk.GetArrayViewFromImage(self)\n            label_img = sitk.GetImageFromArray((label_arr.sum(-1) == 0).astype(np.uint8))\n        else:\n            label_img = sitk.VectorIndexSelectionCast(self, label - 1)\n            if relabel:\n                label_img *= label\n\n        return label_img\n\n    def to_label_image(self):\n        arr, *_ = image_to_array(self)\n        # TODO handle overlapping labels\n        label_arr = np.where(arr.sum(-1) != 0, arr.argmax(-1) + 1, 0)\n        label_img = array_to_image(label_arr, reference_image=self)\n        return label_img\n\n    # TODO also overload other operators (arithmetic, etc.)\n    # with some sensible behaviour\n\n    def __getitem__(self, idx):\n        res = super().__getitem__(idx)\n        if isinstance(res, sitk.Image):\n            res = Segmentation(res, roi_indices=self.roi_indices, raw_roi_names=self.raw_roi_names)\n        return res\n\n    def __repr__(self):\n        return f\"<Segmentation with ROIs: {self.roi_indices!r}>\"\n         \n    def generate_sparse_mask(self, verbose=False) -> SparseMask:\n        \"\"\"\n        Generate a sparse mask from the contours, taking the argmax of all overlaps\n\n        Parameters\n        ----------\n        mask\n            Segmentation object to build sparse mask from\n\n        Returns\n        -------\n        SparseMask\n            The sparse mask object.\n        \"\"\"\n        # print(\"asdlkfjalkfsjg\", self.roi_indices)\n        mask_arr = np.transpose(sitk.GetArrayFromImage(self))\n        for name in self.roi_indices.keys():\n            self.roi_indices[name] = self.existing_roi_indices[name]\n        # print(self.roi_indices)\n        \n        sparsemask_arr = np.zeros(mask_arr.shape[1:])\n        \n        if verbose:\n            voxels_with_overlap = set()\n\n        if len(mask_arr.shape) == 4:\n            for i in range(mask_arr.shape[0]):\n                slice = mask_arr[i, :, :, :]\n                slice *= list(self.roi_indices.values())[i]  # everything is 0 or 1, so this is fine to convert filled voxels to label indices\n                if verbose:\n                    res = self._max_adder(sparsemask_arr, slice)\n                    sparsemask_arr = res[0]\n                    for e in res[1]:\n                        voxels_with_overlap.add(e)\n                else:\n                    sparsemask_arr = np.fmax(sparsemask_arr, slice)  # elementwise maximum\n        else:\n            sparsemask_arr = mask_arr\n        \n        sparsemask = SparseMask(sparsemask_arr, self.roi_indices)\n\n        if verbose:\n            if len(voxels_with_overlap) != 0:\n                warnings.warn(f\"{len(voxels_with_overlap)} voxels have overlapping contours.\")\n        return sparsemask\n\n    def _max_adder(self, arr_1: np.ndarray, arr_2: np.ndarray) -> Tuple[np.ndarray, Set[Tuple[int, int, int]]]:\n        \"\"\"\n        Takes the maximum of two 3D arrays elementwise and returns the resulting array and a list of voxels that have overlapping contours in a set\n\n        Parameters\n        ----------\n        arr_1\n            First array to take maximum of\n        arr_2\n            Second array to take maximum of\n        \n        Returns\n        -------\n        Tuple[np.ndarray, Set[Tuple[int, int, int]]]\n            The resulting array and a list of voxels that have overlapping contours in a set\n        \"\"\"\n        res = np.zeros(arr_1.shape)\n        overlaps = {}  # set of tuples of the coords that have overlap\n        for i in range(arr_1.shape[0]):\n            for j in range(arr_1.shape[1]):\n                for k in range(arr_1.shape[2]):\n                    if arr_1[i, j, k] != 0 and arr_2[i, j, k] != 0:\n                        overlaps.add((i, j, k))\n                    res[i, j, k] = max(arr_1[i, j, k], arr_2[i, j, k])\n        return res, overlaps\n\n    @classmethod\n    def from_dicom_seg(cls, mask, meta):\n        # get duplicates\n        label_counters = {i.SegmentLabel: 1 for i in meta.SegmentSequence}\n        raw_roi_names  = {}  # {i.SegmentLabel: i.SegmentNumber for n, i in meta.SegmentSequence}\n        for n, i in enumerate(meta.SegmentSequence):\n            label = i.SegmentLabel\n            num   = i.SegmentNumber\n\n            if label not in raw_roi_names:\n                raw_roi_names[label] = num\n            else:\n                raw_roi_names[f\"{label}_{label_counters[label]}\"] = num\n                label_counters[label] += 1\n        \n        frame_groups  = meta.PerFrameFunctionalGroupsSequence\n        return cls(mask, raw_roi_names=raw_roi_names, frame_groups=frame_groups)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `accepts_segmentations` decorator in the given code snippet?",
        "answer": "The `accepts_segmentations` decorator is used to wrap functions that operate on images. It ensures that if the input is a Segmentation object, the result is cast to a VectorUInt8 type and returned as a new Segmentation object with the same ROI indices and names. This allows functions to work seamlessly with both regular images and Segmentation objects, maintaining the segmentation-specific properties when applicable."
      },
      {
        "question": "How does the `map_over_labels` function handle background labels in a segmentation?",
        "answer": "The `map_over_labels` function processes each label in a segmentation separately. It has an `include_background` parameter that determines whether to process the background (label 0) or not. If `include_background` is True, it processes labels from 0 to num_labels. If False (default), it processes labels from 1 to num_labels, excluding the background. This allows for flexible handling of background labels in segmentation processing tasks."
      },
      {
        "question": "What is the purpose of the `generate_sparse_mask` method in the Segmentation class, and how does it handle overlapping contours?",
        "answer": "The `generate_sparse_mask` method creates a SparseMask object from the Segmentation. It converts the multi-label segmentation into a single-label representation by taking the argmax of overlapping labels. When `verbose` is True, it uses the `_max_adder` method to detect and report overlapping contours. This method returns both the resulting sparse mask and a set of voxel coordinates where overlaps occur, allowing for analysis of conflicting segmentations while generating a deterministic single-label representation."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def map_over_labels(segmentation, f, include_background=False, return_segmentation=True, **kwargs):\n    if include_background:\n        labels = range(segmentation.num_labels + 1)\n    else:\n        labels = range(1, segmentation.num_labels + 1)\n    res = [f(segmentation.get_label(label=label), **kwargs) for label in labels]\n    if return_segmentation and isinstance(res[0], sitk.Image):\n        # Complete the code here\n    return res",
        "complete": "def map_over_labels(segmentation, f, include_background=False, return_segmentation=True, **kwargs):\n    if include_background:\n        labels = range(segmentation.num_labels + 1)\n    else:\n        labels = range(1, segmentation.num_labels + 1)\n    res = [f(segmentation.get_label(label=label), **kwargs) for label in labels]\n    if return_segmentation and isinstance(res[0], sitk.Image):\n        res = [sitk.Cast(r, sitk.sitkUInt8) for r in res]\n        res = Segmentation(sitk.Compose(*res), roi_indices=segmentation.roi_indices, raw_roi_names=segmentation.raw_roi_names)\n    return res"
      },
      {
        "partial": "class Segmentation(sitk.Image):\n    def __init__(self, \n                 segmentation, \n                 metadata: Optional[dict] = {}, \n                 roi_indices=None, \n                 existing_roi_indices=None, \n                 raw_roi_names={},\n                 frame_groups=None):\n        super().__init__(segmentation)\n        self.num_labels = self.GetNumberOfComponentsPerPixel()\n        self.raw_roi_names = raw_roi_names\n        self.metadata = metadata\n        self.frame_groups = frame_groups\n\n        if not roi_indices:\n            self.roi_indices = {f\"label_{i}\": i for i in range(1, self.num_labels+1)}\n        else:\n            # Complete the code here\n\n        if len(self.roi_indices) != self.num_labels:\n            for i in range(1, self.num_labels+1):\n                if i not in self.roi_indices.values():\n                    self.roi_indices[f\"label_{i}\"] = i\n\n        self.existing_roi_indices = existing_roi_indices",
        "complete": "class Segmentation(sitk.Image):\n    def __init__(self, \n                 segmentation, \n                 metadata: Optional[dict] = {}, \n                 roi_indices=None, \n                 existing_roi_indices=None, \n                 raw_roi_names={},\n                 frame_groups=None):\n        super().__init__(segmentation)\n        self.num_labels = self.GetNumberOfComponentsPerPixel()\n        self.raw_roi_names = raw_roi_names\n        self.metadata = metadata\n        self.frame_groups = frame_groups\n\n        if not roi_indices:\n            self.roi_indices = {f\"label_{i}\": i for i in range(1, self.num_labels+1)}\n        else:\n            self.roi_indices = roi_indices\n            if 0 in self.roi_indices.values():\n                self.roi_indices = {k : v+1 for k, v in self.roi_indices.items()}\n\n        if len(self.roi_indices) != self.num_labels:\n            for i in range(1, self.num_labels+1):\n                if i not in self.roi_indices.values():\n                    self.roi_indices[f\"label_{i}\"] = i\n\n        self.existing_roi_indices = existing_roi_indices"
      }
    ],
    "dependencies": {
      "imports": [
        "warnings",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "functools.wraps",
        "sparsemask.SparseMask",
        "utils.array_to_image",
        "typing.Optional"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/utils-matchNested.R",
    "language": "R",
    "content": "#' Match inside nested elements\n#' \n#' @export\n#'\n#' @details\n#' Intentionally only performs exact matching. Refer to `filterNested` function\n#' for partial matching support with regular expressions.\n#'\n#' @param x\n#' The values to be matched.\n#'\n#' @param table\n#' The values to be matched against.\n#' Applies across rows for `DataFrame` method.\n#' \n#' @param ...\n#' Additional arguments to be passed to the method.\n#' \n#' @param keep_duplicates\n#' A logical value indicating whether to keep duplicates.\n#' \n#' @return `integer`.\n#' A positional vector corresponding to values defined in `table` the same\n#' size as `x`.\n#'\n#' @examples\n#' showMethods(\"matchNested\")\nsetGeneric(\n    name = \"matchNested\",\n    def = function(x, table, ..., keep_duplicates = FALSE) standardGeneric(\"matchNested\"),\n    signature = c(\"x\", \"table\", \"keep_duplicates\")\n)\n\n`matchNested,list` <- \n    function(x, table, keep_duplicates){\n        dt <- lapply(table, unlistNested) |> \n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n}\n\n`matchNested,data.table` <- \n    function(x, table, keep_duplicates){\n        checkmate::assert_data_table(table, min.rows = 1)\n\n        dt <- apply(\n            X = table,\n            MARGIN = 1L,\n            FUN = unlistNested,\n            simplify = FALSE\n            ) |>\n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n}\n\n\n`matchNested,data.frame`  <-\n    function(x, table, keep_duplicates){\n        checkmate::assert_data_frame(table, min.rows = 1)\n\n        dt <- apply(\n            X = table,\n            MARGIN = 1L,\n            FUN = unlistNested,\n            simplify = FALSE\n            ) |>\n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n}\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"list\"\n    ),\n    definition = `matchNested,list`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"numeric\",\n        table = \"list\"\n    ),\n    definition = `matchNested,list`\n)\n\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"data.table\"\n    ),\n    definition = `matchNested,data.table`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"numeric\",\n        table = \"data.table\"\n    ),\n    definition = `matchNested,data.table`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"data.frame\"\n    ),\n    definition = `matchNested,data.frame`\n)\n\n#' @rdname matchNested\n#' @export\nsetMethod(\n    f = \"matchNested\",\n    signature = signature(\n        x = \"character\",\n        table = \"data.frame\"\n    ),\n    definition = `matchNested,data.frame`\n)\n\n#' Convert Nested List to Data Table\n#'\n#' This function converts a nested list into a data table with two columns: \"idx\" and \"value\".\n#'\n#' @param unlisted_elements A nested list to be converted into a data table.\n#' @return A data table with two columns: \"idx\" and \"value\".\n#' @noRd \n#' @keywords internal\n.convert_nested_list_to_dt <- function(unlisted_elements){\n    idx <- rep(seq_along(unlisted_elements), times = lengths(unlisted_elements))\n    elements <- unlist(unlisted_elements, recursive = FALSE, use.names = FALSE)\n    data.table::data.table(\"idx\" = idx, \"value\" = elements)\n}\n\n\n#' Unlists a nested list and removes NA values and duplicates.\n#'\n#' This function takes a nested list as input and unlists it recursively. \n#' It then removes any NA values and duplicates from the resulting vector.\n#'\n#' @param element The nested list to be unlisted.\n#' @return A vector with NA values and duplicates removed.\n#' @examples\n#' nested_list <- list(a = list(1, 2, NA), b = list(3, 4, 5))\n#' unlistNested(nested_list)\n#' # Output: [1] 1 2 3 4 5\n#'\n#' @export\nunlistNested <- function(element){\n    unlist(element, recursive = TRUE, use.names = FALSE) |>\n        stats::na.omit() |>\n        unique()\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `matchNested` function and how does it handle different input types?",
        "answer": "The `matchNested` function is designed to match values inside nested elements. It uses method dispatch to handle different input types, including lists, data.tables, and data.frames. The function returns an integer vector corresponding to the positions of matched values in the `table` argument. It supports both character and numeric input for `x` and can work with various table types."
      },
      {
        "question": "How does the `keep_duplicates` parameter affect the behavior of the `matchNested` function?",
        "answer": "The `keep_duplicates` parameter is a logical value that determines whether duplicate values should be retained in the matching process. When `keep_duplicates` is set to `FALSE` (the default), the function removes duplicates from the converted data table using `dt <- dt[!duplicated(dt$value), , drop = FALSE]`. This ensures that only unique values are considered for matching, potentially improving performance and avoiding redundant matches."
      },
      {
        "question": "What is the role of the `.convert_nested_list_to_dt` helper function in the `matchNested` implementation?",
        "answer": "The `.convert_nested_list_to_dt` function is an internal helper function that converts a nested list into a data table with two columns: 'idx' and 'value'. It is used within the `matchNested` function to transform the input `table` into a standardized format for efficient matching. The function creates an index column ('idx') that corresponds to the original positions in the input list, and a 'value' column containing the flattened elements from the nested structure. This conversion allows for easier and more efficient matching operations in the main function."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setGeneric(\n    name = \"matchNested\",\n    def = function(x, table, ..., keep_duplicates = FALSE) standardGeneric(\"matchNested\"),\n    signature = c(\"x\", \"table\", \"keep_duplicates\")\n)\n\n`matchNested,list` <- \n    function(x, table, keep_duplicates){\n        dt <- lapply(table, unlistNested) |> \n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        # Complete the function here\n    }",
        "complete": "setGeneric(\n    name = \"matchNested\",\n    def = function(x, table, ..., keep_duplicates = FALSE) standardGeneric(\"matchNested\"),\n    signature = c(\"x\", \"table\", \"keep_duplicates\")\n)\n\n`matchNested,list` <- \n    function(x, table, keep_duplicates){\n        dt <- lapply(table, unlistNested) |> \n            .convert_nested_list_to_dt() \n\n        if (!keep_duplicates){\n            dt <- dt[!duplicated(dt$value), , drop = FALSE]\n        }\n\n        dt[dt[[\"value\"]] == x]$idx\n    }"
      },
      {
        "partial": "#' Convert Nested List to Data Table\n#'\n#' This function converts a nested list into a data table with two columns: \"idx\" and \"value\".\n#'\n#' @param unlisted_elements A nested list to be converted into a data table.\n#' @return A data table with two columns: \"idx\" and \"value\".\n#' @noRd \n#' @keywords internal\n.convert_nested_list_to_dt <- function(unlisted_elements){\n    # Complete the function here\n}",
        "complete": "#' Convert Nested List to Data Table\n#'\n#' This function converts a nested list into a data table with two columns: \"idx\" and \"value\".\n#'\n#' @param unlisted_elements A nested list to be converted into a data table.\n#' @return A data table with two columns: \"idx\" and \"value\".\n#' @noRd \n#' @keywords internal\n.convert_nested_list_to_dt <- function(unlisted_elements){\n    idx <- rep(seq_along(unlisted_elements), times = lengths(unlisted_elements))\n    elements <- unlist(unlisted_elements, recursive = FALSE, use.names = FALSE)\n    data.table::data.table(\"idx\" = idx, \"value\" = elements)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/codecover.R",
    "language": "R",
    "content": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\ncovr::codecov(quiet = FALSE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `Sys.unsetenv(\"R_TESTS\")` command in this code snippet?",
        "answer": "The `Sys.unsetenv(\"R_TESTS\")` command is used to unset the R_TESTS environment variable. This is often done to prevent certain tests from running or to modify the behavior of R during package checks or installations. In this context, it's likely used to ensure that the code coverage analysis runs without interference from R's built-in testing mechanisms."
      },
      {
        "question": "What does the `options(covr.fix_parallel_mcexit=TRUE)` line do in this code?",
        "answer": "This line sets an option for the 'covr' package. Specifically, it enables a fix for issues that can occur when running code coverage in parallel. The `covr.fix_parallel_mcexit=TRUE` option helps prevent errors related to parallel processing during code coverage analysis, ensuring more reliable results when using multiple cores or threads."
      },
      {
        "question": "What is the purpose of the `covr::codecov(quiet = FALSE)` function call in this snippet?",
        "answer": "The `covr::codecov(quiet = FALSE)` function call is used to run code coverage analysis and submit the results to Codecov, a service for tracking code coverage. The `quiet = FALSE` argument means that the function will provide verbose output during its execution, which can be useful for debugging or monitoring the progress of the coverage analysis. This is typically used in continuous integration pipelines to ensure that new code changes maintain or improve code coverage."
      }
    ],
    "completion_tasks": [
      {
        "partial": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\ncovr::codecov(",
        "complete": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\ncovr::codecov(quiet = FALSE)"
      },
      {
        "partial": "library(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\n\n# Run codecov with specific options\ncovr::codecov(",
        "complete": "library(covr)\noptions(covr.fix_parallel_mcexit=TRUE)\n\n# Run codecov with specific options\ncovr::codecov(quiet = FALSE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/imageutils.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\n\ndef physical_points_to_idxs(image, points, continuous=False):\n    if continuous:\n        transform = image.TransformPhysicalPointToContinuousIndex\n    else:\n        transform = image.TransformPhysicalPointToIndex\n    \n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    \n    # transform indices to ContourSequence/ContourData-wise\n    t_points = []\n    for slc in points:\n        t_points.append(vectorized_transform(slc)[:,::-1])\n    return t_points\n\n\ndef idxs_to_physical_points(image, idxs):\n    continuous = any([isinstance(i, float) for i in idxs])\n\n    if continuous:\n        transform = image.TransformContinuousIndexToPhysicalPoint\n    else:\n        transform = image.TransformIndexToPhysicalPoint\n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    return vectorized_transform(idxs)\n\n\ndef image_to_array(image):\n    origin, direction, spacing = image.GetOrigin(), image.GetDirection(), image.GetSpacing()\n    array = sitk.GetArrayFromImage(image)\n    return array, origin, direction, spacing\n\n\ndef show_image(image, mask=None, ax=None):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.subplots()\n\n    image_array, *_ = image_to_array(image)\n\n    ax.imshow(image_array, cmap=\"bone\", interpolation=\"bilinear\")\n\n    if mask is not None:\n        mask_array, *_ = image_to_array(mask)\n        mask_array = np.ma.masked_where(mask_array == 0, mask_array)\n\n        ax.imshow(mask_array, cmap=\"tab20\")\n\n    return ax\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `physical_points_to_idxs` function, and how does it handle continuous vs. non-continuous transformations?",
        "answer": "The `physical_points_to_idxs` function converts physical points to image indices. It uses SimpleITK's transformation methods to perform this conversion. The function has a `continuous` parameter that determines whether to use `TransformPhysicalPointToContinuousIndex` (for continuous indices) or `TransformPhysicalPointToIndex` (for discrete indices). It applies the chosen transformation to each point using NumPy's vectorize function and returns the transformed points with reversed coordinates."
      },
      {
        "question": "Explain the purpose of the `idxs_to_physical_points` function and how it determines whether to use continuous or discrete transformation.",
        "answer": "The `idxs_to_physical_points` function converts image indices to physical points. It automatically determines whether to use continuous or discrete transformation by checking if any of the input indices are floats. If floats are present, it uses `TransformContinuousIndexToPhysicalPoint`; otherwise, it uses `TransformIndexToPhysicalPoint`. The function then applies the chosen transformation to all indices using NumPy's vectorize function and returns the physical points."
      },
      {
        "question": "How does the `show_image` function visualize both the image and an optional mask? What plotting libraries does it use?",
        "answer": "The `show_image` function visualizes a SimpleITK image and an optional mask using matplotlib. It first converts the SimpleITK image to a NumPy array using the `image_to_array` function. The image is displayed using `imshow` with a 'bone' colormap. If a mask is provided, it's also converted to a NumPy array, and areas with zero values are masked. The mask is then overlaid on the image using a different colormap ('tab20'). This function allows for easy visualization of medical images with segmentation masks."
      }
    ],
    "completion_tasks": [
      {
        "partial": "import SimpleITK as sitk\nimport numpy as np\n\ndef physical_points_to_idxs(image, points, continuous=False):\n    if continuous:\n        transform = image.TransformPhysicalPointToContinuousIndex\n    else:\n        transform = image.TransformPhysicalPointToIndex\n    \n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    \n    # transform indices to ContourSequence/ContourData-wise\n    t_points = []\n    for slc in points:\n        # Complete the transformation of points\n    return t_points",
        "complete": "import SimpleITK as sitk\nimport numpy as np\n\ndef physical_points_to_idxs(image, points, continuous=False):\n    if continuous:\n        transform = image.TransformPhysicalPointToContinuousIndex\n    else:\n        transform = image.TransformPhysicalPointToIndex\n    \n    vectorized_transform = np.vectorize(lambda x: np.array(transform(x)), signature='(3)->(3)')\n    \n    # transform indices to ContourSequence/ContourData-wise\n    t_points = []\n    for slc in points:\n        t_points.append(vectorized_transform(slc)[:,::-1])\n    return t_points"
      },
      {
        "partial": "import SimpleITK as sitk\nimport numpy as np\n\ndef show_image(image, mask=None, ax=None):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.subplots()\n\n    image_array, *_ = image_to_array(image)\n\n    ax.imshow(image_array, cmap=\"bone\", interpolation=\"bilinear\")\n\n    if mask is not None:\n        # Complete the mask overlay\n\n    return ax",
        "complete": "import SimpleITK as sitk\nimport numpy as np\n\ndef show_image(image, mask=None, ax=None):\n    import matplotlib.pyplot as plt\n    if ax is None:\n        ax = plt.subplots()\n\n    image_array, *_ = image_to_array(image)\n\n    ax.imshow(image_array, cmap=\"bone\", interpolation=\"bilinear\")\n\n    if mask is not None:\n        mask_array, *_ = image_to_array(mask)\n        mask_array = np.ma.masked_where(mask_array == 0, mask_array)\n        ax.imshow(mask_array, cmap=\"tab20\")\n\n    return ax"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy",
        "matplotlib.pyplot"
      ],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/modules/structureset.py",
    "language": "py",
    "content": "import re\nfrom warnings import warn\nfrom typing import Dict, List, Optional, TypeVar\n\nimport numpy as np\nimport SimpleITK as sitk\nfrom pydicom import dcmread\nfrom itertools import groupby\nfrom skimage.draw import polygon2mask\n\nfrom .segmentation import Segmentation\nfrom ..utils import physical_points_to_idxs\n\nT = TypeVar('T')\n\n\ndef _get_roi_points(rtstruct, roi_index):\n    return [np.array(slc.ContourData).reshape(-1, 3) for slc in rtstruct.ROIContourSequence[roi_index].ContourSequence]\n\n\nclass StructureSet:\n    def __init__(self, roi_points: Dict[str, np.ndarray], metadata: Optional[Dict[str, T]] = None):\n        \"\"\"Initializes the StructureSet class containing contour points\n        \n        Parameters\n        ----------\n        roi_points\n            Dictionary of {\"ROI\": [ndarray of shape n x 3 of contour points]}\n        \n        metadata\n            Dictionary of DICOM metadata\n        \"\"\"\n        self.roi_points = roi_points\n        if metadata:\n            self.metadata = metadata\n        else:\n            self.metadata = {}\n\n    @classmethod\n    def from_dicom_rtstruct(cls, rtstruct_path: str) -> 'StructureSet':\n        rtstruct = dcmread(rtstruct_path, force=True)\n        roi_names = [roi.ROIName for roi in rtstruct.StructureSetROISequence]\n        roi_points = {}\n        for i, name in enumerate(roi_names):\n            try:\n                roi_points[name] = _get_roi_points(rtstruct, i)\n            except AttributeError:\n                warn(f\"Could not get points for ROI {name} (in {rtstruct_path}).\")\n\n        metadata = {}\n        \n        return cls(roi_points, metadata)\n\n    @property\n    def roi_names(self) -> List[str]:\n        return list(self.roi_points.keys())\n\n    def _assign_labels(self, \n                       names, \n                       roi_select_first: bool = False,\n                       roi_separate: bool = False):\n        \"\"\"\n        Parameters\n        ----\n        roi_select_first\n            Select the first matching ROI/regex for each OAR, no duplicate matches. \n\n        roi_separate\n            Process each matching ROI/regex as individual masks, instead of consolidating into one mask\n            Each mask will be named ROI_n, where n is the nth regex/name/string.\n        \"\"\"\n        labels = {}\n        cur_label = 0\n        if names == self.roi_names:\n            for i, name in enumerate(self.roi_names):\n                labels[name] = i\n        else:\n            for _, pattern in enumerate(names):\n                if sorted(names) == sorted(list(labels.keys())):  # checks if all ROIs have already been processed.\n                    break\n                if isinstance(pattern, str):\n                    for i, name in enumerate(self.roi_names):\n                        if re.fullmatch(pattern, name, flags=re.IGNORECASE):\n                            labels[name] = cur_label\n                            cur_label += 1\n                else:  # if multiple regex/names to match\n                    matched = False\n                    for subpattern in pattern:\n                        if roi_select_first and matched:  # break if roi_select_first and we're matched\n                            break\n                        for n, name in enumerate(self.roi_names):\n                            if re.fullmatch(subpattern, name, flags=re.IGNORECASE):\n                                matched = True\n                                if not roi_separate:\n                                    labels[name] = cur_label\n                                else:\n                                    labels[f\"{name}_{n}\"] = cur_label\n                                \n                    cur_label += 1\n        return labels\n\n    def get_mask(self, reference_image, mask, label, idx, continuous):\n        size = reference_image.GetSize()[::-1]\n        physical_points = self.roi_points.get(label, np.array([]))\n        mask_points = physical_points_to_idxs(reference_image, physical_points, continuous=continuous)\n        for contour in mask_points:\n            try:\n                z, slice_points = np.unique(contour[:, 0]), contour[:, 1:]\n                if len(z) == 1:  # assert len(z) == 1, f\"This contour ({name}) spreads across more than 1 slice.\"\n                    slice_mask = polygon2mask(size[1:], slice_points)\n                    mask[z[0], :, :, idx] += slice_mask\n            except:  # rounding errors for points on the boundary\n                if z == mask.shape[0]:\n                    z -= 1\n                elif z == -1: #?\n                    z += 1\n                elif z > mask.shape[0] or z < -1:\n                    raise IndexError(f\"{z} index is out of bounds for image sized {mask.shape}.\")\n                \n                # if the contour spans only 1 z-slice \n                if len(z) == 1:\n                    z_idx = int(np.floor(z[0]))\n                    slice_mask = polygon2mask(size[1:], slice_points)\n                    mask[z_idx, :, :, idx] += slice_mask\n                else:\n                    raise ValueError(\"This contour is corrupted and spans across 2 or more slices.\")\n\n    def to_segmentation(self, reference_image: sitk.Image,\n                        roi_names: Dict[str, str] = None,\n                        continuous: bool = True,\n                        existing_roi_indices: Dict[str, int] = None,\n                        ignore_missing_regex: bool = False,\n                        roi_select_first: bool = False,\n                        roi_separate: bool = False) -> Segmentation:\n        \"\"\"Convert the structure set to a Segmentation object.\n\n        Parameters\n        ----------\n        reference_image\n            Image used as reference geometry.\n        roi_names\n            List of ROI names to export. Both full names and\n            case-insensitive regular expressions are allowed.\n            All labels within one sublist will be assigned\n            the same label.\n\n        Returns\n        -------\n        Segmentation\n            The segmentation object.\n\n        Notes\n        -----\n        If `roi_names` contains lists of strings, each matching\n        name within a sublist will be assigned the same label. This means\n        that `roi_names=['pat']` and `roi_names=[['pat']]` can lead\n        to different label assignments, depending on how many ROI names\n        match the pattern. E.g. if `self.roi_names = ['fooa', 'foob']`,\n        passing `roi_names=['foo(a|b)']` will result in a segmentation with \n        two labels, but passing `roi_names=[['foo(a|b)']]` will result in\n        one label for both `'fooa'` and `'foob'`.\n\n        In general, the exact ordering of the returned labels cannot be\n        guaranteed (unless all patterns in `roi_names` can only match\n        a single name or are lists of strings).\n        \"\"\"\n        labels = {}\n        if roi_names is None or roi_names == {}:\n            roi_names = self.roi_names  # all the contour names\n            labels = self._assign_labels(roi_names, roi_select_first, roi_separate)  # only the ones that match the regex\n        elif isinstance(roi_names, dict):\n            for name, pattern in roi_names.items():\n                if isinstance(pattern, str):\n                    matching_names = list(self._assign_labels([pattern], roi_select_first).keys())\n                    if matching_names:\n                        labels[name] = matching_names  # {\"GTV\": [\"GTV1\", \"GTV2\"]} is the result of _assign_labels()\n                elif isinstance(pattern, list):  # for inputs that have multiple patterns for the input, e.g. {\"GTV\": [\"GTV.*\", \"HTVI.*\"]}\n                    labels[name] = []\n                    for pattern_one in pattern:\n                        matching_names = list(self._assign_labels([pattern_one], roi_select_first).keys())\n                        if matching_names:\n                            labels[name].extend(matching_names)  # {\"GTV\": [\"GTV1\", \"GTV2\"]}\n        if isinstance(roi_names, str):\n            roi_names = [roi_names]\n        if isinstance(roi_names, list):  # won't this always trigger after the previous?\n            labels = self._assign_labels(roi_names, roi_select_first)\n        print(\"labels:\", labels)\n        all_empty = True\n        for v in labels.values():\n            if v != []:\n                all_empty = False\n        if all_empty:\n            if not ignore_missing_regex:\n                raise ValueError(f\"No ROIs matching {roi_names} found in {self.roi_names}.\")\n            else:\n                return None\n        labels = {k:v for (k,v) in labels.items() if v != [] }\n        size = reference_image.GetSize()[::-1] + (len(labels),)\n        mask = np.zeros(size, dtype=np.uint8)\n\n        seg_roi_indices = {}\n        if roi_names != {} and isinstance(roi_names, dict):\n            for i, (name, label_list) in enumerate(labels.items()):\n                for label in label_list:\n                    self.get_mask(reference_image, mask, label, i, continuous)\n                seg_roi_indices[name] = i\n\n        else:\n            for name, label in labels.items():\n                self.get_mask(reference_image, mask, name, label, continuous)\n            seg_roi_indices = {\"_\".join(k): v for v, k in groupby(labels, key=lambda x: labels[x])}\n\n        mask[mask > 1] = 1\n        mask = sitk.GetImageFromArray(mask, isVector=True)\n        mask.CopyInformation(reference_image)\n        mask = Segmentation(mask, roi_indices=seg_roi_indices, existing_roi_indices=existing_roi_indices, raw_roi_names=labels)  # in the segmentation, pass all the existing roi names and then process is in the segmentation class\n\n        return mask\n\n    def __repr__(self):\n        return f\"<StructureSet with ROIs: {self.roi_names!r}>\"\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `_assign_labels` method in the `StructureSet` class, and how does it handle different input scenarios?",
        "answer": "The `_assign_labels` method in the `StructureSet` class is responsible for assigning numeric labels to ROI (Region of Interest) names. It handles different input scenarios as follows:\n\n1. If the input `names` matches `self.roi_names`, it assigns sequential labels to each ROI.\n2. For string patterns, it uses regex matching to assign labels to matching ROI names.\n3. For lists of patterns, it can either select the first matching ROI (if `roi_select_first` is True) or process each matching ROI separately (if `roi_separate` is True).\n\nThe method is flexible and can handle various input formats, including single strings, lists of strings, and nested lists for grouping multiple patterns."
      },
      {
        "question": "Explain the functionality of the `get_mask` method in the `StructureSet` class. What are its key parameters and what does it do?",
        "answer": "The `get_mask` method in the `StructureSet` class is responsible for creating a binary mask for a specific ROI (Region of Interest) based on its contour points. Key aspects of this method include:\n\n1. Parameters:\n   - `reference_image`: The image used as a reference for spatial information.\n   - `mask`: The output mask array where the ROI will be drawn.\n   - `label`: The name of the ROI to be processed.\n   - `idx`: The index in the mask array where the ROI should be placed.\n   - `continuous`: A boolean flag for point interpolation.\n\n2. Functionality:\n   - It converts physical contour points to image indices.\n   - For each contour, it creates a 2D polygon mask using `polygon2mask`.\n   - It handles edge cases, such as rounding errors for points on the boundary.\n   - The method adds the created mask to the appropriate slice in the 3D mask array.\n\nThis method is crucial for converting contour data into a volumetric binary mask representation of the ROI."
      },
      {
        "question": "How does the `to_segmentation` method in the `StructureSet` class work, and what are its key features for handling different ROI selection scenarios?",
        "answer": "The `to_segmentation` method in the `StructureSet` class converts the structure set to a `Segmentation` object. Key features and functionality include:\n\n1. Flexible ROI selection:\n   - Accepts various input formats for `roi_names` (None, dict, list, or string).\n   - Supports regex patterns for matching ROI names.\n\n2. Label assignment:\n   - Uses `_assign_labels` to map ROI names to numeric labels.\n   - Handles grouping of multiple ROIs under a single label.\n\n3. Mask creation:\n   - Creates a 3D mask array based on the reference image dimensions.\n   - Calls `get_mask` for each selected ROI to populate the mask array.\n\n4. Special options:\n   - `roi_select_first`: Selects only the first matching ROI for each pattern.\n   - `roi_separate`: Processes each matching ROI as individual masks.\n   - `ignore_missing_regex`: Allows the method to return None if no matching ROIs are found.\n\n5. Output:\n   - Returns a `Segmentation` object containing the created mask and ROI indices.\n\nThis method provides a comprehensive way to convert contour-based structure sets into volumetric segmentations, with various options for ROI selection and processing."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class StructureSet:\n    def __init__(self, roi_points: Dict[str, np.ndarray], metadata: Optional[Dict[str, T]] = None):\n        self.roi_points = roi_points\n        self.metadata = metadata or {}\n\n    @classmethod\n    def from_dicom_rtstruct(cls, rtstruct_path: str) -> 'StructureSet':\n        rtstruct = dcmread(rtstruct_path, force=True)\n        roi_names = [roi.ROIName for roi in rtstruct.StructureSetROISequence]\n        roi_points = {}\n        for i, name in enumerate(roi_names):\n            try:\n                roi_points[name] = _get_roi_points(rtstruct, i)\n            except AttributeError:\n                warn(f\"Could not get points for ROI {name} (in {rtstruct_path}).\")\n        return cls(roi_points, {})\n\n    @property\n    def roi_names(self) -> List[str]:\n        # TODO: Implement this method",
        "complete": "class StructureSet:\n    def __init__(self, roi_points: Dict[str, np.ndarray], metadata: Optional[Dict[str, T]] = None):\n        self.roi_points = roi_points\n        self.metadata = metadata or {}\n\n    @classmethod\n    def from_dicom_rtstruct(cls, rtstruct_path: str) -> 'StructureSet':\n        rtstruct = dcmread(rtstruct_path, force=True)\n        roi_names = [roi.ROIName for roi in rtstruct.StructureSetROISequence]\n        roi_points = {}\n        for i, name in enumerate(roi_names):\n            try:\n                roi_points[name] = _get_roi_points(rtstruct, i)\n            except AttributeError:\n                warn(f\"Could not get points for ROI {name} (in {rtstruct_path}).\")\n        return cls(roi_points, {})\n\n    @property\n    def roi_names(self) -> List[str]:\n        return list(self.roi_points.keys())"
      },
      {
        "partial": "def get_mask(self, reference_image, mask, label, idx, continuous):\n    size = reference_image.GetSize()[::-1]\n    physical_points = self.roi_points.get(label, np.array([]))\n    mask_points = physical_points_to_idxs(reference_image, physical_points, continuous=continuous)\n    for contour in mask_points:\n        try:\n            z, slice_points = np.unique(contour[:, 0]), contour[:, 1:]\n            if len(z) == 1:\n                # TODO: Implement mask creation for single slice\n            else:\n                raise ValueError(\"This contour is corrupted and spans across 2 or more slices.\")\n        except:\n            # TODO: Implement error handling for boundary cases",
        "complete": "def get_mask(self, reference_image, mask, label, idx, continuous):\n    size = reference_image.GetSize()[::-1]\n    physical_points = self.roi_points.get(label, np.array([]))\n    mask_points = physical_points_to_idxs(reference_image, physical_points, continuous=continuous)\n    for contour in mask_points:\n        try:\n            z, slice_points = np.unique(contour[:, 0]), contour[:, 1:]\n            if len(z) == 1:\n                slice_mask = polygon2mask(size[1:], slice_points)\n                mask[z[0], :, :, idx] += slice_mask\n            else:\n                raise ValueError(\"This contour is corrupted and spans across 2 or more slices.\")\n        except:\n            z = int(np.floor(contour[0, 0]))\n            if z == mask.shape[0]:\n                z -= 1\n            elif z == -1:\n                z += 1\n            elif z > mask.shape[0] or z < -1:\n                raise IndexError(f\"{z} index is out of bounds for image sized {mask.shape}.\")\n            slice_mask = polygon2mask(size[1:], slice_points)\n            mask[z, :, :, idx] += slice_mask"
      }
    ],
    "dependencies": {
      "imports": [
        "re",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "warnings.warn",
        "typing.Dict",
        "pydicom.dcmread",
        "itertools.groupby",
        "skimage.draw.polygon2mask",
        "segmentation.Segmentation",
        "utils.physical_points_to_idxs"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/src/RcppExports.cpp",
    "language": "cpp",
    "content": "// Generated by using Rcpp::compileAttributes() -> do not edit by hand\n// Generator token: 10BE3573-1514-4C36-9D1C-5A225CD40393\n\n#include <Rcpp.h>\n\nusing namespace Rcpp;\n\n#ifdef RCPP_USE_GLOBAL_ROSTREAM\nRcpp::Rostream<true>&  Rcpp::Rcout = Rcpp::Rcpp_cout_get();\nRcpp::Rostream<false>& Rcpp::Rcerr = Rcpp::Rcpp_cerr_get();\n#endif\n\n// partialCorQUICKSTOP\nextern \"C\" SEXP partialCorQUICKSTOP(SEXP pin_x, SEXP pin_y, SEXP pobsCor, SEXP pGroupFactor, SEXP pGroupSize, SEXP pnumGroup, SEXP pMaxIter, SEXP pn, SEXP preq_alpha, SEXP ptolerance_par, SEXP plog_decision_boundary, SEXP pseed);\nRcppExport SEXP _PharmacoGx_partialCorQUICKSTOP(SEXP pin_xSEXP, SEXP pin_ySEXP, SEXP pobsCorSEXP, SEXP pGroupFactorSEXP, SEXP pGroupSizeSEXP, SEXP pnumGroupSEXP, SEXP pMaxIterSEXP, SEXP pnSEXP, SEXP preq_alphaSEXP, SEXP ptolerance_parSEXP, SEXP plog_decision_boundarySEXP, SEXP pseedSEXP) {\nBEGIN_RCPP\n    Rcpp::RObject rcpp_result_gen;\n    Rcpp::RNGScope rcpp_rngScope_gen;\n    Rcpp::traits::input_parameter< SEXP >::type pin_x(pin_xSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pin_y(pin_ySEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pobsCor(pobsCorSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pGroupFactor(pGroupFactorSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pGroupSize(pGroupSizeSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pnumGroup(pnumGroupSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pMaxIter(pMaxIterSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pn(pnSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type preq_alpha(preq_alphaSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type ptolerance_par(ptolerance_parSEXP);\n    Rcpp::traits::input_parameter< SEXP >::type plog_decision_boundary(plog_decision_boundarySEXP);\n    Rcpp::traits::input_parameter< SEXP >::type pseed(pseedSEXP);\n    rcpp_result_gen = Rcpp::wrap(partialCorQUICKSTOP(pin_x, pin_y, pobsCor, pGroupFactor, pGroupSize, pnumGroup, pMaxIter, pn, preq_alpha, ptolerance_par, plog_decision_boundary, pseed));\n    return rcpp_result_gen;\nEND_RCPP\n}\n\nstatic const R_CallMethodDef CallEntries[] = {\n    {\"_PharmacoGx_partialCorQUICKSTOP\", (DL_FUNC) &_PharmacoGx_partialCorQUICKSTOP, 12},\n    {NULL, NULL, 0}\n};\n\nRcppExport void R_init_PharmacoGx(DllInfo *dll) {\n    R_registerRoutines(dll, NULL, CallEntries, NULL, NULL);\n    R_useDynamicSymbols(dll, FALSE);\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `BEGIN_RCPP` and `END_RCPP` macros in this code snippet?",
        "answer": "The `BEGIN_RCPP` and `END_RCPP` macros are used to wrap the main body of the function. They set up a try-catch block to handle exceptions that might occur during the execution of the Rcpp code. This ensures that any C++ exceptions are properly translated into R errors, maintaining a consistent error handling mechanism between R and C++."
      },
      {
        "question": "How does this code handle the input parameters for the `partialCorQUICKSTOP` function?",
        "answer": "The code uses `Rcpp::traits::input_parameter< SEXP >::type` to handle input parameters. Each parameter is declared with this type and initialized with the corresponding SEXP (S-expression) argument. This approach allows the function to accept various R data types as input and automatically convert them to appropriate C++ types, facilitating seamless integration between R and C++."
      },
      {
        "question": "What is the purpose of the `R_init_PharmacoGx` function in this code?",
        "answer": "The `R_init_PharmacoGx` function is an initialization function for the R package. It registers the C++ routines (specifically `_PharmacoGx_partialCorQUICKSTOP`) with R's dynamic symbol mechanism. This registration process links the C++ function to its corresponding R interface, allowing R to call the C++ function. The `R_useDynamicSymbols(dll, FALSE)` call disables dynamic lookup, which can improve performance and security."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/GWC.R",
    "language": "R",
    "content": "#' GWC Score\n#' \n#' @inherit CoreGx::gwc\n#' \n#' @examples\n#' data(CCLEsmall)\n#' x <- molecularProfiles(CCLEsmall,\"rna\")[,1]\n#' y <- molecularProfiles(CCLEsmall,\"rna\")[,2]\n#' x_p <- rep(0.05, times=length(x))\n#' y_p <- rep(0.05, times=length(y))\n#' names(x_p) <- names(x)\n#' names(y_p) <- names(y)\n#' gwc(x,x_p,y,y_p, nperm=100)\n#' \n#' @export\ngwc <-\nfunction (x1, p1, x2, p2, method.cor=c(\"pearson\", \"spearman\"), nperm=1e4, \n          truncate.p=1e-16, ...) {\n  CoreGx::gwc(x1, p1, x2, p2, method.cor, nperm, truncate.p, ...)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `gwc` function in this code snippet, and how does it relate to the `CoreGx` package?",
        "answer": "The `gwc` function in this code snippet is a wrapper for the `CoreGx::gwc` function. It inherits documentation from the `CoreGx::gwc` function and passes all its arguments directly to it. This allows users to call the `gwc` function from the current package while using the implementation from the `CoreGx` package."
      },
      {
        "question": "Explain the significance of the `@inherit CoreGx::gwc` roxygen2 tag in the function documentation.",
        "answer": "The `@inherit CoreGx::gwc` roxygen2 tag is used to inherit the documentation from the `CoreGx::gwc` function. This means that all the parameter descriptions, return value information, and other documentation details from the original `CoreGx::gwc` function will be automatically included in the documentation for this wrapper function. This saves time and ensures consistency in documentation between the wrapper and the original function."
      },
      {
        "question": "What is the purpose of the example provided in the function documentation, and what does it demonstrate?",
        "answer": "The example in the function documentation demonstrates how to use the `gwc` function with sample data. It loads a dataset called 'CCLEsmall', extracts RNA molecular profiles, creates p-value vectors, and then calls the `gwc` function with these inputs. This example serves to illustrate the correct usage of the function, including the format of input data and how to set up the necessary parameters. It helps users understand how to apply the function to their own data."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/dicomutils.py",
    "language": "py",
    "content": "import pydicom\nfrom typing import Dict, TypeVar, Union\nimport copy\n\nT = TypeVar('T')\n\n\ndef get_modality_metadata(dicom_data, modality: str):\n    keys = {'ALL': {'BodyPartExamined': 'BodyPartExamined', \n                    'DataCollectionDiameter': 'DataCollectionDiameter', \n                    'NumberofSlices': 'NumberofSlices', \n                    'SliceThickness': 'SliceThickness', \n                    'ScanType': 'ScanType', \n                    'ScanProgressionDirection': 'ScanProgressionDirection', \n                    'PatientPosition': 'PatientPosition', \n                    'ContrastType': 'ContrastBolusAgent',\n                    'Manufacturer': 'Manufacturer',\n                    'ScanOptions': 'ScanOptions',\n                    'RescaleType': 'RescaleType',\n                    'RescaleSlope': 'RescaleSlope',\n                    'ManufacturerModelName': 'ManufacturerModelName'},\n            'CT': {'KVP': 'KVP',\n                   'XRayTubeCurrent': 'XRayTubeCurrent',\n                   'ScanOptions': 'ScanOptions',\n                   'ReconstructionAlgorithm': 'ReconstructionAlgorithm',\n                   'ContrastFlowRate': 'ContrastFlowRate',\n                   'ContrastFlowDuration': 'ContrastFlowDuration',\n                   'ContrastType': 'ContrastBolusAgent',\n                   'ReconstructionMethod': 'ReconstructionMethod',\n                   'ReconstructionDiameter': 'ReconstructionDiameter',\n                   'ConvolutionKernel': 'ConvolutionKernel'},\n            'MR': ['AcquisitionTime', 'AcquisitionContrast', 'AcquisitionType', 'RepetitionTime', 'EchoTime', 'ImagingFrequency', 'MagneticFieldStrength', 'SequenceName'],\n            'PT': ['RescaleType', 'RescaleSlope', 'RadionuclideTotalDose', 'RadionuclideHalfLife']\n            }\n    \n    # initialize metadata dictionary\n    if modality == 'ALL':\n        metadata = {}\n    else:\n        metadata = all_modalities_metadata(dicom_data)\n    \n    # populating metadata\n    if modality == 'RTSTRUCT':\n        if hasattr(dicom_data, 'StructureSetROISequence'):\n            metadata[\"numROIs\"] = str(len(dicom_data.StructureSetROISequence))\n    elif modality in keys:\n        keys_mod = keys[modality]\n        if isinstance(keys_mod, dict):\n            for k in keys_mod:\n                if hasattr(dicom_data, keys_mod[k]):\n                    metadata[k] = getattr(dicom_data, keys_mod[k])\n        elif isinstance(keys_mod, list):\n            for k in keys_mod:\n                if hasattr(dicom_data, k):\n                    metadata[k] = getattr(dicom_data, k)\n        else:\n            print('WAGUANWAGUANWAGUAN')\n\n    return metadata\n\n\ndef all_modalities_metadata(dicom_data: Union[pydicom.dataset.FileDataset, pydicom.dicomdir.DicomDir]) -> Dict[str, T]:\n    metadata = get_modality_metadata(dicom_data, 'ALL')\n    \n    if hasattr(dicom_data, 'PixelSpacing') and hasattr(dicom_data, 'SliceThickness'):\n        pixel_size = copy.copy(dicom_data.PixelSpacing)\n        pixel_size.append(dicom_data.SliceThickness)\n        metadata[\"PixelSize\"] = str(tuple(pixel_size))\n    \n    return metadata\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `get_modality_metadata` function and how does it handle different modalities?",
        "answer": "The `get_modality_metadata` function extracts metadata from DICOM data based on the specified modality. It uses a dictionary of keys for different modalities (ALL, CT, MR, PT) to determine which attributes to extract. For 'ALL' modality, it initializes an empty dictionary, while for other modalities, it first calls `all_modalities_metadata`. The function then populates the metadata dictionary based on the modality-specific keys, handling both dictionary and list type key definitions. For 'RTSTRUCT' modality, it specifically checks for the number of ROIs."
      },
      {
        "question": "How does the function handle the case when the modality is not found in the `keys` dictionary?",
        "answer": "The function doesn't explicitly handle the case when the modality is not found in the `keys` dictionary. If an unknown modality is passed, the function will return the metadata dictionary populated by `all_modalities_metadata` without adding any modality-specific information. This could potentially lead to unexpected behavior or incomplete metadata for unsupported modalities. A potential improvement would be to add error handling or a default case for unknown modalities."
      },
      {
        "question": "What is the purpose of the `all_modalities_metadata` function and how does it differ from `get_modality_metadata`?",
        "answer": "The `all_modalities_metadata` function is used to extract common metadata applicable to all modalities. It first calls `get_modality_metadata` with 'ALL' as the modality to get general metadata. Then, it specifically checks for 'PixelSpacing' and 'SliceThickness' attributes to calculate and add a 'PixelSize' entry to the metadata. This function is called by `get_modality_metadata` for all non-'ALL' modalities to ensure that common metadata is always included before adding modality-specific information."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def get_modality_metadata(dicom_data, modality: str):\n    keys = {\n        'ALL': {'BodyPartExamined': 'BodyPartExamined', 'DataCollectionDiameter': 'DataCollectionDiameter'},\n        'CT': {'KVP': 'KVP', 'XRayTubeCurrent': 'XRayTubeCurrent'},\n        'MR': ['AcquisitionTime', 'AcquisitionContrast'],\n        'PT': ['RescaleType', 'RescaleSlope']\n    }\n    \n    metadata = all_modalities_metadata(dicom_data) if modality != 'ALL' else {}\n    \n    if modality == 'RTSTRUCT':\n        if hasattr(dicom_data, 'StructureSetROISequence'):\n            metadata['numROIs'] = str(len(dicom_data.StructureSetROISequence))\n    elif modality in keys:\n        keys_mod = keys[modality]\n        if isinstance(keys_mod, dict):\n            for k, v in keys_mod.items():\n                if hasattr(dicom_data, v):\n                    metadata[k] = getattr(dicom_data, v)\n        elif isinstance(keys_mod, list):\n            for k in keys_mod:\n                if hasattr(dicom_data, k):\n                    metadata[k] = getattr(dicom_data, k)\n    \n    return metadata",
        "complete": "def get_modality_metadata(dicom_data, modality: str):\n    keys = {\n        'ALL': {'BodyPartExamined': 'BodyPartExamined', 'DataCollectionDiameter': 'DataCollectionDiameter', 'NumberofSlices': 'NumberofSlices', 'SliceThickness': 'SliceThickness', 'ScanType': 'ScanType', 'ScanProgressionDirection': 'ScanProgressionDirection', 'PatientPosition': 'PatientPosition', 'ContrastType': 'ContrastBolusAgent', 'Manufacturer': 'Manufacturer', 'ScanOptions': 'ScanOptions', 'RescaleType': 'RescaleType', 'RescaleSlope': 'RescaleSlope', 'ManufacturerModelName': 'ManufacturerModelName'},\n        'CT': {'KVP': 'KVP', 'XRayTubeCurrent': 'XRayTubeCurrent', 'ScanOptions': 'ScanOptions', 'ReconstructionAlgorithm': 'ReconstructionAlgorithm', 'ContrastFlowRate': 'ContrastFlowRate', 'ContrastFlowDuration': 'ContrastFlowDuration', 'ContrastType': 'ContrastBolusAgent', 'ReconstructionMethod': 'ReconstructionMethod', 'ReconstructionDiameter': 'ReconstructionDiameter', 'ConvolutionKernel': 'ConvolutionKernel'},\n        'MR': ['AcquisitionTime', 'AcquisitionContrast', 'AcquisitionType', 'RepetitionTime', 'EchoTime', 'ImagingFrequency', 'MagneticFieldStrength', 'SequenceName'],\n        'PT': ['RescaleType', 'RescaleSlope', 'RadionuclideTotalDose', 'RadionuclideHalfLife']\n    }\n    \n    metadata = all_modalities_metadata(dicom_data) if modality != 'ALL' else {}\n    \n    if modality == 'RTSTRUCT':\n        if hasattr(dicom_data, 'StructureSetROISequence'):\n            metadata['numROIs'] = str(len(dicom_data.StructureSetROISequence))\n    elif modality in keys:\n        keys_mod = keys[modality]\n        if isinstance(keys_mod, dict):\n            for k, v in keys_mod.items():\n                if hasattr(dicom_data, v):\n                    metadata[k] = getattr(dicom_data, v)\n        elif isinstance(keys_mod, list):\n            for k in keys_mod:\n                if hasattr(dicom_data, k):\n                    metadata[k] = getattr(dicom_data, k)\n    \n    return metadata"
      },
      {
        "partial": "def all_modalities_metadata(dicom_data: Union[pydicom.dataset.FileDataset, pydicom.dicomdir.DicomDir]) -> Dict[str, T]:\n    metadata = get_modality_metadata(dicom_data, 'ALL')\n    \n    # Add code here to handle PixelSpacing and SliceThickness\n    \n    return metadata",
        "complete": "def all_modalities_metadata(dicom_data: Union[pydicom.dataset.FileDataset, pydicom.dicomdir.DicomDir]) -> Dict[str, T]:\n    metadata = get_modality_metadata(dicom_data, 'ALL')\n    \n    if hasattr(dicom_data, 'PixelSpacing') and hasattr(dicom_data, 'SliceThickness'):\n        pixel_size = copy.copy(dicom_data.PixelSpacing)\n        pixel_size.append(dicom_data.SliceThickness)\n        metadata['PixelSize'] = str(tuple(pixel_size))\n    \n    return metadata"
      }
    ],
    "dependencies": {
      "imports": [
        "pydicom",
        "copy"
      ],
      "from_imports": [
        "typing.Dict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeDrugSensitivity.R",
    "language": "R",
    "content": "#' @importFrom BiocParallel bplapply\n.calculateSensitivitiesStar <- function (pSets=list(), exps=NULL, cap=NA,\n        na.rm=TRUE, area.type=c(\"Fitted\", \"Actual\"), nthread=1) {\n    if (missing(area.type)) {\n        area.type <- \"Fitted\"\n    }\n    if (is.null(exps)) {\n        stop(\"expriments is empty!\")\n    }\n    for (study in names(pSets)) {\n        sensitivityProfiles(pSets[[study]])$auc_recomputed_star <- NA\n    }\n    if (!is.na(cap)) {\n        trunc <- TRUE\n    }else{\n        trunc <- FALSE\n    }\n\n    for (i in seq_len(nrow(exps))) {\n        ranges <- list()\n        for (study in names(pSets)) {\n            ranges[[study]] <- as.numeric(sensitivityRaw(pSets[[study]])[\n                exps[i, study], , \"Dose\"\n            ])\n        }\n        ranges <- .getCommonConcentrationRange(ranges)\n        names(ranges) <- names(pSets)\n        for (study in names(pSets)) {\n            myx <- as.numeric(sensitivityRaw(pSets[[study]])[\n                exps[i, study],,\"Dose\"]) %in% ranges[[study]\n            ]\n            sensitivityRaw(pSets[[study]])[exps[i, study], !myx, ] <- NA\n        }\n    }\n\n    op <- options()\n    options(mc.cores=nthread)\n    on.exit(options(op))\n\n    for (study in names(pSets)) {\n        auc_recomputed_star <- unlist(\n            bplapply(rownames(sensitivityRaw(pSets[[study]])),\n                FUN=function(experiment, exps, study, dataset, area.type) {\n                    if (!experiment %in% exps[,study]) return(NA_real_)\n                    return(computeAUC(\n                        concentration=as.numeric(dataset[experiment, , 1]),\n                        viability=as.numeric(dataset[experiment, , 2]),\n                        trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE,\n                        area.type=area.type\n                        ) / 100\n                        )\n                    },\n                exps=exps, study=study, dataset=sensitivityRaw(pSets[[study]]),\n                area.type=area.type)\n            )\n        sensitivityProfiles(pSets[[study]])$auc_recomputed_star <-\n            auc_recomputed_star\n    }\n    return(pSets)\n}\n\n## This function computes AUC for the whole raw sensitivity data of a pset\n.calculateFromRaw <- function(raw.sensitivity, cap=NA, nthread=1,\n        family=c(\"normal\", \"Cauchy\"), scale=0.07, n=1) {\n    family <- match.arg(family)\n\n    AUC <- vector(length=dim(raw.sensitivity)[1])\n    names(AUC) <- dimnames(raw.sensitivity)[[1]]\n\n    IC50 <- vector(length=dim(raw.sensitivity)[1])\n    names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n    trunc <- !is.na(cap)\n\n    if (nthread == 1) {\n        pars <- lapply(names(AUC),\n            FUN=function(exp, raw.sensitivity, family, scale, n) {\n                if (length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 ||\n                        all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n                    NA\n                } else{\n                    logLogisticRegression(raw.sensitivity[exp, , \"Dose\"],\n                        raw.sensitivity[exp, , \"Viability\"], trunc=trunc,\n                        conc_as_log=FALSE, viability_as_pct=TRUE, family=family,\n                        scale=scale, median_n=n)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, family=family, scale=scale,\n            n=n\n        )\n        names(pars) <- dimnames(raw.sensitivity)[[1]]\n        AUC <- unlist(lapply(names(pars),\n            FUN=function(exp, raw.sensitivity, pars) {\n                if (any(is.na(pars[[exp]]))) {\n                    NA\n                } else{\n                    computeAUC(concentration=raw.sensitivity[exp, , \"Dose\"],\n                        Hill_fit=pars[[exp]], trunc=trunc, conc_as_log=FALSE,\n                        viability_as_pct=TRUE)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, pars=pars\n        ))\n        IC50 <- unlist(lapply(names(pars), function(exp, pars) {\n            if (any(is.na(pars[[exp]]))) {\n                NA\n            } else{\n                computeIC50(Hill_fit=pars[[exp]], trunc=trunc,\n                    conc_as_log=FALSE, viability_as_pct=TRUE)\n            }\n        }, pars=pars))\n    } else {\n        pars <- parallel::mclapply(names(AUC),\n            FUN=function(exp, raw.sensitivity, family, scale, n, trunc) {\n                if (length(grep(\"///\", raw.sensitivity[exp, , \"Dose\"])) > 0 ||\n                        all(is.na(raw.sensitivity[exp, , \"Dose\"]))) {\n                    NA\n                } else {\n                    logLogisticRegression(\n                        raw.sensitivity[exp, , \"Dose\"],\n                        raw.sensitivity[exp, , \"Viability\"],\n                        trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE,\n                        family=family, scale=scale, median_n=n)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, family=family, scale=scale, n=n,\n            trunc=trunc, mc.cores=nthread\n        )\n        names(pars) <- dimnames(raw.sensitivity)[[1]]\n        AUC <- unlist(parallel::mclapply(names(pars),\n            FUN=function(exp, raw.sensitivity, pars, trunc) {\n                if (any(is.na(pars[[exp]]))) {\n                    NA\n                } else{\n                    computeAUC(\n                        concentration=raw.sensitivity[exp, , \"Dose\"],\n                        Hill_fit=pars[[exp]],\n                        trunc=trunc, conc_as_log=FALSE, viability_as_pct=TRUE)\n                }\n            },\n            raw.sensitivity=raw.sensitivity, pars=pars, trunc=trunc,\n            mc.cores=nthread\n        ))\n        IC50 <- unlist(parallel::mclapply(names(pars),\n            FUN=function(exp, pars, trunc) {\n                if(any(is.na(pars[[exp]]))) {\n                    NA\n                } else{\n                    computeIC50(Hill_fit=pars[[exp]], trunc=trunc,\n                        conc_as_log=FALSE, viability_as_pct=TRUE)\n                }\n            }, pars=pars, trunc=trunc, mc.cores=nthread\n        ))\n    }\n    names(AUC) <- dimnames(raw.sensitivity)[[1]]\n    names(IC50) <- dimnames(raw.sensitivity)[[1]]\n\n    return(list(\"AUC\"=AUC, \"IC50\"=IC50, \"pars\"=pars))\n}\n\n\n## This function computes intersected concentration range between a list of\n## concentration ranges\n.getCommonConcentrationRange <- function(doses) {\n    min.dose <- 0\n    max.dose <- 10^100\n    for (i in seq_len(length(doses))) {\n        min.dose <- max(min.dose, min(as.numeric(doses[[i]]), na.rm=TRUE),\n            na.rm=TRUE)\n        max.dose <- min(max.dose, max(as.numeric(doses[[i]]), na.rm=TRUE),\n            na.rm=TRUE)\n    }\n    common.ranges <- list()\n    for (i in seq_len(length(doses))) {\n        common.ranges[[i]] <- doses[[i]][\n            seq(which.min(abs(as.numeric(doses[[i]]) - min.dose)), max(\n                which(abs(as.numeric(doses[[i]]) - max.dose) ==\n                    min(abs(as.numeric(doses[[i]]) - max.dose), na.rm=TRUE)\n                ))\n            )\n        ]\n    }\n    return(common.ranges)\n}\n\n## predict viability from concentration data and curve parameters\n.Hill <- function(x, pars) {\n    return(pars[2] + (1 - pars[2]) / (1 + (10 ^ x / 10 ^ pars[3]) ^ pars[1]))\n}\n\n## calculate residual of fit\n## FIXME:: Why is this different from CoreGx?\n#' @importFrom CoreGx .dmedncauchys .dmednnormals .edmednnormals .edmedncauchys\n.residual <- function(x, y, n, pars, scale=0.07, family=c(\"normal\", \"Cauchy\"),\n        trunc=FALSE) {\n    family <- match.arg(family)\n    Cauchy_flag=(family == \"Cauchy\")\n    if (Cauchy_flag == FALSE) {\n        # return(sum((.Hill(x, pars) - y) ^ 2))\n        diffs <- .Hill(x, pars)-y\n        if (trunc == FALSE) {\n            return(sum(-log(.dmednnormals(diffs, n, scale))))\n        } else {\n            down_truncated <- abs(y) >= 1\n            up_truncated <- abs(y) <= 0\n\n            # For up truncated, integrate the cauchy dist up until -\n            #>because anything less gets truncated to 0, and thus the residual\n            #>is -diff, and the prob function becomes discrete For\n            #>down_truncated, 1-cdf(diffs)=cdf(-diffs)\n            return(\n                sum(-log(.dmednnormals(diffs[!(down_truncated | up_truncated)],\n                    n, scale))) +\n                sum(-log(.edmednnormals(-diffs[up_truncated | down_truncated],\n                    n, scale)))\n            )\n\n        }\n    } else {\n        diffs <- .Hill(x, pars) - y\n        if (trunc == FALSE) {\n            return(sum(-log(.dmedncauchys(diffs, n, scale))))\n        } else {\n            down_truncated <- abs(y) >= 1\n            up_truncated <- abs(y) <= 0\n            # For up truncated, integrate the cauchy dist up until -diff because\n            #> anything less gets truncated to 0, and thus the residual is -diff,\n            #>and the prob function becomes discrete For down_truncated,\n            #>1 - cdf(diffs) = cdf(-diffs)\n            return(\n                sum(-log(.dmedncauchys(diffs[!(down_truncated | up_truncated)],\n                    n, scale))) +\n                sum(-log(.edmedncauchys(-diffs[up_truncated | down_truncated],\n                    n, scale))))\n        }\n    }\n}\n\n##FIXME:: Why is this different from CoreGx?\n.meshEval <- function(log_conc, viability, lower_bounds=c(0, 0, -6),\n        upper_bounds=c(4, 1, 6), density=c(2, 10, 2), scale=0.07, n=1,\n        family=c(\"normal\", \"Cauchy\"), trunc=FALSE) {\n    family <- match.arg(family)\n    guess <- c(pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n        pmin(pmax(min(viability), lower_bounds[2]), upper_bounds[2]),\n        pmin(pmax(log_conc[which.min(abs(viability - 1 / 2))], lower_bounds[3]),\n        upper_bounds[3]))\n    guess_residual <- .residual(log_conc, viability, pars=guess, n=n,\n        scale=scale, family=family, trunc=trunc)\n    for (i in seq(from=lower_bounds[1], to=upper_bounds[1],\n            by=1 / density[1])) {\n        for (j in seq(from=lower_bounds[2], to=upper_bounds[2],\n                by=1 / density[2])) {\n            for (k in seq(from=lower_bounds[3], to=upper_bounds[3],\n                    by=1 / density[3])) {\n                test_guess_residual <- .residual(log_conc, viability,\n                    pars=c(i, j, k), n=n, scale=scale, family=family,\n                    trunc=trunc)\n                if (!is.finite(test_guess_residual)) {\n                    warning(paste0(\" Test Guess Residual is: \",\n                        test_guess_residual, \"\\n Other Pars: log_conc: \",\n                        paste(log_conc, collapse=\", \"), \"\\n Viability: \",\n                        paste(viability, collapse=\", \"), \"\\n Scale: \", scale,\n                        \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \",\n                        i, \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n                }\n                if (!length(test_guess_residual)) {\n                    warning(paste0(\" Test Guess Residual is: \",\n                        test_guess_residual,  \"\\n Other Pars: log_conc: \",\n                        paste(log_conc, collapse=\", \"), \"\\n Viability: \",\n                        paste(viability, collapse=\", \"), \"\\n Scale: \", scale,\n                        \"\\n Family: \", family, \"\\n Trunc \", trunc, \"\\n HS: \", i,\n                        \", Einf: \", j, \", logEC50: \", k, \"\\n n: \", n))\n                }\n                if (test_guess_residual < guess_residual) {\n                    guess <- c(i, j, k)\n                    guess_residual <- test_guess_residual\n                }\n            }\n        }\n    }\n    return(guess)\n}\n\n## FIXME:: Documentation?\n#  Fits dose-response curves to data given by the user\n#  and returns the AUC of the fitted curve, normalized to the length of the concentration range.\n#\n#  @param concentration `numeric` is a vector of drug concentrations.\n#\n#  @param viability `numeric` is a vector whose entries are the viability values observed in the presence of the\n#  drug concentrations whose logarithms are in the corresponding entries of the log_conc, expressed as percentages\n#  of viability in the absence of any drug.\n#\n#  @param trunc `logical`, if true, causes viability data to be truncated to lie between 0 and 1 before\n#  curve-fitting is performed.\n#' @importFrom CoreGx .getSupportVec\n#' @export\n#' @keywords internal\n.computeAUCUnderFittedCurve <- function(concentration, viability, trunc=TRUE,\n        verbose=FALSE) {\n    log_conc <- concentration\n    #FIT CURVE AND CALCULATE IC50\n    pars <- unlist(logLogisticRegression(log_conc, viability,\n        conc_as_log=TRUE, viability_as_pct=FALSE, trunc=trunc))\n    x <- .getSupportVec(log_conc)\n    return(1 - trapz(x, .Hill(x, pars)) /\n        (log_conc[length(log_conc)] - log_conc[1]))\n}\n\n#This function is being used in computeSlope\n.optimizeRegression <- function(x, y, x0=-3, y0=100) {\n    beta1 <- (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n    return(beta1)\n}\n\nupdateMaxConc <- function(pSet) {\n    sensitivityInfo(pSeto)$max.conc <- apply(sensitivityRaw(pSet)[, , \"Dose\"],\n        1, max, na.rm=TRUE)\n    return(pSet)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the '.calculateSensitivitiesStar' function and what are its main input parameters?",
        "answer": "The '.calculateSensitivitiesStar' function is designed to calculate sensitivities for multiple pharmacological datasets (pSets). Its main input parameters are: 'pSets' (a list of pharmacological datasets), 'exps' (experiments to analyze), 'cap' (for truncation), 'area.type' (either 'Fitted' or 'Actual'), and 'nthread' (for parallel processing). The function recomputes AUC (Area Under the Curve) values for specified experiments across multiple studies, considering common concentration ranges."
      },
      {
        "question": "How does the '.getCommonConcentrationRange' function work, and why is it important in the context of the '.calculateSensitivitiesStar' function?",
        "answer": "The '.getCommonConcentrationRange' function computes the intersected concentration range between a list of concentration ranges. It finds the maximum of the minimum doses and the minimum of the maximum doses across all input ranges, then selects the doses within this common range for each input. This function is important in '.calculateSensitivitiesStar' because it ensures that sensitivity calculations are performed on a consistent concentration range across different studies, allowing for fair comparisons and standardized analysis of drug responses."
      },
      {
        "question": "What is the purpose of the '.residual' function, and how does it handle different statistical families and truncation?",
        "answer": "The '.residual' function calculates the residual of the fit between observed and predicted values in a dose-response curve. It supports two statistical families: normal and Cauchy. For each family, it computes the negative log-likelihood of the differences between predicted and observed values. When truncation is enabled, it handles up-truncated (y <= 0) and down-truncated (y >= 1) cases separately, using the cumulative distribution function for these cases. This function is crucial for assessing the goodness of fit in the dose-response curve modeling process, adapting to different statistical assumptions and data constraints."
      }
    ],
    "completion_tasks": [
      {
        "partial": "updateMaxConc <- function(pSet) {\n    sensitivityInfo(pSeto)$max.conc <- apply(sensitivityRaw(pSet)[, , \"Dose\"],\n        1, max, na.rm=TRUE)\n    return(pSet)\n}",
        "complete": "updateMaxConc <- function(pSet) {\n    sensitivityInfo(pSet)$max.conc <- apply(sensitivityRaw(pSet)[, , \"Dose\"],\n        1, max, na.rm=TRUE)\n    return(pSet)\n}"
      },
      {
        "partial": ".optimizeRegression <- function(x, y, x0=-3, y0=100) {\n    beta1 <- (sum(x * y) - y0 * sum(x)) / (sum(x * x) - x0 * sum(x))\n    return(beta1)\n}",
        "complete": ".optimizeRegression <- function(x, y, x0=-3, y0=100) {\n    beta1 <- (sum(x * y) - y0 * sum(x)) / (sum(x^2) - x0 * sum(x))\n    return(beta1)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test-utils.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.asDT` function in the given code snippet?",
        "answer": "The `.asDT` function is used to convert a list into a data.table object. In this test, it takes a list `l` with two elements (Name and Age) and converts it into a data.table, which is then verified using the `expect_data_table` function from the testthat package."
      },
      {
        "question": "Which R packages are being used in this code snippet and what are their purposes?",
        "answer": "The code snippet uses three R packages: 1) AnnotationGx: likely contains the `.asDT` function being tested, 2) testthat: provides functions for unit testing in R, such as `test_that` and `expect_data_table`, 3) checkmate: although imported, it's not explicitly used in this snippet but typically provides functions for argument checking in R functions."
      },
      {
        "question": "How is the test case structured in this code, and what is it verifying?",
        "answer": "The test case is structured using the `test_that` function from the testthat package. It creates a list `l`, converts it to a data.table using `.asDT`, and then uses `expect_data_table` to verify that the result is indeed a data.table object. This test is checking if the `.asDT` function correctly converts a list to a data.table."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  # Add expectation here\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})"
      },
      {
        "partial": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  # Create a list here\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})",
        "complete": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\".asDT works\", {\n  l <- list(Name = c(\"John\", \"Doe\"), Age = c(1, 2))\n  dt <- .asDT(l)\n  expect_data_table(dt)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/ops/__init__.py",
    "language": "py",
    "content": "from .ops import *\n",
    "qa_pairs": [
      {
        "question": "What does the line 'from .ops import *' do in Python, and what potential issues might it introduce?",
        "answer": "The line 'from .ops import *' imports all names from the 'ops' module in the current package. This is known as a wildcard import. While convenient, it can introduce several issues: 1) It can pollute the namespace with unexpected names, 2) It makes it unclear which names are being used from the imported module, 3) It can lead to name conflicts if multiple modules are imported this way, and 4) It goes against PEP 8 style guidelines. It's generally recommended to use explicit imports instead."
      },
      {
        "question": "How would you modify the import statement to follow Python best practices?",
        "answer": "To follow Python best practices, you should avoid wildcard imports and instead explicitly import the names you need. For example: 'from .ops import function1, function2, Class1'. If you need to import many names, you can use: 'import .ops as ops' and then use 'ops.function1()' syntax. This makes the code more readable and reduces the risk of namespace pollution."
      },
      {
        "question": "What does the dot (.) before 'ops' in the import statement signify?",
        "answer": "The dot (.) before 'ops' in the import statement indicates a relative import. It means that the 'ops' module is located in the same package as the current module. Relative imports are used to organize related modules within a package. In this case, it's importing from a sibling module named 'ops' within the same package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "from .ops import *\n\ndef calculate_result(a, b):\n    # Perform addition\n    result = add(a, b)\n    \n    # Perform multiplication\n    result = multiply(result, 2)\n    \n    # Perform subtraction\n    result = ",
        "complete": "from .ops import *\n\ndef calculate_result(a, b):\n    # Perform addition\n    result = add(a, b)\n    \n    # Perform multiplication\n    result = multiply(result, 2)\n    \n    # Perform subtraction\n    result = subtract(result, 5)\n    \n    return result"
      },
      {
        "partial": "from .ops import *\n\nclass Calculator:\n    def __init__(self, initial_value):\n        self.value = initial_value\n    \n    def add(self, x):\n        self.value = add(self.value, x)\n        return self\n    \n    def multiply(self, x):\n        ",
        "complete": "from .ops import *\n\nclass Calculator:\n    def __init__(self, initial_value):\n        self.value = initial_value\n    \n    def add(self, x):\n        self.value = add(self.value, x)\n        return self\n    \n    def multiply(self, x):\n        self.value = multiply(self.value, x)\n        return self\n    \n    def get_result(self):\n        return self.value"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": [
        "ops.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/PharmacoSet-utils.R",
    "language": "R",
    "content": "#' @include PharmacoSet-class.R PharmacoSet-accessors.R\nNULL\n\n.local_class <- 'PharmacoSet'\n.local_data <- 'CCLEsmall'\n.local_treatment <- 'drug'\n\n\n#### PharmacoGx dynamic documentation\n####\n#### Warning: for dynamic docs to work, you must set\n#### Roxygen: list(markdown=TRUE, r6=FALSE)\n#### in the DESCRPTION file!\n\n\n# ===================================\n# Utility Method Documentation Object\n# -----------------------------------\n\n\n#' @name PharmacoSet-utils\n#' @eval CoreGx:::.docs_CoreSet_utils(class_=.local_class)\n#' @eval .parseToRoxygen(\"@examples data({data_})\", data_=.local_data)\nNULL\n\n\n# ======================================\n# Subset Methods\n# --------------------------------------\n\n\n\n## ===================\n## ---- subsetBySample\n## -------------------\n\n\n#' @rdname PharmacoSet-utils\n#' @importMethodsFrom CoreGx subsetBySample\n#' @eval CoreGx:::.docs_CoreSet_subsetBySample(class_=.local_class,\n#' data_=.local_data)\nsetMethod('subsetBySample', signature(x='PharmacoSet'), function(x, samples) {\n    callNextMethod(x=x, samples=samples)\n})\n\n\n## ======================\n## ---- subsetByTreatment\n## ----------------------\n\n\n#' @rdname PharmacoSet-utils\n#' @importMethodsFrom CoreGx subsetByTreatment\n#' @eval CoreGx:::.docs_CoreSet_subsetByTreatment(class_=.local_class, \n#' data_=.local_data, treatment_=.local_treatment)\nsetMethod('subsetByTreatment', signature(x='PharmacoSet'),\n        function(x, treatments) {\n    callNextMethod(x=x, treatments=treatments)\n})\n\n\n## ====================\n## ---- subsetByFeature\n## --------------------\n\n\n#' @rdname PharmacoSet-utils\n#' @importFrom CoreGx subsetByFeature\n#' @eval CoreGx:::.docs_CoreSet_subsetByFeature(class_=.local_class, \n#' data_=.local_data)\nsetMethod('subsetByFeature', signature(x='PharmacoSet'), \n        function(x, features, mDataTypes) {\n    callNextMethod(x=x, features=features, mDataTypes)\n})\n\n## ===========\n## ---- subset\n## -----------\n\n#'\n#' \n#' \nsetMethod('subset', signature('PharmacoSet'),\n        function(x, samples, treatments, features, ..., mDataTypes) {\n    callNextMethod(x=x, samples=samples, treatments=treatments, \n        features=features, ..., mDataTypes=mDataTypes)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.local_class`, `.local_data`, and `.local_treatment` variables in this code snippet?",
        "answer": "These variables are used for dynamic documentation generation. `.local_class` is set to 'PharmacoSet', `.local_data` to 'CCLEsmall', and `.local_treatment` to 'drug'. They are likely used in the `@eval` tags to customize the documentation for specific classes, datasets, and treatment types."
      },
      {
        "question": "How does the `subsetBySample` method for the `PharmacoSet` class differ from its parent class implementation?",
        "answer": "The `subsetBySample` method for the `PharmacoSet` class doesn't introduce any new functionality. It uses `callNextMethod(x=x, samples=samples)` to call the parent class implementation, passing along the same arguments. This suggests that the `PharmacoSet` class inherits this method's behavior from its parent class, likely `CoreSet`."
      },
      {
        "question": "What is the purpose of the `@eval` tags used in the Roxygen documentation blocks?",
        "answer": "The `@eval` tags in the Roxygen documentation blocks are used for dynamic documentation generation. They evaluate R expressions to generate documentation content at compile-time. For example, `@eval CoreGx:::.docs_CoreSet_subsetBySample(class_=.local_class, data_=.local_data)` likely generates documentation specific to the `PharmacoSet` class and the 'CCLEsmall' dataset for the `subsetBySample` method."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('subsetBySample', signature(x='PharmacoSet'), function(x, samples) {\n    # Complete the function body\n})",
        "complete": "setMethod('subsetBySample', signature(x='PharmacoSet'), function(x, samples) {\n    callNextMethod(x=x, samples=samples)\n})"
      },
      {
        "partial": "setMethod('subset', signature('PharmacoSet'),\n        function(x, samples, treatments, features, ..., mDataTypes) {\n    # Complete the function body\n})",
        "complete": "setMethod('subset', signature('PharmacoSet'),\n        function(x, samples, treatments, features, ..., mDataTypes) {\n    callNextMethod(x=x, samples=samples, treatments=treatments, \n        features=features, ..., mDataTypes=mDataTypes)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_status.R",
    "language": "R",
    "content": "#' Retrieves the status of a PubChem request\n#'\n#' This function sends a request to PubChem to retrieve the status of a given URL.\n#' It returns the status code and, if specified, the parsed information from the response.\n#'\n#' @param returnMessage Logical indicating whether to return the parsed information from the response.\n#' @param printMessage Logical indicating whether to print the status message.\n#' @param url The URL to send the request to. Default is \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\".\n#'\n#' @return The status code of the response. If \\code{returnMessage} is \\code{TRUE}, the parsed information from the response is also returned.\n#'\n#' @examples\n#' getPubchemStatus()\n#' getPubchemStatus(returnMessage = TRUE)\n#' getPubchemStatus(printMessage = FALSE)\n#'\n#' @export\ngetPubchemStatus <- function(\n    returnMessage = FALSE, printMessage = TRUE,\n    url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\") {\n  funContext <- .funContext(\"getPubchemStatus\")\n\n  request <- .buildURL(url) |> .build_pubchem_request()\n\n  # need to do NULL while loop bc sometimes X-Throttling-Control is not in the response\n  message <- NULL\n\n  while(is.null(message)) {\n    response <- httr2::req_perform(request)\n\n    if (httr2::resp_status(response) == 200) {\n      message <- response$headers[[\"X-Throttling-Control\"]]\n    } else {\n      .warn(\"Request failed. Retrying...\")\n      Sys.sleep(1)\n    }\n  }\n  parsed_info <- .checkThrottlingStatus2(message, printMessage)\n  if (returnMessage) {\n    return(parsed_info)\n  }\n}\n\n\n\n#' names are: request_count, request_time and service\n#' each has status and percent\n#' main throttlers for user are request_count and request_time\n#' main statuses are:\n#'  Green - less than 50% of the permitted request limit has been used\n#'  Yellow - between 50% and 75% of the request limit has been used\n#'  Red - more than 75% of the request limit has been reached\n#'  Black - the limit has been exceeded and requests are being blocked\n#'\n#' @noRd\n#' @keywords internal\n.checkThrottlingStatus2 <- function(message, printMessage) {\n  parsed_info <- .parse_throttling_message(message)\n  if (printMessage) {\n    message(\"Throttling status:\\n\", paste0(strsplit(message, \", \")[[1]], collapse = \"\\n\"))\n  }\n  # Check if the request count or request time is\n  if (parsed_info$service$status == \"Black\") {\n    .warn(\"The request limit has been exceeded and requests are being blocked.\")\n  } else if (parsed_info$service$status %in% c(\"Red\", \"Yellow\")) {\n    .warn(\"The request limit has been reached or is close to being reached.\")\n  } else {\n    .debug(\"The request limit is not close to being reached.\")\n  }\n  return(parsed_info)\n}\n\n#' Parses the throttling message from the response\n#' @noRd\n#' @keywords internal\n.parse_throttling_message <- function(message) {\n  # Split the message into components\n  components <- strsplit(message, \", \")[[1]]\n\n  # Initialize an empty list to store the parsed information\n  parsed_info <- list()\n\n  # Loop through each component and extract the relevant information\n  for (comp in components) {\n    # Split each component into key-value pairs\n    kv <- strsplit(comp, \": \")[[1]]\n    key <- tolower(gsub(\" status\", \"\", kv[1]))\n    key <- gsub(\" \", \"_\", key)\n    value <- kv[2]\n\n    # Extract status and percent\n    status <- sub(\"\\\\s*\\\\(.*\\\\)\", \"\", value)\n    percent <- as.integer(sub(\".*\\\\((\\\\d+)%\\\\).*\", \"\\\\1\", value))\n\n    # Store the extracted information in the parsed_info list\n    parsed_info[[key]] <- list(status = status, percent = percent)\n  }\n\n  return(parsed_info)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemStatus` function and what are its main parameters?",
        "answer": "The `getPubchemStatus` function retrieves the status of a PubChem request. It sends a request to a specified URL (defaulting to a PubChem API endpoint for Aspirin) and returns the status code. Its main parameters are: `returnMessage` (boolean to determine if parsed information should be returned), `printMessage` (boolean to control whether status messages are printed), and `url` (the URL to send the request to)."
      },
      {
        "question": "How does the function handle potential request failures and throttling?",
        "answer": "The function uses a while loop to retry the request if it fails (status code != 200). It checks the 'X-Throttling-Control' header in the response to determine the throttling status. The `.checkThrottlingStatus2` function is then used to parse this information and provide warnings based on the throttling status (Green, Yellow, Red, or Black). If the status is Red or Yellow, it warns that the request limit is close to being reached, and if it's Black, it warns that requests are being blocked."
      },
      {
        "question": "Explain the purpose and functionality of the `.parse_throttling_message` function.",
        "answer": "The `.parse_throttling_message` function is an internal helper function that parses the throttling message received from the PubChem API. It splits the message into components, extracts key information such as request count, request time, and service status. For each component, it extracts the status (e.g., Green, Yellow, Red, Black) and the percentage of the limit used. The function returns a structured list containing this parsed information, which is then used by the `.checkThrottlingStatus2` function to determine appropriate warnings or messages to display."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getPubchemStatus <- function(returnMessage = FALSE, printMessage = TRUE, url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\") {\n  funContext <- .funContext(\"getPubchemStatus\")\n  request <- .buildURL(url) |> .build_pubchem_request()\n  message <- NULL\n\n  while(is.null(message)) {\n    response <- httr2::req_perform(request)\n    if (httr2::resp_status(response) == 200) {\n      message <- response$headers[[\"X-Throttling-Control\"]]\n    } else {\n      .warn(\"Request failed. Retrying...\")\n      Sys.sleep(1)\n    }\n  }\n  parsed_info <- .checkThrottlingStatus2(message, printMessage)\n  # Complete the function here\n}",
        "complete": "getPubchemStatus <- function(returnMessage = FALSE, printMessage = TRUE, url = \"https://pubchem.ncbi.nlm.nih.gov/rest/pug/compound/name/Aspirin/cids/JSON\") {\n  funContext <- .funContext(\"getPubchemStatus\")\n  request <- .buildURL(url) |> .build_pubchem_request()\n  message <- NULL\n\n  while(is.null(message)) {\n    response <- httr2::req_perform(request)\n    if (httr2::resp_status(response) == 200) {\n      message <- response$headers[[\"X-Throttling-Control\"]]\n    } else {\n      .warn(\"Request failed. Retrying...\")\n      Sys.sleep(1)\n    }\n  }\n  parsed_info <- .checkThrottlingStatus2(message, printMessage)\n  if (returnMessage) {\n    return(parsed_info)\n  }\n}"
      },
      {
        "partial": ".parse_throttling_message <- function(message) {\n  components <- strsplit(message, \", \")[[1]]\n  parsed_info <- list()\n\n  for (comp in components) {\n    kv <- strsplit(comp, \": \")[[1]]\n    key <- tolower(gsub(\" status\", \"\", kv[1]))\n    key <- gsub(\" \", \"_\", key)\n    value <- kv[2]\n\n    # Extract status and percent\n    status <- sub(\"\\\\s*\\\\(.*\\\\)\", \"\", value)\n    percent <- as.integer(sub(\".*\\\\((\\\\d+)%\\\\).*\", \"\\\\1\", value))\n\n    # Complete the function here\n  }\n\n  return(parsed_info)\n}",
        "complete": ".parse_throttling_message <- function(message) {\n  components <- strsplit(message, \", \")[[1]]\n  parsed_info <- list()\n\n  for (comp in components) {\n    kv <- strsplit(comp, \": \")[[1]]\n    key <- tolower(gsub(\" status\", \"\", kv[1]))\n    key <- gsub(\" \", \"_\", key)\n    value <- kv[2]\n\n    status <- sub(\"\\\\s*\\\\(.*\\\\)\", \"\", value)\n    percent <- as.integer(sub(\".*\\\\((\\\\d+)%\\\\).*\", \"\\\\1\", value))\n\n    parsed_info[[key]] <- list(status = status, percent = percent)\n  }\n\n  return(parsed_info)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/chembl_helpers.R",
    "language": "R",
    "content": "#' Internal function that returns all possible chembl resources\n#' @keywords internal\n#' @noRd\n.chembl_resources <- function() {\n  c(\n    \"activity\", \"assay\", \"atc_class\", \"binding_site\", \"biotherapeutic\", \"cell_line\",\n    \"chembl_id_lookup\", \"compound_record\", \"compound_structural_alert\", \"document\",\n    \"document_similarity\", \"document_term\", \"drug\", \"drug_indication\", \"drug_warning\",\n    \"go_slim\", \"image\", \"mechanism\", \"metabolism\", \"molecule\", \"molecule_form\",\n    \"organism\", \"protein_classification\", \"similarity\", \"source\", \"status\", \"substructure\",\n    \"target\", \"target_component\", \"target_relation\", \"tissue\", \"xref_source\"\n  )\n}\n\n#' Internal function that returns all possible chembl filter types\n#' @keywords internal\n#' @noRd\n.chembl_filter_types <- function() {\n  c(\n    \"exact\", \"iexact\", \"contains\", \"icontains\", \"startswith\", \"istartswith\",\n    \"endswith\", \"iendswith\", \"regex\", \"iregex\", \"gt\", \"gte\", \"lt\", \"lte\",\n    \"range\", \"in\", \"isnull\", \"search\", \"only\"\n  )\n}\n\n#' Internal function that returns all possible chembl mechanism columns\n#' @keywords internal\n#' @noRd\n.chembl_mechanism_cols <- function() {\n  c(\n    \"action_type\", \"binding_site_comment\", \"direct_interaction\", \"disease_efficacy\",\n    \"max_phase\", \"mec_id\", \"mechanism_comment\", \"mechanism_of_action\",\n    \"mechanism_refs\", \"molecular_mechanism\", \"molecule_chembl_id\",\n    \"parent_molecule_chembl_id\", \"record_id\", \"selectivity_comment\",\n    \"site_id\", \"target_chembl_id\", \"variant_sequence\"\n  )\n}\n\n#' Internal function that returns the complete schema for a Chembl resource\n#' @keywords internal\n#' @noRd\n.chembl_resource_schema <- function(resource) {\n  .build_chembl_request(paste0(resource, \"/schema\")) |>\n    .perform_request() |>\n    .parse_resp_json()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `.chembl_resources()` function and how is it implemented?",
        "answer": "The `.chembl_resources()` function is an internal function that returns all possible ChEMBL resources. It is implemented as a simple function that returns a character vector containing the names of various ChEMBL resources, such as 'activity', 'assay', 'molecule', 'target', etc. This function is likely used to provide a standardized list of available resources for other parts of the package or API."
      },
      {
        "question": "How does the `.chembl_filter_types()` function differ from `.chembl_resources()`, and what might it be used for?",
        "answer": "The `.chembl_filter_types()` function is similar to `.chembl_resources()` in that it returns a character vector, but it contains filter types instead of resources. It includes filter operations like 'exact', 'contains', 'startswith', 'gt' (greater than), 'lt' (less than), etc. This function is likely used in query construction or data filtering operations within the ChEMBL API, allowing users to specify how they want to filter or search for data."
      },
      {
        "question": "What is the purpose of the `.chembl_resource_schema()` function, and how does it differ from the other functions in this code snippet?",
        "answer": "The `.chembl_resource_schema()` function is more complex than the other functions in this snippet. It takes a 'resource' parameter and returns the complete schema for that ChEMBL resource. Unlike the other functions which simply return static vectors, this function makes an API request to fetch the schema. It uses helper functions like `.build_chembl_request()`, `.perform_request()`, and `.parse_resp_json()` to construct the request, send it, and parse the response. This function is likely used to dynamically retrieve and provide structure information about specific ChEMBL resources."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Internal function that returns all possible chembl resources\n#' @keywords internal\n#' @noRd\n.chembl_resources <- function() {\n  c(\n    \"activity\", \"assay\", \"atc_class\", \"binding_site\", \"biotherapeutic\", \"cell_line\",\n    # Add more resources here\n  )\n}",
        "complete": "#' Internal function that returns all possible chembl resources\n#' @keywords internal\n#' @noRd\n.chembl_resources <- function() {\n  c(\n    \"activity\", \"assay\", \"atc_class\", \"binding_site\", \"biotherapeutic\", \"cell_line\",\n    \"chembl_id_lookup\", \"compound_record\", \"compound_structural_alert\", \"document\",\n    \"document_similarity\", \"document_term\", \"drug\", \"drug_indication\", \"drug_warning\",\n    \"go_slim\", \"image\", \"mechanism\", \"metabolism\", \"molecule\", \"molecule_form\",\n    \"organism\", \"protein_classification\", \"similarity\", \"source\", \"status\", \"substructure\",\n    \"target\", \"target_component\", \"target_relation\", \"tissue\", \"xref_source\"\n  )\n}"
      },
      {
        "partial": "#' Internal function that returns the complete schema for a Chembl resource\n#' @keywords internal\n#' @noRd\n.chembl_resource_schema <- function(resource) {\n  # Complete the function body\n}",
        "complete": "#' Internal function that returns the complete schema for a Chembl resource\n#' @keywords internal\n#' @noRd\n.chembl_resource_schema <- function(resource) {\n  .build_chembl_request(paste0(resource, \"/schema\")) |>\n    .perform_request() |>\n    .parse_resp_json()\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  }
]