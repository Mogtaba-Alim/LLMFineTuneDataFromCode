{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset, Dataset\n",
    "from peft import LoraConfig, AutoPeftModelForCausalLM\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    BitsAndBytesConfig,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    GenerationConfig\n",
    ")\n",
    "from trl import SFTTrainer\n",
    "import os\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "import logging\n",
    "from time import perf_counter\n",
    "import pandas as pd\n",
    "import dotenv\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dotenv.load_dotenv()\n",
    "\n",
    "# Setup the API key\n",
    "HF_KEY = os.getenv(\"HF_TOKEN\")\n",
    "\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "model_id=\"meta-llama/Meta-Llama-3.1-8B\"\n",
    "output_model=\"llama3.18B-BHK-Lab-Data-Fine-tunedByMogtaba\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model_and_tokenizer(\n",
    "    model_id: str,\n",
    "    max_seq_length: int = 2048,  # Match with trainer config\n",
    "    load_in_8bit: bool = False,  # Option for 8-bit quantization\n",
    "    trust_remote_code: bool = True  # Important for some models\n",
    ") -> Tuple[AutoModelForCausalLM, AutoTokenizer]:\n",
    "    \"\"\"\n",
    "    Load and configure model and tokenizer with enhanced error handling and options.\n",
    "\n",
    "    Args:\n",
    "        model_id: HuggingFace model identifier\n",
    "        max_seq_length: Maximum sequence length for model/tokenizer\n",
    "        load_in_8bit: Whether to use 8-bit quantization instead of 4-bit\n",
    "        trust_remote_code: Whether to trust remote code in model files\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Initialize tokenizer with safety settings\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_id,\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            model_max_length=max_seq_length,\n",
    "            padding_side=\"right\",  # Consistent padding\n",
    "            truncation_side=\"right\",  # Consistent truncation,\n",
    "            token=HF_KEY,\n",
    "        )\n",
    "\n",
    "        # Handle special tokens more robustly\n",
    "        if tokenizer.pad_token is None:\n",
    "            tokenizer.pad_token = tokenizer.eos_token\n",
    "        if tokenizer.mask_token is None:\n",
    "            tokenizer.mask_token = tokenizer.eos_token\n",
    "\n",
    "        # Configure quantization\n",
    "        if load_in_8bit:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_8bit=True,\n",
    "                llm_int8_threshold=6.0,\n",
    "                llm_int8_has_fp16_weight=True,\n",
    "            )\n",
    "        else:\n",
    "            bnb_config = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_compute_dtype=torch.float16,\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                llm_int8_threshold=6.0,\n",
    "            )\n",
    "\n",
    "        # Load model with enhanced settings\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_id,\n",
    "            quantization_config=bnb_config,\n",
    "#            device_map=\"auto\",\n",
    "            trust_remote_code=trust_remote_code,\n",
    "            torch_dtype=torch.float16,\n",
    "            use_flash_attention_2=False,  # Explicitly disable Flash Attention\n",
    "            use_cache=True,  # Enable KV cache for inference\n",
    "            token=HF_KEY,\n",
    "        )\n",
    "\n",
    "        # Configure model settings\n",
    "        model.config.pretraining_tp = 1\n",
    "        model.config.torch_dtype = torch.float16\n",
    "\n",
    "        # Add model configuration for better training\n",
    "        model.config.pad_token_id = tokenizer.pad_token_id\n",
    "        model.config.max_length = max_seq_length\n",
    "\n",
    "        # Enable gradient checkpointing for memory efficiency\n",
    "        if hasattr(model, \"enable_input_require_grads\"):\n",
    "            model.enable_input_require_grads()\n",
    "\n",
    "        # Log model and tokenizer configuration\n",
    "        logger.info(f\"Model loaded: {model_id}\")\n",
    "        logger.info(f\"Model parameters: {model.num_parameters():,}\")\n",
    "        logger.info(f\"Tokenizer length: {len(tokenizer)}\")\n",
    "        logger.info(f\"Max sequence length: {max_seq_length}\")\n",
    "\n",
    "        return model, tokenizer\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading model/tokenizer: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = get_model_and_tokenizer(\n",
    "        model_id=model_id,\n",
    "        max_seq_length=2048,  # Match with your trainer config\n",
    "        load_in_8bit=False,   # Use 4-bit by default\n",
    "        trust_remote_code=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelInference:\n",
    "    def __init__(self, model, tokenizer, device: str = \"cuda\"):\n",
    "        \"\"\"Initialize the inference class with model and tokenizer.\"\"\"\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "        # Store the original model state\n",
    "        self.model_training_mode = self.model.training\n",
    "\n",
    "        # Default generation config\n",
    "        self.default_gen_config = GenerationConfig(\n",
    "            penalty_alpha=0.6,\n",
    "            do_sample=True,\n",
    "            top_k=5,\n",
    "            temperature=0.5,\n",
    "            repetition_penalty=1.2,\n",
    "            max_new_tokens=200,\n",
    "            pad_token_id=self.tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    def formatted_prompt(self, question: str) -> str:\n",
    "        \"\"\"Format the input prompt with appropriate tokens.\"\"\"\n",
    "        return f\"<|im_start|>user\\n{question}<|im_end|>\\n<|im_start|>assistant:\"\n",
    "\n",
    "    def generate_response(\n",
    "        self,\n",
    "        user_input: str,\n",
    "        gen_config: Optional[GenerationConfig] = None,\n",
    "        **kwargs: Dict[str, Any]\n",
    "    ) -> Dict[str, Any]:\n",
    "        try:\n",
    "            prompt = self.formatted_prompt(user_input)\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.device)\n",
    "\n",
    "            # Use a more stable default configuration\n",
    "            default_gen_config = GenerationConfig(\n",
    "                do_sample=True,\n",
    "                temperature=0.7,\n",
    "                top_p=0.9,\n",
    "                top_k=50,\n",
    "                repetition_penalty=1.1,\n",
    "                max_new_tokens=200,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                use_cache=True\n",
    "            )\n",
    "\n",
    "            # Use provided config or default\n",
    "            gen_config = gen_config or default_gen_config\n",
    "\n",
    "            # Update config with any provided kwargs\n",
    "            for key, value in kwargs.items():\n",
    "                setattr(gen_config, key, value)\n",
    "\n",
    "            self.model.eval()\n",
    "\n",
    "            start_time = perf_counter()\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    generation_config=gen_config\n",
    "                )\n",
    "            response_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            inference_time = perf_counter() - start_time\n",
    "\n",
    "            return {\n",
    "                \"response\": response_text,\n",
    "                \"inference_time\": round(inference_time, 2),\n",
    "                \"input_tokens\": inputs.input_ids.shape[-1],\n",
    "                \"output_tokens\": outputs.shape[-1]\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error generating response: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        finally:\n",
    "            # Restore original model training mode\n",
    "            self.model.train(self.model_training_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, tokenizer):\n",
    "    \"\"\"Test the model with a sample input.\"\"\"\n",
    "    try:\n",
    "        # Initialize inference class\n",
    "        inferencer = ModelInference(model, tokenizer)\n",
    "\n",
    "        # Test input\n",
    "        test_input = 'Yes, give me an example of code to create a file to create a dataset of size 5x5 and populates it with random values and then outputs it as a csv, in python'\n",
    "\n",
    "        # Generate response with custom parameters\n",
    "        result = inferencer.generate_response(\n",
    "            test_input,\n",
    "            temperature=0.7,  # Override default temperature\n",
    "            max_new_tokens=300  # Override default max tokens\n",
    "        )\n",
    "\n",
    "        # Print results\n",
    "        print(\"\\nGenerated Response:\")\n",
    "        print(\"=\" * 50)\n",
    "        print(result[\"response\"])\n",
    "        print(\"\\nMetadata:\")\n",
    "        print(f\"Inference time: {result['inference_time']} seconds\")\n",
    "        print(f\"Input tokens: {result['input_tokens']}\")\n",
    "        print(f\"Output tokens: {result['output_tokens']}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error in test_model: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_train_data(data):\n",
    "    # Initialize an empty list to store each separate prompt-response pair\n",
    "    formatted_data = []\n",
    "\n",
    "    # Process each entry in the dataset\n",
    "    for entry in data:\n",
    "        # Process question-answer pairs for code packages\n",
    "        if entry[\"repo\"].startswith(\"https://github.com\"):\n",
    "          if entry[\"qa_pairs\"]:\n",
    "            for qa_pair in entry[\"qa_pairs\"]:\n",
    "                file = entry['file'].split(\"repos\")[-1]\n",
    "                prompt = (f\"Repository: {entry['repo']}\\n\"\n",
    "                          f\"File Name: {file}\\n\"\n",
    "                          f\"Language: {entry['language']}\\n\"\n",
    "                          f\"File Content:\\n{entry['content'][:4000]}\\n\"\n",
    "                          f\"Question: {qa_pair['question']}\")\n",
    "                response = qa_pair['answer']\n",
    "                formatted_data.append({\n",
    "                    \"text\": f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
    "                })\n",
    "            # Process completion tasks for code packages\n",
    "        if \"completion_tasks\" in entry and entry[\"completion_tasks\"]:\n",
    "          for completion_task in entry[\"completion_tasks\"]:\n",
    "            file = entry['file'].split(\"repos\")[-1]\n",
    "            prompt = (f\"Complete the following code:\\n{completion_task['partial']}\\n\"\n",
    "                      f\"Based on the file name: {file}\\n\"\n",
    "                      f\"With the following content: {entry['content'][:4000]}\")\n",
    "            response = completion_task.get('complete', completion_task['partial'])\n",
    "            imports = entry[\"dependencies\"][\"imports\"]\n",
    "            from_imports = entry[\"dependencies\"][\"from_imports\"]\n",
    "            partial_text = \"The following is the partial code, provide the completion for this code:\\n\"\n",
    "            complete_text = \"The following is the complete code:\\n\"\n",
    "            formatted_data.append({\n",
    "                \"text\": f\"<|im_start|>user\\n{partial_text}{prompt}<|im_end|>\\n<|im_start|>assistant\\n {complete_text} Imports: {imports}\\n From Imports: {from_imports}\\n {response}<|im_end|>\\n\"\n",
    "                })\n",
    "\n",
    "        # Process question-answer pairs for research papers\n",
    "        elif entry[\"repo\"] == \"research_papers\":\n",
    "            for qa_pair in entry[\"qa_pairs\"]:\n",
    "                prompt = (f\"Research Paper: {entry['file']}\\n\"\n",
    "                          f\"Content Excerpt:\\n{entry['content']}\\n\"\n",
    "                          f\"Question: {qa_pair['question']}\")\n",
    "                response = qa_pair['answer']\n",
    "                formatted_data.append({\n",
    "                    \"text\": f\"<|im_start|>user\\n{prompt}<|im_end|>\\n<|im_start|>assistant\\n{response}<|im_end|>\\n\"\n",
    "                })\n",
    "\n",
    "    # Convert the list of dictionaries into a DataFrame\n",
    "    data_df = pd.DataFrame(formatted_data)\n",
    "\n",
    "    # Create a new Dataset from the DataFrame\n",
    "    final_dataset = Dataset.from_pandas(data_df)\n",
    "\n",
    "    return final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the JSON data\n",
    "with open('/home/t126036sa_uhn_ca/content/combined_dataset_train.json') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Load the JSON data\n",
    "with open('/home/t126036sa_uhn_ca/content/combined_dataset_val.json') as f:\n",
    "    val_data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_final = prepare_train_data(train_data)\n",
    "val_data_final = prepare_train_data(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure LoRA (Low-Rank Adaptation) for efficient fine-tuning\n",
    "peft_config = LoraConfig(\n",
    "    r=16,                  # Rank of the update matrices - higher means more capacity to learn but uses more memory\n",
    "    lora_alpha=32,         # Scaling factor for LoRA updates - higher means stronger influence of fine-tuning\n",
    "    lora_dropout=0.1,      # Dropout rate for LoRA layers to prevent overfitting\n",
    "    bias=\"none\",           # Train bias terms using LoRA, giving more flexibility to the model\n",
    "    task_type=\"CAUSAL_LM\", # Specify this is for causal language modeling (predicting next token)\n",
    "    # List of model layers to apply LoRA to - these are the attention layers which are most important for adaptation\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Targeting query, value, key, and output projections\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure all training-related parameters\n",
    "training_arguments = TrainingArguments(\n",
    "    output_dir=output_model,                  # Directory where model checkpoints will be saved\n",
    "    per_device_train_batch_size=2,           # Number of samples processed per GPU - higher uses more memory\n",
    "    gradient_accumulation_steps=32,          # Accumulate gradients over 32 steps - effective batch size = 2*32 = 64\n",
    "    optim=\"paged_adamw_32bit\",              # Use memory-efficient AdamW optimizer with 32-bit precision\n",
    "    learning_rate=3e-4,                      # Learning rate - controls how big the model updates are\n",
    "    lr_scheduler_type=\"cosine_with_restarts\", # Learning rate schedule - reduces LR over time with periodic restarts\n",
    "    warmup_ratio=0.1,                        # Gradually increase LR for first 10% of training to stabilize training\n",
    "    save_strategy=\"epoch\",                   # Save model checkpoint at the end of each epoch\n",
    "    logging_steps=10,                        # Log training metrics every 10 steps for monitoring\n",
    "    num_train_epochs=5,                      # Number of complete passes through the training data\n",
    "    max_steps=500,                           # Maximum number of training steps, regardless of epochs\n",
    "    fp16=True,                              # Use 16-bit floating point precision to save memory\n",
    "    push_to_hub=True,                       # Automatically upload model to Hugging Face Hub\n",
    "    weight_decay=0.01,                      # L2 regularization to prevent overfitting\n",
    "    group_by_length=True,                    # Group similar length sequences together for efficiency\n",
    "\n",
    "    # Add evaluation during training\n",
    "    evaluation_strategy=\"epoch\",     # Run evaluation at the end of each epoch\n",
    "    eval_steps=50,                  # Also evaluate every 50 steps\n",
    "\n",
    "    # Save best model based on evaluation metric\n",
    "    load_best_model_at_end=True,    # Load the best model at the end of training\n",
    "    metric_for_best_model=\"loss\",   # Use loss to determine best model\n",
    "\n",
    "    # Additional logging\n",
    "    logging_dir=\"./logs\",           # Directory for training logs\n",
    "    report_to=[\"tensorboard\"],      # Log to tensorboard for visualization\n",
    "    ddp_find_unused_parameters=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the Supervised Fine-Tuning trainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,                            # The base model to fine-tune\n",
    "    train_dataset=train_data_final,               # The dataset to train on\n",
    "    peft_config=peft_config,                # LoRA configuration from above\n",
    "    dataset_text_field=\"text\",              # Column name in dataset containing the text to train on\n",
    "    args=training_arguments,                # Training arguments from above\n",
    "    tokenizer=tokenizer,                    # Tokenizer for converting text to tokens\n",
    "    packing=True,                           # Pack multiple sequences together to maximize GPU utilization\n",
    "    max_seq_length=3000,                     # Maximum length of input sequences - longer sequences get truncated\n",
    "    eval_dataset=val_data_final,           # Add validation dataset if available\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify the configuration:\n",
    "print(f\"Effective batch size: {training_arguments.per_device_train_batch_size * training_arguments.gradient_accumulation_steps}\")\n",
    "print(f\"Number of trainable parameters: {trainer.model.num_parameters(only_trainable=True)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "labgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
