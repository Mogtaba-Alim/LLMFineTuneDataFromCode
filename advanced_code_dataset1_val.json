[
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/downloadPSet.R",
    "language": "R",
    "content": "#' Return a table of PharmacoSets available for download\n#'\n#' The function fetches a table of all PharmacoSets available for download.\n#' The table includes the dataset names, version information for the data in the PSet,\n#' the date of last update, the name of the PSet, and references for the data contained within,\n#' a DOI for the data, and a direct download link. Download can also be done using the downloadPSet\n#' function.\n#'\n#' Much more information on the processing of the data and data provenance can be found at:\n#' www.orcestra.ca\n#'\n#'\n#' @examples\n#' if (interactive()){\n#'     availablePSets()\n#' }\n#'\n#' @param canonical `logical(1)` Should available PSets show only official\n#'   PSets, or should user generated PSets be included?\n#'\n#' @return A `data.frame` with details about the available PharmacoSet objects\n#' @export\n#' @import jsonlite\navailablePSets <- function(canonical=TRUE){\n\n  if (canonical) {\n    avail.psets <- fromJSON(\"http://www.orcestra.ca/api/psets/canonical\")\n  } else {\n    avail.psets <- fromJSON(\"http://www.orcestra.ca/api/psets/available\")\n  }\n\n\n  pSetTable <- data.frame(\"Dataset Name\" = avail.psets$dataset$name,\n                          \"Date Created\" = avail.psets$dateCreated,\n                          \"PSet Name\" = avail.psets$name,\n                          avail.psets$dataset$versionInfo,\n                          \"DOI\" = avail.psets$doi,\n                          \"Download\" = avail.psets$downloadLink, stringsAsFactors = FALSE, check.names = FALSE)\n\n  return(pSetTable)\n}\n\n#' Download a PharmacoSet object\n#'\n#' This function allows you to download a \\code{PharmacoSet} object for use with this\n#' package. The \\code{PharmacoSets} have been extensively curated and organised within\n#' a PharacoSet class, enabling use with all the analysis tools provided in\n#' \\code{PharmacoGx}. User \\code{availablePSets} to discover which PSets are available.\n#'\n#' @examples\n#' \\dontrun{\n#'     if (interactive()) downloadPSet(\"CTRPv2_2015\")\n#' }\n#'\n#' @section Warning:\n#' BREAKING CHANGES - this function now defaults to `tempdir()` as the download\n#' path! You must specify a saveDir or manually save the PSet if you want\n#' your download to persist past your current R session.`\n#'\n#' @param name \\code{Character} string, the name of the PhamracoSet to download.\n#' Note that this is not the dataset name, but the PSet name - dataset names are\n#' not guaranteed to be unique.\n#' @param saveDir \\code{Character} string with the folder path where the\n#'     PharmacoSet should be saved. Defaults to `tempdir()`. Will create\n#'     directory if it does not exist.\n#' @param pSetFileName \\code{character} string, the file name to save the\n#'   dataset under\n#' @param verbose \\code{bool} Should status messages be printed during download.\n#'   Defaults to TRUE.\n#' @param timeout \\code{numeric} Parameter that lets you extend R's default timeout for\n#'   downloading large files. Defaults for this function to 600.\n#' @return A PSet object with the dataset\n#'\n#' @export\n#' @importFrom downloader download\ndownloadPSet <- function(name, saveDir=tempdir(), pSetFileName=NULL,\n    verbose=TRUE, timeout=600) {\n\n  # change the download timeout since the files are big\n  opts <- options()\n  options(timeout=timeout)\n  on.exit(options(opts))\n\n  pSetTable <- availablePSets(canonical=FALSE)\n\n  whichx <- match(name, pSetTable[, \"PSet Name\"])\n  if (is.na(whichx)) {\n    stop('Unknown Dataset. Please use the availablePSets() function for the table of available PharamcoSets.')\n  }\n\n  if (!file.exists(saveDir)) {\n    dir.create(saveDir, recursive=TRUE)\n  }\n\n  if (is.null(pSetFileName)){\n    pSetFileName <- paste(pSetTable[whichx,\"PSet Name\"], \".rds\", sep=\"\")\n  }\n  if (!file.exists(file.path(saveDir, pSetFileName))) {\n    downloader::download(url = as.character(pSetTable[whichx,\"Download\"]),\n                         destfile=file.path(saveDir, pSetFileName),\n                         quiet=!verbose,\n                         mode='wb')\n  }\n  pSet <- readRDS(file.path(saveDir, pSetFileName))\n  pSet <- updateObject(pSet)\n  saveRDS(pSet, file=file.path(saveDir, pSetFileName))\n  return(pSet)\n}\n\n#' @importFrom utils read.table write.table\n.createPSetEntry <- function(pSet, outfn) {\n\n  if(file.exists(outfn)){\n    pSetTable <- read.table(outfn, as.is=TRUE)\n    newrow <- c(name(pSet), pSet@datasetType, paste(names(pSet@molecularProfiles), collapse=\"/\"), pSet@annotation$dateCreated, NA)\n    pSetTable <- rbind(pSetTable, newrow)\n    rownames(pSetTable) <- pSetTable[,1]\n    write.table(pSetTable, file=outfn)\n  } else {\n    newrow <- c(name(pSet), pSet@datasetType, paste(names(pSet@molecularProfiles), collapse=\"/\"), pSet@annotation$dateCreated, NA)\n    pSetTable <- t(matrix(newrow))\n    colnames(pSetTable) <- c(\"PSet.Name\",\"Dataset.Type\",\"Available.Molecular.Profiles\",\"Date.Updated\",\"URL\")\n    rownames(pSetTable) <- pSetTable[,1]\n    write.table(pSetTable, file=outfn)\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `availablePSets` function and what are its key components?",
        "answer": "The `availablePSets` function retrieves a table of PharmacoSets available for download. It fetches data from an API endpoint, processes the JSON response, and returns a data frame containing information about each PSet, including dataset name, creation date, PSet name, version info, DOI, and download link. The function takes a `canonical` parameter to determine whether to show only official PSets or include user-generated ones."
      },
      {
        "question": "How does the `downloadPSet` function handle large file downloads and what precautions does it take?",
        "answer": "The `downloadPSet` function handles large file downloads by temporarily increasing R's default timeout. It sets a custom timeout value (default 600 seconds) using `options(timeout=timeout)` at the beginning of the function and restores the original options when the function exits using `on.exit(options(opts))`. This ensures that the download has enough time to complete for large files without affecting the global R settings."
      },
      {
        "question": "What is the purpose of the `.createPSetEntry` function and how does it handle existing files?",
        "answer": "The `.createPSetEntry` function creates or updates an entry for a PharmacoSet in a table file. If the output file exists, it reads the existing table, adds a new row with the PSet information, and writes the updated table back to the file. If the file doesn't exist, it creates a new table with the PSet information. The function extracts relevant data from the PSet object, such as name, dataset type, molecular profiles, and creation date, to create the entry."
      }
    ],
    "completion_tasks": [
      {
        "partial": "availablePSets <- function(canonical=TRUE){\n  if (canonical) {\n    avail.psets <- fromJSON(\"http://www.orcestra.ca/api/psets/canonical\")\n  } else {\n    avail.psets <- fromJSON(\"http://www.orcestra.ca/api/psets/available\")\n  }\n\n  pSetTable <- data.frame(\n    \"Dataset Name\" = avail.psets$dataset$name,\n    \"Date Created\" = avail.psets$dateCreated,\n    \"PSet Name\" = avail.psets$name,\n    avail.psets$dataset$versionInfo,\n    \"DOI\" = avail.psets$doi,\n    \"Download\" = avail.psets$downloadLink,\n    stringsAsFactors = FALSE,\n    check.names = FALSE\n  )\n\n  return(pSetTable)\n}",
        "complete": "availablePSets <- function(canonical=TRUE){\n  avail.psets <- fromJSON(paste0(\"http://www.orcestra.ca/api/psets/\", if(canonical) \"canonical\" else \"available\"))\n\n  pSetTable <- data.frame(\n    \"Dataset Name\" = avail.psets$dataset$name,\n    \"Date Created\" = avail.psets$dateCreated,\n    \"PSet Name\" = avail.psets$name,\n    avail.psets$dataset$versionInfo,\n    \"DOI\" = avail.psets$doi,\n    \"Download\" = avail.psets$downloadLink,\n    stringsAsFactors = FALSE,\n    check.names = FALSE\n  )\n\n  return(pSetTable)\n}"
      },
      {
        "partial": "downloadPSet <- function(name, saveDir=tempdir(), pSetFileName=NULL, verbose=TRUE, timeout=600) {\n  options(timeout=timeout)\n  on.exit(options(opts))\n\n  pSetTable <- availablePSets(canonical=FALSE)\n  whichx <- match(name, pSetTable[, \"PSet Name\"])\n  if (is.na(whichx)) {\n    stop('Unknown Dataset. Please use the availablePSets() function for the table of available PharamcoSets.')\n  }\n\n  if (!file.exists(saveDir)) {\n    dir.create(saveDir, recursive=TRUE)\n  }\n\n  if (is.null(pSetFileName)){\n    pSetFileName <- paste(pSetTable[whichx,\"PSet Name\"], \".rds\", sep=\"\")\n  }\n  if (!file.exists(file.path(saveDir, pSetFileName))) {\n    downloader::download(url = as.character(pSetTable[whichx,\"Download\"]),\n                         destfile=file.path(saveDir, pSetFileName),\n                         quiet=!verbose,\n                         mode='wb')\n  }\n  pSet <- readRDS(file.path(saveDir, pSetFileName))\n  pSet <- updateObject(pSet)\n  saveRDS(pSet, file=file.path(saveDir, pSetFileName))\n  return(pSet)\n}",
        "complete": "downloadPSet <- function(name, saveDir=tempdir(), pSetFileName=NULL, verbose=TRUE, timeout=600) {\n  opts <- options(timeout=timeout)\n  on.exit(options(opts))\n\n  pSetTable <- availablePSets(canonical=FALSE)\n  whichx <- match(name, pSetTable[, \"PSet Name\"])\n  if (is.na(whichx)) stop('Unknown Dataset. Please use the availablePSets() function for the table of available PharamcoSets.')\n\n  if (!dir.exists(saveDir)) dir.create(saveDir, recursive=TRUE)\n\n  pSetFileName <- pSetFileName %||% paste0(pSetTable[whichx,\"PSet Name\"], \".rds\")\n  filePath <- file.path(saveDir, pSetFileName)\n\n  if (!file.exists(filePath)) {\n    downloader::download(url = as.character(pSetTable[whichx,\"Download\"]),\n                         destfile=filePath,\n                         quiet=!verbose,\n                         mode='wb')\n  }\n  pSet <- readRDS(filePath)\n  pSet <- updateObject(pSet)\n  saveRDS(pSet, file=filePath)\n  return(pSet)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/negative_controls.py",
    "language": "py",
    "content": "from venv import logger\nimport SimpleITK as sitk\nfrom SimpleITK import Image\nimport numpy as np\nimport random\n\nfrom readii.image_processing import alignImages, getROIVoxelLabel\nfrom readii.utils import get_logger\n\nfrom typing import Optional, Union\nfrom numpy import ndarray\n\nlogger = get_logger()\n\ndef getArrayFromImageOrArray(imageOrArray: Union[Image, ndarray]) -> ndarray:\n    \"\"\"Function to convert a SimpleITK Image to a numpy array.\n    \n    Parameters\n    ----------\n    imageOrArray : sitk.Image | np.ndarray\n        Image or array to convert to numpy array.\n\n    Returns\n    -------\n    np.ndarray\n        Numpy array version of the input image or array.\n        \n    Raises\n    ------\n    ValueError\n        If the input is not a SimpleITK Image or numpy array.\n    \"\"\"\n    assert isinstance(imageOrArray, Image) or isinstance(imageOrArray, ndarray), \\\n        \"Input must be a SimpleITK Image or numpy array.\"\n\n    if isinstance(imageOrArray, Image):\n        return sitk.GetArrayFromImage(imageOrArray)\n    elif isinstance(imageOrArray, ndarray):\n        return imageOrArray    \n\ndef makeShuffleImage(\n    baseImage: Union[Image, ndarray],\n    randomSeed: Optional[int] = None,\n) -> Union[Image, ndarray]:\n    \n    \"\"\"Function to shuffle all pixel values in a sitk.Image or np.ndarray (developed for 3D, should work on 2D as well)\n\n    Parameters\n    ----------\n    baseImage : sitk.Image | np.ndarray\n        Image to shuffle the pixels in. Can be a sitk.Image or np.ndarray.\n    randomSeed : int\n        Value to initialize random number generator with. Set for reproducible results.\n        \n    Returns\n    -------\n    sitk.Image | np.ndarray\n        Image with all pixel values randomly shuffled with same dimensions and object type as input image.\n    \"\"\"\n    # # Check if baseImage is a sitk.Image or np.ndarray\n    arrImage = getArrayFromImageOrArray(baseImage)\n\n    # Get array dimensions to reshape back to\n    imgDimensions = arrImage.shape\n\n    # Flatten the 3D array to 1D so values can be shuffled\n    flatArrImage = arrImage.flatten()\n\n    # Set the random seed for np random generator\n    randNumGen = np.random.default_rng(seed=randomSeed)\n\n    # Shuffle the flat array\n    randNumGen.shuffle(flatArrImage)\n\n    # Reshape the array back into the original image dimensions\n    shuffled3DArrImage = np.reshape(flatArrImage, imgDimensions)\n\n    if type(baseImage) == sitk.Image:\n        # Convert back to sitk Image\n        shuffledImage = sitk.GetImageFromArray(shuffled3DArrImage)\n\n        # Set the origin/direction/spacing from original image to shuffled image\n        alignedShuffledImage = alignImages(baseImage, shuffledImage)\n\n        # Return the shuffled image\n        return alignedShuffledImage\n    \n    else:\n        # Return the shuffled array\n        return shuffled3DArrImage\n\ndef makeRandomImage(\n    baseImage: Union[Image, ndarray],\n    randomSeed: Optional[int] = None,\n) -> Union[sitk.Image, np.ndarray]:\n    \"\"\"Function to generate random pixel values based on the range of values in a sitk.Image or np.ndarray (developed for 3D, should work on 2D as well)\n\n    Parameters\n    ----------\n    baseImage : sitk.Image | np.ndarray\n        Image to randomly generate pixel values. Can be a sitk.Image or np.ndarray.\n    randomSeed : int\n        Value to initialize random number generator with. Set for reproducible results.\n\n    Returns\n    -------\n    sitk.Image | np.ndarray\n        Image with all pixel values randomly generated with same dimensions and object type as input image.\n    \"\"\"\n    # # Check if baseImage is a sitk.Image or np.ndarray\n    arrImage = getArrayFromImageOrArray(baseImage)\n\n    # Get array dimensions to reshape back to\n    imgDimensions = arrImage.shape\n\n    # Get min and max HU values to set as range for random values\n    minVoxelVal = np.min(arrImage)\n    maxVoxelVal = np.max(arrImage)\n\n    # Delete arrImage to save memory\n    del arrImage\n\n    # Set the random seed for np random generator\n    randNumGen = np.random.default_rng(seed=randomSeed)\n\n    # Generate random array with same dimensions as baseImage with values ranging from the minimum to maximum inclusive of the original image\n    random3DArr = randNumGen.integers(\n        low=minVoxelVal, high=maxVoxelVal, endpoint=True, size=imgDimensions\n    )\n\n    if type(baseImage) == sitk.Image:\n        # Convert random array to a sitk Image\n        randomImage = sitk.GetImageFromArray(random3DArr)\n\n        # Set the origin/direction/spacing from the original image to the random image\n        alignedRandomImage = alignImages(baseImage, randomImage)\n\n        # Return the random image\n        return alignedRandomImage\n    \n    else:\n        # Return the random array\n        return random3DArr\n    \ndef makeRandomSampleFromDistributionImage(\n    baseImage: Union[Image, ndarray],\n    randomSeed: Optional[int] = None,\n) -> Union[sitk.Image, np.ndarray]:\n    \"\"\"Function to randomly sample all the pixel values the distribution of existing values in a sitk.Image or np.ndarray.\n\n    Parameters\n    ----------\n    imageToRandomize : sitk.Image | np.ndarray\n        Image to randomly sample the pixels from. Can be a sitk.Image or np.ndarray.\n    randomSeed : int\n        Value to initialize random number generator with. Set for reproducible results.\n    Returns\n    -------\n    sitk.Image | np.ndarray\n        Image with all pixel values randomly sampled from the initial dstribution of the image, with same dimensions and object type as input image.\n    \"\"\"\n    # Check if baseImage is a sitk.Image or np.ndarray\n    arrImage = getArrayFromImageOrArray(baseImage)\n\n    # Get array dimensions to reshape back to\n    imgDimensions = arrImage.shape\n\n    # Flatten the 3D array to 1D so values can be shuffled\n    flatArrImage = arrImage.flatten()\n\n    # Set the random seed for np random number generator\n    randNumGen = np.random.default_rng(seed=randomSeed)\n\n    # Randomly sample values for new array from original image distribution\n    sampled_array = randNumGen.choice(flatArrImage, size=len(flatArrImage), replace=True)\n\n    # Reshape the array back into the original image dimensions\n    randomlySampled3DArrImage = np.reshape(sampled_array, imgDimensions)\n\n    if type(baseImage) == sitk.Image:\n        # Convert back to sitk Image\n        randomlySampledImage = sitk.GetImageFromArray(randomlySampled3DArrImage)\n\n        # Set the origin/direction/spacing from original image to sampled image\n        alignedRandomlySampledImage = alignImages(baseImage, randomlySampledImage)\n        \n        # Return the randomly sampled image \n        return alignedRandomlySampledImage\n    else:\n        # Return the randomly sampled array\n        return randomlySampled3DArrImage\n\ndef negativeControlROIOnly(\n        baseImage: Union[Image, ndarray], \n        roiMask: Union[Image, ndarray], \n        negativeControlType: str = \"shuffled\",\n        randomSeed: Optional[int] = None\n        ) -> Union[Image, ndarray]:\n    \"\"\"Function to apply a negative control to a ROI only, without changing the background of the image.\n\n    Parameters\n    ----------\n    baseImage : sitk.Image | np.ndarray\n        Image to apply negative control to. Can be a sitk.Image or np.ndarray.\n    roiMask : sitk.Image | np.ndarray\n        Mask of the ROI to apply negative control within. Can be a sitk.Image or np.ndarray.\n    negativeControlType : {'shuffled', 'randomized', 'randomized_sampled'}, default 'shuffled'\n        Name of negative control to apply.\n    randomSeed : int, default None    \n        Value to initialize random number generator with. Set for reproducible results.\n    \n    Returns\n    -------\n    sitk.Image | np.ndarray\n        Image with negative control function applied to all pixel values within the ROI.\n    \"\"\"\n\n    if negativeControlType not in [\"shuffled\", \"randomized\", \"randomized_sampled\"]:\n        raise ValueError(\"negativeControlType must be one of 'shuffled', 'randomized', or 'randomized_sampled'\")\n    \n    # Check if baseImage is a sitk.Image or np.ndarray\n    arrBaseImage = getArrayFromImageOrArray(baseImage)\n    \n    # Check if roiMask is a sitk.Image or np.ndarray\n    arrROIMask = getArrayFromImageOrArray(roiMask)\n\n    # Get binary segmentation masks\n    # ROI is 1, background is 0\n    binROIMask = np.where(arrROIMask > 0, 1, 0)\n    if binROIMask.any() == False:\n        raise ValueError(\"ROI mask is all 0s. No pixels in ROI to apply negative control to. ROI pixels should be > 1.\")\n\n    # Get just ROI pixels\n    maskIndices = np.nonzero(binROIMask)\n    # Get a 1D array of just the ROI pixels\n    flatROIBaseValues = arrBaseImage[maskIndices]\n\n    # Get desired negative control of baseImage\n    arrNCROIValues = applyNegativeControl(baseImage = flatROIBaseValues,\n                                          negativeControlType = negativeControlType,\n                                          negativeControlRegion = \"full\",\n                                          randomSeed = randomSeed)\n\n    arrBaseImage[maskIndices] = arrNCROIValues\n\n    # # Apply negative control to ROI pixels and keep original non-ROI pixels\n    # arrNCROIImage = (arrNCBaseImage * binROIMask) + (arrBaseImage * inverseBinROIMask)\n\n    if type(baseImage) == sitk.Image:\n        # Convert back to sitk Image\n        ncROIImage = sitk.GetImageFromArray(arrBaseImage)\n        \n        # Set the origin/direction/spacing from original image to negative control image\n        alignedNCROIImage = alignImages(baseImage, ncROIImage)\n        \n        # Return the negative control image\n        return alignedNCROIImage\n    else:\n        # Return the negative control array\n        return arrBaseImage\n\n\ndef negativeControlNonROIOnly(\n        baseImage: Union[Image, ndarray], \n        roiMask: Union[Image, ndarray], \n        negativeControlType: str = \"shuffled\",\n        randomSeed: Optional[int] = None\n        ) -> Union[Image, ndarray]:\n    \"\"\"Function to apply a negative control to all pixel values outside the ROI, without changing the ROI pixels.\n\n    Parameters\n    ----------\n    baseImage : sitk.Image | np.ndarray\n        Image to apply negative control to. Can be a sitk.Image or np.ndarray.\n    roiMask : sitk.Image | np.ndarray\n        Mask of the ROI to keep original image values within. Can be a sitk.Image or np.ndarray.\n    negativeControlType : {'shuffled', 'randomized', 'randomized_sampled'}, default 'shuffled'\n        Name of negative control to apply.\n    randomSeed : int, default None    \n        Value to initialize random number generator with. Set for reproducible results.\n    \n    Returns\n    -------\n    sitk.Image | np.ndarray\n        Image with negative control function applied to all pixel values within the ROI.\n    \"\"\"\n\n    if negativeControlType not in [\"shuffled\", \"randomized\", \"randomized_sampled\"]:\n        raise ValueError(\"negativeControlType must be one of 'shuffled', 'randomized', or 'randomized_sampled'\")\n    \n    # Check if baseImage is a sitk.Image or np.ndarray\n    arrBaseImage = getArrayFromImageOrArray(baseImage)\n\n    # Check if roiMask is a sitk.Image or np.ndarray\n    arrROIMask = getArrayFromImageOrArray(roiMask)\n\n    # Get binary segmentation masks\n    # ROI is 1, background is 0\n    binNonROIMask = np.where(arrROIMask > 0, 0, 1)\n    if binNonROIMask.any() == False:\n        raise ValueError(\"ROI mask is all 0s. No pixels in ROI to apply negative control to. ROI pixels should be > 1.\")\n\n    # Get just ROI pixels\n    maskIndices = np.nonzero(binNonROIMask)\n    # Get a 1D array of just the ROI pixels\n    flatNonROIBaseValues = arrBaseImage[maskIndices]\n\n    # Get desired negative control of baseImage\n    arrNCNonROIValues = applyNegativeControl(baseImage = flatNonROIBaseValues,\n                                          negativeControlType = negativeControlType,\n                                          negativeControlRegion = \"full\",\n                                          randomSeed = randomSeed)\n    \n    arrBaseImage[maskIndices] = arrNCNonROIValues\n\n    if type(baseImage) == sitk.Image:\n        # Convert back to sitk Image\n        ncNonROIImage = sitk.GetImageFromArray(arrBaseImage)\n        \n        # Set the origin/direction/spacing from original image to negative control image\n        alignedNCNonROIImage = alignImages(baseImage, ncNonROIImage)\n        \n        # Return the negative control image\n        return alignedNCNonROIImage\n    else:\n        # Return the negative control array\n        return arrBaseImage\n\n\n\ndef applyNegativeControl(baseImage: Union[Image, ndarray],\n                         negativeControlType: str = \"shuffled\",\n                         negativeControlRegion: str = \"full\",\n                         roiMask: Optional[Union[Image, ndarray]] = None,\n                         randomSeed: Optional[int] = None\n) -> Union[Image, ndarray]:\n    \"\"\"Function to apply a negative control to a region of interest (ROI) within a sitk.Image or np.ndarray.\n\n    Parameters\n    ----------\n    baseImage : sitk.Image | np.ndarray\n        Image to apply negative control to. Can be a sitk.Image or np.ndarray.\n    negativeControlType : {'shuffled', 'randomized', 'randomized_sampled'}, default 'shuffled'\n        Name of negative control to apply.\n    negativeControlRegion : {'full', 'roi', 'non_roi'}, default 'full'\n        Whether to apply the negative control to the entire image, to the ROI, or to the non-ROI pixels.\n    randomSeed : int, default None    \n        Value to initialize random number generator with. Set for reproducible results.\n    \n    Returns\n    -------\n    sitk.Image | np.ndarray\n        Image with negative control function applied to all pixel values within the specificed region of interest.\n\n    \"\"\"\n    if negativeControlType not in [\"shuffled\", \"randomized\", \"randomized_sampled\"]:\n        raise ValueError(\"negativeControlType must be one of 'shuffled', 'randomized', or 'randomized_sampled'\")\n    if negativeControlRegion not in [\"full\", \"roi\", \"non_roi\"]:\n        raise ValueError(\"regionOfInterest must be one of 'full', 'roi', or 'non_roi'\")\n    \n    if negativeControlRegion == \"full\":\n        if negativeControlType == \"shuffled\":\n            return makeShuffleImage(baseImage, randomSeed)\n        elif negativeControlType == \"randomized\":\n            return makeRandomImage(baseImage, randomSeed)\n        elif negativeControlType == \"randomized_sampled\":\n            return makeRandomSampleFromDistributionImage(baseImage, randomSeed)\n    \n    assert roiMask is not None, \\\n        f\"ROI mask is None. Must pass ROI mask to negative control function for {negativeControlType} negative control.\"\n    \n    if negativeControlRegion == \"roi\":\n        return negativeControlROIOnly(baseImage, roiMask, negativeControlType, randomSeed)\n    else: # negativeControlRegion == \"non_roi\":\n        return negativeControlNonROIOnly(baseImage, roiMask, negativeControlType, randomSeed)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getArrayFromImageOrArray` function, and how does it handle different input types?",
        "answer": "The `getArrayFromImageOrArray` function is designed to convert a SimpleITK Image or numpy array into a numpy array. It handles two input types: 1) If the input is a SimpleITK Image, it uses `sitk.GetArrayFromImage` to convert it to a numpy array. 2) If the input is already a numpy array, it returns it as-is. The function raises a ValueError if the input is neither a SimpleITK Image nor a numpy array."
      },
      {
        "question": "How does the `makeShuffleImage` function work, and what steps does it take to preserve the original image properties?",
        "answer": "The `makeShuffleImage` function shuffles all pixel values in a 3D image while maintaining its original dimensions. It works as follows: 1) Converts the input to a numpy array. 2) Flattens the 3D array to 1D. 3) Shuffles the flattened array using a random number generator. 4) Reshapes the shuffled array back to the original dimensions. 5) If the input was a SimpleITK Image, it converts the shuffled array back to a SimpleITK Image and aligns it with the original image using the `alignImages` function to preserve the original origin, direction, and spacing."
      },
      {
        "question": "What is the difference between the `makeRandomImage` and `makeRandomSampleFromDistributionImage` functions in terms of how they generate new pixel values?",
        "answer": "The `makeRandomImage` and `makeRandomSampleFromDistributionImage` functions differ in how they generate new pixel values: 1) `makeRandomImage` generates completely random values within the range of the original image's minimum and maximum pixel values. It uses `np.random.integers` to create a new array with uniformly distributed random integers. 2) `makeRandomSampleFromDistributionImage` samples values from the existing distribution of pixel values in the original image. It uses `np.random.choice` to randomly select values from the flattened original image, maintaining the original distribution of pixel intensities."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def getArrayFromImageOrArray(imageOrArray: Union[Image, ndarray]) -> ndarray:\n    assert isinstance(imageOrArray, Image) or isinstance(imageOrArray, ndarray), \\\n        \"Input must be a SimpleITK Image or numpy array.\"\n\n    if isinstance(imageOrArray, Image):\n        return sitk.GetArrayFromImage(imageOrArray)\n    elif isinstance(imageOrArray, ndarray):\n        return imageOrArray",
        "complete": "def getArrayFromImageOrArray(imageOrArray: Union[Image, ndarray]) -> ndarray:\n    assert isinstance(imageOrArray, Image) or isinstance(imageOrArray, ndarray), \\\n        \"Input must be a SimpleITK Image or numpy array.\"\n\n    if isinstance(imageOrArray, Image):\n        return sitk.GetArrayFromImage(imageOrArray)\n    return imageOrArray"
      },
      {
        "partial": "def makeRandomSampleFromDistributionImage(\n    baseImage: Union[Image, ndarray],\n    randomSeed: Optional[int] = None,\n) -> Union[sitk.Image, np.ndarray]:\n    arrImage = getArrayFromImageOrArray(baseImage)\n    imgDimensions = arrImage.shape\n    flatArrImage = arrImage.flatten()\n    randNumGen = np.random.default_rng(seed=randomSeed)\n    sampled_array = randNumGen.choice(flatArrImage, size=len(flatArrImage), replace=True)\n    randomlySampled3DArrImage = np.reshape(sampled_array, imgDimensions)\n\n    if isinstance(baseImage, Image):\n        randomlySampledImage = sitk.GetImageFromArray(randomlySampled3DArrImage)\n        alignedRandomlySampledImage = alignImages(baseImage, randomlySampledImage)\n        return alignedRandomlySampledImage\n    else:\n        return randomlySampled3DArrImage",
        "complete": "def makeRandomSampleFromDistributionImage(\n    baseImage: Union[Image, ndarray],\n    randomSeed: Optional[int] = None,\n) -> Union[sitk.Image, np.ndarray]:\n    arrImage = getArrayFromImageOrArray(baseImage)\n    imgDimensions = arrImage.shape\n    flatArrImage = arrImage.flatten()\n    randNumGen = np.random.default_rng(seed=randomSeed)\n    sampled_array = randNumGen.choice(flatArrImage, size=len(flatArrImage), replace=True)\n    randomlySampled3DArrImage = np.reshape(sampled_array, imgDimensions)\n\n    if isinstance(baseImage, Image):\n        randomlySampledImage = sitk.GetImageFromArray(randomlySampled3DArrImage)\n        return alignImages(baseImage, randomlySampledImage)\n    return randomlySampled3DArrImage"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy",
        "random"
      ],
      "from_imports": [
        "venv.logger",
        "SimpleITK.Image",
        "readii.image_processing.alignImages",
        "readii.utils.get_logger",
        "typing.Optional",
        "numpy.ndarray"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_summarizeMolecularProfiles.R",
    "language": "R",
    "content": "library(PharmacoGx)\nlibrary(SummarizedExperiment)\nlibrary(S4Vectors)\n\ncontext(\"Checking summarizeMolecularProfiles function.\")\n\ndata(\"GDSCsmall\")\n\ntest_that(\"Summarize Molecular Profiles fails gracefully.\",{\n  ## FIXME:: No method defition for summarizeMolecularProfiles with class 'missing'\n  #expect_error(summarizeMolecularProfiles(), \"argument \\\"pSet\\\" is missing\")\n  expect_error(summarizeMolecularProfiles(GDSCsmall), \"argument \\\"mDataType\\\" is missing\")\n  expect_error(summarizeMolecularProfiles(GDSCsmall, \"rnaseq\"), \"Invalid mDataType\")\n})\n\ntest_that(\"Summarize Molecular Profiles function outputs data with right dimensions and dimnames, class\", {\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\")\n  expect_equal(colnames(testSummary), sampleNames(GDSCsmall))\n  expect_equivalent(is(testSummary, \"SummarizedExperiment\"), TRUE)\n  expect_length(rownames(testSummary), 300)\n})\n\ntest_that(\"Summarize Molecular Profiles correctly summarizes replicates\", {\n  myx <- \"647-V\" == colData(molecularProfilesSlot(GDSCsmall)$rna)$sampleid\n  testCells <- SummarizedExperiment::assay(molecularProfilesSlot(GDSCsmall)$rna, 1)[,myx]\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"median\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], apply(testCells, 1, median))\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"mean\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], apply(testCells, 1, mean))\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"first\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], testCells[,1])\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"last\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], testCells[,-1])\n\n  GDSCsmall2 <- subsetTo(GDSCsmall, cells = c(\"22RV1\", \"23132-87\"))\n  colData(molecularProfilesSlot(GDSCsmall2)$mutation)$sampleid <- \"22RV1\"\n  testCells <- SummarizedExperiment::assay(molecularProfilesSlot(GDSCsmall2)$mutation, 1)\n\n  testSummary <- summarizeMolecularProfiles(GDSCsmall2, \"mutation\", summary.stat = \"or\")\n  expect_equal(sum(as.numeric(SummarizedExperiment::assay(testSummary, 1)), na.rm=TRUE), 2)\n\n  testSummary <- summarizeMolecularProfiles(GDSCsmall2, \"mutation\", summary.stat = \"and\")\n  expect_equal(sum(as.numeric(SummarizedExperiment::assay(testSummary, 1)), na.rm=TRUE), 0)\n\n})\n\n\ntest_that(\"Summarize Molecular Profiles parameters work as expected\", {\n  expect_equal(summarizeMolecularProfiles(GDSCsmall, \"rna\", summarize=FALSE), molecularProfilesSlot(GDSCsmall)$rna)\n  expect_silent(summarizeMolecularProfiles(GDSCsmall, \"rna\", verbose = FALSE))\n  GDSCsmall2 <- GDSCsmall\n  molecularProfilesSlot(GDSCsmall2)$rna <- molecularProfilesSlot(GDSCsmall2)$rna[,1]\n  expect_equivalent(ncol(summarizeMolecularProfiles(GDSCsmall2, \"rna\", fill.missing = FALSE)), 1)\n  expect_equivalent(ncol(summarizeMolecularProfiles(GDSCsmall2, \"rna\", fill.missing = TRUE)), length(sampleNames(GDSCsmall2)))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `summarizeMolecularProfiles` function in this code, and how does it handle different summary statistics?",
        "answer": "The `summarizeMolecularProfiles` function is used to summarize molecular profiles from a PharmacoSet object. It can handle different summary statistics such as 'median', 'mean', 'first', and 'last' for RNA data, and 'or' and 'and' for mutation data. The function processes the data based on the specified `mDataType` and `summary.stat` parameters, allowing for flexible summarization of molecular data across different samples or replicates."
      },
      {
        "question": "How does the code ensure that the `summarizeMolecularProfiles` function handles errors and edge cases correctly?",
        "answer": "The code uses the `test_that` function to create unit tests that check for proper error handling. It tests for missing arguments, invalid data types, and ensures that the function fails gracefully in these cases. For example, it checks that an error is thrown when the 'mDataType' argument is missing or when an invalid 'mDataType' is provided. These tests help ensure the robustness of the function when dealing with various input scenarios."
      },
      {
        "question": "What methods are used in the code to verify the correctness of the `summarizeMolecularProfiles` function's output?",
        "answer": "The code uses several methods to verify the correctness of the `summarizeMolecularProfiles` function's output:\n1. It checks the dimensions and column names of the output to ensure they match the expected values.\n2. It verifies that the output is of the correct class (SummarizedExperiment).\n3. It compares the summarized values (e.g., median, mean) for specific samples against manually calculated values to ensure accuracy.\n4. It tests different summary statistics (median, mean, first, last, or, and) to confirm they produce the expected results.\n5. It checks that the function handles optional parameters correctly, such as 'summarize' and 'fill.missing'.\nThese comprehensive tests help ensure the function produces accurate and consistent results across various scenarios."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Summarize Molecular Profiles correctly summarizes replicates\", {\n  myx <- \"647-V\" == colData(molecularProfilesSlot(GDSCsmall)$rna)$sampleid\n  testCells <- SummarizedExperiment::assay(molecularProfilesSlot(GDSCsmall)$rna, 1)[,myx]\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"median\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], apply(testCells, 1, median))\n  # Complete the test for other summary statistics\n})",
        "complete": "test_that(\"Summarize Molecular Profiles correctly summarizes replicates\", {\n  myx <- \"647-V\" == colData(molecularProfilesSlot(GDSCsmall)$rna)$sampleid\n  testCells <- SummarizedExperiment::assay(molecularProfilesSlot(GDSCsmall)$rna, 1)[,myx]\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"median\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], apply(testCells, 1, median))\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"mean\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], apply(testCells, 1, mean))\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"first\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], testCells[,1])\n  testSummary <- summarizeMolecularProfiles(GDSCsmall, \"rna\", summary.stat = \"last\")\n  expect_equal(SummarizedExperiment::assay(testSummary, 1)[,\"647-V\"], testCells[,-1])\n})"
      },
      {
        "partial": "test_that(\"Summarize Molecular Profiles parameters work as expected\", {\n  expect_equal(summarizeMolecularProfiles(GDSCsmall, \"rna\", summarize=FALSE), molecularProfilesSlot(GDSCsmall)$rna)\n  expect_silent(summarizeMolecularProfiles(GDSCsmall, \"rna\", verbose = FALSE))\n  GDSCsmall2 <- GDSCsmall\n  molecularProfilesSlot(GDSCsmall2)$rna <- molecularProfilesSlot(GDSCsmall2)$rna[,1]\n  # Complete the test for fill.missing parameter\n})",
        "complete": "test_that(\"Summarize Molecular Profiles parameters work as expected\", {\n  expect_equal(summarizeMolecularProfiles(GDSCsmall, \"rna\", summarize=FALSE), molecularProfilesSlot(GDSCsmall)$rna)\n  expect_silent(summarizeMolecularProfiles(GDSCsmall, \"rna\", verbose = FALSE))\n  GDSCsmall2 <- GDSCsmall\n  molecularProfilesSlot(GDSCsmall2)$rna <- molecularProfilesSlot(GDSCsmall2)$rna[,1]\n  expect_equivalent(ncol(summarizeMolecularProfiles(GDSCsmall2, \"rna\", fill.missing = FALSE)), 1)\n  expect_equivalent(ncol(summarizeMolecularProfiles(GDSCsmall2, \"rna\", fill.missing = TRUE)), length(sampleNames(GDSCsmall2)))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/GDSC/GDSC_sampleMetadata.R",
    "language": "R",
    "content": "## code to prepare `gdsc_sampleMetadata` dataset goes here\nfilePath <- system.file(\"extdata/GDSC\", \"Cell_Lines_Details.xlsx\", package = \"AnnotationGx\")\nrawdata <- readxl::read_excel(filePath, sheet = 1, col_names = TRUE, na = \"NA\") |> data.table::as.data.table()\n\nGDSC_sampleMetadata <-\n  rawdata[`Sample Name` != \"TOTAL:\", .(GDSC.Sample_Name = `Sample Name`, GDSC.COSMIC_ID = `COSMIC identifier`)]\n\nusethis::use_data(GDSC_sampleMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code snippet, and how is it being used?",
        "answer": "The `system.file()` function is used to locate the file path of a specific file within an R package. In this code, it's being used to find the path to the 'Cell_Lines_Details.xlsx' file in the 'extdata/GDSC' directory of the 'AnnotationGx' package. This allows the code to access the file regardless of where the package is installed on the user's system."
      },
      {
        "question": "How is the data being read from the Excel file, and what transformations are applied to it?",
        "answer": "The data is read from the Excel file using the `readxl::read_excel()` function. It reads the first sheet of the file, uses the first row as column names, and replaces 'NA' strings with actual NA values. The resulting data is then converted to a data.table object using `data.table::as.data.table()`. After reading, the code filters out rows where 'Sample Name' is not 'TOTAL:' and selects only the 'Sample Name' and 'COSMIC identifier' columns, renaming them to 'GDSC.Sample_Name' and 'GDSC.COSMIC_ID' respectively."
      },
      {
        "question": "What is the purpose of the `usethis::use_data()` function call at the end of the snippet?",
        "answer": "The `usethis::use_data()` function is used to save R objects as datasets in a package. In this case, it's saving the `GDSC_sampleMetadata` data.table as a dataset in the current package. The `overwrite = TRUE` argument allows it to overwrite any existing dataset with the same name. This function is typically used during package development to include data with the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/GDSC\", \"Cell_Lines_Details.xlsx\", package = \"AnnotationGx\")\nrawdata <- readxl::read_excel(filePath, sheet = 1, col_names = TRUE, na = \"NA\") |> data.table::as.data.table()\n\nGDSC_sampleMetadata <-\n  rawdata[`Sample Name` != \"TOTAL:\", .(GDSC.Sample_Name = `Sample Name`, GDSC.COSMIC_ID = `COSMIC identifier`)]\n\n# Complete the code to save the GDSC_sampleMetadata as an R data object",
        "complete": "filePath <- system.file(\"extdata/GDSC\", \"Cell_Lines_Details.xlsx\", package = \"AnnotationGx\")\nrawdata <- readxl::read_excel(filePath, sheet = 1, col_names = TRUE, na = \"NA\") |> data.table::as.data.table()\n\nGDSC_sampleMetadata <-\n  rawdata[`Sample Name` != \"TOTAL:\", .(GDSC.Sample_Name = `Sample Name`, GDSC.COSMIC_ID = `COSMIC identifier`)]\n\nusethis::use_data(GDSC_sampleMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Complete the code to read the Excel file and create the GDSC_sampleMetadata data table\n\nfilePath <- system.file(\"extdata/GDSC\", \"Cell_Lines_Details.xlsx\", package = \"AnnotationGx\")\n\n# Add code here\n\nusethis::use_data(GDSC_sampleMetadata, overwrite = TRUE)",
        "complete": "filePath <- system.file(\"extdata/GDSC\", \"Cell_Lines_Details.xlsx\", package = \"AnnotationGx\")\nrawdata <- readxl::read_excel(filePath, sheet = 1, col_names = TRUE, na = \"NA\") |> data.table::as.data.table()\n\nGDSC_sampleMetadata <-\n  rawdata[`Sample Name` != \"TOTAL:\", .(GDSC.Sample_Name = `Sample Name`, GDSC.COSMIC_ID = `COSMIC identifier`)]\n\nusethis::use_data(GDSC_sampleMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/autopipeutils.py",
    "language": "py",
    "content": "import glob\nimport os\nimport shutil\nimport pathlib\nimport pickle    \nfrom .nnunet import generate_dataset_json, markdown_report_images\n\n\ndef save_data(self):\n    \"\"\"\n    Saves metadata about processing. \n    \"\"\"\n    files = glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())\n    for file in files:\n        filename = pathlib.Path(file).name\n        if filename == \"init_parameters.pkl\":\n            continue\n        subject_id = os.path.splitext(filename)[0]\n        with open(file,\"rb\") as f:\n            metadata = pickle.load(f)\n        self.output_df.loc[subject_id, list(metadata.keys())] = list(metadata.values())  # subject id targets the rows with that subject id and it is reassigning all the metadata values by key\n        \n    folder_renames = {}\n    for col in self.output_df.columns:\n        if col.startswith(\"folder\"):\n            self.output_df[col] = self.output_df[col].apply(lambda x: x if not isinstance(x, str) else pathlib.Path(x).as_posix().split(self.input_directory)[1][1:]) # rel path, exclude the slash at the beginning\n            folder_renames[col] = f\"input_{col}\"\n    self.output_df.rename(columns=folder_renames, inplace=True)  # append input_ to the column name\n    self.output_df.to_csv(self.output_df_path)  # dataset.csv\n\n    shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n\n    # Save dataset json\n    if self.is_nnunet:  # dataset.json for nnunet and .sh file to run to process it\n        imagests_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n        images_test_location = imagests_path if os.path.exists(imagests_path) else None\n        # print(self.existing_roi_names)\n        generate_dataset_json(pathlib.Path(self.output_directory, \"dataset.json\").as_posix(),\n                              pathlib.Path(self.output_directory, \"imagesTr\").as_posix(),\n                              images_test_location,\n                              tuple(self.nnunet_info[\"modalities\"].keys()),\n                              {v: k for k, v in self.existing_roi_names.items()},\n                              os.path.split(self.input_directory)[1])\n        _, child = os.path.split(self.output_directory)\n        shell_path = pathlib.Path(self.output_directory, child.split(\"_\")[1]+\".sh\").as_posix()\n        if os.path.exists(shell_path):\n            os.remove(shell_path)\n        with open(shell_path, \"w\", newline=\"\\n\") as f:\n            output = \"#!/bin/bash\\n\"\n            output += \"set -e\"\n            output += f'export nnUNet_raw_data_base=\"{self.base_output_directory}/nnUNet_raw_data_base\"\\n'\n            output += f'export nnUNet_preprocessed=\"{self.base_output_directory}/nnUNet_preprocessed\"\\n'\n            output += f'export RESULTS_FOLDER=\"{self.base_output_directory}/nnUNet_trained_models\"\\n\\n'\n            output += f'nnUNet_plan_and_preprocess -t {self.task_id} --verify_dataset_integrity\\n\\n'\n            output += 'for (( i=0; i<5; i++ ))\\n'\n            output += 'do\\n'\n            output += f'    nnUNet_train 3d_fullres nnUNetTrainerV2 {os.path.split(self.output_directory)[1]} $i --npz\\n'\n            output += 'done'\n            f.write(output)\n        markdown_report_images(self.output_directory, self.total_modality_counter)  # images saved to the output directory\n    \n    # Save summary info (factor into different file)\n    markdown_path = pathlib.Path(self.output_directory, \"report.md\").as_posix()\n    with open(markdown_path, \"w\", newline=\"\\n\") as f:\n        output = \"# Dataset Report\\n\\n\"\n        if not self.is_nnunet:\n            output += \"## Patients with broken DICOM references\\n\\n\"\n            output += \"<details>\\n\"\n            output += \"\\t<summary>Click to see the list of patients with broken DICOM references</summary>\\n\\n\\t\"\n            formatted_list = \"\\n\\t\".join(self.broken_patients)\n            output += f\"{formatted_list}\\n\"\n            output += \"</details>\\n\\n\"\n\n        if self.is_nnunet:\n            output += \"## Train Test Split\\n\\n\"\n            # pie_path = pathlib.Path(self.output_directory, \"markdown_images\", \"nnunet_train_test_pie.png\").as_posix()\n            pie_path = pathlib.Path(\"markdown_images\", \"nnunet_train_test_pie.png\").as_posix()\n            output += f\"![Pie Chart of Train Test Split]({pie_path})\\n\\n\"\n            output += \"## Image Modality Distribution\\n\\n\"\n            # bar_path = pathlib.Path(self.output_directory, \"markdown_images\", \"nnunet_modality_count.png\").as_posix()\n            bar_path = pathlib.Path(\"markdown_images\", \"nnunet_modality_count.png\").as_posix()\n            output += f\"![Pie Chart of Image Modality Distribution]({bar_path})\\n\\n\"\n        f.write(output)\n\n    return f\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `save_data` method in this code snippet?",
        "answer": "The `save_data` method is responsible for saving metadata about processing, including saving the output DataFrame to a CSV file, generating a dataset JSON file for nnUNet (if applicable), creating a shell script for nnUNet training, and generating a markdown report with dataset information. It also handles cleaning up temporary files and reorganizing folder structures."
      },
      {
        "question": "How does the code handle the renaming of columns in the output DataFrame that start with 'folder'?",
        "answer": "The code iterates through the columns of the output DataFrame, and for columns starting with 'folder', it applies a lambda function to convert the values to relative paths. It then creates a dictionary `folder_renames` where the keys are the original column names and the values are the new names with 'input_' prepended. Finally, it uses the `rename` method with `inplace=True` to apply these renamings to the DataFrame."
      },
      {
        "question": "What is the purpose of the shell script generated in this code, and what commands does it include?",
        "answer": "The shell script is generated for nnUNet training. It sets environment variables for nnUNet directories, runs the `nnUNet_plan_and_preprocess` command to prepare the dataset, and then uses a for loop to run `nnUNet_train` five times with different fold numbers. This script automates the process of training the nnUNet model on the prepared dataset."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def save_data(self):\n    files = glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())\n    for file in files:\n        filename = pathlib.Path(file).name\n        if filename == \"init_parameters.pkl\":\n            continue\n        subject_id = os.path.splitext(filename)[0]\n        with open(file, \"rb\") as f:\n            metadata = pickle.load(f)\n        # TODO: Update self.output_df with metadata\n\n    folder_renames = {}\n    for col in self.output_df.columns:\n        if col.startswith(\"folder\"):\n            # TODO: Update column values and rename\n\n    self.output_df.to_csv(self.output_df_path)\n    shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n\n    # TODO: Generate dataset.json for nnunet if self.is_nnunet is True\n\n    # TODO: Generate markdown report\n\n    return f",
        "complete": "def save_data(self):\n    files = glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())\n    for file in files:\n        filename = pathlib.Path(file).name\n        if filename == \"init_parameters.pkl\":\n            continue\n        subject_id = os.path.splitext(filename)[0]\n        with open(file, \"rb\") as f:\n            metadata = pickle.load(f)\n        self.output_df.loc[subject_id, list(metadata.keys())] = list(metadata.values())\n\n    folder_renames = {}\n    for col in self.output_df.columns:\n        if col.startswith(\"folder\"):\n            self.output_df[col] = self.output_df[col].apply(lambda x: x if not isinstance(x, str) else pathlib.Path(x).as_posix().split(self.input_directory)[1][1:])\n            folder_renames[col] = f\"input_{col}\"\n    self.output_df.rename(columns=folder_renames, inplace=True)\n    self.output_df.to_csv(self.output_df_path)\n    shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n\n    if self.is_nnunet:\n        imagests_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n        images_test_location = imagests_path if os.path.exists(imagests_path) else None\n        generate_dataset_json(pathlib.Path(self.output_directory, \"dataset.json\").as_posix(),\n                              pathlib.Path(self.output_directory, \"imagesTr\").as_posix(),\n                              images_test_location,\n                              tuple(self.nnunet_info[\"modalities\"].keys()),\n                              {v: k for k, v in self.existing_roi_names.items()},\n                              os.path.split(self.input_directory)[1])\n        _, child = os.path.split(self.output_directory)\n        shell_path = pathlib.Path(self.output_directory, child.split(\"_\")[1]+\".sh\").as_posix()\n        if os.path.exists(shell_path):\n            os.remove(shell_path)\n        with open(shell_path, \"w\", newline=\"\\n\") as f:\n            f.write(\"#!/bin/bash\\nset -e\\n\" +\n                    f'export nnUNet_raw_data_base=\"{self.base_output_directory}/nnUNet_raw_data_base\"\\n' +\n                    f'export nnUNet_preprocessed=\"{self.base_output_directory}/nnUNet_preprocessed\"\\n' +\n                    f'export RESULTS_FOLDER=\"{self.base_output_directory}/nnUNet_trained_models\"\\n\\n' +\n                    f'nnUNet_plan_and_preprocess -t {self.task_id} --verify_dataset_integrity\\n\\n' +\n                    'for (( i=0; i<5; i++ ))\\ndo\\n' +\n                    f'    nnUNet_train 3d_fullres nnUNetTrainerV2 {os.path.split(self.output_directory)[1]} $i --npz\\n' +\n                    'done')\n        markdown_report_images(self.output_directory, self.total_modality_counter)\n\n    markdown_path = pathlib.Path(self.output_directory, \"report.md\").as_posix()\n    with open(markdown_path, \"w\", newline=\"\\n\") as f:\n        f.write(\"# Dataset Report\\n\\n\" +\n                (\"## Patients with broken DICOM references\\n\\n<details>\\n\\t<summary>Click to see the list of patients with broken DICOM references</summary>\\n\\n\\t\" +\n                 \"\\n\\t\".join(self.broken_patients) + \"\\n</details>\\n\\n\" if not self.is_nnunet else \"\") +\n                (\"## Train Test Split\\n\\n![Pie Chart of Train Test Split](markdown_images/nnunet_train_test_pie.png)\\n\\n\" +\n                 \"## Image Modality Distribution\\n\\n![Pie Chart of Image Modality Distribution](markdown_images/nnunet_modality_count.png)\\n\\n\" if self.is_nnunet else \"\"))\n\n    return f"
      },
      {
        "partial": "import glob\nimport os\nimport shutil\nimport pathlib\nimport pickle\nfrom .nnunet import generate_dataset_json, markdown_report_images\n\nclass DataProcessor:\n    def __init__(self, input_directory, output_directory, is_nnunet=False):\n        self.input_directory = input_directory\n        self.output_directory = output_directory\n        self.is_nnunet = is_nnunet\n        self.output_df = None\n        self.output_df_path = None\n        self.broken_patients = []\n        self.existing_roi_names = {}\n        self.nnunet_info = {}\n        self.task_id = None\n        self.base_output_directory = None\n        self.total_modality_counter = None\n\n    def save_data(self):\n        # TODO: Implement the save_data method\n        pass",
        "complete": "import glob\nimport os\nimport shutil\nimport pathlib\nimport pickle\nfrom .nnunet import generate_dataset_json, markdown_report_images\n\nclass DataProcessor:\n    def __init__(self, input_directory, output_directory, is_nnunet=False):\n        self.input_directory = input_directory\n        self.output_directory = output_directory\n        self.is_nnunet = is_nnunet\n        self.output_df = None\n        self.output_df_path = None\n        self.broken_patients = []\n        self.existing_roi_names = {}\n        self.nnunet_info = {}\n        self.task_id = None\n        self.base_output_directory = None\n        self.total_modality_counter = None\n\n    def save_data(self):\n        files = glob.glob(pathlib.Path(self.output_directory, \".temp\", \"*.pkl\").as_posix())\n        for file in files:\n            filename = pathlib.Path(file).name\n            if filename == \"init_parameters.pkl\":\n                continue\n            subject_id = os.path.splitext(filename)[0]\n            with open(file, \"rb\") as f:\n                metadata = pickle.load(f)\n            self.output_df.loc[subject_id, list(metadata.keys())] = list(metadata.values())\n\n        folder_renames = {}\n        for col in self.output_df.columns:\n            if col.startswith(\"folder\"):\n                self.output_df[col] = self.output_df[col].apply(lambda x: x if not isinstance(x, str) else pathlib.Path(x).as_posix().split(self.input_directory)[1][1:])\n                folder_renames[col] = f\"input_{col}\"\n        self.output_df.rename(columns=folder_renames, inplace=True)\n        self.output_df.to_csv(self.output_df_path)\n\n        shutil.rmtree(pathlib.Path(self.output_directory, \".temp\").as_posix())\n\n        if self.is_nnunet:\n            imagests_path = pathlib.Path(self.output_directory, \"imagesTs\").as_posix()\n            images_test_location = imagests_path if os.path.exists(imagests_path) else None\n            generate_dataset_json(pathlib.Path(self.output_directory, \"dataset.json\").as_posix(),\n                                  pathlib.Path(self.output_directory, \"imagesTr\").as_posix(),\n                                  images_test_location,\n                                  tuple(self.nnunet_info[\"modalities\"].keys()),\n                                  {v: k for k, v in self.existing_roi_names.items()},\n                                  os.path.split(self.input_directory)[1])\n            _, child = os.path.split(self.output_directory)\n            shell_path = pathlib.Path(self.output_directory, child.split(\"_\")[1]+\".sh\").as_posix()\n            if os.path.exists(shell_path):\n                os.remove(shell_path)\n            with open(shell_path, \"w\", newline=\"\\n\") as f:\n                f.write(\"#!/bin/bash\\nset -e\\n\" +\n                        f'export nnUNet_raw_data_base=\"{self.base_output_directory}/nnUNet_raw_data_base\"\\n' +\n                        f'export nnUNet_preprocessed=\"{self.base_output_directory}/nnUNet_preprocessed\"\\n' +\n                        f'export RESULTS_FOLDER=\"{self.base_output_directory}/nnUNet_trained_models\"\\n\\n' +\n                        f'nnUNet_plan_and_preprocess -t {self.task_id} --verify_dataset_integrity\\n\\n' +\n                        'for (( i=0; i<5; i++ ))\\ndo\\n' +\n                        f'    nnUNet_train 3d_fullres nnUNetTrainerV2 {os.path.split(self.output_directory)[1]} $i --npz\\n' +\n                        'done')\n            markdown_report_images(self.output_directory, self.total_modality_counter)\n\n        markdown_path = pathlib.Path(self.output_directory, \"report.md\").as_posix()\n        with open(markdown_path, \"w\", newline=\"\\n\") as f:\n            f.write(\"# Dataset Report\\n\\n\" +\n                    (\"## Patients with broken DICOM references\\n\\n<details>\\n\\t<summary>Click to see the list of patients with broken DICOM references</summary>\\n\\n\\t\" +\n                     \"\\n\\t\".join(self.broken_patients) + \"\\n</details>\\n\\n\" if not self.is_nnunet else \"\") +\n                    (\"## Train Test Split\\n\\n![Pie Chart of Train Test Split](markdown_images/nnunet_train_test_pie.png)\\n\\n\" +\n                     \"## Image Modality Distribution\\n\\n![Pie Chart of Image Modality Distribution](markdown_images/nnunet_modality_count.png)\\n\\n\" if self.is_nnunet else \"\"))\n\n        return f"
      }
    ],
    "dependencies": {
      "imports": [
        "glob",
        "os",
        "shutil",
        "pathlib",
        "pickle"
      ],
      "from_imports": [
        "nnunet.generate_dataset_json"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/pipeline.py",
    "language": "py",
    "content": "from argparse import ArgumentParser\nfrom ast import arg\nimport os\nfrom venv import logger\n\nfrom readii.metadata import *\nfrom readii.feature_extraction import *\nfrom readii.utils import get_logger\n\nlogger = get_logger()\n\ndef parser():\n    \"\"\"Function to take command-line arguments and set them up for the pipeline run\n    \"\"\"\n    parser = ArgumentParser(\"READII Feature Extraction Pipeline\")\n\n    # arguments\n    parser.add_argument(\"data_directory\", type=str,\n                        help=\"Path to top-level directory of image dataset. Same as med-imagetools.\")\n    \n    parser.add_argument(\"output_directory\", type=str,\n                       help=\"Path to output directory to save radiomic features and metadata.\")\n    \n    parser.add_argument(\"--roi_names\", type=str, default=None,\n                        help=\"Name of region of interest in RTSTRUCT to perform extraction on.\")\n    \n    parser.add_argument(\"--pyradiomics_setting\", type=str, default=None,\n                        help=\"Path to PyRadiomics configuration YAML file. If none provided, will use \\\n                              default in src/readii/data/.\")\n    \n    parser.add_argument(\"--negative_controls\", type=str, default=None,\n                        help=\"List of negative control types to run feature extraction on. Input as comma-separated list with no spaces.  \\\n                              Options: randomized_full,randomized_roi,randomized_non_roi,shuffled_full,shuffled_roi,shuffled_non_roi,randomized_sampled_full,randomized_sampled_roi,randomized_sampled_non_roi\")\n\n    parser.add_argument(\"--parallel\", action=\"store_true\",\n                        help=\"Whether to run feature extraction in a parallel process. False by default.\")\n\n    parser.add_argument(\"--update\", action=\"store_true\", help=\"Flag to force rerun all steps of pipeline. False by default.\")\n\n    parser.add_argument(\"--random_seed\", type=int,\n                        help=\"Value to set random seed to for reproducible negative controls\")\n\n    parser.add_argument(\"--keep_running\", action=\"store_true\",\n                        help=\"Flag to keep pipeline running even when feature extraction for a patient fails. False by default.\")\n\n    return parser.parse_known_args()[0]\n    \n\n    \n\ndef main():\n    \"\"\"Function to run READII radiomic feature extraction pipeline.\n    \"\"\"\n    args = parser()\n    pretty_args = '\\n\\t'.join([f\"{k}: {v}\" for k, v in vars(args).items()])\n    logger.debug(\n        f\"Arguments:\\n\\t{pretty_args}\"\n    )\n    \n    args_dict = vars(args)\n\n    logger.info(\"Starting readii pipeline...\")\n\n    # Set up output directory\n    outputDir = os.path.join(args.output_directory, \"readii_outputs\")\n    if not os.path.exists(outputDir):\n        logger.info(f\"Directory {outputDir} does not exist. Creating...\")\n        os.makedirs(outputDir)\n    else:\n        logger.warning(f\"Directory {outputDir} already exists. Will overwrite contents.\")\n\n    # Find med-imagetools output files\n    logger.info(\"Finding med-imagetools outputs...\")\n    parentDirPath, datasetName = os.path.split(args.data_directory)\n    imageFileListPath = os.path.join(parentDirPath + \"/.imgtools/imgtools_\" + datasetName + \".csv\")\n    if not os.path.exists(imageFileListPath):\n        # Can we run med-imagetools in here?\n        logger.error(\n            f\"Expected file {imageFileListPath} not found. Check the data_directory argument or run med-imagetools.\"\n        )\n        raise FileNotFoundError(\"Output for med-imagetools not found for this image set. Check the data_directory argument or run med-imagetools.\")\n\n    logger.info(\"Getting segmentation type...\")\n    try:\n        # Get segType from imageFileList to generate the image metadata file and set up feature extraction\n        segType = getSegmentationType(imageFileListPath)\n    except RuntimeError as e:\n        logger.error(str(e))\n        logger.error(\"Feature extraction not complete.\")\n        exit()\n\n\n    # Check if image metadata file has already been created\n    imageMetadataPath = createImageMetadataFile(\n        outputDir, \n        parentDirPath, \n        datasetName, \n        segType, \n        imageFileListPath, \n        args.update)\n    \n    # Check if radiomic feature file already exists\n    radFeatOutPath = os.path.join(outputDir, \"features/\", \"radiomicfeatures_original_\" + datasetName + \".csv\")\n    if not os.path.exists(radFeatOutPath) or args.update:\n        logger.info(\"Starting radiomic feature extraction...\")\n        radiomicFeatures = radiomicFeatureExtraction(imageMetadataPath = imageMetadataPath,\n                                                     imageDirPath = parentDirPath,\n                                                     roiNames = args.roi_names,\n                                                     pyradiomicsParamFilePath = args.pyradiomics_setting,\n                                                     outputDirPath = outputDir,\n                                                     negativeControl = None,\n                                                     parallel = args.parallel,\n                                                     keep_running = args.keep_running)\n    else:\n        logger.info(f\"Radiomic features have already been extracted. See {radFeatOutPath}\")\n\n    # Negative control radiomic feature extraction\n    if args.negative_controls != None:\n        # Get all negative controls to run\n        negativeControlList = args.negative_controls.split(\",\")\n\n        # Perform feature extraction for each negative control type\n        for negativeControl in negativeControlList:\n            ncRadFeatOutPath = os.path.join(outputDir, \"features/\", \"radiomicfeatures_\" + negativeControl + \"_\" + datasetName + \".csv\")\n            if not os.path.exists(ncRadFeatOutPath) or args.update:\n                logger.info(f\"Starting radiomic feature extraction for negative control: {negativeControl}\")\n                ncRadiomicFeatures = radiomicFeatureExtraction(imageMetadataPath = imageMetadataPath,\n                                                               imageDirPath = parentDirPath,\n                                                               roiNames = args.roi_names,\n                                                               pyradiomicsParamFilePath = args.pyradiomics_setting,\n                                                               outputDirPath = outputDir,\n                                                               negativeControl = negativeControl,\n                                                               randomSeed=args.random_seed,\n                                                               parallel = args.parallel,\n                                                               keep_running = args.keep_running)\n            else:\n                logger.info(f\"{negativeControl} radiomic features have already been extracted. See {ncRadFeatOutPath}\")\n\n    \n    logger.info(\"Pipeline complete.\")\n\nif __name__ == \"__main__\":\n    main()",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `parser()` function in this code, and how does it handle command-line arguments?",
        "answer": "The `parser()` function sets up command-line arguments for the READII Feature Extraction Pipeline. It uses Python's `ArgumentParser` to define and parse various arguments such as data directory, output directory, ROI names, and pipeline settings. The function returns the parsed arguments, which can be used to customize the pipeline's behavior."
      },
      {
        "question": "How does the code handle the case when the med-imagetools output file is not found?",
        "answer": "When the med-imagetools output file (imgtools_{datasetName}.csv) is not found, the code raises a `FileNotFoundError` with a custom error message. It first logs an error message using the logger, then raises the exception, suggesting that the user should check the data_directory argument or run med-imagetools."
      },
      {
        "question": "Explain how the code implements negative control radiomic feature extraction and what conditions trigger this process.",
        "answer": "Negative control radiomic feature extraction is implemented when the `--negative_controls` argument is provided. The code splits the argument value into a list of negative control types. For each type, it checks if the corresponding output file exists. If the file doesn't exist or the `--update` flag is set, it runs the `radiomicFeatureExtraction` function with the specific negative control type. This process allows for multiple negative control analyses to be performed based on user input."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def parser():\n    parser = ArgumentParser(\"READII Feature Extraction Pipeline\")\n    \n    parser.add_argument(\"data_directory\", type=str,\n                        help=\"Path to top-level directory of image dataset. Same as med-imagetools.\")\n    \n    parser.add_argument(\"output_directory\", type=str,\n                       help=\"Path to output directory to save radiomic features and metadata.\")\n    \n    # Add more arguments here\n    \n    return parser.parse_known_args()[0]",
        "complete": "def parser():\n    parser = ArgumentParser(\"READII Feature Extraction Pipeline\")\n    \n    parser.add_argument(\"data_directory\", type=str,\n                        help=\"Path to top-level directory of image dataset. Same as med-imagetools.\")\n    \n    parser.add_argument(\"output_directory\", type=str,\n                       help=\"Path to output directory to save radiomic features and metadata.\")\n    \n    parser.add_argument(\"--roi_names\", type=str, default=None,\n                        help=\"Name of region of interest in RTSTRUCT to perform extraction on.\")\n    \n    parser.add_argument(\"--pyradiomics_setting\", type=str, default=None,\n                        help=\"Path to PyRadiomics configuration YAML file. If none provided, will use default in src/readii/data/.\")\n    \n    parser.add_argument(\"--negative_controls\", type=str, default=None,\n                        help=\"List of negative control types to run feature extraction on. Input as comma-separated list with no spaces.\")\n    \n    parser.add_argument(\"--parallel\", action=\"store_true\",\n                        help=\"Whether to run feature extraction in a parallel process. False by default.\")\n    \n    parser.add_argument(\"--update\", action=\"store_true\", help=\"Flag to force rerun all steps of pipeline. False by default.\")\n    \n    parser.add_argument(\"--random_seed\", type=int,\n                        help=\"Value to set random seed to for reproducible negative controls\")\n    \n    parser.add_argument(\"--keep_running\", action=\"store_true\",\n                        help=\"Flag to keep pipeline running even when feature extraction for a patient fails. False by default.\")\n    \n    return parser.parse_known_args()[0]"
      },
      {
        "partial": "def main():\n    args = parser()\n    outputDir = os.path.join(args.output_directory, \"readii_outputs\")\n    if not os.path.exists(outputDir):\n        os.makedirs(outputDir)\n    \n    parentDirPath, datasetName = os.path.split(args.data_directory)\n    imageFileListPath = os.path.join(parentDirPath + \"/.imgtools/imgtools_\" + datasetName + \".csv\")\n    \n    # Add code to handle segmentation type, metadata, and feature extraction\n    \n    logger.info(\"Pipeline complete.\")",
        "complete": "def main():\n    args = parser()\n    outputDir = os.path.join(args.output_directory, \"readii_outputs\")\n    if not os.path.exists(outputDir):\n        os.makedirs(outputDir)\n    \n    parentDirPath, datasetName = os.path.split(args.data_directory)\n    imageFileListPath = os.path.join(parentDirPath + \"/.imgtools/imgtools_\" + datasetName + \".csv\")\n    \n    segType = getSegmentationType(imageFileListPath)\n    \n    imageMetadataPath = createImageMetadataFile(outputDir, parentDirPath, datasetName, segType, imageFileListPath, args.update)\n    \n    radFeatOutPath = os.path.join(outputDir, \"features/\", f\"radiomicfeatures_original_{datasetName}.csv\")\n    if not os.path.exists(radFeatOutPath) or args.update:\n        radiomicFeatures = radiomicFeatureExtraction(imageMetadataPath, parentDirPath, args.roi_names, args.pyradiomics_setting, outputDir, None, args.parallel, args.keep_running)\n    \n    if args.negative_controls:\n        for negativeControl in args.negative_controls.split(\",\"):\n            ncRadFeatOutPath = os.path.join(outputDir, \"features/\", f\"radiomicfeatures_{negativeControl}_{datasetName}.csv\")\n            if not os.path.exists(ncRadFeatOutPath) or args.update:\n                ncRadiomicFeatures = radiomicFeatureExtraction(imageMetadataPath, parentDirPath, args.roi_names, args.pyradiomics_setting, outputDir, negativeControl, args.random_seed, args.parallel, args.keep_running)\n    \n    logger.info(\"Pipeline complete.\")"
      }
    ],
    "dependencies": {
      "imports": [
        "os"
      ],
      "from_imports": [
        "argparse.ArgumentParser",
        "ast.arg",
        "venv.logger",
        "readii.metadata.*",
        "readii.feature_extraction.*",
        "readii.utils.get_logger"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_chembl.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ntest_that(\"build_chembl_request constructs the correct URL\", {\n  # Set up test data\n  resource <- \"target\"\n  field <- \"target_chembl_id\"\n  filter_type <- \"exact\"\n  value <- \"CHEMBL2144069\"\n  format <- \"json\"\n\n  # Call the function\n  url <- AnnotationGx:::.build_chembl_request(resource, field, filter_type, value, format)\n\n  # Check the constructed URL\n  expected_url <-\"https://www.ebi.ac.uk/chembl/api/data/target?target_chembl_id__exact=CHEMBL2144069&format=json\"\n  expect_equal(url$url, expected_url)\n})\n\n\ntest_that(\"getChemblMechanism works\", {\n  # Set up test data\n  chembl_id <- \"CHEMBL1413\"\n\n  # Call the function\n  mechanism <- getChemblMechanism(chembl_id)\n\n  # Check the result\n  expect_data_table(mechanism)\n  expect_equal(nrow(mechanism), 2)\n  expect_equal(ncol(mechanism), 17)\n  expect_equal(mechanism$target_chembl_id, c(\"CHEMBL2363058\", \"CHEMBL2366381\"))\n\n\n  url <- getChemblMechanism(chembl_id, returnURL = T)\n  expect_list(url)\n  expect_equal(url[[1]], \"https://www.ebi.ac.uk/chembl/api/data/mechanism?molecule_chembl_id__in=CHEMBL1413&format=json\")\n})\n\n\ntest_that(\"getChemblResourceFields works\", {\n  mechanism_fields <- getChemblResourceFields(\"mechanism\")\n\n  # should be atomic vector\n  expect_character(mechanism_fields)\n  # should have 17 elements\n  expect_length(mechanism_fields, 17)\n  # should contain the expected fields\n  expect_equal(mechanism_fields, c(\n    \"action_type\", \"binding_site_comment\", \"direct_interaction\", \"disease_efficacy\",\n    \"max_phase\", \"mec_id\", \"mechanism_comment\", \"mechanism_of_action\",\n    \"mechanism_refs\", \"molecular_mechanism\", \"molecule_chembl_id\",\n    \"parent_molecule_chembl_id\", \"record_id\", \"selectivity_comment\",\n    \"site_id\", \"target_chembl_id\", \"variant_sequence\"\n  ))\n})\n\ntest_that(\"queryChemblAPI constructs the correct URL and returns parsed JSON response\", {\n  # Set up test data\n  resource <- \"mechanism\"\n  field <- \"mechanism_of_action\"\n  filter_type <- \"icontains\"\n  value <- \"Muscarinic acetylcholine receptor\"\n  format <- \"json\"\n  expected_url <- \"https://www.ebi.ac.uk/chembl/api/data/mechanism?mechanism_of_action__icontains=Muscarinic%20acetylcholine%20receptor&format=json\"\n\n  request <- AnnotationGx:::.build_chembl_request(resource, field, filter_type, value, format)\n  expect_equal(request$url, expected_url)\n\n  # Call the function\n  response <- queryChemblAPI(resource, field, filter_type, value, format)\n\n  expect_class(response, \"list\")\n\n  expect_length(response, 2)\n})\n\ntest_that(\"getChemblFilterTypes works\", {\n  result <- getChemblFilterTypes()\n\n  expect_class(result, \"character\")\n  expect_length(result, 19)\n\n  expect_true(\"in\" %in% result)\n})\n\ntest_that(\"getChemblResources works\", {\n  result <- getChemblResources()\n\n  expect_class(result, \"character\")\n  expect_length(result, 32)\n\n  expect_true(\"activity\" %in% result)\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `build_chembl_request` function in the given code snippet?",
        "answer": "The `build_chembl_request` function constructs a URL for querying the ChEMBL API. It takes parameters such as resource, field, filter_type, value, and format to create a properly formatted URL string that can be used to make API requests to the ChEMBL database."
      },
      {
        "question": "How does the `getChemblMechanism` function handle different return types?",
        "answer": "The `getChemblMechanism` function has two modes of operation based on the `returnURL` parameter. When `returnURL` is FALSE (default), it returns a data table containing mechanism information for the given ChEMBL ID. When `returnURL` is TRUE, it returns a list containing the constructed URL for the API request instead of making the actual request and processing the data."
      },
      {
        "question": "What is the purpose of the `getChemblResourceFields` function and what does it return for the 'mechanism' resource?",
        "answer": "The `getChemblResourceFields` function retrieves the available fields for a given ChEMBL resource. For the 'mechanism' resource, it returns a character vector containing 17 field names, including 'action_type', 'binding_site_comment', 'direct_interaction', 'disease_efficacy', 'max_phase', 'mec_id', 'mechanism_comment', 'mechanism_of_action', and others. This function is useful for understanding the structure and available data for different ChEMBL resources."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"getChemblMechanism works\", {\n  chembl_id <- \"CHEMBL1413\"\n  mechanism <- getChemblMechanism(chembl_id)\n  expect_data_table(mechanism)\n  expect_equal(nrow(mechanism), 2)\n  expect_equal(ncol(mechanism), 17)\n  expect_equal(mechanism$target_chembl_id, c(\"CHEMBL2363058\", \"CHEMBL2366381\"))\n\n  url <- getChemblMechanism(chembl_id, returnURL = T)\n  expect_list(url)\n  expect_equal(url[[1]], \"https://www.ebi.ac.uk/chembl/api/data/mechanism?molecule_chembl_id__in=CHEMBL1413&format=json\")\n})",
        "complete": "test_that(\"getChemblMechanism works\", {\n  chembl_id <- \"CHEMBL1413\"\n  mechanism <- getChemblMechanism(chembl_id)\n  expect_data_table(mechanism)\n  expect_equal(nrow(mechanism), 2)\n  expect_equal(ncol(mechanism), 17)\n  expect_equal(mechanism$target_chembl_id, c(\"CHEMBL2363058\", \"CHEMBL2366381\"))\n\n  url <- getChemblMechanism(chembl_id, returnURL = TRUE)\n  expect_list(url)\n  expect_equal(url[[1]], \"https://www.ebi.ac.uk/chembl/api/data/mechanism?molecule_chembl_id__in=CHEMBL1413&format=json\")\n})"
      },
      {
        "partial": "test_that(\"queryChemblAPI constructs the correct URL and returns parsed JSON response\", {\n  resource <- \"mechanism\"\n  field <- \"mechanism_of_action\"\n  filter_type <- \"icontains\"\n  value <- \"Muscarinic acetylcholine receptor\"\n  format <- \"json\"\n  expected_url <- \"https://www.ebi.ac.uk/chembl/api/data/mechanism?mechanism_of_action__icontains=Muscarinic%20acetylcholine%20receptor&format=json\"\n\n  request <- AnnotationGx:::.build_chembl_request(resource, field, filter_type, value, format)\n  expect_equal(request$url, expected_url)\n\n  response <- queryChemblAPI(resource, field, filter_type, value, format)\n\n  expect_class(response, \"list\")\n  expect_length(response, 2)\n})",
        "complete": "test_that(\"queryChemblAPI constructs the correct URL and returns parsed JSON response\", {\n  resource <- \"mechanism\"\n  field <- \"mechanism_of_action\"\n  filter_type <- \"icontains\"\n  value <- \"Muscarinic acetylcholine receptor\"\n  format <- \"json\"\n  expected_url <- \"https://www.ebi.ac.uk/chembl/api/data/mechanism?mechanism_of_action__icontains=Muscarinic%20acetylcholine%20receptor&format=json\"\n\n  request <- AnnotationGx:::.build_chembl_request(resource, field, filter_type, value, format)\n  expect_equal(request$url, expected_url)\n\n  response <- queryChemblAPI(resource, field, filter_type, value, format)\n\n  expect_list(response)\n  expect_length(response, 2)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/cellosaurus_annotations.R",
    "language": "R",
    "content": "#' Annotate Cell Accession\n#'\n#' This function takes a Cellosaurus accession and returns annotations for the cell line.\n#'\n#' @param accessions The Cellosaurus accession to annotate.\n#' @param to A character vector specifying the types of annotations to retrieve. Possible values include \"id\", \"ac\", \"hi\", \"sy\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\", and \"dt\".\n#' @param query_only A logical value indicating whether to only return the query string.\n#' @param raw A logical value indicating whether to return the raw response.\n#' \n#' @return A data frame containing the annotations for the cell line.\n#'\n#' @examples\n#' annotateCellAccession(\"CVCL_0031\")\n#' annotateCellAccession(\"CVCL_0031\", to = c(\"id\", \"ac\", \"hi\", \"sy\"))\n#'\n#' @export\nannotateCellAccession <- function(\n    accessions,\n    to = c(\"id\", \"ac\", \"hi\", \"sy\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\", \"dt\"),\n    query_only = FALSE, raw = FALSE\n    )\n{\n    funContext <- .funContext(\"annotateCellAccession\")\n\n    .info(funContext, \"Building Cellosaurus requests...\")\n    requests <- parallel::mclapply(accessions, function(accession) {\n        .build_cellosaurus_request(\n            query = accession,\n            to = to,\n            numResults = 1,\n            apiResource = \"search/cell-line\",\n            output = \"TXT\",\n            sort = NULL,\n            query_only = FALSE\n        )\n    })\n    \n    .info(funContext, \"Performing Requests...\")\n    responses <- .perform_request_parallel(requests, progress = \"Querying Cellosaurus...\")\n    names(responses) <- accessions\n    if(raw) return(responses)\n\n    .info(funContext, \"Parsing Responses...\")\n    responses_dt <- parallel::mclapply(accessions, function(name) {\n        resp <- responses[[name]]\n        .parse_cellosaurus_lines(resp) |> \n            unlist(recursive = FALSE) |> \n            .processEntry() |>\n            .formatSynonyms()\n        }\n    )\n    names(responses_dt) <- accessions\n\n    \n    responses_dt <- data.table::rbindlist(responses_dt, fill = TRUE)\n    \n    return(responses_dt)\n}\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `annotateCellAccession` function and what are its main parameters?",
        "answer": "The `annotateCellAccession` function is designed to retrieve annotations for cell lines based on their Cellosaurus accession numbers. Its main parameters are:\n1. `accessions`: The Cellosaurus accession(s) to annotate.\n2. `to`: A character vector specifying the types of annotations to retrieve (e.g., 'id', 'ac', 'hi', 'sy', etc.).\n3. `query_only`: A logical value indicating whether to only return the query string.\n4. `raw`: A logical value indicating whether to return the raw response."
      },
      {
        "question": "How does the function handle multiple accessions and what parallel processing technique is used?",
        "answer": "The function can handle multiple accessions by using parallel processing. It uses the `parallel::mclapply` function to build Cellosaurus requests and parse responses for each accession in parallel. This approach allows for efficient processing of multiple accessions simultaneously, potentially speeding up the annotation process for large datasets."
      },
      {
        "question": "What is the return value of the `annotateCellAccession` function and how is it formatted?",
        "answer": "The `annotateCellAccession` function returns a data frame containing the annotations for the cell line(s). The function processes the responses, parses the Cellosaurus lines, and formats the synonyms. The final result is created by binding the list of processed responses into a single data table using `data.table::rbindlist(responses_dt, fill = TRUE)`. This ensures that all columns from different accessions are included, even if some accessions don't have all the annotation types."
      }
    ],
    "completion_tasks": [
      {
        "partial": "annotateCellAccession <- function(accessions, to = c(\"id\", \"ac\", \"hi\", \"sy\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\", \"dt\"), query_only = FALSE, raw = FALSE) {\n    funContext <- .funContext(\"annotateCellAccession\")\n\n    .info(funContext, \"Building Cellosaurus requests...\")\n    requests <- parallel::mclapply(accessions, function(accession) {\n        .build_cellosaurus_request(\n            query = accession,\n            to = to,\n            numResults = 1,\n            apiResource = \"search/cell-line\",\n            output = \"TXT\",\n            sort = NULL,\n            query_only = FALSE\n        )\n    })\n    \n    .info(funContext, \"Performing Requests...\")\n    responses <- .perform_request_parallel(requests, progress = \"Querying Cellosaurus...\")\n    names(responses) <- accessions\n    if(raw) return(responses)\n\n    # Complete the function here\n}",
        "complete": "annotateCellAccession <- function(accessions, to = c(\"id\", \"ac\", \"hi\", \"sy\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\", \"dt\"), query_only = FALSE, raw = FALSE) {\n    funContext <- .funContext(\"annotateCellAccession\")\n\n    .info(funContext, \"Building Cellosaurus requests...\")\n    requests <- parallel::mclapply(accessions, function(accession) {\n        .build_cellosaurus_request(\n            query = accession,\n            to = to,\n            numResults = 1,\n            apiResource = \"search/cell-line\",\n            output = \"TXT\",\n            sort = NULL,\n            query_only = FALSE\n        )\n    })\n    \n    .info(funContext, \"Performing Requests...\")\n    responses <- .perform_request_parallel(requests, progress = \"Querying Cellosaurus...\")\n    names(responses) <- accessions\n    if(raw) return(responses)\n\n    .info(funContext, \"Parsing Responses...\")\n    responses_dt <- parallel::mclapply(accessions, function(name) {\n        resp <- responses[[name]]\n        .parse_cellosaurus_lines(resp) |> \n            unlist(recursive = FALSE) |> \n            .processEntry() |>\n            .formatSynonyms()\n    })\n    names(responses_dt) <- accessions\n\n    responses_dt <- data.table::rbindlist(responses_dt, fill = TRUE)\n    \n    return(responses_dt)\n}"
      },
      {
        "partial": "annotateCellAccession <- function(accessions, to = c(\"id\", \"ac\", \"hi\", \"sy\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\", \"dt\"), query_only = FALSE, raw = FALSE) {\n    funContext <- .funContext(\"annotateCellAccession\")\n\n    .info(funContext, \"Building Cellosaurus requests...\")\n    requests <- parallel::mclapply(accessions, function(accession) {\n        # Complete the .build_cellosaurus_request function call here\n    })\n    \n    # Complete the rest of the function here\n}",
        "complete": "annotateCellAccession <- function(accessions, to = c(\"id\", \"ac\", \"hi\", \"sy\", \"ca\", \"sx\", \"ag\", \"di\", \"derived-from-site\", \"misspelling\", \"dt\"), query_only = FALSE, raw = FALSE) {\n    funContext <- .funContext(\"annotateCellAccession\")\n\n    .info(funContext, \"Building Cellosaurus requests...\")\n    requests <- parallel::mclapply(accessions, function(accession) {\n        .build_cellosaurus_request(\n            query = accession,\n            to = to,\n            numResults = 1,\n            apiResource = \"search/cell-line\",\n            output = \"TXT\",\n            sort = NULL,\n            query_only = FALSE\n        )\n    })\n    \n    .info(funContext, \"Performing Requests...\")\n    responses <- .perform_request_parallel(requests, progress = \"Querying Cellosaurus...\")\n    names(responses) <- accessions\n    if(raw) return(responses)\n\n    .info(funContext, \"Parsing Responses...\")\n    responses_dt <- parallel::mclapply(accessions, function(name) {\n        resp <- responses[[name]]\n        .parse_cellosaurus_lines(resp) |> \n            unlist(recursive = FALSE) |> \n            .processEntry() |>\n            .formatSynonyms()\n    })\n    names(responses_dt) <- accessions\n\n    responses_dt <- data.table::rbindlist(responses_dt, fill = TRUE)\n    \n    return(responses_dt)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_drugSensitivitySig.R",
    "language": "R",
    "content": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugSensitivitySig.\")\n\ntest_that(\"Sensitivity result did not change since last time\", {\n\tdata(GDSCsmall)\n\n drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, features = fNames(GDSCsmall, \"rna\")[seq_len(50)])\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivityGDSCSmall.rds\")\n\n\t### TODO:: Determine why this causes 'Fatal error: length > 1 in coercion to logical' when run on Appveyor\n\t# Added verbose = FALSE argument to correct printing issues\n\t#drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, features = fNames(GDSCsmall, \"rna\")[seq_len(50)], sensitivity.cutoff = 0.2, sensitivity.measure=\"auc_recomputed\", verbose = FALSE)\n\t#expect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivity.discreteGDSCSmall.rds\", tolerance = 0.2)\n\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, drugs=treatmentNames(GDSCsmall)[1:2], features = fNames(GDSCsmall, \"rna\")[seq_len(10)], sensitivity.measure=c(\"auc_recomputed\",\"auc_published\"))\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivity.MANOVAGDSCSmall.rds\")\n\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drugSensitivitySig` function in this code snippet, and what are its key parameters?",
        "answer": "The `drugSensitivitySig` function is used to compute drug sensitivity signatures. Its key parameters include:\n- `GDSCsmall`: The dataset being analyzed\n- `mDataType`: Set to 'rna' for RNA data\n- `nthread`: Number of threads for parallel processing\n- `features`: Subset of features to analyze\n- `sensitivity.measure`: Measures of drug sensitivity (e.g., 'auc_recomputed', 'auc_published')\n- `drugs`: Specific drugs to analyze\nThe function returns a drug sensitivity object, which is then compared to reference data for testing purposes."
      },
      {
        "question": "How does this code snippet handle potential changes in the drug sensitivity results over time?",
        "answer": "The code uses the `expect_equal_to_reference` function to compare the current results of `drugSensitivitySig` to previously saved reference data. This approach allows for detecting changes in the output over time, which could be due to updates in the underlying data or changes in the algorithm. If the current results differ from the reference data, the test will fail, alerting developers to investigate the cause of the change."
      },
      {
        "question": "What is the significance of the commented-out section in the code, and what issue does it address?",
        "answer": "The commented-out section represents a test case that was causing a 'Fatal error: length > 1 in coercion to logical' when run on Appveyor. The comment indicates that adding a `verbose = FALSE` argument to the `drugSensitivitySig` function call might correct printing issues. This highlights the importance of platform-specific testing and the need to handle verbose output carefully in different environments. The comment serves as a TODO reminder for developers to investigate and resolve this issue in future updates."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"Sensitivity result did not change since last time\", {\n\tdata(GDSCsmall)\n\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, features = fNames(GDSCsmall, \"rna\")[seq_len(50)])\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivityGDSCSmall.rds\")\n\n\t# Complete the code to test drugSensitivitySig with multiple sensitivity measures\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, drugs=treatmentNames(GDSCsmall)[1:2], features = fNames(GDSCsmall, \"rna\")[seq_len(10)], sensitivity.measure=c(\"auc_recomputed\",\"auc_published\"))\n\t# Add the expect_equal_to_reference statement here\n})",
        "complete": "test_that(\"Sensitivity result did not change since last time\", {\n\tdata(GDSCsmall)\n\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, features = fNames(GDSCsmall, \"rna\")[seq_len(50)])\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivityGDSCSmall.rds\")\n\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, drugs=treatmentNames(GDSCsmall)[1:2], features = fNames(GDSCsmall, \"rna\")[seq_len(10)], sensitivity.measure=c(\"auc_recomputed\",\"auc_published\"))\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivity.MANOVAGDSCSmall.rds\")\n})"
      },
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugSensitivitySig.\")\n\ntest_that(\"Sensitivity result did not change since last time\", {\n\tdata(GDSCsmall)\n\n\t# Complete the code to test drugSensitivitySig with specific parameters\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, features = fNames(GDSCsmall, \"rna\")[seq_len(50)])\n\t# Add the expect_equal_to_reference statement here\n\n\t# Add another test for drugSensitivitySig with different parameters\n})",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugSensitivitySig.\")\n\ntest_that(\"Sensitivity result did not change since last time\", {\n\tdata(GDSCsmall)\n\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, features = fNames(GDSCsmall, \"rna\")[seq_len(50)])\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivityGDSCSmall.rds\")\n\n\tdrug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", nthread=1, drugs=treatmentNames(GDSCsmall)[1:2], features = fNames(GDSCsmall, \"rna\")[seq_len(10)], sensitivity.measure=c(\"auc_recomputed\",\"auc_published\"))\n\texpect_equal_to_reference(drug.sensitivity@.Data, \"drug.sensitivity.MANOVAGDSCSmall.rds\")\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat.R",
    "language": "R",
    "content": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(testthat)\nlibrary(PharmacoGx)\n\ntest_check(\"PharmacoGx\")\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `Sys.unsetenv(\"R_TESTS\")` line in this code snippet?",
        "answer": "The `Sys.unsetenv(\"R_TESTS\")` line is used to unset the R_TESTS environment variable. This is often done in package testing scripts to prevent certain R-specific tests from running, which might interfere with the package's own tests. It ensures a clean testing environment for the package."
      },
      {
        "question": "Which libraries are being loaded in this code snippet and what are their typical uses?",
        "answer": "The code loads two libraries: 'testthat' and 'PharmacoGx'. The 'testthat' library is a popular testing framework for R, used for unit testing. 'PharmacoGx' is likely the package being tested, which appears to be related to pharmacogenomics or pharmaceutical research based on its name."
      },
      {
        "question": "What does the `test_check(\"PharmacoGx\")` function call do in this context?",
        "answer": "The `test_check(\"PharmacoGx\")` function call runs all the tests for the 'PharmacoGx' package. It's a function provided by the 'testthat' package that automatically discovers and executes all test files in the package's tests directory. This is typically the main entry point for running a package's test suite."
      }
    ],
    "completion_tasks": [
      {
        "partial": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(testthat)\nlibrary(PharmacoGx)\n\n# Complete the code to run tests for the PharmacoGx package",
        "complete": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(testthat)\nlibrary(PharmacoGx)\n\ntest_check(\"PharmacoGx\")"
      },
      {
        "partial": "# Unset R_TESTS environment variable\n\n# Load required libraries\n\n# Run tests for PharmacoGx package",
        "complete": "Sys.unsetenv(\"R_TESTS\")\n\nlibrary(testthat)\nlibrary(PharmacoGx)\n\ntest_check(\"PharmacoGx\")"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/transforms/__init__.py",
    "language": "py",
    "content": "",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `isValidSudoku` function and how does it determine if a Sudoku board is valid?",
        "answer": "The `isValidSudoku` function checks if a given 9x9 Sudoku board is valid. It does this by verifying three conditions: 1) Each row contains unique digits from 1-9, 2) Each column contains unique digits from 1-9, and 3) Each 3x3 sub-box contains unique digits from 1-9. The function uses sets to keep track of seen digits in each row, column, and sub-box, returning false if any duplicate is found, and true if all checks pass."
      },
      {
        "question": "How does the code handle empty cells in the Sudoku board?",
        "answer": "The code handles empty cells by checking if the current cell value is not equal to '.'. In Sudoku puzzles, '.' is commonly used to represent empty cells. If a cell contains '.', it is skipped and not added to any of the sets used for checking uniqueness. This allows the function to validate partially filled Sudoku boards as well as complete ones."
      },
      {
        "question": "What is the time complexity of the `isValidSudoku` function, and why?",
        "answer": "The time complexity of the `isValidSudoku` function is O(1). This is because the input size is always fixed at 9x9 for a standard Sudoku board. The function iterates through each cell once, performing constant-time operations (set insertions and checks) for each cell. Since the board size is constant, the total number of operations is also constant, resulting in O(1) time complexity regardless of the board's content."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            # Complete the code here\n    return -1",
        "complete": "def binary_search(arr, target):\n    left, right = 0, len(arr) - 1\n    while left <= right:\n        mid = (left + right) // 2\n        if arr[mid] == target:\n            return mid\n        elif arr[mid] < target:\n            left = mid + 1\n        else:\n            right = mid - 1\n    return -1"
      },
      {
        "partial": "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    # Complete the code here",
        "complete": "def quick_sort(arr):\n    if len(arr) <= 1:\n        return arr\n    pivot = arr[len(arr) // 2]\n    left = [x for x in arr if x < pivot]\n    middle = [x for x in arr if x == pivot]\n    right = [x for x in arr if x > pivot]\n    return quick_sort(left) + middle + quick_sort(right)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/old/cell_model_passports_models.R",
    "language": "R",
    "content": "## code to prepare `cell_model_passports_models` dataset goes here\n# https://cog.sanger.ac.uk/cmp/download/model_list_20240103.csv\n\nfilePath <- system.file(\"extdata\", \"cell_model_passports_list_20240103.csv\", package = \"AnnotationGx\")\ncmp_dt <- data.table::fread(filePath)\ncols <- c(\n  \"model_id\", \"sample_id\", \"model_name\", \"cancer_type_ncit_id\",\n  \"COSMIC_ID\", \"BROAD_ID\", \"CCLE_ID\", \"RRID\"\n)\ncell_model_passports_models <- cmp_dt[, ..cols]\nnames(cell_model_passports_models) <- paste0(\"CMP.\", names(cell_model_passports_models))\n\nusethis::use_data(cell_model_passports_models, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code snippet, and how is it being used?",
        "answer": "The `system.file()` function is used to locate and retrieve the path of a file within an R package. In this code, it's used to find the file 'cell_model_passports_list_20240103.csv' in the 'extdata' directory of the 'AnnotationGx' package. This allows the code to access package-specific data files regardless of where the package is installed on the user's system."
      },
      {
        "question": "How does the code select specific columns from the `cmp_dt` data table, and what are these columns used for?",
        "answer": "The code selects specific columns from `cmp_dt` using the `..cols` syntax in data.table. The selected columns are defined in the `cols` vector, which includes identifiers like 'model_id', 'sample_id', 'model_name', etc. These columns are used to create a subset of the original data, focusing on key identifiers and metadata for cell model passports. This subset is then assigned to the `cell_model_passports_models` variable."
      },
      {
        "question": "What is the purpose of the `names()` function call at the end of the code snippet, and how does it modify the data?",
        "answer": "The `names()` function is used to rename the columns of the `cell_model_passports_models` data table. It prepends 'CMP.' to each column name using `paste0()`. This naming convention likely helps to identify these columns as coming from the Cell Model Passports dataset, which can be useful when merging or comparing this data with other datasets in future analyses."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata\", \"cell_model_passports_list_20240103.csv\", package = \"AnnotationGx\")\ncmp_dt <- data.table::fread(filePath)\ncols <- c(\n  \"model_id\", \"sample_id\", \"model_name\", \"cancer_type_ncit_id\",\n  \"COSMIC_ID\", \"BROAD_ID\", \"CCLE_ID\", \"RRID\"\n)\ncell_model_passports_models <- cmp_dt[, ..cols]\n# Complete the code to rename the columns",
        "complete": "filePath <- system.file(\"extdata\", \"cell_model_passports_list_20240103.csv\", package = \"AnnotationGx\")\ncmp_dt <- data.table::fread(filePath)\ncols <- c(\n  \"model_id\", \"sample_id\", \"model_name\", \"cancer_type_ncit_id\",\n  \"COSMIC_ID\", \"BROAD_ID\", \"CCLE_ID\", \"RRID\"\n)\ncell_model_passports_models <- cmp_dt[, ..cols]\nnames(cell_model_passports_models) <- paste0(\"CMP.\", names(cell_model_passports_models))"
      },
      {
        "partial": "# Load the required package and file\nfilePath <- system.file(\"extdata\", \"cell_model_passports_list_20240103.csv\", package = \"AnnotationGx\")\ncmp_dt <- data.table::fread(filePath)\n\n# Define columns and create dataset\ncols <- c(\n  \"model_id\", \"sample_id\", \"model_name\", \"cancer_type_ncit_id\",\n  \"COSMIC_ID\", \"BROAD_ID\", \"CCLE_ID\", \"RRID\"\n)\ncell_model_passports_models <- cmp_dt[, ..cols]\nnames(cell_model_passports_models) <- paste0(\"CMP.\", names(cell_model_passports_models))\n\n# Complete the code to save the dataset",
        "complete": "# Load the required package and file\nfilePath <- system.file(\"extdata\", \"cell_model_passports_list_20240103.csv\", package = \"AnnotationGx\")\ncmp_dt <- data.table::fread(filePath)\n\n# Define columns and create dataset\ncols <- c(\n  \"model_id\", \"sample_id\", \"model_name\", \"cancer_type_ncit_id\",\n  \"COSMIC_ID\", \"BROAD_ID\", \"CCLE_ID\", \"RRID\"\n)\ncell_model_passports_models <- cmp_dt[, ..cols]\nnames(cell_model_passports_models) <- paste0(\"CMP.\", names(cell_model_passports_models))\n\n# Save the dataset\nusethis::use_data(cell_model_passports_models, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-subsetTo.R",
    "language": "R",
    "content": "# ==== PharmacoSet Class\n\n## FIXED? TODO:: Subset function breaks if it doesnt find cell line in sensitivity info\n#' A function to subset a PharmacoSet to data containing only specified drugs, cells and genes\n#'\n#' This is the prefered method of subsetting a PharmacoSet. This function allows\n#' abstraction of the data to the level of biologically relevant objects: drugs\n#' and cells. The function will automatically go through all of the\n#' combined data in the PharmacoSet and ensure only the requested drugs\n#' and cell lines are found in any of the slots. This allows quickly picking out\n#' all the experiments for a drug or cell of interest, as well removes the need\n#' to keep track of all the metadata conventions between different datasets.\n#'\n#' @examples\n#' data(CCLEsmall)\n#' CCLEdrugs  <- treatmentNames(CCLEsmall)\n#' CCLEcells <- sampleNames(CCLEsmall)\n#' pSet <- subsetTo(CCLEsmall, drugs = CCLEdrugs[1], cells = CCLEcells[1])\n#' pSet\n#'\n#' @param object A \\code{PharmacoSet} to be subsetted\n#' @param cells A list or vector of cell names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all cells will be left in\n#'   the dataset.\n#' @param drugs A list or vector of drug names as used in the dataset to which\n#'   the object will be subsetted. If left blank, then all drugs will be left in\n#'   the dataset.\n#' @param molecular.data.cells A list or vector of cell names to keep in the\n#'   molecular data\n#' @param keep.controls If the dataset has perturbation type experiments, should\n#'   the controls be kept in the dataset? Defaults to true.\n#' @param ... Other arguments passed by other function within the package\n#'\n#' @return A PharmacoSet with only the selected drugs and cells\n#'\n#' @importMethodsFrom CoreGx subsetTo\n#' @export\nsetMethod('subsetTo', signature(object='PharmacoSet'), function(object,\n  cells=NULL, drugs=NULL, molecular.data.cells=NULL, keep.controls=TRUE, ...)\n{\n    .subsetToPharmacoSet(object, cells=cells, drugs=drugs,\n        molecular.data.cells=molecular.data.cells, keep.controls=keep.controls)\n})\n\n\n#' @importFrom CoreGx .intersectList\n#' @keywords internal\n.subsetToPharmacoSet <- function(object, cells=NULL, drugs=NULL,\n    molecular.data.cells=NULL, keep.controls=TRUE, ...)\n{\n  drop=FALSE #TODO:: Is this supposed to be here?\n\n  adArgs = list(...)\n  if ('exps' %in% names(adArgs)) {\n  \texps <- adArgs[['exps']]\n  \tif(is(exps, 'data.frame')) {\n  \t\texps2 <- exps[[name(object)]]\n  \t\tnames(exps2) <- rownames(exps)\n  \t\texps <- exps2\n  \t} else{\n  \t\texps <- exps[[name(object)]]\n  \t}\n  }else {\n    exps <- NULL\n  }\n  if(!missing(cells)){\n    cells <- unique(cells)\n  }\n\n  if(!missing(drugs)){\n    drugs <- unique(drugs)\n  }\n\n  if(!missing(molecular.data.cells)){\n    molecular.data.cells <- unique(molecular.data.cells)\n  }\n\n    ### TODO:: implement strict subsetting at this level!!!!\n    ### TODO:: refactor this monstrosity of a function into helpers\n\n    ### the function missing does not work as expected in the context below, because the arguments are passed to the anonymous\n    ### function in lapply, so it does not recognize them as missing\n\n  molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object), function(SE, cells, drugs, molecular.data.cells){\n\n    molecular.data.type <- ifelse(length(grep('rna', S4Vectors::metadata(SE)$annotation) > 0), 'rna', S4Vectors::metadata(SE)$annotation)\n    if (length(grep(molecular.data.type, names(molecular.data.cells))) > 0) {\n      cells <- molecular.data.cells[[molecular.data.type]]\n    }\n\n        column_indices <- NULL\n\n      if (length(cells)==0 && length(drugs)==0) {\n          column_indices <- seq_len(ncol(SE)) # This still returns the number of samples in an SE, but without a label\n      }\n      if(length(cells)==0 && datasetType(object)=='sensitivity') {\n        column_indices <- seq_len(ncol(SE))\n      }\n\n      cell_line_index <- NULL\n      if(length(cells)!=0) {\n        if (!all(cells %in% sampleNames(object))) {\n              stop('Some of the cell names passed to function did not match to names in the PharmacoSet. Please ensure you are using cell names as returned by the cellNames function')\n        }\n          cell_line_index <- which(SummarizedExperiment::colData(SE)[[\"sampleid\"]] %in% cells)\n        # if (length(na.omit(cell_line_index))==0){\n    #       stop('No cell lines matched')\n    #     }\n      }\n      drugs_index <- NULL\n      if(datasetType(object)=='perturbation' || datasetType(object)=='both'){\n        if(length(drugs) != 0) {\n            if (!all(drugs %in% treatmentNames(object))){\n                  stop('Some of the drug names passed to function did not match to names in the PharmacoSet. Please ensure you are using drug names as returned by the drugNames function')\n            }\n          drugs_index <- which(SummarizedExperiment::colData(SE)[[\"treatmentid\"]] %in% drugs)\n          # if (length(drugs_index)==0){\n    #         stop('No drugs matched')\n    #       }\n          if(keep.controls) {\n            control_indices <- which(SummarizedExperiment::colData(SE)[['xptype']]=='control')\n            drugs_index <- c(drugs_index, control_indices)\n          }\n        }\n      }\n\n      if(length(drugs_index) != 0 && length(cell_line_index) != 0) {\n        if(length(intersect(drugs_index, cell_line_index)) == 0) {\n          stop('This Drug - Cell Line combination was not tested together.')\n        }\n        column_indices <- intersect(drugs_index, cell_line_index)\n      } else {\n        if(length(drugs_index) !=0) {\n        column_indices <- drugs_index\n      }\n        if(length(cell_line_index) !=0) {\n        column_indices <- cell_line_index\n      }\n      }\n\n      row_indices <- seq_len(nrow(SummarizedExperiment::assay(SE, 1)))\n\n      SE <- SE[row_indices, column_indices]\n      return(SE)\n\n  }, cells=cells, drugs=drugs, molecular.data.cells=molecular.data.cells)\n\n  if ((datasetType(object) == 'sensitivity' | datasetType(object) == 'both') & length(exps) != 0) {\n      sensitivityInfo(object) <- sensitivityInfo(object)[exps, , drop=drop]\n      rownames(sensitivityInfo(object)) <- names(exps)\n      if(length(sensitivityRaw(object)) > 0) {\n        sensitivityRaw(object) <- sensitivityRaw(object)[exps, , , drop=drop]\n        dimnames(sensitivityRaw(object))[[1]] <- names(exps)\n      }\n      sensitivityProfiles(object) <- sensitivityProfiles(object)[exps, , drop=drop]\n      rownames(sensitivityProfiles(object)) <- names(exps)\n\n      sensNumber(object) <- .summarizeSensitivityNumbers(object)\n  }\n  else if ((datasetType(object) == 'sensitivity' | datasetType(object) == 'both') & (length(drugs) != 0 | length(cells) != 0)) {\n\n        drugs_index <- which(sensitivityInfo(object)[, \"treatmentid\"] %in% drugs)\n        cell_line_index <- which(sensitivityInfo(object)[,\"sampleid\"] %in% cells)\n        if (length(drugs_index) !=0 & length(cell_line_index) !=0 ) {\n          if (length(intersect(drugs_index, cell_line_index)) == 0) {\n            stop('This Drug - Cell Line combination was not tested together.')\n          }\n          row_indices <- intersect(drugs_index, cell_line_index)\n        } else {\n          if(length(drugs_index)!=0 & length(cells)==0) {\n                row_indices <- drugs_index\n          } else {\n              if(length(cell_line_index)!=0 & length(drugs)==0){\n                  row_indices <- cell_line_index\n              } else {\n              row_indices <- vector()\n              }\n          }\n       }\n        treatmentResponse(object)[names(treatmentResponse(object))[names(treatmentResponse(object))!='n']] <- lapply(treatmentResponse(object)[names(treatmentResponse(object))[names(treatmentResponse(object))!='n']], function(x,i, drop){\n            #browser()\n          if (length(dim(x))==2){\n            return(x[i,,drop=drop])\n          }\n          if (length(dim(x))==3){\n            return(x[i,,,drop=drop])\n          }\n          }, i=row_indices, drop=drop)\n  }\n\n\tif (length(drugs)==0) {\n\t\tif(datasetType(object) == 'sensitivity' | datasetType(object) == 'both'){\n\t\t\tdrugs <- unique(sensitivityInfo(object)[[\"treatmentid\"]])\n\t\t}\n\t\tif(datasetType(object) == 'perturbation' | datasetType(object) == 'both'){\n\t\t\tdrugs <- union(drugs, na.omit(CoreGx::.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(colData(SE)[[\"treatmentid\"]])}))))\n\t\t}\n\t}\n\tif (length(cells)==0) {\n\t\tcells <- union(cells, na.omit(CoreGx::.unionList(lapply(molecularProfilesSlot(object), function(SE){unique(colData(SE)[[\"sampleid\"]])}))))\n        if (datasetType(object) =='sensitivity' | datasetType(object) == 'both'){\n            cells <- union(cells, sensitivityInfo(object)[[\"sampleid\"]])\n        }\n\t}\n\ttreatmentInfo(object) <- treatmentInfo(object)[drugs, , drop=drop]\n\tsampleInfo(object) <- sampleInfo(object)[cells, , drop=drop]\n\tcuration(object)$treatment <- curation(object)$treatment[drugs, , drop=drop]\n\tcuration(object)$sample <- curation(object)$sample[cells, , drop=drop]\n\tcuration(object)$tissue <- curation(object)$tissue[cells, , drop=drop]\n\tif (datasetType(object) == 'sensitivity' | datasetType(object) == 'both'  & length(exps) == 0) {\n\t  sensNumber(object) <- sensNumber(object)[cells, drugs, drop=drop]\n\t}\n\tif (datasetType(object) == 'perturbation' | datasetType(object) == 'both') {\n\t  pertNumber(object) <- pertNumber(object)[cells, drugs, , drop=drop]\n    }\n  return(object)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'subsetTo' method for the PharmacoSet class?",
        "answer": "The 'subsetTo' method is used to subset a PharmacoSet object to data containing only specified drugs, cells, and genes. It allows for abstraction of data to biologically relevant objects (drugs and cells) and ensures that only the requested drugs and cell lines are found in all slots of the PharmacoSet. This method is useful for quickly selecting experiments for specific drugs or cells of interest and eliminates the need to track metadata conventions between different datasets."
      },
      {
        "question": "How does the function handle the case when both 'cells' and 'drugs' parameters are provided but their combination doesn't exist in the dataset?",
        "answer": "If both 'cells' and 'drugs' parameters are provided, the function checks if their combination exists in the dataset. If the intersection of the drug and cell line indices is empty, indicating that the specified drug-cell line combination was not tested together, the function will throw an error with the message 'This Drug - Cell Line combination was not tested together.'"
      },
      {
        "question": "What happens to the control samples in perturbation experiments when subsetting the PharmacoSet?",
        "answer": "The function includes a 'keep.controls' parameter, which defaults to TRUE. When subsetting perturbation experiments, if 'keep.controls' is set to TRUE, the control samples are retained in the dataset even if they don't match the specified drugs. This is done by identifying the control samples using the 'xptype' column in the colData of the SummarizedExperiment object and including their indices in the subset."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod('subsetTo', signature(object='PharmacoSet'), function(object,\n  cells=NULL, drugs=NULL, molecular.data.cells=NULL, keep.controls=TRUE, ...)\n{\n    # Complete the function body\n})",
        "complete": "setMethod('subsetTo', signature(object='PharmacoSet'), function(object,\n  cells=NULL, drugs=NULL, molecular.data.cells=NULL, keep.controls=TRUE, ...)\n{\n    .subsetToPharmacoSet(object, cells=cells, drugs=drugs,\n        molecular.data.cells=molecular.data.cells, keep.controls=keep.controls)\n})"
      },
      {
        "partial": ".subsetToPharmacoSet <- function(object, cells=NULL, drugs=NULL,\n    molecular.data.cells=NULL, keep.controls=TRUE, ...)\n{\n    # Complete the function body\n}",
        "complete": ".subsetToPharmacoSet <- function(object, cells=NULL, drugs=NULL,\n    molecular.data.cells=NULL, keep.controls=TRUE, ...)\n{\n  drop=FALSE\n  adArgs = list(...)\n  exps <- if ('exps' %in% names(adArgs)) {\n    if(is(adArgs[['exps']], 'data.frame')) {\n      exps2 <- adArgs[['exps']][[name(object)]]\n      names(exps2) <- rownames(adArgs[['exps']])\n      exps2\n    } else {\n      adArgs[['exps']][[name(object)]]\n    }\n  } else NULL\n  \n  cells <- if(!missing(cells)) unique(cells) else cells\n  drugs <- if(!missing(drugs)) unique(drugs) else drugs\n  molecular.data.cells <- if(!missing(molecular.data.cells)) unique(molecular.data.cells) else molecular.data.cells\n\n  molecularProfilesSlot(object) <- lapply(molecularProfilesSlot(object), function(SE, cells, drugs, molecular.data.cells){\n    molecular.data.type <- ifelse(length(grep('rna', S4Vectors::metadata(SE)$annotation) > 0), 'rna', S4Vectors::metadata(SE)$annotation)\n    if (length(grep(molecular.data.type, names(molecular.data.cells))) > 0) {\n      cells <- molecular.data.cells[[molecular.data.type]]\n    }\n    column_indices <- if (length(cells)==0 && length(drugs)==0) seq_len(ncol(SE))\n                      else if(length(cells)==0 && datasetType(object)=='sensitivity') seq_len(ncol(SE))\n                      else NULL\n    cell_line_index <- if(length(cells)!=0) {\n      if (!all(cells %in% sampleNames(object))) {\n        stop('Some cell names do not match names in the PharmacoSet')\n      }\n      which(SummarizedExperiment::colData(SE)[['sampleid']] %in% cells)\n    } else NULL\n    drugs_index <- if(datasetType(object) %in% c('perturbation', 'both') && length(drugs) != 0) {\n      if (!all(drugs %in% treatmentNames(object))){\n        stop('Some drug names do not match names in the PharmacoSet')\n      }\n      idx <- which(SummarizedExperiment::colData(SE)[['treatmentid']] %in% drugs)\n      if(keep.controls) c(idx, which(SummarizedExperiment::colData(SE)[['xptype']]=='control')) else idx\n    } else NULL\n    column_indices <- if(length(drugs_index) != 0 && length(cell_line_index) != 0) {\n      if(length(intersect(drugs_index, cell_line_index)) == 0) stop('This Drug - Cell Line combination was not tested together.')\n      intersect(drugs_index, cell_line_index)\n    } else if(length(drugs_index) !=0) drugs_index\n           else if(length(cell_line_index) !=0) cell_line_index\n           else column_indices\n    SE[seq_len(nrow(SummarizedExperiment::assay(SE, 1))), column_indices]\n  }, cells=cells, drugs=drugs, molecular.data.cells=molecular.data.cells)\n\n  # Additional logic for sensitivity data and other subsetting operations\n  # ...\n\n  return(object)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/class-SignatureClass.R",
    "language": "R",
    "content": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.PharmacoSig <- setClass('PharmacoSig', slots=list(\n            Arguments = \"list\",\n            PSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\n\n#' Contructor for the PharmacoSig S4 class\n#'\n#' @param Data  of data to build the signature from\n#' @param PSetName `character` vector containing name of PSet, defaults to ''\n#' @param DateCreated `date` date the signature was created, defaults to `date()`\n#' @param SigType `character` vector specifying whether the signature is sensitivity or perturbation, defaults to 'sensitivity'\n#' @param SessionInfo `sessionInfo` object as retuned by `sesssionInfo()` function, defaults to `sessionInfo()`\n#' @param Call `character` or `call` specifying the constructor call used to make the object, defaults to 'No Call Recorded'\n#' @param Arguments `list` a list of additional arguments to the constructure\n#'\n#' @return A `PharmacoSig` object build from the provided signature data\n#'\n#' @examples\n#' PharmacoSig()\n#'\n#' @export\nPharmacoSig <- function(Data=array(NA, dim=c(0,0,0)), PSetName='', DateCreated=date(), SigType='sensitivity',\n                        SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n    return(.PharmacoSig(Data, Arguments = Arguments, PSetName=PSetName, DateCreated=DateCreated, SigType=SigType,\n                        SessionInfo=SessionInfo, Call=Call))\n}\n\n\n#' Show PharmacoGx Signatures\n#'\n#' @examples\n#' data(GDSCsmall)\n#' drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(GDSCsmall, \"rna\")[1])\n#' drug.sensitivity\n#'\n#' @param object \\code{PharmacoSig}\n#' @return Prints the PharmacoGx Signatures object to the output stream, and returns invisible NULL.\n#' @export\nsetMethod(\"show\", signature=signature(object='PharmacoSig'),\n        function(object) {\n        cat('PharmacoSet Name: ', attr(object, 'PSetName'), \"\\n\")\n        cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n        cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n        cat(\"Number of Drugs: \", dim(object)[[2]], \"\\n\")\n        cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n           })\n\n\n#' Show the Annotations of a signature object\n#'\n#' This funtion prints out the information about the call used to compute the drug signatures, and the session info\n#' for the session in which the computation was done. Useful for determining the exact conditions used to generate signatures.\n#'\n#' @examples\n#' data(GDSCsmall)\n#' drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(GDSCsmall, \"rna\")[1])\n#' showSigAnnot(drug.sensitivity)\n#'\n#' @param object An object of the \\code{PharmacoSig} Class, as\n#' returned by \\code{drugPerturbationSig} or \\code{drugSensitivitySig}\n#'\n#' @return Prints the PharmacoGx Signatures annotations to the output stream, and returns invisible NULL.\n#'\n#' @importMethodsFrom CoreGx showSigAnnot\n#' @export\nsetMethod(\"showSigAnnot\", signature(object=\"PharmacoSig\"), function(object){\n  .showSigAnnotPharmacoSig(object)\n})\n\n\n#' @keywords internal\n.showSigAnnotPharmacoSig <- function(object){\n  print(object@Call)\n  print(object@SessionInfo)\n  return(invisible(NULL))\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `PharmacoSig` S4 class and its constructor function?",
        "answer": "The `PharmacoSig` S4 class is designed to store and manage pharmacological signature data. Its constructor function creates a new `PharmacoSig` object with specified attributes such as data, PSet name, creation date, signature type, session info, and call information. It provides a structured way to handle and analyze pharmacological data in R."
      },
      {
        "question": "How does the `show` method for the `PharmacoSig` class work, and what information does it display?",
        "answer": "The `show` method for the `PharmacoSig` class is implemented using `setMethod`. When called on a `PharmacoSig` object, it displays key information about the object, including the PharmacoSet name, signature type, creation date, number of drugs, and number of genes/probes. This method provides a quick summary of the object's contents without showing the entire dataset."
      },
      {
        "question": "What is the purpose of the `showSigAnnot` method for the `PharmacoSig` class, and how is it implemented?",
        "answer": "The `showSigAnnot` method is designed to display annotation information for a `PharmacoSig` object. It is implemented using `setMethod` and calls an internal function `.showSigAnnotPharmacoSig`. This method prints out the call used to compute the drug signatures and the session info in which the computation was done, providing detailed information about the exact conditions used to generate the signatures."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.PharmacoSig <- setClass('PharmacoSig', slots=list(\n            Arguments = \"list\",\n            PSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\n#' Contructor for the PharmacoSig S4 class\n#'\n#' @param Data  of data to build the signature from\n#' @param PSetName `character` vector containing name of PSet, defaults to ''\n#' @param DateCreated `date` date the signature was created, defaults to `date()`\n#' @param SigType `character` vector specifying whether the signature is sensitivity or perturbation, defaults to 'sensitivity'\n#' @param SessionInfo `sessionInfo` object as retuned by `sesssionInfo()` function, defaults to `sessionInfo()`\n#' @param Call `character` or `call` specifying the constructor call used to make the object, defaults to 'No Call Recorded'\n#' @param Arguments `list` a list of additional arguments to the constructure\n#'\n#' @return A `PharmacoSig` object build from the provided signature data\n#'\n#' @examples\n#' PharmacoSig()\n#'\n#' @export\nPharmacoSig <- function(Data=array(NA, dim=c(0,0,0)), PSetName='', DateCreated=date(), SigType='sensitivity',\n                        SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n    # Complete the function body\n}",
        "complete": "setOldClass('sessionInfo', sessionInfo)\n\n#' @importFrom utils sessionInfo\n.PharmacoSig <- setClass('PharmacoSig', slots=list(\n            Arguments = \"list\",\n            PSetName='character',\n            DateCreated = 'character',\n            SigType = 'character',\n            SessionInfo = 'sessionInfo',\n            Call = 'character'), contains='array')\n\n#' Contructor for the PharmacoSig S4 class\n#'\n#' @param Data  of data to build the signature from\n#' @param PSetName `character` vector containing name of PSet, defaults to ''\n#' @param DateCreated `date` date the signature was created, defaults to `date()`\n#' @param SigType `character` vector specifying whether the signature is sensitivity or perturbation, defaults to 'sensitivity'\n#' @param SessionInfo `sessionInfo` object as retuned by `sesssionInfo()` function, defaults to `sessionInfo()`\n#' @param Call `character` or `call` specifying the constructor call used to make the object, defaults to 'No Call Recorded'\n#' @param Arguments `list` a list of additional arguments to the constructure\n#'\n#' @return A `PharmacoSig` object build from the provided signature data\n#'\n#' @examples\n#' PharmacoSig()\n#'\n#' @export\nPharmacoSig <- function(Data=array(NA, dim=c(0,0,0)), PSetName='', DateCreated=date(), SigType='sensitivity',\n                        SessionInfo=sessionInfo(), Call='No Call Recorded', Arguments = list()){\n    return(.PharmacoSig(Data, Arguments = Arguments, PSetName=PSetName, DateCreated=DateCreated, SigType=SigType,\n                        SessionInfo=SessionInfo, Call=Call))\n}"
      },
      {
        "partial": "#' Show PharmacoGx Signatures\n#'\n#' @examples\n#' data(GDSCsmall)\n#' drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(GDSCsmall, \"rna\")[1])\n#' drug.sensitivity\n#'\n#' @param object \\code{PharmacoSig}\n#' @return Prints the PharmacoGx Signatures object to the output stream, and returns invisible NULL.\n#' @export\nsetMethod(\"show\", signature=signature(object='PharmacoSig'),\n        function(object) {\n        # Complete the function body\n        })",
        "complete": "#' Show PharmacoGx Signatures\n#'\n#' @examples\n#' data(GDSCsmall)\n#' drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\",\n#'              nthread=1, features = fNames(GDSCsmall, \"rna\")[1])\n#' drug.sensitivity\n#'\n#' @param object \\code{PharmacoSig}\n#' @return Prints the PharmacoGx Signatures object to the output stream, and returns invisible NULL.\n#' @export\nsetMethod(\"show\", signature=signature(object='PharmacoSig'),\n        function(object) {\n        cat('PharmacoSet Name: ', attr(object, 'PSetName'), \"\\n\")\n        cat('Signature Type: ', attr(object, 'SigType'), \"\\n\")\n        cat(\"Date Created: \", attr(object, 'DateCreated'), \"\\n\")\n        cat(\"Number of Drugs: \", dim(object)[[2]], \"\\n\")\n        cat(\"Number of Genes/Probes: \", dim(object)[[1]], \"\\n\")\n        })"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_subsetTo.R",
    "language": "R",
    "content": "library(PharmacoGx)\n\ncontext(\"Checking subset.\")\n\ntest_that(\"Intersection result did not change since last time\", {\ndata(CCLEsmall)\nCCLEsmaller <- subsetTo(CCLEsmall, drugs=treatmentNames(CCLEsmall), cells=sampleNames(CCLEsmall))\nexpect_equal(CCLEsmaller@annotation, CCLEsmall@annotation)\nexpect_equal(attributes(CCLEsmaller@molecularProfiles$rna), attributes(CCLEsmall@molecularProfiles$rna))\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'subsetTo' function in this code snippet, and how is it being used?",
        "answer": "The 'subsetTo' function is used to create a subset of the CCLEsmall dataset. In this case, it's creating a new object called CCLEsmaller by subsetting CCLEsmall with all its drugs (treatmentNames) and cells (sampleNames). This effectively creates a copy of the original dataset, which is then used for comparison in the subsequent expect_equal statements."
      },
      {
        "question": "What are the two main assertions being made in this test, and what do they verify?",
        "answer": "The test makes two main assertions using expect_equal:\n1. It checks if the annotation of CCLEsmaller is equal to the annotation of CCLEsmall.\n2. It verifies if the attributes of the 'rna' molecular profiles in CCLEsmaller are equal to those in CCLEsmall.\nThese assertions ensure that the subsetting operation did not alter the annotation data or the structure of the RNA molecular profiles."
      },
      {
        "question": "How does this code utilize the testthat framework, and what is the significance of the 'context' function?",
        "answer": "This code uses the testthat framework for unit testing in R. The 'context' function sets a descriptive label for a group of related tests, in this case 'Checking subset.' The 'test_that' function defines a specific test case with a descriptive name. Inside test_that, multiple expectations (assertions) can be made using functions like expect_equal. This structure helps organize tests and provides clear output when tests fail, making it easier to identify and fix issues in the code being tested."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking subset.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n  data(CCLEsmall)\n  CCLEsmaller <- subsetTo(CCLEsmall, drugs=treatmentNames(CCLEsmall), cells=sampleNames(CCLEsmall))\n  expect_equal(CCLEsmaller@annotation, CCLEsmall@annotation)\n  # Complete the test by adding an expectation for molecularProfiles\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking subset.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n  data(CCLEsmall)\n  CCLEsmaller <- subsetTo(CCLEsmall, drugs=treatmentNames(CCLEsmall), cells=sampleNames(CCLEsmall))\n  expect_equal(CCLEsmaller@annotation, CCLEsmall@annotation)\n  expect_equal(attributes(CCLEsmaller@molecularProfiles$rna), attributes(CCLEsmall@molecularProfiles$rna))\n})"
      },
      {
        "partial": "library(PharmacoGx)\n\ncontext(\"Checking subset.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n  # Load data and create subset\n  # Add expectations to compare CCLEsmaller and CCLEsmall\n})",
        "complete": "library(PharmacoGx)\n\ncontext(\"Checking subset.\")\n\ntest_that(\"Intersection result did not change since last time\", {\n  data(CCLEsmall)\n  CCLEsmaller <- subsetTo(CCLEsmall, drugs=treatmentNames(CCLEsmall), cells=sampleNames(CCLEsmall))\n  expect_equal(CCLEsmaller@annotation, CCLEsmall@annotation)\n  expect_equal(attributes(CCLEsmaller@molecularProfiles$rna), attributes(CCLEsmall@molecularProfiles$rna))\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/callingWaterfall.R",
    "language": "R",
    "content": "#' Drug sensitivity calling using waterfall plots\n#'\n#' @inherit CoreGx::callingWaterfall\n#' @inheritParams CoreGx::callingWaterfall\n#'\n#'\ncallingWaterfall <- function(x, type=c(\"IC50\", \"AUC\", \"AMAX\"),\n    intermediate.fold=c(4, 1.2, 1.2), cor.min.linear=0.95, name=\"Drug\",\n    plot=FALSE) {\n  CoreGx::callingWaterfall(x, type, intermediate.fold, cor.min.linear, name,\n                           plot)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `callingWaterfall` function in this code snippet?",
        "answer": "The `callingWaterfall` function is a wrapper for the `CoreGx::callingWaterfall` function. It's designed to perform drug sensitivity calling using waterfall plots. The function inherits documentation and parameters from the CoreGx package's callingWaterfall function, allowing users to easily access this functionality within their current package or environment."
      },
      {
        "question": "How does this function handle its arguments compared to the original CoreGx::callingWaterfall function?",
        "answer": "This function acts as a pass-through for all arguments to the CoreGx::callingWaterfall function. It defines the same parameters (x, type, intermediate.fold, cor.min.linear, name, and plot) with identical default values, and then passes these arguments directly to CoreGx::callingWaterfall. This approach maintains the same interface and functionality as the original function while potentially adding it to a different package or namespace."
      },
      {
        "question": "What is the significance of the `@inherit` and `@inheritParams` roxygen2 tags in the function documentation?",
        "answer": "The `@inherit CoreGx::callingWaterfall` tag indicates that this function should inherit all documentation from the CoreGx::callingWaterfall function. The `@inheritParams CoreGx::callingWaterfall` tag specifically inherits the parameter documentation from the original function. These tags are used to avoid duplicating documentation and ensure consistency with the original CoreGx function, while allowing for any additional custom documentation to be added if needed."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Drug sensitivity calling using waterfall plots\n#'\n#' @inherit CoreGx::callingWaterfall\n#' @inheritParams CoreGx::callingWaterfall\n#'\n#'\ncallingWaterfall <- function(x, type=c(\"IC50\", \"AUC\", \"AMAX\"),\n    intermediate.fold=c(4, 1.2, 1.2), cor.min.linear=0.95, name=\"Drug\",\n    plot=FALSE) {\n  # Complete the function body\n}",
        "complete": "#' Drug sensitivity calling using waterfall plots\n#'\n#' @inherit CoreGx::callingWaterfall\n#' @inheritParams CoreGx::callingWaterfall\n#'\n#'\ncallingWaterfall <- function(x, type=c(\"IC50\", \"AUC\", \"AMAX\"),\n    intermediate.fold=c(4, 1.2, 1.2), cor.min.linear=0.95, name=\"Drug\",\n    plot=FALSE) {\n  CoreGx::callingWaterfall(x, type, intermediate.fold, cor.min.linear, name,\n                           plot)\n}"
      },
      {
        "partial": "#' Drug sensitivity calling using waterfall plots\n#'\n#' @inherit CoreGx::callingWaterfall\n#' @inheritParams CoreGx::callingWaterfall\n#'\n#'\ncallingWaterfall <- function(x, type=c(\"IC50\", \"AUC\", \"AMAX\"),\n    intermediate.fold=c(4, 1.2, 1.2), cor.min.linear=0.95, name=\"Drug\",\n    plot=FALSE) {\n  CoreGx::callingWaterfall(...)\n}",
        "complete": "#' Drug sensitivity calling using waterfall plots\n#'\n#' @inherit CoreGx::callingWaterfall\n#' @inheritParams CoreGx::callingWaterfall\n#'\n#'\ncallingWaterfall <- function(x, type=c(\"IC50\", \"AUC\", \"AMAX\"),\n    intermediate.fold=c(4, 1.2, 1.2), cor.min.linear=0.95, name=\"Drug\",\n    plot=FALSE) {\n  CoreGx::callingWaterfall(x, type, intermediate.fold, cor.min.linear, name,\n                           plot)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/tests/testthat/test_drugPerturbationSig.R",
    "language": "R",
    "content": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugPerturbationSig.\")\n\ntest_that(\"Perturbation result did not change since last time\", {\n  data(CMAPsmall)\n  drug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\t\n  expect_equal_to_reference(drug.perturbation@.Data, \"drug.perturbationSmall.rds\")\n})\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `drugPerturbationSig` function in this code snippet, and what parameters does it take?",
        "answer": "The `drugPerturbationSig` function is used to compute drug perturbation signatures. In this code, it takes three parameters: `CMAPsmall` (a dataset), `mDataType='rna'` (specifying RNA data type), and `nthread=1` (indicating single-threaded execution). It's used to analyze the effects of drugs on gene expression profiles."
      },
      {
        "question": "How does this code ensure that the drug perturbation results haven't changed since the last execution?",
        "answer": "The code uses the `expect_equal_to_reference` function to compare the current results (`drug.perturbation@.Data`) with a previously saved reference file ('drug.perturbationSmall.rds'). This is a form of regression testing to ensure consistency in results across different runs or code changes."
      },
      {
        "question": "What libraries are being used in this code snippet, and what is their purpose in the context of the test?",
        "answer": "The code uses two libraries: `PharmacoGx` and `parallel`. `PharmacoGx` is likely a pharmacogenomics package that provides the `drugPerturbationSig` function and related functionality. The `parallel` library is imported but not explicitly used in this snippet; it might be used for parallel processing in other parts of the code or in the `drugPerturbationSig` function itself."
      }
    ],
    "completion_tasks": [
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugPerturbationSig.\")\n\ntest_that(\"Perturbation result did not change since last time\", {\n  data(CMAPsmall)\n  drug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\n  # Complete the expect_equal_to_reference function call\n})",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugPerturbationSig.\")\n\ntest_that(\"Perturbation result did not change since last time\", {\n  data(CMAPsmall)\n  drug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\n  expect_equal_to_reference(drug.perturbation@.Data, \"drug.perturbationSmall.rds\")\n})"
      },
      {
        "partial": "library(PharmacoGx)\nrequire(parallel)\n\n# Complete the context and test_that functions\n\ndata(CMAPsmall)\ndrug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\nexpect_equal_to_reference(drug.perturbation@.Data, \"drug.perturbationSmall.rds\")",
        "complete": "library(PharmacoGx)\nrequire(parallel)\ncontext(\"Checking drugPerturbationSig.\")\n\ntest_that(\"Perturbation result did not change since last time\", {\n  data(CMAPsmall)\n  drug.perturbation <- drugPerturbationSig(CMAPsmall, mDataType=\"rna\", nthread=1)\n  expect_equal_to_reference(drug.perturbation@.Data, \"drug.perturbationSmall.rds\")\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/computeSynergy.R",
    "language": "R",
    "content": "# ==== Loewe Additivity\n\n#' @title Inverse function of Hill equation\n#'\n#' @description\n#' For the dose-response Hill equation of a drug defined by\n#' \\eqn{E(x) = E_{inf}+\\frac{1-E_{inf}}{1+(\\frac{x}{EC50})^(\\frac{1}{HS})}},\n#' that computes the response in viability from a dose in micromole for a drug,\n#' this function is the inverse function of the Hill curve that\n#' computes the dose required to produce a given response:\n#' \\eqn{\n#'     f^{-1}(E) = EC50 (\n#'     \\frac{1-E}{E-E_{inf}} )^{\\frac{1}{HS}}\n#'     )\n#' }\n#'\n#' @param viability `numeric` is a vector whose entries are the viability values\n#'     in the range \\[0, 1\\] if `is_pct` is `FALSE` or \\[0, 100\\] if it is\n#'     `TRUE`.\n#' @param EC50 `numeric` is a vector of relative EC50 for drug-response equation.\n#' @param HS `numeric` Hill coefficient of the drug-response equation\n#'     that represents the sigmoidity of the curve.\n#' @param E_inf `numeric` the maximum attanable effect of a drug\n#'     when it is administered with a infinitely high concentration.\n#' @param is_pct `logical` whether both the input viabiliy and `E_inf` are given\n#'     in percentage (\\[0, 100\\]) rather than decimal (\\[0, 1\\]). Default FALSE.\n#'\n#' @return `numeric` concentrations in micromoles required to produce\n#'     `viability` in the corresponding entries.\n#'\n#' @examples\n#' dose <- effectToDose(viability = 80,\n#'                      EC50 = 42,\n#'                      HS = 1,\n#'                      E_inf = 10,\n#'                      is_pct = TRUE)\n#'\n#' @importFrom checkmate assertLogical\n#' @export\neffectToDose <- function(viability, EC50, HS, E_inf, is_pct = FALSE) {\n    assertLogical(is_pct, len = 1)\n    if (is_pct) {\n        viability <- viability / 100\n        E_inf <- E_inf / 100\n    }\n    EC50 * ((1 - viability) / (viability - E_inf))^(1 / HS)\n}\n\n#' @title Loewe Additive Combination Index (CI)\n#'\n#' @description\n#' Computes the Loewe additive combination index (CI) from its definition\n#' \\eqn{\n#'     CI = \\frac{x_1}{f_1^{-1}(E)} +\n#'          \\frac{x_2}{f_2^{-1}(E)}\n#' }\n#'\n#' @param viability `numeric` is a vector whose entries are the viability values\n#'     in the range \\[0, 1\\].\n#' @param treatment1dose `numeric` a vector of concentrations for treatment 1\n#' @param HS_1 `numeric` Hill coefficient of treatment 1\n#' @param E_inf_1 `numeric` the maximum attainable effect of treatment 1.\n#' @param EC50_1 `numeric` relative EC50 of treatment 1.\n#' @param treatment2dose `numeric` a vector of concentrations for treatment 2\n#' @param HS_2 `numeric` Hill coefficient of treatment 2\n#' @param E_inf_2 `numeric` the maximum attainable effect of treatment 2.\n#' @param EC50_2 `numeric` relative EC50 of treatment 2.\n#' @param is_pct `logical` whether both the input viabiliy and E_inf are given\n#'     in percentage (\\[0, 100\\]) rather than decimal (\\[0, 1\\]). Default FALSE.\n#'\n#' @return CI under Loewe additive definition\n#'\n#' @examples\n#' \\dontrun{\n#' tre |>\n#'     endoaggregate(\n#'         assay=\"combo_viability\",\n#'         Loewe = PharmacoGx::computeLoewe(\n#'             treatment1dose = treatment1dose,\n#'             treatment2dose = treatment2dose,\n#'             HS_1 = HS_1,\n#'             HS_2 = HS_2,\n#'             E_inf_1 = E_inf_1,\n#'             E_inf_2 = E_inf_2,\n#'             EC50_1 = EC50_1,\n#'             EC50_2 = EC50_2\n#'         ),\n#'         by = assayKeys(tre, \"combo_viability\")\n#'     ) -> tre\n#' }\n#'\n#' @export\nloeweCI <- function(viability,\n                    treatment1dose, HS_1, E_inf_1, EC50_1,\n                    treatment2dose, HS_2, E_inf_2, EC50_2,\n                    is_pct = FALSE) {\n    (treatment1dose / effectToDose(\n        viability = viability,\n        EC50 = EC50_1,\n        HS = HS_1,\n        E_inf = E_inf_1,\n        is_pct = is_pct)) +\n    (treatment2dose / effectToDose(\n        viability = viability,\n        EC50 = EC50_2,\n        HS = HS_2,\n        E_inf = E_inf_2,\n        is_pct = is_pct))\n}\n\n## Objective function to mimimise for solving E_Loewe\n#' @param viability `numeric` is a vector whose entries are the viability values\n#'     in the range [0, 1].\n#' @param treatment1dose `numeric` a vector of concentrations for treatment 1\n#' @param HS_1 `numeric` Hill coefficient of treatment 1\n#' @param E_inf_1 `numeric` the maximum attainable effect of treatment 1.\n#' @param EC50_1 `numeric` relative EC50 of treatment 1.\n#' @param treatment2dose `numeric` a vector of concentrations for treatment 2\n#' @param HS_2 `numeric` Hill coefficient of treatment 2\n#' @param E_inf_2 `numeric` the maximum attainable effect of treatment 2.\n#' @param EC50_2 `numeric` relative EC50 of treatment 2.\n#'\n#' @return the distance between computed Loewe CI and 1\n#'\n#' @noRd\n.loeweLoss <- function(viability,\n                       treatment1dose, HS_1, E_inf_1, EC50_1,\n                       treatment2dose, HS_2, E_inf_2, EC50_2) {\n    abs(\n        loeweCI(viability = viability,\n                treatment1dose, HS_1, E_inf_1, EC50_1,\n                treatment2dose, HS_2, E_inf_2, EC50_2) - 1\n    )\n}\n\n#' @title Computes Loewe Null References\n#'\n#' @description\n#' Predict the response of a treatment combination under\n#' the Loewe additive null assumption.\n#'\n#' @param treatment1dose `numeric` a vector of concentrations for treatment 1\n#' @param HS_1 `numeric` Hill coefficient of treatment 1\n#' @param E_inf_1 `numeric` viability produced by the maximum attainable effect of treatment 1.\n#' @param EC50_1 `numeric` relative EC50 of treatment 1.\n#' @param treatment2dose `numeric` a vector of concentrations for treatment 2\n#' @param HS_2 `numeric` Hill coefficient of treatment 2\n#' @param E_inf_2 `numeric` viability produced by the maximum attainable effect of treatment 2.\n#' @param EC50_2 `numeric` relative EC50 of treatment 2.\n#' @param tol `numeric` Error tolerance for deviations from Loewe assumption. Loewe predictions with error higher than `tol` will be returned as `NA`. Deafult 0.1.\n#' @param lower_bound `numeric` Lowest possible value for Loewe expected viability. Default 0.\n#' @param upper_bound `numeric` Highest possible value for Loewe expected viability. Default 1.\n#' @param verbose `logical` whether to display warning messages. Default `FALSE`.\n#'\n#' @return `numeric` expected viability under Loewe additive null assumption.\n#'\n#' @export\n#'\n#' @examples\n#' \\dontrun{\n#' tre |>\n#'     endoaggregate(\n#'         assay=\"combo_viability\",\n#'         Loewe = computeLoewe(\n#'             treatment1dose=treatment1dose,\n#'             treatment2dose=treatment2dose,\n#'             HS_1=HS_1,\n#'             HS_2=HS_2,\n#'             E_inf_1=E_inf_1,\n#'             E_inf_2=E_inf_2,\n#'             EC50_1=EC50_1,\n#'             EC50_2=EC50_2\n#'         ),\n#'         by = assayKeys(tre, \"combo_viability\")\n#'     ) -> tre\n#' }\n#'\n#' @importFrom stats optimise\n#' @importFrom checkmate assertNumeric assertLogical\ncomputeLoewe <- function(treatment1dose, HS_1, E_inf_1, EC50_1,\n                         treatment2dose, HS_2, E_inf_2, EC50_2,\n                         tol = 0.1, lower_bound = 0, upper_bound = 1,\n                         verbose = FALSE) {\n\n    len <- length(treatment1dose)\n    assertNumeric(treatment1dose, len = len)\n    assertNumeric(treatment2dose, len = len)\n    assertNumeric(HS_1, len = len)\n    assertNumeric(HS_2, len = len)\n    assertNumeric(E_inf_1, len = len)\n    assertNumeric(E_inf_2, len = len)\n    assertNumeric(EC50_1, len = len)\n    assertNumeric(EC50_2, len = len)\n    assertNumeric(tol, len = 1)\n    assertNumeric(lower_bound, len = 1)\n    assertNumeric(upper_bound, len = 1)\n    assertLogical(verbose, len = 1)\n\n    ## Find viability that minimises the distance between Loewe CI and 1\n    if (verbose) {\n        loewe_guess <- optimise(\n            f = .loeweLoss,\n            lower = lower_bound,\n            upper = upper_bound,\n            treatment1dose = treatment1dose,\n            HS_1 = HS_1, E_inf_1 = E_inf_1, EC50_1 = EC50_1,\n            treatment2dose = treatment2dose,\n            HS_2 = HS_2, E_inf_2 = E_inf_2, EC50_2 = EC50_2\n        )\n    } else {\n        suppressWarnings({\n            loewe_guess <- optimise(\n                f = .loeweLoss,\n                lower = lower_bound,\n                upper = upper_bound,\n                treatment1dose = treatment1dose,\n                HS_1 = HS_1, E_inf_1 = E_inf_1, EC50_1 = EC50_1,\n                treatment2dose = treatment2dose,\n                HS_2 = HS_2, E_inf_2 = E_inf_2, EC50_2 = EC50_2\n            )\n        })\n    }\n\n    guess_err <- loewe_guess$objective\n    loewe_estimate <- loewe_guess$minimum\n\n    if (is.nan(guess_err) | guess_err > tol)\n        loewe_estimate <- NA_real_\n\n    return(loewe_estimate)\n}\n\n\n# ==== Zero Interaction Potency (ZIP)\n\n#' @title Computes ZIP Null References\n#'\n#' @description\n#' Predict the additive response of a treatment combination under\n#' the ZIP null assumption.\n#'\n#' @param treatment1dose `numeric` a vector of concentrations for treatment 1\n#' @param HS_1 `numeric` Hill coefficient of treatment 1\n#' @param EC50_1 `numeric` relative EC50 of treatment 1.\n#' @param E_inf_1 `numeric` viability produced by the maximum attainable effect of treatment 1.\n#'     Default 0 by the original paper.\n#' @param treatment2dose `numeric` a vector of concentrations for treatment 2\n#' @param HS_2 `numeric` Hill coefficient of treatment 2\n#' @param EC50_2 `numeric` relative EC50 of treatment 2.\n#' @param E_inf_2 `numeric` viability produced by maximum effect of treatment 2.\n#'     Default 0 by the original paper.\n#'\n#' @return `numeric` expected viability under ZIP null assumption.\n#'\n#' @examples\n#' (zip <- computeZIP(\n#'   treatment1dose = c(0.1, 0.01, 0.001),\n#'   treatment2dose = c(1, 0.1, 0.01),\n#'   HS_1 = rep(1, 3), HS_2 = rep(1.2, 3),\n#'   EC50_1 = rep(0.01, 3), EC50_2 = rep(0.1, 3),\n#'   E_inf_1 = rep(0, 3), E_inf_2 = rep(0.1, 3)\n#' ))\n#'\n#' @importFrom checkmate assertNumeric\n#'\n#' @export\ncomputeZIP <- function(treatment1dose, HS_1, EC50_1, E_inf_1,\n                       treatment2dose, HS_2, EC50_2, E_inf_2) {\n    len <- length(treatment1dose)\n    assertNumeric(treatment1dose, len = len)\n    assertNumeric(treatment2dose, len = len)\n    assertNumeric(HS_1, len = len)\n    assertNumeric(HS_2, len = len)\n    assertNumeric(E_inf_1, len = len)\n    assertNumeric(E_inf_2, len = len)\n    assertNumeric(EC50_1, len = len)\n    assertNumeric(EC50_2, len = len)\n\n    y_1 <- .Hill(log10(treatment1dose), c(HS_1, E_inf_1, log10(EC50_1)))\n    y_2 <- .Hill(log10(treatment2dose), c(HS_2, E_inf_2, log10(EC50_2)))\n    y_zip <- y_1 * y_2\n    return(y_zip)\n}\n\n#' @title 4-Parameter Hill Equation for Stimuli-Response Curves\n#'\n#' @description\n#' Sigmoidal function which fits well to many stimuli-response associations\n#' observed in biology and pharmacology. In the context of PharmacoGx we\n#' are using it to model treatment-response assocations in cancer cell lines.\n#'\n#' @param dose `numeric()` A vector of `log10(dose)` values (or equivalent for\n#' the stimuli being modelleled).\n#' @param HS `numeric(1)` Hill coefficient (n) which defines the slope of the\n#' dose-response curve at the mid-point. This parameter describes the degree\n#' of sigmoidicity of the Hill curve. HS = 1 corresponds to the rectangular\n#' hyperbola in dose-response space.\n#' @param EC50 `numeric(1)` The dose required to produce 50% of the\n#' theoretically maximal response in the system, `E_inf`. Should be in the same\n#' units as `dose`!\n#' @param E_inf `numeric(1)` Theoretical maximal response (minimal viability)\n#' in the system as a proportion in the range \\\\[0, 1\\\\]. Note that since we are\n#' predicting viability (percent of cells alive after treatment) instead of\n#' response, this value should be low (i.e., more cell killing).\n#' @param E_ninf `numeric(1)` Theoretical minimum response (basal response).\n#' Defaults to 1, which should be the case for most viability experiments since\n#' we expect no cell killing to occur prior to applying a treatment.\n#'\n#' @return `numeric()` Vector of predicted viabilities for the Hill curve defined\n#' by `EC50`, `E_inf`, `E_ninf` and `HS` for each supplied value of `dose`.\n#'\n#' @references\n#' Gesztelyi, R., Zsuga, J., Kemeny-Beke, A., Varga, B., Juhasz, B., &\n#' Tosaki, A. (2012). The Hill equation and the origin of quantitative\n#' pharmacology. Archive for History of Exact Sciences, 66(4), 427\u2013438.\n#' https://doi.org/10.1007/s00407-012-0098-5\n#'\n#' Motulsky, H., & Christopoulos, A. (2004). Fitting models to biological data\n#' using linear and nonlinear regression: A practical guide to curve fitting.\n#' Oxford University Press. See Chapter 41.\n#'\n#' @author\n#' Feifei Li\n#' Petr Smirnov\n#' Christopher Eeles\n#'\n#' @examples\n#' (viability <- hillCurve(\n#'   dose=c(0.1, 0.01, 0.001),\n#'   HS=1.1,\n#'   EC50=0.01,\n#'   E_ninf=1,\n#'   E_inf=0\n#' ))\n#'\n#' @export\nhillCurve <- function(dose, HS, EC50, E_inf, E_ninf) {\n    E_inf + (( E_ninf - E_inf ) / ( 1 + ( 10^dose / 10^EC50 )^(HS) ))\n}\n\n## TODO:: If it works well for fitting 2-way Hill curves, move it to CoreGx\n\n#' @title Compute Logarithm of Hyperbolic Cosine function\n#'\n#' @description\n#' A numerical stable version of `log(cosh(x))`\n#' without floating overflow or underflow.\n#' Originally implemented in `limma` by Gordon K Smyth.\n#'\n#' @param x `numeric` vector or matrix.\n#'\n#' @return `numeric` a vector or matrix with the same dimension as `x`.\n#'\n#' @references\n#' Ritchie ME, Phipson B, Wu D, Hu Y, Law CW, Shi W, Smyth GK (2015). \u201climma powers differential expression analyses for RNA-sequencing and microarray studies.\u201d Nucleic Acids Research, 43(7), e47. doi: 10.1093/nar/gkv007.\n#'\n#' @noRd\n#' @export\n.logcosh <- function(x) {\n    y <- abs(x) - log(2)\n    i <- abs(x) < 1e-4\n    y[i] <- 0.5*x[i]^2\n    i <- !i & (abs(x) < 17)\n    y[i] <- log(cosh(x[i]))\n    y\n}\n\n\n#' @title Log-cosh loss for fitting projected Hill curves\n#'\n#' @description\n#' Compute the log hyperbolic cosine (log-cosh) loss,\n#' which behaves as L2 at small values and as L1 at large values.\n#'\n#' @param par `numeric` a vector of parameters to optimise in the following order:\n#'     `c(HS_proj, E_inf_proj, EC50_proj)`\n#' @param dose_to `numeric` a vector of concentrations of the drug being added to\n#' @param viability `numeric` Observed viability of two treatments; target for fitting curve.\n#' @param E_min_proj `numeric` Projected `E_min` given by\n#'     the viability of the added treatment at a fixed dose.\n#'\n#' @return `numeric` Log-Cosh loss for fitting a 3-parameter Hill curve. See below.\n#'\n#' @noRd\n.fitProjParamsLoss <- function(par, dose_to, viability, E_min_proj) {\n    sum(\n        .logcosh(\n             hillCurve(\n                dose = dose_to,\n                E_ninf = E_min_proj,\n                HS = par[1],\n                E_inf = par[2],\n                EC50 = par[3]\n            ) - viability\n        )\n    )\n}\n\n#' @title Log-cosh loss for fitting projected Hill curves\n#'\n#' @description\n#' Compute the log hyperbolic cosine (log-cosh) loss,\n#' which behaves as L2 at small values and as L1 at large values.\n#'\n#' @param par `numeric` a vector of parameters to optimise\n#' @param x `numeric` a vector of input values to the model\n#' @param y `numeric` a vector of target values\n#' @param fn `numeric` model to fit\n#' @param ... `pairlist` Fall through arguments to `fn`.\n#'\n#' @return `numeric` scalar Log-Cosh loss for fitting a curve.\n#'\n#' @keywords interal\n#' @noRd\n.logcoshLoss <- function(par, x, y, fn, ...) {\n    sum(.logcosh(fn(par = par, x) - y))\n}\n\n#' @title Estimate the projected Hill coefficient, efficacy, and potency\n#'\n#' @description\n#' Estimate the projected shape parameter HS, efficacy `E_inf` and potency `EC50`\n#' in the new dose-response curve of a drug after adding another drug to it\n#' by fitting a 2-parameter dose-response curve.\n#'\n#' @param dose_to `numeric` a vector of concentrations of the drug being added to\n#' @param combo_viability `numeric` observed viability of two treatments; target for fitting curve.\n#' @param dose_add `numeric` a vector of concentrations of the drug added.\n#' @param EC50_add `numeric` relative EC50 of the drug added.\n#' @param HS_add `numeric` Hill coefficient of the drug added.\n#' @param E_inf_add `numeric` Efficacy of the drug added.\n#' @param residual `character` Method used to minimise residual in fitting curves.\n#'     3 methods available: `logcosh`, `normal`, `Cauchy`.\n#'     The default method is `logcosh`.\n#'     It minimises the logarithmic hyperbolic cosine loss of the residuals\n#'     and provides the fastest estimation among the three methods,\n#'     with fitting quality in between `normal` and `Cauchy`;\n#'     recommanded when fitting large-scale datasets.\n#'     The other two methods minimise residuals by\n#'     considering the truncated probability distribution (as in their names) for the residual.\n#'     `Cauchy` provides the best fitting quality but also takes the longest to run.\n#' @param show_Rsqr `logical` whether to show goodness-of-fit value in the result.\n#' @param conc_as_log `logical` indicates whether input concentrations are in log10 scale.\n#' @param loss_args `list` Additional argument to the `loss` function.\n#'   These get passed to losss via `do.call` analagously to using `...`.\n#' @param optim_only `logical(1)` Should the fall back methods when optim fails\n#'\n#' @references\n#' Motulsky, H., & Christopoulos, A. (2004). Fitting dose-response curves. In Fitting models to biological data using linear and nonlinear regression: A practical guide to curve fitting. Oxford University Press.\n#'\n#' @return `list`\n#'      * `HS_proj`: Projected Hill coefficient after adding a drug\n#'      * `E_inf_proj`: Projected efficacy after adding a drug\n#'      * `EC50_proj`: Projected potency after adding a drug\n#'      * `E_ninf_proj`: Projected baseline viability by the added drug\n#'      * `Rsqr`: if `show_Rsqr` is `TRUE`, it will include the R squared value indicating the quality of the fit in the result.\n#'\n#' @importFrom CoreGx .fitCurve2 .reformatData\n#' @importFrom checkmate assertNumeric assertLogical\n#'\n#' @export\nestimateProjParams <- function(dose_to, combo_viability, dose_add, EC50_add, HS_add,\n    E_inf_add = 0,\n    residual = c(\"logcosh\", \"normal\", \"Cauchy\"),\n    show_Rsqr = TRUE,\n    conc_as_log = FALSE,\n    optim_only = FALSE,\n    loss_args = list()\n) {\n\n    len_to <- length(dose_to)\n    assertNumeric(dose_to, len = len_to)\n    assertNumeric(combo_viability, len = len_to)\n    assertNumeric(dose_add, len = 1)\n    assertNumeric(EC50_add, len = 1)\n    assertNumeric(HS_add, len = 1)\n    assertNumeric(E_inf_add, len = 1)\n    assertLogical(show_Rsqr, len = 1)\n    assertLogical(conc_as_log, len = 1)\n    residual <- match.arg(residual)\n\n    ## viability of the drug being added as the minimum baseline response\n    if (conc_as_log) {\n        E_ninf_proj <- .Hill(dose_add, c(HS_add, E_inf_add, log10(EC50_add)))\n    } else {\n        E_ninf_proj <- .Hill(log10(dose_add), c(HS_add, E_inf_add, log10(EC50_add)))\n    }\n    formatted_data <- .reformatData(\n        x = dose_to,\n        y = combo_viability,\n        x_to_log = !conc_as_log,\n        y_to_frac = FALSE, ## subject to change\n        y_to_log = FALSE,\n        trunc = FALSE\n    )\n    log_conc <- formatted_data[[\"x\"]]\n    combo_viability <- formatted_data[[\"y\"]]\n\n    residual_fns <- list(\n        \"normal\" = CoreGx:::.normal_loss,\n        \"Cauchy\" = CoreGx:::.cauchy_loss,\n        \"logcosh\" = .logcoshLoss\n    )\n    ## c(HS, EC50, E_inf, E_ninf)\n    lower_bounds <- c(0, -6, 0)\n    upper_bounds <- c(4, 6, 1)\n    density <- c(2, 5, 10)\n    step <- 0.5 / density\n    gritty_guess <- c(\n        pmin(pmax(1, lower_bounds[1]), upper_bounds[1]),\n        pmin(\n            pmax(\n                log_conc[which.min(abs(combo_viability - 1/2))],\n                lower_bounds[2]\n            ),\n            upper_bounds[2]\n        ),\n        pmin(pmax(min(combo_viability), lower_bounds[3]), upper_bounds[3])\n    )\n\n    ## If we have zero or less degrees of freedom, fix the HS parameter to 1\n    ## This is as per recommendations in Motulsky & Christopoulos (2004)\n    insuff_df <- len_to <= 3\n    fit_curve_args <- list(\n            par = if (insuff_df) gritty_guess[-1] else gritty_guess,\n            x = log_conc,\n            y = combo_viability,\n            fn = function(x, HS, EC50, E_inf, E_ninf) {\n                hillCurve(dose=x, HS, EC50, E_inf, E_ninf)\n            },\n            loss = residual_fns[[residual]],\n            lower = if (insuff_df) lower_bounds[-1] else lower_bounds,\n            upper = if (insuff_df) upper_bounds[-1] else upper_bounds,\n            density = if(insuff_df) density[-1] else density,\n            step = if (insuff_df) step[-1] else step,\n            optim_only = optim_only,\n            loss_args = loss_args,\n            E_ninf = E_ninf_proj\n    )\n    if (insuff_df)\n        fit_curve_args <- c(fit_curve_args, HS = 1)\n\n    proj_params <- do.call(.fitCurve2, fit_curve_args)\n    if (insuff_df)\n        proj_params <- c(1, proj_params)\n\n    proj_params[2] <- 10^proj_params[2]\n\n    if (show_Rsqr) {\n        Rsqr <- attr(proj_params, \"Rsquare\")\n        return(list(\n            HS_proj = proj_params[1],\n            EC50_proj = proj_params[2],\n            E_inf_proj = proj_params[3],\n            E_ninf_proj = E_ninf_proj,\n            Rsqr = Rsqr\n        ))\n    } else {\n        return(list(\n            HS_proj = proj_params[1],\n            E_inf_proj = proj_params[2],\n            EC50_proj = proj_params[3],\n            E_ninf_proj = E_ninf_proj\n        ))\n    }\n}\n\n#' @title Two-way fitting for projected dose-response curve.\n#'\n#' @description\n#' Fit projected dose-response curves with `E_min` as the viability\n#' of the treatment being added to the other treament at a fixed dose.\n#'\n#' @examples\n#' \\dontrun{\n#' combo_profiles <- CoreGx::buildComboProfiles(tre, c(\"HS\", \"EC50\", \"E_inf\", \"viability\"))\n#' combo_twowayFit <- fitTwowayZIP(combo_profiles)\n#' }\n#'\n#' @param combo_profiles [data.table] contains three parameters of dose-response curves\n#'     for each single agent in a drug comnbination,\n#'     and the observed viability of two treatments combined.\n#'\n#' @param residual `character` Method used to minimise residual in fitting curves.\n#'     3 methods available: `c(\"logcosh\", \"normal\", \"Cauchy\")`.\n#'     The default method is `logcosh`.\n#'     It minimises the logarithmic hyperbolic cosine loss of the residuals\n#'     and provides the fastest estimation among the three methods,\n#'     with fitting quality in between `normal` and `Cauchy`;\n#'     recommanded when fitting large-scale datasets.\n#'     The other two methods minimise residuals by\n#'     considering the truncated probability distribution (as in their names) for the residual.\n#'     `Cauchy` provides the best fitting quality but also takes the longest to run.\n#' @param show_Rsqr `logical` whether to show goodness-of-fit value in the result.\n#' @param nthread `integer` Number of cores used to perform computation. Default 1.\n#' @param loss_args `list` Additional argument to the `loss` function.\n#'   These get passed to losss via `do.call` analagously to using `...`.\n#' @param optim_only `logical(1)` Should the fall back methods when optim fails\n#'\n#' @return [data.table] contains parameters of projected dose-response curves\n#'    for adding one treatment to the other.\n#'\n#' @references\n#' Yadav, B., Wennerberg, K., Aittokallio, T., & Tang, J. (2015). Searching for Drug Synergy in Complex Dose\u2013Response Landscapes Using an Interaction Potency Model. Computational and Structural Biotechnology Journal, 13, 504\u2013513. https://doi.org/10.1016/j.csbj.2015.09.001\n#'\n#' @importFrom CoreGx aggregate\n#' @importFrom checkmate assertLogical assertInt assertDataTable\n#' @import data.table\n#' @export\nfitTwowayZIP <- function(\n    combo_profiles,\n    residual = \"logcosh\",\n    show_Rsqr = TRUE,\n    nthread = 1L,\n    optim_only = TRUE,\n    loss_args = list()\n) {\n\n    assertDataTable(combo_profiles, min.rows = 1)\n    assertLogical(show_Rsqr, len = 1)\n    assertInt(nthread, lower = 1L)\n    required_cols <- c(\n        \"treatment1id\", \"treatment2id\", \"treatment1dose\", \"treatment2dose\",\n        \"sampleid\", \"combo_viability\",\n        \"HS_1\", \"HS_2\", \"E_inf_1\", \"E_inf_2\", \"EC50_1\", \"EC50_2\"\n    )\n\n    has_cols <- required_cols %in% colnames(combo_profiles)\n    if (!all(has_cols))\n        stop(\"Missing required columns of parameters: \",\n             paste(required_cols[!has_cols], sep = \", \"),\n             call. = FALSE)\n\n    combo_profiles |>\n        aggregate(\n            estimateProjParams(\n                dose_to = treatment1dose,\n                combo_viability = combo_viability,\n                dose_add = unique(treatment2dose),\n                EC50_add = unique(EC50_2),\n                HS_add = unique(HS_2),\n                E_inf_add = unique(E_inf_2),\n                residual = residual,\n                show_Rsqr = show_Rsqr,\n                optim_only = optim_only,\n                loss_args = loss_args\n            ),\n            moreArgs = list(\n                residual = residual,\n                show_Rsqr = show_Rsqr,\n                optim_only = optim_only,\n                loss_args = loss_args\n            ),\n            by = c(\"treatment1id\", \"treatment2id\", \"treatment2dose\", \"sampleid\"),\n            nthread = nthread,\n            enlist = FALSE\n        ) -> fit_2_to_1\n    combo_profiles |>\n        aggregate(\n            estimateProjParams(\n                dose_to = treatment2dose,\n                combo_viability = combo_viability,\n                dose_add = unique(treatment1dose),\n                EC50_add = unique(EC50_1),\n                HS_add = unique(HS_1),\n                E_inf_add = unique(E_inf_1),\n                residual = residual,\n                show_Rsqr = show_Rsqr,\n                optim_only = optim_only,\n                loss_args = loss_args\n            ),\n            moreArgs = list(\n                residual = residual,\n                show_Rsqr = show_Rsqr,\n                optim_only = optim_only,\n                loss_args = loss_args\n            ),\n            by = c(\"treatment1id\", \"treatment2id\", \"treatment1dose\", \"sampleid\"),\n            nthread = nthread,\n            enlist = FALSE\n        ) -> fit_1_to_2\n\n    combo_twowayFit <- combo_profiles[\n        fit_1_to_2, ,\n        on = c(\n            treatment1id = \"treatment1id\",\n            treatment2id = \"treatment2id\",\n            treatment1dose = \"treatment1dose\",\n            sampleid = \"sampleid\"\n        )\n    ]\n\n    combo_twowayFit <- merge.data.table(\n        combo_twowayFit,\n        fit_2_to_1,\n        by.x = c(\"treatment1id\", \"treatment2id\", \"treatment2dose\", \"sampleid\"),\n        by.y = c(\"treatment1id\", \"treatment2id\", \"treatment2dose\", \"sampleid\"),\n        suffixes = c(\"_1_to_2\", \"_2_to_1\")\n    )\n\n    return(combo_twowayFit)\n}\n\n## == Plot the result of two-way fittings for a drug combination experiment ===\n\n#' @title Plot projected Hill curves\n#'\n#' @description\n#' Plot the two-way projected Hill curves of adding one drug to the other.\n#'\n#' @param combo_twowayFit `data.table`\n#'      containing two-way fitted parameters for multiple drug combination experiments.\n#'\n#' @param treatment1 `character`\n#'      the `treatment1id` to select in `combo_twowayFit` for a drug combination.\n#'\n#' @param treatment2\n#'      the `treatment2id` to select in `combo_twowayFit` for a drug combination.\n#'\n#' @param cellline\n#'      the `sampleid` to select in `combo_twowayFit` for a drug combination experiment.\n#'\n#' @param add_treatment\n#'      The added treatment in projected Hill curves, either integer 1 or 2.\n#'      1 means adding treatment 1 to treatment 2.\n#'\n#' @return produce a plot with projected Hill curves of adding treatment [add_treatment]\n#'\n#' @importFrom graphics plot curve points legend\n#' @importFrom grDevices palette rainbow\n#' @export\n#' @noRd\n#' @examples\n#' \\dontrun{\n#' combo_profiles <- CoreGx::buildComboProfiles(tre, c(\"HS\", \"EC50\", \"E_inf\", \"viability\"))\n#' combo_twowayFit <- fitTwowayZIP(combo_profiles)\n#' .plotProjHill(combo_twowayFit,\n#'               treatment1 = \"Methotrexate\",\n#'               treatment2 = \"Zolendronic Acid\",\n#'               cellline = \"UO-31\",\n#'               add_treatment = 1)\n#' }\n.plotProjHill <- function(combo_twowayFit, treatment1, treatment2,\n                          cellline, add_treatment = 1, title = NULL) {\n\n    required_cols <- c(\"treatment1id\", \"treatment1dose\", \"treatment2id\", \"treatment2dose\",\n                       \"sampleid\", \"combo_viability\", \"HS_1\", \"E_inf_1\", \"EC50_1\", \"HS_2\", \"E_inf_2\",\n                       \"EC50_2\", \"HS_proj_1_to_2\", \"E_inf_proj_1_to_2\", \"EC50_proj_1_to_2\",\n                       \"E_ninf_proj_1_to_2\", \"HS_proj_2_to_1\", \"E_inf_proj_2_to_1\",\n                       \"EC50_proj_2_to_1\", \"E_ninf_proj_2_to_1\")\n    has_cols <- (required_cols %in% colnames(combo_twowayFit))\n    if (!all(has_cols))\n        stop(\"Missing required columns for plotting: \",\n             paste(required_cols[!has_cols]))\n\n    select_combo <- combo_twowayFit[treatment1id == treatment1 &\n                                    treatment2id == treatment2 &\n                                    sampleid == cellline]\n    if (dim(select_combo)[1] <= 0)\n        stop(paste(\"No such drug combination with treatment1id:\", treatment1,\n                   \"and treatment2id:\", treatment2, \"and sampleid:\", cellline))\n\n    if (length(add_treatment) > 1)\n        stop(\"Argument `add_treatment` must be of length 1.\")\n\n    if (!(add_treatment %in% c(1, 2)))\n        stop(\"Argument `add_treatment` must be either 1 or 2.\")\n\n    ## Use variable name as title if not provided\n    if (is.null(title))\n        title <- deparse(substitute(combo_twowayFit))\n\n    ## Colours for each curve of a fixed concentration of the drug added\n\n    has_Rsqr <- c(\"Rsqr_1_to_2\", \"Rsqr_2_to_1\") %in% colnames(combo_twowayFit)\n    if (add_treatment == 2) {\n        ## unique treatment 2 concentrations\n        unique_t2_dose <- unique(select_combo[, treatment2dose])\n        cols <- palette(rainbow(length(unique_t2_dose)))\n        if (has_Rsqr[2])\n            Rsqr_2_to_1 <- vector(mode = \"numeric\", length = length(unique_t2_dose))\n        ## Initialise an empty background canvas\n        plot(\n            NULL, xlim = c(-10, 10), ylim = c(0, 2),\n            ylab = paste(\"Response of adding\", treatment2, \"to\", treatment1),\n            xlab = paste0(\"log10([\", treatment1,\"])\"),\n            main = title\n        )\n        for (i in seq_along(unique_t2_dose)) {\n            dose_add <- unique_t2_dose[i]\n            EC50_proj <- unique(select_combo[treatment2dose == dose_add, EC50_proj_2_to_1])\n            HS_proj <- unique(select_combo[treatment2dose == dose_add, HS_proj_2_to_1])\n            EC50_add <- unique(select_combo[treatment2dose == dose_add, EC50_2])\n            E_inf_add <- unique(select_combo[treatment2dose == dose_add, E_inf_2])\n            E_inf_proj <- unique(select_combo[treatment2dose == dose_add, E_inf_proj_2_to_1])\n            HS_add <- unique(select_combo[treatment2dose == dose_add, HS_2])\n            dose_to <- select_combo[treatment2dose == dose_add, treatment1dose]\n            E_ninf_proj <- PharmacoGx:::.Hill(log10(dose_add), c(HS_add, E_inf_add, log10(EC50_add)))\n            if (has_Rsqr[2])\n                Rsqr_2_to_1[i] <- unique(select_combo[treatment2dose == dose_add, Rsqr_2_to_1])\n            y <- select_combo[treatment2dose == dose_add, combo_viability]\n            curve(\n                PharmacoGx::hillCurve(\n                    E_ninf = E_ninf_proj,\n                    E_inf = E_inf_proj,\n                    HS = HS_proj,\n                    EC50 = log10(EC50_proj),\n                    dose = x\n                ),\n                from = -10, to = 10, add = TRUE, col = cols[i]\n            )\n            points(x = log10(dose_to), y = y, col = cols[i])\n        }\n        if (has_Rsqr[2]) {\n            legend(-10, 2,\n                legend = paste0(\"[\", treatment2, \"] = \", unique_t2_dose,\n                                \", R square = \", round(Rsqr_2_to_1, digits = 4)),\n                col = cols,\n                lty = 1,\n                box.lty = 0\n            )\n        } else {\n            legend(-10, 2,\n                legend = paste0(\"[\", treatment2, \"] = \", unique_t2_dose),\n                col = cols,\n                lty = 1,\n                box.lty = 0\n            )\n        }\n    } else {\n        ## unique treatment 1 concentrations\n        unique_t1_dose <- unique(select_combo[, treatment1dose])\n        cols <- palette(rainbow(length(unique_t1_dose)))\n        ## TODO: Find a nicer way to extract R squared value\n        if (has_Rsqr[1])\n            Rsqr_1_to_2 <- vector(mode = \"numeric\", length = length(unique_t1_dose))\n\n        ## Initialise an empty background canvas\n        plot(\n            NULL, xlim = c(-10, 10), ylim = c(0, 2),\n            ylab = paste(\"Response of adding\", treatment1, \"to\", treatment2),\n            xlab = paste0(\"log10([\", treatment2,\"])\"),\n            main = title\n        )\n        for (i in seq_along(unique_t1_dose)) {\n            dose_add <- unique_t1_dose[i]\n            EC50_proj <- unique(select_combo[treatment1dose == dose_add, EC50_proj_1_to_2])\n            HS_proj <- unique(select_combo[treatment1dose == dose_add, HS_proj_1_to_2])\n            EC50_add <- unique(select_combo[treatment1dose == dose_add, EC50_1])\n            HS_add <- unique(select_combo[treatment1dose == dose_add, HS_1])\n            E_inf_add <- unique(select_combo[treatment1dose == dose_add, E_inf_1])\n            E_ninf_proj <- PharmacoGx:::.Hill(log10(dose_add), c(HS_add, E_inf_add, log10(EC50_add)))\n            E_inf_proj <- unique(select_combo[treatment1dose == dose_add, E_inf_proj_1_to_2])\n            dose_to <- select_combo[treatment1dose == dose_add, treatment2dose]\n            if (has_Rsqr[1])\n                Rsqr_1_to_2[i] <- unique(select_combo[treatment1dose == dose_add, Rsqr_1_to_2])\n            y <- select_combo[treatment1dose == dose_add, combo_viability]\n            curve(\n                PharmacoGx::hillCurve(\n                    E_ninf = E_ninf_proj,\n                    E_inf = E_inf_proj,\n                    HS = HS_proj,\n                    EC50 = log10(EC50_proj),\n                    dose = x\n                ),\n                from = -10, to = 10, add = TRUE, col = cols[i]\n            )\n            points(x = log10(dose_to), y = y, col = cols[i])\n        }\n        if (has_Rsqr[1]) {\n            legend(-10, 2,\n                legend = paste0(\"[\", treatment1, \"] = \", unique_t1_dose,\n                                \", R square = \", round(Rsqr_1_to_2, digits = 4)),\n                col = cols,\n                lty = 1,\n                box.lty = 0\n            )\n        } else {\n            legend(-10, 2,\n                legend = paste0(\"[\", treatment1, \"] = \", unique_t1_dose),\n                col = cols,\n                lty = 1,\n                box.lty = 0\n            )\n        }\n    }\n\n}\n\n#' Generic to compute ZIP delta scores from an S4 object\n#'\n#' @examples\n#' print(\"Generics shouldn't need examples?\")\n#'\n#' @param object `S4` An object to compute delta scores from.\n#' @param ... Allow new arguments to this generic.\n#'\n#' @return Depends on the implemented method.\n#'\n#' @exportMethod computeZIPdelta\nsetGeneric(name = \"computeZIPdelta\",\n           def = function(object, ...) standardGeneric(\"computeZIPdelta\"))\n\n#' @title Compute ZIP delta score\n#'\n#' @description\n#' Following the calculation of ZIP delta score as in Appendix A3.\n#' See reference for details.\n#'\n#' @description Compute ZIP delta score as described in the original paper.\n#'\n#' @param object [TreatmentResponseExperiment]\n#'     The `TreatmentResponseExperiment` from which to extract assays\n#'     `mono_profile` and `combo_viability` to compute ZIP delta scores.\n#' @param residual `character` Method used to minimise residual in fitting curves.\n#'     3 methods available: `c(\"logcosh\", \"normal\", \"Cauchy\")`.\n#'     The default method is `logcosh`.\n#'     It minimises the logarithmic hyperbolic cosine loss of the residuals\n#'     and provides the fastest estimation among the three methods,\n#'     with fitting quality in between `normal` and `Cauchy`;\n#'     recommanded when fitting large-scale datasets.\n#'     The other two methods minimise residuals by\n#'     considering the truncated probability distribution (as in their names) for the residual.\n#'     `Cauchy` provides the best fitting quality but also takes the longest to run.\n#' @param nthread `integer` Number of cores used to perform computation.\n#'     Default 1.\n#' @param show_Rsqr `logical` Whether to show the 2-way curve fitting quality in the result.\n#'     Default FALSE.\n#'\n#' @return [TreatmentResponseExperiment] with assay `combo_scores` containing `delta_scores`\n#'\n#' @examples\n#' \\dontrun{\n#' tre <- computeZIPdelta(tre, residual = \"Cauchy\", nthread = 2L)\n#' }\n#'\n#' @references\n#' Yadav, B., Wennerberg, K., Aittokallio, T., & Tang, J. (2015). Searching for Drug Synergy in Complex Dose\u2013Response Landscapes Using an Interaction Potency Model. Computational and Structural Biotechnology Journal, 13, 504\u2013513. https://doi.org/10.1016/j.csbj.2015.09.001\n#'\n#' @importFrom CoreGx buildComboProfiles aggregate\n#' @importFrom checkmate assertInt assertLogical\n#' @import data.table\n#' @export\n#' @docType methods\nsetMethod(\"computeZIPdelta\", signature(object = \"TreatmentResponseExperiment\"),\n        function(object, residual = \"logcosh\", nthread = 1L,\n        show_Rsqr = FALSE) {\n\n    if (!is.character(residual)) {\n        stop(\"argument `residual` must be type of logical\")\n    } else if (length(residual) != 1) {\n        stop(\"argument `residual` must be of length 1\")\n    }\n\n    assertInt(nthread, lower = 1L)\n    assertLogical(show_Rsqr, len = 1)\n\n    combo_keys <- c(\"treatment1id\", \"treatment2id\",\n                    \"treatment1dose\", \"treatment2dose\", \"sampleid\")\n    combo_profiles <- tryCatch({\n        buildComboProfiles(object, c(\"HS\", \"EC50\", \"E_inf\", \"ZIP\", \"combo_viability\"))\n    }, warning = function(w) {\n        message(paste(\"ZIP reference values have not been pre-computed.\",\n                      \"They will be computed in during delta score calculation.\"))\n        buildComboProfiles(object, c(\"HS\", \"EC50\", \"E_inf\", \"combo_viability\"))\n    })\n    required_params <- c(\"HS_1\", \"HS_2\", \"E_inf_1\", \"E_inf_2\", \"EC50_1\", \"EC50_2\")\n    missing_params <- !(required_params %in% colnames(combo_profiles))\n    if (any(missing_params))\n        stop(\"Missing required paramters for two-way Hill curve fitting: \",\n             paste(required_params[missing_params]))\n    has_ZIP <- \"ZIP\" %in% colnames(combo_profiles)\n    if (has_ZIP) {\n        combo_ZIP <- combo_profiles[, c(combo_keys, \"ZIP\"), with = FALSE]\n        combo_profiles[, ZIP := NULL]\n        setkeyv(combo_ZIP, combo_keys)\n    }\n\n    combo_twowayFit <- fitTwowayZIP(combo_profiles = combo_profiles,\n                                    residual = residual,\n                                    nthread = nthread,\n                                    show_Rsqr = show_Rsqr)\n    setkeyv(combo_twowayFit, combo_keys)\n    if (has_ZIP) {\n        combo_twowayFit <- combo_twowayFit[combo_ZIP, on = combo_keys]\n        if (show_Rsqr) {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose,\n                        ZIP = ZIP\n                    ),\n                    delta_Rsqr_1_to_2 = Rsqr_1_to_2,\n                    delta_Rsqr_2_to_1 = Rsqr_2_to_1,\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        } else {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose,\n                        ZIP = ZIP\n                    ),\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        }\n    } else {\n        if (show_Rsqr) {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose\n                    ),\n                    delta_Rsqr_1_to_2 = Rsqr_1_to_2,\n                    delta_Rsqr_2_to_1 = Rsqr_2_to_1,\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        } else {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose\n                    ),\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        }\n    }\n    setkeyv(delta_scores, combo_keys)\n    ## Add delta scores to combo_scores in input TRE\n    combo_scores <- tryCatch({\n        object$combo_scores\n    }, error = function(e) {\n        NULL\n    })\n    if (is.null(combo_scores)) {\n        ## create a new combo_score assay and save delta scores\n        object$combo_scores <- delta_scores\n    } else {\n        object$combo_scores <- combo_scores[delta_scores, , on = combo_keys]\n    }\n\n    return(object)\n})\n\n#' @title Vector-based version of [computeZIPdelta]\n#'\n#' @description\n#' Following the calculation of ZIP delta score as in Appendix A3.\n#' See reference for details.\n#'\n#' @param treatment1id `character` a vector of identifiers for treatment 1\n#' @param treatment2id `character` a vector of identifiers for treatment 2\n#' @param treatment1dose `numeric` a vector of concentrations for treatment 1\n#' @param treatment2dose `numeric` a vector of concentrations for treatment 2\n#' @param sampleid `character` Cell-line ID of a drug combination screening experiment.\n#' @param HS_1 `numeric` Hill coefficient of treatment 1\n#' @param HS_2 `numeric` Hill coefficient of treatment 2\n#' @param EC50_1 `numeric` relative EC50 of treatment 1.\n#' @param EC50_2 `numeric` relative EC50 of treatment 2.\n#' @param E_inf_1 `numeric` viability produced by the maximum attainable effect of treatment 1.\n#' @param E_inf_2 `numeric` viability produced by the maximum attainable effect of treatment 2.\n#' @param combo_viability `numeric` observed viability of the two treatments combined.\n#' @param ZIP `numeric` pre-computed ZIP reference values.\n#'     If not provided, it will be computed during delta score calculation.\n#' @param residual `character` Method used to minimise residual in fitting curves.\n#'     3 methods available: `c(\"logcosh\", \"normal\", \"Cauchy\")`.\n#'     The default method is `logcosh`.\n#'     It minimises the logarithmic hyperbolic cosine loss of the residuals\n#'     and provides the fastest estimation among the three methods,\n#'     with fitting quality in between `normal` and `Cauchy`;\n#'     recommanded when fitting large-scale datasets.\n#'     The other two methods minimise residuals by\n#'     considering the truncated probability distribution (as in their names) for the residual.\n#'     `Cauchy` provides the best fitting quality but also takes the longest to run.\n#' @param nthread `integer` Number of cores used to perform computation.\n#'     Default 1.\n#' @param show_Rsqr `logical` Whether to show the 2-way curve fitting quality in the result.\n#'     Default FALSE.\n#'\n#' @return `numeric` delta scores of every dose combinations for any given treatment combinations.\n#'\n#' @examples\n#' \\dontrun{\n#' ## ZIP is optional. Will be recomputed if not provided.\n#' combo_profiles <- CoreGx::buildComboProfiles(\n#'      tre, \n#'      c(\"HS\", \"EC50\", \"E_inf\", \"ZIP\", \"combo_viability\"))\n#' combo_profiles[,\n#'         .computeZIPdelta(\n#'             treatment1id = treatment1id,\n#'             treatment2id = treatment2id,\n#'             treatment1dose = treatment1dose,\n#'             treatment2dose = treatment2dose,\n#'             sampleid = sampleid,\n#'             HS_1 = HS_1, HS_2 = HS_2,\n#'             EC50_1 = EC50_1, EC50_2 = EC50_2,\n#'             E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n#'             combo_viability = combo_viability,\n#'             ZIP = ZIP,\n#'             nthread = 4,\n#'             show_Rsqr = TRUE\n#'         )\n#'     ] -> delta_scores\n#' }\n#'\n#' @references\n#' Yadav, B., Wennerberg, K., Aittokallio, T., & Tang, J. (2015). Searching for Drug Synergy in Complex Dose\u2013Response Landscapes Using an Interaction Potency Model. Computational and Structural Biotechnology Journal, 13, 504\u2013513. https://doi.org/10.1016/j.csbj.2015.09.001\n#'\n#' @importFrom CoreGx aggregate\n#' @importFrom checkmate assertNumeric assertInt assertLogical\n#' @import data.table\n#' @keywords internal\n#' @export\n.computeZIPdelta <- function(\n    treatment1id, treatment2id, treatment1dose, treatment2dose, sampleid,\n    HS_1, HS_2, EC50_1, EC50_2, E_inf_1, E_inf_2, combo_viability, ZIP = NULL,\n    residual = \"logcosh\", nthread = 1L, show_Rsqr = FALSE) {\n\n    assertInt(nthread, lower = 1L)\n    assertLogical(show_Rsqr, len = 1)\n    len <- length(treatment1dose)\n    assertNumeric(treatment1dose, len = len)\n    assertNumeric(treatment2dose, len = len)\n    assertNumeric(HS_1, len = len)\n    assertNumeric(HS_2, len = len)\n    assertNumeric(E_inf_1, len = len)\n    assertNumeric(E_inf_2, len = len)\n    assertNumeric(EC50_1, len = len)\n    assertNumeric(EC50_2, len = len)\n    assertNumeric(combo_viability, len = len)\n    if (!is.null(ZIP))\n        assertNumeric(ZIP, len = len)\n\n    combo_keys <- c(\"treatment1id\", \"treatment2id\",\n                    \"treatment1dose\", \"treatment2dose\", \"sampleid\")\n\n    combo_profiles <- data.table(\n        treatment1id = treatment1id,\n        treatment2id = treatment2id,\n        treatment1dose = treatment1dose,\n        treatment2dose = treatment2dose,\n        sampleid = sampleid,\n        combo_viability = combo_viability,\n        HS_1 = HS_1,\n        HS_2 = HS_2,\n        EC50_1 = EC50_1,\n        EC50_2 = EC50_2,\n        E_inf_1 = E_inf_1,\n        E_inf_2 = E_inf_2\n    )\n\n    if (!is.null(ZIP)) {\n        combo_ZIP <- data.table(\n            treatment1id = treatment1id,\n            treatment2id = treatment2id,\n            treatment1dose = treatment1dose,\n            treatment2dose = treatment2dose,\n            sampleid = sampleid,\n            ZIP = ZIP\n        )\n        setkeyv(combo_ZIP, combo_keys)\n    }\n\n    combo_twowayFit <- fitTwowayZIP(combo_profiles = combo_profiles,\n                                    residual = residual,\n                                    nthread = nthread,\n                                    show_Rsqr = show_Rsqr)\n    setkeyv(combo_twowayFit, combo_keys)\n    if (is.null(ZIP)) {\n        if (show_Rsqr) {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose\n                    ),\n                    delta_Rsqr_1_to_2 = Rsqr_1_to_2,\n                    delta_Rsqr_2_to_1 = Rsqr_2_to_1,\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        } else {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose\n                    ),\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        }\n    } else {\n        combo_twowayFit <- combo_twowayFit[combo_ZIP, on = combo_keys]\n        if (show_Rsqr) {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose,\n                        ZIP = ZIP\n                    ),\n                    delta_Rsqr_1_to_2 = Rsqr_1_to_2,\n                    delta_Rsqr_2_to_1 = Rsqr_2_to_1,\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        } else {\n            combo_twowayFit |>\n                aggregate(\n                    delta_score = .deltaScore(\n                        EC50_1_to_2 = EC50_proj_1_to_2,\n                        EC50_2_to_1 = EC50_proj_2_to_1,\n                        EC50_1 = EC50_1, EC50_2 = EC50_2,\n                        HS_1_to_2 = HS_proj_1_to_2,\n                        HS_2_to_1 = HS_proj_2_to_1,\n                        HS_1 = HS_1, HS_2 = HS_2,\n                        E_inf_1 = E_inf_1, E_inf_2 = E_inf_2,\n                        E_inf_2_to_1 = E_inf_proj_2_to_1,\n                        E_inf_1_to_2 = E_inf_proj_1_to_2,\n                        treatment1dose = treatment1dose,\n                        treatment2dose = treatment2dose,\n                        ZIP = ZIP\n                    ),\n                    by = combo_keys,\n                    nthread = nthread\n                ) -> delta_scores\n        }\n    }\n    if (show_Rsqr) {\n        return(as.list(delta_scores[,\n            c(combo_keys, \"delta_score\", \"delta_Rsqr_1_to_2\", \"delta_Rsqr_2_to_1\"),\n            with = FALSE\n        ]))\n    } else {\n        return(as.list(delta_scores[, c(combo_keys, \"delta_score\"), with = FALSE]))\n    }\n}\n\n\n#' @title Calculate ZIP delta score for a drug combination\n#'\n#' @description\n#' Following the calculation of ZIP delta score as in Appendix A3.\n#' See reference for details.\n#'\n#' @param EC50_1_to_2 `numeric` projected EC50 of treatment 2 after adding treatment 1.\n#' @param EC50_2_to_1 `numeric` projected EC50 of treatment 1 after adding treatment 2.\n#' @param EC50_1 `numeric` relative EC50 of treatment 1.\n#' @param EC50_2 `numeric` relative EC50 of treatment 2.\n#' @param HS_1_to_2 `numeric` projected Hill coefficient of treatment 2\n#'     after adding treatment 1.\n#' @param HS_2_to_1 `numeric` projected Hill coefficient of treatment 1\n#'     after adding treatment 2.\n#' @param HS_1 `numeric` Hill coefficient of treatment 1\n#' @param HS_2 `numeric` Hill coefficient of treatment 2\n#' @param E_inf_2_to_1 `numeric` projected maximum attainable effect of\n#'     adding treatment 2 to treatment 1.\n#' @param E_inf_1_to_2 `numeric` projected maximum attainable effect of\n#'     adding treatment 1 to treatment 2.\n#' @param E_inf_1 `numeric` viability produced by the maximum attainable effect of treatment 1.\n#' @param E_inf_2 `numeric` viability produced by the maximum attainable effect of treatment 2.\n#' @param treatment1dose `numeric` a vector of concentrations for treatment 1\n#' @param treatment2dose `numeric` a vector of concentrations for treatment 2\n#' @param ZIP `numeric` pre-computed ZIP reference values.\n#'     If not provided, it will be computed during delta score calculation.\n#'\n#' @return `numeric` a ZIP delta score to quantify synergy for the drug combination.\n#'\n#' @noRd\n#' @references\n#' Yadav, B., Wennerberg, K., Aittokallio, T., & Tang, J. (2015). Searching for Drug Synergy in Complex Dose\u2013Response Landscapes Using an Interaction Potency Model. Computational and Structural Biotechnology Journal, 13, 504\u2013513. https://doi.org/10.1016/j.csbj.2015.09.001\n#' @export\n.deltaScore <- function(EC50_1_to_2, EC50_2_to_1, EC50_1, EC50_2,\n                        HS_1_to_2, HS_2_to_1, HS_1, HS_2,\n                        E_inf_2_to_1, E_inf_1_to_2, E_inf_1, E_inf_2,\n                        treatment1dose, treatment2dose, ZIP = NULL) {\n\n    viability_1 <- .Hill(log10(treatment1dose), c(HS_1, E_inf_1, log10(EC50_1)))\n    viability_2 <- .Hill(log10(treatment2dose), c(HS_2, E_inf_2, log10(EC50_2)))\n    viability_2_to_1 <- hillCurve(\n        dose = treatment1dose,\n        HS = HS_2_to_1,\n        EC50 = EC50_2_to_1,\n        E_ninf = viability_2,\n        E_inf = E_inf_2_to_1\n    )\n    viability_1_to_2 <- hillCurve(\n        dose = treatment2dose,\n        HS = HS_1_to_2,\n        EC50 = EC50_1_to_2,\n        E_ninf = viability_1,\n        E_inf = E_inf_1_to_2\n    )\n    ## avoid re-calculating ZIP references\n    if (is.null(ZIP)) {\n        viability_ZIP <- computeZIP(treatment1dose = treatment1dose,\n                                    treatment2dose = treatment2dose,\n                                    HS_1 = HS_1, HS_2 = HS_2,\n                                    EC50_1 = EC50_1, EC50_2 = EC50_2,\n                                    E_inf_1 = E_inf_1, E_inf_2 = E_inf_2)\n    } else {\n        viability_ZIP <- ZIP\n    }\n    delta <- viability_ZIP - (1/2) * (viability_2_to_1 + viability_1_to_2)\n\n    return(delta)\n}\n\n# ==== Bliss Independence\n\n#' @title Compute Bliss Null References\n#'\n#' @description\n#' Given two `numeric` containing viability of two monotherapy respectively,\n#' Compute Bliss null reference values for the expected response\n#' of the two treatments combined.\n#'\n#' @param viability_1 `numeric` monotherapeutic response of treatment 1.\n#' @param viability_2 `numeric` monotherapeutic response of treatment 2.\n#'\n#' @return `numeric` expected response of the two treatments combined\n#'     under Bliss null assumption.\n#'\n#' @examples\n#' (bliss <- computeBliss(0.75, 0.65))\n#'\n#' @export\ncomputeBliss <- function(viability_1, viability_2) {\n    bliss_ref <- (viability_1 * viability_2)\n    return(bliss_ref)\n}\n\n# ==== Highest Single Agent (HSA)\n\n#' @title Compute HSA Null References\n#'\n#' @description\n#' Given two `numeric` containing viability of two monotherapy respectively,\n#' Compute highest single-agent (HSA) values as the expected response\n#' of the two treatments combined.\n#'\n#' @param viability_1 `numeric` monotherapeutic response of treatment 1.\n#' @param viability_2 `numeric` monotherapeutic response of treatment 2.\n#'\n#' @return `numeric` expected response of the two treatments combined\n#'     using the highest response of the two (lower viability).\n#'\n#' @examples\n#' (hsa <- computeHSA(0.75, 0.65))\n#'\n#' @export\ncomputeHSA <- function(viability_1, viability_2) {\n    HSA_ref <- min(viability_1, viability_2)\n    return(HSA_ref)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `effectToDose` function?",
        "answer": "The `effectToDose` function is the inverse function of the Hill equation. It computes the dose required to produce a given response (viability) based on the parameters of a drug's dose-response curve (EC50, HS, E_inf)."
      },
      {
        "question": "How does the `loeweCI` function calculate the Loewe Additive Combination Index?",
        "answer": "The `loeweCI` function calculates the Loewe Additive Combination Index (CI) using the formula: CI = (x1 / f1^-1(E)) + (x2 / f2^-1(E)), where x1 and x2 are the doses of treatments 1 and 2, and f1^-1 and f2^-1 are the inverse functions of their respective dose-response curves."
      },
      {
        "question": "What is the purpose of the `computeLoewe` function and how does it work?",
        "answer": "The `computeLoewe` function predicts the response of a treatment combination under the Loewe additive null assumption. It works by finding the viability that minimizes the distance between the Loewe CI and 1 using optimization techniques, and returns this value as the expected viability under Loewe additivity."
      }
    ],
    "completion_tasks": [
      {
        "partial": "computeZIP <- function(treatment1dose, HS_1, EC50_1, E_inf_1,\n                       treatment2dose, HS_2, EC50_2, E_inf_2) {\n    len <- length(treatment1dose)\n    assertNumeric(treatment1dose, len = len)\n    assertNumeric(treatment2dose, len = len)\n    assertNumeric(HS_1, len = len)\n    assertNumeric(HS_2, len = len)\n    assertNumeric(E_inf_1, len = len)\n    assertNumeric(E_inf_2, len = len)\n    assertNumeric(EC50_1, len = len)\n    assertNumeric(EC50_2, len = len)\n\n    y_1 <- .Hill(log10(treatment1dose), c(HS_1, E_inf_1, log10(EC50_1)))\n    y_2 <- .Hill(log10(treatment2dose), c(HS_2, E_inf_2, log10(EC50_2)))\n    y_zip <- ",
        "complete": "computeZIP <- function(treatment1dose, HS_1, EC50_1, E_inf_1,\n                       treatment2dose, HS_2, EC50_2, E_inf_2) {\n    len <- length(treatment1dose)\n    assertNumeric(treatment1dose, len = len)\n    assertNumeric(treatment2dose, len = len)\n    assertNumeric(HS_1, len = len)\n    assertNumeric(HS_2, len = len)\n    assertNumeric(E_inf_1, len = len)\n    assertNumeric(E_inf_2, len = len)\n    assertNumeric(EC50_1, len = len)\n    assertNumeric(EC50_2, len = len)\n\n    y_1 <- .Hill(log10(treatment1dose), c(HS_1, E_inf_1, log10(EC50_1)))\n    y_2 <- .Hill(log10(treatment2dose), c(HS_2, E_inf_2, log10(EC50_2)))\n    y_zip <- y_1 * y_2\n    return(y_zip)\n}"
      },
      {
        "partial": "hillCurve <- function(dose, HS, EC50, E_inf, E_ninf) {\n    ",
        "complete": "hillCurve <- function(dose, HS, EC50, E_inf, E_ninf) {\n    E_inf + (( E_ninf - E_inf ) / ( 1 + ( 10^dose / 10^EC50 )^(HS) ))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/cosinePerm.R",
    "language": "R",
    "content": "#' Cosine Permuations\n#' \n#' @inherit CoreGx::cosinePerm\n#' @inheritParams CoreGx::cosinePerm\n#' \n#' @export\ncosinePerm <- function(x, y, nperm=1000, \n                       alternative=c(\"two.sided\", \"less\", \"greater\"), \n                       include.perm=FALSE, nthread=1)\n{\n  CoreGx::cosinePerm(x, y, nperm, alternative, include.perm, nthread)\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@inherit` and `@inheritParams` Roxygen tags in this R function documentation?",
        "answer": "The `@inherit` tag is used to inherit the entire documentation from the `CoreGx::cosinePerm` function, while `@inheritParams` is used to inherit only the parameter documentation from the same function. This allows the current function to reuse documentation from the original function it's wrapping, reducing duplication and ensuring consistency."
      },
      {
        "question": "Why might a developer choose to create a wrapper function like this instead of directly using `CoreGx::cosinePerm`?",
        "answer": "A developer might create a wrapper function like this for several reasons: 1) To provide a simplified interface within their own package, 2) To ensure consistent usage across their codebase, 3) To allow for future modifications or enhancements without changing the core function call, or 4) To make the function directly available in the current package's namespace without requiring users to explicitly call `CoreGx::cosinePerm`."
      },
      {
        "question": "What is the significance of the `@export` tag in this function documentation?",
        "answer": "The `@export` tag indicates that this function should be made publicly available when the package is built and loaded. This means that users of the package will be able to call `cosinePerm` directly after loading the package, without needing to use the full `PackageName::cosinePerm` notation. It's an important tag for functions that are intended to be part of the package's public API."
      }
    ],
    "completion_tasks": [
      {
        "partial": "#' Cosine Permuations\n#' \n#' @inherit CoreGx::cosinePerm\n#' @inheritParams CoreGx::cosinePerm\n#' \n#' @export\ncosinePerm <- function(x, y, nperm=1000, \n                       alternative=c(\"two.sided\", \"less\", \"greater\"), \n                       include.perm=FALSE, nthread=1)\n{\n  # Complete the function body\n}",
        "complete": "#' Cosine Permuations\n#' \n#' @inherit CoreGx::cosinePerm\n#' @inheritParams CoreGx::cosinePerm\n#' \n#' @export\ncosinePerm <- function(x, y, nperm=1000, \n                       alternative=c(\"two.sided\", \"less\", \"greater\"), \n                       include.perm=FALSE, nthread=1)\n{\n  CoreGx::cosinePerm(x, y, nperm, alternative, include.perm, nthread)\n}"
      },
      {
        "partial": "#' @export\ncosinePerm <- function(x, y, nperm=1000, \n                       alternative=c(\"two.sided\", \"less\", \"greater\"), \n                       include.perm=FALSE, nthread=1)\n{\n  # Call the CoreGx::cosinePerm function with the appropriate arguments\n}",
        "complete": "#' @export\ncosinePerm <- function(x, y, nperm=1000, \n                       alternative=c(\"two.sided\", \"less\", \"greater\"), \n                       include.perm=FALSE, nthread=1)\n{\n  CoreGx::cosinePerm(x, y, nperm, alternative, include.perm, nthread)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/image_processing.py",
    "language": "py",
    "content": "from dicom_parser import Series\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pydicom\nfrom radiomics import imageoperations\nimport SimpleITK as sitk\n\nfrom typing import Optional\n\nfrom readii.loaders import (\n    loadDicomSITK,\n    loadRTSTRUCTSITK,\n    loadSegmentation,\n)\n\n\ndef flattenImage(image: sitk.Image) -> sitk.Image:\n    \"\"\"Remove axes of image with size one. (ex. shape is [1, 100, 256, 256])\n\n    Parameters\n    ----------\n    image : sitk.Image\n        Image to remove axes with size one.\n\n    Returns\n    -------\n    sitk.Image\n        image with axes of length one removed.\n    \"\"\"\n    imageArr = sitk.GetArrayFromImage(image)\n\n    imageArr = np.squeeze(imageArr)\n\n    return sitk.GetImageFromArray(imageArr)\n\n\ndef alignImages(originImage: sitk.Image, movingImage: sitk.Image) -> sitk.Image:\n    \"\"\"Align movingImage to the originImage so origin and direction match\n\n    Parameters\n    ----------\n    originImage : sitk.Image\n        Image to use to set direction and origin for the movingImage\n\n    movingImage : sitk.Image\n        Image to align to originImage\n\n    Returns\n    -------\n    sitk.Image\n        movingImage now aligned to originImage\n    \"\"\"\n    movingImage.SetDirection(originImage.GetDirection())\n    movingImage.SetOrigin(originImage.GetOrigin())\n    movingImage.SetSpacing(originImage.GetSpacing())\n\n    return movingImage\n\n\ndef padSegToMatchCT(\n    ctDirPath: str,\n    segImagePath: Optional[str] = None,\n    ctImage: Optional[sitk.Image] = None,\n    alignedSegImage: Optional[sitk.Image] = None,\n) -> sitk.Image:\n    \"\"\"Function to take a segmentation that doesn't have the same slice count as the CT image, maps it to the corresponding\n        CT slices, and pads it with slices containing 0s so it maps properly onto the original image.\n\n    Parameters\n    ----------\n    ctDirPath : str\n        Path to DICOM series folder containing all CT image files. Must be a directory.\n\n    segImagePath : str\n        Path to the DICOM SEG file that corresponds with CT in ctDirPath that has incorrect slice count.\n\n    ctImage : sitk.Image\n        Optional argument, base image to align the padded segmentation image to. If None is passed, will be loaded in from ctFolderPath.\n\n    alignedSegImage : sitk.Image\n        Optional argument, if image has already been loaded it can be passed in to be adjusted.\n        Assumes that flattenImage and alignImages has already been run.\n        If not passed, will use segFilePath to load the image.\n\n    Returns\n    -------\n    sitk.Image\n    Padded segmentation with the same dimensions as the CT.\n\n    Examples\n    --------\n    >>> paddedSeg = padSegToMatchCT(\"/path/to/CT\", \"/path/to/segmentation/1.dcm\")\n\n    >>> lungCT = loadDicomSITK(\"/path/to/CT\")\n    >>> tumourSeg = loadSegmentation(\"/path/to/segmentation/1.dcm\", 'SEG')\n    >>> paddedSeg = padSegToMatchCT(\"/path/to/CT\", ctImage = lungCT, alignedSegImage = tumourSeg)\n    \"\"\"\n\n    # Load the CT image to align the segmentation to if not passed as argument\n    if ctImage is None:\n        ctImage = loadDicomSITK(ctDirPath)\n\n    # Load in the segmentation image if not passed as argument\n    if alignedSegImage is None:\n        if segImagePath is None:\n            raise ValueError(\n                \"Must pass either a loaded and aligned segmentation or the path to load the segmentation from.\"\n            )\n        else:\n            segImage = loadSegmentation(segImagePath, modality=\"SEG\")\n            # Segmentation contains extra axis, flatten to 3D by removing it\n            segImage = flattenImage(segImage)\n            # Segmentation has different origin, align it to the CT for proper feature extraction\n            alignedSegImage = alignImages(ctImage, segImage)\n\n    # Load in header information for the CT and SEG files\n    ctSeries = Series(ctDirPath)\n    segWithHeader = pydicom.dcmread(segImagePath, stop_before_pixels=True)\n\n    # Get the first and last reference ID for the slices of the CT that are in the SEG file\n    lastSliceRef = (\n        segWithHeader.ReferencedSeriesSequence[0]\n        .ReferencedInstanceSequence[0]\n        .ReferencedSOPInstanceUID\n    )\n    firstSliceRef = (\n        segWithHeader.ReferencedSeriesSequence[0]\n        .ReferencedInstanceSequence[-1]\n        .ReferencedSOPInstanceUID\n    )\n\n    # Get the index of the reference IDs in the CT image\n    firstSliceIdx = ctSeries[\"SOPInstanceUID\"].index(firstSliceRef)\n    lastSliceIdx = ctSeries[\"SOPInstanceUID\"].index(lastSliceRef)\n\n    # Convert the segmentation image to an array and pad with 0s so segmentation mask is in the correct indices\n    arrSeg = sitk.GetArrayFromImage(alignedSegImage)\n    padArrSeg = np.pad(\n        arrSeg,\n        (\n            (\n                (firstSliceIdx, (ctSeries.data.shape[-1] - lastSliceIdx - 1)),\n                (0, 0),\n                (0, 0),\n            )\n        ),\n        \"constant\",\n        constant_values=(0),\n    )\n\n    # Convert back to Image object\n    paddedSegImage = sitk.GetImageFromArray(padArrSeg)\n    paddedSegImage = alignImages(ctImage, paddedSegImage)\n\n    return paddedSegImage\n\n\ndef displayImageSlice(\n    image, \n    sliceIdx, \n    cmap=plt.cm.Greys_r, \n    dispMin=None, \n    dispMax=None\n) -> None:\n    \"\"\"Function to display a 2D slice from a 3D image\n        By default, displays slice in greyscale with min and max range set to min and max value in the slice.\n\n    Parameters\n    ----------\n    image : sitk.Image or nd.array\n        The complete image you'd like to display a slice of. If an array, must have slices as first dimension\n    sliceIdx : int\n        Slice index from image to display\n    cmap : matplotlib.colormap\n        Color map to use for plot, see https://matplotlib.org/stable/tutorials/colors/colormaps.html for options\n    dispMin : int\n        Value to use as min for cmap in display\n    dispMax : int\n        Value to use as max for cmap in display\n    \"\"\"\n    # If image is a simple ITK image, convert to array for display\n    if type(image) == sitk.Image:\n        image = sitk.GetArrayFromImage(image)\n\n    # Get min and max value from image to define range in display\n    if dispMin == None:\n        dispMin = image.min()\n    if dispMax == None:\n        dispMax = image.max()\n\n    # Display the slice of the image\n    plt.imshow(image[sliceIdx, :, :], cmap=cmap, vmin=dispMin, vmax=dispMax)\n    plt.axis(\"off\")\n\n\ndef displayCTSegOverlay(\n    ctImage,\n    segImage,\n    sliceIdx=-1,\n    cmapCT=plt.cm.Greys_r,\n    cmapSeg=plt.cm.brg,\n    alpha=0.3,\n    crop=False,\n) -> None:\n    \"\"\"Function to display a 2D slice from a CT with the ROI from a segmentation image overlaid in green\n    Parameters\n    ----------\n    ctImage : sitk.Image or nd.array\n        CT image to display a slice of. If an array, must have slices as first dimension.\n    segImage : sitk.Image or nd.array\n        Segmentation image containing a ROI to overlay with CT. Must be aligned to CT. If an array, must have slices as first dimension\n        and have background labeled as 0s.\n    sliceIdx : int\n        Slice index from image to display. If not provided, will get center slice of ROI to plot.\n    cmapCT : matplotlib.colormap\n        Color map to use for CT plot, see https://matplotlib.org/stable/tutorials/colors/colormaps.html for options. Is greyscale by default.\n    cmapSeg: matplotlib.colormap\n        Color map to use for ROI plot, see https://matplotlib.org/stable/tutorials/colors/colormaps.html for options. Is green by default.\n    alpha : float\n        Value between 0 and 1 indicating transparency of ROI overtop of CT. Default is 0.3\n    crop : bool\n        Whether to crop the output image to the ROI in the segmentation.\n    \"\"\"\n    # If crop indicated, crop the CT and segmentation to just around the ROI\n    if crop:\n        ctImage, segImage = getCroppedImages(ctImage, segImage)\n\n    # If slice index is not provided, get the center slice for the ROI in segImage\n    if sliceIdx == -1:\n        sliceIdx, _, _ = getROICenterCoords(segImage)\n\n    # If image is a simple ITK image, convert to array for display\n    if type(ctImage) == sitk.Image:\n        ctImage = sitk.GetArrayFromImage(ctImage)\n    # If segmentation is a simple ITK image, convert to array for display\n    if type(segImage) == sitk.Image:\n        segImage = sitk.GetArrayFromImage(segImage)\n\n    # Make mask of ROI to ignore background in overlaid plot\n    maskSeg = np.ma.masked_where(segImage == 0, segImage)\n\n    # Plot slice of CT\n    plt.imshow(\n        ctImage[sliceIdx, :, :], cmap=cmapCT, vmin=ctImage.min(), vmax=ctImage.max()\n    )\n    # Plot mask of ROI overtop\n    plt.imshow(\n        maskSeg[sliceIdx, :, :],\n        cmap=cmapSeg,\n        vmin=segImage.min(),\n        vmax=segImage.max(),\n        alpha=alpha,\n    )\n    plt.axis(\"off\")\n\n\ndef getROICenterCoords(segImage: sitk.Image):\n    \"\"\"A function to find the slice number and coordinates for the center of an ROI in a loaded RTSTRUCT or SEG file.\n\n    Parameters\n    ----------\n    segImage\n        sitk.Image, a loaded segmentation image, should be binary with segmentation voxels as a non-zero value\n\n    Returns\n    -------\n    centerSliceIdx : int\n        Index of the centermost slice of the ROI in the image\n    centerColumnPixelIdx : int\n        Column index of the centermost point in the ROI in the center slice.\n    centerRowPixelIdx : int\n        Row index of the centermost point in the ROI in the center slice.\n    \"\"\"\n    # Convert segmentation image to a numpy array\n    arrSeg = sitk.GetArrayFromImage(segImage)\n\n    nonZeroIndices = np.nonzero(arrSeg)\n    nzSliceIndices = nonZeroIndices[0]\n    nzColumnIndices = nonZeroIndices[1]\n    nzRowIndices = nonZeroIndices[2]\n\n    centerSliceIdx = nzSliceIndices[int(len(nzSliceIndices) / 2)]\n    centerColumnPixelIdx = nzColumnIndices[int(len(nzColumnIndices) / 2)]\n    centerRowPixelIdx = nzRowIndices[int(len(nzRowIndices) / 2)]\n\n    return centerSliceIdx, centerColumnPixelIdx, centerRowPixelIdx\n\n\ndef getROIVoxelLabel(segImage: sitk.Image):\n    \"\"\"A function to find the non-zero value that identifies segmentation voxels in a loaded RTSTRUCT or SEG file.\n\n    Parameters\n    ----------\n    segImage\n        sitk.Image, a loaded segmentation image, should be binary with segmentation voxels as a non-zero value\n\n    Returns\n    -------\n    labelValue\n        int, the label value for the segmentation voxels\n    \"\"\"\n\n    # Convert segmentation image to a numpy array\n    arrSeg = sitk.GetArrayFromImage(segImage)\n    # Get all values that aren't 0 - these will identify the ROI\n    roiVoxels = arrSeg[arrSeg != 0]\n    # Confirm that all of these are the same value\n    if np.all(roiVoxels == roiVoxels[0]):\n        labelValue = roiVoxels[0]\n        return int(labelValue)\n    else:\n        raise ValueError(\n            \"Multiple label values present in this segmentation. Must all be the same.\"\n        )\n\n\ndef getCroppedImages(ctImage, segImage, segmentationLabel=None):\n    \"\"\"A function to crop a CT and segmentation to close to the ROI within the segmentation.\n\n    Parameters\n    ----------\n    ctImage : sitk.Image\n        CT image to crop.\n    segImage : sitk.Image\n        Segmentation image containing a ROI to overlay with CT. Must be aligned to CT.\n    segmentationLabel : int\n        Value of pixels within the ROI in the segImage. If not passed, will use getROIVoxelLabel to find it.\n\n    Returns\n    -------\n    croppedCT : sitk.Image\n        CT cropped to bounding box around ROI\n    croppedROI : sitk.Image\n        Segmentation cropped to bounding box around ROI.\n    \"\"\"\n    if segmentationLabel == None:\n        segmentationLabel = getROIVoxelLabel(segImage)\n\n    # Check that CT and segmentation correspond, segmentationLabel is present, and dimensions match\n    segBoundingBox, correctedROIImage = imageoperations.checkMask(\n        ctImage, segImage, label=segmentationLabel\n    )\n    # Update the ROI image if a correction was generated by checkMask\n    if correctedROIImage is not None:\n        segImage = correctedROIImage\n\n    # Crop the image and mask to a bounding box around the mask to reduce volume size to process\n    croppedCT, croppedROI = imageoperations.cropToTumorMask(\n        ctImage, segImage, segBoundingBox\n    )\n\n    return croppedCT, croppedROI\n",
    "qa_pairs": null,
    "completion_tasks": [
      {
        "partial": "def flattenImage(image: sitk.Image) -> sitk.Image:\n    \"\"\"Remove axes of image with size one. (ex. shape is [1, 100, 256, 256])\n\n    Parameters\n    ----------\n    image : sitk.Image\n        Image to remove axes with size one.\n\n    Returns\n    -------\n    sitk.Image\n        image with axes of length one removed.\n    \"\"\"\n    imageArr = sitk.GetArrayFromImage(image)\n\n    # TODO: Remove axes with size one\n\n    return sitk.GetImageFromArray(imageArr)",
        "complete": "def flattenImage(image: sitk.Image) -> sitk.Image:\n    \"\"\"Remove axes of image with size one. (ex. shape is [1, 100, 256, 256])\n\n    Parameters\n    ----------\n    image : sitk.Image\n        Image to remove axes with size one.\n\n    Returns\n    -------\n    sitk.Image\n        image with axes of length one removed.\n    \"\"\"\n    imageArr = sitk.GetArrayFromImage(image)\n    imageArr = np.squeeze(imageArr)\n    return sitk.GetImageFromArray(imageArr)"
      },
      {
        "partial": "def getROICenterCoords(segImage: sitk.Image):\n    \"\"\"A function to find the slice number and coordinates for the center of an ROI in a loaded RTSTRUCT or SEG file.\n\n    Parameters\n    ----------\n    segImage\n        sitk.Image, a loaded segmentation image, should be binary with segmentation voxels as a non-zero value\n\n    Returns\n    -------\n    centerSliceIdx : int\n        Index of the centermost slice of the ROI in the image\n    centerColumnPixelIdx : int\n        Column index of the centermost point in the ROI in the center slice.\n    centerRowPixelIdx : int\n        Row index of the centermost point in the ROI in the center slice.\n    \"\"\"\n    # TODO: Implement the function\n    pass",
        "complete": "def getROICenterCoords(segImage: sitk.Image):\n    \"\"\"A function to find the slice number and coordinates for the center of an ROI in a loaded RTSTRUCT or SEG file.\n\n    Parameters\n    ----------\n    segImage\n        sitk.Image, a loaded segmentation image, should be binary with segmentation voxels as a non-zero value\n\n    Returns\n    -------\n    centerSliceIdx : int\n        Index of the centermost slice of the ROI in the image\n    centerColumnPixelIdx : int\n        Column index of the centermost point in the ROI in the center slice.\n    centerRowPixelIdx : int\n        Row index of the centermost point in the ROI in the center slice.\n    \"\"\"\n    arrSeg = sitk.GetArrayFromImage(segImage)\n    nonZeroIndices = np.nonzero(arrSeg)\n    nzSliceIndices, nzColumnIndices, nzRowIndices = nonZeroIndices\n    centerSliceIdx = nzSliceIndices[len(nzSliceIndices) // 2]\n    centerColumnPixelIdx = nzColumnIndices[len(nzColumnIndices) // 2]\n    centerRowPixelIdx = nzRowIndices[len(nzRowIndices) // 2]\n    return centerSliceIdx, centerColumnPixelIdx, centerRowPixelIdx"
      }
    ],
    "dependencies": {
      "imports": [
        "matplotlib.pyplot",
        "numpy",
        "pydicom",
        "SimpleITK"
      ],
      "from_imports": [
        "dicom_parser.Series",
        "radiomics.imageoperations",
        "typing.Optional",
        "readii.loaders.loadDicomSITK"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/ops/functional.py",
    "language": "py",
    "content": "import SimpleITK as sitk\nimport numpy as np\n\nfrom typing import Sequence, Union, Tuple, Optional\nfrom collections import namedtuple\n\nfrom ..modules import Segmentation\n\n\nINTERPOLATORS = {\n    \"linear\": sitk.sitkLinear,\n    \"nearest\": sitk.sitkNearestNeighbor,\n    \"bspline\": sitk.sitkBSpline,\n}\n\n\ndef resample(image: sitk.Image,\n             spacing: Union[float, Sequence[float], np.ndarray],\n             interpolation: str = \"linear\",\n             anti_alias: bool = True,\n             anti_alias_sigma: Optional[float] = None,\n             transform: Optional[sitk.Transform] = None,\n             output_size: Optional[Sequence[float]] = None) -> sitk.Image:\n    \"\"\"Resample image to a given spacing, optionally applying a transformation.\n\n    Parameters\n    ----------\n    image\n        The image to be resampled.\n\n    spacing\n        The new image spacing. If float, assumes the same spacing in all\n        directions. Alternatively, a sequence of floats can be passed to\n        specify spacing along each dimension. Passing 0 at any position will\n        keep the original spacing along that dimension (useful for in-plane\n        resampling).\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `spacing < image.GetSpacing()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n    transform, optional\n        Transform to apply to input coordinates when resampling. If None,\n        defaults to identity transformation.\n\n    output_size, optional\n        Size of the output image. If None, it is computed to preserve the\n        whole extent of the input image.\n\n    Returns\n    -------\n    sitk.Image\n        The resampled image.\n    \"\"\"\n\n    try:\n        interpolator = INTERPOLATORS[interpolation]\n    except KeyError:\n        raise ValueError(\n            f\"interpolator must be one of {list(INTERPOLATORS.keys())}, got {interpolation}.\"\n        )\n\n    original_spacing = np.array(image.GetSpacing())\n    original_size = np.array(image.GetSize())\n\n    if isinstance(spacing, (float, int)):\n        new_spacing = np.repeat(spacing,\n                                len(original_spacing)).astype(np.float64)\n    else:\n        spacing = np.asarray(spacing)\n        new_spacing = np.where(spacing == 0, original_spacing, spacing)\n\n    if not output_size:\n        new_size = np.floor(original_size * original_spacing / new_spacing).astype(int)\n    else:\n        new_size = np.asarray(output_size)\n\n    rif = sitk.ResampleImageFilter()\n    rif.SetOutputOrigin(image.GetOrigin())\n    rif.SetOutputSpacing(new_spacing)\n    rif.SetOutputDirection(image.GetDirection())\n    rif.SetSize(new_size.tolist())\n\n    if transform is not None:\n        rif.SetTransform(transform)\n\n    downsample = new_spacing > original_spacing\n    if downsample.any() and anti_alias:\n        if not anti_alias_sigma:\n            # sigma computation adapted from scikit-image\n            # https://github.com/scikit-image/scikit-image/blob/master/skimage/transform/_warps.py\n            anti_alias_sigma = np.maximum(1e-11, (original_spacing / new_spacing - 1) / 2)\n        sigma = np.where(downsample, anti_alias_sigma, 1e-11)\n        image = sitk.SmoothingRecursiveGaussian(image, sigma)\n\n    rif.SetInterpolator(interpolator)\n    resampled_image = rif.Execute(image)\n\n    return resampled_image\n\n\ndef resize(image: sitk.Image,\n           size: Union[int, Sequence[int], np.ndarray],\n           interpolation: str = \"linear\",\n           anti_alias: bool = True,\n           anti_alias_sigma: Optional[float] = None)-> sitk.Image:\n    \"\"\"Resize image to a given size by resampling coordinates.\n\n    Parameters\n    ----------\n    image\n        The image to be resize.\n\n    size\n        The new image size. If float, assumes the same size in all directions.\n        Alternatively, a sequence of floats can be passed to specify size along\n        each dimension. Passing 0 at any position will keep the original\n        size along that dimension.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `size < image.GetSize()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n    Returns\n    -------\n    sitk.Image\n        The resized image.\n    \"\"\"\n\n    original_size = np.array(image.GetSize())\n    original_spacing = np.array(image.GetSpacing())\n\n    if isinstance(size, (float, int)):\n        new_size = np.repeat(size, len(original_size)).astype(np.float64)\n    else:\n        size = np.asarray(size)\n        new_size = np.where(size == 0, original_size, size)\n\n    new_spacing = original_spacing * original_size / new_size\n\n    return resample(image,\n                    new_spacing,\n                    anti_alias=anti_alias,\n                    anti_alias_sigma=anti_alias_sigma,\n                    interpolation=interpolation)\n\n\ndef zoom(image: sitk.Image,\n         scale_factor: Union[float, Sequence[float]],\n         interpolation: str = \"linear\",\n         anti_alias: bool = True,\n         anti_alias_sigma: Optional[float] = None) -> sitk.Image:\n    \"\"\"Rescale image, preserving its spatial extent.\n\n    The rescaled image will have the same spatial extent (size) but will be\n    rescaled by `scale_factor` in each dimension. Alternatively, a separate\n    scale factor for each dimension can be specified by passing a sequence\n    of floats.\n\n    Parameters\n    ----------\n    image\n        The image to rescale.\n\n    scale_factor\n        If float, each dimension will be scaled by that factor. If tuple, each\n        dimension will be scaled by the corresponding element.\n\n    interpolation, optional\n        The interpolation method to use. Valid options are:\n        - \"linear\" for bi/trilinear interpolation (default)\n        - \"nearest\" for nearest neighbour interpolation\n        - \"bspline\" for order-3 b-spline interpolation\n\n    anti_alias, optional\n        Whether to smooth the image with a Gaussian kernel before resampling.\n        Only used when downsampling, i.e. when `size < image.GetSize()`.\n        This should be used to avoid aliasing artifacts.\n\n    anti_alias_sigma, optional\n        The standard deviation of the Gaussian kernel used for anti-aliasing.\n\n    Returns\n    -------\n    sitk.Image\n        The rescaled image.\n    \"\"\"\n    dimension = image.GetDimension()\n\n    if isinstance(scale_factor, float):\n        scale_factor = (scale_factor,) * dimension\n\n    centre_idx = np.array(image.GetSize()) / 2\n    centre = image.TransformContinuousIndexToPhysicalPoint(centre_idx)\n\n    transform = sitk.ScaleTransform(dimension, scale_factor)\n    transform.SetCenter(centre)\n\n    return resample(image,\n                    spacing=image.GetSpacing(),\n                    interpolation=interpolation,\n                    anti_alias=anti_alias,\n                    anti_alias_sigma=anti_alias_sigma,\n                    transform=transform,\n                    output_size=image.GetSize())\n\n\ndef rotate(image: sitk.Image,\n           rotation_centre: Sequence[float],\n           angles: Union[float, Sequence[float]],\n           interpolation: str = \"linear\") -> sitk.Image:\n    \"\"\"Rotate an image around a given centre.\n\n    Parameters\n    ----------\n    image\n        The image to rotate.\n\n    rotation_centre\n        The centre of rotation in image coordinates.\n\n    angles\n        The angles of rotation around x, y and z axes.\n\n    Returns\n    -------\n    sitk.Image\n        The rotated image.\n    \"\"\"\n    if isinstance(rotation_centre, np.ndarray):\n        rotation_centre = rotation_centre.tolist()\n\n    rotation_centre = image.TransformIndexToPhysicalPoint(rotation_centre)\n\n    if image.GetDimension() == 2:\n        rotation = sitk.Euler2DTransform(\n            rotation_centre,\n            angles,\n            (0., 0.)  # no translation\n        )\n    elif image.GetDimension() == 3:\n        x_angle, y_angle, z_angle = angles\n\n        rotation = sitk.Euler3DTransform(\n            rotation_centre,\n            x_angle,  # the angle of rotation around the x-axis, in radians -> coronal rotation\n            y_angle,  # the angle of rotation around the y-axis, in radians -> saggittal rotation\n            z_angle,  # the angle of rotation around the z-axis, in radians -> axial rotation\n            (0., 0., 0.)  # no translation\n        )\n    return resample(image,\n                    spacing=image.GetSpacing(),\n                    interpolation=interpolation,\n                    transform=rotation)\n\n\ndef crop(image: sitk.Image,\n         crop_centre: Sequence[float],\n         size: Union[int, Sequence[int], np.ndarray]) -> sitk.Image:\n    \"\"\"Crop an image to the desired size around a given centre.\n\n    Note that the cropped image might be smaller than size in a particular\n    direction if the cropping window exceeds image boundaries.\n\n    Parameters\n    ----------\n    image\n        The image to crop.\n\n    crop_centre\n        The centre of the cropping window in image coordinates.\n\n    size\n        The size of the cropping window along each dimension in pixels. If\n        float, assumes the same size in all directions. Alternatively, a\n        sequence of floats can be passed to specify size along x, y and z\n        dimensions. Passing 0 at any position will keep the original size along\n        that dimension.\n\n    Returns\n    -------\n    sitk.Image\n        The cropped image.\n    \"\"\"\n    crop_centre = np.asarray(crop_centre, dtype=np.float64)\n    original_size = np.asarray(image.GetSize())\n\n    if isinstance(size, int):\n        size = np.array([size for _ in image.GetSize()])\n    else:\n        size = np.asarray(size)\n\n    if (crop_centre < 0).any() or (crop_centre > original_size).any():\n        raise ValueError(\n            f\"Crop centre outside image boundaries. Image size = {original_size}, crop centre = {crop_centre}\"\n        )\n\n    min_coords = np.clip(\n        np.floor(crop_centre - size / 2).astype(np.int64), 0,\n        original_size)\n    min_coords = np.where(size == 0, 0, min_coords)\n\n    max_coords = np.clip(\n        np.floor(crop_centre + size / 2).astype(np.int64), 0,\n        original_size)\n    max_coords = np.where(size == 0, original_size, max_coords)\n\n    min_x, min_y, min_z = min_coords\n    max_x, max_y, max_z = max_coords\n\n    return image[min_x:max_x, min_y:max_y, min_z:max_z]\n\n# def constant_pad(image, size, cval=0.):\n#     if isinstance(size, int):\n#         size_lower = size_upper = [size for _ in image.GetSize()]\n#     elif isinstance(size, (tuple, list, np.ndarray)):\n#         if isinstance(size[0], int):\n#             size_lower = size_upper = size\n#         elif isinstance(size[0], (tuple, list, np.ndarray)):\n#             size_lower = [s[0] for s in size]\n#             size_upper = [s[1] for s in size]\n#     else:\n#         raise ValueError(\n#             f\"Size must be either int, sequence of int or sequence of sequences of ints, got {size}.\"\n#         )\n#     return sitk.ConstantPad(image, size_lower, size_upper, cval)\n\n# def centre_on_point(image, centre):\n#     pass\n\n# def resize_by_cropping_or_padding(image, size, centre=None, cval=0.):\n#     original_size = np.array(image.GetSize())\n#     size = np.asarray(size)\n#     centre = np.asarray(centre) if centre is not None else original_size / 2 # XXX is there any benefit to not using floor div here?\n\n#     crop_dims = np.where(size < original_size)\n\n\ndef bounding_box(mask: sitk.Image, label: int = 1) -> Tuple[Tuple, Tuple]:\n    \"\"\"Find the axis-aligned bounding box of a region descriibed by a\n    segmentation mask.\n\n    Parameters\n    ----------\n    mask\n        Segmentation mask describing the region of interest. Can be an image of\n        type unsigned int representing a label map or `segmentation.Segmentation`.\n\n    label, optional\n        Label to use when computing bounding box if segmentation mask contains\n        more than 1 labelled region.\n\n    Returns\n    -------\n    tuple of tuples\n        The bounding box location and size. The first tuple gives the\n        coordinates of the corner closest to the origin and the second\n        gives the size in pixels along each dimension.\n    \"\"\"\n\n    if isinstance(mask, Segmentation):\n        seg = Segmentation(mask)\n        mask = seg.get_label(label=label, relabel=True)\n\n    filter_ = sitk.LabelShapeStatisticsImageFilter()\n    filter_.Execute(mask)\n    bbox = filter_.GetBoundingBox(label)\n    location = bbox[:len(bbox)//2]\n    size = bbox[len(bbox)//2:]\n    return location, size\n\n\ndef centroid(mask: sitk.Image,\n             label: int = 1,\n             world_coordinates: bool = False) -> tuple:\n    \"\"\"Find the centroid of a labelled region specified by a segmentation mask.\n\n    Parameters\n    ----------\n    mask\n        Segmentation mask describing the region of interest. Can be an image of\n        type unsigned int representing a label map or `segmentation.Segmentation`.\n\n    label, optional\n        Label to use when computing the centroid if segmentation mask contains\n        more than 1 labelled region.\n\n    world_coordinates, optional\n        If True, return centroid in world coordinates, otherwise in image\n        (voxel) coordinates (default).\n\n    Returns\n    -------\n    tuple\n        The centroid coordinates.\n    \"\"\"\n\n    if isinstance(mask, Segmentation):\n        seg = Segmentation(mask)\n        mask = seg.get_label(label=label, relabel=True)\n\n    filter_ = sitk.LabelShapeStatisticsImageFilter()\n    filter_.Execute(mask)\n    centroid_coords = filter_.GetCentroid(label)\n    if not world_coordinates:\n        centroid_coords = mask.TransformPhysicalPointToIndex(centroid_coords)\n    return centroid_coords\n\n\ndef crop_to_mask_bounding_box(image: sitk.Image,\n                              mask: sitk.Image,\n                              margin: Union[int, Sequence[int], np.ndarray] = 0,\n                              label: int = 1) -> Tuple[sitk.Image]:\n    \"\"\"Crop the image using the bounding box of a region of interest specified\n    by a segmentation mask.\n\n    Parameters\n    ----------\n    image\n        The image to crop.\n\n    mask\n        Segmentation mask describing the region of interest. Can be an image of\n        type unsigned int representing a label map or `segmentation.Segmentation`.\n\n    margin, optional\n        A margin that will be added to each dimension when cropping. If int,\n        add the same margin to each dimension. A sequence of ints can also be\n        passed to specify the margin separately along each dimension.\n\n    label, optional\n        Label to use when computing the centroid if segmentation mask contains\n        more than 1 labelled region.\n\n    Returns\n    -------\n    tuple of sitk.Image\n        The cropped image and mask.\n    \"\"\"\n\n    if isinstance(mask, Segmentation):\n        seg = Segmentation(mask)\n        mask = seg.get_label(label=label, relabel=True)\n\n    if isinstance(margin, Sequence):\n        margin = np.asarray(margin)\n\n    bbox_location, bbox_size = bounding_box(mask, label=label)\n    bbox_location, bbox_size = np.array(bbox_location), np.array(bbox_size)\n    crop_size = bbox_size + margin*2\n    crop_centre = bbox_location - margin + crop_size / 2\n\n    image = crop(image, crop_centre, crop_size)\n    mask = crop(mask, crop_centre, crop_size)\n\n    return image, mask, crop_centre\n\n\ndef clip_intensity(image: sitk.Image,\n                   lower: float,\n                   upper: float):\n    \"\"\"Clip image grey level intensities to specified range.\n\n    The grey level intensities in the resulting image will fall in the range\n    [lower, upper].\n\n    Parameters\n    ----------\n    image\n        The intensity image to clip.\n\n    lower\n        The lower bound on grey level intensity. Voxels with lower intensity\n        will be set to this value.\n\n    upper\n        The upper bound on grey level intensity. Voxels with higer intensity\n        will be set to this value.\n\n    Returns\n    -------\n    sitk.Image\n        The clipped intensity image.\n    \"\"\"\n    return sitk.Clamp(image, image.GetPixelID(), lower, upper)\n\n\ndef window_intensity(image: sitk.Image,\n                     window: float,\n                     level: float) -> sitk.Image:\n    \"\"\"Restrict image grey level intensities to a given window and level.\n\n    The grey level intensities in the resulting image will fall in the range\n    [level - window / 2, level + window / 2].\n\n    Parameters\n    ----------\n    image\n        The intensity image to window.\n\n    window\n        The width of the intensity window.\n\n    level\n        The mid-point of the intensity window.\n\n    Returns\n    -------\n    sitk.Image\n        The windowed intensity image.\n    \"\"\"\n    lower = level - window / 2\n    upper = level + window / 2\n    return clip_intensity(image, lower, upper)\n\n\ndef image_statistics(image: sitk.Image,\n                     mask: Optional[sitk.Image] = None,\n                     label: int = 1) -> float:\n    \"\"\"Compute the intensity statistics of an image.\n\n    Returns the minimum, maximum, sum, mean, variance and standard deviation\n    of image intensities.\n    This function also supports computing the statistics in a specific\n    region of interest if `mask` and `label` are passed.\n\n    Parameters\n    ----------\n    image\n        The image used to compute the statistics.\n\n    mask, optional\n        Segmentation mask specifying a region of interest used in computation.\n        Can be an image of type unsigned int representing a label map or\n        `segmentation.Segmentation`. Only voxels falling within the ROI will\n        be considered. If None, use the whole image.\n\n    label, optional\n        Label to use when computing the statistics if segmentation mask contains\n        more than 1 labelled region.\n\n    Returns\n    -------\n    collections.namedtuple\n        The computed intensity statistics in the image or region.\n    \"\"\"\n\n    ImageStatistics = namedtuple(\"ImageStatistics\",\n                                [\"minimum\",\n                                 \"maximum\",\n                                 \"sum\",\n                                 \"mean\",\n                                 \"variance\",\n                                 \"standard_deviation\"\n                                ])\n\n    if mask is not None:\n        if isinstance(mask, Segmentation):\n            seg = Segmentation(mask)\n            mask = seg.get_label(label=label, relabel=True)\n\n        filter_ = sitk.LabelStatisticsImageFilter()\n        filter_.Execute(image, mask)\n        result = ImageStatistics(\n            minimum=filter_.GetMinimum(label),\n            maximum=filter_.GetMaximum(label),\n            sum=filter_.GetSum(label),\n            mean=filter_.GetMean(label),\n            variance=filter_.GetVariance(label),\n            standard_deviation=filter_.GetSigma(label)\n        )\n    else:\n        filter_ = sitk.StatisticsImageFilter()\n        filter_.Execute(image)\n        result = ImageStatistics(\n            minimum=filter_.GetMinimum(),\n            maximum=filter_.GetMaximum(),\n            sum=filter_.GetSum(),\n            mean=filter_.GetMean(),\n            variance=filter_.GetVariance(),\n            standard_deviation=filter_.GetSigma()\n        )\n\n    return result\n\n\ndef standard_scale(image: sitk.Image,\n                   mask: Optional[sitk.Image] = None,\n                   rescale_mean: Optional[float] = None,\n                   rescale_std: Optional[float] = None,\n                   label: int = 1) -> sitk.Image:\n    \"\"\"Rescale image intensities by subtracting the mean and dividing by\n       standard deviation.\n\n    If `rescale_mean` and `rescale_std` are None, image mean and standard\n    deviation will be used, i.e. the resulting image intensities will have\n    0 mean and unit variance. Alternatively, a specific mean and standard\n    deviation can be passed to e.g. standardize a whole dataset of images.\n    If a segmentation mask is passed, only the voxels falling within the mask\n    will be considered when computing the statistics. However, the whole image\n    will still be normalized using the computed values.\n\n    Parameters\n    ----------\n    image\n        The image to rescale.\n\n    mask, optional\n        Segmentation mask specifying a region of interest used in computation.\n        Can be an image of type unsigned int representing a label map or\n        `segmentation.Segmentation`. Only voxels falling within the ROI will\n        be considered. If None, use the whole image.\n\n    rescale_mean, optional\n        The mean intensity used in rescaling. If None, image mean will be used.\n\n    rescale_std, optional\n        The standard deviation used in rescaling. If None, image standard\n        deviation will be used.\n\n    label, optional\n        Label to use when computing the mean and standard deviation if\n        segmentation mask contains more than 1 labelled region.\n\n    Returns\n    -------\n    sitk.Image\n        The rescaled image.\n    \"\"\"\n    if not rescale_mean or not rescale_std:\n        statistics = image_statistics(image, mask, label)\n        rescale_mean = statistics.mean\n        rescale_std = statistics.standard_deviation\n    return (image - rescale_mean) / rescale_std\n\n\ndef min_max_scale(image: sitk.Image,\n                  minimum: float = 0.,\n                  maximum: float = 1.) -> sitk.Image:\n    \"\"\"Rescale image intensities to a given minimum and maximum.\n\n    Applies a linear transformation to image intensities such that the minimum\n    and maximum intensity values in the resulting image are equal to minimum\n    (default 0) and maximum (default 1) respectively.\n\n    Parameters\n    ----------\n    image\n        The image to rescale.\n\n    minimum, optional\n        The minimum intensity in the rescaled image.\n\n    maximum, optional\n        The maximum intensity in the rescaled image.\n\n    Returns\n    -------\n    sitk.Image\n        The rescaled image.\n    \"\"\"\n    return sitk.RescaleIntensity(image, minimum, maximum)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `resample` function in the given code snippet, and what are its key parameters?",
        "answer": "The `resample` function is used to resample an image to a given spacing, optionally applying a transformation. Its key parameters include:\n1. `image`: The SimpleITK Image to be resampled.\n2. `spacing`: The new image spacing (float or sequence of floats).\n3. `interpolation`: The interpolation method to use (e.g., 'linear', 'nearest', 'bspline').\n4. `anti_alias`: Boolean to determine whether to smooth the image before resampling.\n5. `transform`: Optional SimpleITK Transform to apply during resampling.\n6. `output_size`: Optional size of the output image."
      },
      {
        "question": "How does the `zoom` function differ from the `resize` function in terms of their effects on the input image?",
        "answer": "The `zoom` and `resize` functions differ in the following ways:\n1. `zoom` preserves the spatial extent (size) of the image while rescaling it by a given factor. It uses a `ScaleTransform` to achieve this effect.\n2. `resize` changes the size of the image to a specified new size, which may alter the spatial extent of the image.\n3. `zoom` takes a `scale_factor` parameter, which can be a single float or a sequence of floats for each dimension.\n4. `resize` takes a `size` parameter, which specifies the new dimensions of the image.\nBoth functions use the `resample` function internally, but with different parameters to achieve their respective effects."
      },
      {
        "question": "Explain the purpose of the `anti_alias` parameter in the `resample` function and how it is implemented.",
        "answer": "The `anti_alias` parameter in the `resample` function is used to prevent aliasing artifacts when downsampling an image. Its purpose and implementation are as follows:\n1. Purpose: To smooth the image with a Gaussian kernel before resampling when downsampling (i.e., when the new spacing is larger than the original spacing).\n2. Implementation:\n   - It checks if downsampling is occurring by comparing the new spacing with the original spacing.\n   - If downsampling and `anti_alias` is True, it calculates the sigma for the Gaussian smoothing.\n   - The sigma is computed as `(original_spacing / new_spacing - 1) / 2` if not provided.\n   - It then applies a SmoothingRecursiveGaussian filter to the image using the calculated sigma.\n   - This smoothing helps to reduce high-frequency components that could cause aliasing in the downsampled image."
      }
    ],
    "completion_tasks": [
      {
        "partial": "def crop_to_mask_bounding_box(image: sitk.Image,\n                              mask: sitk.Image,\n                              margin: Union[int, Sequence[int], np.ndarray] = 0,\n                              label: int = 1) -> Tuple[sitk.Image]:\n    if isinstance(mask, Segmentation):\n        seg = Segmentation(mask)\n        mask = seg.get_label(label=label, relabel=True)\n\n    if isinstance(margin, Sequence):\n        margin = np.asarray(margin)\n\n    bbox_location, bbox_size = bounding_box(mask, label=label)\n    bbox_location, bbox_size = np.array(bbox_location), np.array(bbox_size)\n    crop_size = bbox_size + margin*2\n    crop_centre = bbox_location - margin + crop_size / 2\n\n    # Complete the function by cropping the image and mask\n    # Return the cropped image, mask, and crop_centre",
        "complete": "def crop_to_mask_bounding_box(image: sitk.Image,\n                              mask: sitk.Image,\n                              margin: Union[int, Sequence[int], np.ndarray] = 0,\n                              label: int = 1) -> Tuple[sitk.Image]:\n    if isinstance(mask, Segmentation):\n        seg = Segmentation(mask)\n        mask = seg.get_label(label=label, relabel=True)\n\n    if isinstance(margin, Sequence):\n        margin = np.asarray(margin)\n\n    bbox_location, bbox_size = bounding_box(mask, label=label)\n    bbox_location, bbox_size = np.array(bbox_location), np.array(bbox_size)\n    crop_size = bbox_size + margin*2\n    crop_centre = bbox_location - margin + crop_size / 2\n\n    image = crop(image, crop_centre, crop_size)\n    mask = crop(mask, crop_centre, crop_size)\n\n    return image, mask, crop_centre"
      },
      {
        "partial": "def standard_scale(image: sitk.Image,\n                   mask: Optional[sitk.Image] = None,\n                   rescale_mean: Optional[float] = None,\n                   rescale_std: Optional[float] = None,\n                   label: int = 1) -> sitk.Image:\n    # Implement the function to rescale image intensities\n    # by subtracting the mean and dividing by standard deviation\n    # Use the provided parameters and helper functions",
        "complete": "def standard_scale(image: sitk.Image,\n                   mask: Optional[sitk.Image] = None,\n                   rescale_mean: Optional[float] = None,\n                   rescale_std: Optional[float] = None,\n                   label: int = 1) -> sitk.Image:\n    if not rescale_mean or not rescale_std:\n        statistics = image_statistics(image, mask, label)\n        rescale_mean = statistics.mean\n        rescale_std = statistics.standard_deviation\n    return (image - rescale_mean) / rescale_std"
      }
    ],
    "dependencies": {
      "imports": [
        "SimpleITK",
        "numpy"
      ],
      "from_imports": [
        "typing.Sequence",
        "collections.namedtuple",
        "modules.Segmentation"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/CTRP/CTRP_treatmentMetadata.R",
    "language": "R",
    "content": "## File: v20.meta.per_compound.txt\n## obtained from \"https://ctd2-data.nci.nih.gov/Public/Broad/CTRPv2.0_2015_ctd2_ExpandedDataset/CTRPv2.0_2015_ctd2_ExpandedDataset.zip\"\n\n# Load the treatment metadata file\nfilePath <- system.file(\"extdata/CTRP\", \"v20.meta.per_compound.txt\", package = \"AnnotationGx\")\nCTRP_treatmentMetadata <- data.table::fread(filePath)[, .(cpd_name, broad_cpd_id)]\n\n# Rename the columns\ndata.table::setnames(\n  CTRP_treatmentMetadata,\n  c(\"cpd_name\", \"broad_cpd_id\"), c(\"CTRP.treatmentid\", \"CTRP.broad_cpd_id\")\n)\n\n# Save the treatment metadata as a data object\nusethis::use_data(CTRP_treatmentMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in this code snippet, and how is it being used?",
        "answer": "The `system.file()` function is used to locate and construct the file path for the 'v20.meta.per_compound.txt' file within the 'AnnotationGx' package. It's being used to ensure that the code can find and access the correct file, regardless of where the package is installed on the user's system. The resulting file path is stored in the `filePath` variable, which is then used to read the data."
      },
      {
        "question": "How does the code handle the renaming of columns in the CTRP_treatmentMetadata data table?",
        "answer": "The code uses the `data.table::setnames()` function to rename the columns of the CTRP_treatmentMetadata data table. It changes 'cpd_name' to 'CTRP.treatmentid' and 'broad_cpd_id' to 'CTRP.broad_cpd_id'. This renaming is done in-place, meaning it modifies the original data table without creating a new one."
      },
      {
        "question": "What is the purpose of the `usethis::use_data()` function call at the end of the snippet, and what does the `overwrite = TRUE` argument do?",
        "answer": "The `usethis::use_data()` function is used to save the CTRP_treatmentMetadata as a data object in the package. This allows the data to be easily accessed and used within the package or by users of the package. The `overwrite = TRUE` argument specifies that if a data object with the same name already exists, it should be overwritten with this new version. This ensures that the most up-to-date version of the data is saved."
      }
    ],
    "completion_tasks": [
      {
        "partial": "# Load the treatment metadata file\nfilePath <- system.file(\"extdata/CTRP\", \"v20.meta.per_compound.txt\", package = \"AnnotationGx\")\nCTRP_treatmentMetadata <- data.table::fread(filePath)[, .(cpd_name, broad_cpd_id)]\n\n# Rename the columns\ndata.table::setnames(\n  CTRP_treatmentMetadata,\n  c(\"cpd_name\", \"broad_cpd_id\"), c(\"CTRP.treatmentid\", \"CTRP.broad_cpd_id\")\n)\n\n# Save the treatment metadata as a data object\n# Complete the code to save the data",
        "complete": "# Load the treatment metadata file\nfilePath <- system.file(\"extdata/CTRP\", \"v20.meta.per_compound.txt\", package = \"AnnotationGx\")\nCTRP_treatmentMetadata <- data.table::fread(filePath)[, .(cpd_name, broad_cpd_id)]\n\n# Rename the columns\ndata.table::setnames(\n  CTRP_treatmentMetadata,\n  c(\"cpd_name\", \"broad_cpd_id\"), c(\"CTRP.treatmentid\", \"CTRP.broad_cpd_id\")\n)\n\n# Save the treatment metadata as a data object\nusethis::use_data(CTRP_treatmentMetadata, overwrite = TRUE)"
      },
      {
        "partial": "# Load the treatment metadata file\nfilePath <- system.file(\"extdata/CTRP\", \"v20.meta.per_compound.txt\", package = \"AnnotationGx\")\n\n# Complete the code to load and process the data",
        "complete": "# Load the treatment metadata file\nfilePath <- system.file(\"extdata/CTRP\", \"v20.meta.per_compound.txt\", package = \"AnnotationGx\")\nCTRP_treatmentMetadata <- data.table::fread(filePath)[, .(cpd_name, broad_cpd_id)]\n\n# Rename the columns\ndata.table::setnames(\n  CTRP_treatmentMetadata,\n  c(\"cpd_name\", \"broad_cpd_id\"), c(\"CTRP.treatmentid\", \"CTRP.broad_cpd_id\")\n)\n\n# Save the treatment metadata as a data object\nusethis::use_data(CTRP_treatmentMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/rankGeneDrugPerturbation.R",
    "language": "R",
    "content": "#################################################\n## Rank genes based on drug effect in the Connectivity Map\n##\n## inputs:\n##      - data: gene expression data matrix\n##\t\t\t- drug: single or vector of drug(s) of interest; if a vector of drugs is provided, they will be considered as being the same drug and will be jointly analyszed\n##\t\t\t- drug.id: drug used in each experiment\n##\t\t\t- drug.concentration: drug concentration used in each experiment\n##\t\t\t- type: cell or tissue type for each experiment\n##\t\t\t- xp: type of experiment (perturbation or control)\n##      - batch: experiment batches\n##      - duration: The duration of the experiment, in a consistent unit\n##\t\t\t- single.type: Should the statitsics be computed for each cell/tissue type separately?\n##      - nthread: number of parallel threads (bound to the maximum number of cores available)\n##\n## outputs:\n## list of datafraes with the statistics for each gene, for each type\n##\t\t\t- list of data.frame with similar results for each type line separately if any\n##\n#################################################\n\nrankGeneDrugPerturbation <-\nfunction (data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n\n\tif (nthread != 1) {\n\t\tavailcore <- parallel::detectCores()\n\t\tif (missing(nthread) || nthread < 1 || nthread > availcore) {\n\t\t\t# print(paste(\"available cores\",availcore,\"allocated\"))\n\t\t\tnthread <- availcore\n\t\t}\n\t\telse{\n\t\t\t# print(paste(\"all\",nthread,\"cores have been allocated\"))\n\t\t}\n\t}\n\tif (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n\t\tstop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n\t}\n\tnames(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n\tif (!all(complete.cases(type, xp, batch, duration))) {\n\t\tstop(\"type, batch, duration and xp should not contain missing values!\")\n\t}\n## is the drug in the dataset?\n\tdrugix <- drug.id %in% drug\n\n\tif (sum(drugix) == 0) {\n\t\twarning(sprintf(\"Drug(s) %s not in the dataset\", paste(drug, collapse=\", \")))\n\t\treturn(list(\"all.type\"=NULL, \"single.type\"=NULL))\n\t}\n## select xps with controls or with the drug(s) of interest\n\tiix <- xp==\"control\" | drugix\n\tdata <- data[iix, ,drop=FALSE]\n\tdrug.id <- drug.id[iix]\n\tdrug.concentration <- drug.concentration[iix]\n\ttype <- type[iix]\n\txp <- xp[iix]\n\tbatch <- batch[iix]\n\tduration <- duration[iix]\n\n\tres.type <- NULL\n\n## build input matrix\n\tinpumat <- NULL\n## for each batch/vehicle of perturbations+controls (test within each batch/vehicle to avoid batch effect)\n\tubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n\tnames(ubatch) <- paste(\"batch\", ubatch, sep=\"\")\n\n\tfor (bb in seq_len(length(ubatch))) {\n## identify the perturbations and corresponding control experiments\n\t\txpix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"perturbation\"]\n\t\tctrlix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"control\"]\n\n\t\tif (all(!is.na(c(xpix, ctrlix))) && length(xpix) > 0 && length(ctrlix) > 0) {\n\t\t\tif (!all(is.element(ctrlix, rownames(data)))) {\n\t\t\t\tstop(\"data for some control experiments are missing!\")\n\t\t\t}\n\t\t\tif (verbose) {\n\t\t\t\tcat(sprintf(\"type %s: batch %i/%i -> %i vs %i\\n\", utype[bb], bb, length(ubatch), length(xpix), length(ctrlix)))\n\t\t\t}\n## transformation of drug concentrations values\n\t\t\tconc <- drug.concentration * 10^6\n\t\t\tinpumat <- rbind(inpumat, data.frame(\"treated\"=c(rep(1, length(xpix)), rep(0, length(ctrlix))), \"type\"=c(type[xpix], type[ctrlix]), \"batch\"=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"), \"concentration\"=c(conc[xpix], conc[ctrlix]), \"duration\"= c(duration[xpix], duration[ctrlix])))\n\t\t}\n\t}\n\n\tinpumat[ , \"type\"] <- factor(inpumat[ , \"type\"], ordered=FALSE)\n\tinpumat[ , \"batch\"] <- factor(inpumat[ , \"batch\"], ordered=FALSE)\n\n\tif (nrow(inpumat) < 3 || length(sort(unique(inpumat[ , \"concentration\"]))) < 2 || length(unique(inpumat[ , \"duration\"])) < 2) {\n## not enough experiments in drug list\n\t\twarning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n\t\treturn(list(\"all.type\"=NULL, \"single.type\"=NULL))\n\t}\n\n\tres <- NULL\n\tutype <- sort(unique(as.character(inpumat[ , \"type\"])))\n\tltype <- list(\"all\"=utype)\n\tif(single.type) {\n\t\tltype <- c(ltype, as.list(utype))\n\t\tnames(ltype)[-1] <- utype\n\t}\n\tfor(ll in seq_len(length(ltype))) {\n## select the type of cell line/tissue of interest\n\t\tinpumat2 <- inpumat[!is.na(inpumat[ , \"type\"]) & is.element(inpumat[ , \"type\"], ltype[[ll]]), , drop=FALSE]\n\t\tinpumat2 <- inpumat2[complete.cases(inpumat2), , drop=FALSE]\n\t\tif (nrow(inpumat2) < 3 || length(sort(unique(inpumat2[ , \"concentration\"]))) < 2) {\n## not enough experiments in data\n\t\t\tnc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n\t\t\trest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n\t\t} else {\n## test perturbation vs control\n\t\t\tif(nthread > 1) {\n## parallel threads\n\t\t\t\tsplitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n\t\t\t\tsplitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n\t\t\t\tmcres <- parallel::mclapply(splitix, function(x, data, inpumat) {\n\t\t\t\t\t\t\t\t\t\t\tres <- t(apply(data[rownames(inpumat), x, drop=FALSE], 2, geneDrugPerturbation, concentration=inpumat[ , \"concentration\"], type=inpumat[ , \"type\"], batch=inpumat[ , \"batch\"], duration=inpumat[,\"duration\"]))\n\t\t\t\t\t\t\t\t\t\t\treturn(res)\n\t\t\t\t\t\t\t\t\t\t\t}, data=data, inpumat=inpumat2)\n\t\t\t\trest <- do.call(rbind, mcres)\n\t\t\t} else {\n\t\t\t\trest <- t(apply(data[rownames(inpumat2), , drop=FALSE], 2, geneDrugPerturbation, concentration=inpumat2[ , \"concentration\"], type=inpumat2[ , \"type\"], batch=inpumat2[ , \"batch\"], duration=inpumat2[,\"duration\"]))\n\t\t\t}\n\t\t}\n\t\trest <- cbind(rest, \"fdr\"=p.adjust(rest[ , \"pvalue\"], method=\"fdr\"))\n\t\tres <- c(res, list(rest))\n\t}\n\tnames(res) <- names(ltype)\n\treturn(res)\n}\n\n## End\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `rankGeneDrugPerturbation` function, and what are its main inputs and outputs?",
        "answer": "The `rankGeneDrugPerturbation` function is designed to rank genes based on drug effects in the Connectivity Map. Its main inputs include a gene expression data matrix, drug information (ID, concentration), experimental details (cell/tissue type, experiment type, batch, duration), and optional parameters for analysis. The function outputs a list of dataframes containing statistics for each gene, potentially separated by cell/tissue type if specified."
      },
      {
        "question": "How does the function handle parallel processing, and what conditions need to be met for it to use multiple threads?",
        "answer": "The function uses parallel processing when `nthread` is not equal to 1. It detects available cores using `parallel::detectCores()` and allocates threads accordingly. If `nthread` is missing, less than 1, or greater than available cores, it uses all available cores. Parallel processing is implemented using `parallel::mclapply()` to split the workload across multiple cores when analyzing gene expression data."
      },
      {
        "question": "What steps does the function take to handle potential batch effects in the data analysis process?",
        "answer": "To mitigate batch effects, the function processes data within each batch separately. It identifies unique batches for perturbation experiments and their corresponding controls. The analysis is performed for each batch individually, comparing perturbations to controls within the same batch. This approach helps to isolate and control for potential variations between different experimental batches, ensuring more accurate comparisons of drug effects across different conditions."
      }
    ],
    "completion_tasks": [
      {
        "partial": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) {\n    stop(\"type, batch, duration and xp should not contain missing values!\")\n  }\n  drugix <- drug.id %in% drug\n  if (sum(drugix) == 0) {\n    warning(sprintf(\"Drug(s) %s not in the dataset\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n  # Complete the function\n}",
        "complete": "rankGeneDrugPerturbation <- function(data, drug, drug.id, drug.concentration, type, xp, batch, duration, single.type=FALSE, nthread=1, verbose=FALSE) {\n  if (nthread != 1) {\n    availcore <- parallel::detectCores()\n    if (missing(nthread) || nthread < 1 || nthread > availcore) {\n      nthread <- availcore\n    }\n  }\n  if (any(c(length(drug.id), length(drug.concentration), length(type), length(xp), length(batch), length(duration)) != nrow(data))) {\n    stop(\"length of drug.id, drug.concentration, type, xp, duration and batch should be equal to the number of rows of data!\")\n  }\n  names(drug.id) <- names(drug.concentration) <- names(type) <- names(batch) <- names(duration) <- rownames(data)\n  if (!all(complete.cases(type, xp, batch, duration))) {\n    stop(\"type, batch, duration and xp should not contain missing values!\")\n  }\n  drugix <- drug.id %in% drug\n  if (sum(drugix) == 0) {\n    warning(sprintf(\"Drug(s) %s not in the dataset\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n  iix <- xp==\"control\" | drugix\n  data <- data[iix, ,drop=FALSE]\n  drug.id <- drug.id[iix]\n  drug.concentration <- drug.concentration[iix]\n  type <- type[iix]\n  xp <- xp[iix]\n  batch <- batch[iix]\n  duration <- duration[iix]\n  \n  inpumat <- NULL\n  ubatch <- sort(unique(batch[!is.na(xp) & xp == \"perturbation\"]))\n  names(ubatch) <- paste(\"batch\", ubatch, sep=\"\")\n  \n  for (bb in seq_len(length(ubatch))) {\n    xpix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"perturbation\"]\n    ctrlix <- rownames(data)[complete.cases(batch, xp) & batch == ubatch[bb] & xp == \"control\"]\n    if (all(!is.na(c(xpix, ctrlix))) && length(xpix) > 0 && length(ctrlix) > 0) {\n      if (!all(is.element(ctrlix, rownames(data)))) {\n        stop(\"data for some control experiments are missing!\")\n      }\n      conc <- drug.concentration * 10^6\n      inpumat <- rbind(inpumat, data.frame(\"treated\"=c(rep(1, length(xpix)), rep(0, length(ctrlix))), \"type\"=c(type[xpix], type[ctrlix]), \"batch\"=paste(\"batch\", c(batch[xpix], batch[ctrlix]), sep=\"\"), \"concentration\"=c(conc[xpix], conc[ctrlix]), \"duration\"= c(duration[xpix], duration[ctrlix])))\n    }\n  }\n  \n  inpumat[, \"type\"] <- factor(inpumat[, \"type\"], ordered=FALSE)\n  inpumat[, \"batch\"] <- factor(inpumat[, \"batch\"], ordered=FALSE)\n  \n  if (nrow(inpumat) < 3 || length(sort(unique(inpumat[, \"concentration\"]))) < 2 || length(unique(inpumat[, \"duration\"])) < 2) {\n    warning(sprintf(\"Not enough data for drug(s) %s\", paste(drug, collapse=\", \")))\n    return(list(\"all.type\"=NULL, \"single.type\"=NULL))\n  }\n  \n  res <- NULL\n  utype <- sort(unique(as.character(inpumat[, \"type\"])))\n  ltype <- list(\"all\"=utype)\n  if(single.type) {\n    ltype <- c(ltype, as.list(utype))\n    names(ltype)[-1] <- utype\n  }\n  \n  for(ll in seq_len(length(ltype))) {\n    inpumat2 <- inpumat[!is.na(inpumat[, \"type\"]) & is.element(inpumat[, \"type\"], ltype[[ll]]), , drop=FALSE]\n    inpumat2 <- inpumat2[complete.cases(inpumat2), , drop=FALSE]\n    if (nrow(inpumat2) < 3 || length(sort(unique(inpumat2[, \"concentration\"]))) < 2) {\n      nc <- c(\"estimate\", \"se\", \"n\", \"tstat\", \"fstat\", \"pvalue\")\n      rest <- matrix(NA, nrow=nrow(data), ncol=length(nc), dimnames=list(rownames(data), nc))\n    } else {\n      if(nthread > 1) {\n        splitix <- parallel::splitIndices(nx=ncol(data), ncl=nthread)\n        splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n        mcres <- parallel::mclapply(splitix, function(x, data, inpumat) {\n          res <- t(apply(data[rownames(inpumat), x, drop=FALSE], 2, geneDrugPerturbation, concentration=inpumat[, \"concentration\"], type=inpumat[, \"type\"], batch=inpumat[, \"batch\"], duration=inpumat[,\"duration\"]))\n          return(res)\n        }, data=data, inpumat=inpumat2)\n        rest <- do.call(rbind, mcres)\n      } else {\n        rest <- t(apply(data[rownames(inpumat2), , drop=FALSE], 2, geneDrugPerturbation, concentration=inpumat2[, \"concentration\"], type=inpumat2[, \"type\"], batch=inpumat2[, \"batch\"], duration=inpumat2[,\"duration\"]))\n      }\n    }\n    rest <- cbind(rest, \"fdr\"=p.adjust(rest[, \"pvalue\"], method=\"fdr\"))\n    res <- c(res, list(rest))\n  }\n  names(res) <- names(ltype)\n  return(res)\n}"
      },
      {
        "partial": "geneDrugPerturbation <- function(x, concentration, type, batch, duration) {\n  # Implement the function to calculate drug perturbation statistics\n  # for a single gene\n  \n  # Hint: Use lm() to fit a linear model\n  \n  # Return a named vector with statistics\n}",
        "complete": "geneDrugPerturbation <- function(x, concentration, type, batch, duration) {\n  df <- data.frame(x = x, concentration = concentration, type = type, batch = batch, duration = duration)\n  model <- lm(x ~ concentration + type + batch + duration, data = df)\n  summary_model <- summary(model)\n  \n  estimate <- coef(model)[\"concentration\"]\n  se <- summary_model$coefficients[\"concentration\", \"Std. Error\"]\n  n <- nrow(df)\n  tstat <- summary_model$coefficients[\"concentration\", \"t value\"]\n  fstat <- summary_model$fstatistic[1]\n  pvalue <- summary_model$coefficients[\"concentration\", \"Pr(>|t|)\"]\n  \n  return(c(estimate = estimate, se = se, n = n, tstat = tstat, fstat = fstat, pvalue = pvalue))\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/pubchem_rest.R",
    "language": "R",
    "content": "#' Retrieve PubChem compound information\n#'\n#' This function retrieves compound information from PubChem using the PubChem REST API.\n#' Used by other functions to retrieve compound information.\n#'\n#' @param ids A vector of compound identifiers.\n#' @param from The source namespace of the compound identifiers. Default is 'cid'.\n#' @param to The target namespace for the compound information. Default is 'property'.\n#' @param properties A character vector specifying the properties to retrieve.\n#' @param raw Logical indicating whether to return the raw query results. Default is FALSE.\n#' @param query_only Logical indicating whether to only perform the query without retrieving the results. Default is FALSE.\n#' @param output The format of the query results. Default is 'JSON'.\n#' @param ... Additional arguments to be passed to the query_pubchem_rest function.\n#'\n#' @return A data.table containing the retrieved compound information.\n#'\n#' @examples\n#' properties <- c(\"Title\", \"MolecularFormula\", \"InChIKey\", \"CanonicalSMILES\")\n#' getPubchemCompound(c(3672, 176870), from = \"cid\", to = \"property\", properties = properties)\n#'\n#' @export\ngetPubchemCompound <- function(\n    ids, from = \"cid\", to = \"property\", properties = c(\"Title\", \"InChIKey\"),\n    raw = FALSE, query_only = FALSE, output = \"JSON\", ...\n) {\n\n  funContext <- .funContext(\"getPubchemCompound\")\n  to_ <- if (to == \"property\") {\n    checkmate::assert_atomic(properties, all.missing = FALSE)\n    checkmate::assert_character(properties)\n    to <- paste0(to, \"/\", paste0(properties, collapse = \",\"))\n  } else {\n    to\n  }\n\n  .info(funContext, \"Building PubChem REST queries...\")\n  requests <- lapply(ids, function(x) {\n    .build_pubchem_rest_query(\n      id = x, domain = \"compound\", namespace = from, operation = to_, output = output,\n      raw = raw, query_only = query_only, ...\n    )\n  })\n  if (query_only) return(requests)\n\n  tryCatch({\n    .info(funContext, \"Retrieving compound information...\")\n    resps_raw <- httr2::req_perform_sequential(\n      requests, \n      on_error = \"continue\", \n      progress = \"Querying PubCHEM REST API....\"\n    )\n    names(resps_raw) <- ids\n  }, error = function(e) {\n    .err(funContext, \" An error occurred while retrieving the compound information:\\n\", e)\n  })\n  \n  .debug(funContext, \" Number of responses: \", length(resps_raw))\n  if (raw) return(resps_raw)\n\n  # Parse the responses\n  .info(funContext, \"Parsing PubChem REST responses...\")\n  resps <- .parse_pubchem_rest_responses(resps_raw)\n\n  # filter failed \n  # if any query failed, return the failed queries as attributes\n  failed <- sapply(resps_raw, httr2::resp_is_error, USE.NAMES = T)\n  if (any(failed)) {\n    .warn(funContext, \" Some queries failed. See the 'failed' object for details.\")\n    failures <- lapply(resps_raw[failed], function(resp) {\n      .parse_resp_json(resp)$Fault\n    })\n  } else {\n    failures <- NULL\n  }\n\n  # Combine the responses\n  # might be able to just do the else part...\n  if (from != \"name\") { \n    responses <- data.table::rbindlist(resps, fill = TRUE)\n  } else {\n    responses <- data.table::rbindlist(resps, idcol = from, fill = TRUE)\n  }\n  data.table::setnames(responses, \"V1\", to, skip_absent = TRUE)\n\n  attributes(responses)$failed <- failures\n\n  return(responses)\n}\n\n\n#' Map compound names to PubChem CIDs\n#'\n#' This function maps compound names to PubChem CIDs using the PubChem REST API.\n#'\n#' @param names A character vector of compound names.\n#' @param first Logical indicating whether to return only the first CID for each compound name (default is FALSE).\n#' @param ... Additional arguments to be passed to the getPubchemCompound function.\n#'\n#' @return A character vector of PubChem CIDs.\n#'\n#' @examples\n#' mapCompound2CID(c(\"aspirin\", \"caffeine\"))\n#'\n#' @export\nmapCompound2CID <- function(\n    names, first = FALSE, ...) {\n  result <- getPubchemCompound(\n    ids = names, from = \"name\", to = \"cids\", ...\n  )\n\n  if (first) {\n    return(result[!duplicated(result$name), ])\n  } else {\n    return(result)\n  }\n}\n\n\n#' Map PubChem Compound IDs to Properties\n#'\n#' This function maps PubChem Compound IDs to specified properties using the PubChem REST API.\n#' See `getPubchemProperties` for a list of available properties.\n#'\n#' @param ids A vector of PubChem Compound IDs.\n#' @param properties A vector of property names to retrieve for each compound.\n#' @param ... Additional arguments to be passed to the `getPubchemCompound` function.\n#'\n#' @return A data frame containing the mapped properties for each compound.\n#'\n#' @examples\n#' mapCID2Properties(ids = c(123, 456), properties = c(\"MolecularWeight\", \"CanonicalSMILES\"))\n#'\n#' @export\nmapCID2Properties <- function(\n    ids, properties, ...) {\n  getPubchemCompound(\n    ids = ids, from = \"cid\", to = \"property\", properties = properties, ...\n  )\n}\n\n#' Retrieves the PubChem XML schema and extracts property information.\n#'\n#' This function retrieves the PubChem XML schema from the specified URL and\n#' extracts the property information from it. The property information includes\n#' the name and type of each property.\n#'\n#' @return A data table containing the extracted property information.\n#'\n#' @export\ngetPubchemProperties <- function() {\n  url <- \"https://pubchem.ncbi.nlm.nih.gov/pug_rest/pug_rest.xsd\"\n  response <- .build_request(url) |>\n    .perform_request()\n\n  node_list <- xml2::read_xml(response$body) |>\n    xml2::xml_children() |>\n    xml2::as_list()\n\n  properties <- node_list[[3]]$complexType$sequence$element$complexType$sequence\n\n  lapply(properties, function(x) {\n    list(\n      name = attr(x, \"name\"),\n      type = gsub(\"xs:\", \"\", attr(x, \"type\"))\n    ) |> .asDT()\n  }) |> data.table::rbindlist()\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `getPubchemCompound` function and what are its main parameters?",
        "answer": "The `getPubchemCompound` function retrieves compound information from PubChem using the PubChem REST API. Its main parameters are:\n- `ids`: A vector of compound identifiers\n- `from`: The source namespace of the compound identifiers (default 'cid')\n- `to`: The target namespace for the compound information (default 'property')\n- `properties`: A character vector specifying the properties to retrieve\n- `raw`: Logical indicating whether to return raw query results\n- `query_only`: Logical indicating whether to only perform the query without retrieving results"
      },
      {
        "question": "How does the `mapCompound2CID` function work and what does it return?",
        "answer": "The `mapCompound2CID` function maps compound names to PubChem CIDs using the PubChem REST API. It takes a character vector of compound names as input and returns a character vector of PubChem CIDs. The function uses `getPubchemCompound` internally with `from = 'name'` and `to = 'cids'`. If the `first` parameter is set to TRUE, it returns only the first CID for each compound name, otherwise it returns all matching CIDs."
      },
      {
        "question": "What is the purpose of the `getPubchemProperties` function and how does it work?",
        "answer": "The `getPubchemProperties` function retrieves the PubChem XML schema and extracts property information. It works as follows:\n1. It sends a request to the PubChem XML schema URL\n2. Parses the XML response\n3. Extracts property information including name and type\n4. Returns a data table containing the extracted property information\n\nThis function is useful for getting a list of available properties that can be used with other PubChem-related functions in the package."
      }
    ],
    "completion_tasks": [
      {
        "partial": "getPubchemCompound <- function(\n    ids, from = \"cid\", to = \"property\", properties = c(\"Title\", \"InChIKey\"),\n    raw = FALSE, query_only = FALSE, output = \"JSON\", ...\n) {\n  funContext <- .funContext(\"getPubchemCompound\")\n  to_ <- if (to == \"property\") {\n    checkmate::assert_atomic(properties, all.missing = FALSE)\n    checkmate::assert_character(properties)\n    to <- paste0(to, \"/\", paste0(properties, collapse = \",\"))\n  } else {\n    to\n  }\n\n  .info(funContext, \"Building PubChem REST queries...\")\n  requests <- lapply(ids, function(x) {\n    .build_pubchem_rest_query(\n      id = x, domain = \"compound\", namespace = from, operation = to_, output = output,\n      raw = raw, query_only = query_only, ...\n    )\n  })\n  if (query_only) return(requests)\n\n  # Complete the function here\n}",
        "complete": "getPubchemCompound <- function(\n    ids, from = \"cid\", to = \"property\", properties = c(\"Title\", \"InChIKey\"),\n    raw = FALSE, query_only = FALSE, output = \"JSON\", ...\n) {\n  funContext <- .funContext(\"getPubchemCompound\")\n  to_ <- if (to == \"property\") {\n    checkmate::assert_atomic(properties, all.missing = FALSE)\n    checkmate::assert_character(properties)\n    to <- paste0(to, \"/\", paste0(properties, collapse = \",\"))\n  } else {\n    to\n  }\n\n  .info(funContext, \"Building PubChem REST queries...\")\n  requests <- lapply(ids, function(x) {\n    .build_pubchem_rest_query(\n      id = x, domain = \"compound\", namespace = from, operation = to_, output = output,\n      raw = raw, query_only = query_only, ...\n    )\n  })\n  if (query_only) return(requests)\n\n  tryCatch({\n    .info(funContext, \"Retrieving compound information...\")\n    resps_raw <- httr2::req_perform_sequential(\n      requests, \n      on_error = \"continue\", \n      progress = \"Querying PubCHEM REST API....\"\n    )\n    names(resps_raw) <- ids\n  }, error = function(e) {\n    .err(funContext, \" An error occurred while retrieving the compound information:\\n\", e)\n  })\n  \n  .debug(funContext, \" Number of responses: \", length(resps_raw))\n  if (raw) return(resps_raw)\n\n  .info(funContext, \"Parsing PubChem REST responses...\")\n  resps <- .parse_pubchem_rest_responses(resps_raw)\n\n  failed <- sapply(resps_raw, httr2::resp_is_error, USE.NAMES = T)\n  if (any(failed)) {\n    .warn(funContext, \" Some queries failed. See the 'failed' object for details.\")\n    failures <- lapply(resps_raw[failed], function(resp) {\n      .parse_resp_json(resp)$Fault\n    })\n  } else {\n    failures <- NULL\n  }\n\n  responses <- if (from != \"name\") {\n    data.table::rbindlist(resps, fill = TRUE)\n  } else {\n    data.table::rbindlist(resps, idcol = from, fill = TRUE)\n  }\n  data.table::setnames(responses, \"V1\", to, skip_absent = TRUE)\n\n  attributes(responses)$failed <- failures\n\n  return(responses)\n}"
      },
      {
        "partial": "mapCompound2CID <- function(\n    names, first = FALSE, ...) {\n  result <- getPubchemCompound(\n    ids = names, from = \"name\", to = \"cids\", ...\n  )\n\n  # Complete the function here\n}",
        "complete": "mapCompound2CID <- function(\n    names, first = FALSE, ...) {\n  result <- getPubchemCompound(\n    ids = names, from = \"name\", to = \"cids\", ...\n  )\n\n  if (first) {\n    return(result[!duplicated(result$name), ])\n  } else {\n    return(result)\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/src/imgtools/utils/nnunet.py",
    "language": "py",
    "content": "from typing import Tuple, List\nimport os\nimport pathlib\nimport glob\nimport json\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef markdown_report_images(output_folder, modality_count):\n    modalities = list(modality_count.keys())\n    modality_totals = list(modality_count.values())\n    if not os.path.exists(pathlib.Path(output_folder, \"markdown_images\").as_posix()):\n        os.makedirs(pathlib.Path(output_folder, \"markdown_images\").as_posix())\n    plt.figure(1)\n    plt.bar(modalities, modality_totals)\n    plt.savefig(pathlib.Path(output_folder, \"markdown_images\", \"nnunet_modality_count.png\").as_posix())\n\n    plt.figure(2)\n    train_total = len(glob.glob(pathlib.Path(output_folder, \"labelsTr\", \"*.nii.gz\").as_posix()))\n    test_total = len(glob.glob(pathlib.Path(output_folder, \"labelsTs\", \"*.nii.gz\").as_posix()))\n    plt.pie([train_total, test_total], labels=[f\"Train - {train_total}\", f\"Test - {test_total}\"])\n    plt.savefig(pathlib.Path(output_folder, \"markdown_images\", \"nnunet_train_test_pie.png\").as_posix())\n\n# this code is taken from:\n# Division of Medical Image Computing, German Cancer Research Center (DKFZ)\n# in the nnUNet and batchgenerator repositories\n\n\ndef save_json(obj, file: str, indent: int = 4, sort_keys: bool = True) -> None:\n    with open(file, 'w') as f:\n        json.dump(obj, f, sort_keys=sort_keys, indent=indent)\n\n\ndef get_identifiers_from_splitted_files(folder: str):\n    uniques = np.unique([i[:-12] for i in subfiles(folder, suffix='.nii.gz', join=False)])\n    return uniques\n\n\ndef subfiles(folder: str, join: bool = True, prefix: str = None, suffix: str = None, sort: bool = True) -> List[str]:\n    if join:\n        path_fn = os.path.join\n    else:\n        def path_fn(x, y): return y\n        \n    res = [path_fn(folder, i) for i in os.listdir(folder) if os.path.isfile(os.path.join(folder, i))\n           and (prefix is None or i.startswith(prefix))\n           and (suffix is None or i.endswith(suffix))]\n    if sort:\n        res.sort()\n    return res\n\n\ndef generate_dataset_json(output_file: str, imagesTr_dir: str, imagesTs_dir: str, modalities: Tuple,\n                          labels: dict, dataset_name: str, sort_keys=True, license: str = \"hands off!\", dataset_description: str = \"\",\n                          dataset_reference=\"\", dataset_release='0.0'):\n    \"\"\"\n    :param output_file: This needs to be the full path to the dataset.json you intend to write, so\n    output_file='DATASET_PATH/dataset.json' where the folder DATASET_PATH points to is the one with the\n    imagesTr and labelsTr subfolders\n    :param imagesTr_dir: path to the imagesTr folder of that dataset\n    :param imagesTs_dir: path to the imagesTs folder of that dataset. Can be None\n    :param modalities: tuple of strings with modality names. must be in the same order as the images (first entry\n    corresponds to _0000.nii.gz, etc). Example: ('T1', 'T2', 'FLAIR').\n    :param labels: dict with int->str (key->value) mapping the label IDs to label names. Note that 0 is always\n    supposed to be background! Example: {0: 'background', 1: 'edema', 2: 'enhancing tumor'}\n    :param dataset_name: The name of the dataset. Can be anything you want\n    :param sort_keys: In order to sort or not, the keys in dataset.json\n    :param license:\n    :param dataset_description:\n    :param dataset_reference: website of the dataset, if available\n    :param dataset_release:\n    :return:\n    \"\"\"\n    train_identifiers = get_identifiers_from_splitted_files(imagesTr_dir)\n\n    if imagesTs_dir is not None:\n        test_identifiers = get_identifiers_from_splitted_files(imagesTs_dir)\n    else:\n        test_identifiers = []\n\n    json_dict = {}\n    json_dict['name'] = dataset_name\n    json_dict['description'] = dataset_description\n    json_dict['tensorImageSize'] = \"4D\"\n    json_dict['reference'] = dataset_reference\n    json_dict['licence'] = license\n    json_dict['release'] = dataset_release\n    json_dict['modality'] = {str(i): modalities[i] for i in range(len(modalities))}\n    json_dict['labels'] = {str(i): labels[i] for i in labels.keys()}\n\n    json_dict['numTraining'] = len(train_identifiers)\n    json_dict['numTest'] = len(test_identifiers)\n    json_dict['training'] = [\n        {'image': \"./imagesTr/%s.nii.gz\" % i, \"label\": \"./labelsTr/%s.nii.gz\" % i} for i\n        in\n        train_identifiers]\n    json_dict['test'] = [\"./imagesTs/%s.nii.gz\" % i for i in test_identifiers]\n\n    if not output_file.endswith(\"dataset.json\"):\n        print(\"WARNING: output file name is not dataset.json! This may be intentional or not. You decide. \"\n              \"Proceeding anyways...\")\n    save_json(json_dict, os.path.join(output_file), sort_keys=sort_keys)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `markdown_report_images` function in the given code snippet?",
        "answer": "The `markdown_report_images` function generates and saves two visualization images for a report: 1) A bar chart showing the count of different modalities, and 2) A pie chart displaying the distribution of training and test data. It creates these images in a 'markdown_images' subfolder within the specified output folder."
      },
      {
        "question": "How does the `subfiles` function handle file filtering and sorting?",
        "answer": "The `subfiles` function filters files in a given folder based on optional prefix and suffix parameters. It can join the folder path with filenames or return just the filenames. The function also sorts the resulting list by default, but this can be disabled with the `sort` parameter set to False. It uses list comprehension with conditional checks to efficiently filter the files."
      },
      {
        "question": "What is the primary purpose of the `generate_dataset_json` function and what key information does it include in the output JSON?",
        "answer": "The `generate_dataset_json` function creates a JSON file that describes a medical imaging dataset. It includes information such as dataset name, description, image tensor size, modalities, labels, number of training and test samples, and file paths for training and test images. This JSON serves as a standardized way to document the dataset's structure and contents, which is crucial for reproducibility and ease of use in machine learning pipelines."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "glob",
        "json",
        "numpy",
        "matplotlib.pyplot"
      ],
      "from_imports": [
        "typing.Tuple"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/plotPSig.R",
    "language": "R",
    "content": "#' Plots a PharmacoSig object into a Volcano Plot\n#' \n#' Given a PharmacoSig, this will plot a volcano plot, with parameters to set cutoffs \n#' for a significant effect size, p value, to pick multiple testing correction strategy, \n#' and to change point colors. Built on top of ggplot, it will return the plot object which\n#' can be easily customized as any other ggplot. \n#' \n#' @examples\n#' data(GDSCsmall)\n#' drug.sensitivity <- drugSensitivitySig(GDSCsmall, mDataType=\"rna\", \n#'              nthread=1, features = fNames(GDSCsmall, \"rna\")[1])\n#' plot(drug.sensitivity)\n#' \n#' @param x `PharmacoSig` a PharmacoSig object, result of drugSensitivitySig\n#'  or drugPerturbationSig\n#' @param adjust.method `character(1)` or `logical(1)` either FALSE for no adjustment,\n#' or one of the methods implemented by p.adjust. Defaults to FALSE for no \n#' correction\n#' @param drugs `character` a vector of drug names for which to plot the estimated\n#' associations with gene expression \n#' @param features `character` a vector of features for which to plot the estimated\n#' associations with drug treatment \n#' @param effect_cutoff the cutoff to use for coloring significant effect sizes. \n#' @param signif_cutoff the cutoff to use for coloring significance by p value or\n#' adjusted p values. Not on log scale.\n#' @param color one color if no cutoffs set for plotting. A vector of colors otherwise\n#' used to color points the in three categories above. \n#' @param ... additional arguments, not currently used, but left here for consistency with plot\n#' @return returns a ggplot object, which by default will be evaluated and the plot displayed, or\n#' can be saved to a variable for further customization by adding ggplot elements to the returned\n#' graph\n#'\n#' @export\n#' @import ggplot2\n#' @include class-SignatureClass.R\n#' @method plot PharmacoSig\nplot.PharmacoSig <- function(x, adjust.method, drugs, features, effect_cutoff, signif_cutoff, color, ...){\n\tdots <- list(...)\n\tndots <- length(dots)\n\t\n\t# if(length(dim(x))==2){\n\t# \tdim(x) <- c(1, dim(x))\n\t# } else if(length(dim(x)) == 1) {\n\t# \tdim(x) <- c(1, 1, dim(x))\n\t# }\n\n\tif(missing(adjust.method)){\n\t\tadjust.method <- FALSE\n\t}\n\n\tif(missing(drugs)){\n\t\tdrugs <- colnames(x)\n\t}\n\n\tif(missing(features)){\n\t\tfeatures <- rownames(x)\n\t}\n\n\tif(!missing(color)){\n\t\tif(!is.null(dots[[\"colour\"]])){\n\t\t\twarning(\"Both color and colour parameters provided. Will take union of both. This is probably a mistake.\")\n\t\t\tcolor <- union(color, dots[[\"colour\"]])\n\t\t}\n\t} else if (!is.null(dots[[\"colour\"]])){\n\t\tcolor <- dots[[\"colour\"]]\n\t} ## Case if both missing handled in logic below\n\n\n\tif(isFALSE(adjust.method)){\n\t\tp.adjust.f <- function(x) return(x)\n\t} else {\n\t\tp.adjust.f <- function(x) return(p.adjust(x, method=adjust.method))\n\t}\n\n\tx.m <- data.frame(X = as.vector(x[features,drugs,c(\"estimate\")]), \n\t\t\t\t\t  Y = -log10(p.adjust.f(as.vector(x[features,drugs,c(\"pvalue\")]))))\n\taxis.labs <- c(\"Estimate\", ifelse(isFALSE(adjust.method) || adjust.method == \"none\", \"-Log10 P Value\", \"-Log10 Corrected P Value\"))\n\n\n\tplot.elements <- ggplot() + xlab(axis.labs[1]) + ylab(axis.labs[2])\n\n\t\n\tif(!missing(effect_cutoff) | !missing(signif_cutoff)) {\n\t\tx.m$Cutoff <- \"Not Significant\"\n\n\t\tif(!missing(signif_cutoff)){\n\t\t\tx.m$Cutoff[x.m$Y >= -log10(signif_cutoff)] <- \"Significant P Value\"\n\n\t\t\tif(!missing(effect_cutoff)){\n\t\t\t\tx.m$Cutoff[(x.m$Y >= -log10(signif_cutoff)) & (abs(x.m$X) >= effect_cutoff)] <- \"Significant P Value and Effect\"\n\t\t\t}\n\n\t\t} else {\n\t\t\tx.m$Cutoff[(abs(x.m$X) >= effect_cutoff)] <- \"Significant Effect\"\n\t\t}\n\n\t\tplot.elements <- plot.elements + geom_point(aes(X, Y, color = Cutoff), data=x.m)\n\n\t\tif(!missing(color)){ ## this is handled here because we want different behaviour based on if we have significance based coloring or not\n\t\t\tplot.elements <- plot.elements + scale_colour_manual(values = color)\n\t\t}\n\n\t} else {\n\n\t\tif(missing(color)){ ## this is handled here because we want different behaviour based on if we have significance based coloring or not\n\t\t\tcolor <- \"black\"\n\t\t}\n\n\t\tx.m$Cutoff <- NA_character_\n\n\t\tplot.elements <- plot.elements + geom_point(aes(X, Y), color = color, data=x.m)\n\t}\n\n\n\tplot.elements\n}\n\n#  Plots a PharmacoSig object into a Volcano Plot\n# \n# \n# @S3method plot PharmacoSig\nsetMethod(\"plot\", \"PharmacoSig\", plot.PharmacoSig)\n\n\n\n",
    "qa_pairs": [
      {
        "question": "What is the main purpose of the `plot.PharmacoSig` function, and what type of plot does it generate?",
        "answer": "The `plot.PharmacoSig` function is designed to plot a PharmacoSig object into a Volcano Plot. It visualizes the relationship between effect size (estimates) and statistical significance (-log10 p-values) for drug sensitivity or perturbation signatures. The function returns a ggplot object, allowing for further customization."
      },
      {
        "question": "How does the function handle multiple testing correction, and what options are available for this correction?",
        "answer": "The function handles multiple testing correction through the `adjust.method` parameter. If `adjust.method` is set to FALSE (default), no correction is applied. Otherwise, it can be set to any method implemented by the `p.adjust` function. The correction is applied using `p.adjust.f <- function(x) return(p.adjust(x, method=adjust.method))`. The y-axis label is updated accordingly to reflect whether corrected p-values are used."
      },
      {
        "question": "How does the function determine the color of points in the plot, and what parameters control this behavior?",
        "answer": "The function determines point colors based on the `effect_cutoff` and `signif_cutoff` parameters. If either of these is provided, points are categorized as 'Not Significant', 'Significant P Value', 'Significant Effect', or 'Significant P Value and Effect'. The `color` parameter can be used to specify custom colors for these categories. If no cutoffs are set, all points are colored using a single color (default is black if not specified). The coloring logic is implemented using conditional statements and the `scale_colour_manual` function from ggplot2."
      }
    ],
    "completion_tasks": [
      {
        "partial": "plot.PharmacoSig <- function(x, adjust.method, drugs, features, effect_cutoff, signif_cutoff, color, ...) {\n  if(missing(adjust.method)) {\n    adjust.method <- FALSE\n  }\n  \n  if(missing(drugs)) {\n    drugs <- colnames(x)\n  }\n  \n  if(missing(features)) {\n    features <- rownames(x)\n  }\n  \n  # Complete the function to create a volcano plot using ggplot2\n  # Handle missing parameters, create the data frame, and return the plot\n}",
        "complete": "plot.PharmacoSig <- function(x, adjust.method, drugs, features, effect_cutoff, signif_cutoff, color, ...) {\n  if(missing(adjust.method)) {\n    adjust.method <- FALSE\n  }\n  \n  if(missing(drugs)) {\n    drugs <- colnames(x)\n  }\n  \n  if(missing(features)) {\n    features <- rownames(x)\n  }\n  \n  p.adjust.f <- if(isFALSE(adjust.method)) function(x) x else function(x) p.adjust(x, method=adjust.method)\n  \n  x.m <- data.frame(\n    X = as.vector(x[features, drugs, \"estimate\"]),\n    Y = -log10(p.adjust.f(as.vector(x[features, drugs, \"pvalue\"])))\n  )\n  \n  axis.labs <- c(\"Estimate\", ifelse(isFALSE(adjust.method) || adjust.method == \"none\", \"-Log10 P Value\", \"-Log10 Corrected P Value\"))\n  \n  plot <- ggplot(x.m, aes(X, Y)) +\n    xlab(axis.labs[1]) + ylab(axis.labs[2])\n  \n  if(!missing(effect_cutoff) | !missing(signif_cutoff)) {\n    x.m$Cutoff <- \"Not Significant\"\n    if(!missing(signif_cutoff)) {\n      x.m$Cutoff[x.m$Y >= -log10(signif_cutoff)] <- \"Significant P Value\"\n      if(!missing(effect_cutoff)) {\n        x.m$Cutoff[(x.m$Y >= -log10(signif_cutoff)) & (abs(x.m$X) >= effect_cutoff)] <- \"Significant P Value and Effect\"\n      }\n    } else if(!missing(effect_cutoff)) {\n      x.m$Cutoff[abs(x.m$X) >= effect_cutoff] <- \"Significant Effect\"\n    }\n    plot <- plot + geom_point(aes(color = Cutoff))\n    if(!missing(color)) {\n      plot <- plot + scale_colour_manual(values = color)\n    }\n  } else {\n    plot <- plot + geom_point(color = if(missing(color)) \"black\" else color)\n  }\n  \n  return(plot)\n}"
      },
      {
        "partial": "setMethod(\"plot\", \"PharmacoSig\", function(x, ...) {\n  # Implement the plot method for PharmacoSig objects\n  # Call the plot.PharmacoSig function with appropriate arguments\n})",
        "complete": "setMethod(\"plot\", \"PharmacoSig\", function(x, ...) {\n  plot.PharmacoSig(x, ...)\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/intersectPSets.R",
    "language": "R",
    "content": "#' Intersects objects of the PharmacoSet class, subsetting them to the common\n#' drugs and/or cell lines as selected by the user.\n#'\n#' Given a list of PharmacoSets, the function will find the common drugs,\n#' and/or cell lines, and return PharmacoSets that contain data only pertaining\n#' to the common drugs, and/or cell lines. The mapping between dataset\n#' drug and cell names is done using annotations found in the\n#' PharmacoSet object's internal curation slot\n#'\n#' @examples\n#' data(GDSCsmall)\n#' data(CCLEsmall)\n#' common <- intersectPSet(list('GDSC'=GDSCsmall,'CCLE'=CCLEsmall),\n#'                         intersectOn = c(\"drugs\", \"cell.lines\"))\n#' common$CGP\n#' common$CCLE\n#'\n#' @param pSets \\code{list} a list of PharmacoSet objects, of which the function\n#'   should find the intersection\n#' @param intersectOn \\code{character} which identifiers to intersect on,\n#'   drugs, cell lines, or concentrations\n#' @param drugs a \\code{character} vector of common drugs between pSets.\n#' In case user is intersted on getting intersection on certain drugs,\n#' they can provide their list of drugs.\n#' @param cells a \\code{character}vector of common cell lines between pSets.\n#' In case user is intersted on getting intersection on certain cell lines,\n#' they can provide their list of cell lines\n#' @param strictIntersect \\code{boolean} Should the intersection keep only the drugs\n#'   and cell lines that have been tested on together?\n#' @param verbose \\code{boolean} Should the function announce its key steps?\n#' @param nthread \\code{numeric} The number of cores to use to run intersection on\n#'   concentrations\n#'\n#' @return A \\code{list} of pSets, contatining only the intersection\n#'\n#' @importFrom S4Vectors metadata\n#' @importFrom SummarizedExperiment colData\n#' @importFrom CoreGx .intersectList\n#'\n#' @export\nintersectPSet <- function(pSets,\n           intersectOn=c(\"drugs\", \"cell.lines\", \"concentrations\"),\n           cells,\n           drugs,\n           strictIntersect=FALSE, verbose=TRUE, nthread=1)\n{\n  if (verbose) {\n    message(\"Intersecting large PSets may take a long time ...\")\n  }\n\n  if(\"concentrations\" %in% intersectOn && anyNA(sapply(pSets, function(x) return(sensitivityRaw(x))))) {\n    stop(\"Intersecting on concentrations requires all PSets to have raw data included.\")\n  }\n  ## TODO: Fix the strict intersection!!!!!!\n  if (length(pSets) == 1) {\n    return(pSets)\n  }\n  if (length(pSets) > 1) {\n    if(is.null(names(pSets)) ){\n\n      names(pSets) <- sapply(pSets, name)\n\n    }\n    if (\"drugs\" %in% intersectOn){\n      common.drugs <- .intersectList(lapply(pSets, function(x) return(treatmentNames(x))))\n      if(!missing(drugs)) {\n        common.drugs <- intersect(common.drugs, drugs)\n      }\n      if (length(common.drugs) == 0) {\n        stop(\"No drugs is in common between pSets!\")\n      }\n    }\n    if (\"cell.lines\" %in% intersectOn){\n      common.cells <- .intersectList(lapply(pSets, function(x){return(sampleNames(x))}))\n      if(!missing(cells)) {\n        common.cells <- intersect(common.cells, cells)\n      }\n      if (length(common.cells) == 0) {\n        stop(\"No cell lines is in common between pSets!\")\n      }\n    }\n    if ((\"drugs\" %in% intersectOn) & (\"cell.lines\" %in% intersectOn)) {\n      common.exps <- .intersectList(lapply(pSets, function (x){\n        if (\"sampleid\" %in% colnames(sensitivityInfo(x)) & \"treatmentid\" %in% colnames(sensitivityInfo(x))) {\n          paste(sensitivityInfo(x)$sampleid, sensitivityInfo(x)$treatmentid, sep = \"_\")\n        } else { NULL }\n      }))\n      # expMatch <- data.frame(lapply(pSets,\n      #   function (x, common.exps){\n      #     if (\"sampleid\" %in% colnames(sensitivityInfo(x)) & \"treatmentid\" %in% colnames(sensitivityInfo(x))){\n\n      #       myx <- match(paste(sensitivityInfo(x)$sampleid, sensitivityInfo(x)$treatmentid, sep = \"_\") ,common.exps)\n\n      #       res <- rownames(sensitivityInfo(x))[!is.na(myx)]\n\n      #       names(res) <- common.exps[na.omit(myx)]\n\n      #       res <- res[common.exps]\n\n      #       return(res)\n\n      #     } else { NULL }\n      #   }, common.exps=common.exps))\n      expMatch <- lapply(pSets,\n                         function (x, common.exps){\n                           if (\"sampleid\" %in% colnames(sensitivityInfo(x)) & \"treatmentid\" %in% colnames(sensitivityInfo(x))){\n\n                             myx <- match(paste(sensitivityInfo(x)$sampleid, sensitivityInfo(x)$treatmentid, sep = \"_\") ,common.exps)\n\n                             res <- rownames(sensitivityInfo(x))[!is.na(myx)]\n\n                             names(res) <- common.exps[na.omit(myx)]\n\n                             res <- res[common.exps]\n\n                             return(res)\n\n                           } else { NULL }\n                         }, common.exps=common.exps)\n      # }, common.exps=common.exps)\n\n      if(strictIntersect){\n        if(length(unique(sapply(expMatch, length)))>1){\n          stop(\"Strict Intersecting works only when each PSet has 1 replicate per cell-drug pair. Use collapseSensitvityReplicates to reduce the sensitivity data as required\")\n        }\n        expMatch <- data.frame(expMatch,  stringsAsFactors=FALSE)\n        # expMatch2 <- as.matrix(expMatch2)\n        rownames(expMatch) <- common.exps\n        colnames(expMatch) <- names(pSets)\n\n      } else {\n\n        expMatch <- lapply(expMatch, function(x){names(x) <- x; return(x)})\n      }\n    }\n    if ((\"drugs\" %in% intersectOn) & (\"cell.lines\" %in% intersectOn) & (\"concentrations\" %in% intersectOn)) {\n\n      if(length(unique(sapply(expMatch, length)))>1){\n        stop(\"Intersecting on concentrations works only when each PSet has 1 replicate per cell-drug pair. Use collapseSensitvityReplicates to reduce the sensitivity data as required\")\n      }\n\n      expMatch <- data.frame(expMatch,  stringsAsFactors=FALSE)\n      # expMatch2 <- as.matrix(expMatch2)\n      rownames(expMatch) <- common.exps\n      colnames(expMatch) <- names(pSets)\n\n      pSets <- .calculateSensitivitiesStar(pSets, exps=expMatch, cap=100, nthread=nthread)\n    }\n    if (\"cell.lines\" %in% intersectOn)\n      {\n      molecular.types  <- NULL\n      for (pSet in pSets)\n        {\n        for (SE in molecularProfilesSlot(pSet)) {\n          molecular.types <- union(molecular.types, ifelse (\n            length(grep(\"rna\", S4Vectors::metadata(SE)$annotation) > 0),\n            \"rna\", S4Vectors::metadata(SE)$annotation))\n        }\n      }\n      common.molecular.cells <- list()\n      for (molecular.type in molecular.types)\n        {\n        if(strictIntersect){\n          common.molecular.cells[[molecular.type]] <-\n            .intersectList(lapply(pSets, function (pSet)\n            {\n              SEs <- names(unlist(sapply(molecularProfilesSlot(pSet), function(SE)\n              {\n                grep(molecular.type, S4Vectors::metadata(SE)$annotation)})))\n                if(length(SEs) > 0)\n                {\n                  return(.intersectList(sapply(SEs, function(SE)\n                  {\n                    if (length(grep(\n                      molecular.type, S4Vectors::metadata(\n                        molecularProfilesSlot(pSet)[[SE]])$annotation)) > 0)\n                      {\n                      intersect(colData(molecularProfilesSlot(pSet)[[SE]])$sampleid, common.cells)\n                      }\n                    })))\n                }\n            }))\n        }else{\n          common.molecular.cells[[molecular.type]] <-\n            .intersectList(lapply(pSets, function (pSet) {\n              SEs <- names(unlist(sapply(molecularProfilesSlot(pSet), function(SE)\n                {\n                grep(molecular.type, S4Vectors::metadata(SE)$annotation)})))\n                return(CoreGx::.unionList(sapply(SEs, function(SE)\n                  {\n                  if (length(grep(molecular.type, S4Vectors::metadata(molecularProfilesSlot(pSet)[[SE]])$annotation)) > 0)\n                    {\n                    intersect(SummarizedExperiment::colData(molecularProfilesSlot(pSet)[[SE]])$sampleid, common.cells)\n                    }\n                  })))\n                }))\n              }\n        }\n    }\n\n\n\n\n    for (i in seq_along(pSets)) {\n      if((\"drugs\" %in% intersectOn) & (\"cell.lines\" %in% intersectOn)){\n        if(strictIntersect){\n          pSets[[i]] <- subsetTo(pSets[[i]], drugs=common.drugs, cells=common.cells, exps=expMatch, molecular.data.cells=common.molecular.cells)\n\n        } else {\n          pSets[[i]] <- subsetTo(pSets[[i]], drugs=common.drugs, cells=common.cells, molecular.data.cells=common.molecular.cells)\n        }\n      } else if((\"cell.lines\" %in% intersectOn)) {\n        pSets[[i]] <- subsetTo(pSets[[i]], cells=common.cells, molecular.data.cells=common.molecular.cells)\n\n      } else if((\"drugs\" %in% intersectOn)) {\n        pSets[[i]] <- subsetTo(pSets[[i]], drugs=common.drugs)\n\n      }\n    }\n    return(pSets)\n  }\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `intersectPSet` function and what are its main parameters?",
        "answer": "The `intersectPSet` function is designed to intersect objects of the PharmacoSet class, subsetting them to common drugs and/or cell lines. Its main parameters are:\n- `pSets`: A list of PharmacoSet objects to intersect\n- `intersectOn`: Specifies which identifiers to intersect on (drugs, cell lines, or concentrations)\n- `cells` and `drugs`: Optional vectors to specify certain cell lines or drugs for intersection\n- `strictIntersect`: Boolean to determine if only drugs and cell lines tested together should be kept\n- `verbose`: Boolean to control function announcements\n- `nthread`: Number of cores to use for concentration intersection"
      },
      {
        "question": "How does the function handle the intersection of drugs and cell lines when both are specified in the `intersectOn` parameter?",
        "answer": "When both 'drugs' and 'cell.lines' are specified in `intersectOn`, the function:\n1. Finds common drugs using `.intersectList` on `treatmentNames`\n2. Finds common cell lines using `.intersectList` on `sampleNames`\n3. Creates `common.exps` by combining sample IDs and treatment IDs\n4. Generates `expMatch` to map common experiments across datasets\n5. If `strictIntersect` is TRUE, ensures each PSet has 1 replicate per cell-drug pair\n6. Subsets each PSet in the list to include only the common drugs, cell lines, and experiments"
      },
      {
        "question": "What additional steps does the function take when 'concentrations' is included in the `intersectOn` parameter along with 'drugs' and 'cell.lines'?",
        "answer": "When 'concentrations' is included along with 'drugs' and 'cell.lines' in `intersectOn`, the function:\n1. Checks if all PSets have raw data included\n2. Ensures each PSet has 1 replicate per cell-drug pair\n3. Creates a data frame `expMatch` with common experiments\n4. Calls `.calculateSensitivitiesStar` function to recalculate sensitivities based on the common concentrations\n5. This step allows for a more precise intersection that takes into account the specific drug concentrations used in each experiment across datasets"
      }
    ],
    "completion_tasks": [
      {
        "partial": "intersectPSet <- function(pSets,\n           intersectOn=c(\"drugs\", \"cell.lines\", \"concentrations\"),\n           cells,\n           drugs,\n           strictIntersect=FALSE, verbose=TRUE, nthread=1)\n{\n  if (verbose) {\n    message(\"Intersecting large PSets may take a long time ...\")\n  }\n\n  if(\"concentrations\" %in% intersectOn && anyNA(sapply(pSets, function(x) return(sensitivityRaw(x))))) {\n    stop(\"Intersecting on concentrations requires all PSets to have raw data included.\")\n  }\n\n  if (length(pSets) == 1) {\n    return(pSets)\n  }\n\n  if (length(pSets) > 1) {\n    if(is.null(names(pSets)) ){\n      names(pSets) <- sapply(pSets, name)\n    }\n\n    # TODO: Implement the rest of the function\n  }\n}",
        "complete": "intersectPSet <- function(pSets,\n           intersectOn=c(\"drugs\", \"cell.lines\", \"concentrations\"),\n           cells,\n           drugs,\n           strictIntersect=FALSE, verbose=TRUE, nthread=1)\n{\n  if (verbose) {\n    message(\"Intersecting large PSets may take a long time ...\")\n  }\n\n  if(\"concentrations\" %in% intersectOn && anyNA(sapply(pSets, function(x) return(sensitivityRaw(x))))) {\n    stop(\"Intersecting on concentrations requires all PSets to have raw data included.\")\n  }\n\n  if (length(pSets) == 1) {\n    return(pSets)\n  }\n\n  if (length(pSets) > 1) {\n    if(is.null(names(pSets)) ){\n      names(pSets) <- sapply(pSets, name)\n    }\n\n    if (\"drugs\" %in% intersectOn){\n      common.drugs <- .intersectList(lapply(pSets, function(x) return(treatmentNames(x))))\n      if(!missing(drugs)) {\n        common.drugs <- intersect(common.drugs, drugs)\n      }\n      if (length(common.drugs) == 0) {\n        stop(\"No drugs is in common between pSets!\")\n      }\n    }\n\n    if (\"cell.lines\" %in% intersectOn){\n      common.cells <- .intersectList(lapply(pSets, function(x){return(sampleNames(x))}))\n      if(!missing(cells)) {\n        common.cells <- intersect(common.cells, cells)\n      }\n      if (length(common.cells) == 0) {\n        stop(\"No cell lines is in common between pSets!\")\n      }\n    }\n\n    # ... (rest of the function implementation)\n\n    for (i in seq_along(pSets)) {\n      if((\"drugs\" %in% intersectOn) & (\"cell.lines\" %in% intersectOn)){\n        if(strictIntersect){\n          pSets[[i]] <- subsetTo(pSets[[i]], drugs=common.drugs, cells=common.cells, exps=expMatch, molecular.data.cells=common.molecular.cells)\n        } else {\n          pSets[[i]] <- subsetTo(pSets[[i]], drugs=common.drugs, cells=common.cells, molecular.data.cells=common.molecular.cells)\n        }\n      } else if((\"cell.lines\" %in% intersectOn)) {\n        pSets[[i]] <- subsetTo(pSets[[i]], cells=common.cells, molecular.data.cells=common.molecular.cells)\n      } else if((\"drugs\" %in% intersectOn)) {\n        pSets[[i]] <- subsetTo(pSets[[i]], drugs=common.drugs)\n      }\n    }\n    return(pSets)\n  }\n}"
      },
      {
        "partial": "if (\"drugs\" %in% intersectOn & \"cell.lines\" %in% intersectOn) {\n  common.exps <- .intersectList(lapply(pSets, function (x){\n    if (\"sampleid\" %in% colnames(sensitivityInfo(x)) & \"treatmentid\" %in% colnames(sensitivityInfo(x))) {\n      paste(sensitivityInfo(x)$sampleid, sensitivityInfo(x)$treatmentid, sep = \"_\")\n    } else { NULL }\n  }))\n\n  expMatch <- lapply(pSets, function (x, common.exps){\n    # TODO: Implement the rest of the function\n  }, common.exps=common.exps)\n\n  if(strictIntersect){\n    # TODO: Implement strict intersect logic\n  } else {\n    # TODO: Implement non-strict intersect logic\n  }\n}",
        "complete": "if (\"drugs\" %in% intersectOn & \"cell.lines\" %in% intersectOn) {\n  common.exps <- .intersectList(lapply(pSets, function (x){\n    if (\"sampleid\" %in% colnames(sensitivityInfo(x)) & \"treatmentid\" %in% colnames(sensitivityInfo(x))) {\n      paste(sensitivityInfo(x)$sampleid, sensitivityInfo(x)$treatmentid, sep = \"_\")\n    } else { NULL }\n  }))\n\n  expMatch <- lapply(pSets, function (x, common.exps){\n    if (\"sampleid\" %in% colnames(sensitivityInfo(x)) & \"treatmentid\" %in% colnames(sensitivityInfo(x))){\n      myx <- match(paste(sensitivityInfo(x)$sampleid, sensitivityInfo(x)$treatmentid, sep = \"_\") ,common.exps)\n      res <- rownames(sensitivityInfo(x))[!is.na(myx)]\n      names(res) <- common.exps[na.omit(myx)]\n      res <- res[common.exps]\n      return(res)\n    } else { NULL }\n  }, common.exps=common.exps)\n\n  if(strictIntersect){\n    if(length(unique(sapply(expMatch, length)))>1){\n      stop(\"Strict Intersecting works only when each PSet has 1 replicate per cell-drug pair. Use collapseSensitvityReplicates to reduce the sensitivity data as required\")\n    }\n    expMatch <- data.frame(expMatch,  stringsAsFactors=FALSE)\n    rownames(expMatch) <- common.exps\n    colnames(expMatch) <- names(pSets)\n  } else {\n    expMatch <- lapply(expMatch, function(x){names(x) <- x; return(x)})\n  }\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/med-imagetools.git",
    "file": "../../../../repos/med-imagetools/tests/test_ops.py",
    "language": "py",
    "content": "import os\nimport pathlib\nimport pytest\nimport SimpleITK as sitk\nimport numpy as np\nimport h5py\nfrom imgtools.ops import *\n\n@pytest.fixture(scope=\"session\")\ndef output_path():\n    curr_path = pathlib.Path(__file__).parent.parent.resolve()\n    out_path = pathlib.Path(curr_path, \"temp_outputs\").as_posix()\n    if not os.path.exists(out_path):\n        os.makedirs(out_path)\n    return out_path\n\nimg_shape   = (100, 100, 100)\ndirection   = (1, 0, 0, 0, 1, 0, 0, 0, 1)\norigin      = (37, 37, 37)\nspacing     = (.37, .37, .37)\n\n# build blank image sample\nblank = sitk.Image(img_shape, sitk.sitkInt16)#np.zeros((100,100,100))\nblank.SetDirection(direction)\nblank.SetOrigin(origin)\nblank.SetSpacing(spacing)\n\nclass TestOutput:\n    @pytest.mark.parametrize(\"op\", [NumpyOutput, HDF5Output])#, \"CT,RTDOSE,PT\"])\n    def test_output(self, op, output_path):\n        # get class name\n        class_name = op.__name__\n        \n        # save output\n        saver = op(output_path, create_dirs=False)\n        saver(class_name, blank)\n        saved_path = pathlib.Path(output_path, saver.filename_format.format(subject_id=class_name)).as_posix()\n        \n        # check output\n        if class_name == \"HDF5Output\":\n            f = h5py.File(saved_path, \"r\")\n            img = f['image']\n            assert tuple(img.attrs['origin'])    == origin\n            assert tuple(img.attrs['direction']) == direction\n            assert tuple(img.attrs['spacing'])   == spacing\n        elif class_name == \"NumpyOutput\":\n            img = np.load(saved_path)\n        \n        # class-agnostic\n        assert img.shape == img_shape\n        \n        \nclass TestTransform:\n    @pytest.mark.parametrize(\"op,params\", [(Resample, {\"spacing\": 3.7}), \n                                           (Resize, {\"size\": 10}), \n                                           (Zoom, {\"scale_factor\": .1}), \n                                           (Crop, {\"crop_centre\": (20, 20, 20), \"size\": 10}), \n                                           (CentreCrop, {\"size\": 10})])\n    def test_transform(self, op, params):\n        transform = op(**params)\n        new_img   = transform(blank)\n        \n        # check output\n        # resample\n        if isinstance(transform, Resample):\n            assert new_img.GetSpacing() == (3.7, 3.7, 3.7)\n        \n        # not zoom\n        if not isinstance(transform, Zoom):\n            assert new_img.GetSize() == (10, 10, 10)\n\n        # zoom\n        if isinstance(transform, Zoom):\n            assert new_img.GetSize() == (100, 100, 100) \n            assert new_img.GetSpacing() == (.37, .37, .37)\n\nclass TestIntensity:\n    @pytest.mark.parametrize(\"op,params\", [(ClipIntensity, {\"lower\": 0, \"upper\": 500}),\n                                           (WindowIntensity, {\"window\": 500, \"level\": 250}),\n                                           (StandardScale, {}),\n                                           (MinMaxScale, {\"minimum\": 0, \"maximum\": 1000})])\n    def test_intesity(self, op, params):\n        img_cube = copy.deepcopy(blank)\n        img_cube[5:15,5:15,5:15] = 1000\n\n        intensify = op(**params)\n        new_img   = intensify(img_cube)\n        stats = ImageStatistics()(new_img)\n\n        # check output\n        if isinstance(intensify, (ClipIntensity, WindowIntensity)):\n            assert stats.sum == 5e5\n        elif isinstance(intensify, StandardScale):\n            assert np.allclose(stats.mean, 0.)\n            assert np.allclose(stats.standard_deviation, 1., rtol=1e-1)     \n            print(stats.minimum, stats.maximum)",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `@pytest.fixture(scope=\"session\")` decorator in the given code snippet, and how does it relate to the `output_path` function?",
        "answer": "The `@pytest.fixture(scope=\"session\")` decorator is used to create a fixture named `output_path`. This fixture has a session scope, meaning it will be created once for the entire test session and shared across all test functions that require it. The `output_path` function creates a temporary directory for test outputs, ensuring it exists before running the tests. This fixture can be used by multiple test functions to access a common output directory without recreating it for each test."
      },
      {
        "question": "Explain the purpose of the `TestOutput` class and how it uses parameterization to test different output types.",
        "answer": "The `TestOutput` class is designed to test different output methods for saving image data. It uses pytest's parameterization (`@pytest.mark.parametrize`) to run the same test with different output classes (`NumpyOutput` and `HDF5Output`). The test method `test_output` creates an instance of the specified output class, saves a blank image, and then verifies that the saved image has the correct properties (shape, origin, direction, and spacing) depending on the output type. This approach allows testing multiple output formats with a single test method, reducing code duplication and ensuring consistent testing across different output types."
      },
      {
        "question": "How does the `TestTransform` class handle different image transformation operations, and what assertions are made to verify the correctness of each transformation?",
        "answer": "The `TestTransform` class tests various image transformation operations using parameterization. It applies different transformations (Resample, Resize, Zoom, Crop, CentreCrop) to a blank image and verifies the results. The test method uses conditional assertions based on the type of transformation:\n1. For Resample, it checks if the new image spacing is correct (3.7, 3.7, 3.7).\n2. For all transformations except Zoom, it verifies that the new image size is (10, 10, 10).\n3. For Zoom, it checks that the image size remains (100, 100, 100) and the spacing is unchanged.\nThis approach ensures that each transformation produces the expected results in terms of image size and properties, allowing for comprehensive testing of the transformation operations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "class TestTransform:\n    @pytest.mark.parametrize(\"op,params\", [(Resample, {\"spacing\": 3.7}), \n                                           (Resize, {\"size\": 10}), \n                                           (Zoom, {\"scale_factor\": .1}), \n                                           (Crop, {\"crop_centre\": (20, 20, 20), \"size\": 10}), \n                                           (CentreCrop, {\"size\": 10})])\n    def test_transform(self, op, params):\n        transform = op(**params)\n        new_img   = transform(blank)\n        \n        # check output\n        # TODO: Implement assertions for each transform type",
        "complete": "class TestTransform:\n    @pytest.mark.parametrize(\"op,params\", [(Resample, {\"spacing\": 3.7}), \n                                           (Resize, {\"size\": 10}), \n                                           (Zoom, {\"scale_factor\": .1}), \n                                           (Crop, {\"crop_centre\": (20, 20, 20), \"size\": 10}), \n                                           (CentreCrop, {\"size\": 10})])\n    def test_transform(self, op, params):\n        transform = op(**params)\n        new_img   = transform(blank)\n        \n        # check output\n        if isinstance(transform, Resample):\n            assert new_img.GetSpacing() == (3.7, 3.7, 3.7)\n        elif not isinstance(transform, Zoom):\n            assert new_img.GetSize() == (10, 10, 10)\n        elif isinstance(transform, Zoom):\n            assert new_img.GetSize() == (100, 100, 100) \n            assert new_img.GetSpacing() == (.37, .37, .37)"
      },
      {
        "partial": "class TestIntensity:\n    @pytest.mark.parametrize(\"op,params\", [(ClipIntensity, {\"lower\": 0, \"upper\": 500}),\n                                           (WindowIntensity, {\"window\": 500, \"level\": 250}),\n                                           (StandardScale, {}),\n                                           (MinMaxScale, {\"minimum\": 0, \"maximum\": 1000})])\n    def test_intesity(self, op, params):\n        img_cube = copy.deepcopy(blank)\n        img_cube[5:15,5:15,5:15] = 1000\n\n        intensify = op(**params)\n        new_img   = intensify(img_cube)\n        stats = ImageStatistics()(new_img)\n\n        # TODO: Implement assertions for each intensity operation",
        "complete": "class TestIntensity:\n    @pytest.mark.parametrize(\"op,params\", [(ClipIntensity, {\"lower\": 0, \"upper\": 500}),\n                                           (WindowIntensity, {\"window\": 500, \"level\": 250}),\n                                           (StandardScale, {}),\n                                           (MinMaxScale, {\"minimum\": 0, \"maximum\": 1000})])\n    def test_intesity(self, op, params):\n        img_cube = copy.deepcopy(blank)\n        img_cube[5:15,5:15,5:15] = 1000\n\n        intensify = op(**params)\n        new_img   = intensify(img_cube)\n        stats = ImageStatistics()(new_img)\n\n        if isinstance(intensify, (ClipIntensity, WindowIntensity)):\n            assert stats.sum == 5e5\n        elif isinstance(intensify, StandardScale):\n            assert np.allclose(stats.mean, 0.)\n            assert np.allclose(stats.standard_deviation, 1., rtol=1e-1)"
      }
    ],
    "dependencies": {
      "imports": [
        "os",
        "pathlib",
        "pytest",
        "SimpleITK",
        "numpy",
        "h5py"
      ],
      "from_imports": [
        "imgtools.ops.*"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/geneDrugSensitivityPBCorr.R",
    "language": "R",
    "content": "log_denom <- function(suc, total, p){\n  tmp <- 0;\n    # if(log10(p)>-32)\n    # {\n  tmp <- (suc)*log(p)\n    # } else {\n    #  warning(\"p reached precision threshold\")\n    # }\n    # if(log10(1.0-p)>-32)\n    # {\n  tmp <- tmp + (total - suc)*log(1 - p)\n    # } else {\n    #    warning(\"1-p reached precision threshold\")\n    # }\n  return(tmp)\n\n}\nlog_denom <- function(suc, total, p){\n  return((suc)*log(p) + (total - suc)*log(1 - p))\n\n}\n\n### Implementing algorithm from quick stop paper\n\n## decision boundary is inverse of error prob\n## do everything in log scale because of numerical precision.\ncorPermute <- function(sample_function, req_alpha=0.05, tolerance_par = req_alpha*0.001, log_decision_boundary = 10, max_iter = 1/req_alpha*100){\n\n  num.larger <- 0\n\n  cur_success <- 0\n  cur_iter <- 1\n\n  log_cur_PiN <- log(1) # Initialization so the logic can stay the same through all loops\n\n  p1 <- req_alpha\n  p2 <- p1 + tolerance_par\n\n  pr_min_1 <- 1/2\n\n  while(cur_iter < max_iter){\n    # vec1 <- sample(vec1)\n    # perm.cor <- cor(vec1, vec2, use=\"complete.obs\")\n\n    # success <- abs(perm.cor) > abs(obs.cor)\n\n    success <- sample_function()\n\n    if(success){\n      cur_success <- cur_success + 1\n      log_cur_PiN <- log_cur_PiN + log_denom(1,1,pr_min_1)\n    } else {\n      log_cur_PiN <- log_cur_PiN + log_denom(0,1,pr_min_1)\n    }\n    # if(pr_min_1 >= p2){\n    #   log_cur_suph1 <- log_denom(cur_success, cur_iter, p1)\n    #   log_cur_suph2 <- log_denom(cur_success, cur_iter, pr_min_1)\n    # } else if(pr_min_1 <= p1){\n    #   log_cur_suph1 <- log_denom(cur_success, cur_iter, pr_min_1)\n    #   log_cur_suph2 <- log_denom(cur_success, cur_iter, p2)\n    # } else {\n    #   log_cur_suph1 <- log_denom(cur_success, cur_iter, p1)\n    #   log_cur_suph2 <- log_denom(cur_success, cur_iter, p2)\n    # }\n    if(pr_min_1<p1) {\n      log_cur_suph1 <- log_denom(cur_success, cur_iter, pr_min_1)\n    } else {\n      log_cur_suph1 <- log_denom(cur_success, cur_iter, p1)\n    }\n    if(pr_min_1>p2) {\n      log_cur_suph2 <- log_denom(cur_success, cur_iter, pr_min_1)\n    } else {\n      log_cur_suph2 <- log_denom(cur_success, cur_iter, p2)\n    }\n\n    cur_iter <- cur_iter + 1\n    pr_min_1 <- (cur_success + 1/2)/cur_iter\n\n    # if(cur_success == 0){\n    #   next\n    # }\n\n    if(log_cur_PiN - log_cur_suph2 > log_decision_boundary){\n      return(list(significant = TRUE, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n    }\n    if(log_cur_PiN - log_cur_suph1 > log_decision_boundary){\n      return(list(significant = FALSE, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n    }\n  }\n  return(list(significant = NA, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n}\n\n\n\n## Helper Functions\n##TODO:: Add  function documentation\n#' @importFrom stats quantile\n.rescale <- function(x, na.rm=FALSE, q=0)\n{\n  if(q == 0) {\n    ma <- max(x, na.rm=na.rm)\n    mi <- min(x, na.rm=na.rm)\n  } else {\n    ma <- quantile(x, probs=1-(q/2), na.rm=na.rm)\n    mi <- quantile(x, probs=q/2, na.rm=na.rm)\n  }\n  xx <- (x - mi) / (ma - mi)\n  return(xx)\n}\n\n## TODO: decide better what to do with no variance cases.\ncor.boot <- function(data, w){\n  return(cor(data[w,1], data[w,2]))\n}\n\n\n#' Calculate The Gene Drug Sensitivity\n#'\n#' This version of the function uses a partial correlation instead of standardized linear models, for discrete predictive features\n#' Requires at least 3 observations per group.\n#'\n#' @param x A \\code{numeric} vector of gene expression values\n#' @param type A \\code{vector} of factors specifying the cell lines or type types\n#' @param batch A \\code{vector} of factors specifying the batch\n#' @param drugpheno A \\code{numeric} vector of drug sensitivity values (e.g.,\n#'   IC50 or AUC)\n#' @param test A \\code{character} string indicating whether resampling or analytic based tests should be used\n#' @param req_alpha \\code{numeric}, number of permutations for p value calculation\n#' @param nBoot \\code{numeric}, number of bootstrap resamplings for confidence interval estimation\n#' @param conf.level \\code{numeric}, between 0 and 1. Size of the confidence interval required\n#' @param max_perm \\code{numeric} the maximum number of permutations that QUICKSTOP can do before giving up and returning NA.\n#'   Can be set globally by setting the option \"PharmacoGx_Max_Perm\", or left at the default of \\code{ceiling(1/req_alpha*100)}.\n#' @param verbose \\code{boolean} Should the function display messages?\n#'\n#' @return A \\code{vector} reporting the effect size (estimateof the coefficient\n#'   of drug concentration), standard error (se), sample size (n), t statistic,\n#'   and F statistics and its corresponding p-value.\n#'\n#' @examples\n#' print(\"TODO::\")\n#'\n#' @importFrom stats sd complete.cases lm glm anova pf formula var\n#' @importFrom boot boot boot.ci\ngeneDrugSensitivityPBCorr <- function(x, type, batch, drugpheno,\n  test = c(\"resampling\", \"analytic\"),\n  req_alpha = 0.05,\n  nBoot = 1e3,\n  conf.level = 0.95,\n  max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)),\n  verbose=FALSE) {\n\n  test <- match.arg(test)\n\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE,\n  FUN.VALUE=list(1)), check.names=FALSE)\n\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || all(duplicated(x)[-1L])) {\n    ## not enough samples with complete information or no variation in gene expression\n    return(rest)\n  }\n\n  ## taking at least 5 times the number of boot samples as number of observations, as with binary data many boot samples\n  ## tend to be NA, and if the number of non-NA drops below number of obs, the emperical influence cannot be determined\n  ## for BCA interval calculation\n\n  if(test==\"resampling\"){\n    nBoot <- max(nBoot, nn*5)\n  }\n\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n\n\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1){\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  # ff1 <- sprintf(\"%s + x\", ff0)\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  ## control for tissue type\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  ## control for batch\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n\n\n  if(!is.factor(dd[[2]])){\n    stop(\"Molecular Feature is not discrete, but point biserial correlation was requested\")\n  } else if(length(unique(dd[[2]]))>2) {\n\n    stop('More than two discrete settings for moleuclar feature not currently supported')\n\n  } else if(length(unique(dd[[2]]))==1 || min(table(dd[[2]]))<3) {\n    warning(\"Some features had less than 3 observations per category, returning NA.\")\n    return(rest)\n\n  } else{\n    dd[[2]] <- as.numeric(dd[[2]]) ## converting to numeric codings for downstream modeling\n  }\n\n\n  if(any(unlist(lapply(drugpheno,is.factor)))){\n\n    stop(\"Currently only continous output allowed for point biserial correlations\")\n\n\n  } else{\n\n    if(ncol(dd) > 2){\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L # taking the residual degrees of freedom minus 2 parameters estimated for pearson cor.\n    } else { ## doing this if statement in the case there are some numerical differences between mean centred values and raw values\n    var1 <- dd[,\"drugpheno.1\"]\n    var2 <- dd[,\"x\"]\n    df <- nn - 2L\n  }\n\n  obs.cor <- cor(var1, var2, use=\"complete.obs\")\n\n\n    ## NB: just permuting the residuals would leads to Type I error inflation,\n    ## from an underestimation due to ignoring variance in the effects of the covariates.\n    ## See: https://www.tandfonline.com/doi/abs/10.1080/00949650008812035\n    ## Note that the above paper does not provide a single method recommended in all cases\n    ## We apply the permutation of raw data method, as it is most robust to small sample sizes\n  if(test == \"resampling\"){\n      ## While the logic is equivalent regardless of if there are covariates for calculating the point estimate,\n      ## (correlation is a subcase of partial correlation), for computational efficency in permuation testing we\n      ## split here and don't do extranous calls to lm if it is unnecessay.\n\n    if(ncol(dd) > 2){\n\n      # if(!getOption(\"PharmacoGx_useC\")|| ncol(dd)!=3){ ## not yet implemented\n\n        ## implementing a much more efficient method for the particular case where we have 3 columns with assumption that\n        ## column 3 is the tissue.\n      if(ncol(dd)==3){\n        sample_function <- function(){\n\n          partial.dp <- sample(dd[,1], nrow(dd))\n          partial.x <- sample(dd[,2], nrow(dd))\n\n          for(gp in unique(dd[,3])){\n            partial.x[dd[,3]==gp] <- partial.x[dd[,3]==gp]-mean(partial.x[dd[,3]==gp])\n            partial.dp[dd[,3]==gp] <- partial.dp[dd[,3]==gp]-mean(partial.dp[dd[,3]==gp])\n          }\n\n          perm.cor <- cor(partial.dp, partial.x, use=\"complete.obs\")\n          return(abs(obs.cor) < abs(perm.cor))\n        }\n      } else {\n        sample_function <- function(){\n\n          dd2 <- dd\n          dd2[,1] <- sample(dd[,1], nrow(dd))\n          dd2[,2] <- sample(dd[,2], nrow(dd))\n\n          partial.dp <- residuals(lm(formula(ffd), dd2))\n          partial.x <- residuals(lm(formula(ffx), dd2))\n\n          perm.cor <- cor(partial.dp, partial.x, use=\"complete.obs\")\n          return(abs(obs.cor) < abs(perm.cor))\n        }\n      }\n\n      p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n      significant <- p.value$significant\n      p.value <- p.value$p.value\n\n\n      # } else {\n\n      #   x <- dd[,1]\n      #   y <- dd[,2]\n      #   GR <- as.integer(factor(dd[,3]))-1L\n      #   GS <- as.integer(table(factor(dd[,3])))\n      #   NG <- length(table(factor(dd[,3])))\n      #   N <- as.numeric(length(x))\n\n      #   p.value <-PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, 1e7,N, req_alpha, req_alpha/100, 10L, runif(2))\n      #   significant <- p.value[[1]]\n      #   p.value <- p.value[[2]]\n      # }\n\n\n      pcor.boot <- function(ddd, w){\n        ddd <- ddd[w,]\n          ## Taking care of an edge case where only one covariate factor level is left after resampling\n        ddd[,-c(1,2)] <- ddd[,-c(1,2),drop=FALSE][,apply(ddd[,-c(1,2),drop=FALSE], 2, function(x) return(length(unique(x))))>=2]\n\n        if(ncol(ddd)==3){\n          partial.dp <- ddd[,1]\n          partial.x <- ddd[,2]\n          for(gp in unique(ddd[,3])){\n            partial.x[ddd[,3]==gp] <- partial.x[ddd[,3]==gp]-mean(partial.x[ddd[,3]==gp])\n            partial.dp[ddd[,3]==gp] <- partial.dp[ddd[,3]==gp]-mean(partial.dp[ddd[,3]==gp])\n          }\n\n        } else if(ncol(ddd)==2){\n          partial.dp <- ddd[,1]\n          partial.x <- ddd[,2]\n        } else {\n\n          partial.dp <- residuals(lm(formula(ffd), ddd))\n          partial.x <- residuals(lm(formula(ffx), ddd))\n\n        }\n\n        return(cor(partial.dp, partial.x, use=\"complete.obs\"))\n      }\n\n      boot.out <- boot(dd, pcor.boot, R=nBoot)\n\n      cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n        error = function(e) {\n          if(e$message == \"estimated adjustment 'w' is infinite\"){\n            warning(\"estimated adjustment 'w' is infinite for some features\")\n            return(c(NA_real_,NA_real_))\n          } else {\n            stop(e)\n          }\n        })\n    } else {\n    # if(!getOption(\"PharmacoGx_useC\")){\n\n      ## At this point we have verified that we are doing the normal (nor partial) PBCC,\n      ## and we also verified that only 2 unique values of var2 exist. Therefore, diff\n      ## should return a single result.\n      ## Note that the PBCC permutation only depends on the mean differences, the\n      ## denominator is proprtional to the total variance\n      ## in var1 and inverse of the sqrt of the proportions between groups,\n      ## both of which stay constant through the permutation. Therefore, we skip\n      ## the needless normalization step in this permutation procedure.\n      ## Note that this does not apply to bootstrapping.\n\n      obs.mean.diff <- diff(tapply(var1, var2, mean))\n      sample_function <- function(){\n        v1 <- sample(var1)\n        return(abs(obs.mean.diff) < abs(diff(tapply(v1, var2, mean))))\n      }\n\n      p.value <- corPermute(sample_function, req_alpha = req_alpha, max_iter=max_perm)\n      significant <- p.value$significant\n      p.value <- p.value$p.value\n    # } else {\n\n    #   x <- as.numeric(var1)\n    #   y <- as.numeric(var2)\n    #   GR <- rep(0L, length(x))\n    #   GS <- as.integer(length(x))\n    #   NG <- 1\n    #   N <- as.numeric(length(x))\n\n    #   p.value <-PharmacoGx:::partialCorQUICKSTOP(x, y, obs.cor, GR, GS, NG, 1e7,N, req_alpha, req_alpha/100, 10L, runif(2))\n    #   significant <- p.value[[1]]\n    #   p.value <- p.value[[2]]\n    # }\n\n\n\n\n      boot.out <- boot(data.frame(var1, var2), cor.boot, R=nBoot)\n      cint <- tryCatch(boot.ci(boot.out, conf = conf.level, type=\"bca\")$bca[,4:5],\n        error = function(e) {\n          if(e$message == \"estimated adjustment 'w' is infinite\" || e$message == \"Error in if (const(t, min(1e-08, mean(t, na.rm = TRUE)/1e+06))) { : \\n  missing value where TRUE/FALSE needed\\n\"){\n            warning(\"estimated adjustment 'w' is infinite for some features\")\n            return(c(NA_real_,NA_real_))\n          } else {\n            stop(e)\n          }\n        })\n    }\n\n      ## Think about if the partial cor should also be refit for each (Probably, different lines would be fit if points are missing...)\n\n  } else if(test == \"analytic\"){\n      # if(ncol(dd) > 2){\n      #   df <- nn - 2L - controlled.var\n\n      # } else {\n      #   df <- nn - 2L\n      #   # cor.test.res <- cor.test(dd[,\"drugpheno.1\"], dd[,\"x\"], method=\"pearson\", use=\"complete.obs\")\n      # }\n    stat <- sqrt(df) * obs.cor/sqrt(1-obs.cor^2) ## Note, this is implemented in same order of operations as cor.test\n    p.value <- 2*min(pt(stat, df=df), pt(stat, df=df, lower.tail = FALSE))\n      ## Implementing with fisher transform and normal dist for consistency with R's cor.test\n    z <- atanh(obs.cor)\n    sigma <- 1/sqrt(df - 1)\n    cint <- tanh(z + c(-1, 1) * sigma * qnorm((1 + conf.level)/2))\n    significant <- p.value < req_alpha\n  }\n\n}\n\nrest <- c(\"estimate\"=obs.cor, \"n\"=nn, df=df, significant = as.numeric(significant), \"pvalue\"=p.value, \"lower\" = cint[1], \"upper\" = cint[2])\n\n\nreturn(rest)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `log_denom` function in the given code snippet?",
        "answer": "The `log_denom` function calculates the log-likelihood of observing a certain number of successes (`suc`) out of a total number of trials (`total`), given a probability `p`. It computes this using the formula: `(suc * log(p)) + ((total - suc) * log(1 - p))`. This function is typically used in statistical calculations, particularly in the context of binomial distributions or logistic regression."
      },
      {
        "question": "What is the main purpose of the `corPermute` function and how does it work?",
        "answer": "The `corPermute` function implements a permutation test algorithm for correlation analysis, likely based on the 'quick stop' method. It iteratively samples from a given distribution (provided by `sample_function`) and updates a test statistic. The function continues until it reaches a decision boundary or a maximum number of iterations. It returns whether the result is significant, the estimated p-value, the number of iterations, and the number of successful samples. This method is used for efficient hypothesis testing when the exact distribution is unknown or difficult to compute analytically."
      },
      {
        "question": "In the `geneDrugSensitivityPBCorr` function, what is the purpose of the `test` parameter and how does it affect the function's behavior?",
        "answer": "The `test` parameter in `geneDrugSensitivityPBCorr` determines whether the function uses a resampling-based or analytic approach for statistical testing. When `test = 'resampling'`, the function uses bootstrap resampling and permutation tests to estimate confidence intervals and p-values. When `test = 'analytic'`, it uses closed-form statistical formulas to compute these values. The resampling approach is generally more robust for complex models or when assumptions of parametric tests may not hold, while the analytic approach is faster but relies on certain distributional assumptions."
      }
    ],
    "completion_tasks": [
      {
        "partial": "corPermute <- function(sample_function, req_alpha=0.05, tolerance_par = req_alpha*0.001, log_decision_boundary = 10, max_iter = 1/req_alpha*100) {\n  num.larger <- 0\n  cur_success <- 0\n  cur_iter <- 1\n  log_cur_PiN <- log(1)\n  p1 <- req_alpha\n  p2 <- p1 + tolerance_par\n  pr_min_1 <- 1/2\n\n  while(cur_iter < max_iter) {\n    success <- sample_function()\n\n    if(success) {\n      cur_success <- cur_success + 1\n      log_cur_PiN <- log_cur_PiN + log_denom(1,1,pr_min_1)\n    } else {\n      log_cur_PiN <- log_cur_PiN + log_denom(0,1,pr_min_1)\n    }\n\n    # Complete the code here\n\n  }\n  return(list(significant = NA, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n}",
        "complete": "corPermute <- function(sample_function, req_alpha=0.05, tolerance_par = req_alpha*0.001, log_decision_boundary = 10, max_iter = 1/req_alpha*100) {\n  num.larger <- 0\n  cur_success <- 0\n  cur_iter <- 1\n  log_cur_PiN <- log(1)\n  p1 <- req_alpha\n  p2 <- p1 + tolerance_par\n  pr_min_1 <- 1/2\n\n  while(cur_iter < max_iter) {\n    success <- sample_function()\n\n    if(success) {\n      cur_success <- cur_success + 1\n      log_cur_PiN <- log_cur_PiN + log_denom(1,1,pr_min_1)\n    } else {\n      log_cur_PiN <- log_cur_PiN + log_denom(0,1,pr_min_1)\n    }\n\n    if(pr_min_1<p1) {\n      log_cur_suph1 <- log_denom(cur_success, cur_iter, pr_min_1)\n    } else {\n      log_cur_suph1 <- log_denom(cur_success, cur_iter, p1)\n    }\n    if(pr_min_1>p2) {\n      log_cur_suph2 <- log_denom(cur_success, cur_iter, pr_min_1)\n    } else {\n      log_cur_suph2 <- log_denom(cur_success, cur_iter, p2)\n    }\n\n    cur_iter <- cur_iter + 1\n    pr_min_1 <- (cur_success + 1/2)/cur_iter\n\n    if(log_cur_PiN - log_cur_suph2 > log_decision_boundary) {\n      return(list(significant = TRUE, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n    }\n    if(log_cur_PiN - log_cur_suph1 > log_decision_boundary) {\n      return(list(significant = FALSE, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n    }\n  }\n  return(list(significant = NA, \"p.value\" = pr_min_1, num_iter=cur_iter, num_larger=cur_success))\n}"
      },
      {
        "partial": "geneDrugSensitivityPBCorr <- function(x, type, batch, drugpheno, test = c(\"resampling\", \"analytic\"), req_alpha = 0.05, nBoot = 1e3, conf.level = 0.95, max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)), verbose=FALSE) {\n  test <- match.arg(test)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE, FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || all(duplicated(x)[-1L])) {\n    return(rest)\n  }\n\n  if(test==\"resampling\") {\n    nBoot <- max(nBoot, nn*5)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n  xx <- x[ccix]\n\n  # Complete the code here\n\n}",
        "complete": "geneDrugSensitivityPBCorr <- function(x, type, batch, drugpheno, test = c(\"resampling\", \"analytic\"), req_alpha = 0.05, nBoot = 1e3, conf.level = 0.95, max_perm = getOption(\"PharmacoGx_Max_Perm\", ceiling(1/req_alpha*100)), verbose=FALSE) {\n  test <- match.arg(test)\n  colnames(drugpheno) <- paste(\"drugpheno\", seq_len(ncol(drugpheno)), sep=\".\")\n  drugpheno <- data.frame(vapply(drugpheno, function(x) {\n    if (!is.factor(x)) {\n      x[is.infinite(x)] <- NA\n    }\n    return(list(x))\n  }, USE.NAMES=TRUE, FUN.VALUE=list(1)), check.names=FALSE)\n\n  ccix <- complete.cases(x, type, batch, drugpheno)\n  nn <- sum(ccix)\n\n  rest <- c(\"estimate\"=NA_real_, \"n\"=as.numeric(nn), \"df\"=NA_real_, significant = NA_real_,\"pvalue\"=NA_real_, \"lower\" = NA_real_, \"upper\" = NA_real_)\n\n  if(nn <= 3 || all(duplicated(x)[-1L])) {\n    return(rest)\n  }\n\n  if(test==\"resampling\") {\n    nBoot <- max(nBoot, nn*5)\n  }\n\n  drugpheno <- drugpheno[ccix,,drop=FALSE]\n  xx <- x[ccix]\n\n  if(ncol(drugpheno)>1) {\n    stop(\"Partial Correlations not implemented for multiple output\")\n  } else {\n    ffd <- \"drugpheno.1 ~ . - x\"\n    ffx <- \"x ~ . - drugpheno.1\"\n  }\n\n  dd <- data.frame(drugpheno, \"x\"=xx)\n\n  if(length(sort(unique(type[ccix]))) > 1) {\n    dd <- cbind(dd, type=type[ccix])\n  }\n  if(length(sort(unique(batch[ccix]))) > 1) {\n    dd <- cbind(dd, batch=batch[ccix])\n  }\n\n  if(!is.factor(dd[[2]])) {\n    stop(\"Molecular Feature is not discrete, but point biserial correlation was requested\")\n  } else if(length(unique(dd[[2]]))>2) {\n    stop('More than two discrete settings for moleuclar feature not currently supported')\n  } else if(length(unique(dd[[2]]))==1 || min(table(dd[[2]]))<3) {\n    warning(\"Some features had less than 3 observations per category, returning NA.\")\n    return(rest)\n  } else {\n    dd[[2]] <- as.numeric(dd[[2]])\n  }\n\n  if(any(unlist(lapply(drugpheno,is.factor)))) {\n    stop(\"Currently only continous output allowed for point biserial correlations\")\n  } else {\n    if(ncol(dd) > 2) {\n      lm1 <- lm(formula(ffd), dd)\n      var1 <- residuals(lm1)\n      var2 <- residuals(lm(formula(ffx), dd))\n      df <- lm1$df - 2L\n    } else {\n      var1 <- dd[,\"drugpheno.1\"]\n      var2 <- dd[,\"x\"]\n      df <- nn - 2L\n    }\n\n    obs.cor <- cor(var1, var2, use=\"complete.obs\")\n\n    if(test == \"resampling\") {\n      # Implement resampling test\n    } else if(test == \"analytic\") {\n      stat <- sqrt(df) * obs.cor/sqrt(1-obs.cor^2)\n      p.value <- 2*min(pt(stat, df=df), pt(stat, df=df, lower.tail = FALSE))\n      z <- atanh(obs.cor)\n      sigma <- 1/sqrt(df - 1)\n      cint <- tanh(z + c(-1, 1) * sigma * qnorm((1 + conf.level)/2))\n      significant <- p.value < req_alpha\n    }\n  }\n\n  rest <- c(\"estimate\"=obs.cor, \"n\"=nn, df=df, significant = as.numeric(significant), \"pvalue\"=p.value, \"lower\" = cint[1], \"upper\" = cint[2])\n\n  return(rest)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/data-raw/GDSC/GDSC_treatmentMetadata.R",
    "language": "R",
    "content": "filePath <- system.file(\"extdata/GDSC\", \"GDSC2_8.4_treatmentMetadata.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath) \n\nGDSC_treatmentMetadata <- \n    rawdata[, .(GDSC.treatmentid = `DRUG_NAME`, GDSC.synonyms = `SYNONYMS`, GDSC.drug_id = `DRUG_ID`)]\n\nusethis::use_data(GDSC_treatmentMetadata, overwrite = TRUE)\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `system.file()` function in the first line of the code snippet?",
        "answer": "The `system.file()` function is used to locate and retrieve the file path of a specific file within the installed R package. In this case, it's locating the 'GDSC2_8.4_treatmentMetadata.csv' file in the 'extdata/GDSC' directory of the 'AnnotationGx' package."
      },
      {
        "question": "How is the `data.table::fread()` function being used in this code, and what advantage does it offer?",
        "answer": "The `data.table::fread()` function is used to read the CSV file specified by `filePath`. It's part of the data.table package and is typically faster than base R's `read.csv()` for large files. The `::` notation ensures that the `fread()` function from the data.table package is used, even if another package with a function of the same name is loaded."
      },
      {
        "question": "What transformation is being applied to the `rawdata` to create `GDSC_treatmentMetadata`, and why might this be useful?",
        "answer": "The `rawdata` is being subset and renamed using data.table syntax. It selects three columns (`DRUG_NAME`, `SYNONYMS`, and `DRUG_ID`) and renames them to `GDSC.treatmentid`, `GDSC.synonyms`, and `GDSC.drug_id` respectively. This transformation creates a more concise dataset with standardized column names, which can be useful for further analysis or integration with other datasets in the GDSC (Genomics of Drug Sensitivity in Cancer) project."
      }
    ],
    "completion_tasks": [
      {
        "partial": "filePath <- system.file(\"extdata/GDSC\", \"GDSC2_8.4_treatmentMetadata.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath) \n\nGDSC_treatmentMetadata <- \n    rawdata[, .(GDSC.treatmentid = `DRUG_NAME`, GDSC.synonyms = `SYNONYMS`, ",
        "complete": "filePath <- system.file(\"extdata/GDSC\", \"GDSC2_8.4_treatmentMetadata.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath) \n\nGDSC_treatmentMetadata <- \n    rawdata[, .(GDSC.treatmentid = `DRUG_NAME`, GDSC.synonyms = `SYNONYMS`, GDSC.drug_id = `DRUG_ID`)]"
      },
      {
        "partial": "filePath <- system.file(\"extdata/GDSC\", \"GDSC2_8.4_treatmentMetadata.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath) \n\nGDSC_treatmentMetadata <- \n    rawdata[, .(GDSC.treatmentid = `DRUG_NAME`, GDSC.synonyms = `SYNONYMS`, GDSC.drug_id = `DRUG_ID`)]\n\nusethis::use_data(",
        "complete": "filePath <- system.file(\"extdata/GDSC\", \"GDSC2_8.4_treatmentMetadata.csv\", package = \"AnnotationGx\")\nrawdata <- data.table::fread(filePath) \n\nGDSC_treatmentMetadata <- \n    rawdata[, .(GDSC.treatmentid = `DRUG_NAME`, GDSC.synonyms = `SYNONYMS`, GDSC.drug_id = `DRUG_ID`)]\n\nusethis::use_data(GDSC_treatmentMetadata, overwrite = TRUE)"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/cellosaurus.R",
    "language": "R",
    "content": "#' Get the list of fields in the Cellosaurus schema\n#'\n#' This function retrieves the list of fields available in the Cellosaurus schema.\n#' It internally calls the `.cellosaurus_schema()` function to fetch the schema\n#' and extracts the list of fields from it.\n#'\n#' @param common Logical indicating whether to return only the common fields. Default is FALSE.\n#' @param upper Logical indicating whether to return the fields in uppercase. Default is FALSE.\n#'\n#' @return A character vector containing the list of fields in the Cellosaurus schema.\n#'\n#' @examples\n#' cellosaurus_fields()\n#' cellosaurus_fields(common = TRUE)\n#' cellosaurus_fields(upper = TRUE)\n#'\n#' @export\ncellosaurus_fields <- function(common = FALSE, upper = FALSE) {\n  if(common == TRUE) {\n    fields <- c(\"id\", \"ac\", \"acas\", \"sy\", \"dr\", \"di\", \"din\", \"dio\", \"ox\", \"cc\",\n    \"sx\", \"ag\", \"oi\", \"hi\", \"ch\", \"ca\",  \"dt\", \"dtc\", \"dtu\", \"dtv\", \"from\", \"group\")\n  } else{\n    schema <- .cellosaurus_schema()\n    fields <- schema$components$schemas$Fields$enum\n  }\n\n  if(upper == TRUE) {\n    fields <- toupper(fields)\n  }else{\n    fields <- tolower(fields)\n  }\n\n  return(fields)\n}\n\n#' Get the Cellosaurus API version\n#'\n#' This function retrieves the version of the Cellosaurus API.\n#'\n#' @return The version of the Cellosaurus API.\n#'\n#' @examples\n#' cellosaurusAPIVersion()\n#'\n#' @export\ncellosaurusAPIVersion <- function() {\n  .cellosaurus_schema()$info$version\n}\n\n# fields <- c(\"AC\", \"CA\", \"DT\", \"ID\", \"DI\", \"DR\", \"HI\", \"OI\", \"OX\", \"AG\", \"SX\", \"SY\") |> tolower()\n\n#' Map Cell Line IDs to Accession Numbers\n#'\n#' This function maps cell line IDs to accession numbers using the Cellosaurus database.\n#'\n#' @param ids A character vector of cell line IDs.\n#' @param numResults The maximum number of results to return for each query. Default is 1000.\n#' @param from The type of input IDs. Possible values are \"idsy\" (default), \"ac\", \"id\", \"sy\", and \"misspelling\".\n#' @param sort The sorting order of the results. Possible values are \"ac\" (default), \"id\", \"sy\", and \"misspelling\".\n#' @param keep_duplicates Logical indicating whether to keep duplicate results. Default is FALSE.\n#' @param fuzzy Logical indicating whether to perform a fuzzy search. Default is FALSE.\n#' @param query_only Logical indicating whether to return only the query URLs. Default is FALSE.\n#' @param raw Logical indicating whether to return the raw HTTP responses. Default is FALSE.\n#' @param parsed Logical indicating whether to parse the response text. Default is TRUE.\n#' @param ... Additional arguments to be passed to the underlying functions.\n#'\n#' @return A data.table containing the mapped cell line IDs and accession numbers.\n#'\n#' @examples\n#' mapCell2Accession(ids = c(\"A549\", \"MCF7\"))\n#'\n#' @export\nmapCell2Accession <- function(\n    ids, numResults = 10000, from = \"idsy\", sort = \"ac\", keep_duplicates = FALSE, \n    fuzzy = FALSE, query_only = FALSE, raw = FALSE, parsed = TRUE, ...\n) {\n\n  funContext <- .funContext(\"mapCell2Accession\")\n\n  # Input validation and coercion\n  if (!is.character(ids)) {\n    .warn(\"Input names are not character, coercing to character\")\n    ids <- as.character(ids)\n  }\n\n  to = c(\"ac\", \"id\", \"sy\", \"misspelling\", \"dr\", \"cc\", \"ca\", \"di\", \"ag\", \"sx\", \"hi\")\n\n  # create query list\n  .info(funContext, \"Creating Cellosaurus queries\")\n\n  queries <- .create_cellosaurus_queries(ids, from, fuzzy)\n  names(queries) <- ids\n\n  .info(funContext, \"Building Cellosaurus requests\")\n  # build the list of requests\n  requests <- parallel::mclapply(queries, function(query) {\n    .build_cellosaurus_request(\n      query = query,\n      to = to,\n      numResults = numResults,\n      sort = sort,\n      output = \"TXT\",\n      fuzzy = fuzzy,\n      ...\n    )\n  })\n\n  if (query_only) return(lapply(requests, function(req) req$url))\n  \n  # Submit requests using parallel httr2 since cellosaurus doesnt throttle\n  .info(funContext, \"Performing Cellosaurus queries\")\n  responses <- .perform_request_parallel(requests, progress = \"Querying Cellosaurus...\")\n  names(responses) <- as.character(ids) # in case its an numeric ID  like cosmic ids\n  if (raw) return(responses)\n\n  # parse the responses\n  .info(funContext, \"Parsing Cellosaurus responses\")\n  responses_dt <- parallel::mclapply(ids, function(name) {\n    resp <- responses[[name]]\n\n    resp <- .parse_cellosaurus_lines(resp)\n    if(length(resp) == 0L){\n      .warn(paste0(\"No results found for \", name))\n      result <- data.table::data.table()\n      result$query <- name\n      return(result)\n    }\n    response_dt <- .parse_cellosaurus_text(resp, name, parsed, keep_duplicates)\n    response_dt\n  }) \n  \n\n  responses_dt <- data.table::rbindlist(responses_dt, fill = TRUE)\n\n  return(responses_dt)\n\n}\n\n\n#' Parses the lines of a cellosaurus response\n#'\n#' This function takes a response object and parses the lines of the response\n#' to extract specific sections of the cellosaurus data.\n#'\n#' @param resp The response object containing the cellosaurus data\n#' @return A list of parsed lines from the cellosaurus data\n#' \n#' @keywords internal\n#' @noRd\n.parse_cellosaurus_lines <- function(resp){\n  lines <- httr2::resp_body_string(resp)  |>\n            strsplit(\"\\n\") |> \n            unlist()\n  \n  Map(\n    f = function(lines, i, j) {\n        lines[i:(j - 1L)]\n    },\n    i = grep(pattern = \"^ID\\\\s+\", x = lines, value = FALSE),\n    j = grep(pattern = \"^//$\", x = lines, value = FALSE),\n    MoreArgs = list(\"lines\" = lines),\n    USE.NAMES = FALSE\n  )\n  \n}\n\n#' parse responses\n#' \n#' @noRd \n#' @keywords internal\n.parse_cellosaurus_text <- function(resp, name, parsed = FALSE, keep_duplicates = FALSE){\n\n  responses_dt <- lapply(\n      X = resp,\n      FUN = .processEntry\n  ) \n  tryCatch({\n    responses_dt <- data.table::rbindlist(responses_dt, fill = TRUE)\n  }, error = function(e) {\n    .err(paste0(\"Error parsing response for \", name, \": \", e$message))\n  }) \n\n  responses_dt <- .formatSynonyms(responses_dt)\n\n  if(!parsed) {\n    responses_dt$query <- name\n    return(responses_dt[, c(\"cellLineName\", \"accession\", \"query\")])\n  }\n\n\n  result <- .find_cellosaurus_matches(responses_dt, name, keep_duplicates)\n  result$query <- name \n  result <- result[, c(\"cellLineName\", \"accession\", \"query\")]\n\n  return(result)\n\n}\n\n#' Splits cellosaurus lines into a named list\n#'\n#' This function takes a vector of cellosaurus lines and splits them into a named list.\n#' The lines are split based on the delimiter \"   \" (three spaces).\n#'\n#' @param lines A vector of cellosaurus lines\n#' @return A named list where each element corresponds to a unique identifier and contains the associated lines\n#' @examples\n#' lines <- c(\"ID1   Line 1\", \"ID1   Line 2\", \"ID2   Line 1\", \"ID2   Line 2\")\n#' AnnotationGx:::.split_cellosaurus_lines(lines)\n#' # Output:\n#' # $ID1\n#' # [1] \"Line 1\" \"Line 2\"\n#' #\n#' # $ID2\n#' # [1] \"Line 1\" \"Line 2\"\n#'\n#' @noRd\n.split_cellosaurus_lines <- function(lines){\n  x <- strSplit(lines, split = \"   \")\n  x <- split(x[, 2L], f = x[, 1L])\n  x\n}\n\n\n## This function processes an entry in the cellosaurus database.\n## It splits the input string, organizes the data into a nested list,\n## handles optional keys, removes discontinued identifiers from the DR field,\n## and converts the resulting list into a data table.\n.processEntry <- function(x){\n  requiredKeys = c(\"AC\", \"CA\", \"DT\", \"ID\")\n  nestedKeys = c(\"DI\", \"DR\", \"HI\")\n  optionalKeys = c(\"AG\", \"SX\", \"SY\")\n  specialKeys = c(\"CC\")\n\n  x <- .split_cellosaurus_lines(x)\n  \n  if(\"CC\" %in% names(x)){\n    x <- .formatComments(x)\n  }\n\n  # create a single row dt from the list\n  dt <- data.table::data.table(\n    ID = x[[\"ID\"]],\n    AC = x[[\"AC\"]]\n  )\n\n  for (name in setdiff(requiredKeys, c(\"ID\", \"AC\"))) {\n    dt[[name]] <- x[[name]]\n  }\n  for (key in optionalKeys) {\n    dt[[key]] <- ifelse(\n      is.null(x[[key]]), \n      NA_character_, \n      x[[key]]\n    )\n  }\n  for (key in nestedKeys) {\n    dt[[key]]  <- ifelse(\n      is.null(x[[key]]),\n      NA_character_,\n      list(.splitNestedCol(x, key, \"; \")[[key]])\n    )\n  }\n  for (key in specialKeys) {\n    dt[[key]] <- ifelse(\n      is.null(x[[key]]),\n      NA_character_,\n      x[key]\n    )\n  }\n\n  ## Filter out discontinued identifiers from DR (e.g. \"CVCL_0455\").\n  discontinued <- grep(\n    pattern = \"Discontinued:\",\n    x = x[[\"CC\"]],\n    fixed = TRUE,\n    value = TRUE\n  )\n  if (isTRUE(length(discontinued) > 0L)) {\n    discontinued <- sub(\n      pattern = \"^Discontinued: (.+);.+$\",\n      replacement = \"\\\\1\",\n      x = discontinued\n    )\n    dt[[\"DR\"]] <- list(setdiff(x = x[[\"DR\"]], y = discontinued))\n  }\n  # create data.table of lists\n  responses_dt <- dt\n\n  old_names <- c(\"AC\", \"AG\", \"AS\", \"CA\", \"CC\", \"DI\", \"DR\", \"DT\", \"HI\", \"ID\", \n            \"OI\", \"OX\", \"RX\", \"ST\", \"SX\", \"SY\", \"WW\")\n\n  new_names <- c(\"accession\", \"ageAtSampling\", \"secondaryAccession\", \"category\", \n    \"comments\", \"diseases\", \"crossReferences\", \"date\", \"hierarchy\", \"cellLineName\",\n    \"originateFromSameIndividual\", \"speciesOfOrigin\", \"referencesIdentifiers\", \n    \"strProfileData\", \"sexOfCell\", \"synonyms\", \"webPages\")\n      \n  data.table::setnames(responses_dt, old = old_names, new = new_names, skip_absent = TRUE)\n  responses_dt\n}\n\n\n\n#' Find Cellosaurus Matches\n#'\n#' This function searches for matches in a data table based on a given name.\n#' It first tries to find an exact match as the cellLineName to avoid cases where\n#' the first row is the wrong cell line but the query is in a synonym, and the second row\n#' is the correct cell line. If an exact match is not found, it searches for matches\n#' in the data table using the query and cleaned name. If no matches are found, it\n#' creates an empty data table with the columns \"cellLineName\", \"accession\", and \"query\".\n#'\n#' @param responses_dt A data table containing the responses.\n#' @param name The name to search for.\n#' @param keep_duplicates A logical value indicating whether to keep duplicate matches.\n#'\n#' @return A data table with the matched results, or an empty data table if no matches are found.\n#'\n#' @examples\n#' responses_dt <- data.table::data.table(\n#'   cellLineName = c(\"Cell Line 1\", \"Cell Line 2\", \"Cell Line 3\"),\n#'   accession = c(\"Accession 1\", \"Accession 2\", \"Accession 3\"),\n#'   synonyms = list(c(\"Synonym 1\", \"Synonym 2\"), c(\"Synonym 3\"), c(\"Synonym 4\"))\n#' )\n#' \n#' .find_cellosaurus_matches(responses_dt, \"Cell Line 2\")\n#'\n#' @noRd\n#' @keywords internal\n.find_cellosaurus_matches <- function(\n  responses_dt, \n  name, \n  keep_duplicates = FALSE\n){\n  # save original name\n  query <- name\n  name <- cleanCharacterStrings(name)\n\n  # first try for exact match as cellLineName to avoid the case where\n  # the first row is the wrong cellline but the query is in a synonym\n  # but the second row is the correct cellline\n  # TODO:: REFACTOR THIS TO NOT REPEAT THE CONDITIONAL \n  if(any(responses_dt$cellLineName == query)){\n    data.table::setkeyv(responses_dt, \"cellLineName\")\n    result <- responses_dt[query]\n  } else if(length(matchNested(query, responses_dt, keep_duplicates = keep_duplicates)) > 0){\n    matches <- matchNested(query, responses_dt, keep_duplicates = keep_duplicates)\n    result <- responses_dt[matches]\n  } else if(length(matchNested(name, responses_dt, keep_duplicates = keep_duplicates)) > 0){\n    matches <- matchNested(name, responses_dt, keep_duplicates = keep_duplicates)\n    result <- responses_dt[matches]\n  } else if(any(cleanCharacterStrings(responses_dt$cellLineName) == name)){\n    matches <- cleanCharacterStrings(responses_dt$cellLineName) == name\n    result <- responses_dt[matches][1]\n  } else if(length(matchNested(name, lapply(responses_dt$synonyms, cleanCharacterStrings)))> 0 ){\n    matches <- matchNested(name, lapply(responses_dt$synonyms, cleanCharacterStrings))\n    result <- responses_dt[matches]\n  } else{\n    .warn(paste0(\"No results found for \", query))\n    # create an empty data.table with the following columns:\n    # c(\"cellLineName\", \"accession\", \"query\")\n    result <- data.table::data.table(\n      cellLineName = NA_character_,\n      accession = NA_character_,\n      query = query\n    )\n  }\n  return(result)\n}\n\n\n\n#' Format the `synonyms` column\n#'\n#' @note Updated 2023-01-24.\n#' @noRd\n.formatSynonyms <- function(responses_dt) {\n  .splitCol(\n      object = responses_dt,\n      colName = \"synonyms\",\n      split = \"; \"\n  )\n}\n\n\n#' Format the `comments` column\n#'\n#' @note Updated 2023-09-22.\n#' @noRd\n.formatComments <- function(object) {\n    test_ <- strSplit(object[[\"CC\"]], \": \", n = 2)\n    test_ <- split(test_[, 2L], f = test_[, 1L])\n\n    test_ <- sapply(test_, strsplit, split = \"; \")\n\n    object[[\"CC\"]] <- test_\n    object\n}",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `cellosaurus_fields` function and what are its main parameters?",
        "answer": "The `cellosaurus_fields` function retrieves the list of fields available in the Cellosaurus schema. It has two main parameters: `common` (a logical value to return only common fields, default is FALSE) and `upper` (a logical value to return fields in uppercase, default is FALSE)."
      },
      {
        "question": "How does the `mapCell2Accession` function handle input validation and what does it return?",
        "answer": "The `mapCell2Accession` function coerces input IDs to character if they are not already. It performs input validation by checking if the input is a character vector. The function returns a data.table containing the mapped cell line IDs and accession numbers, along with other relevant information depending on the parameters used."
      },
      {
        "question": "What is the purpose of the `.parse_cellosaurus_lines` function and how does it process the response?",
        "answer": "The `.parse_cellosaurus_lines` function is an internal function that parses the lines of a Cellosaurus response. It takes a response object, extracts the body as a string, splits it into lines, and then uses the `Map` function to extract sections of the Cellosaurus data between 'ID' and '//' markers. This function helps in organizing the raw response data into a more structured format for further processing."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/tests/testthat/test_pubchem_rest_1.R",
    "language": "R",
    "content": "library(AnnotationGx)\nlibrary(testthat)\nlibrary(checkmate)\n\ncompounds <- c(\"temozolomide\", \"erlotinib\", \"TRETINOIN\", \"TRAMETINIB\", \"epigallocatechin-3-monogallate\")\n\n# Comprehensive Tests:\ntest_that(\"AnnotationGx::mapCompound2CID 5 Correct Drugs\", {\n  expected_cids <- c(5394, 176870, 444795, 11707110, 65064)\n\n  result <- mapCompound2CID(names = compounds)\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    any.missing = FALSE,\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n  result <- mapCompound2CID(names = compounds, first = TRUE)\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    any.missing = FALSE,\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n})\n\ntest_that(\"AnnotationGx::mapCID2Properties works\", {\n  result <- mapCID2Properties(ids = c(5394, 176870), properties = c(\"MolecularWeight\", \"CanonicalSMILES\"))\n  expect_data_table(\n    x = result,\n    types = c(\"integer\", \"character\", \"character\"),\n    any.missing = FALSE,\n    ncols = 3,\n    nrows = 2,\n    col.names = \"named\"\n  )\n})\n\ntest_that(\"getPubchemProperties works\", {\n  result <- getPubchemProperties()\n\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"character\"),\n    any.missing = FALSE,\n    ncols = 2,\n    min.rows = 45,\n    col.names = \"named\"\n  )\n})\n\n\ntest_that(\"AnnotationGx::getPubchemCompound 1 Incorrect Drug\", {\n  # Test for an incorrect drug, scoped so it doesnt affect the other tests\n  compounds <- c(\"BAD_DRUG_NAME\")\n  result <- getPubchemCompound(ids = compounds, from = \"name\", to = \"cids\")\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n\n  failed_queries <- attributes(result)$failed\n\n  expect_list(\n    failed_queries,\n    len = 1,\n    any.missing = FALSE,\n    names = \"named\"\n  )\n\n  expect_equal(names(failed_queries), c(\"BAD_DRUG_NAME\"))\n})\n\n\ntest_that(\"AnnotationGx::getPubchemCompound 2 Incorrect Drugs in a list\", {\n  # Test for an incorrect drug, scoped so it doesnt affect the other tests\n  compounds <- c(\"BAD_DRUG_NAME\", compounds, \"Another bad drug\")\n  result <- getPubchemCompound(ids = compounds, from = \"name\", to = \"cids\")\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n\n  failed_queries <- attributes(result)$failed\n\n  expect_list(\n    failed_queries,\n    len = 2,\n    any.missing = FALSE,\n    names = \"named\"\n  )\n\n  expect_equal(names(failed_queries), c(\"BAD_DRUG_NAME\", \"Another bad drug\"))\n})\n\ntest_that(\"AnnotationGx::getPubchemCompound errors if cid and not integer\", {\n  expect_error(\n    AnnotationGx::getPubchemCompound(\n      ids = c(5394, \"PUGREST.BadRequest\"),\n      from = \"cid\", to = \"property\",\n      properties = c(\"Title\", \"MolecularFormula\", \"InChIKey\", \"CanonicalSMILES\")\n    )\n  )\n})\n\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `mapCompound2CID` function in the given code snippet, and how is it being tested?",
        "answer": "The `mapCompound2CID` function is used to map compound names to their corresponding PubChem Compound IDs (CIDs). In the test, it's being used with a list of 5 compound names. The test checks if the function returns a data table with the correct structure (2 columns: character and integer), the right number of rows (equal to the number of input compounds), and no missing values. It also tests the function with the `first = TRUE` parameter, which likely returns only the first CID for each compound if multiple exist."
      },
      {
        "question": "How does the code handle incorrect drug names in the `getPubchemCompound` function, and what assertions are made to verify this behavior?",
        "answer": "The code tests the `getPubchemCompound` function with incorrect drug names to ensure proper error handling. It uses `expect_data_table` to check if the function still returns a data table with the correct structure, even for incorrect inputs. The code also checks for a 'failed' attribute in the result, using `expect_list` to verify that it contains the names of the failed queries. For example, when 'BAD_DRUG_NAME' is used, the test expects this name to appear in the failed queries list. This ensures that the function gracefully handles invalid inputs without crashing."
      },
      {
        "question": "What is the purpose of the `getPubchemProperties` function, and how is its output validated in the test?",
        "answer": "The `getPubchemProperties` function appears to retrieve a list of available properties from PubChem. The test for this function uses `expect_data_table` to verify that the output is a data table with specific characteristics. It checks that the result has 2 columns of character type, no missing values, named columns, and at least 45 rows. This suggests that the function is expected to return a comprehensive list of property names and their descriptions, with a minimum of 45 different properties available from PubChem."
      }
    ],
    "completion_tasks": [
      {
        "partial": "test_that(\"AnnotationGx::mapCompound2CID 5 Correct Drugs\", {\n  expected_cids <- c(5394, 176870, 444795, 11707110, 65064)\n\n  result <- mapCompound2CID(names = compounds)\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    any.missing = FALSE,\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n  result <- mapCompound2CID(names = compounds, first = TRUE)\n  # Complete the test for the second result\n})",
        "complete": "test_that(\"AnnotationGx::mapCompound2CID 5 Correct Drugs\", {\n  expected_cids <- c(5394, 176870, 444795, 11707110, 65064)\n\n  result <- mapCompound2CID(names = compounds)\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    any.missing = FALSE,\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n  result <- mapCompound2CID(names = compounds, first = TRUE)\n  expect_data_table(\n    x = result,\n    types = c(\"character\", \"integer\"),\n    any.missing = FALSE,\n    ncols = 2,\n    nrows = length(compounds),\n    col.names = \"named\"\n  )\n})"
      },
      {
        "partial": "test_that(\"AnnotationGx::getPubchemCompound errors if cid and not integer\", {\n  # Complete the test to check for an error when non-integer CID is provided\n})",
        "complete": "test_that(\"AnnotationGx::getPubchemCompound errors if cid and not integer\", {\n  expect_error(\n    AnnotationGx::getPubchemCompound(\n      ids = c(5394, \"PUGREST.BadRequest\"),\n      from = \"cid\", to = \"property\",\n      properties = c(\"Title\", \"MolecularFormula\", \"InChIKey\", \"CanonicalSMILES\")\n    )\n  )\n})"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/readii.git",
    "file": "../../../../repos/readii/src/readii/feature_extraction.py",
    "language": "py",
    "content": "from token import OP\nfrom venv import logger\nfrom imgtools.io import read_dicom_series\nfrom itertools import chain\nfrom joblib import Parallel, delayed\nfrom radiomics import featureextractor, imageoperations, logging\n\nimport os\nimport pandas as pd\nimport numpy as np\nimport SimpleITK as sitk\n\nfrom readii.image_processing import (\n    flattenImage, \n    alignImages, \n    padSegToMatchCT, \n    getROIVoxelLabel, \n    displayImageSlice, \n    displayCTSegOverlay, \n    getROICenterCoords, \n    getCroppedImages,    \n)\n\nfrom readii.loaders import (\n    loadDicomSITK, \n    loadRTSTRUCTSITK, \n    loadSegmentation,\n) \n\nfrom readii.metadata import (\n    saveDataframeCSV, \n    matchCTtoSegmentation,\n    getSegmentationType,\n)\nfrom readii.negative_controls import (\n    applyNegativeControl,\n)\n\nfrom readii.utils import get_logger\n\nfrom typing import Optional, Any\nfrom collections import OrderedDict\n\nlogger = get_logger()\n\ndef singleRadiomicFeatureExtraction(\n    ctImage: sitk.Image,\n    roiImage: sitk.Image,\n    pyradiomicsParamFilePath: Optional[str] = \"./src/readii/data/default_pyradiomics.yaml\",\n    negativeControl: Optional[str] = None,\n    randomSeed: Optional[int] = None,\n) -> OrderedDict[Any, Any]:\n    \"\"\"Function to perform radiomic feature extraction for a single CT image and its corresponding segmentation.\n       CT and segmentation will be aligned and cropped prior to extraction.\n\n    Parameters\n    ----------\n    ctImage : sitk.Image\n        CT image to perform feature extraction on. Will be cropped and potentially generate a negative control (see negativeControl arg)\n    roiImage : sitk.Image\n        Region of interest (ROI) to extract radiomic features from within the CT.\n    pyradiomicsParamFilePath : str\n        Path to file containing configuration settings for pyradiomics feature extraction. Will use the provided config file in 'data/' by default if no file passed in.\n    negativeControl : str\n        Name of negative control to generate from the CT to perform feature extraction on. If set to None, will extract features from original CT image.\n    randomSeed : int\n        Value to set random seed with for negative control creation to be reproducible.\n        \n    Returns\n    -------\n    OrderedDict[Any, Any]\n        Dictionary containing image metadata, versions for key packages used for extraction, and radiomic features\n    \"\"\"\n    # If no pyradiomics paramater file passed, use default\n    if pyradiomicsParamFilePath == None:\n        pyradiomicsParamFilePath = \"./src/readii/data/default_pyradiomics.yaml\"\n\n    # In case segmentation contains extra axis, flatten to 3D by removing it\n    roiImage = flattenImage(roiImage)\n    # Segmentation has different origin, align it to the CT for proper feature extraction\n    alignedROIImage = alignImages(ctImage, roiImage)\n\n    # Get pixel value for the segmentation\n    segmentationLabel: int = getROIVoxelLabel(alignedROIImage)\n\n    # Check that CT and segmentation correspond, segmentationLabel is present, and dimensions match\n    segBoundingBox, correctedROIImage = imageoperations.checkMask(ctImage, alignedROIImage, label=segmentationLabel)\n    \n    # Update the ROI image if a correction was generated by checkMask\n    if correctedROIImage is not None:\n        alignedROIImage = correctedROIImage\n\n    if negativeControl != None:\n        logger.info(f\"Generating {negativeControl} negative control for CT.\")\n        # Split negative control type into negative control and region of interest\n        if \"non_roi\" in negativeControl:\n            negativeControlType =  negativeControl.rsplit(\"_\", 2)[0]\n            negativeControlRegion = \"non_roi\"\n        else:\n            negativeControlComponents = negativeControl.rsplit(\"_\", 1)\n            negativeControlType = negativeControlComponents[0]\n            negativeControlRegion = negativeControlComponents[1]\n        logger.debug(f\"Negative control region: {negativeControlRegion}\")\n        logger.debug(f\"Negative control type: {negativeControlType}\")\n        # Make negative control version of ctImage\n        ctImage_nc: sitk.Image | np.ndarray = applyNegativeControl(\n            baseImage=ctImage,\n            negativeControlType=negativeControlType,\n            negativeControlRegion=negativeControlRegion,\n            roiMask=alignedROIImage,\n            randomSeed=randomSeed\n        )\n        croppedCT, croppedROI = imageoperations.cropToTumorMask(ctImage_nc, alignedROIImage, segBoundingBox)\n    else:\n        # Crop the image and mask to a bounding box around the mask to reduce volume size to process\n        croppedCT, croppedROI = imageoperations.cropToTumorMask(ctImage, alignedROIImage, segBoundingBox)\n\n    # Load PyRadiomics feature extraction parameters to use\n    # Initialize feature extractor with parameters\n    try:\n        logger.info(\"Setting up Pyradiomics feature extractor...\")\n        featureExtractor = featureextractor.RadiomicsFeatureExtractor(pyradiomicsParamFilePath)\n    except OSError as e:\n        logger.error(f\"Supplied pyradiomics parameter file {pyradiomicsParamFilePath} does not exist or is not at that location: {e}\")\n        raise\n\n    try:\n        logger.info(\"Starting radiomic feature extraction...\")\n        # Extract radiomic features from CT with segmentation as mask\n        idFeatureVector = featureExtractor.execute(croppedCT, croppedROI, label=segmentationLabel)\n    except Exception as e:\n        logger.error(f\"An error occurred while extracting radiomic features: {e}\")\n        raise\n\n    return idFeatureVector\n\n\ndef radiomicFeatureExtraction(\n    imageMetadataPath: str,\n    imageDirPath: str,\n    roiNames: Optional[str] = None,\n    pyradiomicsParamFilePath: Optional[str] = \"src/readii/data/default_pyradiomics.yaml\",\n    outputDirPath: Optional[str] = None,\n    negativeControl: Optional[str] = None,\n    randomSeed: Optional[int] = None,\n    parallel: bool = False,\n    keep_running: bool = False\n) -> pd.DataFrame:\n    \"\"\"Perform radiomic feature extraction using PyRadiomics on CT images with a corresponding segmentation.\n       Utilizes outputs from med-imagetools (https://github.com/bhklab/med-imagetools) run on the image dataset.\n\n    Parameters\n    ----------\n    imageMetadataPath : str\n        Path to csv file created by matchCTtoSegmentation function that contains a CT and matching segmentation in each row.\n    imageDirPath : str\n        Path to the directory containing the directory of CT and segmentation images. This directory should contain the .imgtools directory from the med-imagetools run\n        and be the same as the input path used in med-imagetools\n    roiNames : str\n        Name pattern for the ROIs to load for the RTSTRUCTs. Can be None for DICOM SEG segmentations.\n    pyradiomicsParamFilePath : str\n        Path to file containing configuration settings for pyradiomics feature extraction. Will use the provided config file in 'data/' by default if no file passed in.\n    outputDirPath : str\n        Path to directory save the dataframe of extracted features to as a csv\n    negativeControl : str\n        Name of negative control to generate from the CT to perform feature extraction on. If set to None, will extract features from original CT image.\n    randomSeed : int\n        Value to set random seed with for negative control creation to be reproducible.\n    parallel : bool\n        Flag to decide whether to run extraction in parallel.\n    keep_running : bool\n        Flag to keep pipeline running even when feature extraction for a patient fails.\n    Returns\n    -------\n    pd.DataFrame\n        Dataframe containing the image metadata and extracted radiomic features.\n    \"\"\"\n    # Setting pyradiomics verbosity lower\n    radiomics_logger: logging.Logger = logging.getLogger(\"radiomics\")\n    radiomics_logger.setLevel(logging.ERROR)\n\n    # If no pyradiomics paramater file passed, use default\n    if pyradiomicsParamFilePath == None:\n        pyradiomicsParamFilePath = \"./src/readii/data/default_pyradiomics.yaml\"\n\n    # Load in summary file generated by radiogenomic_pipeline\n    pdImageInfo = pd.read_csv(imageMetadataPath, header=0)\n\n    # Get array of unique CT series' IDs to iterate over\n    ctSeriesIDList = pdImageInfo[\"series_CT\"].unique()\n\n    def featureExtraction(ctSeriesID):\n        \"\"\"Function to extract PyRadiomics features for all ROIs present in a CT. Inner function so it can be run in parallel with joblib.\"\"\"\n        # Get all info rows for this ctSeries\n\n        try:\n            ctSeriesInfo = pdImageInfo.loc[pdImageInfo[\"series_CT\"] == ctSeriesID]\n            patID = ctSeriesInfo.iloc[0][\"patient_ID\"]\n\n            logger.info(f\"Processing {patID}\")\n\n            # Get absolute path to CT image files\n            ctDirPath = os.path.join(imageDirPath, ctSeriesInfo.iloc[0][\"folder_CT\"])\n            # Load CT by passing in specific series to find in a directory\n            ctImage = read_dicom_series(path=ctDirPath, series_id=ctSeriesID)\n\n            # Get list of segmentations to iterate over\n            segSeriesIDList = ctSeriesInfo[\"series_seg\"].unique()\n\n            # Initialize dictionary to store radiomics data for each segmentation (image metadata + features)\n            ctAllData = []\n\n            # Loop over every segmentation associated with this CT - only loading CT once\n            for segCount, segSeriesID in enumerate(segSeriesIDList):\n                segSeriesInfo = ctSeriesInfo.loc[ctSeriesInfo[\"series_seg\"] == segSeriesID]\n\n                # Check that a single segmentation file is being processed\n                if len(segSeriesInfo) > 1:\n                    # Check that if there are multiple rows that it's not due to a CT with subseries (this is fine, the whole series is loaded)\n                    if not segSeriesInfo.duplicated(subset=[\"series_CT\"], keep=False).all():\n                        raise RuntimeError(\n                            \"Some kind of duplication of segmentation and CT matches not being caught. Check seg_and_ct_dicom_list in radiogenomic_output.\"\n                        )\n\n                # Get absolute path to segmentation image file\n                segFilePath = os.path.join(\n                    imageDirPath, segSeriesInfo.iloc[0][\"file_path_seg\"]\n                )\n                # Get dictionary of ROI sitk Images for this segmentation file\n                segImages = loadSegmentation(\n                    segFilePath,\n                    modality=segSeriesInfo.iloc[0][\"modality_seg\"],\n                    baseImageDirPath=ctDirPath,\n                    roiNames=roiNames,\n                )\n\n                # Check that this series has ROIs to extract from (dictionary isn't empty)\n                if not segImages:\n                    log_msg = f\"CT {ctSeriesID} and segmentation {segSeriesID} has no ROIs or no ROIs with the label {roiNames}. Moving to next segmentation.\"\n                    logger.warning(log_msg)\n\n                else:\n                    # Loop over each ROI contained in the segmentation to perform radiomic feature extraction\n                    for roiCount, roiImageName in enumerate(segImages):\n                        # ROI counter for image metadata output\n                        roiNum = roiCount + 1\n\n                        # Extract features listed in the parameter file\n                        logger.info(f\"Calculating radiomic features for segmentation: {roiImageName}\")\n\n                        # Get sitk Image object for this ROI\n                        roiImage = segImages[roiImageName]\n\n                        # Exception catch for if the segmentation dimensions do not match that original image\n                        try:\n                            # Check if segmentation just has an extra axis with a size of 1 and remove it\n                            if roiImage.GetDimension() > 3 and roiImage.GetSize()[3] == 1:\n                                roiImage = flattenImage(roiImage)\n\n                            # Check that image and segmentation mask have the same dimensions\n                            if ctImage.GetSize() != roiImage.GetSize():\n                                # Checking if number of segmentation slices is less than CT\n                                if ctImage.GetSize()[2] > roiImage.GetSize()[2]:\n                                    logger.warning(\n                                        f\"Slice number mismatch between CT and segmentation for {patID}.\"\n                                        f\"ctImage.GetSize(): {ctImage.GetSize()}\"\n                                        f\"roiImage.GetSize(): {roiImage.GetSize()}\"\n                                        \"Padding segmentation to match.\"\n                                    )\n                                    roiImage = padSegToMatchCT(\n                                        ctDirPath, segFilePath, ctImage, roiImage\n                                    )\n                                    logger.warning(\n                                        f\"Padded segmentation to match CT for {patID}.\"\n                                        \"roiImage.GetSize() after padding: {roiImage.GetSize()}\"\n                                    )\n                                else:\n                                    raise RuntimeError(\n                                        \"CT and ROI dimensions do not match.\"\n                                    )\n\n                        # Catching CT and segmentation size mismatch error\n                        except RuntimeError as e:\n                            logger.error(str(e))\n\n                        # Extract radiomic features from this CT/segmentation pair\n                        idFeatureVector = singleRadiomicFeatureExtraction(\n                            ctImage,\n                            roiImage=roiImage,\n                            pyradiomicsParamFilePath=pyradiomicsParamFilePath,\n                            negativeControl=negativeControl,\n                            randomSeed=randomSeed\n                        )\n\n                        # Create dictionary of image metadata to append to front of output table\n                        sampleROIData = {\n                            \"patient_ID\": patID,\n                            \"study_description\": segSeriesInfo.iloc[0][\n                                \"study_description_CT\"\n                            ],\n                            \"series_UID\": segSeriesInfo.iloc[0][\"series_CT\"],\n                            \"series_description\": segSeriesInfo.iloc[0][\n                                \"series_description_CT\"\n                            ],\n                            \"image_modality\": segSeriesInfo.iloc[0][\"modality_CT\"],\n                            \"instances\": segSeriesInfo.iloc[0][\"instances_CT\"],\n                            \"seg_series_UID\": segSeriesInfo.iloc[0][\"series_seg\"],\n                            \"seg_modality\": segSeriesInfo.iloc[0][\"modality_seg\"],\n                            \"seg_ref_image\": segSeriesInfo.iloc[0][\"reference_ct_seg\"],\n                            \"roi\": roiImageName,\n                            \"roi_number\": roiNum,\n                            \"negative_control\": negativeControl,\n                        }\n\n                        # Concatenate image metadata with PyRadiomics features\n                        sampleROIData.update(idFeatureVector)\n                        # Store this ROI's info in the segmentation level list\n                        ctAllData.append(sampleROIData)\n\n            return ctAllData\n            ###### END featureExtraction #######\n        except Exception as e:\n            if keep_running:\n                logger.error(f\"Error processing patient {patID}, series {ctSeriesID}: {e}\")\n                # Log the error and continue without raising the exception\n            else:\n                # Raise the exception if keep_running is False\n                raise e\n\n    # Extract radiomic features for each CT, get a list of dictionaries\n    # Each dictioary contains features for each ROI in a single CT\n    if not parallel:\n        # Run feature extraction over samples in sequence - will be slower\n        features = [featureExtraction(ctSeriesID) for ctSeriesID in ctSeriesIDList]\n    else:\n        # Run feature extraction in parallel\n        features = Parallel(n_jobs=-1, require=\"sharedmem\")(\n            delayed(featureExtraction)(ctSeriesID) for ctSeriesID in ctSeriesIDList\n        )\n\n\n    logger.info(\"Finished feature extraction.\")\n\n    # Filter out None and ensure each result is a list (even if it's empty)\n    features = [f if isinstance(f, list) else [f] for f in features if f is not None]\n\n    # Flatten the list of dictionaries (happens when there are multiple ROIs or SEGs associated with a single CT)\n    flatFeatures = list(chain.from_iterable(features))\n    # Convert list of feature sets into a pandas dataframe to save out\n    featuresTable = pd.DataFrame(flatFeatures)\n\n    if outputDirPath != None:\n        if not os.path.exists(outputDirPath):\n            logger.info(f\"Directory {outputDirPath} does not exist. Creating...\")\n            os.makedirs(outputDirPath)\n        else:\n            logger.warning(f\"Directory {outputDirPath} already exists. Will overwrite contents.\")\n\n        logger.info(\"Saving output to file...\")\n        datasetName = imageMetadataPath.partition(\"match_list_\")[2]\n        # Setup output file name with the dataset name as a suffix\n        if negativeControl == None:\n            outFileName = \"radiomicfeatures_original_\" + datasetName\n        else:\n            # Add negative control identifier to output file name\n            outFileName = \"radiomicfeatures_\" + negativeControl + \"_\" + datasetName\n\n        # Join outputDirPath, a features directory, and the output file name\n        outputFilePath = os.path.join(outputDirPath, \"features/\", outFileName)\n        # Save out the features\n        saveDataframeCSV(featuresTable, outputFilePath)\n\n    return featuresTable\n",
    "qa_pairs": [
      {
        "question": "What is the purpose of the `singleRadiomicFeatureExtraction` function and what are its main input parameters?",
        "answer": "The `singleRadiomicFeatureExtraction` function performs radiomic feature extraction for a single CT image and its corresponding segmentation. Its main input parameters are:\n1. `ctImage`: A SimpleITK Image object representing the CT image.\n2. `roiImage`: A SimpleITK Image object representing the Region of Interest (ROI) segmentation.\n3. `pyradiomicsParamFilePath`: Path to the PyRadiomics configuration file.\n4. `negativeControl`: Optional parameter to generate a negative control from the CT image.\n5. `randomSeed`: Optional parameter to set a random seed for reproducible negative control creation."
      },
      {
        "question": "How does the `radiomicFeatureExtraction` function handle multiple segmentations associated with a single CT image?",
        "answer": "The `radiomicFeatureExtraction` function handles multiple segmentations associated with a single CT image by:\n1. Loading the CT image once.\n2. Iterating over each segmentation series ID associated with the CT.\n3. For each segmentation:\n   a. Loading the segmentation file.\n   b. Extracting ROIs from the segmentation.\n   c. Performing feature extraction for each ROI using the `singleRadiomicFeatureExtraction` function.\n4. Storing the extracted features for all ROIs and segmentations in a list of dictionaries.\n5. Finally, flattening the list of dictionaries to create a single pandas DataFrame containing features for all ROIs across all segmentations."
      },
      {
        "question": "What is the purpose of the `parallel` parameter in the `radiomicFeatureExtraction` function, and how does it affect the execution of the feature extraction process?",
        "answer": "The `parallel` parameter in the `radiomicFeatureExtraction` function determines whether the feature extraction process should be run in parallel or sequentially:\n\n1. When `parallel` is set to `False` (default), the function processes each CT series sequentially using a list comprehension:\n   ```python\n   features = [featureExtraction(ctSeriesID) for ctSeriesID in ctSeriesIDList]\n   ```\n\n2. When `parallel` is set to `True`, the function uses joblib's `Parallel` and `delayed` to process CT series in parallel:\n   ```python\n   features = Parallel(n_jobs=-1, require=\"sharedmem\")(\n       delayed(featureExtraction)(ctSeriesID) for ctSeriesID in ctSeriesIDList\n   )\n   ```\n\nParallel execution can significantly speed up the feature extraction process, especially for large datasets, by utilizing multiple CPU cores simultaneously."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [
        "os",
        "pandas",
        "numpy",
        "SimpleITK"
      ],
      "from_imports": [
        "token.OP",
        "venv.logger",
        "imgtools.io.read_dicom_series",
        "itertools.chain",
        "joblib.Parallel",
        "radiomics.featureextractor",
        "readii.image_processing.flattenImage",
        "readii.loaders.loadDicomSITK",
        "readii.metadata.saveDataframeCSV",
        "readii.negative_controls.applyNegativeControl",
        "readii.utils.get_logger",
        "typing.Optional",
        "collections.OrderedDict"
      ]
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/AnnotationGx.git",
    "file": "../../../../repos/AnnotationGx/R/AnnotationGx-data.R",
    "language": "R",
    "content": "#' gdsc_sampleMetadata is some preprocessed sample metadata from the GDSC dataset\n#'\n#' A preprocessed version of the sample metadata from the GDSC dataset. \n#' This dataset is provided in the package to test the functionality of the package.\n#' The original dataset can be downloaded from the CancerRxGene website.\n#'\n#' @format A data table with 5 columns and 1001 rows.\n#' \\describe{\n#'  \\item{GDSC.Sample_Name}{`char` The name of the cell line in the GDSC dataset.}\n#'  \\item{GDSC.COSMIC_ID}{`int` The COSMIC ID of the cell line in the GDSC dataset.}\n#' }\n#' @usage data(GDSC_sampleMetadata)\n#' @examples\n#' data(GDSC_sampleMetadata)\n#' head(GDSC_sampleMetadata)\n#' @source https://www.cancerrxgene.org/\n\"GDSC_sampleMetadata\"\n\n#' GDSC_treatmentMetadata is some preprocessed treatment metadata from the GDSC dataset\n#' \n\"GDSC_treatmentMetadata\"\n\n\n#' CCLE_sampleMetadata is some preprocessed sample metadata from the CCLE dataset\n#' \n\"CCLE_sampleMetadata\"\n\n#' CCLE_treatmentMetadata is some preprocessed treatment metadata from the CCLE dataset\n#' \n\"CCLE_treatmentMetadata\"\n\n#' CTRP_sampleMetadata is some preprocessed sample metadata from the CTRP dataset\n#' \n\"CTRP_sampleMetadata\"\n\n#' CTRP_treatmentMetadata is some preprocessed treatment metadata from the CTRP dataset\n#' \n\"CTRP_treatmentMetadata\"\n\n#' gCSI_sampleMetadata is some preprocessed sample metadata from the gCSI dataset\n#' \n\"gCSI_sampleMetadata\"\n\n#' gCSI_treatmentMetadata is some preprocessed treatment metadata from the gCSI dataset\n#' \n\"gCSI_treatmentMetadata\"",
    "qa_pairs": [
      {
        "question": "What is the purpose of the 'GDSC_sampleMetadata' object in this R code snippet?",
        "answer": "The 'GDSC_sampleMetadata' object is a preprocessed version of sample metadata from the GDSC (Genomics of Drug Sensitivity in Cancer) dataset. It is included in the package to test its functionality and contains information about cell lines, including their names and COSMIC IDs."
      },
      {
        "question": "How many columns and rows does the 'GDSC_sampleMetadata' data table contain, and what information is provided in these columns?",
        "answer": "The 'GDSC_sampleMetadata' data table contains 5 columns and 1001 rows. Two of the columns are explicitly described: 'GDSC.Sample_Name' (character type) contains the name of the cell line, and 'GDSC.COSMIC_ID' (integer type) contains the COSMIC ID of the cell line in the GDSC dataset."
      },
      {
        "question": "What is the common pattern observed in the naming convention of the objects in this code snippet, and what does it suggest about the structure of the package?",
        "answer": "The objects in this code snippet follow a common naming pattern: [Dataset]_[MetadataType]. For example, 'GDSC_sampleMetadata', 'CCLE_treatmentMetadata', etc. This suggests that the package contains preprocessed metadata for multiple cancer-related datasets (GDSC, CCLE, CTRP, gCSI), with separate objects for sample and treatment metadata for each dataset."
      }
    ],
    "completion_tasks": null,
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  },
  {
    "repo": "https://github.com/bhklab/PharmacoGx.git",
    "file": "../../../../repos/PharmacoGx/R/methods-drugSensitivitySig.R",
    "language": "R",
    "content": "#' Creates a signature representing the association between gene expression (or\n#' other molecular profile) and drug dose response, for use in drug sensitivity\n#' analysis.\n#'\n#' Given a Pharmacoset of the sensitivity experiment type, and a list of drugs,\n#' the function will compute a signature for the effect gene expression on the\n#' molecular profile of a cell. The function returns the estimated coefficient,\n#' the t-stat, the p-value and the false discovery rate associated with that\n#' coefficient, in a 3 dimensional array, with genes in the first direction,\n#' drugs in the second, and the selected return values in the third.\n#'\n#' @examples\n#' data(GDSCsmall)\n#' drug.sensitivity <- drugSensitivitySig(GDSCsmall,\n#'   mDataType = \"rna\",\n#'   nthread = 1, features = fNames(GDSCsmall, \"rna\")[1]\n#' )\n#' print(drug.sensitivity)\n#'\n#' @param object `PharmacoSet` a PharmacoSet of the perturbation experiment type\n#' @param mDataType `character` which one of the molecular data types to use\n#'   in the analysis, out of dna, rna, rnaseq, snp, cnv\n#' @param drugs `character` a vector of drug names for which to compute the\n#'   signatures. Should match the names used in the PharmacoSet.\n#' @param features `character` a vector of features for which to compute the\n#'   signatures. Should match the names used in correspondant molecular data in PharmacoSet.\n#' @param cells `character` allows choosing exactly which cell lines to include for the signature fitting.\n#'   Should be a subset of sampleNames(pSet)\n#' @param tissues `character` a vector of which tissue types to include in the signature fitting.\n#'   Should be a subset of sampleInfo(pSet)$tissueid\n#' @param nthread `numeric` if multiple cores are available, how many cores\n#'   should the computation be parallelized over?\n#' @param returnValues `character` Which of estimate, t-stat, p-value and fdr\n#'   should the function return for each gene drug pair?\n#' @param sensitivity.measure `character` which measure of the drug dose\n#'   sensitivity should the function use for its computations? Use the\n#'   sensitivityMeasures function to find out what measures are available for each PSet.\n#' @param molecular.summary.stat `character` What summary statistic should be used to\n#'   summarize duplicates for cell line molecular profile measurements?\n#' @param sensitivity.summary.stat `character` What summary statistic should be used to\n#'   summarize duplicates for cell line sensitivity measurements?\n#' @param sensitivity.cutoff `numeric` Allows the user to binarize the sensitivity data using this threshold.\n#' @param standardize `character` One of \"SD\", \"rescale\", or \"none\", for the form of standardization of\n#'   the data to use. If \"SD\", the the data is scaled so that SD = 1. If rescale, then the data is scaled so that the 95%\n#'   interquantile range lies in \\[0,1\\]. If none no rescaling is done.\n#' @param molecular.cutoff Allows the user to binarize the sensitivity data using this threshold.\n#' @param molecular.cutoff.direction `character` One of \"less\" or \"greater\", allows to set direction of binarization.\n#' @param verbose `logical` 'TRUE' if the warnings and other informative message shoud be displayed\n#' @param parallel.on One of \"gene\" or \"drug\", chooses which level to parallelize computation (by gene, or by drug).\n#' @param modeling.method One of \"anova\" or \"pearson\". If \"anova\", nested linear models (including and excluding the molecular feature) adjusted for\n#'   are fit after the data is standardized, and ANOVA is used to estimate significance. If \"pearson\", partial correlation adjusted for tissue of origin are\n#'   fit to the data, and a Pearson t-test (or permutation) test are used. Note that the difference is in whether standardization is done across the whole\n#'   dataset (anova) or within each tissue (pearson), as well as the test applied.\n#' @param inference.method Should \"analytic\" or \"resampling\" (permutation testing + bootstrap) inference be used to estimate significance.\n#'   For permutation testing, QUICK-STOP is used to adaptively stop permutations. Resampling is currently only implemented for \"pearson\" modelling method.\n#' @param ... additional arguments not currently fully supported by the function\n#'\n#' @return `array` a 3D array with genes in the first dimension, drugs in the\n#'   second, and return values in the third.\n#'\n#' @importMethodsFrom CoreGx drugSensitivitySig\n#' @export\nsetMethod(\n  \"drugSensitivitySig\",\n  signature(object = \"PharmacoSet\"),\n  function(object, mDataType, drugs, features, cells, tissues, sensitivity.measure = \"auc_recomputed\",\n           molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n           sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n           returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n           sensitivity.cutoff, standardize = c(\"SD\", \"rescale\", \"none\"), molecular.cutoff = NA,\n           molecular.cutoff.direction = c(\"less\", \"greater\"),\n           nthread = 1, parallel.on = c(\"drug\", \"gene\"), modeling.method = c(\"anova\", \"pearson\"),\n           inference.method = c(\"analytic\", \"resampling\"), verbose = TRUE, ...) {\n    .drugSensitivitySigPharmacoSet(\n      object, mDataType, drugs, features, cells, tissues, sensitivity.measure,\n      molecular.summary.stat, sensitivity.summary.stat, returnValues,\n      sensitivity.cutoff, standardize, molecular.cutoff, molecular.cutoff.direction,\n      nthread, parallel.on, modeling.method, inference.method, verbose, ...\n    )\n  }\n)\n\n#' @import parallel\n#' @importFrom SummarizedExperiment assayNames assay\n#' @keywords internal\n.drugSensitivitySigPharmacoSet <- function(object,\n                                           mDataType,\n                                           drugs,\n                                           features,\n                                           cells,\n                                           tissues,\n                                           sensitivity.measure = \"auc_recomputed\",\n                                           molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n                                           sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n                                           returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n                                           sensitivity.cutoff, standardize = c(\"SD\", \"rescale\", \"none\"),\n                                           molecular.cutoff = NA,\n                                           molecular.cutoff.direction = c(\"less\", \"greater\"),\n                                           nthread = 1,\n                                           parallel.on = c(\"drug\", \"gene\"),\n                                           modeling.method = c(\"anova\", \"pearson\"),\n                                           inference.method = c(\"analytic\", \"resampling\"),\n                                           verbose = TRUE,\n                                           ...) {\n\n  ### This function needs to: Get a table of AUC values per cell line / drug\n  ### Be able to recompute those values on the fly from raw data if needed to change concentration\n  ### Be able to choose different summary methods on fly if needed (need to add annotation to table to tell what summary method previously used)\n  ### Be able to extract genomic data\n  ### Run rankGeneDrugSens in parallel at the drug level\n  ### Return matrix as we had before\n\n  # sensitivity.measure <- match.arg(sensitivity.measure)\n  molecular.summary.stat <- match.arg(molecular.summary.stat)\n  sensitivity.summary.stat <- match.arg(sensitivity.summary.stat)\n  standardize <- match.arg(standardize)\n  molecular.cutoff.direction <- match.arg(molecular.cutoff.direction)\n  parallel.on <- match.arg(parallel.on)\n  dots <- list(...)\n  ndots <- length(dots)\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n\n\n  if (is.null(dots[[\"sProfiles\"]]) & !all(sensitivity.measure %in% colnames(sensitivityProfiles(object)))) {\n    stop(sprintf(\"Invalid sensitivity measure for %s, choose among: %s\", annotation(object)$name, paste(colnames(sensitivityProfiles(object)), collapse = \", \")))\n  }\n\n  if (!(mDataType %in% names(molecularProfilesSlot(object)))) {\n    stop(sprintf(\"Invalid mDataType for %s, choose among: %s\", annotation(object)$name, paste(names(molecularProfilesSlot(object)), collapse = \", \")))\n  }\n  switch(S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation,\n    \"mutation\" = {\n      if (!is.element(molecular.summary.stat, c(\"or\", \"and\"))) {\n        stop(\"Molecular summary statistic for mutation must be either 'or' or 'and'\")\n      }\n    },\n    \"fusion\" = {\n      if (!is.element(molecular.summary.stat, c(\"or\", \"and\"))) {\n        stop(\"Molecular summary statistic for fusion must be either 'or' or 'and'\")\n      }\n    },\n    \"rna\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop(\"Molecular summary statistic for rna must be either 'mean', 'median', 'first' or 'last'\")\n      }\n    },\n    \"cnv\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop(\"Molecular summary statistic for cnv must be either 'mean', 'median', 'first' or 'last'\")\n      }\n    },\n    \"rnaseq\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop(\"Molecular summary statistic for rna must be either 'mean', 'median', 'first' or 'last'\")\n      }\n    },\n    \"isoform\" = {\n      if (!is.element(molecular.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n        stop(\"Molecular summary statistic for rna must be either 'mean', 'median', 'first' or 'last'\")\n      }\n    },\n    stop(sprintf(\"No summary statistic for %s has been implemented yet\", S4Vectors::metadata(molecularProfilesSlot(object)[[mDataType]])$annotation))\n  )\n\n  if (!is.element(sensitivity.summary.stat, c(\"mean\", \"median\", \"first\", \"last\"))) {\n    stop(\"Sensitivity summary statistic for sensitivity must be either 'mean', 'median', 'first' or 'last'\")\n  }\n\n  if (missing(sensitivity.cutoff)) {\n    sensitivity.cutoff <- NA\n  }\n  if (missing(drugs)) {\n    if(is.null(dots[[\"sProfiles\"]])){\n      drugn <- drugs <- treatmentNames(object)\n    } else {\n      drugn <- drugs <- rownames(dots[[\"sProfiles\"]])\n    }\n  } else {\n    drugn <- drugs\n  }\n\n  if (missing(cells)) {\n    celln <- cells <- sampleNames(object)\n  } else {\n    celln <- cells\n  }\n\n  availcore <- parallel::detectCores()\n\n  if (nthread > availcore) {\n    nthread <- availcore\n  }\n\n  if (parallel.on == \"drug\") {\n    nthread_drug <- nthread\n    nthread_gene <- 1\n  } else {\n    nthread_gene <- nthread\n    nthread_drug <- 1\n  }\n\n  if (missing(features)) {\n    features <- rownames(featureInfo(object, mDataType))\n  } else {\n    fix <- is.element(features, rownames(featureInfo(object, mDataType)))\n    if (verbose && !all(fix)) {\n      warning(sprintf(\"%i/%i features can be found\", sum(fix), length(features)))\n    }\n    features <- features[fix]\n  }\n\n  # if(missing(modeling.method)){\n  #   modeling.method <- \"anova\"\n  # }\n  #\n  # if(missing(inference.method)){\n  #   inference.method <- \"analytic\"\n  # }\n\n  if (is.null(dots[[\"sProfiles\"]])) {\n    drugpheno.all <- lapply(sensitivity.measure, function(sensitivity.measure) {\n      return(t(summarizeSensitivityProfiles(object,\n        sensitivity.measure = sensitivity.measure,\n        summary.stat = sensitivity.summary.stat,\n        verbose = verbose\n      )))\n    })\n  } else {\n    sProfiles <- dots[[\"sProfiles\"]]\n    drugpheno.all <- list(t(sProfiles))\n  }\n\n  dix <- is.element(drugn, do.call(colnames, drugpheno.all))\n  if (verbose && !all(dix)) {\n    warning(sprintf(\"%i/%i drugs can be found\", sum(dix), length(drugn)))\n  }\n  if (!any(dix)) {\n    stop(\"None of the drugs were found in the dataset\")\n  }\n  drugn <- drugn[dix]\n\n  cix <- is.element(celln, do.call(rownames, drugpheno.all))\n  if (verbose && !all(cix)) {\n    warning(sprintf(\"%i/%i cells can be found\", sum(cix), length(celln)))\n  }\n  if (!any(cix)) {\n    stop(\"None of the cells were found in the dataset\")\n  }\n  celln <- celln[cix]\n\n  if (!missing(tissues)) {\n    celln <- celln[sampleInfo(object)[celln, \"tissueid\"] %in% tissues]\n  } else {\n    tissues <- unique(sampleInfo(object)[celln, \"tissueid\"])\n  }\n\n  molecularProfilesSlot(object)[[mDataType]] <- summarizeMolecularProfiles(\n    object = object,\n    mDataType = mDataType,\n    summary.stat = molecular.summary.stat,\n    binarize.threshold = molecular.cutoff,\n    binarize.direction = molecular.cutoff.direction,\n    verbose = verbose\n  )[features, ]\n\n  if (!is.null(dots[[\"mProfiles\"]])) {\n    mProfiles <- dots[[\"mProfiles\"]]\n    SummarizedExperiment::assay(molecularProfilesSlot(object)[[mDataType]]) <- mProfiles[features, colnames(molecularProfilesSlot(object)[[mDataType]]), drop = FALSE]\n  }\n\n  drugpheno.all <- lapply(drugpheno.all, function(x) {\n    x[intersect(phenoInfo(object, mDataType)[, \"sampleid\"], celln), , drop = FALSE]\n  })\n\n  molcellx <- phenoInfo(object, mDataType)[, \"sampleid\"] %in% celln\n\n  type <- as.factor(sampleInfo(object)[phenoInfo(object, mDataType)[molcellx, \"sampleid\"], \"tissueid\"])\n\n  if (\"batchid\" %in% colnames(phenoInfo(object, mDataType))) {\n    batch <- phenoInfo(object, mDataType)[molcellx, \"batchid\"]\n  } else {\n    batch <- rep(NA, times = nrow(phenoInfo(object, mDataType)))\n  }\n  batch[!is.na(batch) & batch == \"NA\"] <- NA\n  batch <- as.factor(batch)\n  names(batch) <- phenoInfo(object, mDataType)[molcellx, \"sampleid\"]\n  batch <- batch[rownames(drugpheno.all[[1]])]\n  if (verbose) {\n    message(\"Computing drug sensitivity signatures...\")\n  }\n\n  ### Calculate approximate number of perms needed\n\n\n\n  if (is.null(dots[[\"req_alpha\"]])) {\n    req_alpha <- 0.05 / (nrow(molecularProfilesSlot(object)[[mDataType]])) ## bonferonni correction\n  } else {\n    req_alpha <- dots[[\"req_alpha\"]]\n  }\n\n\n\n  # splitix <- parallel::splitIndices(nx = length(drugn), ncl = nthread_drug)\n  # splitix <- splitix[vapply(splitix, length, FUN.VALUE=numeric(1)) > 0]\n  mcres <- parallel::mclapply(seq_along(drugn), function(x, drugn, expr, drugpheno, type, batch, standardize, nthread, modeling.method, inference.method, req_alpha) {\n    res <- NULL\n    for (i in drugn[x]) {\n      ## using a linear model (x ~ concentration + cell + batch)\n      dd <- lapply(drugpheno, function(x) x[, i])\n      dd <- do.call(cbind, dd)\n      colnames(dd) <- seq_len(ncol(dd))\n      if (!is.na(sensitivity.cutoff)) {\n        dd <- factor(ifelse(dd > sensitivity.cutoff, 1, 0), levels = c(0, 1))\n      }\n      rr <- rankGeneDrugSensitivity(data = expr, drugpheno = dd, type = type, batch = batch, single.type = FALSE, standardize = standardize, nthread = nthread, verbose = verbose, modeling.method = modeling.method, inference.method = inference.method, req_alpha)\n      res <- c(res, list(rr$all))\n    }\n    names(res) <- drugn[x]\n    return(res)\n  },\n  drugn = drugn, expr = t(molecularProfiles(object, mDataType)[features, molcellx, drop = FALSE]),\n  drugpheno = drugpheno.all, type = type, batch = batch, nthread = nthread_gene, standardize = standardize,\n  modeling.method = modeling.method, inference.method = inference.method,\n  req_alpha = req_alpha, mc.cores = nthread_drug, mc.preschedule = FALSE\n  )\n\n  res <- do.call(c, mcres)\n  res <- res[!vapply(res, is.null, FUN.VALUE = logical(1))]\n  drug.sensitivity <- array(NA,\n    dim = c(\n      nrow(featureInfo(object, mDataType)[features, , drop = FALSE]),\n      length(res), ncol(res[[1]])\n    ),\n    dimnames = list(rownames(featureInfo(object, mDataType)[features, , drop = FALSE]), names(res), colnames(res[[1]]))\n  )\n  for (j in seq_len(ncol(res[[1]]))) {\n    ttt <- vapply(res, function(x, j, k) {\n      xx <- array(NA, dim = length(k), dimnames = list(k))\n      xx[rownames(x)] <- x[, j, drop = FALSE]\n      return(xx)\n    },\n    j = j,\n    k = rownames(featureInfo(object, mDataType)[features, , drop = FALSE]),\n    FUN.VALUE = numeric(dim(drug.sensitivity)[1])\n    )\n    drug.sensitivity[rownames(featureInfo(object, mDataType)[features, , drop = FALSE]), names(res), j] <- ttt\n  }\n\n  drug.sensitivity <- PharmacoSig(drug.sensitivity,\n    PSetName = name(object),\n    Call = as.character(match.call()),\n    SigType = \"Sensitivity\",\n    Arguments = list(\n      \"mDataType\" = mDataType,\n      \"drugs\" = drugs,\n      \"features\" = features,\n      \"cells\" = cells,\n      \"tissues\" = tissues,\n      \"sensitivity.measure\" = sensitivity.measure,\n      \"molecular.summary.stat\" = molecular.summary.stat,\n      \"sensitivity.summary.stat\" = sensitivity.summary.stat,\n      \"returnValues\" = returnValues,\n      \"sensitivity.cutoff\" = sensitivity.cutoff,\n      \"standardize\" = standardize,\n      \"molecular.cutoff\" = molecular.cutoff,\n      \"molecular.cutoff.direction\" = molecular.cutoff.direction,\n      \"nthread\" = nthread,\n      \"verbose\" = verbose\n    )\n  )\n\n  return(drug.sensitivity)\n}\n",
    "qa_pairs": [
      {
        "question": "What is the primary purpose of the `drugSensitivitySig` function in this code?",
        "answer": "The primary purpose of the `drugSensitivitySig` function is to create a signature representing the association between gene expression (or other molecular profiles) and drug dose response for use in drug sensitivity analysis. It computes a signature for the effect of gene expression on the molecular profile of a cell, returning estimated coefficients, t-statistics, p-values, and false discovery rates for gene-drug pairs."
      },
      {
        "question": "How does the function handle parallel processing for improved performance?",
        "answer": "The function uses parallel processing to improve performance. It detects the number of available cores and allows the user to specify the number of threads to use with the `nthread` parameter. The parallelization can be applied either at the drug level or gene level, controlled by the `parallel.on` parameter. It uses the `mclapply` function from the `parallel` package to distribute the workload across multiple cores."
      },
      {
        "question": "What statistical methods are available for modeling and inference in this function?",
        "answer": "The function offers two modeling methods: 'anova' and 'pearson', specified by the `modeling.method` parameter. For 'anova', nested linear models are fit after data standardization, and ANOVA is used to estimate significance. For 'pearson', partial correlations adjusted for tissue of origin are fit, and a Pearson t-test is used. The function also provides two inference methods: 'analytic' and 'resampling' (permutation testing + bootstrap), controlled by the `inference.method` parameter. The resampling method uses QUICK-STOP for adaptive stopping of permutations."
      }
    ],
    "completion_tasks": [
      {
        "partial": "setMethod(\n  \"drugSensitivitySig\",\n  signature(object = \"PharmacoSet\"),\n  function(object, mDataType, drugs, features, cells, tissues, sensitivity.measure = \"auc_recomputed\",\n           molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n           sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n           returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n           sensitivity.cutoff, standardize = c(\"SD\", \"rescale\", \"none\"), molecular.cutoff = NA,\n           molecular.cutoff.direction = c(\"less\", \"greater\"),\n           nthread = 1, parallel.on = c(\"drug\", \"gene\"), modeling.method = c(\"anova\", \"pearson\"),\n           inference.method = c(\"analytic\", \"resampling\"), verbose = TRUE, ...) {\n    # Complete the function body\n  }\n)",
        "complete": "setMethod(\n  \"drugSensitivitySig\",\n  signature(object = \"PharmacoSet\"),\n  function(object, mDataType, drugs, features, cells, tissues, sensitivity.measure = \"auc_recomputed\",\n           molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n           sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n           returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n           sensitivity.cutoff, standardize = c(\"SD\", \"rescale\", \"none\"), molecular.cutoff = NA,\n           molecular.cutoff.direction = c(\"less\", \"greater\"),\n           nthread = 1, parallel.on = c(\"drug\", \"gene\"), modeling.method = c(\"anova\", \"pearson\"),\n           inference.method = c(\"analytic\", \"resampling\"), verbose = TRUE, ...) {\n    .drugSensitivitySigPharmacoSet(\n      object, mDataType, drugs, features, cells, tissues, sensitivity.measure,\n      molecular.summary.stat, sensitivity.summary.stat, returnValues,\n      sensitivity.cutoff, standardize, molecular.cutoff, molecular.cutoff.direction,\n      nthread, parallel.on, modeling.method, inference.method, verbose, ...\n    )\n  }\n)"
      },
      {
        "partial": ".drugSensitivitySigPharmacoSet <- function(object,\n                                           mDataType,\n                                           drugs,\n                                           features,\n                                           cells,\n                                           tissues,\n                                           sensitivity.measure = \"auc_recomputed\",\n                                           molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n                                           sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n                                           returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n                                           sensitivity.cutoff, standardize = c(\"SD\", \"rescale\", \"none\"),\n                                           molecular.cutoff = NA,\n                                           molecular.cutoff.direction = c(\"less\", \"greater\"),\n                                           nthread = 1,\n                                           parallel.on = c(\"drug\", \"gene\"),\n                                           modeling.method = c(\"anova\", \"pearson\"),\n                                           inference.method = c(\"analytic\", \"resampling\"),\n                                           verbose = TRUE,\n                                           ...) {\n  # Complete the function body\n}",
        "complete": ".drugSensitivitySigPharmacoSet <- function(object,\n                                           mDataType,\n                                           drugs,\n                                           features,\n                                           cells,\n                                           tissues,\n                                           sensitivity.measure = \"auc_recomputed\",\n                                           molecular.summary.stat = c(\"mean\", \"median\", \"first\", \"last\", \"or\", \"and\"),\n                                           sensitivity.summary.stat = c(\"mean\", \"median\", \"first\", \"last\"),\n                                           returnValues = c(\"estimate\", \"pvalue\", \"fdr\"),\n                                           sensitivity.cutoff, standardize = c(\"SD\", \"rescale\", \"none\"),\n                                           molecular.cutoff = NA,\n                                           molecular.cutoff.direction = c(\"less\", \"greater\"),\n                                           nthread = 1,\n                                           parallel.on = c(\"drug\", \"gene\"),\n                                           modeling.method = c(\"anova\", \"pearson\"),\n                                           inference.method = c(\"analytic\", \"resampling\"),\n                                           verbose = TRUE,\n                                           ...) {\n  molecular.summary.stat <- match.arg(molecular.summary.stat)\n  sensitivity.summary.stat <- match.arg(sensitivity.summary.stat)\n  standardize <- match.arg(standardize)\n  molecular.cutoff.direction <- match.arg(molecular.cutoff.direction)\n  parallel.on <- match.arg(parallel.on)\n  dots <- list(...)\n  ndots <- length(dots)\n  modeling.method <- match.arg(modeling.method)\n  inference.method <- match.arg(inference.method)\n\n  if (is.null(dots[[\"sProfiles\"]]) & !all(sensitivity.measure %in% colnames(sensitivityProfiles(object)))) {\n    stop(sprintf(\"Invalid sensitivity measure for %s, choose among: %s\", annotation(object)$name, paste(colnames(sensitivityProfiles(object)), collapse = \", \")))\n  }\n\n  if (!(mDataType %in% names(molecularProfilesSlot(object)))) {\n    stop(sprintf(\"Invalid mDataType for %s, choose among: %s\", annotation(object)$name, paste(names(molecularProfilesSlot(object)), collapse = \", \")))\n  }\n\n  # ... (rest of the function implementation)\n\n  return(drug.sensitivity)\n}"
      }
    ],
    "dependencies": {
      "imports": [],
      "from_imports": []
    },
    "project_dependencies": []
  }
]