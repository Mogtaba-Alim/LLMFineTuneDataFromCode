{
  "repo": "research_papers",
  "file": "Multi-institutional_Prognostic_Modeling_in_Head_and_Neck_Cancer:_Evaluating_Impact_and_Generalizability_of_Deep_Learning_and_Radiomics.pdf",
  "language": "research_paper",
  "content": "RESEARCH ARTICLEhttps / /doi.org/10.1158/2767-9764.CRC-22-0152 OPEN ACCESS Check for updates Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impactand Generalizability of Deep Learning andRadiomics Michal Kazmierski1, 2, Mattea Welch1, 2, 3,  Sejin Kim1, 2,  Chris McIntosh1, 3, 4,  Katrina Rey-McIntyre4, Shao Hui Huang4, 5, T i r t hP a t e l3, 4,  Tony Tadic4, 5,  Michael Milosevic3, 4, 5, Fei-Fei Liu4, 5,  Adam Ryczkowski6, 7,  Joanna Kazmierska7, 8,  Zezhong Ye9, 10,  Deborah Plana9, 10,  Hugo J.W.L., Aerts910, 11,  Benjamin H. Kann9, 10,  Scott V. Bratman1, 4, 5, A n d r e wJ .H o p e4, 5, and Benjamin Haibe-Kains1, 2 ABSTRACT Artificialintelligence(AI)andmachinelearning(ML)arebecomingcriti- calindevelopinganddeployingpersonalizedmedicineandtargetedclinical trials.RecentadvancesinMLhaveenabledtheintegrationofwiderrangesofdataincludingbothmedicalrecordsandimaging(radiomics).However, the development of prognostic models is complex as no modeling strat- egy is universally superior to others and validation of developed modelsrequires large and diverse datasets to demonstrate that prognostic mod- els developed (regardless of method) from one dataset are applicable to otherdatasetsbothinternallyandexternally.Usingaretrospectivedatasetof 2, 552 patients from a single institution and a strict evaluation frame- work that included external validation on three external patient cohorts (873patients), wecrowdsourcedthedevelopmentofMLmodelstopredictoverall survival in head and neck cancer (HNC) using electronic medical records (EMR) and pretreatment radiol ogical images., To assess the rela- tivecontributionsofradiomicsinpredictingHNCprognosis wecompared12 different models using imaging and/or EMR data., The model with the highest accuracy used multitask learning on clinical data and tumor vol- ume achieving high prognostic accur acy for 2-year and lifetime survival prediction, outperformingmodelsrelyingonclinicaldataonly, engineered radiomics, or complex deep neural network architecture., However whenwe attempted to extend the best performing models from this large train- ing dataset to other institutions, we observed significant reductions in the performanceofthemodelinthosedatasets, highlightingtheimportanceofdetailedpopulation-basedreportingforAI/MLmodelutilityandstronger validationframeworks., 1. We have developed highly prognostic models for overall survival in HNC using EMRs and pretreatment radiological images based on a large, retrospectivedatasetof2, 552patientsfromourinstitution., 2. DiverseMLapproacheswereusedbyindependentinvestigators.The model with the highest accuracy used multitask learning on clinical dataandtumorvolume., 3. External validation of the top three performing models on three datasets (873 patients) with significant differences in the distribu- tionsofclinicalanddemographicvariablesdemonstratedsignificant decreasesinmodelperformance., Significance ML combined with simple prognostic factors outperformed multiple advanced CT radiomics and deep learning methods., ML mod-els provided diverse solutions for prognosis of patients with HNC but theirprognosticvalueisaffectedbydifferencesinpatientpopulationsand requireextensivevalidation., 1Department of Medical Biophysics University of Toronto,  Toronto,  Ontario, Canada.2Princess Margaret Cancer Centre,  Toronto,  Ontario,  Canada.3TECHNA Institute,  Toronto, Ontario,  Canada.4Radiation Medicine Program,  Princess Margaret Cancer Centre,  Toronto,  Ontario, Canada.5Department of Radiation Oncology,  University of Toronto,  Ontario, Canada.6Department of Medical Physics,  Greater Poland Cancer Centre,  Poznan, Poland.7Department of Electroradiology,  University of Medical Sciences,  Poznan, Poland.8Department of Radiotherapy II,  Greater Poland Cancer Centre,  Poznan, Poland.9Artificial Intelligence in Medicine (AIM) Program,  Mass General Brigham, Harvard Medical School,  Boston, Massachusetts., 10Department of Radiation Oncology Dana-Farber Cancer Institute / Brigham and Womens Hosptial, Boston,  Massachusetts.11Radiology andNuclear Medicine,  CARIM and GROW,  Maastricht University, Maastricht,  the Netherlands., M. Kazmierski, M. Welch, and S. Kim contributed as co-first authors to this article. Corresponding Author Benjamin Haibe-Kains,  Princess Margaret Research,  University Health Network, 101 College Street,  Princess Margaret Cancer ResearchTower,  11-310,  Toronto,  Ontario M5G 1L7, Canada., Phone +1 (416) 581-8626,  E-mail,  bhaibeka@uhnresearch.ca doi, 10.1158/2767-9764.CRC-22-0152 This open access article is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license., 2023 The Authors Published by the American Association for Cancer Research AACRJournals.org Cancer Res Commun, 3(6) June 2023 1140 Generalizability of Radiomics in Predictive Models of HNC Introduction The use of computer algorithms including artificial intelligence (AI) and ma- chine learning (ML) to assist in clinical oncology tasks such as predicting patientprognosis( 1).AI/MLhasbeenincreasinglyemployedwhenattempting toprocessclinicaldatafrommultiplesourcesandaidindiagnosis( 2), progno- sis(3), andcourseoftreatmentdecisions( 4), enablingamorepreciseapproach to clinical management taking individual patient characteristics into account(5)., The need for more personalized care is particularly evident in head and neck cancer (HNC) which exhibits significant heterogeneity in clinical pre- sentation,  tumor biology, and outcomes ( 6, 7), making it difficult to select the optimal management strategy for each patient., Hence, there is a current need forbetterprognostictoolstoguideclinicaldecisionmaking( 8,9). One potential sourceof novel prognostic information is the imaging datacol- lectedaspartofstandardcare.Imagingdatahavethepotentialtoincreasethe scopeofrelevantprognosticfactorsinanoninvasivemannerascomparedwith genomicsorpathology, whilehighvolumeandintrinsiccomplexityrenderitan excellentusecaseforML., Radiomics isanumbrellatermfortheemergingfield ofresearchaimingtodevelopnewnoninvasivequantitativeprognosticandpre-dictiveimagingbiomarkersusingbothhand-engineered( 10)anddeeplearning techniques( 11).InHNC, radiomicshasbeenusedtopredictpatientoutcomes (1214),  treatment response ( 15, 16),  toxicity ( 17, 18),  and discover associations betweenimagingandgenomicmarkers( 1921)., Despite the large number of promising retrospective studies the adoption of prognosticmodelsutilizingradio micsinclinicalworkflowsislimited( 22, 23)., Multiplefactorshaveaffectedadoptionincludinglackofaclearsuperiorpredic- tivemodelingstrategy( 24) relativelysmall, single-institution datasetslacking sufficientvalidationandgeneralizability, andinsufficienttransparencyandre-producibility of ML research ( 25, 26)., Although significant progress has been madeincertainareas[e.g. ensuringconsistencybetweendifferentengineered feature toolkits ( 27)], many studies do not provide sufficient details or un- derlying materials (e.g.,  code, data) to be reproduced by other groups ( 22,  28)., In addition the lack of benchmark datasets makes comparing different approacheschallenging.Currentimagequantificationmethodsbasedonengi-neeredfeaturesarealsolimitedbecauseofhighratesofcorrelatesandpotential redundancywithacceptedclinicalvariablesandbiomarkers( 13, 29, 30)., To overcome these limitations we implemented a collaborative challenge focused on reproducibility, transparency,  and generalizability of radiomic modeling for HNC prognosis., Furthermore we performed extensive external validationofmodelstoexplorehowmodelsderivedfromonedatasetmightex-tendtootherdatasetsandtodeterminewhetherstate-of-the-artmodelswould beextensible.Toachievethis, weusedalargeinternaldatasetof2, 552patients withHNCandthreeexternalindependentHNCpatientcohorts(873patients), multipleintrainstitutionalindependentinvestigatorsdevelopedHNCprognos- ticmodelsbasedonroutinepretreatmentCTimaginganddatafromelectronic medicalrecords(EMR).Webenchmarkedthe12modelingapproachesagainst baselineclinicalandradiomicsmodelsandstatisticallycomparedtheirperfor- mance.ThemodelwiththehighestaccuracyusedmultitasklearningonEMR data and tumor volume, outperforming more complex deep learning mod- els.Bycrowdsourcingourmodeldevelopmentandengagingexternalresearch groupsforvalidation, wewereabletodemonstratehowcollaborativeresearchcanbeusedtoexpeditethedevelopmentofmorerobustradiomicsmodelsfor cancerresearch.Materials and Methods RADCURE Prognostic Modeling Challenge ThechallengewasorganizedbytheRadiomicsforRadiotherapyResearchini- tiativeattheUniversityHealthNetwork( radiomics.ca )andwasopentoanyone within the University Health Network system., The protocol describing the training and test data as well as the evaluation metrics and ranking of partici- pantswaspredefinedandupdatedonthebasisoftheparticipantsfeedback, theprotocol is publicly available on GitHub (uhn-radcure-challenge)., In brief all participantshadaccesstothetrainingdatawithground-truthoutcomelabels, while the test set was held out for final evaluation., The primary objective wasto predict 2-year overall survival (OS) with the secondary goals of predicting a patients lifetime risk of death and full survival curve., We chose the binary endpointasitiscommonlyusedintheliteratureandreadilyamenabletomanystandardMLmethods.Theevaluationofthemodelsperformancewasprimar- ilybasedonthebinaryendpoint, whichwasusedtorankthesubmissions.We alsousedaverageprecision(AP)asasecondaryperformancemeasuretobreak anysubmissionties, duetoitshighersensitivitytoclassimbalance.Optionally, a subset of the models was also evaluated for their prognostic value,  that is, riskandsurvivalcurvepredictions.Importantly, theparticipantswereblinded to test set outcomes and only submitted predictions to be evaluated by the or- ganizers., We additionally created a set of benchmark models for comparison.We did not enforce any particular model type, image processing or input data (provideditwaspartoftheofficialtrainingset), althoughwedidencouragepar- ticipantstosubmitpredictionsbasedonEMRfeatures, images, andcombineddataseparately(iftheychosetouseallofthedatamodalities)., Training Dataset We collected a retrospective dataset of 2 552 patients with HNC treated withradiotherapyorcombinedradiotherapyandsystemictherapyatPrincessMar- garet (PM) Cancer Centre between 2005 and 2017 (Supplementary Table S1), which we split into training and test subsets by date of diagnosis (20052015and 20162018 for training and independent test set, respectively)., The study wasapprovedbytheInstitutionalResearchEthicsBoard(#17-5871).Theinclu- sioncriteriawere (i)availabilityofplanningCTimageandtargetcontours, (ii) atleast2yearsfollow-up(ordeathbeforethattime), and(iii)nodistantmetas- tases at diagnosis and no prior surgery., Primary gross tumor volumes (GTV)weredelineatedbyradiationoncologistsaspartofroutinetreatmentplanning., Foreachpatient weexportedtheCTimageandprimaryGTVbinarymaskin NRRDformat.Wealsoextractedthefollow-upinformation(currentasofApril2020)., The dataset was split into training ( n=1 802) and test ( n=750) sub- setsaccordingtothedateofdiagnosis(SupplementaryFig.S1).Thedatasetwas hostedonaninstitutionalhigh-performancecomputingclusterwithmulticoreCPUs and general-purpose graphics processing units which were available to allresearchpartnersformodeltraining., Baseline Models To provide baselines for comparison and a reference point for our collab-orating partners, we created three benchmark models using,  (i) standard prognostic factors used in the clinic [age, sex,  T/N stage,  and humanpapillomavirus (HPV) status] ( baseline-clinical ), (ii) primary tu- mor volume only ( baseline-volume ), (iii) handcrafted imaging features (baseline-radiomics )., All categorical variables were one-hot en- coded and missing data were handled by creating additional category representing missing value (e.g., Not tested for HPV status)., For the AACRJournals.org Cancer Res Commun; 3(6) June 2023 1141 Kazmierski et al. baseline-radiomics model we extracted all available first order, shape and textural features from the original image and all available filters (1, 316 features in total) using the PyRadiomics package (version 2.2.0,  ref., 31)a n d performedfeatureselectionusingmaximumrelevance-minimumredundancymethod ( 32). The number of selected features and model hyperparameters (l 2regularization strength) were tuned using grid search with 5-fold cross- validation., All models were built using logistic regression for the binary endpointandaproportionalhazardsmodelforthesurvivalendpoint., Tasks and Performance Metrics The main objective of the work was to predict binarized 2-year OS with the supplementarytaskofpredictinglifetimeriskofdeathandfullsurvivalcurves(in 1-month intervals from 0 to 23 months)., To evaluate and compare model performanceonthe2-yearbinarizedsurvivalpredictiontask weusedareaun- der the ROC curve (AUROC), which is a ranking metric computed over allpossible decision thresholds ( 33)., We additionally computed the area under precision-recallcurvealsoreferredtoasAP, usingtheformula, AP=/summationdisplay n(RnRn1)Pn,  where Rnand Pnare the precision and recall at a given threshold, respectively., While AUROC is insensitive to class balance AP considers the positive classonly, which can reveal pathologies under high class imbalance ( 34)., In addi- tion both metrics consider all possible operating points, which removes the need to choose a particular decision threshold (which can vary depending onthedownstreamclinicaltask).Becausethedatasetdidnotincludepatientswith follow-up time less than 2 years, we did not correct the binary metrics for censoringbias., For the lifetime risk prediction task we used concordance ( C) index,  defined as, C=/summationtext iuncensored/summationtext tj>ti1{ri>rj}+1 21/braceleftbig ri=rj/bracerightbig /summationtext iuncensored1{tj>ti}, where tiisthetimeuntildeathorcensoringforpatient i, riisthepredictedrisk scoreforpatient i, and1{}istheindicatorfunction.Theagreementbetweenthe performancemeasureswasgood(Pearson r=0.88betweenAUROCandAP, r=0.82betweenAUROCand C-index).WecomparedtheAUROCachieved bythebestmodeltotheothermodelsusingone-sided ttestandcorrectedfor multiplecomparisonsbycontrollingtheFDRat5%level., Independent Validation Wefirstassessedtheperformanceofth eprognosticmodelsusinganindepen- dentinternaldatasetcomposedofallRADCUREpatientswhosediagnosistook placeafterDecember22, 2013.Tofurtherassessthegeneralizabilityofthebestpe rf o rm i n gm od e l s , a sw e l la sth er e p r od u c i b i l i t yo fth ec h a l l e n g efr a m ew o r k , weevaluatedtheperformanceofthetopthreemodelsandthebestdeeplearn- ing model on three external datasets., The HN1 dataset is a publicly availablecollection 1of 137 patients with oropharynx and larynx tumors treated with radiochemotherapy at MAASTRO Clinic in Maastricht, the Netherlands and hasbeenusedpreviouslyinradiomicsstudies( 35).TheMDACCdataset2con- tains datafrom627patients withoropharynxcancertreated atMDAnderson 1https, //wiki.cancerimagingarchive.net/display/Public/Head-Neck-Radiomics-HN1 2https, //wiki.cancerimagingarchive.net/display/Public/HNSCCCancerCenter( 36).Finally, theGPCCHNdatasetisaprivatedatasetof298pa- tientstreatedatGreaterPolandCancerCentreinPoznan, Poland.TheHN1and MDACC datasets were obtained from the Cancer Imaging Archive ( 37)., For all three datasets we used the same patient selection criteria and preprocess-ing workflow as for the main training dataset., Validation on the HN1 dataset was performed internally within our institution. In the case of MDACC and GPCCHNdatasets weprovidedexternalcollaborators(atDana-FarberCancer InstituteandGreaterPolandCancerCentre, respectively)withcode, documen- tationandpretrainedmodelsandaskedthemtocomputethepredictionsusingtheirowninfrastructure, wewereavailabletoprovidetechnicalsupportifnec- essary., We statistically compared the variablesof interest between the internal testsetandeachoftheexternaltestdatasetsusingpairwise  2testandcorrected formultiplecomparisonsbycontrollingtheFDRat5%level., Research Reproducibility Thecodeusedtopreparethedatatrainthebaselinemodels, evaluatethemod- els, andanalyzetheresultsisavailableonGithubat https, //github.com/bhklab/ uhn-radcure-challenge .Wealsosharethemodelcodeforallthemodelsinthe same repository., Furthermore we are planning to make the complete dataset,  includinganonymizedimages, contoursandEMRdataavailableontheCancer ImagingArchive., Data Availability T h ed a t au s e di nt h i ss t u d ya r eas u b s e to ft h eR A D C U R Ed a t a s e tp u b l i c l yavailable on The Cancer Imaging Archive ( 37), https, //doi.org/10.7937/J47W- NM11., Results Toassesstheperformanceofadiversesetofprognosticmodelingstrategies we have organized an institutional competition designed to leverage a large com- pendiumofinternalandexternaldata, aswellastheexpertiseofindependentinvestigatorsusingstrictevaluationandvalidationframework( Fig.1)., The Challenge The challenge was conceived and organized by the Radiomics for Radiother- apy Research initiative at the University Health Network ( radiomics.ca )., The protocol describing the training and test data as well as the evaluation met-ricsandrankingofparticipantswaspredefined, whichhasbeenmadepublicly available(uhn-radcure-challenge).TheChallengeofficiallyopenedonApril14, 2020withtheregistrationclosingonApril30, 2020.Atotaloffourteamsreg-istered by the deadline and had access to the training data until July 25, 2020, whichwasthedeadlineforthefinalsubmissionoftheirbestmodels(eachteam couldsubmitamaximumofthreemodels).Theblindevaluationofthe12sub-mittedmodelsbytheChallengeorganizersontheindependenttestdatasettook placeuntilJuly30, 2020.ThewinnerswereannouncedonJuly31, 2020., Dataset Given the complexity of predicting survival in patients with HNC we col- lected the largest dataset to date combining EMR (i.e.,  clinical,  demographic, and interventional data) and radiological imaging data for 2, 552 patients with HNCtreatedwithdefinitiveradiotherapy.Allthedatawerecollectedandgen- erated within the PM Cancer Centre., The dataset was divided into a training set (70%) and an independent test set (30%) based on the date of diagno- sis at a predefined timepoint (December 22, 2013)., We made pretreatment 1142Cancer Res Commun 3(6) June 2023 https, / /doi.org/10.1158/2767-9764.CRC-22-0152 |CANCER RESEARCH COMMUNICATIONS Generalizability of Radiomics in Predictive Models of HNC FIGURE 1 Overview of methodology and dataset., EMR and imaging data from a large cohort of patients with HNC were made available to our research partners., The training set consisting of patients diagnosed before a prespecified date was released together with the ground-truth outcom e information (OS) and used for prognostic model development., The test set was kept private and only made available (without outcome data) after thedevelopment phase was completed., We also developed a set of simple but strong baseline models to serve as benchmark for comparison as well as a reference point for our research partners during the development of their models., To assess the generalizability of the developed prognostic models to new patient populations we performed external validation of all the models in three external datasets., Preprocessing and evaluation using the HN1dataset was performed in-house while evaluation using the MDACC and GPCCHN datasets was performed by external collaborators., AACRJournals.org Cancer Res Commun; 3(6) June 2023 1143 Kazmierski et al. TABLE 1 Summary of challenge submissions and performance metrics. All models achieved performance better than random (FDR <5%) Rank Description AUROC AP C-index 1 Deepmultitask logistic regression usingEMRfeaturesandtumorvolume., 0.823 [0.7770.866] 0.505 [0.4200.602] 0.801 [0.7570.842] 2 Fuzzy logistic regression (binary) and Cox proportional hazards model (risk prediction) using EMR features and tumor volume.0.816 [0.7670.860] 0.502 [0.4180.598] 0.746 [0.7000.788] 3 Fuzzy logistic regression (binary) or Cox proportional hazards model (risk prediction) using EMR features and engineered radiomic features.0.808 [0.7580.856] 0.490 [0.4060.583] 0.748 [0.7030.792] 4 Multitask logistic regression using EMR features., 0.798 [0.7480.845] 0.429 [0.3560.530] 0.785 [0.7400.827] 5 3D convnet using cropped image patch around the tumor with EMR features concatenated before binary classification layer.0.786 [0.7340.837] 0.420 [0.3470.525] 0.774 [0.7250.819] 6 2D convnet using largest GTV image and contour slices with EMR features concatenated after additional nonlinear encoding before binary classification layer.0.783 [0.7300.834] 0.438 [0.3600.540] 0.773 [0.7240.820] 7 3D DenseNet using cropped image patch around the tumor with EMR features concatenated before multitask prediction layer.0.780 [0.7330.824] 0.353 [0.2900.440] 0.781 [0.7400.819] 8 Multilayer perceptron (MLP) with SELU activation and binary output layer using EMR features.0.779 [0.7210.832] 0.415 [0.3430.519] 0.768 [0.7140.817] 9 Two-stream 3D DenseNet with multitask prediction layer using cropped patch around the tumor and additional downsampled context patch.0.766 [0.7180.811] 0.311 [0.2600.391] 0.748 [0.7030.790] 10 2D convnet using largest GTV image and contour slices and binary output layer.0.735 [0.6770.792] 0.357 [0.2890.455] 0.722 [0.6670.774] 11 3D convnet using cropped image patch around the tumor and binary output layer.0.717 [0.6610.770] 0.268 [0.2250.339] 0.706 [0.6530.756] 12 Fuzzy logistic regression (binary) and Cox proportional hazards model (risk prediction) using engineered radiomic features.0.716 [0.6550.772] 0.341 [0.2720.433] 0.695 [0.6380.749] contrast-enhanced CT images and binary masks of primary GTV available to our research partners., We also provided the set of available clinical variables extracted from EMR including demographic (age at diagnosis,  sex),  clinical (T,  N and overall stage,  disease site, performance status,  and HPV infectionstatus) and treatment-related (radiation dose in Gy, use of systemic therapy) characteristicsformodelingpurposes( Fig.1).Inaddition, outcomedata(time todeathorcensoring, eventindicator)wereavailableforthetrainingdataonly., Model Training and Evaluation Criteria All research participants had access to the training data with ground-truthoutcome labels (OS), while the test set was held out for final evaluation., The primaryobjectivewastopredict2-yearOS withthesecondarygoalsofpredict- ingapatientslifetimeriskofdeathandfullsurvivalcurve.Wechosethebinaryendpointasitiscommonlyusedintheliteratureandreadilyamenabletomany standardMLmethods.Theprimaryevaluationmetricforthebinaryendpoint, which was used to rank the models,  was the AUROC., We also used AP as asecondaryperformancemeasuretobreakanymodelties duetoitshighersen- sitivity to class imbalance ( 38)., Prediction of lifetime risk and survival curve predictionswasoptional andtheywerescoredusingthe C-index(39).Impor- tantly, researchpartnerswereblindedtotestsetoutcomesandonlysubmitted predictions to be evaluated by the organizers., We additionally created a set of benchmark models using only clinical only imaging data or a combination ofbothforcomparison(seeMaterialsandMethods).Wedidnotenforceanypar- ticularmodeltype, imageprocessingorinputdata(provideditwaspartofthe officialtrainingset), althoughwedidencourageparticipantstosubmitpredic- tions based on EMR features,  images, and combined data separately (if they chosetouseallofthedatamodalities).Overview of Models Twelve crowd-sourced models were developed by independent investigatorsduringaninstitutionalchallengeover2monthsbyleveragingEMRandimag-ing data., These models can be broadly classified as using EMR factors only imaging only, or combining all data sources ( Fig., 2ATable 1, f o rd e t a i l e d descriptions, seeSupplementaryMaterialsandMethods).Inadditiontothere-quired 2-year event probabilities, 10 models included lifetime risk predictions and seven included the full predicted survival curves., All models performed significantly better than random on all performance measures ( p<0.0001 bypermutationtest).Thetopmodelperformedsignificantlybetterintermsof AUROC than every other model (FDR <5%), exceptthesecond-best(FDR > 5%)., Most participants who used the imaging data relied on convolutional neural networks (convnets) to automatically learn predictive representations, only two combined and one radiomics-only model used handcrafted features(Fig., 2A). Of the convnets models two of three relied on three-dimensional (3D)convolutionoperations( Fig.2A).AlthoughallEMR-onlyapproachesused the same input data, there was significant variation in the kind of model used(linear and nonlinear, binary classifiers,  proportional hazards and multitask models, Fig., 2A). The combined models used EMR data together with either tumorvolume( n=2) engineeredradiomics( n=1)ordeeplearning( n=3)., Deep Learning Using Imaging Only Achieves Good Performance and Outperforms Engineered Radiomics Among the radiomics-only models, deep learningbased approaches per- formed better than hand-engineered features., In particular nearly all deep learningmodels(exceptone)outperformed baseline-radiomics andall 1144Cancer Res Commun, 3(6) June 2023 https, / /doi.org/10.1158/2767-9764.CRC-22-0152 |CANCER RESEARCH COMMUNICATIONS Generalizability of Radiomics in Predictive Models of HNC FIGURE 2 Model results., A,Overview of model characteristics. The characteristics are grouped into input data prediction head (i.e., how were the survival predictions made) and model type (whether the model involved any nonlinearities and/or convolutions)., PH: proportional hazards, MLP:multilayer perceptron, *: age, sex, stage, HPV status. BD Performance of all models,  including benchmark models,  in terms of 2-year AUROC, 2-year average precision and C-index of the lifetime risk,  respectively., The results are ranked by AUROC (numbers above bars indicate the overall rank of each model). Error bars represent 95% confidence intervals computed using 10,000 stratified bootstrap replicates. Dashed gray lines indicate randomguessingperformance (0.5for AUROCand C-index,0.14for AP). EG showthe KaplanMeiersurvivalestimates in low- and high-riskgroupsidentified by the best performing model in each category (combined, EMR only and radiomics),  respectively., Test set patients were stratified into two groups based on the predicted 2-year event probability at 0.5 threshold., In each case there were significant differences in survival between the predicted ri sk groups (HR, 8.64,  5.96,  and 4.50,  respectively,  p<10 18for all)., AACRJournals.org Cancer Res Commun; 3(6) June 2023 1145 Kazmierski et al. FIGURE 3 Volume dependence of predictions. Spearman rank correlation of the predictions of each model with tumor volume against performance in terms of AUROC ( A)a n d C-index ( B), respectively., The top models fall into an optimal region of low (but nonzero) volume correlation and high performance., Note that while models 1 and 2 used tumor volume as one of the input variables their predictions correlate with volume only moderately (<0.5), indicating they are able to exploit additional information present in the EMR features., Higher correlation leads to decreased performance as the predictions are increasingly driven by volume only., Most radiomics-only models fall in the high correlation region ( 0.5) although deep learning predictions correlate at notably lower level than engineered features., Interestingly the best radiomics submission (number 9) achieves the lowes t volume correlation, suggesting that it might be using volume-independent imaging characteristics., other models (Model 12; Fig. 2) in the binary prediction task (the smaller dif- ferences in C-index can be explained by the fact that most of the deep models were designed for binary classification only, and we used their binary predic-tionsasaproxyforlifetimeriskscores).Althoughourresultsindicatethatdeep learningmodelsyieldbetterperformanc ethanmodelsrelyingsolelyonhand- engineeredradiomicfeatures, thisobservation, however, theincreasingnumberof radiomics toolkits keep increasing and they offer a wealth of feature types and configuration options ( 27) prevent us from drawing a definitive conclu- sion.Nevertheless, ourresultsshowth atacarefully-tunedDLmodelcanlearn featureswithsuperiordiscriminativepowergivenasufficientlylargedataset., Theconvnet-basedmodelsshowvaryinglevelsofperformance mostlikelydue todifferencesinarchitecturesandpriorimageprocessing.Notably, thebest3D architecture(Model9)achievessuperiorperformancetothetwo-dimensional (2D)VGGNet(number10).Itincorporatesseveralinnovativefeatures, includ-ing dense connectivity ( 40, 41), two-stream architecture with a downsampled context window around the tumor and a dedicated survival prediction head (detaileddescriptioninSupplementaryMaterialsandMethods)., EMR Features Show Better Prognostic Value than Deep Learning Even in Combination While imaging features can lead to strong prognostic models, the small per- formance gap between EMR and combined models using deep radiomics (submissions 4, 6, 7) in most cases suggests the models do not learn comple-mentary image representations and that the performance is driven primarily bytheEMRfeatures( Fig.2).Althoughonecombinedmodelusingengineered features (number 3) achieved good performance, it performed worse than theexactsamemodelusingEMRfeaturesandvolumeonly(number2), indicating that the added complexity of radiomic features reduces performance, and the engineeredfeatureswerenotstrongpredictorsontheirown.Moreover, noneof theradiomics-onlymodelsperformedbetterthananyoftheEMR-onlymodels (although one convnet did outperform baseline-clinical )., A possibleexplanationissuboptimalmodeldesignthatfailstoexploitthecomplementar- ity between the data sources., All of the deep learning solutions incorporated EMR features in an ad hocfashion by concatenating them with the image representation vector and passing them to the final classification layer., While this approach is widely used, it is not clear that it is optimal in this context. Moresophisticatedmethodsofincorporatingadditionalpatient-levelinforma-tion suchas for example, joint latent spaces( 42) should be explored in future research., Impact of Volume Dependence on Model Performance Recentliteraturehasdemonstratedthatmanyradiomicsignaturesshowstrong dependenceontumorvolume( 29, 30), whichisasimpleimage-derivedfeature and a known prognostic factor in HNC ( 43)., We evaluated the correlation of all binary predictions with volume using Spearman rank correlation ( Fig., 3). Boththebaselineradiomicsmodelandthesubmissionusinghandcraftedfea- tures show high correlation (Spearman =0.79 and =0.85, respectively), suggesting that their predictions are driven primarily by volume dependency.Thepredictionsoftwooutofthreeconvnetsalsoshowmoderatetostrongcor- relationwithtumorvolume, albeitsmallerthanengineeredfeatures( >0.5)., Interestingly predictions of the best radiomics-only model (number 9) showo n l yw e a kc o r r e l a t i o nw i t hv o l u m e( =0.22) and are more discriminative thanvolumealone(AUROC =0.7 7 ) , s u gg e s ti n gth a ti tm i gh tbepo s s i b l et o learnvolume-independentimage-basedpredictors., Top Performing Model Multitask Learning with Simple I m a g eF e a t u r e sa n dE M RD a t a The top performing model (number 1) combined EMR data with a simple image-derivedmeasure, andusedaMLmodeltailoredtosurvivalprediction, a schematicoverviewofthesubmissionisshownin Fig.4.Theapproachisbased on multitask logistic regression (MTLR), first proposed by Yu and colleagues (44)Incontrastwithotherapproaches, whichfocusedonthebinaryendpoint only, MTLR is able to exploit time-to-event information by fitting a sequence 1146Cancer Res Commun, 3(6) June 2023 https, / /doi.org/10.1158/2767-9764.CRC-22-0152 |CANCER RESEARCH COMMUNICATIONS Generalizability of Radiomics in Predictive Models of HNC FIGURE 4 Top performing model., A,Overview of Deep MTLR. The model combines EMR features with tumor volume using a neural network and learns to jointly predict the probability of death at all intervals on the discretized time axis, allowing it to achieve good performance in both the binarized and lifetime risk prediction tasks., A predicted survival curve can be constructed for each individual to determine the survival probabili ty at any timepoint., B,Importance of combined input data for performance on the binary endpoint. Training the deep MTLR on EMR features only led to notably worse performance. Furthermore using a deep convolutional neural network in place of tumor volume did not improve the 2-year AUROC., of dependent logistic regression models to each interval on a discretized time axis effectively learning to predict a complete survival curve for each patient in multitask fashion., By making no restrictive assumptions about the form of the survival function the model is able to learn flexible relations between covariates and event probability that are potentially time-varying and non-proportional., We note that many prognostic models in clinical and radiomics literatureuseproportionalhazards(PH)models( 35, 45), however, thisignores the potential time-varying effect of features which MTLR is able to learn., No-tably whencomparedwiththesecond-bestmodel(whichreliesonaPHmodel) it achieves superior performance for lifetime risk prediction ( C=0.801 vs. 0.746).Theaddedflexibilityandinformation-sharingcapacityofmultitasking alsoenablesMTLRtooutperformothermodelsonthebinarytask(AUROC = 0.823, AP =0.505), eventhoughitisnotexplicitlytrainedtomaximizepredic- tiveperformanceat2years, thepredictedprobabilitiesarealsobettercalibrated(Supplementary Fig., S2).The topperforming model relies on high-level EMR features which are widely-used easy to interpret and show strong univariate association with survival (see Supplementary Materials and Methods)., Theresearch partner incorporated nonlinear interactions by passing the features through a single-layer neural network with exponential linear unit activation (46, 47),  which resulted in better performance in the development stage., The only image-derived feature used is primary tumor volume, a known prognos- tic factor in HNC. Using EMR features only led to a decrease in performance(AUROC =0.798, AP =0.429), asdidreplacingtumorvolumewithdeepimage representationslearnedbya3Dconvnet(AUROC =0.766, Fig.4A)., External Validation of Best Performing Models To evaluate the capacity of all the models to generalize to new patient popu- lations, we tested their performance on three external HNC datasets ( Figs., 1B and5). There were significant differences in the distributions of several clini- cal and demographic variables between the challenge test set and the external datasets, mostnotablydiseasesiteandHPVstatus, aswellasthetargetoutcome prevalence (Supplementary Table S1)., In line with the significant distribution shiftweobservedadropinperformanceintwooutofthreedatasets, although the performance remained significantly better than random ( p<0.0001 forall models performance by permutation test)., However the top performing model maintained its rank in all datasets except GPCCHN, where it was out- performedbythesimplerlinearmodelusingEMRfeaturesandvolume(model number2).Theoverallrankingwasfairlyconsistentbetweendatasets, withthe exception of HN1 where engineered radiomics and deep learning combinedwith EMR features achieved higher performance than in the original test set (modelnumbers3and5), however, theydidnotoutperformthetopperforming modelusingEMRfeaturesandtumorvolume.Allvalidatedmodelsperformedbetter than tumor volume alone, with the exception of the MDACC dataset where only the top performing model outperformed volume alone by a small margin., Discussion The ability to estimate a patients future disease progression is a critical pre- cursor to personalized medicine., This prediction guides treatment decisions as well as difficult personal decisions regarding risks and benefits of intensivetreatment, AI and ML have the potential to assist in this important clinical decision making paradigm., However it is not yet clear from published stud- ieswhetherthispotentialtoassistwill translatetochangesinclinicaldecision makingduetolimitedabilitiestoassessgeneralizabilityofmodels.Inthiswork, thedifferentacademicbackgroundsandcomputationalapproachesusedbyour collaboratingpartnersresultedinadiversecollectionofmodelstoevaluate.In addition, our multisite external validation of the top performing models pro- vided insights into the importance of external evaluation and understandingdifferencesinpatientpopulations., Thebestindividualapproachachievedstrongperformanceonboth2-yearand lifetime risk prediction using a multitask survival modeling framework., Thisdemonstrates the benefit of using a flexible approach designed specifically for the task of survival prediction., In addition because the approach relies on widely used and easy-to-interpret features (e.g., tumor stage,  volume), it is at- tractive from a clinical standpoint as a risk stratification and monitoring tool., The model predictions are highly significant even when adjusted for disease AACRJournals.org Cancer Res Commun, 3(6) June 2023 1147 Kazmierski et al., FIGURE 5 Externalvalidationperformanceofthetopthreesubmissionsandthebestdeeplearningsubmission(number5), AUROC( A)and C-index (B)., The red line indicates the performance achieved by the winning submission in the challenge test set. The performance of tumor volume alone is shown for comparison. sitedemonstratingthepotentialoflearningfromlargecross-sectionaldatasets, as opposed to highly curated patient subsets (which has been the dominant paradigmthusfar).However, itshouldalsobementionedthattheselectionof modelingmethodsandmodelinputsweredeterminedbyindependentinvesti- gatorsduringaninstitutionalchallenge, thereforeitisfeasiblethatmorehighly performantstrategiesexistandwerenotconsideredinthisstudy., Wefurtherassessedthegeneralizabilityofthebestprognosticmodelsusingin- dependent patient cohorts from three different institutions., In this validation phase thetopperformingmodelretaineditswinningrankintwoofthethree datasets.Theobserveddifferencesinperformancehelpusinvestigatethegen-eralizabilityofourmodels.Wehypothesizethatthedecreaseinperformanceis duetoadistributionshiftinclinicalanddemographiccharacteristics, aswellas inimageacquisitionparameters(particularlyformodels3and5).Inparticular, theGPCCHNdatasethasadisproportionatelyhighnumberofHPV patients comparedtothetrainingdataset(SupplementaryTableS1).Thisfindingoflim-itedgeneralizabilityhassignificantimplicationsfortheuseofclinicaltrialdata inthedevelopment ofpredictivemodelsorinthevalidationofmodelswithin thehighlyselectedclinicaltrialspopulationasthesetrialpopulationsmayhavesignificantvariationsfromroutineclinicalpractice( 48, 49)., This is important to highlight because a models clinical utility is not defined solelybyitsgeographicalgeneralizability( 50).Furthermore, ourexternalvali- dationexperimentsalsotestedthereusabilityofourmodelsandcode.Through interinstitutional collaborations, our code was applied to both public and pri-vate datasets by individuals not involved in development, demonstrating the accessibility of our methodologies and motivating utilization of open science forscrutinyandadvancementofscientificresults.Towardthisend, wehavealsomadeourlargetrainingsetavailableforscientificscrutinyandcollaborationvia TheCancerImagingArchive(37)whichallowsaccesstoboththeimagingand associatedclinicalvariablesusedinthisproject.Wewouldencourageallfutureeffortstocontinuetoshareasmuchdataasisfeasibletoensurereproducibility withinthefield., The utility of radiomics in HNC survival prediction has been investigated in recent studies ( 1315, 16)., We have identified several strong radiomics predic- tors however, the best performing individual model used EMR features, withprimary tumor volume as the only image-derived feature., Our conclusions match those of Ger and colleagues ( 13) who did not find significant improve- mentinprognosticperformanceofhandcraftedCTandPETimagingfeatures in HNC compared with volume alone and of Vallieres and colleagues ( 15), whosebestperformingmodelforoverallsurvivalalsocombinedEMRfeatures and volume., We further showed that although deep learningbased imag- ingmodelsgenerallyoutperformedapproachesbasedonhandcraftedfeatures, none proved superior to the combined EMR-volume model,  even when com- bined with EMR data., Deep learning methods achieve excellent performance inmanyimageprocessingtasks( 51)however, currentapproachesrequiresub- stantialamountsoftrainingdata.Theendpointcontributionofindividualdata modalities can be characterized by comparing the models performance with thebaselineEMR, volume, andradiomicsmodels( Fig.2).Bycomparingindi- vidualbaselinemodelswithdevelopedmodels, wecanseehowdifferentmodel designscontributetopredictiveperformance.Thecontributionofeachmodal- ity and their combined improvements can be seen in MTLRs performanceacrossvariousinputs( Fig.4B)., While our training dataset is the largest publicly-available single-institution HCN imaging collections, it is still relatively small compared with natural im- agedatasetsusedinMLresearch, whichoftencontainmillionsofsamples( 52)., Althoughsuchlargesamplesizesmightbeinfeasibleinamedicalcontext bet-ter data collection and sharing practices can help build more useful databases [the UK Biobank ( 53) or The Cancer Genome Atlas ( 54) are excellent exam- ples]., This is especially important in diseases with low event rates where asubstantialnumberofpatientsmightbeneededtocapturethevariationinphe- notype and outcomes., The inferior performance of radiomic models can also beattributedtosuboptimalimagingdata.Whilethepossibilitytoeasilyextractretrospective patient cohorts makes routine clinical images attractive for ra- diomicsresearch, theyareoftenacquiredforclinicalpurposesandmaynotbe sufficientlystandardizedfornewbiomarkerdiscovery.CTimagesinparticular mightnotaccuratelyreflectthebiologicaltumorcharacteristicsduetoinsuffi- cientresolution, sensitivitytoacquisitionparametersandnoise( 55, 56), aswell asthesourceofimagecontrast, whichisessentiallyelectrondensityofthetis- sue which demonstrates little texture at current image scales., This highlights the broader need of greater collaboration between ML researchers clinicians, 1148Cancer Res Commun,  3(6) June 2023 https, / /doi.org/10.1158/2767-9764.CRC-22-0152 |CANCER RESEARCH COMMUNICATIONS Generalizability of Radiomics in Predictive Models of HNC and physicists, also in data selection and experiment designwith reciprocal feedback( 57, 58)., Our study has several potential limitations. Participation in the model de- velopment was restricted to researchers at one institution which limited the number of models that could be explored., In addition the hand-engineered radiomics submissions relied on one radiomics toolkit (PyRadiomics), whileotherwidely-usedtoolkitsmakeuseofpotentiallydifferentfeaturesetsanddef- initions, however, thankstorecenteffortsinimagebiomarkerstandardization, thefeatureshavebeenshowntobelargelyconsistentbetweenthemajorimple-mentations ( 27)., While smoking status may be considered a relevant variable for HNC prognosis it is not guaranteed to be included in all predictive mod- elsanddoesnotsignificantlyimprovepredictiveperformance(SupplementaryFig.S3andS4).Furthermore, becauseourstudywasstructuredasaChallenge, selectionofmodelingmethodsandinputvariableswerelefttothediscretionof the participants., It is likely that more sophisticated ensembling methods [e.g. Bayesianmodelaveraging( 59)orstacking( 60)]couldachieveevenbetterper- formance by weighing the models according to their strengths., We leave thisexplorationforfuturework. Inthefuture wewouldliketofurtherenhancethediversityofapproachesand help us validate our conclusions by expanding our crowdsourcing efforts be-yond our institution., We are also working on collecting additional outcome information including recurrence, distant metastasis,  and treatment toxicity, which would provide a richer set of prediction targets and might be more rel-evant from a clinical standpoint., The importance of ML and AI as tools of precision medicine will continue to grow. However it is only through trans- parentandreproducibleresearchthatintegratesdiverseknowledgethatwecan begintorealizethefullpotentialofthesemethodsandpermitintegrationinto clinicalpractice.AnyclinicaltrialsthatemployAI/MLmodelsasacomponentof either patient selection, treatment selection, or attempt to translate models derivedfromclinicaltrialspopulationsshouldbeawareofintrinsicrisksrelated togeneralizabilitywhichmayimpactperformanceofthosemodelsoutsidetheboundsofthedataspaceinwhichthemodelsweregenerated., Authors Disclosures C.McIntoshreportspersonalfeesfromRaySearchandotherfromCurateout-sidethesubmittedwork, inaddition, C.McIntoshhasapatenttoEP3007771A1licensed., H.J.W.L. Aerts reports grants from NIH, EU, and personal fees from Onc.AI outside the submitted work. S.V. Bratman reports personal fees and other from Adela outside the submitted work; in addition, S.V. Bratman has apatenttoPCT/US2014/02502issuedlicensed, andwithroyaltiespaid, apatent to PCT/CA2018/000141 issued and licensed, a patent to PCT/CA2018/000203 pending and licensed,  and a patent to US63/041, 151 pending and licensed., A.J. Hope reports grants from Canadian Institutes of Health Research dur- ing the conduct of the study. B. Haibe-Kains reports personal fees from CodeOceanIncBreakThroughCancer, CQDM, andgrantsfromRocheoutsidethe submittedwork.Nodisclosureswerereportedbytheotherauthors., Authors Contributions M. Kazmierski Conceptualization,  data curation,  software, formal analy- sis,  validation,  visualization,  methodology,  writing-original draft., M. Welch: Data curation, formal analysis, methodology. S. KimData curation,  soft- ware,  formal analysis,  validation,  visualization,  methodology, writing-review and editing., C. McIntosh Software,  formal analysis,  writing-review and edit- ing.K.Rey-McIntosh,  Resources, datacuration, investigation, writing-review and editing., S.H. Huang: Resources, data curation, investigation, writing- review and editing. T. Patel:Resources, data curation, software, investigation, writing-review and editing. T. Tadic: Resources, data curation, software, writing-review and editing. M. Milosevic: Resources, data curation, software, writing-reviewandediting. F.-F. Liu: Resources,writing-reviewandediting. A. Ryczkowski: Resources,datacuration,writing-reviewandediting. J.Kazmier- ska:Resources, data curation, writing-review and editing. Z. Ye:Resources, datacuration,writing-reviewandediting. D. Plana: Resources,datacuration, writing-reviewandediting. H.J.W.L.Aerts: Resources,datacuration,software, investigation,writing-reviewandediting. B.H.Kann: Datacuration,software, investigation,writing-reviewandediting. S.V.Bratman Conceptualization, re- sources,  data curation,  investigation, writing-review and editing., A.J. Hope: Conceptualization, resources, data curation, investigation, writing-review andediting.B. Haibe-Kains Conceptualization,  resources,  supervision,  funding acquisition, projectadministration, writing-reviewandediting., Acknowledgments WewouldliketothankthePrincessMargaretHeadandNeckCancergroupfor the support with data collection and curation., This includes Li Tong, KatrinaRey-McIntyre, Shao Hui Huang and Drs. Andrea McNiven John Cho,  Scott Bratman, EzraHahn, AliHosni, JohnKim, BrianOSullivan, AndrewJ.Hope, JohnWaldronandJolieRingash.WewouldalsoliketoacknowledgeZhibinLuandtheHPC4Healthteamforthetechnicalsupport.M.K.wassupportedbythe Strategic Training in Transdisciplinary Radiation Science for the 21st Century Program (STARS21) scholarship., This study was supported by the Canadian InstitutesofHealthResearchProjectScheme(Grant#426366). Note Supplementary data for this article are available at Cancer Research Comm-unicationsOnline( https, //aacrjournals.org/cancerrescommun/ )., Received April 07 2022,  revised November 14,  2022,  accepted May 19,  2023,  publishedfirstJune29, 2023., References 1. KossN. Computer-aided prognosis: II. Developmentof aprognostic algorithm. Arch Intern Med 2022;127: 448-59. 2. Ardila D, Kiraly AP, Bharadwaj S, Choi B, Reicher JJ, Peng L, et al. End-to-end lungcancerscreeningwiththree-dimensionaldeeplearningonlow-dosechest computed tomography. Nat Med 2019;25: 954-61.3. ChangS-WAbdul-KareemS, MericanAF, ZainRB.Oralcancerprognosisbased on clinicopathologic and genomic markers using a hybrid of feature selection and machine learning methods., BMC Bioinformatics 2013;14: 170. 4. Mahadevaiah G Rv P,  Bermejo I,  Jaffray D,  Dekker A, Wee L. Artifi- cial intelligence-based clinical decision support in modern medical physics, AACRJournals.org Cancer Res Commun,  3(6) June 2023 1149 Kazmierski et al., selection, acceptance, commissioning, and quality assurance. Med Phys 2020;47: e228-35. 5. Shrager J, Tenenbaum JM. Rapid learning for precision oncology. Nat Rev Clin Oncol 2014;11: 109-18. 6. Pai SI, Westra WH. Molecular pathology of head and neck cancer: implications for diagnosis, prognosis, and treatment. Annu Rev Pathol Mech Dis 2009;4: 49-70. 7. LeemansCR,BraakhuisBJM,BrakenhoffRH.Themolecularbiologyofheadand neck cancer. Nat Rev Cancer 2011;11: 9-22. 8. MirghaniHAmenF, BlanchardP, MoreauF, GuigayJ, HartlDM, etal.Treatment de-escalationinHPV-positiveoropharyngealcarcinoma, ongoingtrials, critical issues and perspectives., Int J Cancer 2015;136: 1494-503. 9. OSullivan B, Huang SH, Siu LL, Waldron J, Zhao H, Perez-Ordonez B, et al. Deintensification candidate subgroups in human papillomavirusrelated oropharyngeal cancer according to minimal risk of distant metastasis., J Clin Oncol 2013;31: 543-50. 10. GilliesRJ,KinahanPE,HricakH.Radiomics:imagesaremorethanpictures,they are data. Radiology 2019;278: 563-77. 11. LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521: 436-44. 12. Wong AJ, Kanwar A, Mohamed AS, Fuller CD. Radiomics in head and neck cancer: from exploration to application. Transl Cancer Res 2016;5: 371-82. 13. GerRBZhouS, ElgohariB, ElhalawaniH, MackinDM, MeierJG, etal.Radiomics features of the primary tumor fail to improve prediction of overall survival in largecohortsofCT-andPET-imagedheadandneckcancerpatients.PLoSOne 2019, 14,  e0222509., 14. Lv W Ashrafinia S,  Ma J,  Lu L,  Rahmim A. Multi-level multi-modality fusion ra- diomics, application to PET and CT imaging for prognostication of head and neck cancer., IEEE J Biomed Health Inform 2020;24: 2268-77. 15. Vallieres M, Kay-Rivest E, Perrin LJ, Liem X, Furstoss C, Aerts HJWL, et al. Radiomics strategies for risk assessment of tumour failure in head-and-neck cancer. Sci Rep 2017;7: 10117. 16. Diamant A, Chatterjee A, Vallieres M, Shenouda G, Seuntjens J. Deep learning in head & neck cancer outcome prediction. Sci Rep 2019;9: 2764. 17. SheikhKLeeSH, ChengZ, LakshminarayananP, PengL, HanP, etal.Predicting acute radiation induced xerostomia in head and neck Cancer using MR and CT Radiomics of parotid and submandibular glands., Radiat Oncol 2019;14: 131. 18. van Dijk LV, Langendijk JA, Zhai T-T, Vedelaar TA, Noordzij W, Steenbakkers RJHM, et al. Delta-radiomics features during radiotherapy improve the prediction of late xerostomia. Sci Rep 2019;9: 12483. 19. Huang C, Cintra M, Brennan K, Zhou M, Colevas AD, Fischbein N, et al. Devel- opment and validation of radiomic signatures of head and neck squamous cellcarcinoma molecular features and subtypes., EBioMedicine 2019;45: 70-80. 20. ZhuYMohamedASR, LaiSY, YangS, KanwarA, WeiL, etal.Imaging-genomic study of head and neck squamous cell carcinoma, associations between ra- diomic phenotypes and genomic mechanisms via integration of the cancer genome atlas and the cancer imaging archive., JCO Clin Cancer Inform 2019;3: 1-9. 21. Grossmann P Stringfield O,  El-Hachem N,  Bui MM,  Velazquez ER,  Parmar C, etal.Definingthebiologicalbasisofradiomicphenotypesinlungcancer.Elife, 6,  e23421., 22. SanduleanuS,WoodruffHC,deJongEEC,vanTimmerenJE,JochemsA,Dubois L, et al. Tracking tumor biology with radiomics: a systematic review utilizing aradiomics quality score. Radiother Oncol 2018;127: 349-60. 23. Morin O, Vallieres M, Jochems A, Woodruff HC, Valdes G, Braunstein SE, et al. A deep look into the future of quantitative imaging in oncology a statement of working principles and proposal for change., Int J Radiat Oncol Biol Phys 2018;102: 1074-82. 24. WolpertDH,MacreadyWG.Nofreelunchtheoremsforoptimization.IEEETrans Evol Computat 1997;1: 67-82. 25. Hutson M. Artificial intelligence faces reproducibility crisis. Science 2018;359: 725-6. 26. Bluemke DA, Moy L, Bredella MA, Ertl-Wagner BB, Fowler KJ, Goh VJ, et al. Assessing radiology research on artificial intelligence a brief guide forauthors, reviewers, andreadersfromtheradiologyeditorialboard.Radiology 2020, 294,  487-9., 27. Zwanenburg A, Vallieres M, Abdalah MA, Aerts HJWL, Andrearczyk V, Apte A, et al. The image biomarker standardization initiative standardized quan- titative radiomics for high-throughput image-based phenotyping., Radiology 2020;295: 328-38. 28. Haibe-Kains B Adam GA,  Hosny A,  Khodakarami F, Massive Analysis Quality Control(MAQC)SocietyBoardofDirectors, WaldronL, etal.Transparencyand reproducibility in artificial intelligence., Nature 2020;586: E14-6. 29. Welch ML, McIntosh C, Haibe-Kains B, Milosevic MF, Wee L, Dekker A, et al. Vulnerabilities of radiomic signature development the need for safeguards.Radiother Oncol 2019, 130, 2-9., 30. T raversoAKazmierskiM, ZhovannikI, W elchM, W eeL, JaffrayD, etal.Machine learning helps identifying volume-confounding effects in radiomics., Physica Med 2020;71: 24-30. 31. van Griethuysen JJM, Fedorov A, Parmar C, Hosny A, Aucoin N, Narayan V, et al. Computational radiomics system to decode the radiographic phenotype. Cancer Res 2017;77: e104-7. 32. PengHLongF, DingC.Featureselectionbasedonmutualinformationcriteria of max-dependency, max-relevance,  and min-redundancy., IEEE Trans Pattern Anal Mach Intell 2005;27: 1226-38. 33. FawcettT.AnintroductiontoROCanalysis.PatternRecognitLett2006;27:861- 74. 34. LeverJ,KrzywinskiM,AltmanN.Classificationevaluation.NatMethods2016;13: 603-4. 35. AertsHJWLVelazquezER, LeijenaarRTH, ParmarC, GrossmannP, CarvalhoS, etal.Decodingtumourphenotypebynoninvasiveimagingusingaquantitativeradiomics approach., Nat Commun 2014;5: 4006. 36. Grossberg AJ, Mohamed ASR, Elhalawani H, Bennett WC, Smith KE, Nolan TS, et al. Imaging and clinical data archive for head and neck squamous cell carcinoma patients treated with radiotherapy., Sci Data 2018;5: 180173. 37. ClarkKVendtB, SmithK, FreymannJ, KirbyJ, KoppelP, etal.Thecancerimag- ing archive (TCIA), maintaining and operating a public information repository., J Digit Imaging 2013;26: 1045-57. 38. Saito T Rehmsmeier M. The precision-recall plot is more informative than the ROCplotwhenevaluatingbinaryclassifiersonimbalanceddatasets.PLoSOne 2015, 10,  e0118432., 39. Harrell FE, Lee KL, Mark DB. Multivariable prognostic models Issues in de- veloping models,  evaluating assumptions and adequacy, and measuring andreducing errors., Stat Med 1996;15: 361-87. 40. Huang G, Liu Z, van der Maaten L, Weinberger KQ. Densely connected convolutionalnetworks;2016.Availablefrom: http://arxiv.org/abs/1608.06993 . 41. De Fauw J, Ledsam JR, Romera-Paredes B, Nikolov S, Tomasev N, Blackwell S, et al. Clinically applicable deep learning for diagnosis and referral in retinal disease. Nat Med 2018;24: 1342-50. 42. Cheerla A, Gevaert O. Deep learning with multimodal representation for pancancer prognosis prediction. Bioinformatics 2019;35: i446-54. 43. LinCS,deOliveiraSantosAB,SilvaELE,deMatosLL,AjubMoysesR,Vamondes Kulcsar MA, et al. Tumor volume as an independent predictive factor of worse survival in patients with oral cavity squamous cell carcinoma, tumor volume as predictive of survival in patients with oral cavity SCC., Head Neck 2017;39:960-4. 44. Yu C-N Greiner R,  Lin H-C, Baracos V. Learning patient-specific can- cer survival distributions as a sequence of dependent regressors., In: Shawe-Taylor J, Zemel RS, et al. editors. Advances in neural information processing systems 24. Curran Associates, Inc.; 2011. p. 1845-53. Avail- ablefrom http, //papers.nips.cc/paper/4210-learning-patient-specific-cancer- survival-distributions-as-a-sequence-of-dependent-regressors.pdf ., 45. MukherjeePMukherjeeP, ZhouM, LeeE, SchichtA, BalagurunathanY, etal.A shallowconvolutionalneuralnetworkpredictsprognosisoflungcancerpatients in multi-institutional computed tomography image datasets., Nat Mach Intell 2020;2: 274-82. 46. Fotso S. Deep neural networks for survival analysis based on a multi-task framework; 2018. Available from: http://arxiv.org/abs/1801.05512 . 1150Cancer Res Commun 3(6) June 2023 https, / /doi.org/10.1158/2767-9764.CRC-22-0152 |CANCER RESEARCH COMMUNICATIONS Generalizability of Radiomics in Predictive Models of HNC 47., ClevertD-AUnterthinerT, HochreiterS.Fastandaccuratedeepnetworklearn- ing by exponential linear units (ELUs),  2016., Available from: http://arxiv.org/ abs/1511.07289 . 4 8 . R o g e r sJ R L i uC , H r i p c s a kG , C h e u n gY K , W e n gC .C o m p a r i s o no fc l i n i c a l characteristics between clinical trial participants and nonparticipants using electronic health record data., JAMA Netw Open 2021;4: e214732. 49. RothwellPM.Externalvalidityofrandomisedcontrolledtrials:Towhomdothe results of this trial apply? Lancet 2005;365: 82-93. 50. Futoma J, Simons M, Panch T, Doshi-Velez F, Celi LA. The myth of generalis- ability in clinical research and machine learning in health care. Lancet Digital Health 2020;2: e489-92. 51. Kolesnikov A, Beyer L, Zhai X, Puigcerver J, Yung J, Gelly S, et al. Big Transfer (BiT):generalvisualrepresentationlearning;2020.Availablefrom: http://arxiv. org/abs/1912.11370 . 52. Sun C Shrivastava A,  Singh S, Gupta A. Revisiting unreasonable effectiveness of data in deep learning era., In Proceedings of the 2017 IEEE International Conference on Computer Vision (ICCV). 2017 Sep 2229; Venice, Italy, 2017. p. 843-52. 53. Sudlow C, Gallacher J, Allen N, Beral V, Burton P, Danesh J, et al. UK Biobank an open accessresource for identifying the causes of a wide range of complex diseases of middle and old age., PLoS Med 2015;12: e1001779. 54. Cancer Genome Atlas Research Network Weinstein JN,  Collisson EA,  Mills GB,  ShawKRM, OzenbergerBA, etal.TheCancerGenomeAtlaspan-canceranalysis project., Nat Genet 2013;45: 1113-20. 55. Fave X, Cook M, Frederick A, Zhang L, Yang J, Fried D, et al. Preliminary inves- tigation into sources of uncertainty in quantitative imaging features. Comput Med Imaging Graph 2015;44: 54-61. 56. Hatt M Tixier F,  Pierce L,  Kinahan PE,  Le Rest CC, Visvikis D. Characterization of PET/CT images using texture analysis,  the past, the present ...any future?, Eur J Nucl Med Mol Imaging 2019;44: 151-65. 57. MateenBALileyJ, DennistonAK, HolmesCC, VollmerSJ.Improvingthequality ofmachinelearninginhealthapplicationsandclinicalresearch.NatMachIntell 2020, 2,  554-6.58., Kazmierska J, Hope A, Spezi E, Beddar S, Nailon WH, Osong B, et al. From multisource data to clinical decision aids in radiation oncology the need for a clinical data science community., Radiother Oncol 2020;153: 43-54. 59. Dietterich TG. Ensemble methods in machine learning. In: Multiple classifier systems. MCS 2000. Lecture Notes in Computer Science, vol 1857. Berlin, Heidelberg: Springer; 2000. p. 1-15. 60. Wolpert DH. Stacked generalization. Neural Netw 1992;5: 241-59. 61. Hawkins PG, Mierzwa ML, Bellile E, Jackson WC, Malloy KM, Chinn SB, et al. Impact of American Joint Committee on Cancer Eighth Edition clinical stage and smoking history on oncologic outcomes in human papillomavirus- associated oropharyngeal squamous cell carcinoma., Head Neck 2019;41: 857-64. 62. LassenPHuangSH, SuJ, OSullivanB, WaldronJ, AndersenM, etal.Impactof tobacco smoking on radiotherapy outcomes in 1875 HPV-positive oropharynx cancer patients., J Clin Orthod 2019;37: 6047-7. 63. Huang SH, Koyfman S, OSullivan B. Unfinished business in classifying HPV- Positiveoropharyngealcarcinoma identifyingthebadapplesinagoodstaging barrel., Oncologist 2022;27: 4-6. 64. Kingma DP, Ba J. Adam: a method for stochastic optimization; 2014. Available from:http://arxiv.org/abs/1412.6980 . 65. Haibe-Kains B Desmedt C,  Rothe F,  Piccart M,  Sotiriou C, Bontempi G. A fuzzy gene expression-based computational approach improves breast cancer prognostication., Genome Biol 2010;11: R18. 66. PedregosaFVaroquauxG, GramfortA, MichelV, ThirionB, GriselO, etal.Scikit- learn, machine learning in python., J Mach Learn Res 2011;12: 2825-30. 67. IoffeSSzegedyC.Batchnormalization, acceleratingdeepnetworktrainingby reducinginternalcovariateshift.ICML15, Proceedingsofthe32ndInternationalConference on Machine Learning,  2015,  448-56., 68. Klambauer G, Unterthiner T, Mayr A, Hochreiter S. Self-normalizing neural networks; 2017. Available from: http://arxiv.org/abs/1706.02515 . 69. Simonyan K, Zisserman A. Very deep convolutional networks for large-scale image recognition; 2015. Available from: http://arxiv.org/abs/1409.1556 . AACRJournals.org Cancer Res Commun; 3(6) June 2023 1151",
  "qa_pairs": [
    {
      "question": "What is the main objective of the research in this paper?",
      "answer": "The main objective of the research in this paper is to develop and evaluate prognostic models for predicting overall survival in patients with head and neck cancer (HNC) using machine learning (ML) techniques that integrate electronic medical records (EMR) and pretreatment radiological images. The study aims to assess the impact and generalizability of these models across different patient populations by utilizing a large retrospective dataset from a single institution and validating the models on external cohorts. Additionally, the research seeks to explore the contributions of radiomics in improving prognostic accuracy and to establish a robust framework for the external validation of ML models in clinical oncology."
    },
    {
      "question": "Can you summarize the abstract of the paper?",
      "answer": "The abstract of the paper discusses the development and evaluation of machine learning (ML) models for predicting overall survival in head and neck cancer (HNC) using a large dataset of 2,552 patients from a single institution, along with external validation on three additional cohorts totaling 873 patients. The study highlights the integration of electronic medical records (EMR) and pretreatment radiological images (radiomics) in creating prognostic models. The authors compared 12 different modeling approaches, finding that the most accurate model utilized multitask learning on clinical data and tumor volume, outperforming other models based solely on clinical data, engineered radiomics, or complex deep learning architectures. However, when these models were applied to external datasets, their performance significantly decreased, underscoring the need for robust validation frameworks and detailed population-based reporting to enhance the generalizability of AI/ML models in clinical settings. The findings emphasize that while ML combined with simple prognostic factors can outperform advanced radiomics and deep learning methods, the prognostic value of these models is influenced by variations in patient populations and necessitates extensive validation."
    },
    {
      "question": "What are the softwares and computational tools that were used in this paper?",
      "answer": "The research paper utilized several software and computational tools, including:\n\n1. **PyRadiomics**: This package was used for extracting radiomic features from imaging data. Specifically, version 2.2.0 of PyRadiomics was mentioned in the context of extracting first-order, shape, and textural features from CT images.\n\n2. **Logistic Regression and Cox Proportional Hazards Model**: These statistical models were employed for building the baseline models and evaluating the performance of the prognostic models.\n\n3. **High-Performance Computing Cluster**: The dataset was hosted on an institutional high-performance computing cluster equipped with multicore CPUs and general-purpose graphics processing units (GPUs), which were utilized for model training.\n\n4. **GitHub**: The code used for preparing the data, training the baseline models, evaluating the models, and analyzing the results was made publicly available on GitHub.\n\n5. **Statistical Software**: The paper mentions the use of statistical methods for comparing model performance, including area under the ROC curve (AUROC) and average precision (AP), which implies the use of statistical software for these calculations.\n\nThese tools and software facilitated the development, training, and evaluation of machine learning models for predicting overall survival in head and neck cancer patients."
    },
    {
      "question": "Describe the methodology used in the paper.",
      "answer": "The methodology used in the paper \"Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impact and Generalizability of Deep Learning and Radiomics\" involved several key components:\n\n1. **Dataset Collection**: A retrospective dataset was compiled from 2,552 patients with head and neck cancer (HNC) treated at the Princess Margaret Cancer Centre between 2005 and 2017. This dataset included electronic medical records (EMR) and pretreatment radiological images. The dataset was split into a training set (1,802 patients) and a test set (750 patients) based on the date of diagnosis.\n\n2. **Model Development**: The study employed a crowdsourcing approach where multiple independent investigators developed machine learning (ML) models to predict overall survival (OS) using the training dataset. A total of 12 different modeling approaches were compared, utilizing either EMR data, imaging data, or a combination of both.\n\n3. **Model Evaluation Framework**: The evaluation framework included internal validation using the training and test datasets, as well as external validation on three independent cohorts totaling 873 patients. The primary objective was to predict 2-year OS, with secondary goals of predicting lifetime risk of death and full survival curves.\n\n4. **Benchmark Models**: Three baseline models were created for comparison: one using standard clinical prognostic factors, one using primary tumor volume, and one using handcrafted radiomics features. These models served as benchmarks against which the performance of the developed models was measured.\n\n5. **Performance Metrics**: The performance of the models was assessed using several metrics, including the area under the receiver operating characteristic curve (AUROC), average precision (AP), and concordance index (C-index) for lifetime risk predictions. The models were ranked based on AUROC, which was the primary evaluation metric.\n\n6. **Statistical Analysis**: Statistical comparisons were made between the models using one-sided t-tests and adjustments for multiple comparisons. The study also included an analysis of the correlation between model predictions and tumor volume to assess the influence of volume on model performance.\n\n7. **External Validation**: The best-performing models were validated on three external datasets, with attention to differences in clinical and demographic variables between the training and external datasets. This was crucial for assessing the generalizability of the models.\n\n8. **Reproducibility and Data Sharing**: The code and data used in the study were made publicly available on GitHub and The Cancer Imaging Archive to promote transparency and reproducibility in research.\n\nOverall, the methodology combined a robust dataset, diverse modeling approaches, and a comprehensive validation framework to evaluate the prognostic capabilities of ML and radiomics in predicting outcomes for patients with head and neck cancer."
    },
    {
      "question": "What are the key findings of the paper?",
      "answer": "The key findings of the paper \"Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impact and Generalizability of Deep Learning and Radiomics\" are as follows:\n\n1. **Development of Prognostic Models**: The study successfully developed highly prognostic models for overall survival in head and neck cancer (HNC) using electronic medical records (EMR) and pretreatment radiological images from a large retrospective dataset of 2,552 patients.\n\n2. **Model Performance**: Among the twelve different machine learning (ML) models evaluated, the model with the highest accuracy utilized multitask learning on clinical data and tumor volume. This model demonstrated superior prognostic accuracy for both 2-year and lifetime survival predictions compared to models that relied solely on clinical data, engineered radiomics, or complex deep learning architectures.\n\n3. **External Validation and Generalizability**: The top-performing models were externally validated on three independent patient cohorts (873 patients). The results indicated significant decreases in model performance across these external datasets, underscoring the importance of population-based reporting and the need for robust validation frameworks in AI/ML model development.\n\n4. **Comparison of Data Sources**: The study found that ML models combining EMR features with simple imaging data outperformed more complex models based solely on advanced CT radiomics or deep learning methods. This suggests that traditional clinical data may provide more reliable prognostic information in certain contexts.\n\n5. **Impact of Patient Population Differences**: The observed performance drop in external validation highlighted the challenges of generalizability in predictive modeling, particularly when patient populations differ significantly in clinical and demographic characteristics.\n\n6. **Recommendations for Future Research**: The authors emphasized the need for extensive validation of prognostic models across diverse patient populations and encouraged the sharing of datasets to enhance reproducibility and transparency in radiomics research.\n\nOverall, the findings suggest that while AI and ML hold promise for improving prognostic modeling in HNC, careful consideration of model design, validation, and the characteristics of patient populations is crucial for effective clinical application."
    },
    {
      "question": "How was the data analyzed in the study?",
      "answer": "In the study, the data analysis was conducted through a multi-institutional prognostic modeling challenge that involved several key steps:\n\n1. **Data Collection**: A retrospective dataset of 2,552 patients with head and neck cancer (HNC) was collected from a single institution, along with three external patient cohorts consisting of 873 patients. The dataset included electronic medical records (EMR) and pretreatment radiological images.\n\n2. **Model Development**: Independent investigators developed multiple machine learning (ML) models to predict overall survival in HNC. A total of 12 different models were compared, utilizing various combinations of imaging and EMR data.\n\n3. **Performance Metrics**: The primary evaluation metric for model performance was the area under the receiver operating characteristic curve (AUROC), which assesses the ability of the model to distinguish between patients who survived and those who did not. Additionally, average precision (AP) and the concordance index (C-index) were used to evaluate the models' performance in predicting lifetime risk and survival curves.\n\n4. **Statistical Comparison**: The performance of the models was statistically compared using one-sided t-tests, and corrections for multiple comparisons were made to control the false discovery rate (FDR) at a 5% level.\n\n5. **External Validation**: The top-performing models were validated on three external datasets to assess their generalizability. The performance of these models was compared against baseline models that used standard clinical factors, tumor volume, and engineered radiomics features.\n\n6. **Reproducibility and Transparency**: The study emphasized the importance of reproducibility and transparency in research. The code used for data preparation, model training, and evaluation was made publicly available on GitHub, and the complete dataset was planned to be shared on The Cancer Imaging Archive.\n\nOverall, the analysis involved a comprehensive approach that included model development, performance evaluation, statistical comparison, and external validation to ensure the robustness and applicability of the prognostic models developed for HNC."
    },
    {
      "question": "Was the data in the study pre-processed in anyway? If so how?",
      "answer": "Yes, the data in the study was pre-processed in several ways:\n\n1. **Data Collection and Structuring**: The researchers collected a retrospective dataset of 2,552 patients with head and neck cancer (HNC) treated at the Princess Margaret Cancer Centre. This dataset included electronic medical records (EMR) and pretreatment radiological images. The data was split into training and test subsets based on the date of diagnosis.\n\n2. **Inclusion Criteria**: Specific inclusion criteria were applied to ensure the quality of the dataset. These included the availability of planning CT images and target contours, a minimum follow-up period of two years (or death before that time), and no distant metastases at diagnosis or prior surgery.\n\n3. **Image and Mask Export**: For each patient, the CT images and primary gross tumor volume (GTV) binary masks were exported in NRRD format, which is a standard format for storing medical imaging data.\n\n4. **Handling of Missing Data**: All categorical variables were one-hot encoded, and missing data were addressed by creating additional categories to represent missing values (e.g., \"Not tested\" for HPV status).\n\n5. **Feature Extraction**: For the baseline-radiomics model, the researchers extracted a comprehensive set of features from the imaging data using the PyRadiomics package. This included first-order, shape, and textural features, totaling 1,316 features. Feature selection was performed using a maximum relevance-minimum redundancy method.\n\n6. **Model Training and Evaluation**: The models were built using logistic regression for binary endpoints and proportional hazards models for survival endpoints. Hyperparameters were tuned using grid search with 5-fold cross-validation.\n\n7. **External Validation**: The study also included external validation of the models on three independent datasets, ensuring that the preprocessing workflow was consistent across datasets to assess generalizability.\n\nThese preprocessing steps were crucial for ensuring the integrity and usability of the data for developing and validating prognostic models in head and neck cancer."
    },
    {
      "question": "What conclusions were drawn in the paper?",
      "answer": "The conclusions drawn in the paper \"Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impact and Generalizability of Deep Learning and Radiomics\" are as follows:\n\n1. **Development of Prognostic Models**: The study successfully developed highly prognostic models for overall survival in head and neck cancer (HNC) using electronic medical records (EMRs) and pretreatment radiological images from a large retrospective dataset of 2,552 patients.\n\n2. **Model Performance**: Among the various machine learning (ML) approaches tested, the model that achieved the highest accuracy utilized multitask learning on clinical data and tumor volume. This model outperformed others that relied solely on clinical data, engineered radiomics, or complex deep learning architectures.\n\n3. **External Validation Challenges**: When the best-performing models were applied to external datasets (873 patients), there was a significant reduction in performance. This highlighted the importance of detailed population-based reporting for AI/ML model utility and the need for stronger validation frameworks.\n\n4. **Generalizability Issues**: The study emphasized that while ML models can provide diverse solutions for prognosis in HNC, their prognostic value is affected by differences in patient populations. Extensive validation across different datasets is essential to ensure the robustness and applicability of these models.\n\n5. **Comparison of Data Sources**: The findings indicated that ML models combining simple prognostic factors (like EMR data and tumor volume) outperformed more advanced CT radiomics and deep learning methods. This suggests that the integration of clinical data may be more beneficial than relying solely on complex imaging features.\n\n6. **Need for Further Research**: The authors call for further research to enhance the generalizability of prognostic models and to explore more sophisticated modeling strategies that can better integrate multiple data sources.\n\nIn summary, the paper concludes that while significant advancements have been made in developing prognostic models for HNC using AI and ML, challenges remain in terms of generalizability and the need for extensive validation across diverse patient populations."
    },
    {
      "question": "Can you provide a summary of the literature review from the paper?",
      "answer": "The literature review in the paper discusses the increasing role of artificial intelligence (AI) and machine learning (ML) in clinical oncology, particularly in predicting patient prognosis. It highlights the complexity of developing prognostic models, noting that no single modeling strategy is universally superior. The review emphasizes the necessity for large and diverse datasets to validate models, ensuring their applicability across different populations.\n\nKey points from the literature review include:\n\n1. **Integration of Data**: The review underscores the potential of combining various data sources, including electronic medical records (EMR) and imaging data (radiomics), to enhance prognostic accuracy in head and neck cancer (HNC).\n\n2. **Radiomics**: It introduces radiomics as a promising field aimed at developing non-invasive quantitative imaging biomarkers. The review cites numerous studies where radiomics has been used to predict outcomes, treatment responses, and associations with genomic markers in HNC.\n\n3. **Challenges in Adoption**: Despite the promising results from retrospective studies, the review identifies barriers to the clinical adoption of radiomics, such as the lack of a clear superior predictive modeling strategy, small sample sizes, and insufficient validation and reproducibility in ML research.\n\n4. **Need for Standardization**: The review calls for standardization in radiomics methodologies and the importance of providing sufficient details and materials for reproducibility.\n\n5. **Collaborative Approaches**: The authors advocate for collaborative challenges to enhance reproducibility, transparency, and generalizability in radiomic modeling for HNC prognosis.\n\n6. **External Validation**: The review stresses the importance of external validation to assess how well models developed from one dataset can perform on others, highlighting the need for diverse patient cohorts in model training.\n\nOverall, the literature review sets the stage for the research presented in the paper, emphasizing the importance of robust, validated models that can be generalized across different patient populations in the context of HNC prognosis."
    },
    {
      "question": "What future research directions do the authors suggest in the paper?",
      "answer": "The authors suggest several future research directions in the paper:\n\n1. **Enhancing Model Diversity**: They express a desire to expand their crowdsourcing efforts beyond their institution to enhance the diversity of modeling approaches and validate their conclusions.\n\n2. **Collecting Additional Outcome Information**: The authors plan to gather more comprehensive outcome data, including recurrence, distant metastasis, and treatment toxicity, which would provide a richer set of prediction targets and be more clinically relevant.\n\n3. **Improving Generalizability**: They highlight the need for future studies to address the generalizability of AI/ML models, particularly in the context of clinical trials, to ensure that models developed from clinical trial populations can be effectively applied in routine clinical practice.\n\n4. **Exploring Advanced Modeling Techniques**: The authors suggest that more sophisticated ensembling methods, such as Bayesian model averaging or stacking, could be explored to potentially achieve better performance by leveraging the strengths of various models.\n\n5. **Investigating Joint Latent Spaces**: They recommend exploring more sophisticated methods of incorporating additional patient-level information, such as joint latent spaces, to improve model performance.\n\n6. **Collaboration Across Disciplines**: The authors emphasize the importance of collaboration between machine learning researchers, clinicians, and physicists in data selection and experiment design to enhance the quality and applicability of their models.\n\nThese directions aim to improve the robustness, applicability, and clinical utility of prognostic models in head and neck cancer using AI and machine learning techniques."
    },
    {
      "question": "What statistical techniques were used in the paper?",
      "answer": "The research paper utilized several statistical techniques for modeling and evaluating prognostic outcomes in head and neck cancer. The key statistical techniques mentioned in the paper include:\n\n1. **Logistic Regression**: This was used for binary endpoint predictions, specifically for predicting 2-year overall survival (OS).\n\n2. **Cox Proportional Hazards Model**: This model was employed for risk prediction regarding lifetime risk of death.\n\n3. **Multitask Learning**: The top-performing model used multitask logistic regression, which allows for the simultaneous prediction of multiple outcomes, leveraging time-to-event data.\n\n4. **Area Under the Receiver Operating Characteristic Curve (AUROC)**: This metric was used to evaluate the performance of the models in terms of their ability to discriminate between patients who survived and those who did not.\n\n5. **Average Precision (AP)**: This was used as a secondary performance measure to assess model performance, particularly in cases of class imbalance.\n\n6. **Concordance Index (C-index)**: This metric was used to evaluate the performance of models in predicting lifetime risk and survival curves.\n\n7. **Statistical Comparisons**: One-sided t-tests were used to compare the AUROC of the best model against other models, with adjustments for multiple comparisons using the false discovery rate (FDR) method.\n\n8. **Spearman Rank Correlation**: This was used to assess the correlation between model predictions and tumor volume, indicating how volume dependence affected model performance.\n\nThese techniques collectively contributed to the development and validation of robust prognostic models for head and neck cancer, emphasizing the importance of statistical rigor in evaluating machine learning approaches in clinical settings."
    },
    {
      "question": "Can you describe the experimental setup in the paper?",
      "answer": "The experimental setup described in the paper involved a multi-institutional prognostic modeling challenge focused on predicting overall survival in patients with head and neck cancer (HNC) using machine learning (ML) and radiomics. Here are the key components of the experimental setup:\n\n1. **Dataset**: The study utilized a large retrospective dataset comprising 2,552 patients treated at the Princess Margaret Cancer Centre between 2005 and 2017. This dataset was split into a training set (1,802 patients) and a test set (750 patients) based on the date of diagnosis. The inclusion criteria for patients included the availability of planning CT images, at least two years of follow-up, and no distant metastases at diagnosis.\n\n2. **Data Types**: The data used for modeling included electronic medical records (EMR) and pretreatment radiological images. The EMR contained clinical, demographic, and treatment-related information, while the imaging data consisted of contrast-enhanced CT images and binary masks of the primary gross tumor volumes (GTV).\n\n3. **Model Development**: The challenge was organized to allow independent investigators to develop prognostic models. Participants had access to the training data with ground-truth outcome labels, while the test set was kept private for final evaluation. The primary objective was to predict 2-year overall survival (OS), with secondary goals of predicting lifetime risk of death and full survival curves.\n\n4. **Modeling Approaches**: A total of 12 different modeling approaches were developed by participants, which included various machine learning techniques, such as logistic regression, fuzzy logistic regression, Cox proportional hazards models, and deep learning models (e.g., convolutional neural networks). The models were evaluated based on their ability to predict binary outcomes (2-year OS) and were ranked using metrics like area under the ROC curve (AUROC) and average precision (AP).\n\n5. **External Validation**: The top-performing models were externally validated on three independent cohorts comprising 873 patients from different institutions. This validation aimed to assess the generalizability of the models across diverse patient populations.\n\n6. **Evaluation Metrics**: The performance of the models was primarily evaluated using AUROC for the binary endpoint, with additional metrics such as AP and concordance index (C-index) used for assessing lifetime risk predictions.\n\n7. **Collaboration and Transparency**: The challenge emphasized reproducibility and transparency, with all code and data made available on GitHub and plans to share the complete dataset on The Cancer Imaging Archive.\n\nOverall, the experimental setup was designed to rigorously evaluate the performance of various ML models in predicting survival outcomes in HNC, while also addressing the challenges of generalizability and reproducibility in predictive modeling."
    },
    {
      "question": "What are the implications of the research findings?",
      "answer": "The research findings from the paper \"Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impact and Generalizability of Deep Learning and Radiomics\" have several significant implications:\n\n1. **Advancement in Prognostic Models**: The study demonstrates that machine learning (ML) models, particularly those that integrate electronic medical records (EMRs) and imaging data, can achieve high prognostic accuracy for overall survival in head and neck cancer (HNC). This suggests that ML can enhance personalized medicine by providing tailored predictions based on individual patient data.\n\n2. **Importance of Model Generalizability**: The findings highlight a critical limitation in the generalizability of ML models across different patient populations. The observed performance drop when applying the best-performing models to external datasets underscores the necessity for robust validation frameworks and detailed population-based reporting. This indicates that models developed in one clinical setting may not perform as well in another, emphasizing the need for diverse training datasets.\n\n3. **Comparison of Modeling Approaches**: The research reveals that simpler models using EMR features and tumor volume can outperform more complex deep learning models and engineered radiomics. This challenges the assumption that more complex algorithms are always superior and suggests that effective prognostic models can be developed using straightforward, interpretable features.\n\n4. **Need for Extensive Validation**: The study stresses the importance of extensive validation of prognostic models across various datasets to ensure their reliability and applicability in clinical practice. This is crucial for the adoption of AI/ML tools in oncology, as models must be proven effective in diverse clinical scenarios.\n\n5. **Clinical Decision-Making**: The ability to predict patient outcomes accurately can significantly influence clinical decision-making, guiding treatment strategies and patient management. The integration of AI/ML into clinical workflows could lead to more informed decisions regarding treatment intensity and patient monitoring.\n\n6. **Encouragement for Collaborative Research**: The collaborative approach taken in this study, which involved multiple institutions and researchers, demonstrates the potential benefits of shared knowledge and resources in advancing cancer research. This model of collaboration could be a pathway to overcoming challenges related to data scarcity and model validation.\n\n7. **Future Research Directions**: The findings suggest that future research should focus on improving the generalizability of models by incorporating diverse datasets and exploring more sophisticated modeling techniques that can better integrate various data types. Additionally, there is a call for more standardized imaging practices to enhance the reliability of radiomics in clinical settings.\n\nIn summary, the implications of this research extend beyond the immediate findings, influencing future directions in cancer prognosis, the integration of AI in clinical practice, and the collaborative nature of medical research."
    },
    {
      "question": "What are the limitations and delimitations mentioned in the paper?",
      "answer": "The research paper outlines several limitations and delimitations related to the study of prognostic modeling in head and neck cancer using machine learning and radiomics. Here are the key points:\n\n### Limitations:\n1. **Single Institution Participation**: The model development was restricted to researchers at one institution, which limited the diversity of models that could be explored.\n2. **Reliance on One Radiomics Toolkit**: The hand-engineered radiomics submissions were based on a single toolkit (PyRadiomics), which may not capture the full range of features available in other widely-used toolkits.\n3. **Missing Variables**: Smoking status, a relevant variable for head and neck cancer prognosis, was not guaranteed to be included in all predictive models and did not significantly improve predictive performance.\n4. **Potential for More Sophisticated Methods**: The study suggests that more advanced ensembling methods (e.g., Bayesian model averaging or stacking) could potentially achieve better performance, indicating that the current methods may not be fully optimized.\n5. **Generalizability Issues**: The findings highlight the intrinsic risks related to generalizability, which may impact the performance of models outside the data space in which they were generated.\n\n### Delimitations:\n1. **Focus on Specific Data Types**: The study focused on electronic medical records (EMR) and pretreatment radiological images, which may limit the applicability of the findings to other types of data or cancer types.\n2. **Challenge Structure**: The study was structured as a challenge, meaning that the selection of modeling methods and input variables was left to the discretion of the participants, which may have influenced the outcomes.\n3. **Limited Outcome Information**: The study primarily focused on overall survival and did not include additional outcome information such as recurrence, distant metastasis, or treatment toxicity, which could provide a richer set of prediction targets.\n\nThese limitations and delimitations are crucial for understanding the context and applicability of the research findings in clinical practice and future studies."
    },
    {
      "question": "What recommendations do the authors make in the paper?",
      "answer": "The authors of the paper make several recommendations based on their findings:\n\n1. **Importance of External Validation**: The authors emphasize the necessity of extensive external validation for prognostic models developed using machine learning (ML) and artificial intelligence (AI). They highlight that models trained on one dataset may not perform well on different populations due to variations in clinical and demographic characteristics.\n\n2. **Detailed Population-Based Reporting**: They recommend that detailed reporting of population characteristics should be included in studies to better assess the utility of AI/ML models across different patient populations.\n\n3. **Model Development Strategies**: The authors suggest that future research should explore more sophisticated methods for integrating different data modalities (e.g., electronic medical records and imaging data) to enhance model performance. They note that current approaches often do not optimally exploit the complementary information available from various sources.\n\n4. **Data Sharing and Collaboration**: The authors advocate for greater collaboration and data sharing within the research community to improve the reproducibility and generalizability of ML models. They plan to make their dataset publicly available to facilitate further research.\n\n5. **Focus on Simplicity and Interpretability**: They recommend that models should prioritize simplicity and interpretability, particularly when using features that are widely recognized and easy to understand in clinical practice.\n\n6. **Future Research Directions**: The authors suggest that future studies should aim to collect additional outcome information, such as recurrence and treatment toxicity, to provide a richer set of prediction targets that are more relevant for clinical decision-making.\n\n7. **Caution in Clinical Application**: They advise caution in the application of AI/ML models in clinical settings, particularly regarding their generalizability and the potential risks associated with using models developed from highly selected clinical trial populations.\n\nThese recommendations aim to enhance the robustness, applicability, and clinical utility of prognostic models in head and neck cancer and beyond."
    },
    {
      "question": "Who funded the research in the paper?",
      "answer": "The research in the paper was funded by the Canadian Institutes of Health Research Project Scheme (Grant #426366)."
    },
    {
      "question": "Is there any conflict of interest disclosed in the paper?",
      "answer": "Yes, the paper discloses several conflicts of interest. Specifically, the following authors reported conflicts:\n\n1. **C. McIntosh**: Reports personal fees from RaySearch and other from Curate outside the submitted work, and has a patent (EP3007771A1) licensed.\n2. **H.J.W.L. Aerts**: Reports grants from NIH and EU, and personal fees from Onc.AI outside the submitted work.\n3. **S.V. Bratman**: Reports personal fees and other from Adela outside the submitted work, and has multiple patents issued and pending.\n4. **A.J. Hope**: Reports grants from Canadian Institutes of Health Research during the conduct of the study.\n5. **B. Haibe-Kains**: Reports personal fees from CodeOcean Inc, BreakThrough Cancer, CQDM, and grants from Roche outside the submitted work.\n\nNo disclosures were reported by the other authors."
    },
    {
      "question": "What ethical considerations are discussed in the paper?",
      "answer": "The paper discusses several ethical considerations related to the use of artificial intelligence (AI) and machine learning (ML) in developing prognostic models for head and neck cancer (HNC). Here are the key ethical points highlighted:\n\n1. **Generalizability and External Validation**: The paper emphasizes the importance of ensuring that AI/ML models are generalizable across different patient populations. The authors note that models developed from one dataset may not perform well when applied to other datasets, which raises ethical concerns about the reliability of these models in clinical practice. This highlights the need for extensive validation to avoid misleading predictions that could affect patient care.\n\n2. **Transparency and Reproducibility**: The authors stress the necessity for transparency in the development of AI/ML models. They point out that many studies do not provide sufficient details or underlying materials (such as code and data) to allow for reproducibility. This lack of transparency can hinder trust in the models and their clinical applicability, which is an ethical concern regarding the integrity of research.\n\n3. **Data Privacy and Security**: While not explicitly detailed in the provided content, the use of electronic medical records (EMR) and imaging data for model training raises ethical issues related to patient privacy and data security. Researchers must ensure that patient data is handled in compliance with ethical standards and regulations to protect patient confidentiality.\n\n4. **Bias and Equity**: The paper notes that differences in clinical and demographic variables between datasets can lead to biased predictions. This raises ethical concerns about equity in healthcare, as models that are not adequately validated across diverse populations may exacerbate existing disparities in treatment outcomes.\n\n5. **Clinical Decision-Making**: The authors discuss the potential impact of AI/ML on clinical decision-making, emphasizing the ethical responsibility to ensure that these tools support rather than replace human judgment. The integration of AI/ML into clinical workflows must be done thoughtfully to avoid over-reliance on technology that may not be fully understood or validated.\n\n6. **Informed Consent**: Although not directly mentioned, the use of patient data for research purposes typically requires informed consent. Ethical considerations around ensuring that patients are adequately informed about how their data will be used are crucial.\n\nOverall, the paper highlights the need for ethical vigilance in the development and application of AI/ML models in healthcare, particularly in ensuring that these technologies enhance patient care without compromising ethical standards."
    },
    {
      "question": "Which studies are most frequently cited in the paper?",
      "answer": "Based on the provided content from the research paper, the most frequently cited studies include:\n\n1. **Ger et al. (2019)** - This study discusses the failure of radiomics features of the primary tumor to improve the prediction of overall survival in large cohorts of CT- and PET-imaged head and neck cancer patients.\n2. **Vallieres et al. (2017)** - This research focuses on radiomics strategies for risk assessment of tumor failure in head-and-neck cancer.\n3. **Diamant et al. (2019)** - This study explores deep learning in head and neck cancer outcome prediction.\n4. **Huang et al. (2019)** - This research develops and validates radiomic signatures of head and neck squamous cell carcinoma molecular features and subtypes.\n5. **Zhu et al. (2019)** - This study investigates associations between radiomic phenotypes and genomic mechanisms via integration of the cancer genome atlas and the cancer imaging archive.\n\nThese studies are referenced multiple times throughout the paper, indicating their significance in the context of the research being conducted on prognostic modeling in head and neck cancer."
    },
    {
      "question": "Can you explain the technical terms used in the paper?",
      "answer": "Certainly! Here are explanations of some of the technical terms used in the paper:\n\n1. **Artificial Intelligence (AI)**: A branch of computer science that aims to create systems capable of performing tasks that typically require human intelligence, such as understanding natural language, recognizing patterns, and making decisions.\n\n2. **Machine Learning (ML)**: A subset of AI that involves the use of algorithms and statistical models to enable computers to improve their performance on a task through experience (i.e., data).\n\n3. **Radiomics**: An emerging field that involves extracting a large number of quantitative features from medical images using data-characterization algorithms. These features can be used to develop predictive models for patient outcomes.\n\n4. **Prognostic Models**: Statistical models used to predict the outcome of a disease, such as survival rates or disease progression, based on various patient and disease characteristics.\n\n5. **Electronic Medical Records (EMR)**: Digital versions of patients' paper charts that contain the medical and treatment history of patients in one practice. EMRs are used to streamline the clinician's workflow and improve patient care.\n\n6. **Multitask Learning**: A type of machine learning where multiple tasks are learned simultaneously, allowing the model to leverage shared information across tasks to improve performance.\n\n7. **Deep Learning**: A subset of ML that uses neural networks with many layers (deep networks) to analyze various factors of data. It is particularly effective in processing large amounts of unstructured data, such as images.\n\n8. **Concordance Index (C-index)**: A measure of the predictive accuracy of a risk model, particularly in survival analysis. It quantifies how well the model predicts the order of events (e.g., time to death).\n\n9. **Area Under the Receiver Operating Characteristic Curve (AUROC)**: A performance measurement for classification problems at various threshold settings. It represents the degree of separability between classes; a higher AUROC indicates better model performance.\n\n10. **Average Precision (AP)**: A metric used to evaluate the precision of a model across different thresholds, particularly useful in imbalanced datasets.\n\n11. **Cox Proportional Hazards Model**: A statistical technique used in survival analysis to explore the relationship between the survival time of patients and one or more predictor variables.\n\n12. **Feature Selection**: The process of selecting a subset of relevant features for model construction. It helps improve model performance and reduce overfitting.\n\n13. **Generalizability**: The ability of a model to perform well on unseen data from different populations or settings than those used for training.\n\n14. **Benchmark Models**: Standard models used as a point of reference to evaluate the performance of new models. They often consist of simpler or well-established methods.\n\n15. **Data Curation**: The process of organizing and maintaining data to ensure its quality and usability for research purposes.\n\nThese terms are integral to understanding the methodologies and findings presented in the research paper regarding head and neck cancer prognosis using AI and ML techniques."
    },
    {
      "question": "What data sources were used in the paper, and are they accessible for further research?",
      "answer": "The research paper utilized a combination of data sources for its analysis, specifically:\n\n1. **Electronic Medical Records (EMR)**: The study included clinical, demographic, and interventional data extracted from the EMRs of 2,552 patients with head and neck cancer (HNC) treated at the Princess Margaret Cancer Centre.\n\n2. **Pretreatment Radiological Images**: The study also incorporated imaging data, specifically contrast-enhanced CT images, which were used to derive features relevant to the prognosis of HNC.\n\n3. **External Validation Datasets**: The models were validated using three external patient cohorts totaling 873 patients, which included:\n   - The HN1 dataset (137 patients) from the MAASTRO Clinic in Maastricht, Netherlands, publicly available through The Cancer Imaging Archive.\n   - The MDACC dataset (627 patients) from MD Anderson Cancer Center, also accessible via The Cancer Imaging Archive.\n   - The GPCCHN dataset (298 patients) from Greater Poland Cancer Centre, which is a private dataset.\n\nRegarding accessibility for further research, the paper states that the complete dataset, including anonymized images, contours, and EMR data, will be made available on The Cancer Imaging Archive. This indicates that the data sources used in the study will be accessible for further research, promoting transparency and reproducibility in the field. Additionally, the code used for model training and evaluation is available on GitHub, further facilitating access for researchers interested in building upon this work."
    },
    {
      "question": "Can you summarize the research paper?",
      "answer": "The research paper titled \"Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impact and Generalizability of Deep Learning and Radiomics\" investigates the use of artificial intelligence (AI) and machine learning (ML) to develop prognostic models for overall survival in patients with head and neck cancer (HNC). The study utilizes a large retrospective dataset of 2,552 patients from a single institution, along with external validation from three additional cohorts comprising 873 patients.\n\nKey findings include:\n\n1. **Model Development**: The researchers crowdsourced the development of ML models that predict overall survival using electronic medical records (EMR) and pretreatment radiological images. They compared 12 different modeling approaches, focusing on the contributions of radiomics versus clinical data.\n\n2. **Performance of Models**: The best-performing model employed multitask learning on clinical data and tumor volume, achieving high prognostic accuracy for both 2-year and lifetime survival predictions. This model outperformed others that relied solely on clinical data, engineered radiomics, or complex deep learning architectures.\n\n3. **Generalizability Issues**: When the top models were applied to external datasets, there was a notable decline in performance, underscoring the importance of population-based reporting and robust validation frameworks for AI/ML models. The study emphasizes that while ML can enhance prognostic capabilities, the variability in patient populations significantly affects model performance.\n\n4. **Significance of Findings**: The results indicate that ML models that integrate simple prognostic factors can outperform more complex radiomics and deep learning methods. The study highlights the need for extensive validation of prognostic models across diverse patient populations to ensure their clinical utility.\n\n5. **Future Directions**: The authors advocate for collaborative research efforts to improve the reproducibility and generalizability of radiomics models in clinical settings. They also stress the importance of sharing datasets and methodologies to foster transparency and advancement in the field.\n\nIn conclusion, the paper presents a comprehensive evaluation of ML approaches in predicting survival outcomes for HNC patients, revealing both the potential and limitations of these technologies in clinical practice."
    },
    {
      "question": "What is the significance of this research in its field?",
      "answer": "The significance of this research lies in its contribution to the field of personalized medicine and prognostic modeling in head and neck cancer (HNC). The study demonstrates the potential of artificial intelligence (AI) and machine learning (ML) to enhance the accuracy of survival predictions by integrating diverse data sources, including electronic medical records (EMR) and radiological imaging. \n\nKey points of significance include:\n\n1. **Development of Robust Prognostic Models**: The research successfully developed highly prognostic models for overall survival in HNC, utilizing a large dataset of 2,552 patients. This is significant as it addresses the complexity of prognostic modeling in a heterogeneous disease like HNC.\n\n2. **Comparison of Modeling Approaches**: By comparing 12 different ML models, the study highlights that simpler models using EMR features and tumor volume can outperform more complex deep learning and engineered radiomics models. This finding challenges the assumption that more complex algorithms are always superior, emphasizing the importance of model simplicity and interpretability in clinical settings.\n\n3. **Generalizability and External Validation**: The research underscores the importance of external validation by testing the models on three independent cohorts. The observed performance drop in external datasets highlights the need for careful consideration of population differences when applying AI/ML models in clinical practice. This aspect is crucial for ensuring that models developed in one setting can be effectively utilized in others.\n\n4. **Implications for Clinical Decision-Making**: The findings suggest that AI and ML can significantly aid in clinical decision-making by providing more accurate prognostic information, which is essential for personalized treatment strategies. This aligns with the growing trend towards precision medicine in oncology.\n\n5. **Open Science and Reproducibility**: The study promotes transparency and reproducibility by making the data and code publicly available. This approach encourages collaboration and scrutiny within the research community, which is vital for advancing the field of AI in medicine.\n\nOverall, this research contributes to the understanding of how AI and ML can be effectively integrated into clinical oncology, particularly in the context of HNC, and sets a foundation for future studies aimed at improving patient outcomes through personalized treatment approaches."
    },
    {
      "question": "How does this paper contribute to the existing body of knowledge?",
      "answer": "The paper titled \"Multi-institutional Prognostic Modeling in Head and Neck Cancer, Evaluating Impact and Generalizability of Deep Learning and Radiomics\" contributes to the existing body of knowledge in several significant ways:\n\n1. **Integration of AI and ML in Clinical Oncology**: The research highlights the increasing role of artificial intelligence (AI) and machine learning (ML) in developing personalized medicine approaches, particularly in the context of head and neck cancer (HNC). By demonstrating the application of these technologies to predict overall survival using electronic medical records (EMR) and radiological images, the study underscores the potential of AI/ML to enhance clinical decision-making.\n\n2. **Large and Diverse Datasets**: The study utilizes a large retrospective dataset of 2,552 patients from a single institution, along with external validation on three additional cohorts comprising 873 patients. This approach not only strengthens the findings through extensive validation but also addresses the common limitation of small, single-institution datasets in previous studies.\n\n3. **Comparative Analysis of Prognostic Models**: The paper presents a comparative analysis of 12 different prognostic models, assessing their performance based on various data inputs (EMR, imaging, and combined data). This comprehensive evaluation provides insights into the relative contributions of different data modalities, revealing that simpler models using EMR and tumor volume can outperform more complex deep learning models.\n\n4. **Generalizability of Models**: A key contribution of the research is its focus on the generalizability of prognostic models across different patient populations. The observed performance drop when applying the best models to external datasets highlights the importance of understanding population-based differences and the need for robust validation frameworks in AI/ML research.\n\n5. **Methodological Rigor and Transparency**: The study emphasizes the importance of reproducibility and transparency in AI/ML research. By making the challenge framework, datasets, and code publicly available, the authors promote open science and encourage further research in the field, which is crucial for advancing the reliability of predictive models in clinical settings.\n\n6. **Identification of Effective Features**: The findings indicate that while radiomics can provide valuable prognostic information, the best-performing models relied primarily on EMR features and tumor volume. This challenges the assumption that more complex imaging features necessarily lead to better predictive performance, suggesting that simpler, well-understood clinical variables may be more effective in certain contexts.\n\n7. **Implications for Future Research**: The paper sets the stage for future investigations into the integration of diverse data sources and modeling strategies, advocating for collaborative efforts to enhance the robustness of prognostic models in oncology. It also calls for further exploration of advanced modeling techniques that can better leverage the strengths of different data modalities.\n\nIn summary, this research article significantly advances the understanding of how AI and ML can be effectively utilized in predicting outcomes for head and neck cancer patients, while also addressing critical issues related to model validation, generalizability, and the integration of clinical data."
    },
    {
      "question": "Are there any novel techniques or approaches introduced in this paper?",
      "answer": "Yes, the paper introduces several novel techniques and approaches in the context of prognostic modeling for head and neck cancer (HNC). Here are the key innovations:\n\n1. **Multitask Learning Model**: The paper highlights the use of a multitask logistic regression (MTLR) model, which is designed to predict not only binary outcomes (such as 2-year overall survival) but also to estimate a patient's lifetime risk of death and generate full survival curves. This approach allows the model to exploit time-to-event information and learn flexible relationships between covariates and event probabilities.\n\n2. **Crowdsourced Model Development**: The study implemented a collaborative challenge that involved multiple independent investigators developing machine learning (ML) models using a large dataset. This crowdsourcing approach aimed to enhance the diversity and robustness of the models developed.\n\n3. **Integration of Electronic Medical Records (EMR) and Imaging Data**: The research combined clinical data from EMRs with radiological imaging data, specifically using tumor volume as the only image-derived feature. This integration demonstrated that simple prognostic factors from EMRs could outperform more complex models relying solely on engineered radiomics or deep learning methods.\n\n4. **External Validation Framework**: The study emphasized the importance of external validation by testing the developed models on three independent patient cohorts. This approach aimed to assess the generalizability of the models across different populations, highlighting the need for robust validation frameworks in AI/ML model development.\n\n5. **Benchmarking Against Baseline Models**: The authors created baseline models using standard clinical prognostic factors, tumor volume, and handcrafted imaging features to benchmark the performance of the developed models. This comparative analysis provided insights into the contributions of different data modalities.\n\nOverall, the paper presents a comprehensive framework for developing and validating prognostic models in HNC, emphasizing the utility of combining diverse data sources and the importance of external validation in ensuring model generalizability."
    },
    {
      "question": "What are the potential real-world applications of this research?",
      "answer": "The research presented in the paper has several potential real-world applications, particularly in the field of oncology, specifically for head and neck cancer (HNC). Here are some key applications:\n\n1. **Personalized Medicine**: The development of machine learning (ML) models that predict overall survival in HNC patients can lead to more personalized treatment plans. By integrating electronic medical records (EMR) and radiological imaging data, clinicians can tailor therapies based on individual patient prognoses.\n\n2. **Clinical Decision Support**: The prognostic models can serve as decision support tools for oncologists, helping them to assess the likely outcomes of different treatment options. This can enhance the decision-making process regarding treatment intensity and strategies, particularly in heterogeneous cancers like HNC.\n\n3. **Risk Stratification**: The ability to predict a patient's lifetime risk of death and survival curves can aid in stratifying patients based on their risk profiles. This stratification can inform clinical trials and treatment protocols, allowing for more effective resource allocation and management of high-risk patients.\n\n4. **External Validation Frameworks**: The study emphasizes the importance of validating models across diverse patient populations. This can lead to the establishment of robust frameworks for evaluating the generalizability of predictive models, which is crucial for their adoption in clinical practice.\n\n5. **Integration of Radiomics in Clinical Workflows**: The findings suggest that radiomics, when combined with clinical data, can enhance prognostic accuracy. This integration can lead to the development of new imaging biomarkers that are clinically relevant and can be used in routine practice.\n\n6. **Research Collaboration and Data Sharing**: The study promotes collaborative research and the sharing of datasets, which can accelerate advancements in cancer research. By making datasets publicly available, it encourages further studies and validation efforts, fostering a more open scientific community.\n\n7. **Training and Education**: The methodologies developed in this research can be used to train healthcare professionals in the application of AI and ML in oncology, enhancing their skills in utilizing advanced technologies for patient care.\n\n8. **Future Research Directions**: The insights gained from this research can guide future studies aimed at improving predictive modeling in oncology, potentially leading to the discovery of new prognostic factors and treatment responses.\n\nOverall, the research has the potential to significantly impact clinical practices in oncology, improve patient outcomes, and contribute to the advancement of personalized medicine."
    },
    {
      "question": "How does the paper address potential biases in the research?",
      "answer": "The paper addresses potential biases in the research primarily through its rigorous validation framework and the use of diverse datasets. Here are the key points regarding how the paper tackles biases:\n\n1. **External Validation**: The study emphasizes the importance of external validation by testing the developed models on three independent patient cohorts. This approach helps to assess the generalizability of the models and identify any performance drops due to differences in patient populations.\n\n2. **Diverse Datasets**: The research utilized a large internal dataset of 2,552 patients from a single institution and compared model performance across three external datasets with a total of 873 patients. This diversity in datasets helps mitigate biases that could arise from training models on a homogenous group.\n\n3. **Population-Based Reporting**: The authors highlight the significance of detailed population-based reporting for AI/ML model utility. They note that the performance of models can significantly decrease when applied to datasets with different clinical and demographic distributions, indicating the need for careful consideration of population characteristics in model development.\n\n4. **Statistical Comparisons**: The paper includes statistical comparisons of clinical and demographic variables between the internal test set and external datasets, which helps to identify and account for potential biases related to differences in patient characteristics.\n\n5. **Transparency and Reproducibility**: The authors advocate for transparency in the modeling process and have made their code and data publicly available. This openness allows for scrutiny and replication of the research, which is essential for identifying and addressing biases.\n\n6. **Limitations Acknowledgment**: The paper acknowledges limitations such as the restriction of model development to researchers at one institution and the reliance on a single radiomics toolkit. By recognizing these limitations, the authors demonstrate an awareness of potential biases that could affect the results.\n\nIn summary, the paper employs a comprehensive strategy to address potential biases through external validation, diverse datasets, statistical comparisons, transparency, and acknowledgment of limitations, thereby enhancing the robustness and generalizability of the findings."
    },
    {
      "question": "What validation methods were used to ensure the reliability of the results?",
      "answer": "The research paper employed several validation methods to ensure the reliability of the results:\n\n1. **Internal Validation**: The study utilized a large internal dataset consisting of 2,552 patients with head and neck cancer (HNC) to develop prognostic models. The dataset was split into training and test subsets based on the date of diagnosis, allowing for an independent internal validation of the models.\n\n2. **External Validation**: The top-performing models were externally validated on three independent patient cohorts totaling 873 patients. This step was crucial to assess the generalizability of the models across different populations and clinical settings. The external datasets included patients from different institutions, which helped to evaluate how well the models performed when applied to diverse patient demographics and clinical characteristics.\n\n3. **Statistical Comparison**: The performance of the models was statistically compared using metrics such as the area under the receiver operating characteristic curve (AUROC) and average precision (AP). These metrics were used to rank the models and assess their predictive accuracy.\n\n4. **Benchmarking**: The study established baseline models using standard prognostic factors and compared the performance of the developed models against these benchmarks. This provided a reference point to evaluate the improvements offered by the new models.\n\n5. **Blinded Evaluation**: Participants in the model development were blinded to the outcomes of the test set, ensuring that the evaluation of model performance was unbiased.\n\n6. **Reproducibility and Transparency**: The code and data used for model training and evaluation were made publicly available on GitHub, promoting transparency and allowing other researchers to reproduce the results.\n\nThese validation methods collectively contributed to the robustness and reliability of the findings in the study, highlighting the importance of extensive validation in the development of predictive models in clinical settings."
    },
    {
      "question": "Are there any contradictions between this paper's findings and previous research?",
      "answer": "The findings of the paper present some contradictions with previous research in the field of head and neck cancer (HNC) prognosis using radiomics and machine learning (ML). Specifically:\n\n1. **Performance of Radiomics vs. EMR Features**: The paper concludes that the best-performing model for predicting overall survival in HNC utilized electronic medical records (EMR) features combined with tumor volume, rather than relying heavily on radiomics. This finding contradicts earlier studies, such as those by Ger et al. (2019) and Vallieres et al. (2017), which suggested that radiomics features could significantly enhance prognostic models. The current study indicates that handcrafted CT and PET imaging features did not improve prognostic performance compared to tumor volume alone, which challenges the notion that radiomics inherently provides superior predictive power.\n\n2. **Generalizability of Models**: The paper highlights significant drops in model performance when applied to external datasets, suggesting that previous studies may have overestimated the generalizability of radiomics-based models. This is particularly relevant given that many earlier studies did not adequately address the need for extensive validation across diverse patient populations, which the current research emphasizes as crucial for the utility of AI/ML models in clinical settings.\n\n3. **Deep Learning vs. Engineered Features**: While the paper acknowledges that deep learning models generally outperformed those based on handcrafted features, it also finds that none of the deep learning models surpassed the performance of the combined EMR-volume model. This contrasts with the prevailing view in the literature that deep learning approaches would consistently outperform traditional methods in medical imaging tasks.\n\nOverall, the current study suggests a reevaluation of the reliance on radiomics and deep learning in HNC prognosis, advocating for a more integrated approach that prioritizes EMR data and simpler models, which may not align with the expectations set by previous research in the field."
    }
  ]
}