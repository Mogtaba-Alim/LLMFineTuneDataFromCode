{
  "repo": "research_papers",
  "file": "Orchestrating_and_sharing_large_multimodal_data_for_transparent_and_reproducible_research.pdf",
  "language": "research_paper",
  "content": "ARTICLE Orchestrating and sharing large multimodal data for transparent and reproducible research Anthony Mammoliti1, 2,  Petr Smirnov1, 2,  Minoru Nakano1,  Zhaleh Sa fikhani1, 2,  Christopher Eeles1,  Heewon Seo1, 2, Sisira Kadambat Nair1,  Arvind S. Mer1, 2,  Ian Smith1, 2,  Chantal Ho1,  Gangesh Beri1, Rebecca Kusko3,  Massive Analysis Quality Control (MAQC) Society Board of Directors*,  Eva Lin4, Yihong Yu4,  Scott Martin4,  Marc Hafner4, 5& Benjamin Haibe-Kains1, 2, 6, 7 Reproducibility is essential to open science, as there is limited relevance for findings that can not be reproduced by independent research groups, regardless of its validity., It is thereforecrucial for scientists to describe their experiments in suf ficient detail so they can be repro- duced, scrutinized,  challenged,  and built upon., However the intrinsic complexity and con-tinuous growth of biomedical data makes it increasingly dif ficult to process, analyze,  and share with the community in a FAIR ( findable,  accessible,  interoperable, and reusable) manner., To overcome these issues we created a cloud-based platform called ORCESTRA(orcestra.ca ), which provides a flexible framework for the reproducible processing of multi- modal biomedical data., It enables processing of clinical genomic and perturbation pro files of cancer samples through automated processing pipelines that are user-customizable.ORCESTRA creates integrated and fully documented data objects with persistent identi fiers (DOI) and manages multiple dataset versions, which can be shared for future studies.https, //doi.org/10.1038/s41467-021-25974-w OPEN 1Princess Margaret Cancer Centre, University Health Network,  Toronto,  Ontario,  Canada.2Department of Medical Biophysics, University of Toronto,  Toronto,  Ontario,  Canada.3Immuneering Corporation,  Cambridge,  MA, USA.4Department of Discovery Oncology,  Genentech Inc,  South San Francisco, USA.5Department of Oncology Bioinformatics,  Genentech Inc,  South San Francisco, USA.6Department of Computer Science,  University of Toronto,  Toronto,  Ontario, Canada.7Ontario Institute of Cancer Research,  Toronto,  Ontario,  Canada., *A list of authors and their af filiations appears at the end of the paper.email bhaibeka@uhhnresearch.ca NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications 11234567890(), , , The demand for large volumes of multimodal biomedical data has grown drastically, partially due to active research in personalized medicine,  and further understanding diseases13., This shift has made reproducing research findings much more challenging because of the need to ensure the use of adequate data-handling methods, resulting in the validity and relevance of studies to be questioned4, 5., Even though sharing of data immensely helps in reproducing study results6 current sharing practices are inadequate with respect to the size of data and corresponding infrastructure requirements for transfer and storage2, 7., As computational processing required to process bio- medical data is becoming increasingly complex3 expertise is now needed for building the tools and work flows for this large-scale handling1, 2., There have been multiple community efforts in creating standardized work flow languages such as the Common Work flow Language (CWL) and the Work flow De finition Language (WDL), along with associated work flow management systems such as Snakemake8and Next flow9, in order to promote reproducibility10, 11., However a steep learning curve is encoun- tered for these programming-heavy solutions, in comparison to user-friendly data-processing platforms like Galaxy, which pro- vide both storage and computational resources but have limited features and scalability1214., While sharing these computational work flows along with metadata,  is of utmost importance, they are often missing15,  negatively impacting data provenance and transparency16., There is a dire need for reproducible and trans- parent solutions for processing and analyzing large multimodal data that are scalable while providing full data provenance., Biomedical data can expand into a plethora of data types such as in vitro and in vivo pharmacogenomics, toxicogenomics,  radiogenomics,  and clinical genomics., These data are a prime example of multimodal biomedical data with a long history of sharing in the field of biomarker discovery., Preclinical pharma- cogenomics involves the use of a genome-wide association approach to identify correlations between compound/treatment response and molecular pro filing, such as gene expression1719.I n addition, omics technologies have also been utilized in tox-icological pro filing for identifying the effect of compound toxicity on humans 20, and in radiogenomics data to uncover genomic correlates of radiation response21., These rich preclinical data are often combined with clinical genomics data generated over the past decades22with the aim to test whether preclinical biomarkers can be translated in clinical settings to ultimately improve patient care., Given the diversity of human diseases and therapies researchers can hardly rely on a single dataset and bene fit from collecting as much data as possible from all possible sources, calling for better sharing of data that are highly standardized and processed in a transparent and reproducible way., The generation of large volumes of data has led to a sharing paradigm in the research community where data are more accessible and open for public use., For studies to be reproduced and investigated for integrity and generalization by other researchers the sharing of raw and processed data is crucial., However, providing open access to data is not enough to achieve full reproducibility. To increase the value of open data one must clearly describe how the data are being curated and made amenable for analysis, and the shared data must be findable,  accessible,  interoperable,  and reusable, as outlined in the FAIR data principles23., These foundational principles include providing rich metadata that is detail-oriented including a persistent unique identi fier (findability), accessing (meta)data with authentication and the unique identi fier using a communications protocol (accessibility), assigning (meta)data with a commonly understood format/language (interoperability), and achieving data prove- nance with an accessible usage-license (reusability)., The Massive Analysis and Quality Control (MAQC) Society24has beenestablished to promote the use of a community-agreed standard for sharing multimodal biomedical data in order to achieve reproducibility in the field, such as through the FAIR principles., Therefore when translated into practice, these principles wouldpromote the reproducible and transparent handling and sharing of data and code, which would allow researchers to utilize and build from each other s work and accelerate new discoveries., However there are many genomic data maintainers and reposi- tories that do not meet the FAIR data principles for sharing data and pipelines., A common prevalent example of this is the use of one pipeline for data processing with no documentation pro- viding justi fication for the pipeline choice, impacting the dataset released,  which is often only a single version., In order to address these issues we developed ORCESTRA (orcestra.ca), a cloud-based platform that provides a transparent,  reproducible, and flexible computational framework for processing and sharing large multimodal biomedical data., The ORCESTRA platform orchestrates data-processing pipelines in order to curate customized versioned,  and fully documented data objects, which can be extended to a multitude of data types., This includes 11 pharmacogenomic (in vitro) 3 toxicogenomic, 1 xenographic pharmacogenomic (in vivo),  1 compendium of clinical genomic (21 studies), and 1 radiogenomics data objects that can be explored for a wide range of analyses., ORCESTRA is publicly accessible via orcestra.ca . Results The increasing utilization and demand for big data have resulted in the need for effective data orchestration 25, which is a process that involves organizing,  gathering, and coordinating the dis-tribution of data from multiple locations across a cluster of computational resources (e.g., virtual machines) with speci fic processing requirements., An ideal orchestration platform for handling large-scale heterogeneous data would consist of the following, (1) a de fined work flow,  (2) a programming model/ framework 25, (3) broad availability of computing infrastructure (e.g.,  virtual machines with storage systems), and (4) a security framework to prevent unauthorized access to data and compu- tational resources., At the work flow level data from different sources/lineages,  including data that are not static, must be effectively managed through the de finition of work flow compo- nents (tasks) that interact and rely on one another25., Moreover a programming model should be utilized for the work flow com- ponents responsible for handling the respective data (static and dynamic), such as a batch processing model (e.g.,  MapReduce)25., Lastly the utilization of a scalable computational environment, such as academic and commercial cloud-computing platforms, would allow for the management and processing of big data, providing the necessary computational resources,  ability to transfer data, and monitoring of executed work flows and respective components/tasks, further enabling tracking data provenance., There exist multiple orchestration tools with various features to our knowledge, that are currently being used for the storage,  processing,  and sharing of genomic data, namely Pachyderm,  DNAnexus,  Databricks,  and Lifebit (Table 1)., We opted for Pachyderm an open-source orchestration tool for multi-stage language-agnostic data-processing pipelines, main- taining complete reproducibility and provenance through the use of Kubernetes, as it provides the following functionalities,  Programming language, Pachyderm supports creating and deploying language-agnostic pipelines across on-premise or cloud infrastructures, a feature also supported by DNAnexus,  Databricks,  and Lifebit., Large dataset support Users can upload and process large datasets through the use of the Pachyderm file system (PFS), ARTICLE NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w 2 NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications where the data are exposed in its respective container for utili- zation in pipelines while being placed in an object storage (e.g., Azure Blob,  AWS bucket)., Automatic pipeline triggering Reproducibility and provenance are guaranteed via automatic pipeline triggering when existing data are modi fied or newly added, which results in the generation of new versions of an output data object., However because automatic triggering requires the state of each pod within the Kubernetes cluster to be saved, there is a permanent allocation ofCPU/RAM for each pod (and therefore each pipeline), whichrequires a user to create a cluster with potentially costly resources., The other platforms do not require permanent allocation of resources as for example, Lifebit allows users to spin up instanceson-demand to meet the computational requirements for a given pipeline., Reprocessing A feature that is found in Pachyderm,  DNA- Nexus, and Lifebit is the prevention of recomputation for each pipeline trigger, which comes in handy when a pipeline contains processed raw data that does not need to be reprocessed if there is a change in metadata such as an annotation file., Docker utilization Each pipeline can be equipped with a Docker image connected to Docker Hub for running various toolkits, which allows for simplistic pipeline updating when there are future updates to any component of the Docker image., Docker usage is also translated across the other platforms as well. Versioning of data and pipelines with unique identi fiers Each commit, an operation for submitting and tracking changes to a data source, is supplied with a unique identi fier, which is updated with each new commit (parent child system)., This allows users to track different versions of a pipeline and dataset with ease. However with Databricks and Lifebit,  this feature is partially supported, as not every pipeline and respective input/output file(s) are provided with a unique identi fier, even when data are updated through commits., Parallelism support A pipeline can be parallelized via a con- stant or coef ficient strategy in Pachyderm using workers, which is useful for workloads with large computational requirements., When a constant is set Pachyderm will create the exact number of workers speci fied (e.g. constant, 5,  5 workers),  that will parallelize across nodes in the cluster., The coef ficient will result in Pachy- derm creating a number of workers based on the number of nodes available (multiple nodes), which will also specify the number of workers per node (e.g.,  coef ficient,  2.0,  20 nodes, 40 workers,  2.0 workers per node)., The other platforms also support parallelization including automatic parallelization of samples across instances.Data-versioning system, Pachyderm provides direct GitHub integration for data versioning, which enables users to track changes at the file level and submit updates to Pachyderm through commits triggered through webhooks on GitHub., In addition this also provides users with the ability to publicly view,  track, and share all updates made to a pipeline or file connected to Pachyderm with ease., Open access Pachyderm provides a free and open-source version of the tool that contains all the functionalities required to develop a platform ensuring transparent and reproducible pro- cessing of multimodal data., Despite these advantages, the choice of Pachyderm is not without compromises. We list below the functionalities that Pachyderm is lacking but would have been bene ficial to develop our platform, Direct mounting of the data, Pachyderm (v.1.9.3) does not allow for direct mounting of data from a cloud storage system (e.g., bucket) to a Pachyderm repository., Data must be manually transferred to the tool s own file system using the Pachyderm put file command, resulting in essentially an additional copy of the data within a cloud environment., Databricks and Lifebit enable decreasing computation time and cost by not copying data into a file system for it to be used by the platform., This is important when large data sizes will be used in an analysis which allows a user to simply store their data in a bucket/blob storage account, and mount it to the platform of interest, giving the user the ability to also use the data with other platforms or cloud services without having to repeatedly copy it in an inef ficient manner., Cost-ef ficiency Pachyderm utilizes VM s through a Kubernetes cluster of deployment on a cloud environment, which arecostly to keep running inde finitely., Therefore utilizing Pachy- derm on a cloud infrastructure impacts cost-ef ficiency, in com- parison to an on-premise high-performance computing (HPC) infrastructure., A notable feature that is supported by Lifebit is cost-ef ficiency through low-priority instance utilization on a cloud provider, allowing for users to execute large-scale analyses at a reduced cost., Resource allocation Pachyderm requires persistent RAM/CPU allocation for each pipeline within the Kubernetes cluster, even after a pipeline is successfully executed,  which permits automatic pipeline triggering., Thus an increased amount of computational resources (VM s scaled up/out) may be required for speci fic pipelines, which also impacts cost-ef ficiency., The ORCESTRA platform . Building on the strengths of the Pachyderm orchestration tool we have developed ORCESTRA, a cloud-based platform for data sharing and processing ofTable 1 Data-processing platforms and their respective features for handling multimodal data., Features ORCESTRA (Pachyderm) DNAnexus Databricks Lifebit Create language-agnostic pipelines in the cloud    Large dataset support (TB in size)    Automatic pipeline triggering with updated data (out-of-the-box)  XXX Prevents recomputation of entire dataset with each new pipeline trigger  X  Docker utilization    Every pipeline run and data sources are versioned with a unique identi fier aa Parallelism support    Versioning system (e.g., GitHub) for pipelines and input data    Open access (free)  XXX Direct mounting of data (no copying into file system) X X  Automatic cost-ef ficiency implementation for instances (low-priority) X X X  No permanent resource allocation for a pipeline (memory/CPU) X  aIndicates partial support of the feature., Each feature was tested against each platform using biomedical data as an input data source.NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w ARTICLE NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications 3 biomedical data based on automation, reproducibility,  and transparency., ORCESTRA allows users to create a custom data object that stores molecular pro files perturbation (chemical and radiation) pro files, and experimental metadata for the respective samples and patients, allowing for integrative analysis of the molecular and perturbation and clinical data (Fig., 1). The plat- form utilizes datasets from the largest biomedical consortia including 17 curated data objects containing genomics,  pharma- cological,  toxicological, radiation,  and clinical data (Supplemen- tary Table 1)., The data objects can accommodate all types of molecular pro file data however, ORCESTRA currently integrates gene expression (RNA-sequencing,  microarray), copy number variation,  mutation,  and fusion molecular data., For RNA-seq data users can select a reference genome of interest, a combi-nation of quanti fication tools and their respective versions, along with reference transcriptomes from two genome databases (Ensembl, Gencode) to generate custom RNA-seq expression profiles for all of the cell lines in the dataset., Therefore each data object will be generated through a custom orchestrated Pachy- derm pipeline path, where each piece of input data,  pipeline, and output data option is tracked and given a unique identi fier toensure the entire process is completely transparent and repro- ducible., To ensure data-object generation is fully transparent and that provenance is completely de fined each data object is auto- matically uploaded to Zenodo and given a public DOI, where theDOI is shared via a persistent web page that possesses a detailed overview of the data that each DOI-associated data object con- tains and how it was generated., This includes publication sources treatment sensitivity information and source,  raw data source, exact pipelines parameters used for the processing tools of choice, and URLs to reference genomes and transcriptomes used by the tool(s)., Moreover a BioCompute Object is automatically gener- ated alongside each data object, which is a standardized record supported by the U.S. Food and Drug Administration (FDA) for communicating bioinformatic pipelines and verifying/validating them in order to aid in the reproducibility of experiments 26., This includes sharing information such as pipeline steps data input/ output sources, and software utilized with their respective ver- sions and parameters., In addition release notes are also provided by ORCESTRA where the number of samples,  treatments, sen- sitivity experiments,  and molecular pro file data are tracked between versions of a dataset, allowing users to identify changes between each new data update that were released from the respective consortium and pushed to the platform., This metadata page gets automatically sent to each user via email providing users with one custom page that hosts all of the information required to understand how the data object was generated., Therefore all of the data used in the data object is shared in a transparent manner, where researchers can identify the true origins of all data used with con fidence and effectively reproduce results., Data-object generation . ORCESTRA comes with a web- application interface allowing users to interact with the data- processing and data-sharing layers., Users can search existing data objects in the Search/Request view by filtering existing data objects with the data object Parameters panel., Users can filter existing data objects by selecting datasets with associated drug sensitivity releases, genome references,  RNA-seq transcriptomes,  RNA-seq processing tools with respective versions, which associates with other respective DNA data types (mutation or CNV) and RNA data types (microarray or RNA-seq)., Changes in the parameter selections trigger the web app to submit a query request to a MongoDB database which returns a filtered list of data objects (Fig., 2). The data-object table is then re-rendered with an updated list of data objects. This allows users to search through existing data objects to determine if a data object that satisfies users parameter selections already exists, preventing recomputation., Information about the datasets and tools used to generate a data object can be viewed by clicking on a data-object name and navigating to its data-object metadata web page., Users can obtain information such as associated publications links to the raw drug sensitivity and molecular pro file data as well as a Zenodo DOI., In addition the individual data-object view pro- vides users with the option to download the data object of choice directly from the view., Users can request a customized data object in the Search/ Request view by turning the Request data object toggle on., This action recon figures the dropdown options in the data object Parameters panel to be in request mode, and displays,  on the Summary panel, two text input fields for entering a custom name for the data object and a user s email to receive a notification upon data-object pipeline completion, with the accompanied Zenodo DOI and custom ORCESTRA metadata page link., Pachyderm continuously scans for a new request from the web-app which will automatically trigger the respective Fig., 1 Summary of samples treatments, and molecular pro files utilized for data-object generation in ORCESTRA., Molecular data sample, and treatment information are combined to yield 17 unique data objects from a variety of biomedical data types.ARTICLE NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w 4 NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications pipelines to build the custom data object, while storing a unique ORCESTRA ID,  Pachyderm pipeline commit ID, and Zenodo DOI into the MongoDB database, which increases the level of data provenance and reproducibility, as each data object can be identi fied through three unique identi fiers after creation (Fig., 2). The data-object filtering process as described above continues to function as users select the request parameters, which displays existing data object(s) that satisfy users parameter selections., Upon selecting all the required parameters the Submit Request  button becomes active for users to submit the pipeline request., Data-object usage . The data objects generated by ORCESTRA can be utilized to execute large-scale analyses for advancing biomedical research (Supplementary Methods)., The platform harnesses various open-source R Bioconductor packages within its Pachyderm work flows in order to create the data objects for each biomedical data type offered., These packages include PharmacoGx for pharmacogenomics data27 ToxicoGx for tox- icogenomics data20, Xeva for xenographic pharmacogenomics data19,  MetaGxPancreas for clinical data22, and RadioGx for radiogenomics data28., The GRAY UHNBreast,  CCLE, and GDSC2 pharmacogenomic data objects were utilized to showcase the strong association between ERBB2 mRNA expression and Lapatinib drug response (AAC) across all datasets (Supplemen- tary Fig., 1)29,30. In addition the consistency of Lapatinib response was investigated between CTRPv2 and GDSC1/2, where a stronger consistency was observed between CTRPv2 and GDSC2, as they use the same pharmacological assay (Cell Titre Glo), in comparison to CTRPv2 and GDSC1 (Supplementary Fig., 2)31.T o highlight drug compound toxicity the Open TG-GATEs Human data object was used to identify top differentially expressed genes formost drug-induced liver injury (DILI) drug acetaminophen and no DILI drug chloramphenicol on primary human hepa- tocytes (Supplementary Fig., 3)20. For xenographic pharmacoge- nomics the Novartis patient-derived xenograft encyclopedia (PDXE) data object expressed a strong correlation between tras- tuzumab response and ERBB2 expression from breast cancer patient-derived xenograft models (Supplementary Fig., 4)19. The prognostic value of the Pancreatic Cancer Overall Survival Pre- dictor (PCOSP) and clinical models was investigated across pancreatic cancer patients in the MetaGxPancreas data object (Supplementary Fig., 5)32. Lastly for radiogenomics, the Cleve- land data object highlighted the correlation between gene expression and radiosensitivity (AUC area under the fitted radiation survival curve) across tissue types (Supplementary Fig., 6)28. All analyses/ figures can be reproduced via a CodeOcean custom compute capsule ( https //codeocean.com/capsule/ 9215268/tree ),  which hosts the data objects,  respective code, and generated figures,  allowing for full transparency., Data-object metrics . The platform provides several usage metrics for users. These metrics can be accessed through Home ,Sta- tistics , and Request Status views. The Home view provides an overview of currently available datasets tools and references to generate data objects,  most downloaded data objects, and a number of pending or in-process data-object requests., The Statistics view provides a visualized data-object popularity ranking along with a plot of the number of cell lines,  drugs,  and genes for the canonical data objects, including intersection, which can be accessed by clicking the View Statistics button in the Home view., The Request Status view displays a tabulated list of data-object requests that are either pending (the request has been submitted and saved, but has not been processed in Pachyderm), or in-process (the request has been submitted and is processed in Pachyderm)., User accounts for data-object tracking . The platform offers users the option to register for an account with a valid email address. Registered users are able to select existing data objects in theSearch/Request view and save them as their favorites  which can be accessed in the User Pro fileview., However the web application keeps track of data-object requests submitted by users based on their email addresses even without registration., These data objects are automatically added to a user s favorite data objects and can be viewed in the User Pro fileview., External data uploading and sharing . The platform enables users to request the processing of their own research data into a curated dataset through the Data Submission feature., This feature is accessible only to the registered users. In order to submit their data the users are asked to complete the data submission form, with the help of data submission guideline documentation and sample data files provided in the documentation section., Upon submitting the request, an email noti fication is sent to ORCESTRA administrator. The administrator then veri fies the submitted data and con figures Pachyderm to process the submitted data., Platform security . In order to prevent unauthorized access to our computational resources including data and virtual machines, security measures have been implemented throughout our Azure ecosystem., Because Pachyderm resides within a Kubernetes cluster it is important to prevent outside access to the KubernetesFig., 2 ORCESTRA web-application connectivity with data-processing layer through commit identi fier (ID) scanning for user-selected pipeline requests, and subsequent data-object DOI tracking with MongoDB queries., The web-application layer receives pipeline requests under the form of JavaScript Object Notation (JSON) file and updates the ORCESTRA database with each data-object digital object identi fier (DOI) and commit ID., The orchestration functionality scans for new pipeline requests and executes them to generate a versioned data object, which is uploaded to Zenodo to ret rieve a DOI in the data-sharing layer.NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w ARTICLE NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications 5 API server, which was executed through Azure Active Directory., This enforces role-based access control (RBAC) to the cluster, allowing selective access to it. RBAC is also extended to the storage solutions utilized by ORCESTRA allowing us to monitordata that is imported and accessed while preventing unauthorized access., Discussion The high-dimensionality complexity, and scale of multimodal data present unprecedented challenges for researchers in the biomedical field, in regard to their ability to effectively manage,  track,  and process the data., The nature of heterogeneous and complex data negatively impacts data provenance through incomplete or no accompaniment of metadata for a dataset, resulting in the uncer- tainty of a data lineage 3335., Because the granularity of metadata is a determinant of the value of a dataset36 it should provide a rich description of dataset content,  following the FAIR data principles, which includes information about dataset origin,  how it was gen- erated, if there were any modi fications that were made to it from precedent versions, and what these modi fications were 23, 37, 38., When the FAIR data principles are not met issues with reprodu- cibility in the biomedical sciences follow, where data are either not shared or results/estimates and claims cannot be checked for cor- rectness., However datasets published online,  including ones that reside in repositories and from journals, are often not accompanied by suf ficient metadata39.I nt h e field of genomics, issues with metadata often include mislabelling or misannotation of data (e.g., incorrect identi fication numbers),  improper data characterization (e.g., mapping files to respective samples and protocols), and inconsistency in the way metadata are presented (nonuniform structure used across consortia)16., Provenance also extends to the computational work flows that are developed to process datasets2 a s sharing relevant source code is often not provided15along with relevant documentation about the work flow, such as in graphical- user interface (GUI) based systems like Galaxy, affecting the ability to reproduce results2., In addition data maintainers and consortia, such as the Cancer Cell Line Encyclopedia (CCLE)40and the Genomics of Drug Sensitivity in Cancer (GDSC), often only process the dataset using one pipeline that they believe is the most suitable, without documenting supporting evidence as to why the chosen processing pipeline was selected over other competing ones in the field41, 42., This issue is also present in other data types such as xenographic or metagenomics data where the molecular data are processed and normalized using only one pipeline22, 43., Therefore only a single version of the dataset is released, which makes it difficult for other researchers to perform a diverse set of analyses that require the use of different processing pipelines on the dataset., Lastly it is important to note that datasets evolve and are therefore not static, as new data are added and respectively depreciated, which further highlights the need for transparent data-sharing practices, especially at the file level where updates can be easily identi fied., There are multiple data portals created for accessing and sharing biomedical data but with limitations in regard to reproducibility (Supplementary Table 2)., Below are sharing practices that are adopted across various data types,  such as pharmacogenomics, toxicogenomics,  radiogenomics,  xenographic pharmacogenomics,  and clinical genomics data, Pharmacogenomics ., The Genomic Data Commons Data Portal (NIH/NCI GDC) hosts raw data for the Cancer Cell Lines Encyclopedia (CCLE) from the Broad Institute, including RNA,  whole-exome,  and whole-genome sequencing data, allowing users to select and download the data type(s) of interest., Obtaining the data can be done through direct download or their GDC DataTransfer Tool by providing a manifest file that possesses the unique identi fiers (UUID) of each file, which also allow users to locate the files again through the portal, along with their corre- sponding run,  analysis,  and experimental metadata., This isadvantageous as all the raw data (public and controlled access),  for both datasets, are located within one portal and can be accessed in an ef ficient manner., However the recent addition of new CCLE data (e.g.,  additional RNA-seq cell lines) 41, is found on the European Nucleotide Archive (ENA),  but not on GDC, resulting in data source inconsistency that becomes dif ficult to manage and follow for users., Current and previous versions of other CCLE data (i.e. annotation, drug response) are hosted on a Broad Institute portal, with no release notes or documentation present with each version, forcing researchers to manually iden- tify changes within each file after every release., GRAY a dataset generated by Dr. Joe Gray s lab at the Oregon Health and Science University, has had three updates with raw data hosted on NCBI, with drug response and annotation data hosted on SYNAPSE,  DRYAD, and/or the papers supplementary section4446.I n addition, drug-response data can also be found on the LINCS data portal., Because each version of the dataset is associated with a different respective paper the data are scattered among various repositories, which makes it challenging to keep track of each source, and for each source to ensure that the data remain readily available, as one failed link would make it dif ficult for a researcher to reproduce any results., However for the GRAY dataset, NCBI provides detailed information about the methodology used for the experiments, SYNAPSE provides a wiki and contact source for the dataset and a provenance tracker for each file that is uploaded, and DRYAD stores each publications data as a package organized with subsequent descriptions to keep data organized., A promi- nent example in effective data-sharing practices is DepMap (depmap.org )47 which provides a portal to download molecular and pharmacological data from a variety of consortia, with an interface that allows users to dive into the multiple data releases for a given dataset, which is accompanied by descriptive metadata such as associated publications and file-level descriptions., This provides users with the ability to download a dataset directly from a source or combine them together to form a custom dataset, all while being able to compare different updates/versions in an interactive manner., However the portal does not allow users to select different processing pipelines and lacks details regarding the pipelines used for some of the processed data hosted, such as molecular data (e.g.,  genomic tools used), which highlights a need for increased granularity in the portal., Toxicogenomics . The Life Science Database (LSDB) Archive is a database that hosts datasets by Japanese researchers ( https, // dbarchive.biosciencedbc.jp/ ),  such as the TG-GATE toxicogenomics dataset48., The database provides rich metadata for users such as a DOI and clickable sections that provide granular details about each file in the dataset, which includes a description of the file contents andfile attributes (e.g., data columns and respective descriptions for each column)., In addition the da tabase allowed for TG-GATE to provide a timeline of updates to the dataset, where data corrections are posted with accompanying corrected files and a description of the update, which allows maintainers to be transparent with users about the dataset lineage., However even though the maintainers for TG-GATE have indicated that the dataset was updated, detailed file-level changes are not provided, along with the processing pipelines and/or information regarding how the data was generated/ processed into their resulting formats., Xenographic and radiogenomics . The largest datasets for patient- derived tumor xenograft and radiogenomics studies are availableARTICLE NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w 6 NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications through supplementary materials attached to their scienti fic publications21, 43., These supplementary data provide users with information about the methods used to generate the data however,  access is dependent on the journal itself, which raises issuesregarding the potential of broken data links., In addition the amount of data that can be added to a publication via a supple- mentary section may be limited due to journal restrictions, which increases the likelihood of files being distributed across other data- sharing platforms (e.g., SYNAPSE),  increasing the dif ficulty in locating and keeping track of dataset updates, or resulting in a reliance of contacting authors to obtain a complete dataset that cannot be otherwise shared via the journals web interface., Clinical genomics . Over the years clinical genomics data has been stored and shared across a wide range of consortia such as NCBI (GEO/EGA) and/or as supplementary material to a pub- lication., However this inconsistency has led to the development of several data compendia to consolidate the data for transparent mining/managing, sharing,  and analysis,  such as Oncomine 49, MultiAssayExperiments R package for multiple experimental assays50, and curatedData R packages for molecular pro file analysis51., In addition the MetaGx R packages were developed to allow users to retrieve a compendium of transcriptomic data and standardized metadata from a wide array of studies and cancer types (pancreas, breast,  ovarian),  allowing for integrative analysis of the data for biomarker discovery22., In order to address these issues of primary data acquisition and sharing pertaining to multiple studies across a range of biomedical data types, ORCESTRA harnesses a flexible framework that allows for sharing the respective data in a transparent manner., More specifically all data sources and associated publications are clearly communicated to users for a speci fied data object, in order to limit the need to execute additional source searches outside of the platform., In addition all pipelines are shared with users via a direct GitHub link on the metadata web pages, ensuring that users can re- create the objects themselves from the respective data sources., This also includes the Docker image used for each data-object creation ensuring that computational environments remain consistent for data-object generation outside of the platform, if needed., All data objects also have a persistent identi fier (DOI) allowing them to be referenced back to one source,  increasing data transparency and provenance, unlike other consortia., Lastly the platform provides credit and data disclaimers to all primary data generators, which are packaged with data sources,  respective pipeline code,  associated publications, and persistent identi fiers,  into one package/environ- ment that is bene ficial, as it promotes the FAIR principles in the platform., However it must be noted that one platform solution alone will not be suf ficient to solve all issues with data sharing, which is still facing multiple sociopolitical challenges in the scienti fic community., ORCESTRA provides a space that enablesthe uni fication of all primary data sources across multiple studies into one location, which is a step forward in standardizing the manner in which these data are processed and shared within the research community., To encourage user uptake we plan on regularly updating the platform with additional datasets and data types., In addition we plan on automating the manner in which users can upload their own data for processing through standardized processing pipelines, which will further limit human intervention for generating data objects., Due to the open-source nature of the platform we hope to invoke more community involvement by allowing users to run local instances of the platform to process their own data in a reproducible and transparent manner using built-in pipelines., Lastly we aim to implement a metrics system to keep track of data objects used in future publications, in order to demonstrate its impact in the research community., In conclusion the ORCESTRA platform provides a new paradigm for sharing ready-to-analyze multimodal data while ensuring full transparency and reproducibility of the curation, processing,  and annotation processes., ORCESTRA provides the data provenance and versioning tools necessary to maximize the reusability of data, a cornerstone of Open Science., Methods In order for the platform to be as transparent as possible it harnesses an archi- tecture with three distinct layers that not only works independently to process and interpret precedent data, but also have the capacity to scale (Fig., 3). Web-app layer . The first layer contains the web application which was developed using a Node.js API and React front-end with MongoDB as a database., The layer provides the user with an interaction point to the ORCESTRA platform allowing users to first select the data type they wish to explore,  either Pharmacogenomics, Toxicogenomics,  Xenographic pharmacogenomics,  Radiogenomics,  or Clinical genomics., They can then search for existing data objects request a new data object by entering pipeline parameters,  view data-object request status, and register a personal account to save existing data objects of choice., Data-processing layer . The second data-processing layer encompasses a Kuber- netes cluster on Microsoft Azure that hosts Pachyderm, which utilizes Docker images for running R packages., All of the RNA-seq raw data have been pre- processed with Kallisto and Salmon Snakemake pipelines using an HPC environ- ment, and subsequently pushed to assigned data repositories on Pachyderm, allowing for speci fied selection from the web-app (transcriptome and tool version)., Microarray cnv,  mutation, and fusion data are either processed directly with Pachyderm due to low computational requirements or aggregated into the dataobjects from public sources., The Pachyderm pipelines aggregate repositories that host data generated on an HPC environment or on GitHub ( https, //github.com/ BHKLAB-Pachyderm ) into a Docker image that builds a data object based on user specifications (e.g., RNA-seq data processed by Kallisto v.0.46.1,  inclusion of only CNV data) (Fig., 4). The GitHub hosted files can be viewed at the file level for changes and edited which automatically triggers the Pachyderm pipeline with the new modi fications to produce a new data object., A unique feature of Pachyderm is the prevention of reprocessing computed data such as where an update of RNA-seq annotations will not trigger the reprocessing of thousands of drug-response data, which reduces computation time., In addition Pachyderm can be turned on/ off by shutting on/off the computational resources that it utilizes in a cloud Fig., 3 The ORCESTRA framework layers for pipeline selection data-object generation, and digital object identi fier (DOI) sharing with a custom metadata web page., The web-application layer allows users to request custom data objects which are generated through Pachyderm in a Kubernetes cluster within the data-processing layer., Each versioned data object is automatically pushed to the data-sharing layer and uploaded to Zenodo to obt ain a DOI., Data objects that have already been processed result in the immediate sharing of custom metadata web pages with users via email.NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w ARTICLE NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications 7 environment (e.g., virtual machines)., Therefore ORCESTRA can control costs by preventing the need for the resources to be constantly running, as it will only generate data objects when needed,  including requests sent in by users., TheORCESTRA costs for both virtual machines and storage per year on average is collectively ~$2800 CAD., Virtual machines contribute to ~$1300 CAD of this cost, while storage contributes to ~$1500 CAD. Pipelines are located in ORCESTRAs GitHub page ( https//github.com/BHKLAB- Pachyderm ), which are executed by running pachtl create-pipeline on their respective JSON file., Each JSON file has speci fied inputs that are accessed by the pipeline along with a command section that runs a given script., These scripts are responsible for data generation which is output into a storage container on Azure., Pipeline repositoriesare denoted by a pipelines suffix in the repository name. Data-sharing layer . Each generated data object enters the third data-sharing layer where the data object gets automatically uploaded to an online data-sharing repository known as Zenodo, with a DOI so that the data object can be given a persistent location on the internet to be uniquely identi fied., The generated DOI is then associated with a custom metadata web page that is generated based on the contents of the data object., A BioCompute Object is also generated alongside the data objects which are automatically deposited to Zenodo, where their DOI is also shared via the custom metadata web pages., Data disclaimers usage policies, and credits to the original data generators are communicated to users in order to ensurethe data is accessed and shared in an acceptable manner., In addition to publicly sharing curated datasets the platform leverages Zenodo s access control feature to enable users to keep their curated dataset access restricted.Users may choose to keep the dataset private when submitting their own data for curation through the Data Submission feature., When pachyderm uploads the processed dataset to Zenodo by using their API it adds a set of parameters to the upload request to keep access right of the uploaded data to the restricted status., The uploaded data with restricted status is only accessible upon request. Similarly on the ORCESTRA web application, the information about the private dataset is only accessible to the user who submitted the data., The user may choose to grant access to view information about the private dataset by using the shareablelink generation feature that is available on the private dataset page., Finally the platform offers users an ability to publish the dataset by clicking the Publish Dataset button on the private dataset page., Upon receiving this request the web application updates the database to indicate that the visibility of the dataset is public , and executes a series of API requests to Zenodo to change the access right toopen , making the dataset publicly accessible., Reporting summary . Further information on research design is available in the Nature Research Reporting Summary linked to this article., Data availability The GRAY dataset used in this study has been deposited at https //doi.org/10.5061/ dryad.03n60 and is provided under CC0 1.0 Universal (CC0 1.0) Public Domain Dedication license., The CCLE dataset used in this study has been deposited at https //data.broadinstitute.org/ccle_legacy_data and is provided under the Creative Commons Attribution 4.0 license., The CTRPv2 dataset used in this study has been deposited at https //portals.broadinstitute.org/ctrp and is provided under the Creative Commons Attribution 4.0 license., The gCSI dataset used in this study has been deposited at http // research-pub.gene.com/gCSI_GRvalues2019/ and is provided under the Creative Commons BY 4.0 license., The FIMM dataset used in this study has been deposited at https //doi.org/10.1038/nature20171 and is provided under the Creative Commons BY 4.0 license., The GDSC dataset used in this study has been deposited at https // www.cancerrxgene.org and has the following data-usage policy,  https, // depmap.sanger.ac.uk/documentation/data-usage-policy/ ., The UHNBreast dataset used in this study has been deposited at https //codeocean.com/capsule/6718332/ and is provided under the Creative Commons BY 4.0 license., The Open TG-GATEs dataset used in thisstudy has been deposited at Lifescience Database Archive https, // dbarchive.biosciencedbc.jp/en/open-tggates/download.html and are provided under Creative Commons Attribution-Share Alike 2.1 Japan., The EMEXP2458 dataset used inthis study has been deposited at https //www.ebi.ac.uk/arrayexpress/experiments/E- MEXP-2458/ and is provided under permissive license at https, //www.ebi.ac.uk/ arrayexpress/help/FAQ.html#data_restrictions ., The DrugMatrix dataset used in this study has been deposited at (diXa Data Warehouse www.dev.ebi.ac.uk/fg/dixa/ study ID DIXA-033) and is attributed to the National Toxicology Program and may be copiedand distributed without permission., The PDXE dataset used in this study has beendeposited at https //pubmed.ncbi.nlm.nih.gov/26479923/ and may be utilized under NCBI and author guidelines., The MetaGxPancreas dataset used in this study has been deposited at http //bioconductor.org/packages/release/data/experiment/html/ MetaGxPancreas.html and is provided under Creative Commons Attribution 4.0 International License., All of the data objects are publicly available on ORCESTRA (orcestra.ca) via dedicated documented web pages, which include respective digital object identi fiers (DOI) and Zenodo links for each data object generated., Data for the case studies in the manuscript can be accessed in a custom compute capsule on Code Ocean athttps, //codeocean.com/capsule/9215268/tree ., Code availability All of the code used by ORCESTRA is publicly available on GitHub via the Apache 2.0 license, https, //github.com/BHKLAB-Pachyderm ., All analyses performed using the data objects can be reproduced through a custom compute capsule on Code Ocean, https, // codeocean.com/capsule/9215268/tree ., Received: 18 March 2021; Accepted: 8 September 2021; References 1. Madduri, R. et al. Reproducible big data science: a case study in continuous FAIRness. PLoS ONE 14, e0213013 (2019). Fig. 4 The Cloud-based deployment of the ORCESTRA data-processing layer automatically versions data using Pachyderm and shares generated data objects through Zenodo via a persistent identi fier (DOI)., Each file and pipeline in the Pachyderm environment are provided a unique identi fier allowing for each data object to be versioned.ARTICLE NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w 8 NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications 2., Kanwal, S., Khan, F. Z., Lonie, A. & Sinnott, R. O. Investigating reproducibility and tracking provenance a genomic work flow case study. BMC Bioinforma. 18, 337 (2017). 3. Toga, A. W. & Dinov, I. D. Sharing big biomedical data. J. Big Data 2,112 (2015). 4. Huang, Y. & Gottardo, R. Comparability and reproducibility of biomedical data. Brief. Bioinform. 14, 391 401 (2013). 5. Patil, P., Peng, R. D. & Leek, J. T. A visual tool for de fining reproducibility and replicability. Nat. Hum. Behav. 3, 650 652 (2019). 6. Finak, G. et al. DataPackageR reproducible data preprocessing, standardization and sharing using R/Bioconductor for collaborative dataanalysis., Gates Open Res. 2, 31 (2018). 7. Suthakar, U., Magnoni, L., Smith, D. R., Khan, A. & Andreeva, J. An ef ficient strategy for the collection and storage of large volumes of data for computation. J. Big Data 3, 21 (2016). 8. Koster, J. & Rahmann, S. Snakemake a scalable bioinformatics work flow engine. Bioinformatics 28, 2520 2522 (2012). 9. Di Tommaso, P. et al. Next flow enables reproducible computational work flows. Nat. Biotechnol. 35, 316 319 (2017). 10. Goble, C. et al. FAIR computational work flows. Data Intell. 2, 108 121 (2020). 11. Kulkarni, N. et al. Reproducible bioinformatics project: a community for reproducible bioinformatics analysis pipelines. BMC Bioinforma. 19, 349 (2018). 12. Ahmed, A. E. et al. Managing genomic variant calling work flows with Swift/T. PLoS ONE 14, e0211608 (2019). 13. Bourgey, M. et al. GenPipes: an open-source framework for distributed and scalable genomic analyses. https://doi.org/10.1101/459552 (2019). 14. Afgan, E. et al. The Galaxy platform for accessible, reproducible and collaborative biomedical analyses: 2016 update. Nucleic Acids Res. 44, W3W10 (2016). 15. Mangul, S. et al. Systematic benchmarking of omics computational tools. Nat. Commun. 10, 1393 (2019). 16. Learned, K. et al. Barriers to accessing public cancer genomic data. Sci. Data 6, 98 (2019). 17. Sanoudou, D., Mountzios, G., Arvanitis, D. A. & Pectasides D. Array-based pharmacogenomics of molecular-targeted therapies in oncology.Pharmacogenomics J., 12, 185 196 (2012). 18. T. P. A.,  M.,  S. S.,  Jose,  A.,  Chandran,  L. & Zachariah,  S. M. Pharmacogenomics, the right drug to the right person., J. Clin. Med. Res. 1, 191194 (2009). 19. Mer, A. S. et al. Integrative pharmacogenomics analysis of patient-derived xenografts. Cancer Res. 79, 4539 4550 (2019). 2 0 . N a i r S .K .e ta l .T o x i c o D B , a ni n t e g r a t e dd a t a b a s et om i n ea n dv i s u a l i z e large-scale toxicogenomic datasets., Nucleic Acids Res. 48, W455 W462 (2020). 21. Yard, B. D. et al. A genetic basis for the variation in the vulnerability of cancer to DNA damage. Nat. Commun. 7,114 (2016). 22. Gendoo, D. M. A. et al. MetaGxData clinically annotated breast, ovarian and pancreatic cancer datasets and their use in generating a multi-cancer gene signature., Sci. Rep. 9, 8770 (2019). 23. Wilkinson, M. D. et al. The FAIR guiding principles for scienti fic data management and stewardship. Sci. Data 3, 160018 (2016). 24. Shi, L. et al. The international MAQC Society launches to enhance reproducibility of high-throughput technologies. Nat. Biotechnol. 35, 1127 1128 (2017). 25. Barika, M. et al. Orchestrating big data analysis work flows in the cloud research challenges,  survey, and future directions., ACM Comput. Survey 52, 141 (2019). 26. Simonyan, V., Goecks, J. & Mazumder R. Biocompute objects a step towards evaluation and validation of biomedical scienti fic computations., PDA J. Pharm. Sci. Technol. 71, 136 146 (2017). 27. Smirnov, P. et al. PharmacoGx: an R package for analysis of large pharmacogenomic datasets. Bioinformatics 32, 1244 1246 (2016). 28. Manem, V. S. K. et al. Modeling cellular response in large-scale radiogenomic databases to advance precision radiotherapy. Cancer Res. 79, 6227  6237 (2019). 29. Haibe-Kains, B. et al. Inconsistency in large pharmacogenomic studies. Nature 504, 389 393 (2013). 30. Sa fikhani, Z. et al. Revisiting inconsistency in large pharmacogenomic studies. F1000Res. 5, 2333 (2016). 31. Hatzis, C. et al. Enhancing reproducibility in cancer drug screening: how do we move forward? Cancer Res. https://doi.org/10.1158/0008-5472.CAN-14- 0725 (2014). 32. Sandhu, V. et al. Meta-analysis of 1 200 transcriptomic pro files identi fies a prognostic model for pancreatic ductal adenocarcinoma., JCO Clin. Cancer Inform. 355602. https://doi.org/10.1101/355602 (2019).33. Razick, S. et al. The eGenVar data management system cataloguing and sharing sensitive data and metadata for the life sciences., Database 2014 , bau027 (2014). 34. Hu, R., Yan, Z., Ding, W. & Yang, L. T. A survey on data provenance in IoT. World Wide Web J. Biol. https://doi.org/10.1007/s11280-019-00746-1 (2019). 3 5 . S i v a r a j a h ,U . ,K a m a l ,M .M . I r a n i Z .&W e e r a k k o d y , V .C r i t i c a la n a l y s i s of big data challenges and analytical methods., J. Bus. Res. 70, 263 286 (2017). 36. Corpas, M., Kovalevskaya, N. V., McMurray, A. & Nielsen, F. G. G. A FAIR guide for data providers to maximise sharing of human genomic data. PLoS Comput. Biol. 14, e1005873 (2018). 37. Wise, J. et al. Implementation and relevance of FAIR data principles in biopharmaceutical R&D. Drug Discov. Today 24, 933 938 (2019). 38. Boeckhout, M., Zielhuis, G. A. & Bredenoord, A. L. The FAIR guiding principles for data stewardship: fair enough? Eur. J. Hum. Genet. 26, 931 936 (2018). 39. Kanduri C.,  Domanska,  D.,  Hovig,  E. & Sandve, G. K. Genome build information is an essential part of genomic track files.Genome Biol., 18, 175 (2017). 40. Barretina, J. et al. The Cancer Cell Line Encyclopedia enables predictive modelling of anticancer drug sensitivity. Nature 483, 603 607 (2012). 41. Ghandi, M. et al. Next-generation characterization of the Cancer Cell Line Encyclopedia. Nature 569, 503 508 (2019). 42. Haverty, P. M. et al. Reproducible pharmacogenomic pro filing of cancer cell line panels. Nature 533, 333 337 (2016). 43. Gao, H. et al. High-throughput screening using patient-derived tumor xenografts to predict clinical trial drug response., Nat. Med. 21, 1318 1325 (2015). 44. Hafner, M. et al. Quanti fication of sensitivity and resistance of breast cancer cell lines to anti-cancer drugs using GR metrics., Sci. Data 4, 170166 (2017). 45. Daemen, A. et al. Modeling precision treatment of breast cancer. Genome Biol. 14, R110 (2013). 46. Heiser, L. M. et al. Subtype and pathway speci fic responses to anticancer compounds in breast cancer. Proc. Natl Acad. Sci. USA 109, 2724 2729 (2012). 47. Tsherniak, A. et al. De fining a cancer dependency map. Cell170, 564 576.e16 (2017). 48. Igarashi, Y. et al. Open TG-GATEs: a large-scale toxicogenomics database. Nucleic Acids Res. 43, D921 D927 (2015). 49. Rhodes, D. R. et al. Oncomine 3.0 genes,  pathways,  and networks in a collection of 18, 000 cancer gene expression pro files.Neoplasia 9,  166 180 (2007)., 50. Ramos, M. et al. Software for the integration of multiomics experiments in bioconductor. Cancer Res. 77, e39 e42 (2017). 51. Ganzfried, B. F. et al. curatedOvarianData: clinically annotated data for the ovarian cancer transcriptome. Database 2013 , bat013 (2013). Acknowledgements This project is supported by the Canadian Institutes of Health Research (CIHR) under the frame of ERA PerMed., The implementation of the ORCESTRA platform has beenpartially supported by Genome Canada and Ontario Genomics via a Bioinformatics and Computational Biology (B/CB) grant and the Natural Sciences and Engineering Research Council of Canada Natural Sciences and Engineering Research Council of Canada., Wethank Helia Mohammadi (Microsoft Canada) Stephanie Chicoine (Microsoft Canada), Nicole Mumford (AirGate) and Michael Masters (Pachyderm) for helping setting up the Pachyderm on Microsoft Azure and optimizing the cloud services., Author contributions A.M. created the Pachyderm pipelines deployed the Azure Kubernetes environment, and wrote the manuscript., P.S. contributed to the Pachyderm pipelines and code for pro- cessing gCSI and GDSC data. M.N. contributed the front and back-end of the web application and the manuscript writing. Z.S. provided the code for processing the GRAYand gCSI data objects. C.E. assisted with the web-application front-end updated Rpackages used by the platform for compatibility, and contributed to the interpretation of the results (data objects)., H.S. assisted with incorporating the mutation pro files for the data objects. S.K.N., A.S.M., and I.S. contributed to the interpretation of the results (dataobjects). C.H. and G.B. contributed to the front-end of the web application. R.K. and M.S.provided guidance for ensuring the platform meets FAIR data-sharing principles. E.L., Y.Y., S.M., and M.H. contributed data for the gCSI data object. B.H.-K. designed and supervised the work. Competing interests B.H.K. is a shareholder and paid consultant for Code Ocean Inc. Code Ocean Inc. did notparticipate in the design and the execution of the study., All remaining authors declare nocompeting interests.NATURE COMMUNICATIONS | https //doi.org/10.1038/s41467-021-25974-w ARTICLE NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications 9 Additional information Supplementary information The online version contains supplementary material available at https, //doi.org/10.1038/s41467-021-25974-w ., Correspondence and requests for materials should be addressed to Benjamin Haibe- Kains. Peer review information Nature Communications thanks Chancey Christenson Sarah Harris and the other, anonymous,  reviewer(s) for their contribution to the peer review of this work., Reprints and permission information is available at http //www.nature.com/reprints Publisher s note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional af filiations.Open Access This article is licensed under a Creative Commons Attribution 4.0 International License, which permits use,  sharing,  adaptation,  distribution and reproduction in any medium or format, as long as you giveappropriate credit to the original author(s) and the source, provide a link to the CreativeCommons license,  and indicate if changes were made., The images or other third partymaterial in this article are included in the article s Creative Commons license, unless indicated otherwise in a credit line to the material., If material is not included in the article s Creative Commons license and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly fromthe copyright holder., To view a copy of this license, visit http://creativecommons.org/ licenses/by/4.0/ . The Author(s) 2021 Massive Analysis Quality Control (MAQC) Society Board of Directors Thakkar Shraddha8, Rebecca Kusko3,  Susanna-Assunta Sansone9,  Weida Tong8,  Russ D. Wol finger10, Christopher E. Mason11,  Wendell Jones12, Joaquin Dopazo13& Cesare Furlanello14 8National Center for Toxicological Research, US Food and Drug Administration,  Jefferson,  AR,  USA.9Engineering Science Department, Oxford e-Research Centre,  University of Oxford,  Oxford,  UK.10SAS Institute Inc,  Cary,  NC, USA.11Weill Cornell Medicine,  New York,  NY,  USA.12Q2 Solutions,  Morrisville,  NC, USA.13Hospital Virgen del Rocio,  Sevilla,  Spain.14Fondazione Bruno Kessler,  Trento, Italy.ARTICLE NATURE COMMUNICATIONS | https, //doi.org/10.1038/s41467-021-25974-w 10 NATURE COMMUNICATIONS | (2021) 12, 5797 | https, //doi.org/10.1038/s41467-021-25974-w | www.nature.com/naturecommunications,",
  "qa_pairs": [
    {
      "question": "What is the main objective of the research in this paper?",
      "answer": "The main objective of the research in this paper is to develop and implement a cloud-based platform called ORCESTRA, which facilitates the transparent and reproducible processing and sharing of large multimodal biomedical data. The platform aims to address the challenges associated with the complexity and growth of biomedical data by providing a flexible framework that ensures data is processed in a manner that adheres to the FAIR principles (findable, accessible, interoperable, and reusable). ORCESTRA allows for the creation of integrated, fully documented data objects with persistent identifiers, enabling researchers to share and utilize these data objects for future studies while promoting reproducibility and transparency in biomedical research."
    },
    {
      "question": "Can you summarize the abstract of the paper?",
      "answer": "The abstract of the paper discusses the importance of reproducibility in scientific research, particularly in the context of biomedical data, which is becoming increasingly complex and voluminous. The authors highlight the challenges faced in processing, analyzing, and sharing this data in a manner that adheres to the FAIR principles (findable, accessible, interoperable, and reusable). To address these challenges, they introduce ORCESTRA, a cloud-based platform designed for the reproducible processing of multimodal biomedical data. ORCESTRA allows for automated, customizable processing pipelines for clinical genomic and perturbation profiles of cancer samples, resulting in integrated, fully documented data objects that can be shared and versioned for future studies. The platform aims to enhance transparency and reproducibility in biomedical research by providing a flexible framework for data orchestration and sharing."
    },
    {
      "question": "What are the softwares and computational tools that were used in this paper?",
      "answer": "The research paper describes the use of several software and computational tools, primarily focusing on the ORCESTRA platform, which is built on top of the Pachyderm orchestration tool. Here are the key software and tools mentioned:\n\n1. **ORCESTRA**: A cloud-based platform developed for processing and sharing large multimodal biomedical data. It utilizes Pachyderm for data orchestration.\n\n2. **Pachyderm**: An open-source orchestration tool that supports multi-stage, language-agnostic data-processing pipelines. It ensures reproducibility and data provenance through Kubernetes.\n\n3. **Kubernetes**: A container orchestration system used to manage the deployment of the Pachyderm platform on Microsoft Azure.\n\n4. **Docker**: Used for creating and managing containerized applications within the Pachyderm pipelines.\n\n5. **MongoDB**: A NoSQL database used for storing metadata and managing data object requests within the ORCESTRA platform.\n\n6. **R Bioconductor Packages**: Various open-source R packages are utilized within the Pachyderm workflows for data processing, including:\n   - **PharmacoGx**: For pharmacogenomics data analysis.\n   - **ToxicoGx**: For toxicogenomics data analysis.\n   - **Xeva**: For xenographic pharmacogenomics data.\n   - **MetaGxPancreas**: For clinical data analysis.\n   - **RadioGx**: For radiogenomics data analysis.\n\n7. **Zenodo**: An online data-sharing repository where generated data objects are uploaded and assigned a Digital Object Identifier (DOI) for persistent access.\n\n8. **Snakemake**: A workflow management system mentioned as part of the data processing pipelines for RNA-seq data.\n\n9. **CodeOcean**: A platform mentioned for hosting a custom compute capsule that allows for the reproducibility of analyses performed using the data objects.\n\nThese tools collectively support the goals of reproducibility, transparency, and efficient data management in biomedical research as outlined in the paper."
    },
    {
      "question": "Describe the methodology used in the paper.",
      "answer": "The methodology used in the paper revolves around the development and implementation of a cloud-based platform called ORCESTRA, designed to facilitate the transparent and reproducible processing and sharing of large multimodal biomedical data. The key components of the methodology are as follows:\n\n1. **Platform Architecture**: ORCESTRA is structured into three distinct layers:\n   - **Web Application Layer**: Developed using Node.js and React, this layer allows users to interact with the platform, search for existing data objects, request new data objects, and manage their accounts.\n   - **Data-Processing Layer**: This layer utilizes a Kubernetes cluster on Microsoft Azure, hosting Pachyderm, which manages data-processing pipelines. It employs Docker images to run various R packages for data processing. The pipelines are designed to aggregate data from different sources and allow for user-customizable processing.\n   - **Data-Sharing Layer**: Generated data objects are automatically uploaded to Zenodo, where they receive a Digital Object Identifier (DOI) for persistent identification. This layer also includes the generation of BioCompute Objects to communicate bioinformatics pipelines.\n\n2. **Data Processing and Versioning**: The platform allows for the creation of custom data objects that integrate various types of molecular and clinical data. Each data object is tracked through unique identifiers, ensuring complete transparency and reproducibility. The use of Pachyderm enables automatic versioning of data and pipelines, preventing unnecessary recomputation when only metadata changes.\n\n3. **User Interaction and Customization**: Users can request customized data objects by specifying parameters through the web application. The platform processes these requests by triggering the appropriate Pachyderm pipelines, which are designed to handle various types of biomedical data, including pharmacogenomics, toxicogenomics, and clinical genomics.\n\n4. **Metadata and Provenance**: ORCESTRA emphasizes the importance of rich metadata to ensure data findability, accessibility, interoperability, and reusability (FAIR principles). Each data object includes detailed metadata about its origin, processing steps, and associated publications, which are automatically communicated to users.\n\n5. **Security Measures**: The platform implements security protocols to prevent unauthorized access to computational resources and data. Role-based access control (RBAC) is utilized to manage permissions within the Kubernetes cluster.\n\n6. **Community Engagement and Open Access**: The platform is designed to be open-source, encouraging community involvement. Users can run local instances of ORCESTRA and contribute to the development of pipelines and data processing workflows.\n\nOverall, the methodology combines advanced computational tools, cloud infrastructure, and user-friendly interfaces to address the challenges of reproducibility and transparency in biomedical research."
    },
    {
      "question": "What are the key findings of the paper?",
      "answer": "The key findings of the paper \"Orchestrating and sharing large multimodal data for transparent and reproducible research\" are as follows:\n\n1. **Need for Reproducibility in Biomedical Research**: The paper emphasizes the importance of reproducibility in scientific research, particularly in the biomedical field, where the complexity and volume of data can hinder the ability to replicate findings.\n\n2. **Development of ORCESTRA**: The authors introduced a cloud-based platform named ORCESTRA, designed to facilitate the reproducible processing and sharing of large multimodal biomedical data. This platform allows for automated, customizable processing pipelines for clinical genomic and perturbation profiles of cancer samples.\n\n3. **Integration of FAIR Principles**: ORCESTRA adheres to the FAIR principles (Findable, Accessible, Interoperable, and Reusable) for data management, ensuring that data is well-documented, versioned, and can be easily shared among researchers.\n\n4. **Data Provenance and Versioning**: The platform creates integrated data objects that are fully documented and assigned persistent identifiers (DOIs), allowing for effective tracking of data provenance and changes across different versions of datasets.\n\n5. **User-Friendly Interface**: ORCESTRA provides a web application interface that enables users to search for existing data objects, request new data objects, and track the status of their requests, enhancing accessibility and usability.\n\n6. **Support for Diverse Data Types**: The platform supports a wide range of biomedical data types, including pharmacogenomics, toxicogenomics, radiogenomics, and clinical genomics, facilitating integrative analyses across these domains.\n\n7. **Community Engagement and Open Access**: The authors highlight the importance of community involvement in the platform's development and the commitment to open access, allowing researchers to utilize and build upon the data and tools provided by ORCESTRA.\n\n8. **Challenges in Current Data Sharing Practices**: The paper discusses existing limitations in data sharing practices, such as inadequate metadata, lack of documentation for processing pipelines, and the challenges posed by the evolving nature of datasets.\n\n9. **Future Directions**: The authors propose plans for further enhancing the platform, including automating data uploads, expanding the types of datasets available, and implementing metrics to track the impact of the data objects on future research.\n\nOverall, the paper presents ORCESTRA as a significant advancement in the management and sharing of biomedical data, addressing critical issues related to reproducibility, transparency, and accessibility in research."
    },
    {
      "question": "How was the data analyzed in the study?",
      "answer": "The data in the study was analyzed using a cloud-based platform called ORCESTRA, which orchestrates data-processing pipelines to curate customized, versioned, and fully documented data objects. The platform employs Pachyderm, an open-source orchestration tool, to manage multi-stage, language-agnostic data-processing pipelines. This setup ensures complete reproducibility and data provenance through the use of unique identifiers for each commit and data object.\n\nThe analysis involved several key components:\n1. **Data Processing Pipelines**: ORCESTRA allows users to create pipelines that can process various types of biomedical data, including pharmacogenomics, toxicogenomics, and clinical genomics. Users can select specific parameters for data processing, which triggers the respective pipelines.\n\n2. **Version Control and Provenance**: Each data object generated is tracked with unique identifiers, ensuring that users can follow the lineage of the data and understand how it was processed. This includes automatic versioning when data is modified or newly added.\n\n3. **Integration of Multiple Data Types**: The platform integrates diverse data types and allows for the combination of molecular profiles with clinical data, enabling comprehensive analyses.\n\n4. **Metadata and Documentation**: Each data object is accompanied by rich metadata, including details about the data sources, processing parameters, and links to relevant publications. This transparency is crucial for reproducibility.\n\n5. **Automated Sharing and DOI Assignment**: Once a data object is generated, it is automatically uploaded to Zenodo, where it receives a Digital Object Identifier (DOI) for persistent access and citation.\n\n6. **User Interaction**: The platform features a web application that allows users to search for existing data objects, request new data objects, and track the status of their requests.\n\nOverall, the analysis was designed to be transparent, reproducible, and user-friendly, addressing the challenges associated with handling large volumes of multimodal biomedical data."
    },
    {
      "question": "Was the data in the study pre-processed in anyway? If so how?",
      "answer": "Yes, the data in the study was pre-processed. The pre-processing involved several steps and methodologies:\n\n1. **Automated Processing Pipelines**: The study utilized a cloud-based platform called ORCESTRA, which allows for the automated processing of clinical genomic and perturbation profiles of cancer samples through customizable pipelines.\n\n2. **Specific Tools for RNA-seq Data**: For RNA-seq data, the study mentioned that the raw data were pre-processed using Kallisto and Salmon Snakemake pipelines within a high-performance computing (HPC) environment. This indicates that specific algorithms and tools were employed to process the RNA-seq data before it was pushed to the data repositories.\n\n3. **Integration of Data Types**: The platform integrates various data types, including gene expression, copy number variation, mutation, and fusion molecular data, which suggests that different pre-processing steps were applied depending on the data type.\n\n4. **Version Control and Provenance**: The ORCESTRA platform ensures that each data object is versioned and tracked, allowing for reproducibility and transparency in how the data were processed.\n\n5. **User Customization**: Users can select specific parameters and tools for processing, which allows for tailored pre-processing based on the research needs.\n\nOverall, the study emphasizes the importance of pre-processing in ensuring data quality and reproducibility in biomedical research."
    },
    {
      "question": "What conclusions were drawn in the paper?",
      "answer": "The paper concludes that the ORCESTRA platform represents a significant advancement in the sharing and processing of large multimodal biomedical data, addressing the challenges of transparency and reproducibility in research. It emphasizes the importance of adhering to the FAIR (Findable, Accessible, Interoperable, and Reusable) data principles to enhance the reproducibility of scientific findings. ORCESTRA provides a flexible, cloud-based framework that allows for the automated processing of diverse biomedical data types, ensuring that data objects are fully documented and versioned with persistent identifiers (DOIs). This facilitates better data sharing practices and promotes collaboration among researchers, ultimately aiming to accelerate discoveries in the biomedical field. The authors also highlight the need for ongoing community involvement and the potential for future enhancements to the platform, including user-uploaded data processing and improved metrics for tracking the impact of shared data in research publications."
    },
    {
      "question": "Can you provide a summary of the literature review from the paper?",
      "answer": "The literature review in the paper discusses the growing demand for large volumes of multimodal biomedical data, driven by advancements in personalized medicine and the need for deeper understanding of diseases. It highlights the challenges of reproducibility in research due to complex data-handling methods and inadequate sharing practices, which raise questions about the validity and relevance of studies. The review emphasizes the importance of sharing both raw and processed data to enhance reproducibility, aligning with the FAIR principles (Findable, Accessible, Interoperable, Reusable).\n\nThe authors note that while there have been community efforts to standardize workflow languages and management systems to promote reproducibility, many existing solutions have steep learning curves or limited features. They point out that the lack of comprehensive metadata and documentation often hampers data provenance and transparency, which are critical for reproducibility.\n\nThe review also discusses various types of biomedical data, including pharmacogenomics, toxicogenomics, radiogenomics, and clinical genomics, and the historical context of data sharing in biomarker discovery. It underscores the necessity for better data sharing practices that are standardized and processed transparently to facilitate reproducibility.\n\nFurthermore, the authors mention the establishment of the Massive Analysis and Quality Control (MAQC) Society to promote community-agreed standards for sharing multimodal biomedical data. They argue that adherence to the FAIR principles is essential for improving the reproducibility and transparency of data handling and sharing, ultimately enabling researchers to build upon each other's work and accelerate discoveries in the biomedical field."
    },
    {
      "question": "What future research directions do the authors suggest in the paper?",
      "answer": "The authors of the paper suggest several future research directions, which include:\n\n1. **Regular Updates to the Platform**: They plan to regularly update the ORCESTRA platform with additional datasets and data types to enhance its utility and relevance in the research community.\n\n2. **Automation of Data Uploads**: There is an intention to automate the process by which users can upload their own data for processing through standardized pipelines, which would reduce the need for human intervention in generating data objects.\n\n3. **Community Involvement**: The authors hope to encourage more community involvement by allowing users to run local instances of the ORCESTRA platform. This would enable researchers to process their own data in a reproducible and transparent manner using built-in pipelines.\n\n4. **Implementation of a Metrics System**: They aim to implement a metrics system to track the usage of data objects in future publications. This would help demonstrate the impact of the ORCESTRA platform within the research community.\n\nThese directions reflect a commitment to enhancing the platform's capabilities and fostering a collaborative research environment that prioritizes transparency and reproducibility."
    },
    {
      "question": "What statistical techniques were used in the paper?",
      "answer": "The paper discusses the development of the ORCESTRA platform for processing and sharing large multimodal biomedical data. While it does not explicitly detail specific statistical techniques used in the analysis, it mentions the utilization of various computational tools and workflows that are commonly associated with statistical analysis in biomedical research.\n\nKey points related to statistical techniques include:\n\n1. **Data Processing Pipelines**: The platform employs automated processing pipelines that are customizable by users. These pipelines likely incorporate statistical methods for data analysis, although specific techniques are not listed.\n\n2. **Integration of Omics Technologies**: The paper references the use of omics technologies (e.g., pharmacogenomics, toxicogenomics, radiogenomics, clinical genomics) which typically involve statistical analyses such as differential expression analysis, correlation studies, and regression modeling.\n\n3. **Use of R Bioconductor Packages**: The platform utilizes various open-source R Bioconductor packages for creating data objects, which often include statistical methods for analyzing high-dimensional data.\n\n4. **BioCompute Objects**: The generation of BioCompute Objects includes standardized records that may involve statistical validation of bioinformatics pipelines, although specific statistical tests or models are not mentioned.\n\n5. **Data Versioning and Provenance Tracking**: The emphasis on data versioning and tracking suggests that statistical methods for assessing data integrity and reproducibility are integral to the platform's functionality.\n\nIn summary, while the paper does not specify particular statistical techniques, it implies the use of a range of statistical methods through the integration of computational tools, workflows, and R packages commonly used in biomedical data analysis."
    },
    {
      "question": "Can you describe the experimental setup in the paper?",
      "answer": "The experimental setup described in the paper revolves around the development and utilization of the ORCESTRA platform, a cloud-based framework designed for the reproducible processing and sharing of large multimodal biomedical data. Here are the key components of the experimental setup:\n\n1. **Platform Architecture**: ORCESTRA is structured into three distinct layers:\n   - **Web Application Layer**: Developed using Node.js and React, this layer allows users to interact with the platform, select data types, search for existing data objects, and request new data objects by entering specific pipeline parameters.\n   - **Data-Processing Layer**: This layer operates on a Kubernetes cluster hosted on Microsoft Azure, utilizing Pachyderm for managing data-processing pipelines. It employs Docker images to run various R packages for data analysis. The pipelines are designed to handle different types of biomedical data, including RNA-seq, microarray, copy number variation, mutation, and fusion data.\n   - **Data-Sharing Layer**: Generated data objects are automatically uploaded to Zenodo, where they receive a Digital Object Identifier (DOI) for persistent identification. This layer also includes the generation of BioCompute Objects to communicate bioinformatics pipelines and ensure reproducibility.\n\n2. **Data Processing Pipelines**: The platform supports the creation of customizable, language-agnostic data-processing pipelines that can handle large datasets. Users can select specific processing tools and versions, and the system tracks all inputs and outputs, ensuring complete transparency and reproducibility.\n\n3. **Data Object Generation**: ORCESTRA allows users to create custom data objects that integrate molecular profiles, perturbation profiles, and experimental metadata. Each data object is versioned and tracked with unique identifiers, facilitating reproducibility.\n\n4. **User Interaction**: Users can request specific data objects through the web application by selecting parameters related to drug sensitivity, genome references, and processing tools. The system prevents unnecessary recomputation of data, optimizing resource usage.\n\n5. **Security Measures**: The platform implements security protocols to prevent unauthorized access to computational resources and data. Role-based access control (RBAC) is enforced within the Kubernetes cluster to manage access to data and processing capabilities.\n\n6. **Cost Management**: ORCESTRA is designed to control operational costs by allowing users to turn off computational resources when not in use, ensuring that resources are only allocated when data processing is actively taking place.\n\nIn summary, the experimental setup leverages a sophisticated cloud-based architecture that integrates data processing, sharing, and security, enabling researchers to handle large volumes of multimodal biomedical data in a reproducible and transparent manner."
    },
    {
      "question": "What are the implications of the research findings?",
      "answer": "The research findings presented in the paper have several significant implications for the field of biomedical research, particularly in relation to data sharing, reproducibility, and the overall integrity of scientific findings. Here are the key implications:\n\n1. **Enhanced Reproducibility**: The development of the ORCESTRA platform addresses the critical need for reproducibility in biomedical research. By providing a cloud-based framework that allows for the transparent processing and sharing of multimodal data, researchers can replicate experiments more easily, thereby increasing the reliability of scientific findings.\n\n2. **Improved Data Sharing Practices**: The findings highlight the inadequacies of current data sharing practices, particularly regarding the size and complexity of biomedical datasets. ORCESTRA facilitates better sharing of both raw and processed data, adhering to the FAIR principles (Findable, Accessible, Interoperable, and Reusable). This promotes a culture of openness and collaboration in the research community.\n\n3. **Standardization of Data Processing**: The platform allows for the creation of standardized workflows for data processing, which is crucial given the diverse types of biomedical data. This standardization helps ensure that data is processed consistently, reducing variability that can arise from different processing methods.\n\n4. **Transparency and Provenance**: By automatically documenting the data processing steps and providing unique identifiers for datasets, ORCESTRA enhances data provenance. This transparency allows researchers to trace the origins and modifications of datasets, which is essential for validating results and ensuring data integrity.\n\n5. **Facilitation of Personalized Medicine**: The ability to integrate and analyze large volumes of multimodal data can significantly advance research in personalized medicine. By making it easier to access and analyze diverse datasets, researchers can uncover insights that may lead to more tailored and effective treatments for patients.\n\n6. **Community Engagement and Collaboration**: The open-source nature of ORCESTRA encourages community involvement, allowing researchers to contribute to and benefit from shared resources. This collaborative approach can accelerate discoveries and innovations in the biomedical field.\n\n7. **Addressing Challenges of Big Data**: The findings underscore the challenges posed by the increasing volume and complexity of biomedical data. ORCESTRA's orchestration of data processing pipelines helps manage these challenges, making it feasible for researchers to handle large datasets without requiring extensive computational expertise.\n\n8. **Future Research Directions**: The platform sets a precedent for future developments in data sharing and processing in biomedical research. It opens avenues for further enhancements, such as automating data uploads and integrating additional datasets, which can continuously improve the research landscape.\n\nIn summary, the research findings suggest that the ORCESTRA platform represents a significant advancement in the management and sharing of biomedical data, with broad implications for reproducibility, transparency, and collaboration in scientific research."
    },
    {
      "question": "What are the limitations and delimitations mentioned in the paper?",
      "answer": "The paper discusses several limitations and delimitations regarding the orchestration and sharing of large multimodal biomedical data for transparent and reproducible research. Here are the key points:\n\n### Limitations:\n1. **Complexity of Data Processing**: The intrinsic complexity and continuous growth of biomedical data make it increasingly challenging to process, analyze, and share data in a FAIR (findable, accessible, interoperable, and reusable) manner.\n2. **Inadequate Sharing Practices**: Current data-sharing practices are insufficient to handle the size of data and the corresponding infrastructure requirements for transfer and storage.\n3. **Steep Learning Curve**: Existing programming-heavy solutions for data processing, such as CWL and WDL, present a steep learning curve compared to user-friendly platforms, which may limit accessibility for some researchers.\n4. **Lack of Documentation**: Many genomic data maintainers do not provide sufficient documentation regarding the choice of processing pipelines, which can impact the reproducibility and transparency of the datasets.\n5. **Single Version Release**: Often, only a single version of a dataset is released, making it difficult for researchers to perform diverse analyses that require different processing pipelines.\n6. **Resource Allocation Costs**: The requirement for persistent RAM/CPU allocation for each pipeline in the Kubernetes cluster can lead to high costs, impacting cost-efficiency.\n\n### Delimitations:\n1. **Focus on Specific Data Types**: The ORCESTRA platform is designed to handle specific types of multimodal biomedical data, including pharmacogenomics, toxicogenomics, xenographic pharmacogenomics, and clinical genomics, which may limit its applicability to other types of data.\n2. **Use of Pachyderm**: The choice of Pachyderm as the orchestration tool, while beneficial for reproducibility and provenance, also comes with certain limitations, such as the lack of direct mounting of data from cloud storage systems and the need for manual data transfer.\n3. **Public Accessibility**: While ORCESTRA is publicly accessible, the platform's functionalities may be limited by the need for users to have a certain level of technical expertise to navigate and utilize the system effectively.\n\nThese limitations and delimitations highlight the challenges faced in the field of biomedical data sharing and the specific focus of the ORCESTRA platform in addressing these challenges while acknowledging its constraints."
    },
    {
      "question": "What recommendations do the authors make in the paper?",
      "answer": "The authors of the paper make several recommendations aimed at improving the reproducibility and transparency of biomedical research involving large multimodal data. Here are the key recommendations:\n\n1. **Adoption of FAIR Principles**: The authors emphasize the importance of adhering to the FAIR principles (Findable, Accessible, Interoperable, and Reusable) for data sharing. They recommend that researchers ensure their data is accompanied by rich metadata that describes the dataset's origin, generation methods, and any modifications made over time.\n\n2. **Enhanced Documentation**: They suggest that data maintainers and repositories should provide comprehensive documentation regarding the processing pipelines used for data generation. This includes justifications for the choice of specific pipelines and the sharing of relevant source code.\n\n3. **Use of Standardized Workflows**: The authors advocate for the use of standardized workflow languages and management systems (like CWL and WDL) to promote reproducibility, while also acknowledging the need for user-friendly platforms that can accommodate researchers with varying levels of programming expertise.\n\n4. **Development of ORCESTRA**: The authors recommend the use of their developed platform, ORCESTRA, which provides a cloud-based solution for processing and sharing multimodal biomedical data. They highlight its capabilities for creating versioned, fully documented data objects that can be easily shared and accessed.\n\n5. **Community Engagement**: They encourage community involvement in the development and use of open-source platforms like ORCESTRA, allowing researchers to run local instances and contribute to the platform's growth.\n\n6. **Metrics for Impact Assessment**: The authors propose implementing a metrics system to track the usage of data objects in future publications, which would help demonstrate the platform's impact on the research community.\n\n7. **Automation of Data Processing**: They recommend automating the data processing requests to minimize human intervention, thereby enhancing efficiency and reproducibility.\n\nThese recommendations aim to address the challenges associated with the complexity and scale of biomedical data, ultimately fostering a more transparent and reproducible research environment."
    },
    {
      "question": "Who funded the research in the paper?",
      "answer": "The research in the paper was funded by the Canadian Institutes of Health Research (CIHR) under the frame of ERA PerMed. Additionally, the implementation of the ORCESTRA platform was partially supported by Genome Canada and Ontario Genomics via a Bioinformatics and Computational Biology (B/CB) grant, as well as the Natural Sciences and Engineering Research Council of Canada."
    },
    {
      "question": "Is there any conflict of interest disclosed in the paper?",
      "answer": "Yes, there is a conflict of interest disclosed in the paper. Specifically, the author Benjamin Haibe-Kains is noted to be a shareholder and paid consultant for Code Ocean Inc. However, it is stated that Code Ocean Inc. did not participate in the design and execution of the study. All remaining authors declare no competing interests."
    },
    {
      "question": "What ethical considerations are discussed in the paper?",
      "answer": "The paper discusses several ethical considerations related to the sharing and processing of large multimodal biomedical data, particularly in the context of reproducibility and transparency in research. Here are the key ethical considerations highlighted:\n\n1. **Reproducibility and Transparency**: The paper emphasizes the ethical obligation of researchers to ensure that their findings can be reproduced by independent groups. This involves providing detailed descriptions of experiments and methodologies to allow for scrutiny and validation of results.\n\n2. **Data Sharing Practices**: Ethical concerns arise from inadequate data sharing practices, which can hinder reproducibility. The authors argue that sharing raw and processed data is crucial for other researchers to verify results and ensure the integrity of scientific findings.\n\n3. **FAIR Principles**: The paper advocates for adherence to the FAIR (Findable, Accessible, Interoperable, and Reusable) principles in data management. This includes ethical considerations around providing rich metadata, ensuring data accessibility with appropriate licenses, and maintaining data provenance to enhance trust in shared data.\n\n4. **Data Provenance and Documentation**: The lack of proper documentation and justification for data processing choices can lead to ethical issues regarding data integrity. The authors stress the importance of documenting workflows and providing evidence for the selection of specific data processing pipelines.\n\n5. **Access Control and Privacy**: The paper discusses the need for security measures to prevent unauthorized access to sensitive data, highlighting ethical responsibilities in protecting participant privacy and ensuring that data is used appropriately.\n\n6. **Community Standards**: The establishment of community-agreed standards for data sharing and processing is presented as an ethical imperative to promote reproducibility and transparency in biomedical research.\n\n7. **Equity in Data Access**: The authors touch on the ethical implications of ensuring equitable access to data and resources, which is essential for fostering collaboration and advancing scientific knowledge across diverse research communities.\n\nOverall, the paper underscores the ethical dimensions of data management in biomedical research, advocating for practices that enhance reproducibility, transparency, and responsible sharing of data."
    },
    {
      "question": "Which studies are most frequently cited in the paper?",
      "answer": "The most frequently cited studies in the paper are as follows:\n\n1. **Madduri, R. et al. (2019)** - \"Reproducible big data science: a case study in continuous FAIRness.\" PLoS ONE 14, e0213013.\n2. **Kanwal, S. et al. (2017)** - \"Investigating reproducibility and tracking provenance a genomic workflow case study.\" BMC Bioinformatics 18, 337.\n3. **Toga, A. W. & Dinov, I. D. (2015)** - \"Sharing big biomedical data.\" J. Big Data 2, 112.\n4. **Huang, Y. & Gottardo, R. (2013)** - \"Comparability and reproducibility of biomedical data.\" Brief. Bioinform. 14, 391-401.\n5. **Patil, P. et al. (2019)** - \"A visual tool for defining reproducibility and replicability.\" Nat. Hum. Behav. 3, 650-652.\n6. **Finak, G. et al. (2018)** - \"DataPackageR reproducible data preprocessing, standardization and sharing using R/Bioconductor for collaborative data analysis.\" Gates Open Res. 2, 31.\n7. **Suthakar, U. et al. (2016)** - \"An efficient strategy for the collection and storage of large volumes of data for computation.\" J. Big Data 3, 21.\n8. **Koster, J. & Rahmann, S. (2012)** - \"Snakemake a scalable bioinformatics workflow engine.\" Bioinformatics 28, 2520-2522.\n9. **Di Tommaso, P. et al. (2017)** - \"Nextflow enables reproducible computational workflows.\" Nat. Biotechnol. 35, 316-319.\n10. **Goble, C. et al. (2020)** - \"FAIR computational workflows.\" Data Intell. 2, 108-121.\n\nThese references highlight key studies related to reproducibility, data sharing, and workflow management in biomedical research, which are central themes in the paper."
    },
    {
      "question": "Can you explain the technical terms used in the paper?",
      "answer": "Certainly! Here are explanations of some of the technical terms used in the paper:\n\n1. **Reproducibility**: The ability to obtain consistent results using the same methods and data as a previous study. It is a fundamental principle of scientific research, ensuring that findings can be verified by others.\n\n2. **FAIR Principles**: An acronym for Findable, Accessible, Interoperable, and Reusable. These principles guide the management of data to enhance its usability and sharing among researchers.\n\n3. **Multimodal Biomedical Data**: Data that comes from various sources and types, such as genomic, clinical, and pharmacological data, which can be used together to gain insights into health and disease.\n\n4. **Cloud-based Platform**: A service that provides computing resources over the internet, allowing users to access and process data without needing local infrastructure.\n\n5. **ORCESTRA**: The name of the cloud-based platform developed in the study for orchestrating and sharing large multimodal biomedical data.\n\n6. **Data Provenance**: The documentation of the origins and history of data, including how it was created, processed, and modified. This is crucial for ensuring data integrity and reproducibility.\n\n7. **Persistent Identifiers (DOI)**: Digital Object Identifiers are unique alphanumeric strings assigned to a document or dataset, providing a permanent link to its location on the internet.\n\n8. **Automated Processing Pipelines**: Predefined sequences of data processing steps that can be executed automatically to analyze data, reducing the need for manual intervention.\n\n9. **Kubernetes**: An open-source platform for managing containerized applications across a cluster of machines, providing tools for deploying, scaling, and managing applications.\n\n10. **Docker**: A platform that allows developers to automate the deployment of applications inside lightweight containers, which can run consistently across different computing environments.\n\n11. **BioCompute Object**: A standardized record that describes bioinformatics workflows, including the steps taken, data inputs and outputs, and software used, aimed at enhancing reproducibility.\n\n12. **Versioning**: The process of assigning unique identifiers to different iterations of data or software, allowing users to track changes and access previous versions.\n\n13. **Metadata**: Data that provides information about other data, such as its origin, structure, and context, which is essential for understanding and using datasets effectively.\n\n14. **Pharmacogenomics**: The study of how genes affect a person's response to drugs, which can help tailor personalized medicine approaches.\n\n15. **Toxicogenomics**: The study of the effects of toxic substances on gene expression and function, often used to assess the safety of chemicals and drugs.\n\n16. **Radiogenomics**: The field that combines genomic data with radiological imaging data to understand the relationship between genetics and responses to radiation therapy.\n\n17. **Data Sharing**: The practice of making data available to other researchers or the public, which is essential for transparency and collaboration in scientific research.\n\nThese terms are integral to understanding the methodologies and objectives discussed in the paper, which focuses on improving data sharing and reproducibility in biomedical research."
    },
    {
      "question": "What data sources were used in the paper, and are they accessible for further research?",
      "answer": "The research paper utilized a variety of data sources, specifically mentioning the integration of datasets from the largest biomedical consortia, which include:\n\n1. **Pharmacogenomics Data**: 11 curated data objects.\n2. **Toxicogenomics Data**: 3 curated data objects.\n3. **Xenographic Pharmacogenomics Data**: 1 curated data object.\n4. **Clinical Genomics Data**: 21 studies compiled into 1 data object.\n5. **Radiogenomics Data**: 1 curated data object.\n\nThe specific datasets mentioned in the paper include:\n- GRAY dataset\n- CCLE dataset\n- CTRPv2 dataset\n- gCSI dataset\n- FIMM dataset\n- GDSC dataset\n- UHNBreast dataset\n- Open TG-GATEs dataset\n- EMEXP2458 dataset\n- DrugMatrix dataset\n- PDXE dataset\n- MetaGxPancreas dataset\n\nThese datasets are accessible for further research, as they have been deposited in various repositories with specific licenses that allow for public access. For example, the GRAY dataset is available at [Dryad](https://doi.org/10.5061/dryad.03n60), and the CCLE dataset can be accessed at [Broad Institute](https://data.broadinstitute.org/ccle_legacy_data). Additionally, all data objects generated through the ORCESTRA platform are publicly available at [orcestra.ca](http://orcestra.ca), which includes dedicated documented web pages with respective digital object identifiers (DOIs) and Zenodo links for each data object. \n\nIn summary, the data sources used in the paper are accessible for further research, and the authors have ensured that the datasets are shared in a transparent manner, adhering to the FAIR principles (Findable, Accessible, Interoperable, and Reusable)."
    },
    {
      "question": "Can you summarize the research paper?",
      "answer": "The research paper titled \"Orchestrating and sharing large multimodal data for transparent and reproducible research\" discusses the challenges of reproducibility in biomedical research due to the complexity and growth of multimodal data. The authors emphasize the importance of detailed experimental descriptions to facilitate reproducibility and the need for data to be shared in a FAIR (findable, accessible, interoperable, and reusable) manner.\n\nTo address these challenges, the authors developed a cloud-based platform called ORCESTRA (orcestra.ca), which provides a flexible framework for the reproducible processing of multimodal biomedical data. ORCESTRA allows for the automated processing of clinical genomic and perturbation profiles of cancer samples, creating integrated and fully documented data objects with persistent identifiers (DOIs) that can be shared for future studies.\n\nThe paper outlines the increasing demand for large volumes of multimodal biomedical data, particularly in personalized medicine, and the inadequacies of current data-sharing practices. It highlights the need for standardized workflows and metadata to ensure data provenance and transparency. The authors describe how ORCESTRA orchestrates data-processing pipelines, manages multiple dataset versions, and provides a user-friendly interface for researchers to create and share data objects.\n\nThe platform utilizes various open-source tools and integrates with existing data repositories, ensuring that all data processing is transparent and reproducible. The authors also discuss the architecture of ORCESTRA, which consists of a web application layer, a data-processing layer using Kubernetes and Pachyderm, and a data-sharing layer that uploads generated data objects to Zenodo.\n\nIn conclusion, the ORCESTRA platform aims to enhance the reproducibility and transparency of biomedical research by providing a comprehensive solution for processing and sharing multimodal data, thereby promoting open science and facilitating further research advancements."
    },
    {
      "question": "What is the significance of this research in its field?",
      "answer": "The significance of this research lies in its contribution to enhancing reproducibility and transparency in biomedical research, particularly in the context of handling large multimodal data. The study addresses critical challenges faced by researchers in the biomedical field, such as the complexity and volume of data, which often hinder the ability to reproduce findings and validate results. \n\nBy developing the ORCESTRA platform, the authors provide a cloud-based solution that facilitates the reproducible processing and sharing of diverse biomedical data types. This platform adheres to the FAIR principles (Findable, Accessible, Interoperable, and Reusable), ensuring that data is not only shared but also accompanied by rich metadata that enhances its usability and traceability. \n\nThe research emphasizes the importance of detailed documentation and version control in data processing workflows, which are essential for maintaining data provenance and supporting the integrity of scientific findings. By integrating automated processing pipelines and providing persistent identifiers (DOIs) for data objects, ORCESTRA allows researchers to track changes and access historical versions of datasets, thereby fostering a culture of transparency and collaboration in the scientific community.\n\nOverall, this research represents a significant step towards addressing the reproducibility crisis in biomedical sciences by providing tools and frameworks that promote open science, enhance data sharing practices, and ultimately accelerate discoveries in personalized medicine and disease understanding."
    },
    {
      "question": "How does this paper contribute to the existing body of knowledge?",
      "answer": "The paper titled \"Orchestrating and sharing large multimodal data for transparent and reproducible research\" contributes to the existing body of knowledge in several significant ways:\n\n1. **Development of ORCESTRA Platform**: The authors introduce ORCESTRA, a cloud-based platform designed to facilitate the reproducible processing and sharing of large multimodal biomedical data. This platform addresses the challenges of managing complex biomedical datasets by providing a flexible and user-friendly framework that supports automated processing pipelines.\n\n2. **Enhancing Reproducibility**: The paper emphasizes the importance of reproducibility in scientific research, particularly in the biomedical field. By creating a platform that allows for the generation of fully documented data objects with persistent identifiers (DOIs), the authors enhance the ability of researchers to reproduce studies and validate findings, which is crucial for the integrity of scientific research.\n\n3. **Integration of FAIR Principles**: The authors align their platform with the FAIR (Findable, Accessible, Interoperable, and Reusable) data principles, which are essential for modern data sharing practices. By ensuring that data is well-documented and easily accessible, ORCESTRA promotes better data management and sharing practices within the research community.\n\n4. **Addressing Data Complexity**: The paper discusses the increasing complexity and volume of biomedical data, particularly in the context of personalized medicine. ORCESTRA is designed to handle various data types, including pharmacogenomics, toxicogenomics, and clinical genomics, thereby providing a comprehensive solution for researchers dealing with diverse datasets.\n\n5. **Community Engagement and Open Science**: The authors advocate for open science by making ORCESTRA publicly accessible and encouraging community involvement. The platform allows researchers to share their own data and utilize existing datasets, fostering collaboration and accelerating discoveries in biomedical research.\n\n6. **Technical Innovations**: The paper details the technical aspects of the ORCESTRA platform, including its use of Pachyderm for data orchestration, which supports versioning and reproducibility. This technical foundation contributes to the advancement of computational tools available for biomedical research.\n\n7. **Case Studies and Practical Applications**: The authors provide examples of how ORCESTRA can be utilized for various analyses, demonstrating its practical applications in real-world research scenarios. This not only showcases the platform's capabilities but also serves as a guide for other researchers looking to adopt similar methodologies.\n\nOverall, this paper significantly advances the field of biomedical research by providing a robust platform that enhances data sharing, reproducibility, and transparency, thereby addressing critical challenges faced by researchers in managing and analyzing complex datasets."
    },
    {
      "question": "Are there any novel techniques or approaches introduced in this paper?",
      "answer": "Yes, the paper introduces a novel cloud-based platform called ORCESTRA, which is designed for the orchestration and sharing of large multimodal biomedical data. This platform provides a flexible framework for the reproducible processing of clinical genomic and perturbation profiles of cancer samples through automated and customizable processing pipelines. Key features of ORCESTRA include:\n\n1. **Automated Processing Pipelines**: Users can customize pipelines for processing various types of biomedical data, which enhances reproducibility and transparency.\n\n2. **Version Control**: ORCESTRA manages multiple dataset versions and creates integrated, fully documented data objects with persistent identifiers (DOIs), facilitating future studies and data sharing.\n\n3. **Data Provenance**: The platform ensures complete data provenance by tracking the entire process of data object generation, including the input data, processing pipelines, and output data.\n\n4. **BioCompute Objects**: Each data object generated is accompanied by a BioCompute Object, which is a standardized record that helps communicate bioinformatic pipelines and verify them, aiding in reproducibility.\n\n5. **User-Friendly Interface**: ORCESTRA features a web application interface that allows users to search for existing data objects, request new data objects, and track the status of their requests.\n\n6. **Integration with Zenodo**: The platform automatically uploads generated data objects to Zenodo, providing a DOI and a custom metadata web page for each object, which enhances data sharing and accessibility.\n\nOverall, ORCESTRA represents a significant advancement in the field of biomedical data processing and sharing, addressing challenges related to reproducibility, transparency, and data management in research."
    },
    {
      "question": "What are the potential real-world applications of this research?",
      "answer": "The research presented in the paper discusses the development of the ORCESTRA platform, which is designed to facilitate the orchestration and sharing of large multimodal biomedical data. The potential real-world applications of this research include:\n\n1. **Enhanced Reproducibility in Biomedical Research**: By providing a framework for reproducible processing and sharing of multimodal data, ORCESTRA can help researchers validate findings and ensure that studies can be replicated by independent groups. This is crucial for the credibility of scientific research.\n\n2. **Support for Personalized Medicine**: The platform can be utilized to analyze clinical genomic and pharmacogenomic data, which is essential for advancing personalized medicine. Researchers can identify correlations between genetic profiles and treatment responses, ultimately improving patient care.\n\n3. **Integration of Diverse Data Types**: ORCESTRA allows for the integration of various types of biomedical data, such as pharmacogenomics, toxicogenomics, radiogenomics, and clinical genomics. This capability can facilitate comprehensive analyses that consider multiple factors affecting disease and treatment outcomes.\n\n4. **Facilitation of Collaborative Research**: By enabling the sharing of fully documented and versioned data objects, ORCESTRA promotes collaboration among researchers. This can lead to accelerated discoveries and innovations in the biomedical field.\n\n5. **Standardization of Data Sharing Practices**: The platform adheres to FAIR (Findable, Accessible, Interoperable, Reusable) principles, which can help standardize data sharing practices across the biomedical research community. This can improve data provenance and transparency.\n\n6. **Automated Data Processing Pipelines**: ORCESTRA's automated processing pipelines can streamline the analysis of large datasets, making it easier for researchers to handle complex data without requiring extensive computational expertise.\n\n7. **Regulatory Compliance and Validation**: The generation of BioCompute Objects alongside data objects can aid in regulatory compliance and validation processes, particularly in contexts where reproducibility and transparency are mandated by regulatory bodies like the FDA.\n\n8. **Educational Tool for Researchers**: The platform can serve as an educational resource for researchers learning about data processing and analysis in the context of biomedical research, providing a user-friendly interface and access to robust computational tools.\n\nOverall, the ORCESTRA platform has the potential to significantly impact the fields of biomedical research and personalized medicine by improving data management, sharing, and analysis practices."
    },
    {
      "question": "How does the paper address potential biases in the research?",
      "answer": "The paper addresses potential biases in the research primarily through the implementation of the ORCESTRA platform, which emphasizes transparency, reproducibility, and comprehensive documentation of data processing workflows. Here are the key strategies outlined in the paper to mitigate biases:\n\n1. **Detailed Documentation**: The ORCESTRA platform generates fully documented data objects that include detailed metadata about the datasets, processing pipelines, and the parameters used. This level of detail allows researchers to scrutinize and understand the data generation process, which is crucial for identifying and addressing potential biases.\n\n2. **Version Control**: By managing multiple dataset versions and providing unique identifiers (DOIs) for each version, the platform allows researchers to track changes over time. This feature helps in understanding how data modifications may introduce biases and facilitates the assessment of the impact of these changes on research outcomes.\n\n3. **Standardized Workflows**: The paper highlights the importance of using standardized workflows for data processing, which can reduce variability and biases that may arise from using different methods or tools. The platform supports the use of community-agreed standards, such as the FAIR principles, to ensure that data is handled consistently.\n\n4. **Data Provenance**: The emphasis on data provenance\u2014tracking the origin and history of data\u2014helps in identifying potential sources of bias. By providing a clear lineage of the data, researchers can better evaluate the integrity and reliability of the datasets used in their analyses.\n\n5. **Community Engagement**: The platform encourages community involvement by allowing users to share their workflows and data processing methods. This collaborative approach can help identify and mitigate biases that may not be apparent to individual researchers.\n\n6. **BioCompute Objects**: The generation of BioCompute Objects, which are standardized records for bioinformatics pipelines, aids in verifying and validating the computational processes used. This contributes to reproducibility and helps ensure that biases introduced by specific computational choices are documented and can be assessed.\n\nBy implementing these strategies, the paper aims to enhance the transparency and reproducibility of biomedical research, ultimately reducing the potential for biases that could affect the validity of research findings."
    },
    {
      "question": "What validation methods were used to ensure the reliability of the results?",
      "answer": "The research paper describes several validation methods used to ensure the reliability of the results generated by the ORCESTRA platform for processing and sharing multimodal biomedical data. These methods include:\n\n1. **Automated Pipeline Triggering**: The platform utilizes automatic pipeline triggering when existing data are modified or newly added. This ensures that any changes in the data lead to the generation of new versions of the output data object, maintaining the integrity and reproducibility of the results.\n\n2. **Data Versioning**: Each data object and pipeline is versioned with unique identifiers, allowing users to track changes over time. This versioning system is crucial for reproducibility, as it enables researchers to identify and utilize specific versions of data and processing pipelines.\n\n3. **BioCompute Objects**: The platform generates BioCompute Objects alongside each data object. These standardized records communicate bioinformatic pipelines and help verify and validate them, which aids in the reproducibility of experiments.\n\n4. **Rich Metadata**: The platform ensures that each data object is accompanied by detailed metadata, including information about the data's origin, processing parameters, and any modifications made. This adherence to the FAIR (Findable, Accessible, Interoperable, and Reusable) principles enhances the transparency and reliability of the data.\n\n5. **Release Notes**: ORCESTRA provides release notes that track the number of samples, treatments, sensitivity experiments, and molecular profile data across different versions of a dataset. This allows users to identify changes between updates and understand the evolution of the data.\n\n6. **Public DOI and Zenodo Integration**: Each data object is automatically uploaded to Zenodo and assigned a public Digital Object Identifier (DOI). This facilitates easy access and citation of the data, contributing to its reliability and reproducibility.\n\n7. **User-Customizable Pipelines**: The platform allows users to create custom data objects through user-customizable pipelines, ensuring that researchers can tailor the processing to their specific needs while maintaining a transparent and reproducible workflow.\n\nThese validation methods collectively enhance the reliability of the results produced by the ORCESTRA platform, ensuring that the data processing and sharing practices meet high standards of reproducibility and transparency in biomedical research."
    },
    {
      "question": "Are there any contradictions between this paper's findings and previous research?",
      "answer": "The paper discusses the development of the ORCESTRA platform aimed at improving the reproducibility and transparency of biomedical research through better data sharing and processing practices. It highlights several challenges faced in the field, particularly regarding the handling of large multimodal datasets and the inadequacies of current data-sharing practices.\n\nIn terms of contradictions with previous research, the paper does not explicitly state any direct contradictions with past findings. However, it does critique existing practices in data sharing and reproducibility, suggesting that many genomic data repositories do not adhere to the FAIR principles (Findable, Accessible, Interoperable, Reusable). This implies a contrast with earlier studies that may have claimed sufficient reproducibility or transparency in data sharing practices. \n\nMoreover, the paper emphasizes the need for comprehensive metadata and documentation in data sharing, which may contradict previous research that did not prioritize or adequately address these aspects. The authors argue that the lack of detailed metadata and the use of single processing pipelines without justification have led to issues with reproducibility, which could be seen as a challenge to the validity of findings from studies relying on such practices.\n\nIn summary, while the paper does not present explicit contradictions, it critiques existing methodologies and practices in the field, suggesting that they fall short of the standards necessary for reproducibility and transparency, which may indirectly challenge the reliability of previous research outcomes that did not adhere to these standards."
    }
  ]
}