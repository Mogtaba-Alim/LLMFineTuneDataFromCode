{
  "repo": "research_papers",
  "file": "Evaluation_of_statistical_approaches_for_association_testing_in_noisy_drug_screening_data.pdf",
  "language": "research_paper",
  "content": "Evaluation of statistical approaches for association testing in noisy drug screening data Petr Smirnov1, 2,  Ian Smith1, 2,  Zhaleh Safikhani2,  Wail Baalawi2,  Farnoosh Khodakarami2,  Eva Lin3, Yihong Yu3,  Scott Martin3,  Janosch Ortmann4,  Tero Aittokallio5, 6, 7, 8, Marc Hafner9 and Benjamin HaibeKains1, 2, 10* Background Modern biological research is often based on the collection and analysis of massive and high-dimensional datasets., Advances in robotics microfluidics,  sequencing, and infor - mation technologies have enabled the profiling of both molecular cell states and phe - notypes across millions of conditions, with 100s of millions or more associations that can be explored among the measured variables., With so many relationships to consider filtering out those that are not significant, both in strength and probability of occurring through random chance, is a primary task in analyzing these data., Towards this end Abstract Background, Identifying associations among biological variables is a major chal lenge in modern quantitative biological research, particularly given the systemic and statistical noise endemic to biological systems., Drug sensitivity data has proven to be a particularly challenging field for identifying associations to inform patient treatment., Results To address this, we introduce two semiparametric variations on the com monly used concordance index, the robust concordance index and the kernelized concordance index (rCI,  kCI), which incorporate measurements about the noise distribution from the data., We demonstrate that common statistical tests applied to the concordance index and its variations fail to control for false positives, and introduce efficient implementations to compute p values using adaptive permutation testing., We then evaluate the statistical power of these coefficients under simulation and compare with Pearson and Spearman correlation coefficients., Finally, we evaluate the various statistics in matching drugs across pharmacogenomic datasets. Conclusions We observe that the rCI and kCI are better powered than the concord ance index in simulation and show some improvement on real data., Surprisingly we observe that the Pearson correlation was the most robust to measurement noise among the different metrics., Keywords Pharmacogenomics,  Statistics,  Power analysis,  Nonparametric statistics, Association testing,  Drug sensitivity,  BiomarkerOpen Access  The Author(s) 2022., Open Access This article is licensed under a Creative Commons Attribution 4.0 International License which permits use,  sharing,  adaptation,  distribution and reproduction in any medium or format, as long as you give appropriate credit to the original author(s) and the source, provide a link to the Creative Commons licence,  and indicate if changes were made., The images or other third party material in this article are included in the articles Creative Commons licence, unless indicated otherwise in a credit line to the mate  rial., If material is not included in the articles Creative Commons licence and your intended use is not permitted by statutory regulation or exceeds the permitted use, you will need to obtain permission directly from the copyright holder., To view a copy of this licence, visit http:// creat iveco mmons. org/ licen ses/ by/4. 0/. The Creative Commons Public Domain Dedication waiver (http:// creat iveco mmons. org/ publi cdoma in/ zero/1. 0/) applies to the data made available in this article unless otherwise stated in a credit line to the data.RESEARCHSmirnov et al., BMC Bioinformatics (2022) 23188 https, //doi.org/10.1186/s12859-022-04693-z BMC Bioinformatics Petr Smirnov and Ian Smith have contributed equally to this work *Correspondence, bhaibeka@uhnresearch.ca 2 Princess Margaret Cancer Center,  University Health Network,  Toronto, Canada Full list of author information is available at the end of the article Page 2 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 statistical inference and machine learning have become standard tools of a scientist try - ing to make sense of massive biological data [13]., Major challenges in the statistical analyses of biological data include the non-stand - ard distributions of observed values and the noisiness of the measurements, especially for high throughput phenotypic measurements., The direct measurements often need to be transformed and/or normalized before the values can be interpreted and compared with results from other experiments, even within the same study., These transformations result in values that are often relative bounded, highly skewed and/or distributed with heavy tails., Furthermore the complexity of the experimental design and assays used, as well as the necessary compromises to enable high throughput collection of data (for example, working with small cell numbers and liquid volumes in plates with thousands of wells), make measurement noise unavoidable within these data [46]., Fortunately by taking replicate measurements of the same conditions, it is possible to directly measure the degree of noise introduced by a given experimental protocol., A common approach in identifying associations from biological data is computing similarities between vectors of biological measurements., Correlation coefficients are very widely used measures of similarity of which the most popular is Pearsons Prod - uct Moment Correlation., Pearsons correlation assesses the degree of linear association between variables. The standard statistical test of significance for Pearsons correlation makes parametric assumptions including normality and homoscedasticity of the inputs,  and testing may be sensitive to outliers., Non-parametric correlation measures such as the Spearman rank correlation and its associated tests, can detect nonlinear, mono - tonic associations but are commonly accepted as less powered to detect significant lin - ear associations [710]., The concordance index (CI) a linear transformation of Kendalls Tau, is a non-parametric association measure with the advantage of being highly adapt - able to missing and censored data, where the exact value of a data point is bounded but not precisely known., These non-parametric statistics also come with commonly associ - ated tests for significance including exact tests (often requiring no ties in the data),  and asymptotically correct tests., The asymptotic tests make weak assumptions on the dis - tribution of the data however, they converge only in the limit of large sample sizes, and rates of convergence are difficult to characterize in practice., Beyond correlation coef - ficients other metrics are also used, for instance cosine similarity from the field of infor - mation retrieval (which reduces to Pearson for centered data) [1113]., In addition any measure of distance can be transformed into a measure of similarity through usage of an appropriate kernel., A brief introductory level overview of computing similarities can be found in [14]. In our work we examine the behaviour of the common correlation coefficients when applied to regimes motivated by high-throughput screens of cancer cell lines measuring viability after treatment with compounds., We show that for realistic sample sizes com - mon statistical tests applied to the concordance index fail to control for false positives, reinforcing the need for testing against a non-parametric,  permutation-derived null distribution., We carefully examine the power of permutation tests of standard correla - tion measures in simulated settings, including the addition of simulated noise of realistic magnitudes., We also introduce two novel modifications of the concordance index the robust CI (rCI) and kernelized CI (kCI), which take into account the exact quantification Page 3 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 of noise possible through comparing replicates into the assessment of similarity between two vectors of measurements., We characterize these novel statistics through compari - son with existing correlation coefficients and provide efficient algorithms and imple - mentations to compute them., We found that the standard CI is more powerful under a permutation test in detecting non-zero correlations for bounded and skewed distribu - tions., We also surprisingly observed that the Pearson correlation was more robust to measurement noise than the other correlation coefficients investigated, including the proposed rCI and kCI statistics., Related work Assessing how correlation coefficients behave when applied to non-normal data and in the presence of noise has been previously investigated., Published studies have examined the power of correlation based permutation tests as well as compared permutation tests to those based on asymptotic limiting behaviours of the coefficients [1517]., Work by Bishara and Hitter [16] (with followup work by Puth et al. [17]) not only shows a com - prehensive comparison of Type 1 error control and power between several parametric and resampling-based tests for the Pearson and Spearman correlations, but also con - tains a review of early simulation work on permutation testing of the Pearson correla - tion., These previous studies tested a wide range of unbounded distributions but did not investigate simultaneously skewed and bounded distributions, a situation which arises frequently in analysis of high throughput phenotypic screens., Previous studies (except - ing Choks work [15]) have also left out the Concordance Index/Kendalls Tau correlation or any other pairwise ranking based methods from their comparisons., Previous studies have also investigated the effect of assay/measurement noise [18] or non-normal distrib - uted data [19] on the bias of the sample Pearson correlation coefficient, but not on the statistical power to detect a significant effect., The robustness of other correlation coef - ficients to the presence of such measurement noise likewise remains unstudied., Finally modifications of CI have been proposed previously in literature, both in the context of biological data [5], as well as in the computer science literature for assessing information retrieval system performance [20]., However these modified CI statistics were used pri - marily as a performance metric for concordance between predictors and observations, and not as a measure of correlation between two sets of measurements., Our study differs from existing studies in several aspects. First we investigate larger sample sizes and lower p-value thresholds for significance (  levels), more reflective of the high-throughput assays and the necessary corrections for the multitude of cor - relations assessed when analyzing modern biological data., Unlike previous studies our intent is not an exhaustive characterization of different tests for significance of corre - lation coefficients across a variety of distributions., Rather we advocate permutation testing as a general method applicable across correlation coefficients, and ask which coefficient is most powerful at detecting a significant (monotonic) effect., We investi - gate the statistical power only in the normally distributed case and a single bounded and skewed distribution, focusing instead on comparing different correlations and assessing their performance under noise., Our robust and kernelized Concordance Index meas - ures are also unique in using replicate measurements to directly address the effect of Page 4 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 noise in the analyzed data, and we evaluate their performance in detecting significant correlations., Motivating data preclinical drug sensitivity screening While the questions asked in this paper are general and not necessarily tied to the anal - ysis of any particular type of biological assay or experiment, our analyses were moti - vated by applications to preclinical drug sensitivity screening., These screens usually test the ability of compounds to inhibit cell growth and/or induce cell death across a panel of cancer cell line models grown in vitro., Each cell line is treated at several concentra - tions of compound and the cell line growth at a particular time-point is compared to a matched untreated control to derive a % viability value for each concentration level., In the analysis of these data it is common to fit a Hill Curve to the multi-dose viabil - ity measurements of a cancer cell line in response to treatment by a particular drug, as shown in Additional file 1,  Figure S1 [21,  22]., From these dose-response curves sum - mary metrics are derived that try to capture the sensitivity of a particular cell line to treatment with the compound in a single number., The most commonly used summary metrics are the IC50 the concentration at which the curve crosses 50%, and the Area Above or Under the Curve (AAC or AUC)., The IC50 is a measure of compound potency while the AAC/AUC averages the signal between potency and maximum efficacy of a compound., The advantage of the IC50 is that it is in micromolar units and can be com - pared directly between experiments, however,  it is not guaranteed to exist in all cases., The AUC/AAC metrics can always be calculated but they are dependent on the exact concentration ranges chosen for a particular experiment., In practice many research - ers prefer the IC50 measure because of its natural interpretation as a concentration., For examples within our study we will be using the AAC, as it has been shown to be more consistent between published datasets and does not suffer from missing or truncated values [6, 23]., As is standard practice [24] we calculate the area above the curve in log concentration space, and normalize the AAC by the total possible area for each experi - ment determined by the range of concentration measured, leading to values lying in the bounded range between 0 and 1., Note that this is similar to a calculation of mean viabil - ity over the concentrations range as a fraction., Coefficients considered The general problem is to identify monotonic associations between pairs of variables drawn from a range of potential distributions, and to that end,  we considered several parametric and non-parametric association statistics., The Concordance Index (CI) is defined as the fraction of pairs of observations that are ordered the same way by two variables, and it is a linear transformation of Kendalls Tau., Formally for two variables x and y,  CI is defined as, where N the number of observations of x and y. CI has a range of [0,  1], is 0 when the two variables are perfectly anti-correlated,  1 if they are perfectly correlated, and 0.5 (1)CI=# Concordant Pairs # Pairs=2 N(N1)/summationdisplay i, jI(xi>xj, yi>yj) Page 5 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 in expectation over the space of all possible orderings,  i.e., if there is no association between the variables., Even though the Concordance Index is a useful non-parametric statistic we hypoth - esize that incorporating parametric information about two observations are can increase the robustness of the Concordance Index to noise., For instance in physical and biologi - cal measurements, measurement error often has a characteristic scale smaller than the range of possible measurements., If measurement error is localized increasing magni - tude of the difference between the value of two observations increases the confidence that the two observations are ordered correctly by the measurements., The Robust Concordance Index (rCI) is a modification of CI that only considers pairs of points that are sufficiently dissimilar in both variables., We define two thresholds x0 and y and define rCI as The Kernelized Concordance Index (kCI) is a generalization of rCI, where instead of only considering valid pairs, every pair is assigned a weight according to a kernel., The rCI is a special case of kCI with a heavyside kernel where valid pairs are weighted with 1 and invalid pairs are weighted with 0., We choose the kernel so as the difference in meas - urements for either x or y approach 0 the weight for the pair of observations should tend to 0,  and as the difference grows large, the weight should tend to 1., A sigmoid of the form 1 1+ekxc is one such kernel with these properties and we outline a procedure for fit - ting the kernel to problem-specific empirical data in the supplemental methods., The definition of the kCI formally is: Where /Delta1x=| xixj| and /Delta1y=| yiyj| . It is not necessary to define the kernel purely on the difference in the observations though we chose to do so for simplicity., For purposes of comparison in subsequent analyses,  we compared CI,  rCI, and kCI with Pearson and Spearman correlation, which are also commonly used as test statistics for association., Both rCI and kCI have parameters that must be tuned for the intended data. The thresholds in rCI and the kernel shape in kCI can be interpreted as a characteristic range within which differences cannot be discriminated from statistical noise or measurement error., In the Methods we define a method for choosing these parameters using prob - lem-specific empirical data and replicate measurements., For our simulations below, we fit the parameters to real data from pharmacogenomic studies. Software To efficiently compute CI rCI,  and kCI, we developed an open-source R software pack - age, wCI (weighted Concordance Index) that implements all three statistics and is freely available from https, // github., com/ bhklab/ wCI under the GPLv3. The CI can be computed in O(NlogN) time because counting the number of inver - sions in a permutation is computationally equivalent to sorting [25]., Together with (2)rCI=# Concordant Valid Pairs # Valid Pairs=I(xi>xjyi>yj, |xixj|> x, |yiyj|> y) I(|xixj|> x, |yiyj|> y) (3) kCI=2/summationtext i, jw(x , y)I(xi>xj, yi>yj) /summationtext i, jw(x , y) Page 6 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 our work defining the rCI statistic, we present an algorithm for computing rCI also in O(NlogN) time, described in a reference implementation included in the afore - mentioned repository., While the algorithm looks quite complex in implementation intuitively it exploits three insights, The first is that the concordance index in can be computed in O(NlogN) time by sorting both vectors by the ordering permutation for one, and then counting the number of hops each element in the other makes during a merge sort., The second is using virtual elements with values at xx and x+x  as well as yy and y+y , and computing the difference between the number of real elements hopped over during the merge sort for the + and  elements, with the differ - ence between these two capturing the number of inverted pairs falling within the 2 margin (deemed invalid pairs)., The third is to repeat the algorithm twice once starting with X sorted and once with Y sorted, while comparing the computed invalid pair counts and applying the inclusion/exclusion principle to compute the pairs deemed invalid dur - ing a single sort, or during both sorts., To keep the time complexity to O(NlogN) we use a data structure to record the location of corresponding real and virtual items in X and Y and track their location when they are moved during the sort., In its most general case, kCI can depend on a unique weight for each pair of elements. As such it needs to read from memory /parenleftbigg n 2/parenrightbigg values, and therefore cannot be computed faster than O(N2) ., For certain restricted classes of kernels faster computation may be possible, as evidenced by interpreting rCI as kCI with a Heavyside kernel, but an explo - ration of these special cases is outside the scope of our work., Results Inflation of Pvalues for association testing Assessing statistical significance requires comparing an observed value of the test statis - tic with a proper null distribution., The most commonly tested null hypothesis is one of no association usually formalized as coefficient of association equaling zero for the pop - ulation - this is the null hypothesis considered throughout this work., There exist analyti - cal formulations for the distribution under the null hypothesis for all the coefficients above, except for kCI., For the Pearson correlation the statistic t=r/radicalBig n2 1r2 is known to asymptotically follow a t-distribution with n-2 degrees of freedom, and is most com - monly used in significance testing., For the CI a widely applied analytical distribution of the concordance index, due to its general applicability to data including censoring and ties, was introduced by Noether [26], and advocated for application to the CI by Pencina and DAgostino [27]., The derived distribution is also only asymptotically correct for non- Gaussian data. For Spearmans rho statistical testing is commonly done using an Edge - worth series expansion introduced by David et al., [28]. To evaluate the accuracy of these analytical formulations we took independent sam - ples from relevant distributions, computed the similarity according to each coefficient, and assessed the statistical significance using the analytical formulas., Under the null hypothesis the p-values calculated from the analytical formula should be drawn from a uniform [0, 1] distribution., We summarize these findings with a Q-Q plot for both normally distributed data and beta-distributed data (Fig., 1a and c, Additional file 1: fig - ure S2). In the context of testing association with a larger number K of features - for Page 7 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 instance, 104 genes - multiple hypothesis testing requires the significance threshold be at least as small as the reciprocal of the number of features, <1 K ., To illustrate this we show the fraction of samples with analytical p-value less than several values of  (Fig., 1b and d). These results show that for the rCI and CI statistics the analytical p-values are unre - liable in the regime under 1000 observations, and that the problem is exacerbated for samples from non-normal distributions., This is also true to a smaller extent for the Pear - son correlation test in the Beta(1.2, 4.5) case., The consequence of using the analytical p-values when the number of observations is small is an inflated type 1 error rate and false positive associations., Characterization of the null distribution of CI The null hypothesis for the CI is that the two variables have no association, and thus each permutation of the variables is equally likely., A permutation  on n elements has 0246802468QQ Plot of p values of perm utations of Norm(01), n=200k, length=100 log10 Empir ical Quantile (Rank)log10 Theoretical Quantile (computed p value) Pearson Spearm an CI RCI d=0.2 RCI d=0.1 RCI d=0.05 (a)0.0000.0020.0040.0060.008 02 50 5007 50 1000 N pointsFraction with p < 1e 3) metric ci pearson rci_d=0.05 rci_d=0.1 rci_d=0.2 spearm anFalse Po sitive Rates fo r p < 1e 3, 200000 random dr aws from Norm (0, 1) (b) 024681 01 20 2 4 6 81 01 2QQ Plot of p values of perm utations of Beta(1.2,  4.5),  n=200k, length=100 log10 Empir ical Quantile (Rank)log10 Theoretical Quantile (computed p value) Pearson Spearm an CI RCI d=0.2 RCI d=0.1 RCI d=0.05 (c)0.0000.0050.0100.015 02 50 5007 50 1000 N pointsFraction with p < 1e 3) metric ci pearson rci_d=0.05 rci_d=0.1 rci_d=0.2 spearm anFalse Po sitive Rates fo r p < 1e 3, 200000 random dr aws from Beta(1.2,  4.5) (d) Fig., 1 The asymptotic approximation of the CI null distribution produces an excess of small p values. We took independent samples from a normal and beta distribution computed their similarity using the coefficients above, and calculated asymptotic p values using the approximations from the text., Because the samples are independent, their p value distribution should be uniform. The Q Q plots for normal (a) and beta (c) distributions for samples of length N = 100 sampled 200 000 times shows an excess of small p values for CI and rCI., In the case of the normal distribution p values of 104 occur over twenty times more often than would be expected, and for the beta distribution nearly one hundred times more often for rCI., (b, Normal) and (d, beta) summarize the frequency of p<103 for different sample sizes. As the number of samples grows large the asymptotic approximation becomes more correct, but even in the regime of hundreds of samples, extreme p values occur several times more often than they should under the null Page 8 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 an inversion if there exist two elements i, j that are out of order,  i.e., ( i)> ( j) but i< j . The null distribution on CI is then the distribution on the number of inversions over the space of permutations of n elements., We have devised an exact null that can correctly compute the number of inversions on permutations of sets, in the absence of ties,  of up to N = 170 elements., In cases of more than 170 elements calculating the exact distri - bution breaks numerical precision of our environment, as the number of permutations goes as N!, and the range on the number of inversions is C(N, 2). The null distribution is also exact for pairs of variables which have a tie structure that can be represented as a multiset, i.e.,  observations can be partitioned into equivalence classes based on ties., To demonstrate that the exact formula correctly calculates the null for CI with no ties we computed the number of inversions from K = 1e6 permutations of N elements and compared this with our exact null., Comparing the exact distribution of the concordance index (1 - number of inversions divided by number of pairs) to the empirically observed distribution for the simulated permutations (Fig., 2a) we see very good consistency between simulation and our implementation of the exact calculation. We also computed the p-value according to our exact distribution for each simulation sample and saw a very good concordance of the distribution of p-values with the uniform distribution, as expected under the null (Fig., 2b). In general, if both variables have ties, the analytical null is unknown. This occurs with CI when both inputs have ties with rCI in almost all circumstances, and for CI with right-censored data as in survival analysis contexts, all of which have a tie struc - ture that cannot be represented by a multiset., Similarly, the exact analytical null for kCI is unknown. The only unbiased solution for computing statistical significance in cases where the limiting null approximation breaks down is a permutation null., Unfortunately, this is computationally costly. Power analysis We set out to investigate the power of the newly defined rCI coefficient in comparison to the other commonly used correlation coefficients discussed above., As the theoretical (a) (b)0.3 0.4 0.5 0.6 0.702468 10 12Comparison of Analy tical Null with Pe rmutation Nul l Concordance Inde xProbability DensityAnalytical Nul l Permutation Null N=1e6 024680246 8QQ plot of CI p values of permu tations of Beta(1, 10),  n=1000k, length=100 log10 Empiri cal Quantilelog10 Theoretical Quantile Exact Analytical Null Asymptotic Null Fig., 2 The analytical null accurately computes exact p values for CI. a The analytical distribution matches a permutation null of K = 1e6 samples of length 100 from a standard normal distribution., As CI is entirely nonparametric, the choice of distribution is irrelevant. b The Q Q plot shows the  log 10 empirical rank of the CI on the x axis and the  log 10 theoretical quantile from the analytical null (red) and asymptotic null (blue)., The analytical p values are both monotonic and correctly approximate the uniform distribution (grey) Page 9 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 distribution of the rCI statistic even under assumptions of normally distributed data is unknown, we analyze the power of these statistics in simulation., We sampled from a simulated bivariate standard normal distribution with a known correlation between the two variables r>0 ., For evaluating the power, we treat the population correlation as the effect size. We evaluated the statistical power of the Pearson Spearman, CI and rCI coef - ficients for detecting an association at a fixed significance level  , using a permutation- based test., In these simulations we investigated the effects of effect size,  sample size, and the rCI  parameter on the power of these respective statistics., Lacking a principled way to choose a kernel for the kCI applied to normally distributed data in simulation, we did not include the kCI statistic in our analysis., We began by evaluating the effect of the new  parameter on the power of the rCI statistic seeking to answer the question of what choice of  is optimal for this new statistic., We took 10 000 samples of length N=100 from bivariate normal distribu - tions with expected correlations of 0.2, 0.3 and 0.4,  representing low ( <10% ), medium ( 40% ) and high ( >80% ) power situations for the Pearson correlation with this sample size., Examining the medium power case ( r=0.3 ) we see that the Spearman and CI sta - tistics are similarly powered under this permutation test, whereas Pearson,  as expected in this bivariate normal case, is significantly better powered than the most widely used non-parametric correlation coefficients (Fig., 3a). The rCI shows a trend to increasing power with increasing  until approximately =1.2 after which power decreases and at around =1.5 the power of the rCI becomes worse than the CI or Spearman statistics., Note that in this case the marginal standard deviation of the data is 1, so the  parameter can be directly interpreted as a multiple of the standard deviation of the population., To compare the behaviour of the rCI statistic over the three different effect sizes investi - gated we normalized the power at each  to a percentage of the max power observed in simulation across all choices of  (Fig., 3b). While the exact location of the maximum var - ies slightly with the effect size the power is fairly stable in a range around =0.75 and =1.25 for all three investigated regimes, and close to the maximum achieved., Moti - vated by this observation, we chose =1 for our subsequent simulations. We then explored a range of expected population Pearson correlations between 0 and 0.5 looking at 1000 samples of N=100 long vectors for each effect size and computing the empirical power observed in simulation (Fig., 3c). Unsurprisingly the Pearson statis - tic is most powerful across the range, followed by rCI (at =1 ),  and then Spearman and CI practically indistinguishable., These simulations reinforce the benefit of the rCI modi - fication over existing fully non-parametric statistics across the full range from weakly powered to well powered situations., Finally we wished to investigate how the sample size on which the correlation coef - ficients are calculate affects their power in a permutation test., For this we decided to calculate a level-set of constant power as decreasing effect size is traded off for increas - ing sample size, fixing the expected Pearson power to be at 50%, and investigating sam - ple sizes from N=50 to N=300 ,  in steps of 50., For this simulation, we did 10, 000 samples from the distribution to calculate the power. We then normalized the power observed at each sample size as percentage of observed Pearson power to adjust for slight errors in calculation of the level set (Fig., 3d). We see a clear trend among rCI and the non-parametric statistics of increased power compared to the Pearson correlation as Page 10 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 sample size increases,  however, while rCI achieves >90% of the Pearson power already at N=50 and >95% by N=300 , the non-parametric statistics remain at less than 90% power even at N=300., Power simulation in nonnormal data Our ultimate motivation for investigating the performance of existing correlation coef - ficients and proposing modifications of the concordance index was their application to noisy biological data., Prior to examining real data we sought to evaluate the power of these permutation tests in cases where the marginal distributions of the observed vari - ables is non-normal., Motivated by our intended application of drug screening data we chose to investigate data drawn from bivariate beta distributions., In particular we sim - ulated data drawn from distributions with both marginals fixed as Beta (1.2, 4.5) dis- tributions (Additional file 1,  figure S3a), and varying Pearson correlations between the two sampled variables., Further details on the simulation are described in the methods section. Unlike the normally distributed data we did not optimize the selection of delta param - eters for rCI through power simulations., Rather we derived these parameters from real pharmacogenomics data, by choosing a threshold on differences between AAC 0.250.300.350.40 0.00 .5 1.01 .5 2.0 DeltaPowerMethod CI rCI Pearson Spear manN=100, Eff ect Siz e = 0.3, alpha = 0.001 (a)0.60.70.80.91.0 0.00 .5 1.01 .5 2.0 DeltaPower as % of max po wer Effect Siz e 0.2 0.3 0.4N=100, alpha = 0.001 (b) 0.000.250.500.751.00 0.00 .1 0.20 .3 0.40 .5 Effect Siz ePowerMethod CI rCI Pearson Spear manN=100, Delta = 1, alpha = 0.001 (c)0.850.900.951.00 100 200 300 Sample Size% of Pe arson Po wer Method CI rCI Pearson Spear manPower = 0.5, Delta = 1,  alpha = 0.00 1 (d) Fig., 3 Power analysis for data simulated using the bivariate Gaussian family. a displays the effect of the  parameter on the empirical power at a fixed effect size of population r=0.3., Other statistics unaffected by the parameter are plotted for comparison. b displays the empirically observed power for the rCI statistic only plotting the dependence on delta at 3 different effect sizes., The power is normalized as percent of maximum power achieved for each effect size to highlight the optimal region for choosing delta., c empirical power for as the population expected Pearson correlation increases. d empirical power for a varying sample size as the effect size is modified to keep a theoretically constant power for the Pearson correlation of 0.5., Power is plotted as the percent of achieved Pearson correlation power in simulation Page 11 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 measurements that maximizes our classification performance of replicate vs non-rep - licate measurements (see Methods)., The optimal threshold was very close to 0.1, so for the simulations, a delta of 0.1 was used. We also included the kCI statistic in these simu - lations deriving kernel parameters from the same data (see Methods)., We first examined the empirical power of these statistics at a modest sample size of 100 samples varying the correlation between the two variables., We again explored a range of expected Pearson correlations between 0 and 0.5 looking at 1000 samples of N=100 long vectors at each effect size., While the Pearson correlation was more pow - erful at weaker effect sizes at larger effect sizes (where all statistics achieve >50% power in simulation), the CI and kCI statistic become slightly more powerful in detecting sig - nificant effects at this sample size (Additional file 1, Figure S3b)., The rCI statistic per - forms similarly to Spearman over the range of effect sizes investigated both falling short of the Pearson correlation until all statistics converge to 100% power., We then investigated how sample size affects the power of these statistics. Similar to our previous simulations with normally distributed data we used a theoretical calcula - tion of a level set of 50% power for the Pearson correlation test., With the data distribu - tion deviating significantly from normal this was even more approximate than before, so we once again normalized everything relative to the power we observed for the Pearson correlation in simulation., We again took 10,000 samples each for vectors of length 100- 300, in step sizes of 50. We observe that as sample size increases larger than 100 the power of the CI statistics in a permutation test continues to increase over the Pearson correlation (Additional file 1, Figure S3c)., The kCI statistic behaves similarly to the Pear - son correlation in these simulations while the rCI and Spearman,  both gaining power relative to the Pearson correlation, but do not equalize in power even in our largest sam - ple size tested,  300., We repeated the same simulations but adjusting effect size to hold a theoretical power of 0.25 (Additional file 1,  Figure S3d)., Consistent with our previous results at a sample size of 100 the Pearson correlation was the most powerful statistic, but as the sample size grew the CI overtook the Pearson correlation in power, and it was the most powerful statistic at sample sizes of 300., The rest of the statistics increased in power similarly as before but starting off relatively less powerful compared to Pearson., Power simulation in nonnormal data with additive noise Finally we repeated the simulations at fixed sample size and varying expected cor - relations with Beta distributed data above, with the addition of noise sampled from a Laplace distribution, mimicking noise seen between replicate experiments in AAC measurements, as detailed in the methods section., The additive noise was significantly smaller in magnitude than the range of the data and as the rCI  and kCI kernels were picked using the replicate measurements, we expected these statistics to be advantaged in terms of power to detect effects corrupted by noise., All statistics were less powerful after the addition of noise (Additional file 1: Fig - ure S4b). This is unsurprising as from the point of view of a sample statistic, the data generation process is opaque, a particular sample generated with added noise is indistin - guishable from data generated with a weaker between-variable correlation., In these sim - ulations the rCI and kCI exhibit a similar drop in power to non-parametric statistics., Interestingly the Pearson correlation was actually less affected by the addition of noise Page 12 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 than all the other statistics, leading to the Pearson correlation outperforming all other statistics in observed power throughout the range of effect sizes simulated., It is possible that these results are observed because the addition of additive noise increases slightly the variance of the data, and may decrease the skewness of the distribution, reducing the deviation from normally distributed data., Querying pharmacogenomic datasets With some theoretical properties explored we sought to evaluate the practical perfor - mance of these coefficients on real pharmacogenomic data., Large-scale pharmacog - enomic studies have treated panels of cancer cell lines with drugs at a range of doses., The sensitivity of a cell line to a drug can be summarized with the area above the dose response curve (AAC), and assaying a panel of cell lines gives a vector of drug sensi - tivities for a particular drug., The studies considered are described in the Methods. For each drug measured in two studies across a common set of cell lines we computed the similarity coefficients between that drugs response vector in one study against all other drugs response vectors in the other study., We then found the rank of the matched drug (Fig. 4a). This has been shown to be a challenging problem in that drug sensitivi - ties do not necessarily replicate well across pharmacogenomic studies [6, 23]., The sta - tistic with best resilience to the noise and artifacts in the pharmacogenomic studies will tend to rank matched drugs better., Because some of the drugs have been tested on very few common cell lines across datasets we also considered only those comparisons made with more than 50 cell lines (Fig., 4b). In addition to examining the distribution of the matched-drug ranks we summarized the results by calculating the area under the empirical Cumulative Distribution Function (CDF) (Table 1)., The five metrics had similar performance, though Pearson had the consistently best recall. The kCI showed improved recall across datasets compared to CI and Spearman correlation suggesting that incorporating information characterizing the noise in the datasets can improve identification of association., While this benchmark is informative 0.00 .2 0.4 0.6 0.81 .00.0 0.2 0.4 0.6 0.8 1.0Drug Recall Analy sis, all drugs RankCumulative FractionPearson Spear man CI rCI kCI (a)0.00 .2 0.40 .6 0.81 .00.0 0.2 0.4 0.6 0.8 1.0Drug Recall Anal ysis, N > 50 cell lines RankCumulative FractionPearson Spearm an CI rCI kCI (b) Fig., 4 Drug recall analysis across pharmacogenomic datasets. For all pairs of datasets the similarity between the vector of cell line responses for all pairs of drugs is computed with each coefficient for (a) all drugs and (b) those drugs with at least fifty cell lines in common across datasets., For drugs present in both datasets, the rank of the matched drug relative to all drugs is extracted. The x axis is the rank of the matched drug, where 0 is most similar and 1 is least similar. The yaxis is the empirical CDF of the matched drugs for a given rank or the fraction of matched drugs with rank less than x Page 13 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 it is ultimately inconclusive about the practical capabilities of the different coefficients to identify biomarkers of drug sensitivity., Discussion Evaluation and significance testing of coefficients of association is a ubiquitous step in modern biological data analyses., However the behaviour of common correlation coef - ficients applied to noisy data originating from distributions seen in modern biological data is not well characterized., Our study addressed this question for high-throughput cell line viability screening. We proposed new measures of association for noisy data investigated the correctness of significance testing for common correlation coefficients, introduced efficient and accurate implementations for calculating significance for the concordance index, and investigated the power of permutation testing coefficients of association in distributions mimicking our area of application., Finally we demonstrated that incorporating information about the measurement noise into calculating associa - tions between vectors of observations, through our proposed kCI approach, demon - strates improved performance over the standard concordance index in a cross-dataset replicability task., Researchers in biology will often compute and test thousands of correlations requiring multiple testing correction to keep the expected number of false positives under control, often done through a False Discovery Rate or Family Wise Error Rate correction proce - dure., Hypothesis testing in these situations require methods that can compute p-values accurately in the range of 1e-4 - 1e-10, depending on the number of hypotheses inves - tigated, meaning that correct tail end behaviour dominates the accuracy of inference., Pencina and DAgostino [27] investigated the use of Noethers method to calculate 95% confidence intervals in the presence of complicated tie structures due to right censoring of one vector, and showed the method to have moderately precise coverage., Unfortu - nately our data suggests that the extension of this approach to calculating p values for hypothesis testing does not control false positives at lower alphas than 0.05, at moderate to large sample sizes., This is the case not only for the rCI statistic which introduces tied pairs in the data but also for the concordance index applied to data with no ties., While this may be unsurprising given the asymptotic arguments for the derivation of Noethers method what caught us off guard in our results was the slow rate at which the control of false positives converged, even for the case without ties and sampling from a bivariate normal distribution., As much as 500 samples were not enough to restore proper false Table 1 A table of the areas under the empirical CDF curves for drug recall across datasets Because the drug recall computes the rank of the matched drug, this is equivalent to 1 - the mean rank of matched drugs across studiesCoefficient Area under CDF Area under CDF, N > 50 cell lines Pearson 0.8899 0.9382 Spearman 0.8704 0.9230 CI 0.8698 0.9238 rCI 0.8639 0.9222 kCI 0.8808 0.9341 Page 14 of 24 Smirnov et al., BMC Bioinformatics (2022) 23188 positive control at an alpha of 0.001, and given the behaviour observed in the Q-Q plots for smaller sample sizes, we expect the sample size required for more stringent alphas will be substantially larger., Permutation tests are a versatile tool for assessing significance without making strong parametric assumptions about the generating distribution for the data., For permutation testing to be theoretically justified in making inferences about a population parameter, the data has to be independently and identically sampled, and each observation has to satisfy exchangeability between the two variables measured [29]., These assumptions are all shared by both exact tests for non-parametric correlations and asymptotic analytical tests for the Pearson and CI correlations., Therefore from a theoretical basis, the permu - tation test can only increase the range of valid situations for its application., Two con - cerns often raised by practitioners regarding applying permutation testing is a perceived loss of power, and the computational cost of simulating permutation null distributions., It is important to note that an inflation of false positives will lead in practice to lower p values and the illusion of more power originating from analytical tests., Previous studies suggest however that the power of the permutation test for the Pearson correlation is very nearly identical to the Pearson t test in all cases where the t test retains good type 1 error control [16, 29], with the t test providing significantly more power only in cases where data is nearly normally distributed, and at low ( 5 ) sample sizes [16]., Concerns about the computational cost of permutation testing are valid especially in the context of multiple testing correction., However approaches such as QUICKSTOP [30], for cases where the required alpha after correction is known, as well as other approaches focused on ensuring FDR control [31], can reduce by orders of magnitude the number of per - mutations needed to perform inference., Combined with modern computer hardware permutation testing,  even at high-throughput data scale , becomes feasible., Therefore based on our results assessing Type I error rates for asymptotic approximations of the CI distribution, and prior work mentioned above, we advocate for permutation testing to be adopted as the default testing strategy for tests of association, and require justification for reverting to analytical formulas for calculating p values for correlation coefficients., Once it has been established that permutation testing is necessary to obtain correct p value estimates for sample sizes of interest, we compared the power of the different sta - tistics under the permutation test., Pearsons correlation coefficient remained the most powerful statistic to detect linear associations between normally distributed variables when using a permutation test, but with only a marginal advantage., The power differ - ence between the Pearson CI,  rCI and Spearman are small, and probably of little signifi - cance in practice at our investigated sample sizes and  ., The newly defined rCI statistic exhibited most power with the delta parameter in the range of approximately 1 stand - ard deviation of the data, and the value of the optimal delta was fairly stable over differ - ent effect sizes., While less powerful than the Pearson correlation in this simulation the semi-parametric rCI was more powerful than non-parametric statistics, suggesting that in cases where the Pearson is not a suitable statistic (for example, looking for non-linear relationships),  the rCI could be the tool of choice., Beyond the Gaussian case none of Pearson, Spearman or CI are consistently most powerful over the range of effect sizes investigated in bivariate beta distributed data at a moderate sample size of 100., The performance of our newly defined statistics, rCI Page 15 of 24 Smirnov et al. BMC Bioinformatics (2022) 23:188 and kCI, was similar to the others. Interestingly we noticed that the Concordance Index seemed to outperform the Pearson correlation in the well powered ( >50% ) regime., Strikingly holding the power of the Pearson test at 50% while increasing sample size, the relative power gain of the Concordance Index increases., This suggests that for standard regimes of biological data analysis where data is moderately large (100s of samples),  and non-normally distributed, a permutation test based on the Concordance Index may be better powered to detect significant effects than the widely used Pearson correlation., We must caution however that precise rankings of these correlations in distributions very different from the skewed and bounded data we investigated cannot be extrapolated from our results., As a demonstration of this fact when noise was added to the simulated data, Pearson was found to once again be more powerful than the Concordance Index., There are, of course, limitations to this work. First, we only investigated the task of detecting linear associations from data. Undeniably non-linear effects play a major role in biology, however linear analysis is often the first step in understanding any biological system., A major focus of our work has been on the concordance index (and by exten - sion the Kendall Tau) measure of correlation., We have however omitted any investiga - tion of the CI (and our derivatives) to censored data where it is most commonly applied., Finally due to computational constraints, we could not exhaustively search the space of distributions,  sample sizes,  hyper-parameters, and effect strengths., Conclusion The experiments carried out in our work can be summarized into three categories investigating the Type I error control of common tests applied to correlation statis - tics in simulation, investigating the power of permutation tests based on common and novel (rCI, kCI) correlation statistics in simulation, and benchmarking the performance of novel and existing correlation statistics on real high throughput drug screening data (Additional file 1, Figure S5)., From the first set of simulations we conclude that applying asymptotically derived tests to the regime of moderate (N 500) sample sizes can lead to a significant inflation in false positives when looking at  small enough to correct for multiple testing., While we present an approach to calculate exact p-values for the Concordance Index in a subset of cases (dependent on the presence and structure of ties in the data), we advocate for permutation testing as a default approach for assessing significance for correlation tests., We then introduce two modifications to the Concordance Index the rCI and kCI sta - tistics, motivated by increasing robustness of the Concordance Index to noisy data., In simulations we demonstrate that CI and kCI are best powered of the tested correlations when the data deviates significantly from normality and sample sizes are large enough., However the permutation test based on Pearson correlation showed better robustness to noise than all other statistics., Finally we benchmarked the new correlation coeffi - cients against Pearson, Spearman and CI in a cross-study replicate similarity task., In this benchmark, Pearson correlation and the kCI demonstrated the best performance. Ultimately we conclude that researchers working with high-throughput drug screen - ing data will be best served by using the Pearson correlation as a measure of correlation, with significance estimates derived from permutation tests., This result is surprising giv - ing that the distribution of both the data and the noise in the data deviate significantly Page 16 of 24 Smirnov et al., BMC Bioinformatics (2022) 23:188 from normality. Nevertheless both existing non-parametric correlations and novel simi - larity metrics designed specifically with the task in mind failed to consistently outper - form the humble Pearson correlation., Methods Cell line screening datasets considered As mentioned previously our investigations were motivated by the task of finding cor - relations between high throughput compound screens measuring viability in cancer cell lines., For this study we considered data originating from six different screening data - sets, the Genomics of Drug Sensitivity in Cancer (versions 1 and 2) [3234], the Cancer Therapeutics Response Portal v2 [3537],  the Genentech Cell Screening Initiative [6], the Finnish Institute of Medicine cell line screening dataset [38] and the Oregon Health and Science University Breast Cancer Screen [39]., All the data used in this study was previously published except for the data from the Genentech Cell Screening Initiative, where an updated version of the original dataset was used for our analyses., All datasets were downloaded using the PharmacoGx R package [40], from orcestra.ca [41]. For the analyses in this manuscript all dose-response curves were summarized as the Area Above the Curve, using a 3 parameter Hill Curve model fit using the Pharma - coGx package, as previously described [42]., Characterization of the null distributions There are four situations to consider: 1. Concordance Index with no ties - the two vectors are composed of unique elements, i.e. i,j such that xi=xj or yi=yj 2. Concordance Index with ties - i,j such that xi=xj or yi=yj . This problem breaks down further into two cases (A) the tie structure is transitive and can be represented by one multiset - if i= j and j=k , then i=k ,  or (B) the tie structure cannot be rep - resented by a multiset,  so i, j, k|i=j, j=k, ik ,  where = denotes a tie in at least one of the vectors., 3. Robust Concordance Index - we define two thresholds tx and ty such that if |xixj|<tx or |yiyj|<ty the pair is considered invalid and ignored in the CI calculation., The denominator in the count of inversions is the number of valid pairs where both /Delta1x and /Delta1y exceed their respective thresholds., As with non-transitive tied CI rCIs tie structure does not necessarily form an equivalence relation and is not transitive., 4. Kernelized Concordance Index assigns a weight w to all pairs that is a function of one or both of the differences between the values., Larger differences in x and y can be considered a pair with greater confidence. The choice of kernel is typically monotone increasing with delta, for instance a sigmoid. rCI can be thought of as a special case of kCI where the kernel is a Heavyside step function. The exact analytical nulls for 1 and 2A are known and presented here. For cases 2B 3,  and 4,  the exact null is unknown, so statistical significance must be assessed with permu - tation testing., Page 17 of 24 Smirnov et al. BMC Bioinformatics (2022) 23188 Case 1 In the cases that there are no ties, calculating the concordance index can be reduced to counting inversions in a permutation., Given two lists of numbers without ties x and y,  to calculate CI (x,  y), it is sufficient to find the ordering permutations x and y for x and y respectively, and then count the number of inversions in x1 y ., Computationally this is equivalent to ordering both lists such that x is sorted, and then counting inversions in y., The simplest way to generate the probability distribution on the number of inversions from a set is to use a generating polynomial or an ordinary generating function., The dis - tribution on the permutations which result in a particular inversion number is repre - sented as coefficients in a polynomial on x, the number (or probability, if normalized) of permutations yielding k inversions is the coefficient of xk., In the absence of ties from [43],  the following result holds, let In(k) denote the num - ber of permutations of S with k inversions., Then The right hand side of (4) admits exact recursive and iterative methods for adding ele - ments by multiplying polynomials., This is equivalent to convolutions on discrete proba - bility distributions. We note that this formula is widely used in practice for example, the R Statistical language [44] implements this exact formula for calculating the significance of Kendalls Tau correlation for small sample sizes (by default 50 at the time of writing) in the absence of ties, testing against the null hypothesis of no association., Case 2a The best way to think about permuting elements with ties between them is as follows Each equivalence class formed by the ties in the original vectors x and y is assigned to a specific element ej ., Let the set of distinct elements to be permuted be denoted E={e1 ..., en} and let ajZ>0 denote the multiplicity of element ej ,  i.e., how many ele - ments are in this equivalence class. Thus there are :=/summationtextn j=1j elements in total. We denote by M={e1 1, ...,enn} the multiset containing all elements (with ties). It turns out that an analogous result to (4) can be obtained for multisets. The original reference is from 1915see [45]but its easier to read in modern notation such as pre - sented in [46]., Let inv( ) denote the number of inversions of a permutation of the mul - tiset (set with ties) M. The distribution of inv can be shown to be with the q-factorial being defined by(4) n(x), =n(n1 )/2/summationdisplay j=1In(x)xk=n/productdisplay j=1j1/summationdisplay k=1xk., (5)DM(x)=/summationdisplay SMxinv( )=/bracketleftbigg  1...n/bracketrightbigg x=!x 1!x..n!x (6) m!x=r/productdisplay k=1/parenleftBig 1+x+...+xk1/parenrightBig Page 18 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 (The expression on the right-hand side of (5) is also called the q-multinomial coefficient.), Observe that by splitting the sum over SM according to the number of inversions, where IM(k) denotes the number of permutations of the multiset M with k inversions., Thus, ( 5) is the exact multiset analogue of (4). The right hand side of (4 ) gives a recursive formula for expressing In(k) in terms of the In1 (j) , in terms of the generating function this reads The proof if this formula (presented in [43]) proceeds by looking at permutations of the first n1 elements and then inserting the last element at all possible position., By keep - ing track of how many extra inversions this insertion introduces, we arrive at (8). This argument extends rather well to the case with ties let M be the multiset as described in the introduction and denote by M the set obtained from M by remov - ing one occurrence of en ., That is, if M=e1 1, ..,enn , then and in particular if |M|=n then |M|=n 1 . We can give the following analogue of (8): with m!x defined in (6). We note that this case where the Concordance Index can be reduced to counting inversions on multisets is of practical interest., Specifically, it will arise whenever only one of x or y contains exact ties. However it is limited to exact ties, as representing the vectors as a multiset requires transitivity of ties, and therefore cannot be applied to censored data., Notes on efficient implementation Calculating the distribution of inversions over random permutations requires poly - nomial multiplication, or equivalently,  convolution of discrete distributions., This can be accomplished in O(nlog(n)) time using the Fast Fourier Transform once trans - formed, polynomial multiplication is elementwise instead of convolutional., Numeri - cal precision limitations introduce very slightly complex coefficients after the inverse transform introduces, which can be dealt with by rounding to the nearest integer., The naive application of FFT for polynomial multiplication would be to take the two factors transform them,  multiply,  and inverse transform them., We implemented a method by which the sequence of polynomials to multiply is generated transformed and multiplied simultaneously, and inverse transformed once to further optimize the process., (7)DM(x)=/summationdisplay k0/summationdisplay SM inv( )=kxinv( )=/summationdisplay k0/summationdisplay SM inv( )=kxk=/summationdisplay k0IM(k)xk (8) n(x)=/parenleftBiggn1/summationdisplay k=0xk/parenrightBigg n1(x)., (9) M=e1 1e2 2, ..., en1 n1, en1 n , (10) DM(x)=/bracketleftbigg  n/bracketrightbigg xDM(x)=!x (n)!xn!xDM(x), Page 19 of 24 Smirnov et al., BMC Bioinformatics (2022) 23 188 Computing the distribution on inversions for Case 2a requires dividing polynomi - als., It turns out that in all cases encountered when applying the recursive formula (10) - with the Dn ratios of /producttextn+m k=nzk to /producttextm+1 k=1zk , for each polynomial in the denominator, there always exists a polynomial in the numerator which is a polynomial multiple of the denominator such that the coefficients are integral and the remainder is 0., In our experience doing any polynomial division - including in FFT space - was prohibi - tively inaccurate, so we implemented a method to simplify the Dn s to just the product of the quotients, eliminating the division entirely., Efficient implementations of these distributions in R are shared with the wCI R package. Cases 2b  4 We note that the exact distribution under the assumption of no association between the two vectors x and y for Cases 2b - 4 are not known., For Cases 2b, as well as 3, the multiset representation breaks down. It is likely that a completely different approach will be needed to efficiently (in polynomial time) compute these general cases., Note that the CI applied to data with censoring falls into this regime. The continuous nature of the kCI kernel likewise suggests that the null distribu - tion can not be computed combinatorically., If solved however the distribution for rCI will likely be a limiting case as the kernel approaches the discontinuous Heavyside function., Power analysis All power analysis was conducted through simulations in the R programming lan - guage., A link to the code is provided at the end of the manuscript. Each power analysis simulation followed the same steps: 1. Two vectors of data of chosen length N were sampled from a bivariate distribution with a known expected Pearson correlation R as the effect size measure., 2. A permutation test was conducted using each of the Pearson Spearman,  CI, rCI and kCI (for Beta distributed data) as a test statistic, using the adaptive permutation testing algorithm described below., Tests resulting in P values under the chosen  of 0.001 were considered to have rejected the Null hypothesis, and therefore success - fully detected the dependence between the two vectors., 3. Steps 1-2 were repeated 1000 or 10000 times (as described in the results section), recording for each iteration and each statistic the whether the Null was rejected., The power at each effect size was calculated as the percentage of cases in which the Null was rejected by each statistic., 4. Steps 1-3 were repeated for effect sizes ranging from R=0 , to R=0.5 , in 0.1 step increments. Bivariate Normal Data was generated using the function mvrnorm from the MASS R package. Page 20 of 24 Smirnov et al. BMC Bioinformatics (2022) 23 188 Simulating data distributed as a bivariate beta We construct a bivariate Beta distribution with a fixed expected Pearson correlation from the family of distributions described in [47]., Briefly the distributions are con - structed by sampling (U1, U2, U3) from a trivariate Dirichlet distribution,  and trans - forming the variables as follows, X=U1+U3 ,  Y=U2+U3 ., This construction is a 4 parameter family with 3 parameters fixed by the shapes of the marginal distributions of X and Y., The fourth parameter is then optimized to yield the desired correlation between X and Y using the Brent method as implemented by the R optim function., Adaptive permutation testing Adaptive stopping during permutation testing was used for all power analyses., Briefly adaptive stopping allows the permutation test to complete once there are enough ran - dom resamples to determine whether a particular sample falls above or below a prede - fined significance threshold  ., For these analyses the QUICK-STOP algorithm presented by [30] was implemented in the R programming language., The parameters used for our simulation were as follows the indifference parameter d was set to 0.001  , the Type I and II error probability was set to e10 , and a stopping criteria was implemented to halt after 1001/ permutations., Choice of rCI thresholds and kCI Kernel The Robust and Kernelized Concordance indices have parameters that can be optimized for the particular distribution of interest, the rCI threshold and the kCI kernel., Because both coefficients are premised on downweighting pairs of points with differences that cannot be distinguished from statistical noise, replicate measurements are needed to characterize the noise distribution from the data., Conceptually the goal is to identify pairs of points with true measurements that are different given the random variable of the delta between the measurements., The approach is similar to Storeys FDR [48]. Let x be a quantity for a set of N points, and x be the measurement of x. For a pair of points i and j, consider /Delta1| xixj| . The null hypothesis h0 is that the quantity is the same for the pair, i.e. xi=xj . The alternate hypothesis is that the two values are differ - ent: xi =xj . Let S be a set of empirical deltas where the true difference is unknown. For repeated measurements of the same point the null hypothesis is necessarily true, which gives the distribution of  under they null hypothesis., Let S0 be a set of empirical deltas from replicates. The problem is then to deconvolve S into a mixture of null and alternate hypotheses. The main result uses the empirical CDF (eCDF) on S and S0 . From Bayes Theorem we have the probability of a pair of points having equal values given a measured delta exceeding some value t as, Here, the numerator is the eCDF of /Delta1 for replicates multiplied by the estimate of the fraction of observed points for which the null hypothesis is true, and the denominator is the eCDF of /Delta1 for the population of measurements., The inequality emerges because (11) P(h0|>t )=P( > t|h0)P(h0) P( > t)P( > t|h0) P( > t) Page 21 of 24 Smirnov et al., BMC Bioinformatics (2022) 23:188 P(h0)1 . Similarly the probability of the alternate hypothesis can be computed from above by the law of total probability, that P(h0)+P(h1)=1., We consider deltas exceeding the threshold t as positive cases and those with deltas less than t as negative cases., From this a confusion matrix can be generated as a func - tion of the threshold, True Value Positive Negative rCI decision boundary Positive P(h1|> t) P(h0|> t) Negative P(h1|t) P(h0|t) The threshold for rCI is given by choosing t= that maximizes the Matthews Cor - relation Coefficient (MCC) from the confusion matrix above., While a contingency table cannot be perfectly summarized by any one value MCC has been shown to be a balanced measure of classification even when the class sizes are different., For the kCI kernel it is desirable to have a monotonic kernel that is 0 at /Delta1=0 and without loss of generality tends to 1 as /Delta1 gets large., The P(h1|> t) fits these criteria well, with the additional stipulation at 0. For portability we fit a sigmoid function to P(h1|> t) ,  and this is the kernel, the weight assigned to a pair of points is the estimate of the probability that they have different true values., The thresholds or kernels for both rCI and kCI depend only on /Delta1 and not on the spe - cific values of the measurements., This implicitly assumes homoscedasticity of /Delta1 over the range of the measurements. While it might be useful to explore this further this choice was motivated by practicality, as there often is insufficient data to characterize the het - eroscedasticity of /Delta1., Simulation of artificial noise To simulate noise representative of real pharmacological screening data, we first exam - ined the distribution of pairwise differences in AAC values for experimental replicates done by the same study across all of the datasets described above., Upon observing this data, it was clear that it was not normally distributed. Using Maximum Likelihood Esti - mation we fit a Gaussian [ aexp ((xb)2 2c2) ] and Laplace distribution [ 1 2bexp(|x| b) ] to the data, and found the Laplace fit to be qualitatively good, and adequate for use in a simulation context (Additional file 1,  Figure S4a)., For power simulations sampling from the Beta distribution with artificially added noise two vectors of correlated data with a chosen expected correlation were sampled as above., A vector of the same length was then sampled from the MLE Laplace noise distribution and added to one of the two vectors of simulated data., Finally any values under 0 or over 1 after the addition of noise were truncated to keep the data bounded in the correct range., Querying pharmacogenomic datasets To gauge the performance of the coefficients on practical pharmacogenomic data, we considered the four pharmacogenomic datasets mentioned above obtained from PharmacoGx., We compared two datasets at a time, and compared the matrix of drug Page 22 of 24 Smirnov et al. BMC Bioinformatics (2022) 23 188 sensitivities summarized with the area above the dose response curve as described above., First, the pair of matrices were restricted to those cell lines common to both studies. Then for each drug in the first study, the similarity to all drugs in the second study was computed using each of the coefficients., For those drugs assayed in both studies the rank of the similarity for the matched drugs was found relative to the similarity of the drug to all drugs in the second study., This self-rank of a drug was computed over all pairs of the pharmacogenomic datasets. We summarized the ranks using the area under the CDF of the ranks. An area under the CDF of the ranks of 1 indicates that the most similar drug in the second study to that of the first was the matched drug in all cases., If the matched drug ranks were uniformly distributed - indicating no correspondence between studies then the area under the curve would be 0.5., An obvious caveat is that the intersections of cell lines across studies was often small. If few common cell lines were measured then the drug was poorly character - ized and the comparison was less meaningful., To mitigate this, in Fig. 4b, we con - sidered only those drugs with at least 50 cell lines in common across datasets. This improved the performance of all the coefficients. It is also worth considering the per - formance of the different metrics for different regimes of drugs, i.e., those with tar - geted effects with few sensitive cell lines contrasted with those with broad effects and many sensitive cell lines., The parameters for the rCI and kCI were chosen based on combined replicates from all studies the threshold for rCI was set to 0.12,  and the kCI kernel had parameters (-27.52,  0.0646)., Abbreviations CI Concordance index,  rCI,  Robust concordance index,  kCI, Kernelized concordance index,  AAC ,  Area above the (drug dose response) curve,  AUC , Area under the (drug dose response) curve,  IC50,  Half maximal inhibitory concentration, Q Q plot,  Quantile quantile plot., Supplementary Information The online version contains supplementary material available at https // doi., org/ 10. 1186/ s12859 022 04693z. Additional file 1. Supplementary Figures S1S5. Acknowledgements We thank Richard Bourgon for reviewing an early draft of this work the constructive feedback provided made this work stronger and more rigorous., Author contributions P .S. and I.S. designed the analysis developed the code and statistical analysis,  performed the analysis, and wrote the paper., Z.S. and W.B. formulated the concept of the modified metrics and contributed analysis tools and methods. F.K. developed the algorithm for a fast implementation of the rCI code. E.L., Y.Y., S.M., and M.H. designed and collected phar  macogenomic experimental data. J.O. provided statistical analysis and characterized the null distributions for inversions. T.A., M.H., and B.H.K. conceived of, oversaw, and refined the analysis. All authors reviewed the manuscript. All authors read and approved the final manuscript. Funding This work was supported by the Academy of Finland (Grants 326238 340141 and 344698 to TA), European Unions Hori zon 2020 Research and Innovation Programme (ERA PerMed JAKSTAT TARGET), and the Cancer Society of Finland (TA)., Availability of data and materials The wCI package and analysis code is available on github at https, // github., com/ bhklab/ wci_ manus cript the pharma cogenomic datasets used in the analysis is available via the orcestra.ca platform [41], and the code and data are acces sible in a Code Ocean capsule with https, // doi., org/ 10. 24433/ CO. 42162 95. v1. Page 23 of 24 Smirnov et al. BMC Bioinformatics (2022) 23 188 Declarations Ethics approval and consent to participate Not applicable., Consent for publication Not applicable. Competing interests BHK is a shareholder and paid consultant for Code Ocean Inc. The authors have no other competing interests to declare. Author details 1 Department of Medical Biophysics, University of Toronto, Toronto, Canada. 2 Princess Margaret Cancer Center, University Health Network, Toronto, Canada. 3 Department of Discovery Oncology, Genentech Inc., South San Francisco, USA. 4 Departement danalytique operations et technologies de linformation, Ecole des sciences de la gestion,  Universite du Quebec a Montreal,  Montreal,  Canada., 5 Institute for Molecular Medicine Finland (FIMM) Helsinki Institute of Life Science (HiLIFE), University of Helsinki,  Helsinki,  Finland., 6 Institute for Cancer Research, Oslo University Hospital, Oslo, Norway. 7 Oslo Centre for Biostatistics and Epidemiology, University of Oslo, Oslo, Norway. 8 iCAN Digital Precision Cancer Medicine Flagship, Helsinki, Finland. 9 Department of Oncology Bioinformatics, Genentech Inc., South San Francisco, USA. 10 Vector Institute, Toronto, Canada. Received: 18 October 2021 Accepted: 15 April 2022 References 1. Greene CS, Tan J, Ung M, Moore JH, Cheng C. Big data bioinformatics. J Cell Physiol. 2014;229:1896900. 2. Ching T, et al. Opportunities and obstacles for deep learning in biology and medicine. J R Soc Interface. 2018;15:20170387. 3. Moore JH. Bioinformatics. J Cell Physiol. 2007;213:3659. 4. Tsimring LS. Noise in biology reports on progress in physics. Phys Soc (Great Britain). 2014;77:026601. 5. Costello JC, et al. A community effort to assess and improve drug sensitivity prediction algorithms. Nat Biotechnol. 2014;32:120212. 6. Haverty PM, et al. Reproducible pharmacogenomic profiling of cancer cell line panels. Nature. 2016;533:3337. 7. Hotelling H Pabst MR. Rank correlation and tests of significance involving no assumption of normality., Ann Math Stat. 1936;7:2943. 8. Siegel S. Nonparametric statistics. Am Stat. 1957;11:139. 9. Bradley JV. Distributionfree statistical tests. http:// cites eerx. ist. psu. edu/ viewd oc/ summa ry? https://doi. org/10.1.1.977.3717 1968. 10. Fowler RL. Power and robustness in product moment correlation. Appl Psychol Meas. 1987;11:41928. 11. Salton G, Buckley C. Term weighting approaches in automatic text retrieval. Inf Process Manage. 1988;24:51323. 12. Song L Langfelder P ,  Horvath S. Comparison of co expression measures,  mutual information, correlation,  and model based indices., BMC Bioinformatics. 2012;13:121. 13. Henkel R, et al. Notions of similarity for systems biology models. Brief Bioinform. 2018;19:7788. 14. Metcalf L Casey W. in Cybersecurity and Applied Mathematics (eds Metcalf,  L. & Casey, W.) 322 (Syngress,  Boston,  Jan. 2016)., ISBN: 9780128044520. https:// doi. org/ 10. 1016/ B9780 12 804452 0. 000026. 15. Chok NS. Pearsons Versus Spearmans and Kendalls Correlation Coeffcients for Continuous Data University of Pitts burgh ETD., Sept.: University of Pittsburgh; 2010. 16. Bishara AJ, Hittner JB. Testing the significance of a correlation with nonnormal data comparison of pearson,  spear  man, transformation,  and resampling approaches., Psychol Methods. 2012;17:399417. 17. Puth M T, Neuhauser M, Ruxton GD. Effective use of Pearsons product moment correlation coeffcient. Anim Behav. 2014;93:1839. 18. Saccenti E, Hendriks MHWB, Smilde AK. Corruption of the Pearson correlation coeffcient by measurement error and its estimation bias, and correction under different error models., Sci Rep. 2020;10:438. 19. Bishara AJ, Hittner JB. Reducing bias and error in the correlation coeffcient due to nonnormality. Educ Psychol Measur. 2015;75:785804. 20. Cormack GV, Lynam TR. Power and bias of subset pooling strategies. In proceedings of the 30th annual international ACM SIGIR Conference on research and development in information retrieval (association for computing machinery, New York,  NY,  USA,  July 2007),  837838. https, // doi., org/ 10. 1145/ 12777 41. 12779 34. 21. Prinz H. Hill coeffcients, dose response curves and allosteric mechanisms. J Chem Biol. 2010;3:3744. 22. Beam AL, Motsinger Reif AA. Optimization of nonlinear dose  and concentrationresponse models utilizing evolu tionary computation., Dose Response. 2011;9:387409. 23. Haibe Kains B, et al. Inconsistency in large pharmacogenomic studies. Nature. 2013;504:38993. 24. Safikhani Z, Selby H, Sayad A, Hatzis C, Haibe Kains B. In High Throughput Screening Methods, Dec. 2016;181213. https:// doi. org/ 10. 1039/ 97817 82626 770 00181. 25. Knight WR. A computer method for calculating Kendalls Tau with ungrouped data. J Am Stat Assoc. 1966;61:4369. 26. Noether GE. Elements of nonparametric statistics. (John Wiley & Sons, Jan. 1967). 27. Pencina MJ, DAgostino RB. Overall C as a measure of discrimination in survival analysis model specific population value and confidence interval estimation., Stat Med. 2004;23:210923. Page 24 of 24 Smirnov et al. BMC Bioinformatics (2022) 23188  fast, convenient online submission  thorough peer review by experienced researchers in your field  rapid publication on acceptance  support for research data, including large and complex data types  gold Open Access which fosters wider collaboration and increased citations maximum visibility for your research, over 100M website views per year  At BMC,  research is always in progress., Learn more biomedcentral.com/submissionsReady to submit y our researc h Ready to submit y our researc h ?, Choose BMC and benefit fr om: ? Choose BMC and benefit fr om: 28. David ST, Kendall MG, Stuart A. Some questions of distribution in the theory of rank correlation. Biometrika. 1951;38:13140. 29. Hayes AF, Permutation test is not distributionfree: testing H/Em 0:=0 . Psychol Methods 1, 184. (19960101). 30. Hecker J, et al. A flexible and nearly optimal sequential testing approach to randomized testing: QUICK STOP . Genet Epidemiol. 2020;44:13947. 31. Guo W Peddada S. Adaptive choice of the number of bootstrap samples in large scale multiple testing., Statistical applications in genetics and molecular biology 7, Article13. 2008. 32. Iorio F, et al. A landscape of pharmacogenomic interactions in cancer. Cell. 2016;166:74054. 33. Yang W, et al. Genomics of drug sensitivity in cancer (GDSC) a resource for therapeutic biomarker discovery in cancer cells., Nucleic Acids Res. 2013;41:95561. 34. Garnett MJ, et al. Systematic identification of genomic markers of drug sensitivity in cancer cells. Nature. 2012;483:5705. 35. Seashore Ludlow B, et al. Harnessing connectivity in a large scale smallmolecule sensitivity dataset. Cancer Discov. 2015;5:121023. 36. Rees MG, et al. Correlating chemical sensitivity and basal gene expression reveals mechanism of action. Nat Chem Biol. 2016;12:10916. 37. Basu A, et al. An interactive resource to identify cancer genetic and lineage dependencies targeted by small mol ecules., Cell. 2013;154:115161. 38. Mpindi JP , et al. Consistency in drug response profiling. Nature. 2016;540:E56. 39. Hafner M, et al. Quantification of sensitivity and resistance of breast cancer cell lines to anti cancer drugs using gr metrics., Scientific Data. https:// doi. org/ 10. 1038/ sdata. 2017. 166 2017. 40. Smirnov P , et al. PharmacoGx: an R package for analysis of large pharmacogenomic datasets. Bioinformatics. 2016;32:12446. 41. Mammoliti A, et al. Orchestrating and sharing large multimodal data for transparent and reproducible research. bioRxiv, 2020.09.18.303842. 2021. 42. Safikhani Z, et al. Revisiting inconsistency in large pharmacogenomic studies. F1000Research 5, 2333. 2017. 43. Margolius BH. Permutations with inversions. J Integer Seq 4. https:// cs. uwate rloo. ca/ journ als/ JIS/ VOL4/ MARGO LIUS/ inver sions. pdf 2001. 44. R Core Team. R: A Language and environment for statistical computing manual. R Foundation for Statistical Com puting (Vienna, Austria, 2020). 45. MacMahon PA. Combinatory analysis Volumes I and II isbn,  9780821828328 (American Mathematical Soc., 2001)., 46. Remmel JB, Wilson AT. An extension of MacMahons equidistribution theorem to ordered set partitions. J Combin Theory Ser A. 2015;134:24277. 47. Olkin I, Trikalinos TA. Constructions for a bivariate beta distribution. arXiv: 1406. 5881 [math, stat]. June 2014. 48. Storey JD. A direct approach to false discovery rates. J R Stat Soc B. 2002;64:47998. Publishers Note Springer Nature remains neutral with regard to jurisdictional claims in published maps and institutional affiliations.,",
  "qa_pairs": [
    {
      "question": "What is the main objective of the research in this paper?",
      "answer": "The main objective of the research in this paper is to evaluate and improve statistical approaches for association testing in noisy drug screening data, specifically in the context of drug sensitivity data from cancer cell lines. The authors introduce two novel statistical measures, the robust concordance index (rCI) and the kernelized concordance index (kCI), which are designed to account for measurement noise in biological data. They aim to assess the performance of these new measures compared to traditional correlation coefficients (such as Pearson and Spearman) in terms of their power to detect significant associations, while also addressing issues related to false positive rates in statistical testing. The study ultimately seeks to enhance the reliability of association testing in high-dimensional biological datasets, which is crucial for informing patient treatment in pharmacogenomics."
    },
    {
      "question": "Can you summarize the abstract of the paper?",
      "answer": "The abstract of the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" summarizes the challenges of identifying associations among biological variables, particularly in drug sensitivity data, which is often affected by systemic and statistical noise. The authors introduce two new semiparametric variations of the concordance index, namely the robust concordance index (rCI) and the kernelized concordance index (kCI), which account for noise distribution in the data. They demonstrate that traditional statistical tests applied to these indices often fail to control for false positives and propose efficient methods for computing p-values using adaptive permutation testing. The study evaluates the statistical power of these new indices in simulations and compares them with Pearson and Spearman correlation coefficients. The findings indicate that rCI and kCI outperform the standard concordance index in simulations and show some improvement in real data, while Pearson correlation remains robust against measurement noise."
    },
    {
      "question": "What are the softwares and computational tools that were used in this paper?",
      "answer": "The research paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" mentions the development of an open-source R software package called **wCI (weighted Concordance Index)**. This package implements three statistics: the Concordance Index (CI), the Robust Concordance Index (rCI), and the Kernelized Concordance Index (kCI). The software is freely available on GitHub under the GPLv3 license.\n\nAdditionally, the paper indicates that power analysis simulations were conducted using the **R programming language**, specifically utilizing the **MASS R package** for generating bivariate normal data. The authors also mention the use of the **PharmacoGx R package** for downloading pharmacogenomic datasets and summarizing dose-response curves.\n\nIn summary, the primary software and computational tools used in the paper are:\n1. **R programming language**\n2. **wCI package** (open-source R package for computing CI, rCI, and kCI)\n3. **MASS R package** (for generating bivariate normal data)\n4. **PharmacoGx R package** (for pharmacogenomic data analysis)"
    },
    {
      "question": "Describe the methodology used in the paper.",
      "answer": "The methodology used in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. involves several key components aimed at assessing and improving statistical approaches for identifying associations in high-dimensional biological datasets, particularly in the context of drug sensitivity screening. Here\u2019s a detailed breakdown of the methodology:\n\n1. **Introduction of New Statistical Measures**:\n   - The authors propose two novel semiparametric variations of the commonly used concordance index: the Robust Concordance Index (rCI) and the Kernelized Concordance Index (kCI). These indices are designed to incorporate information about the noise distribution in the data, which is critical in biological measurements that often contain significant noise.\n\n2. **Evaluation of Existing Statistical Tests**:\n   - The paper evaluates the performance of common statistical tests applied to the concordance index and its variations. The authors find that these tests often fail to control for false positives, especially in the presence of noise, which is a significant concern in biological data analysis.\n\n3. **Permutation Testing**:\n   - To address the limitations of traditional statistical tests, the authors introduce adaptive permutation testing as a method to compute p-values. This approach does not rely on parametric assumptions about the data distribution and is shown to be more reliable for assessing statistical significance in the context of high-dimensional datasets.\n\n4. **Simulation Studies**:\n   - The authors conduct extensive simulation studies to evaluate the statistical power of the proposed indices (rCI and kCI) compared to traditional correlation coefficients like Pearson and Spearman. These simulations involve generating data from various distributions, including normal and beta distributions, and assessing the performance of the different statistical measures under varying conditions of noise and sample sizes.\n\n5. **Benchmarking Against Real Data**:\n   - The methodology includes benchmarking the performance of the proposed indices against real pharmacogenomic datasets. The authors compute the similarity between drug response vectors across different studies and assess the ability of each statistical measure to rank matched drugs effectively.\n\n6. **Characterization of Null Distributions**:\n   - The authors characterize the null distributions for the concordance index and its variations, particularly in cases with ties and measurement noise. They develop exact null distributions for specific cases and advocate for permutation testing when the exact distributions are unknown.\n\n7. **Software Implementation**:\n   - An open-source R package named wCI is developed to facilitate the computation of the concordance index, rCI, and kCI. This package allows for efficient calculation of these statistics and is made publicly available for use by the research community.\n\n8. **Parameter Optimization**:\n   - The methodology includes a systematic approach to optimize the parameters for the rCI and kCI based on empirical data from pharmacogenomic studies. This ensures that the indices are tailored to the specific characteristics of the datasets being analyzed.\n\nIn conclusion, the methodology of the paper combines theoretical advancements in statistical measures with practical applications in drug sensitivity screening, supported by rigorous simulation studies and real data analysis, ultimately advocating for the use of permutation testing as a robust approach for significance testing in high-dimensional biological data."
    },
    {
      "question": "What are the key findings of the paper?",
      "answer": "The key findings of the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. can be summarized as follows:\n\n1. **Introduction of Novel Metrics**: The authors introduced two semiparametric variations of the commonly used concordance index: the robust concordance index (rCI) and the kernelized concordance index (kCI). These metrics are designed to incorporate information about the noise distribution in the data, which is particularly relevant in the context of drug sensitivity data.\n\n2. **Limitations of Common Statistical Tests**: The study found that common statistical tests applied to the concordance index and its variations often fail to control for false positives, especially in smaller sample sizes. This highlights the need for more robust testing methods.\n\n3. **Permutation Testing**: The authors developed efficient implementations for computing p-values using adaptive permutation testing, which they argue is a more reliable method for assessing significance in the presence of noise.\n\n4. **Statistical Power Analysis**: Through simulations, the authors evaluated the statistical power of the rCI and kCI compared to traditional Pearson and Spearman correlation coefficients. They found that the rCI and kCI generally exhibited better power than the standard concordance index, particularly in noisy data scenarios.\n\n5. **Robustness of Pearson Correlation**: Surprisingly, the Pearson correlation coefficient was found to be the most robust to measurement noise among the metrics tested, suggesting that it remains a strong choice for detecting linear associations even in the presence of noise.\n\n6. **Application to Pharmacogenomic Data**: The performance of the various statistics was evaluated using real pharmacogenomic datasets. The results indicated that while the kCI showed some improvement over the standard concordance index and Spearman correlation, the Pearson correlation consistently demonstrated the best performance in terms of recall when matching drugs across datasets.\n\n7. **Recommendations for Researchers**: The authors advocate for the use of permutation testing as the default approach for assessing significance in correlation tests, particularly in high-throughput drug screening data. They conclude that despite the non-normality of the data, the Pearson correlation remains a reliable measure of association.\n\nOverall, the paper emphasizes the importance of robust statistical methods in the analysis of high-dimensional biological data, particularly in the context of drug sensitivity screening."
    },
    {
      "question": "How was the data analyzed in the study?",
      "answer": "In the study, the data was analyzed using a combination of statistical approaches, primarily focusing on association testing in noisy drug screening data. The researchers introduced two novel semiparametric variations of the commonly used concordance index: the robust concordance index (rCI) and the kernelized concordance index (kCI). These indices were designed to incorporate measurements about the noise distribution present in the data.\n\nThe analysis involved several key steps:\n\n1. **Statistical Testing**: The study evaluated the performance of common statistical tests applied to the concordance index and its variations, highlighting that these tests often failed to control for false positives. To address this, the authors introduced efficient implementations for computing p-values using adaptive permutation testing.\n\n2. **Simulation Studies**: The researchers conducted simulations to assess the statistical power of the proposed coefficients (rCI and kCI) compared to traditional correlation coefficients like Pearson and Spearman. They simulated data under various conditions, including the addition of noise, to evaluate how well each statistic performed in detecting significant associations.\n\n3. **Power Analysis**: The power of the different correlation coefficients was analyzed through simulations, where they compared the ability of each statistic to detect associations at fixed significance levels. This included examining the effects of sample size and effect size on the power of the tests.\n\n4. **Real Data Evaluation**: Finally, the study applied the various statistics to real pharmacogenomic datasets to assess their performance in matching drugs across different studies. The results were summarized using the area under the empirical cumulative distribution function (CDF) of the ranks of matched drugs, allowing for a comparison of the effectiveness of the different metrics.\n\nOverall, the analysis emphasized the importance of using permutation testing to derive accurate p-values and demonstrated that the proposed rCI and kCI statistics could improve the robustness of association testing in the presence of measurement noise."
    },
    {
      "question": "Was the data in the study pre-processed in anyway? If so how?",
      "answer": "Yes, the data in the study was pre-processed in several ways. The key pre-processing steps mentioned in the paper include:\n\n1. **Normalization of Area Above the Curve (AAC)**: The AAC values, which summarize the sensitivity of cancer cell lines to drug treatments, were normalized by the total possible area determined by the range of concentrations measured. This normalization leads to values lying in a bounded range between 0 and 1.\n\n2. **Transformation and Normalization**: The paper discusses that direct measurements often need to be transformed and/or normalized before they can be interpreted and compared with results from other experiments. This is particularly important given the non-standard distributions of observed values and the noisiness of the measurements in high-throughput phenotypic measurements.\n\n3. **Handling Measurement Noise**: The study emphasizes the importance of accounting for measurement noise, which is inherent in biological data. By taking replicate measurements of the same conditions, the researchers could directly measure the degree of noise introduced by the experimental protocol. This information was then incorporated into the statistical analyses through the development of the Robust Concordance Index (rCI) and Kernelized Concordance Index (kCI).\n\n4. **Use of Replicate Measurements**: The thresholds for the rCI and the kernel parameters for the kCI were optimized based on empirical data from replicate measurements, allowing the researchers to characterize the noise distribution from the data effectively.\n\nThese pre-processing steps were crucial for ensuring that the statistical analyses conducted in the study were robust and reliable, particularly in the context of noisy biological data."
    },
    {
      "question": "What conclusions were drawn in the paper?",
      "answer": "The conclusions drawn in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. can be summarized as follows:\n\n1. **Performance of New Indices**: The study introduces two novel statistical measures, the robust concordance index (rCI) and the kernelized concordance index (kCI), which are designed to improve the detection of associations in the presence of noise in biological data. The findings indicate that both rCI and kCI demonstrate better statistical power compared to the traditional concordance index (CI) in simulated environments and show some improvement in real data applications.\n\n2. **Robustness of Pearson Correlation**: Surprisingly, the Pearson correlation coefficient was found to be the most robust against measurement noise among the various metrics evaluated, including the proposed rCI and kCI. This suggests that despite its parametric assumptions, Pearson correlation remains a strong choice for detecting linear associations in noisy datasets.\n\n3. **Type I Error Control**: The paper highlights that common statistical tests applied to the concordance index fail to adequately control for false positives, especially in smaller sample sizes. The authors advocate for the use of permutation testing as a more reliable method for assessing significance in correlation tests, particularly in high-throughput biological data analysis.\n\n4. **Practical Implications for Drug Sensitivity Studies**: In the context of pharmacogenomic datasets, the study demonstrates that incorporating noise characteristics into the analysis (via kCI) can enhance the identification of drug sensitivity associations. However, the performance of the Pearson correlation and kCI was noted to be superior in terms of ranking matched drugs across datasets.\n\n5. **Recommendations for Researchers**: The authors recommend that researchers working with high-throughput drug screening data utilize the Pearson correlation for measuring correlation, with significance estimates derived from permutation tests. This approach is suggested to be the most effective for controlling false positives and achieving reliable results in the analysis of drug sensitivity data.\n\nOverall, the paper emphasizes the importance of selecting appropriate statistical methods for association testing in the presence of noise and provides evidence supporting the continued use of Pearson correlation alongside newer methods like rCI and kCI."
    },
    {
      "question": "Can you provide a summary of the literature review from the paper?",
      "answer": "The literature review in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. discusses several key themes and findings relevant to the analysis of high-dimensional biological data, particularly in the context of drug sensitivity testing. \n\n1. **Challenges in Biological Data Analysis**: The review highlights the inherent challenges in analyzing biological data, such as non-standard distributions and measurement noise, especially in high-throughput phenotypic measurements. It emphasizes the need for data transformations and normalization to make the data interpretable.\n\n2. **Correlation Coefficients**: The authors discuss various correlation coefficients used to assess associations between biological variables, including Pearson's Product Moment Correlation, Spearman's Rank Correlation, and the Concordance Index (CI). They note that while Pearson's correlation is widely used, it has limitations, particularly regarding its assumptions of normality and sensitivity to outliers.\n\n3. **Non-parametric Measures**: The review mentions non-parametric measures like Spearman's correlation and CI, which are advantageous for handling non-normal data. However, it points out that these measures may have lower power in detecting significant linear associations compared to parametric methods.\n\n4. **Previous Studies**: The authors reference previous studies that have examined the performance of correlation coefficients under various conditions, including the presence of noise and non-normal distributions. They note that while some studies have explored the power of permutation tests for correlation coefficients, there has been limited investigation into the performance of these tests in the context of skewed and bounded distributions typical in biological data.\n\n5. **Modifications to CI**: The literature review also discusses prior work on modifications to the CI, which have primarily focused on performance metrics rather than correlation measures. The authors argue that their proposed modifications, the Robust Concordance Index (rCI) and Kernelized Concordance Index (kCI), are novel contributions that incorporate noise measurement into the assessment of similarity between biological measurements.\n\n6. **Need for Robust Testing Methods**: The review concludes by advocating for the use of permutation testing as a more reliable method for assessing significance in correlation analyses, particularly in the context of high-throughput biological data, where traditional analytical methods may lead to inflated false positive rates.\n\nOverall, the literature review sets the stage for the authors' investigation into the performance of various statistical approaches in the context of noisy drug screening data, emphasizing the importance of robust statistical methods in biological research."
    },
    {
      "question": "What future research directions do the authors suggest in the paper?",
      "answer": "The authors suggest several future research directions in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data.\" Here are the key points:\n\n1. **Exploration of Non-linear Relationships**: The authors acknowledge that their current work primarily focuses on detecting linear associations. They suggest that future research should investigate non-linear effects, which are also significant in biological systems.\n\n2. **Application to Censored Data**: The authors note that their study did not explore the application of the Concordance Index (CI) and its modifications (rCI and kCI) to censored data, which is a common scenario in survival analysis. Future work could address this gap.\n\n3. **Broader Distribution Analysis**: The authors indicate that their simulations were limited to specific distributions (e.g., skewed and bounded distributions). They recommend further exploration of a wider range of distributions, sample sizes, and hyper-parameters to better understand the performance of the proposed metrics.\n\n4. **Optimization of Parameters**: The authors suggest that future studies could focus on optimizing the parameters for the robust and kernelized concordance indices (rCI and kCI) for different datasets and experimental conditions.\n\n5. **Integration with Other Statistical Methods**: The authors propose that integrating their methods with other statistical approaches could enhance the robustness and applicability of association testing in high-throughput biological data.\n\n6. **Real-world Applications**: They encourage further validation of their proposed methods on diverse real-world datasets beyond pharmacogenomic studies to assess their generalizability and effectiveness.\n\nThese directions highlight the need for continued research to refine statistical methods for analyzing complex biological data, particularly in the context of drug sensitivity and other high-dimensional datasets."
    },
    {
      "question": "What statistical techniques were used in the paper?",
      "answer": "The paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" employs several statistical techniques, including:\n\n1. **Concordance Index (CI)**: A non-parametric measure of association that assesses the degree of agreement between two variables.\n\n2. **Robust Concordance Index (rCI)**: A modification of the CI that incorporates thresholds to ignore pairs of observations that are too similar, thus increasing robustness to noise.\n\n3. **Kernelized Concordance Index (kCI)**: A further generalization of the rCI that assigns weights to pairs based on the differences between their values, allowing for a more nuanced assessment of similarity.\n\n4. **Pearson Correlation Coefficient**: A widely used parametric measure of linear correlation between two variables.\n\n5. **Spearman Rank Correlation**: A non-parametric measure that assesses how well the relationship between two variables can be described using a monotonic function.\n\n6. **Permutation Testing**: An adaptive permutation testing method is introduced to compute p-values for the association tests, which helps control for false positives and does not rely on strong parametric assumptions about the data distribution.\n\n7. **Power Analysis**: The paper evaluates the statistical power of the proposed indices (rCI and kCI) and compares them with Pearson and Spearman correlations under various simulated conditions, including the presence of noise.\n\nThese techniques are evaluated in the context of drug sensitivity data, focusing on their robustness and effectiveness in identifying significant associations amidst noise in biological measurements."
    },
    {
      "question": "Can you describe the experimental setup in the paper?",
      "answer": "The experimental setup described in the paper involves evaluating statistical approaches for association testing in noisy drug screening data, specifically focusing on high-throughput drug sensitivity screening in cancer cell lines. Here are the key components of the experimental setup:\n\n1. **Data Sources**: The study utilized data from multiple pharmacogenomic datasets, including the Genomics of Drug Sensitivity in Cancer (GDSC), the Cancer Therapeutics Response Portal, and others. These datasets contain information on cancer cell lines treated with various compounds at different concentrations.\n\n2. **Drug Sensitivity Measurement**: Each cancer cell line was treated with compounds at several concentrations, and the cell viability was measured at a specific time point. The viability values were compared to matched untreated controls to derive a percentage viability for each concentration level.\n\n3. **Statistical Measures**: The paper introduced two novel statistical measures, the Robust Concordance Index (rCI) and the Kernelized Concordance Index (kCI), designed to account for measurement noise in the data. These measures were compared against traditional metrics like Pearson and Spearman correlation coefficients.\n\n4. **Simulation Studies**: The authors conducted simulation studies to evaluate the statistical power of the proposed measures under various conditions, including the presence of noise. They generated synthetic datasets with known correlations and applied permutation tests to assess the performance of different correlation coefficients.\n\n5. **Power Analysis**: The power of the statistical tests was analyzed by varying sample sizes and effect sizes. The authors aimed to determine how well each statistic could detect significant associations in the presence of noise and non-normal distributions.\n\n6. **Implementation**: An open-source R package, wCI, was developed to compute the concordance index, rCI, and kCI efficiently. The package allows for the implementation of the proposed statistical methods in practical applications.\n\n7. **Benchmarking**: The performance of the new metrics was benchmarked against existing correlation coefficients using real pharmacogenomic data, focusing on the ability to rank matched drugs across different studies.\n\nOverall, the experimental setup emphasizes the integration of statistical testing with real-world biological data, addressing the challenges posed by measurement noise and the need for robust association testing in high-throughput drug screening contexts."
    },
    {
      "question": "What are the implications of the research findings?",
      "answer": "The research findings from the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" have several important implications for the field of pharmacogenomics and the analysis of high-dimensional biological data:\n\n1. **Improved Statistical Methods**: The introduction of the robust concordance index (rCI) and kernelized concordance index (kCI) provides researchers with more powerful tools for detecting associations in noisy data. These methods incorporate information about the noise distribution, which is crucial in biological datasets where measurement noise is prevalent.\n\n2. **False Positive Control**: The study highlights the inadequacy of traditional statistical tests when applied to the concordance index and its variations, particularly in controlling false positives. This finding underscores the necessity of using adaptive permutation testing as a more reliable method for significance testing in high-throughput biological data.\n\n3. **Power Analysis**: The research demonstrates that the rCI and kCI are better powered than the standard concordance index in simulations and show improvements in real data scenarios. This suggests that these new indices can enhance the detection of true associations, which is vital for identifying effective biomarkers and drug sensitivities.\n\n4. **Robustness to Measurement Noise**: The findings indicate that the Pearson correlation coefficient, despite being a traditional method, is surprisingly robust to measurement noise compared to other correlation metrics. This suggests that it may still be a valuable tool in certain contexts, particularly when data is not heavily skewed or bounded.\n\n5. **Practical Applications in Drug Sensitivity Screening**: The evaluation of these statistical methods in the context of drug sensitivity data emphasizes their potential to improve the identification of effective treatments for cancer patients. By enhancing the ability to detect true drug responses across different datasets, these methods could lead to better personalized medicine approaches.\n\n6. **Recommendations for Researchers**: The authors advocate for the adoption of permutation testing as the default approach for assessing significance in correlation tests, particularly in biological research where data often deviates from normality. This recommendation could lead to more accurate conclusions in studies involving high-dimensional datasets.\n\n7. **Future Research Directions**: The study opens avenues for further exploration of non-linear associations and the application of these statistical methods to other types of biological data, potentially broadening their impact beyond drug sensitivity studies.\n\nIn summary, the implications of this research are significant for improving statistical analyses in pharmacogenomics, enhancing the reliability of findings in noisy biological datasets, and ultimately contributing to more effective patient treatment strategies."
    },
    {
      "question": "What are the limitations and delimitations mentioned in the paper?",
      "answer": "The paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. discusses several limitations and delimitations in its research. Here are the key points:\n\n### Limitations:\n1. **Focus on Linear Associations**: The study primarily investigates linear associations, acknowledging that non-linear effects are also significant in biological contexts but were not explored in this work.\n   \n2. **Exclusion of Censored Data**: The analysis omits the application of the Concordance Index (CI) and its derivatives to censored data, which is a common scenario in biological data analysis.\n\n3. **Computational Constraints**: Due to computational limitations, the authors could not exhaustively explore the entire space of distributions, sample sizes, hyper-parameters, and effect strengths, which may affect the generalizability of their findings.\n\n4. **Power of Tests**: The study notes that while permutation tests are advocated, the power of these tests can be sensitive to the sample size and the specific characteristics of the data, which may not always yield optimal results.\n\n### Delimitations:\n1. **Specific Application Context**: The research is motivated by preclinical drug sensitivity screening, which provides a specific context for the analysis and may limit the applicability of the findings to other types of biological data.\n\n2. **Statistical Methods Focus**: The study focuses on evaluating specific statistical methods (e.g., robust concordance index and kernelized concordance index) and their performance compared to traditional methods like Pearson and Spearman correlations, which may not encompass all potential statistical approaches.\n\n3. **Sample Size and Effect Size**: The analysis is conducted under certain sample sizes and effect sizes, which were predetermined for the simulations, potentially limiting the exploration of other configurations that could yield different insights.\n\nThese limitations and delimitations highlight the scope of the research and the specific conditions under which the findings are applicable, guiding future research directions and considerations."
    },
    {
      "question": "What recommendations do the authors make in the paper?",
      "answer": "The authors of the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" make several key recommendations based on their findings:\n\n1. **Adoption of Permutation Testing**: The authors advocate for the use of permutation testing as the default method for assessing significance in correlation tests. They emphasize that permutation tests provide more reliable p-value estimates, particularly in the context of high-throughput biological data, where traditional asymptotic methods may inflate false positive rates.\n\n2. **Use of Robust and Kernelized Concordance Indices**: The authors introduce two novel statistics, the Robust Concordance Index (rCI) and the Kernelized Concordance Index (kCI), which incorporate information about measurement noise. They recommend these indices for situations where data is noisy, as they demonstrated improved performance over traditional correlation coefficients in detecting significant associations.\n\n3. **Preference for Pearson Correlation**: Despite the introduction of new metrics, the authors conclude that the Pearson correlation remains the most powerful statistic for detecting linear associations in normally distributed data. They suggest that researchers should continue to use Pearson correlation for its robustness, particularly when significance estimates are derived from permutation tests.\n\n4. **Consideration of Sample Size and Effect Size**: The authors highlight the importance of sample size and effect size in determining the power of different statistical tests. They recommend careful consideration of these factors when designing studies and interpreting results.\n\n5. **Caution with Analytical p-Values**: The authors caution against relying on analytical p-values derived from asymptotic distributions, especially in cases of small sample sizes or non-normal data distributions, as these can lead to misleading conclusions.\n\nOverall, the authors emphasize the need for robust statistical methods tailored to the complexities of biological data, advocating for a shift towards more reliable testing strategies in the field of pharmacogenomics."
    },
    {
      "question": "Who funded the research in the paper?",
      "answer": "The research in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" was funded by the Academy of Finland (Grants 326238, 340141, and 344698 to Tero Aittokallio), the European Union's Horizon 2020 Research and Innovation Programme (ERA PerMed JAKSTAT TARGET), and the Cancer Society of Finland (Tero Aittokallio)."
    },
    {
      "question": "Is there any conflict of interest disclosed in the paper?",
      "answer": "Yes, there is a conflict of interest disclosed in the paper. The authors mention that Benjamin Haibe-Kains (BHK) is a shareholder and paid consultant for Code Ocean Inc. However, the authors state that they have no other competing interests to declare."
    },
    {
      "question": "What ethical considerations are discussed in the paper?",
      "answer": "The paper discusses several ethical considerations related to the analysis of high-dimensional biological data, particularly in the context of drug sensitivity screening. Here are the key ethical points highlighted:\n\n1. **Data Integrity and Accuracy**: The authors emphasize the importance of accurate statistical methods to ensure that the conclusions drawn from biological data are valid. Misleading results due to incorrect statistical testing can lead to inappropriate clinical decisions, which raises ethical concerns about patient safety and treatment efficacy.\n\n2. **Multiple Testing and False Positives**: The paper discusses the challenges of multiple hypothesis testing in biological research, where thousands of correlations may be tested simultaneously. The authors advocate for robust statistical methods, such as permutation testing, to control the false discovery rate. This is crucial to prevent false positives that could misguide research and clinical practices.\n\n3. **Transparency and Reproducibility**: The authors highlight the need for transparency in the methods used for data analysis. They provide open-source software for their statistical methods, promoting reproducibility in research. Ethical research practices require that other scientists can verify and replicate findings.\n\n4. **Use of Human Data**: While the paper focuses on computational methods, it indirectly touches on the ethical implications of using human-derived data in pharmacogenomic studies. Researchers must ensure that such data is collected and used in compliance with ethical standards, including informed consent and respect for privacy.\n\n5. **Impact on Patient Treatment**: The ultimate goal of the research is to improve drug sensitivity predictions, which directly impacts patient treatment decisions. Ethical considerations arise regarding the potential consequences of incorrect predictions on patient outcomes, emphasizing the responsibility of researchers to ensure their findings are reliable and applicable.\n\n6. **Conflict of Interest**: The paper discloses potential conflicts of interest, such as one of the authors being a shareholder and paid consultant for a company. Ethical research requires transparency about financial interests that could influence study outcomes or interpretations.\n\nOverall, the paper underscores the importance of ethical considerations in the analysis of biological data, particularly regarding accuracy, transparency, and the potential impact on patient care."
    },
    {
      "question": "Which studies are most frequently cited in the paper?",
      "answer": "The most frequently cited studies in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. include:\n\n1. **Bishara and Hittner (2012)** - This study is referenced for its comprehensive comparison of Type 1 error control and power between several parametric and resampling-based tests for the Pearson and Spearman correlations.\n2. **Puth et al. (2014)** - This work follows up on the study by Bishara and Hittner and is cited for similar reasons regarding correlation testing.\n3. **Chok's work (2010)** - This study is mentioned in the context of correlation coefficients but is noted as an exception in the comparisons made in previous studies.\n4. **Studies on the effect of assay/measurement noise (2016)** - This reference discusses the impact of measurement noise on statistical power and bias in correlation coefficients.\n5. **Noether (1967)** - Cited for introducing analytical formulations for the distribution under the null hypothesis for the concordance index.\n6. **Pencina and D'Agostino (2004)** - This study is referenced for its work on confidence intervals in the presence of ties due to right censoring.\n7. **David et al. (1951)** - Cited for their work on statistical testing involving rank correlation.\n8. **Haverty et al. (2016)** - This study discusses reproducibility in pharmacogenomic profiling and is relevant to the context of drug sensitivity data.\n\nThese studies are frequently referenced throughout the paper to support various claims and methodologies related to statistical testing and correlation analysis in biological data."
    },
    {
      "question": "Can you explain the technical terms used in the paper?",
      "answer": "Certainly! Here are explanations of some of the technical terms used in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\":\n\n1. **High-dimensional datasets**: Refers to datasets that have a large number of variables (features) compared to the number of observations (samples). This is common in biological research where many measurements are taken simultaneously.\n\n2. **Statistical noise**: Random variations in data that can obscure the true signal or relationship between variables. In biological systems, this noise can arise from measurement errors, biological variability, and other factors.\n\n3. **Concordance Index (CI)**: A statistical measure used to assess the agreement between two rankings or measurements. It is defined as the proportion of pairs of observations that are concordant (i.e., ordered in the same way) versus discordant.\n\n4. **Robust Concordance Index (rCI)**: A modified version of the CI that accounts for measurement noise by only considering pairs of observations that are sufficiently dissimilar. This helps to improve the reliability of the association measure in the presence of noise.\n\n5. **Kernelized Concordance Index (kCI)**: An extension of the rCI that assigns weights to pairs of observations based on their differences. This allows for a more nuanced assessment of similarity, where larger differences contribute more to the similarity measure.\n\n6. **Permutation testing**: A non-parametric statistical method used to assess the significance of an observed statistic by comparing it to a distribution generated by permuting the data. This approach does not rely on assumptions about the underlying distribution of the data.\n\n7. **p-values**: A statistical measure that helps to determine the significance of results. A low p-value indicates strong evidence against the null hypothesis, suggesting that the observed effect is unlikely to have occurred by chance.\n\n8. **Power analysis**: A method used to determine the likelihood that a study will detect an effect when there is an effect to be detected. It involves calculating the probability of correctly rejecting the null hypothesis.\n\n9. **Pearson correlation coefficient**: A measure of the linear correlation between two variables, indicating the strength and direction of their relationship. It assumes that the data are normally distributed.\n\n10. **Spearman rank correlation**: A non-parametric measure of correlation that assesses how well the relationship between two variables can be described using a monotonic function. It is based on the ranks of the data rather than their raw values.\n\n11. **Biomarker**: A biological molecule found in blood, other body fluids, or tissues that can indicate a normal or abnormal process, or a condition or disease. In this context, it refers to indicators of drug sensitivity in cancer treatment.\n\n12. **Pharmacogenomics**: The study of how genes affect a person's response to drugs. This field combines pharmacology (the science of drugs) and genomics (the study of genes and their functions) to develop effective, safe medications tailored to a person's genetic makeup.\n\n13. **Hill Curve**: A mathematical model used to describe the dose-response relationship of a drug, often used in pharmacology to understand how different concentrations of a drug affect cell viability.\n\n14. **Area Above the Curve (AAC)**: A summary metric derived from dose-response curves that quantifies the overall effect of a drug across a range of concentrations. It is used to assess drug potency and efficacy.\n\n15. **Type I error**: The incorrect rejection of a true null hypothesis (false positive). In the context of this paper, it refers to incorrectly identifying an association when none exists.\n\n16. **False Discovery Rate (FDR)**: A method used in multiple hypothesis testing to control the expected proportion of incorrectly rejected null hypotheses (false positives).\n\nThese terms are integral to understanding the statistical methodologies and analyses presented in the paper, particularly in the context of evaluating drug sensitivity data in cancer research."
    },
    {
      "question": "What data sources were used in the paper, and are they accessible for further research?",
      "answer": "The research paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. utilized several data sources for their analysis, specifically focusing on pharmacogenomic datasets. The datasets mentioned in the paper include:\n\n1. **Genomics of Drug Sensitivity in Cancer (GDSC)** - versions 1 and 2.\n2. **Cancer Therapeutics Response Portal (CTRP) v2**.\n3. **Genentech Cell Screening Initiative**.\n4. **Finnish Institute of Medicine cell line screening dataset**.\n5. **Oregon Health and Science University Breast Cancer Screen**.\n\nAll the data used in this study was previously published, except for the data from the Genentech Cell Screening Initiative, where an updated version of the original dataset was used. The datasets were downloaded using the PharmacoGx R package from the orcestra.ca platform.\n\nRegarding accessibility for further research, the paper states that the pharmacogenomic datasets used in the analysis are available via the orcestra.ca platform. Additionally, the authors have made their analysis code available in an open-source R package called **wCI** (weighted Concordance Index), which can be accessed on GitHub at https://github.com/bhklab/wCI. This package and the associated analysis code are freely available, allowing other researchers to utilize the methods and datasets for further research."
    },
    {
      "question": "Can you summarize the research paper?",
      "answer": "The research paper titled \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. focuses on the challenges of identifying associations in high-dimensional biological datasets, particularly in the context of drug sensitivity data. The authors highlight the systemic and statistical noise present in biological systems, which complicates the task of association testing.\n\nTo address these challenges, the paper introduces two novel statistical measures: the robust concordance index (rCI) and the kernelized concordance index (kCI). These measures are designed to incorporate information about the noise distribution in the data, aiming to improve the detection of significant associations. The authors demonstrate that traditional statistical tests applied to the concordance index and its variations often fail to control for false positives, leading to unreliable results.\n\nThe study evaluates the statistical power of these new coefficients through simulations and compares them with established correlation coefficients such as Pearson and Spearman. The results indicate that the rCI and kCI exhibit better power than the standard concordance index in simulated scenarios and show some improvement in real data applications. Interestingly, the Pearson correlation coefficient was found to be the most robust against measurement noise among the tested metrics.\n\nThe paper concludes that while the rCI and kCI provide useful enhancements for association testing in noisy data, the Pearson correlation remains a strong choice for detecting linear associations, particularly when significance is assessed through permutation testing. The authors advocate for the adoption of permutation testing as a standard approach for evaluating significance in correlation analyses, especially in high-throughput biological data contexts."
    },
    {
      "question": "What is the significance of this research in its field?",
      "answer": "The significance of the research presented in the paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" lies in its contribution to the field of bioinformatics and pharmacogenomics, particularly in the analysis of high-dimensional biological data. Here are the key points highlighting its significance:\n\n1. **Addressing Noise in Biological Data**: The research tackles the pervasive issue of noise in biological measurements, especially in drug sensitivity data. By introducing robust statistical methods that account for measurement noise, the study enhances the reliability of association testing in noisy datasets, which is crucial for accurate biological interpretations.\n\n2. **Development of Novel Statistical Metrics**: The introduction of two new semiparametric variations of the concordance index (the robust concordance index (rCI) and the kernelized concordance index (kCI)) provides researchers with advanced tools for assessing associations in high-throughput data. These metrics are specifically designed to improve power and control false positives, which are common pitfalls in traditional statistical approaches.\n\n3. **Comparative Analysis of Statistical Methods**: The paper conducts a thorough evaluation of various statistical methods, including Pearson and Spearman correlation coefficients, in the context of high-throughput drug screening. By demonstrating the strengths and weaknesses of these methods, the research guides practitioners in selecting the most appropriate statistical tools for their analyses.\n\n4. **Implications for Drug Sensitivity Screening**: The findings have direct implications for pharmacogenomic studies, where identifying significant associations between drug responses and genetic variables is critical for personalized medicine. The improved statistical methods can lead to better identification of biomarkers for drug sensitivity, ultimately aiding in the development of more effective treatment strategies.\n\n5. **Advocacy for Permutation Testing**: The research advocates for the use of permutation testing as a standard approach for significance testing in correlation analyses. This recommendation is based on the demonstrated limitations of traditional analytical methods in controlling false positives, particularly in the context of high-dimensional data.\n\n6. **Contribution to Methodological Rigor**: By providing efficient implementations of the proposed statistical methods and demonstrating their application on real pharmacogenomic datasets, the study contributes to the methodological rigor in the field. This is essential for ensuring reproducibility and reliability in biological research.\n\nIn summary, this research significantly advances the statistical methodologies used in bioinformatics, particularly for analyzing complex and noisy biological data, thereby enhancing the accuracy and reliability of findings in pharmacogenomics and drug sensitivity studies."
    },
    {
      "question": "How does this paper contribute to the existing body of knowledge?",
      "answer": "The paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. contributes to the existing body of knowledge in several significant ways:\n\n1. **Introduction of Novel Statistical Measures**: The authors propose two new semiparametric variations of the commonly used concordance index, namely the robust concordance index (rCI) and the kernelized concordance index (kCI). These measures are designed to better account for the noise inherent in biological data, particularly in the context of drug sensitivity testing.\n\n2. **Evaluation of Existing Methods**: The paper critically evaluates the performance of traditional statistical tests, such as Pearson and Spearman correlation coefficients, when applied to high-dimensional and noisy biological datasets. The authors demonstrate that these common tests often fail to control for false positives, particularly in the context of small sample sizes and non-normal distributions.\n\n3. **Permutation Testing Framework**: The authors introduce an efficient implementation of adaptive permutation testing to compute p-values for the proposed indices and other correlation measures. This approach provides a more reliable method for assessing statistical significance without the stringent assumptions required by traditional parametric tests.\n\n4. **Power Analysis**: The study conducts extensive simulations to compare the statistical power of the proposed rCI and kCI against traditional correlation coefficients. The results indicate that the new indices can outperform existing methods in detecting significant associations in the presence of noise, particularly in high-throughput drug screening scenarios.\n\n5. **Application to Real Data**: The authors validate their proposed methods using real pharmacogenomic datasets, demonstrating their practical utility in identifying drug sensitivity associations across different studies. This application highlights the relevance of their work to ongoing challenges in the field of cancer research and personalized medicine.\n\n6. **Recommendations for Future Research**: The paper advocates for the adoption of permutation testing as a standard approach for assessing significance in correlation analyses, especially in biological contexts where data often deviate from normality. This recommendation is based on their findings regarding the limitations of traditional analytical methods.\n\n7. **Contribution to Methodological Rigor**: By addressing the issues of measurement noise and the need for robust statistical methods in high-dimensional biological data, this paper enhances the methodological rigor in the analysis of drug sensitivity data, which is crucial for advancing personalized medicine.\n\nOverall, this research provides valuable insights and tools for researchers working with complex biological datasets, particularly in the context of pharmacogenomics and drug discovery."
    },
    {
      "question": "Are there any novel techniques or approaches introduced in this paper?",
      "answer": "Yes, the paper introduces two novel techniques for association testing in noisy drug screening data: the **Robust Concordance Index (rCI)** and the **Kernelized Concordance Index (kCI)**. \n\nThese indices are modifications of the commonly used Concordance Index and are designed to incorporate measurements about the noise distribution present in the data. The rCI focuses on pairs of observations that are sufficiently dissimilar, while the kCI generalizes this approach by assigning weights to all pairs based on a kernel function, which reflects the degree of difference between the measurements. \n\nThe authors demonstrate that these new indices are better powered than the traditional Concordance Index in simulations and show some improvement in real data applications, particularly in the context of pharmacogenomic datasets. Additionally, they provide efficient implementations for computing p-values using adaptive permutation testing, addressing issues related to false positives in common statistical tests applied to the concordance index and its variations."
    },
    {
      "question": "What are the potential real-world applications of this research?",
      "answer": "The research paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. discusses several potential real-world applications of their findings, particularly in the field of pharmacogenomics and drug sensitivity testing. Here are the key applications highlighted in the paper:\n\n1. **Improved Drug Sensitivity Testing**: The study introduces two novel statistical measures, the robust concordance index (rCI) and the kernelized concordance index (kCI), which are designed to better handle the noise inherent in biological data. These measures can enhance the accuracy of identifying associations between drug sensitivity and genetic or phenotypic variables in cancer cell lines, potentially leading to more effective personalized treatment strategies.\n\n2. **Cross-Dataset Replicability**: The paper evaluates the performance of the proposed statistical methods in matching drug responses across different pharmacogenomic datasets. This capability is crucial for validating findings in drug sensitivity studies, as it can help researchers identify consistent patterns of drug efficacy across various studies, thereby improving the reliability of pharmacogenomic research.\n\n3. **Biomarker Discovery**: By utilizing the improved statistical methods for association testing, researchers can better identify biomarkers that predict patient responses to specific drugs. This can lead to the development of targeted therapies that are tailored to individual patient profiles based on their genetic makeup and the predicted effectiveness of certain drugs.\n\n4. **High-Throughput Screening Analysis**: The methodologies developed in this research can be applied to high-throughput screening data, which is common in drug discovery. The ability to accurately analyze large datasets with many variables can accelerate the identification of promising drug candidates and optimize the drug development process.\n\n5. **Statistical Methodology Advancement**: The findings advocate for the adoption of permutation testing as a standard approach for assessing significance in correlation tests, particularly in biological research where data often do not meet the assumptions of traditional parametric tests. This advancement can improve the robustness and reliability of statistical analyses in various biological and medical research contexts.\n\nIn summary, the research has significant implications for enhancing drug sensitivity testing, improving cross-study replicability, facilitating biomarker discovery, and advancing statistical methodologies in high-throughput biological data analysis."
    },
    {
      "question": "How does the paper address potential biases in the research?",
      "answer": "The paper addresses potential biases in the research primarily through the introduction of robust statistical methods and careful evaluation of existing correlation coefficients in the context of noisy biological data. Here are the key strategies employed:\n\n1. **Introduction of Novel Metrics**: The authors propose two new semiparametric variations of the concordance index, namely the robust concordance index (rCI) and the kernelized concordance index (kCI). These metrics are designed to incorporate measurements about the noise distribution from the data, which helps mitigate biases introduced by measurement noise.\n\n2. **Evaluation of Statistical Tests**: The paper critically evaluates common statistical tests applied to the concordance index and its variations, highlighting their failure to control for false positives, especially in the presence of noise. This evaluation underscores the importance of using appropriate statistical methods to avoid biases in significance testing.\n\n3. **Permutation Testing**: The authors advocate for the use of permutation testing as a more reliable method for assessing significance without the strong parametric assumptions that can lead to biases. They demonstrate that traditional analytical methods can inflate Type I error rates, particularly in smaller sample sizes, which can bias results.\n\n4. **Simulation Studies**: The paper includes extensive simulation studies to assess the power of the proposed metrics and to compare them with traditional correlation coefficients (Pearson and Spearman). By simulating various noise conditions and distributions, the authors can evaluate how different methods perform under realistic scenarios, thus addressing potential biases related to the choice of statistical method.\n\n5. **Robustness to Noise**: The findings indicate that the proposed rCI and kCI metrics show improved performance in detecting significant correlations in the presence of noise compared to traditional methods. This suggests that incorporating noise characteristics into the analysis can reduce biases associated with measurement error.\n\n6. **Multiple Testing Corrections**: The authors discuss the necessity of applying multiple testing corrections to control the expected number of false positives, which is crucial in high-throughput biological studies where many correlations are tested simultaneously.\n\nOverall, the paper emphasizes the importance of using robust statistical methods and careful evaluation of biases in the context of high-dimensional biological data, particularly in drug sensitivity studies."
    },
    {
      "question": "What validation methods were used to ensure the reliability of the results?",
      "answer": "The research paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Petr Smirnov et al. employed several validation methods to ensure the reliability of the results:\n\n1. **Permutation Testing**: The authors introduced adaptive permutation testing to compute p-values, which helps control for false positives. This method does not rely on parametric assumptions about the data distribution and is particularly useful in the context of high-dimensional biological data.\n\n2. **Simulation Studies**: The statistical power of the proposed robust concordance index (rCI) and kernelized concordance index (kCI) was evaluated through simulation studies. These simulations included varying sample sizes and levels of noise to assess how well the different correlation coefficients performed under realistic conditions.\n\n3. **Comparison with Established Metrics**: The performance of the new metrics (rCI and kCI) was compared against traditional correlation coefficients such as Pearson and Spearman correlations. This benchmarking involved analyzing their power in detecting significant associations in both simulated and real pharmacogenomic datasets.\n\n4. **Robustness to Measurement Noise**: The study specifically examined how well the different statistics controlled for measurement noise, which is prevalent in biological data. The authors found that the Pearson correlation was particularly robust to noise, while the rCI and kCI showed improvements in power under certain conditions.\n\n5. **Cross-Dataset Replicability**: The authors evaluated the performance of the correlation coefficients in matching drugs across pharmacogenomic datasets, assessing their ability to rank matched drugs correctly. This practical application served as a real-world validation of the proposed methods.\n\n6. **Type I Error Control**: The paper discussed the Type I error rates associated with common tests applied to correlation statistics, highlighting the importance of using permutation tests to avoid inflated false positive rates, especially in smaller sample sizes.\n\nThese methods collectively contributed to the validation of the results and the robustness of the statistical approaches proposed in the study."
    },
    {
      "question": "Are there any contradictions between this paper's findings and previous research?",
      "answer": "The paper \"Evaluation of statistical approaches for association testing in noisy drug screening data\" by Smirnov et al. presents findings that both align with and contradict previous research in the field of statistical analysis of biological data, particularly in the context of drug sensitivity screening.\n\n### Contradictions with Previous Research:\n\n1. **Robustness of Pearson Correlation**: The authors found that the Pearson correlation coefficient was surprisingly more robust to measurement noise than the proposed robust concordance index (rCI) and kernelized concordance index (kCI). This finding contradicts some previous studies that suggested non-parametric methods (like Spearman and Kendall's Tau) would generally outperform Pearson correlation in the presence of noise and non-normal distributions. For instance, prior work indicated that non-parametric statistics are preferred when dealing with skewed or bounded distributions, which is common in biological data.\n\n2. **Power of Concordance Index**: The paper indicates that the standard concordance index (CI) was more powerful under permutation tests in detecting non-zero correlations for bounded and skewed distributions, which contrasts with earlier studies that have not consistently shown the CI to outperform other correlation measures in similar contexts. Previous research often emphasized the limitations of CI in high-dimensional settings, particularly when ties and noise are present.\n\n3. **Type I Error Control**: The authors observed that common statistical tests applied to the concordance index and its variations failed to control for false positives effectively, particularly in smaller sample sizes. This finding challenges the assumptions made in earlier studies that analytical tests could provide reliable p-values in these scenarios, suggesting a need for caution when applying traditional methods without considering the specific characteristics of the data.\n\n### Summary:\nWhile the paper builds on existing literature by introducing new statistical measures (rCI and kCI) and advocating for permutation testing as a more reliable method for significance testing, it also presents findings that challenge the prevailing views on the robustness of non-parametric statistics and the effectiveness of traditional analytical methods in the context of noisy biological data. The authors' results highlight the complexity of statistical analysis in high-throughput drug screening and suggest that established practices may need to be re-evaluated in light of their findings."
    }
  ]
}